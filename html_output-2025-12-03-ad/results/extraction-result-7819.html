<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-df18feef3766466ad73bd3e7e7ab88ad2c4d9b36</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df18feef3766466ad73bd3e7e7ab88ad2c4d9b36" target="_blank">LLM4ED: Large Language Models for Automatic Equation Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data, substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.</p>
                <p><strong>Paper Abstract:</strong> Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-guided Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-guided Automatic Equation Discovery of Nonlinear Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses prompting of large language models to generate candidate symbolic equations in string form, parses them with SymPy into expression trees, fits constants (sparse regression for PDEs, BFGS for ODEs), scores candidates with a normalized RMSE + parsimony penalty, and iteratively refines elites via two LLM-guided optimization modes: self-improvement (local edits using in-context equation-score examples) and evolutionary search (LLM-directed crossover/mutation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large Language Models for Automatic Equation Discovery of Nonlinear Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5-turbo (default); experiments also run with Llama2-7B and GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Prompted LLMs with manual prompt engineering and in‑context examples (few‑shot); LLMs used as generators and as black‑box optimizers (no fine‑tuning, no chain‑of‑thought training), guided iterative prompting for self‑improvement and for evolutionary operators.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not a collection of scholarly papers: input is observational / simulated numerical data from canonical PDE systems (Burgers, Chafee–Infante, Kuramoto–Sivashinsky, PDE_divide, Fisher–KPP, 2D Navier–Stokes) with explicit spatial/time discretizations (see Table 3) and ODEBench / Strogatz ODE trajectories (16 1D ODEs) — data sizes and discretizations reported per system (e.g., Burgers x in [-8,8) m=256, t in [0,10] n=201). No corpus of papers or text preprocessing was used as input for law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-based symbolic generation + LLM-guided optimization; pipeline: (1) prompt LLM to generate diverse equation strings from a predefined symbol library; (2) parse strings with SymPy to expression trees; (3) determine numeric constants (PDEs: split terms and perform sparse regression (STRidge-like) to obtain coefficients; ODEs: optimize embedded constants via BFGS / scipy.optimize.minimize); (4) compute fitness score; (5) keep top-K elites in a priority queue; (6) iterate by supplying elites as in‑context examples and instructing the LLM to do either local edits (self-improvement using equation-score pairs) or global evolutionary operations (LLM-directed crossover and mutation) in alternating cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Physical governing equations (differential equations): both partial differential equations (PDEs) and ordinary differential equations (ODEs) describing nonlinear dynamical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Algebraic/differential expression strings converted to symbolic expression trees (SymPy); PDEs represented as linear combinations of function terms Theta(u,x) times coefficient vector xi (Theta(u,x) · xi); ODEs represented as symbolic skeletons with numeric constants fitted.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Simulated / numerical datasets for canonical PDEs listed in Table 3 (Burgers, Chafee–Infante, KS, PDE_divide, Fisher-KPP, 2D Navier–Stokes) with explicit spatial/time discretizations; ODE evaluation on 16 one-dimensional systems from ODEBench / Strogatz (Table 6), each with two trajectory sets (train / test initial conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>PDEs: coefficient error E = (1/n) sum |xi* - xi| / |xi| ×100%; ODEs: reconstruction and generalization measured by R^2 between solution trajectories and observed trajectories; internal fitness score S = (1 - ζ1 × m) / (1 + NRMSE) where NRMSE = (1/σ_{dot x}) sqrt( (1/N) sum (dot x - F(x))^2 ), and m is number of terms (parsimony penalty ζ1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PDE coefficient errors (mean ± std): Burgers 1.25 ± 1.63%; Chafee–Infante 0.05 ± 0.03%; KS 0.5 ± 0.2%; PDE_divide 0.15 ± 0.09%; Fisher–KPP 1.34 ± 0.38%; Navier–Stokes (vorticity form) 0.15 ± 0.09% (see Table 3). ODE discovery: across 16 ODEs, reconstruction R^2 > 0.99 on training for 15/16 systems (93.75%) and R^2 > 0.99 on test (new initial condition) for 11/16 systems (68.75%) in their reported best runs (Table 4); per-equation R^2 reported in Table 4 (many entries 0.999 on train; test values vary). Reported LLM backbone results (Table 5): GPT-4 achieved 15/16 training R^2>0.99 and 11/16 test R^2>0.99; GPT-3.5-turbo: 13/16 train >0.99, 12/16 test >0.99; Llama2-7B: lower counts and higher invalids. Hyperparameters: M=10 generated expressions per iteration, P=100 total iterations, K=5 priority queue, LLM temperature T=0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to PySR (evolutionary SR) and ODEformer (transformer-based SR). Summary from Table 5: PySR achieved 15/16 (93.75%) R^2>0.99 on training and 10/16 (62.5%) R^2>0.99 on test; ODEformer achieved 11/16 (68.75%) train R^2>0.99 and 6/16 (37.5%) test R^2>0.99. Authors report their LLM-guided framework attains comparable reconstruction performance and better generalization than PySR and ODEformer on ODEBench.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Numerical validation: fit discovered F̂, numerically integrate ODE/PDE (solution trajectories), compare to observed trajectories (R^2); for PDEs validate by coefficient error relative to ground-truth coefficients; tests include held‑out initial conditions for ODE generalization; repeated experiments (multiple runs) and success rates reported; ablation studies compare optimization strategies (self‑improvement, genetic algorithm, alternating).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note limitations: (1) input is observational/simulated data, not textual scholarly corpora — framework depends on data quality and spatial/temporal discretization; (2) dependence on LLM capability: weaker LLMs (Llama2-7B) produced more invalid equations and worse GA/self‑improvement behavior; (3) self‑improvement can get trapped in local optima while GA may be unstable and depends on initial population quality — they mitigate by alternating methods; (4) robustness to sparse/noisy observations not fully validated and requires further study; (5) computational latency dominated by remote LLM API calls (10–40s per iteration), and API cost and parallelization tradeoffs noted; (6) need for better natural-language prompt designs to incorporate prior knowledge and reduce search space.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Prompts and prompt templates (initialization, self‑improvement, GA) are provided in Appendix A; LLM outputs are constrained by symbol libraries; SymPy used as a domain tool to parse/validate generated strings; constants solved via STRidge/sparse regression for PDEs and BFGS nonlinear optimization for ODEs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4ED: Large Language Models for Automatic Equation Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models as evolutionary optimizers <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Mathematical discoveries from program search with large language models <em>(Rating: 2)</em></li>
                <li>SymbolicGPT: A generative transformer model for symbolic regression <em>(Rating: 1)</em></li>
                <li>ODEformer: Symbolic regression of dynamical systems with transformers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7819",
    "paper_id": "paper-df18feef3766466ad73bd3e7e7ab88ad2c4d9b36",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [
        {
            "name_short": "LLM-guided Equation Discovery",
            "name_full": "Large Language Model-guided Automatic Equation Discovery of Nonlinear Dynamics",
            "brief_description": "A framework that uses prompting of large language models to generate candidate symbolic equations in string form, parses them with SymPy into expression trees, fits constants (sparse regression for PDEs, BFGS for ODEs), scores candidates with a normalized RMSE + parsimony penalty, and iteratively refines elites via two LLM-guided optimization modes: self-improvement (local edits using in-context equation-score examples) and evolutionary search (LLM-directed crossover/mutation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Large Language Models for Automatic Equation Discovery of Nonlinear Dynamics",
            "llm_name": "GPT-3.5-turbo (default); experiments also run with Llama2-7B and GPT-4",
            "llm_type": "Prompted LLMs with manual prompt engineering and in‑context examples (few‑shot); LLMs used as generators and as black‑box optimizers (no fine‑tuning, no chain‑of‑thought training), guided iterative prompting for self‑improvement and for evolutionary operators.",
            "input_corpus_description": "Not a collection of scholarly papers: input is observational / simulated numerical data from canonical PDE systems (Burgers, Chafee–Infante, Kuramoto–Sivashinsky, PDE_divide, Fisher–KPP, 2D Navier–Stokes) with explicit spatial/time discretizations (see Table 3) and ODEBench / Strogatz ODE trajectories (16 1D ODEs) — data sizes and discretizations reported per system (e.g., Burgers x in [-8,8) m=256, t in [0,10] n=201). No corpus of papers or text preprocessing was used as input for law extraction.",
            "extraction_method": "LLM-based symbolic generation + LLM-guided optimization; pipeline: (1) prompt LLM to generate diverse equation strings from a predefined symbol library; (2) parse strings with SymPy to expression trees; (3) determine numeric constants (PDEs: split terms and perform sparse regression (STRidge-like) to obtain coefficients; ODEs: optimize embedded constants via BFGS / scipy.optimize.minimize); (4) compute fitness score; (5) keep top-K elites in a priority queue; (6) iterate by supplying elites as in‑context examples and instructing the LLM to do either local edits (self-improvement using equation-score pairs) or global evolutionary operations (LLM-directed crossover and mutation) in alternating cycles.",
            "law_type": "Physical governing equations (differential equations): both partial differential equations (PDEs) and ordinary differential equations (ODEs) describing nonlinear dynamical systems.",
            "law_representation": "Algebraic/differential expression strings converted to symbolic expression trees (SymPy); PDEs represented as linear combinations of function terms Theta(u,x) times coefficient vector xi (Theta(u,x) · xi); ODEs represented as symbolic skeletons with numeric constants fitted.",
            "evaluation_dataset": "Simulated / numerical datasets for canonical PDEs listed in Table 3 (Burgers, Chafee–Infante, KS, PDE_divide, Fisher-KPP, 2D Navier–Stokes) with explicit spatial/time discretizations; ODE evaluation on 16 one-dimensional systems from ODEBench / Strogatz (Table 6), each with two trajectory sets (train / test initial conditions).",
            "evaluation_metrics": "PDEs: coefficient error E = (1/n) sum |xi* - xi| / |xi| ×100%; ODEs: reconstruction and generalization measured by R^2 between solution trajectories and observed trajectories; internal fitness score S = (1 - ζ1 × m) / (1 + NRMSE) where NRMSE = (1/σ_{dot x}) sqrt( (1/N) sum (dot x - F(x))^2 ), and m is number of terms (parsimony penalty ζ1).",
            "performance_results": "PDE coefficient errors (mean ± std): Burgers 1.25 ± 1.63%; Chafee–Infante 0.05 ± 0.03%; KS 0.5 ± 0.2%; PDE_divide 0.15 ± 0.09%; Fisher–KPP 1.34 ± 0.38%; Navier–Stokes (vorticity form) 0.15 ± 0.09% (see Table 3). ODE discovery: across 16 ODEs, reconstruction R^2 &gt; 0.99 on training for 15/16 systems (93.75%) and R^2 &gt; 0.99 on test (new initial condition) for 11/16 systems (68.75%) in their reported best runs (Table 4); per-equation R^2 reported in Table 4 (many entries 0.999 on train; test values vary). Reported LLM backbone results (Table 5): GPT-4 achieved 15/16 training R^2&gt;0.99 and 11/16 test R^2&gt;0.99; GPT-3.5-turbo: 13/16 train &gt;0.99, 12/16 test &gt;0.99; Llama2-7B: lower counts and higher invalids. Hyperparameters: M=10 generated expressions per iteration, P=100 total iterations, K=5 priority queue, LLM temperature T=0.9.",
            "baseline_comparison": true,
            "baseline_performance": "Compared to PySR (evolutionary SR) and ODEformer (transformer-based SR). Summary from Table 5: PySR achieved 15/16 (93.75%) R^2&gt;0.99 on training and 10/16 (62.5%) R^2&gt;0.99 on test; ODEformer achieved 11/16 (68.75%) train R^2&gt;0.99 and 6/16 (37.5%) test R^2&gt;0.99. Authors report their LLM-guided framework attains comparable reconstruction performance and better generalization than PySR and ODEformer on ODEBench.",
            "validation_method": "Numerical validation: fit discovered F̂, numerically integrate ODE/PDE (solution trajectories), compare to observed trajectories (R^2); for PDEs validate by coefficient error relative to ground-truth coefficients; tests include held‑out initial conditions for ODE generalization; repeated experiments (multiple runs) and success rates reported; ablation studies compare optimization strategies (self‑improvement, genetic algorithm, alternating).",
            "limitations": "Authors note limitations: (1) input is observational/simulated data, not textual scholarly corpora — framework depends on data quality and spatial/temporal discretization; (2) dependence on LLM capability: weaker LLMs (Llama2-7B) produced more invalid equations and worse GA/self‑improvement behavior; (3) self‑improvement can get trapped in local optima while GA may be unstable and depends on initial population quality — they mitigate by alternating methods; (4) robustness to sparse/noisy observations not fully validated and requires further study; (5) computational latency dominated by remote LLM API calls (10–40s per iteration), and API cost and parallelization tradeoffs noted; (6) need for better natural-language prompt designs to incorporate prior knowledge and reduce search space.",
            "notes": "Prompts and prompt templates (initialization, self‑improvement, GA) are provided in Appendix A; LLM outputs are constrained by symbol libraries; SymPy used as a domain tool to parse/validate generated strings; constants solved via STRidge/sparse regression for PDEs and BFGS nonlinear optimization for ODEs.",
            "uuid": "e7819.0",
            "source_info": {
                "paper_title": "LLM4ED: Large Language Models for Automatic Equation Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models as evolutionary optimizers",
            "rating": 2
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2
        },
        {
            "paper_title": "Mathematical discoveries from program search with large language models",
            "rating": 2
        },
        {
            "paper_title": "SymbolicGPT: A generative transformer model for symbolic regression",
            "rating": 1
        },
        {
            "paper_title": "ODEformer: Symbolic regression of dynamical systems with transformers",
            "rating": 2
        }
    ],
    "cost": 0.011185249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models for Automatic Equation Discovery of Nonlinear Dynamics</h1>
<p>Mengge Du<br>College of Engineering Peking University Beijing</p>
<p>Yuntian Chen<br>Ningbo Institute of Digital Twin, Eastern Institute of Technology<br>Ningbo<br>ychen@eitech.edu.cn</p>
<p>Zhongzheng Wang<br>College of Engineering<br>Peking University<br>Beijing</p>
<p>Longfeng Nie<br>School of Environmental Science and Engineering<br>Southern University of Science and Technology<br>Shenzhen</p>
<h2>Dongxiao Zhang</h2>
<p>Ningbo Institute of Digital Twin
Eastern Institute of Technology, Ningbo
National Center for Applied Mathematics Shenzhen (NCAMS)
Southern University of Science and Technology, Shenzhen
dzhang@eitech.edu.cn</p>
<h4>Abstract</h4>
<p>Equation discovery aims to directly extract physical laws from data and has emerged as a pivotal research domain in nonlinear systems. Previous methods based on symbolic mathematics have achieved substantial advancements but often require handcrafted representation rules and complex optimization algorithms. In this paper, we introduce a novel framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically extracting governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse candidate equations in string form and then evaluate the generated equations based on observations. The best equations are preserved and further refined iteratively using the reasoning capacity of LLMs. We propose two alternately iterated strategies to collaboratively optimize the generated equations. The first strategy uses LLMs as a black-box optimizer to achieve equation self-improvement based on historical samples and their performance. The second strategy instructs LLMs to perform evolutionary operations for a global search. Experiments are conducted on various nonlinear systems described by partial differential equations (PDEs), including Burgers' equation, the Chafee-Infante equation, and the Navier-Stokes equation. Results demonstrate that our framework can discover correct equations that reveal the underlying physical laws. Further comparisons with state-of-the-art models on extensive ordinary differential equations (ODEs) showcase that the equations discovered by our framework possess physical meaning and better generalization capability on unseen data.</p>
<p>Keywords Symbolic equation discovery $\cdot$ Large language models $\cdot$ Evolutionary search $\cdot$ Prompt learning.</p>
<h2>1 Introduction</h2>
<p>Physical laws often follow concise governing equations, which are crucial for our understanding and transformation of the natural world. With the development of artificial intelligence, simulation of the evolution of nonlinear systems through deep learning has gradually emerged [1, 2, 3]. However, these methods are limited by black-box models and lack interpretability. To tackle this issue, equation discovery methods that uncover potential physical laws from</p>
<p>observations with explicit mathematical formulas have received increasing attention, which can not only facilitate a deeper understanding of physical processes but also provide domain guidance for data-driven models and enhance their predictive robustness [4, 5]. Moreover, with the governing equation incorporated as physical constraints, neural networks can be equipped with physical intuition and possess better extrapolation ability [6, 7].
In nonlinear systems, states of interest often follow various differential equations, such as ordinary differential equations, in the form of $\dot{\mathbf{x}}=f(\mathbf{x}(t))$, where $\mathbf{x}(t)=\left{x_{1}(t), x_{2}(t), \ldots, x_{n}(t)\right}^{T} \in \mathbb{R}^{m}$ denotes the state variables with the spatial dimension of $m$. The main objective of equation discovery is to find the explicit expression of $f$. Traditionally, this process was based on first principles, which often require experts in the relevant domain to engage in extensive mathematical derivations. In recent years, data-driven methods are gradually rising because of their superior efficiency and applicability [8, 9]. In particular, SINDy (Sparse Identification of Nonlinear Dynamics) has emerged as an effective method to tackle this challenge [10]. It assumes that the form of $f$ can be simplified as a linear combination of a series of candidate basis functions, where the basis function library is often predetermined based on prior knowledge. With the advantages of high computational efficiency and simple methodology, SINDy has achieved good performance across various fields [11, 12, 13, 14]. Nevertheless, the reliance on prior knowledge inherently constrains the applicability of this approach, rendering it challenging to uncover more intricate representational forms. Concurrently, the progress of numerous intelligent optimization algorithms has contributed to the utilization of symbolic mathematics in identifying governing equations with more flexible forms. EQL (Equation Learner) [15, 16] endeavors to utilize the topological structure of networks to represent equations with different combinations and substitute activations with arithmetic operators, such as + and - . An alternative approach seeks to represent equations with expression trees, aiming to discover the optimal equation by optimizing the tree structure. Common optimization methods are based on gradient descent [17, 18, 19], reinforcement learning [20, 21, 22, 23, 24], or evolutionary algorithms [25, 26, 27]. These approaches substantially diminish the reliance on prior physical knowledge, enabling wider application scenarios. However, laborious and intricate algorithm design and coding efforts are required for equation generation and optimization, which is not conducive to wide-scale promotion.
Transformer-based large language models (LLMs) have continuously emerged and have achieved remarkable results in various application domains in recent years [28, 29, 30]. A vast number of trainable parameters and a large diverse training corpus enable LLMs to possess strong generation and reasoning capabilities. Some recent studies have started to explore the potential of LLMs in mathematical reasoning [31], algorithmic optimization [32], and code generation [33], with some even employing LLMs as direct optimizers to tackle black-box optimization challenges [34]. A salient question is whether we can leverage LLMs to automatically complete equation discovery without additional parametric models and optimization processes.
In this paper, we propose a LLM-based framework for automatic equation discovery, as shown in Fig. 1. Initial equations are first generated in string format after prompting LLMs with a clear symbol library and problem descriptions. The equations can be seamlessly parsed and transformed into expression trees via the domain tool in symbolic mathematics and evaluated based on the score function and data. Elite equations are preserved in the priority queue and incorporated into prompts to guide iterative optimization by LLMs. During the optimization phase, LLMs can serve as an optimizer to conduct the self-improvement process. Some local refinements are applied to the historical equations based on the analysis of the inherent relationship between the combinations of symbols and their performance. In addition, well-designed prompts are used to guide LLMs to apply user-defined evolution operators on elite equations, promoting the generation of more diverse equation combinations. These two approaches are iteratively employed in an alternating manner to refine the structure of generated equations until the optimal equation satisfies the termination conditions. Our framework has been tested for uncovering the correct PDE equations in several canonical nonlinear systems and has verified that the two optimization approaches of local modification and random evolution have a synergistic effect. In addition, we further validated our framework on sixteen one-dimensional ODE systems, and the results showed that it could achieve comparable performance to the state-of-the-art and have better generalization capabilities. Our main contributions are as follows:</p>
<ul>
<li>We propose an automated equation discovery framework that utilizes the natural language generation and reasoning capabilities of LLMs. The framework eliminates the need for manually crafting intricate programs for equation generators and optimizers and is totally parametric-free during optimization.</li>
<li>We employ manually designed prompts to guide LLMs in executing two optimization approaches: selfimprovement and evolutionary search. The alternating iterative optimization strategy effectively strikes a balance between exploration and exploitation.</li>
<li>We validate the efficacy of our framework through a series of experiments on ODEs and PDEs. The results demonstrate that its performance is on par with or even better than the state-of-the-art symbolic regression (SR) methods, especially in generalization capability. The framework encourages more extensive research and application of LLMs in the domain of equation discovery.</li>
</ul>
<p>Large Language Models for Automatic Equation Discovery of Nonlinear Dynamics</p>
<h1>2 Related Works</h1>
<h3>2.1 Symbolic Equation discovery</h3>
<p>Symbolic mathematics-based methods can directly uncover the potential relationships between variables from data. With the development of computational equipment and machine learning, these methods have gradually gained increasing attention. Equation discovery tasks typically encompass three phases: generation, evaluation, and optimization. In the generation stage, based on certain context-free s [35, 36], equations in mathematical form are typically transformed into expression trees. The internal nodes of the expression tree are predefined operators (e.g.,,$+,-$ ) and operands (e.g., observations $x$ or constant). By conducting a top-down traversal of the expression, a unique sequential representation can be generated. This representation is more concise and enables more efficient batch generation and gradient-based optimization [20, 37]. Some constraints are carefully designed to generate dimensional consistent expressions and ensure the physical and mathematical rationality. In the evaluation stage, the main focus is to assess the performance of the discovered equations in terms of their fit to the data and complexity. Finally, in the optimization stage, the commonly utilized algorithms mainly include genetic programming [38], gradient descent-based neural network models [15], and recently emerging reinforcement learning models [20, 21]. At the same time, pretrained models based on transformers have gradually emerged [39, 18, 40, 41]. These models are trained on a large amount of data and can directly output the discovered equation results based on the observations, greatly accelerating the inference speed. Evidently, approaches founded on symbolic mathematics necessitate manually designed algorithms in multiple aspects, elevating the learning and application barrier. Conversely, our framework, guided by natural language, significantly streamlines the generation and optimization components, enabling researchers to concentrate solely on the evaluation aspect, where domain expertise is genuinely essential.</p>
<h3>2.2 Large language model for optimization</h3>
<p>The powerful language understanding and generation capabilities of large models have led to their extensive application in various fields [28, 29, 30, 42]. Studies have recently demonstrated the feasibility of employing prompt engineering to direct LLMs in addressing optimization problems. One approach is to directly use LLMs as optimizers in a selfimprovement updating manner [43, 44]. Taking into account the problem definition and previously generated solutions, LLMs can be directed to refine candidate solutions iteratively. The findings suggest that LLMs possess the capability to progressively improve the generated solutions by building upon the knowledge gained from past optimization results. Other related works attempt to combine LLMs with evolutionary search methods to solve optimization problems. Prompts can be designed to instruct LLMs to execute evolutionary algorithms to incrementally enhance the existing solutions within the population. This synergistic combination ultimately leads to the discovery of novel insights and advancements in addressing open research questions, including combinatorial optimization problems like (e.g. traveling salesman problems [34]), multiobjective evolutionary optimization [45], prompt optimization [46], algorithm design [32, 31], game design [47], and evolutionary strategies [48].
Our method pioneered the application of LLMs in the field of equation discovery, constructing a plug-and-play discovery framework. By leveraging natural language, we have seamlessly integrated the self-improvement capabilities of LLMs with evolutionary search techniques, which effectively strikes a balance between exploitation and exploration. The proposed method ensures the stability and efficiency of optimization while finding the globally optimal equation.</p>
<h2>3 Methods</h2>
<h3>3.1 Problem overview</h3>
<p>The goal of the equation discovery task is to identify an explicit mathematical expression $\mathcal{F}$, defined by mathematical symbols, based on a given set of observations. The true form $\mathcal{F}$ should satisfy</p>
<p>$$
\dot{x}=\mathcal{F}(x ; \xi), \quad \mathcal{F}: \mathbb{R}^{D} \rightarrow \mathbb{R}
$$</p>
<p>where the state variable $x(t) \in \mathbb{R}^{D} ; \dot{x}$ refers to the time derivatives; and $\xi$ denotes the possible constants. We aim to find an optimal expression $\mathcal{F}$ that accurately describes the true underlying physical laws in the dynamical system while keeping the form concise. The form of $\mathcal{F}$ may differ slightly for nonlinear systems governed by different types of equations. In this paper, we consider two types of governing equations: PDEs and ODEs. For ODEs, the form of equations can be generated by freely combining symbols from a predefined library, including constants $\xi$. The value of $\xi$ is typically determined using optimization techniques that minimize a specific data fitting metric, such as the mean square error (MSE). For PDEs, the right-hand side of the equation often consists of the combinations of state variables (e.g., $u$ ) and their spatial derivatives (e.g., $u_{x}$ and $u_{x x}$ ). For example, the Burgers' equation is represented</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the proposed framework.
as $u_{t}=a u u_{x}+b u_{x x}$. Similar to the previous SINDy-based methods [10, 49], we simplify $\mathcal{F}$ to be represented by a linear combination of a series of basis function terms $\Theta(u, x)$. The difference is that function terms can be represented by any combination of symbols without constants rather than the predefined monomials. Constants only appear as coefficients of the function terms, i.e., $\mathcal{F} \approx \Theta(u, x) \cdot \xi$. The coefficients $\xi$ of the function terms can then be obtained through sparse regression.
In this paper, the skeleton of $\mathcal{F}$ is generated and refined by LLMs. We can further empower LLMs with SymPy [50], a domain-specific, open-source Python library for symbolic mathematics, to parse string-form equations and convert them into expression trees, which facilitates the evaluation of data fitting.</p>
<h1>3.2 Framework</h1>
<p>Our framework employs natural language to guide LLMs in generating and refining equations, which is shown in Fig. 2. First, LLMs draw upon extensive prior training data, and they tend to produce mathematically reasonable expressions with concrete descriptions of the physical process. Second, we employ an alternating iterative approach that combines self-improvement and evolutionary search to refine the generated equations. Users are only required to concentrate on establishing appropriate evaluation criteria, i.e., the score function, to precisely evaluate the generated equations. Equations with higher scores are used to update a priority queue that retains the top $K$ optimal samples up to the current iteration, while bad samples are discarded. These elite equations are preserved in equation-score format and can be incorporated as in-context examples in prompts to instruct LLMs to generate better-fitting equations. The specific components and procedures of the framework are described below in detail.</p>
<h3>3.3 Prompt Engineering</h3>
<p>Throughout the entire equation discovery process, the generation and optimization process are instructed with natural language-based prompts, which follow a unified structure in the three processes of initialization, evolution, and self-improvement. The standard format consists of the following components, as shown in Fig. 1.</p>
<ul>
<li>Task descriptions: This part primarily explains the scientific task and defines the symbol library, including operators (e.g.,,$+,-$ and operands (e.g., $x$,const). If the mechanism of the entire physical process is understood, the physical meanings of different variables can also be provided in this context. This is essential for producing effective and reasonable equations and can significantly reduce the search space.</li>
<li>Historical examples: To guide LLMs to generate better-fitting equations, $M$ high-quality equations from past iterations are incorporated as in-context examples in the prompts. The selected equations are chosen from two sources. First, all of the $K$ expressions within the priority queue are used for stable and efficient optimization. Second, we select $M-K$ expressions from the last iteration to maintain sampling diversity. Notably, the presentation of these samples within the prompt varies according to the optimization techniques employed. In the evolution process, only high-quality equations in string format are presented, while in the self-improvement process, historical samples are shown in the form of equation-score pairs.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Workflow of the proposed framework.</p>
<ul>
<li>Instructions: This part is primarily aimed at guiding LLMs to generate and refine equations. In the initialization stage, LLMs are required to freely combine symbols from the library to produce equations of arbitrary form and length. In the optimization stage, LLMs are mainly guided to generate refined equations based on historical equations according to different optimization strategies. For the self-improvement process, how to conduct the local modifications should be clearly introduced. The instructions in the LLM-guided evolutionary search should emphasize the concrete implementation details of the selection and evolution of equations.</li>
<li>Other hints or constraints: If the relationship of physical variables is available, we can directly describe the requirements for the structure of the generated equations through natural language. In the optimization stage, we can further define local modifications and evolutionary operators and provide possible examples as few-shot prompts. In addition, some hints about the format of outputs can be incorporated here.</li>
</ul>
<p>The utilized prompts in this paper are demonstrated in Appendix A.</p>
<h1>3.4 Initialization</h1>
<p>The initial equation population can be generated through LLMs or based on prior knowledge, i.e., manually predefined equations. In this study, we employ prompts to direct LLMs in randomly generating the initial population with a given symbol library and problem descriptions. First, LLMs have been trained on extensive text data, enabling them to learn numerous effective equation representations. Consequently, the generated equations generally follow mathematical principles. Second, constraints can be established using natural language, thereby preventing the occurrence of equations that violate the specified conditions. For instance, constraints can include restricting the equation length, frequency of specific symbols, and preventing the generation of invalid nested combinations. Traditionally, implementing these constraints necessitated intricate code, such as generating equations based on probabilistic context-free s [35, 51] or subtly modifying probabilities during the symbol sampling process [20].</p>
<h1>3.5 Evaluation</h1>
<p>LLMs excels at creative generation based on enormous corpus, but need to be further strengthened with domain tools and human-designed feedback to deal with the symbolic discovery task. Regarding the equation skeletons that have been generated in string format, we can employ Sympy [50] to parse and instantiate them as corresponding symbolic expression trees. Before evaluating them, we first need to determine the parameters in the expressions, i.e., constants, and then further score them according to the designed score function.</p>
<h3>3.5.1 Constant optimization</h3>
<p>This study considers two types of governing equations: PDEs and ODEs. Depending on the specific features of the equations they represent, we adopt two distinct approaches to evaluate the constants. For PDEs, constants mainly appear as coefficients of function terms. Therefore, we first need to decompose the expression tree by splitting it into equation terms based on the " + " and "-" operators at the top of the tree, and then further solve for the coefficients using sparse regression methods, as shown in Fig. 3. Terms with nontrivial coefficients will be kept and others will be removed for simplicity.</p>
<p>$$
\xi_{p d e}^{*}=\arg \min <em t="t">{\xi}\left|\Theta(u, x) \cdot \xi-u</em>\right|<em 2="2">{2}^{2}+\lambda|\xi|</em>
$$}^{2</p>
<p>For ODEs, constants can appear at any position in the expression tree. We first generate equation skeleton through LLMs, and then utilize the Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) [52] to execute the following optimization objective.</p>
<p>$$
\xi_{o d e}^{*}=\operatorname{argmin}<em i="1">{\xi} \sum</em>
$$}^{n} \frac{1}{n}\left(\dot{x}-\mathcal{F}\left(x_{i} ; \xi\right)\right)^{2</p>
<p>Rounds of optimization iterations are performed using scipy.optimize.minimize to ultimately determine all the constants in the expression tree. Note that if the generated equations do not contain constant operands, sparse regression techniques can be employed to assign coefficients other than 1 to each term, thereby improving the accuracy of the discovered equations and facilitating the identification of lengthy true equations.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Determination of PDE coefficients.</p>
<h3>3.5.2 Score function</h3>
<p>After obtaining the values of the constants in the equation, we designed a score function to evaluate the performance of the generated equations.</p>
<p>$$
\begin{gathered}
S=\frac{1-\zeta_{1} \times m}{1+N R M S E} \
N R M S E=\frac{1}{\sigma_{\dot{x}}} \sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(\dot{x}<em i="i">{t</em>
\end{gathered}
$$}}-\mathcal{F}\left(x_{i}\right)\right)^{2}</p>
<p>where the normalized root-mean-square error (NRMSE) is employed as a fitness metric to evaluate the discrepancy between the left and right sides of the equation. We penalize the number of equation terms $m$ in the equation numerator to encourage finding more concise forms and $\zeta$ refers to the penalty coefficient. Through the designed score function, we can assign a score to each equation, then select elite equations, and introduce them into the prompt to guide subsequent optimization.</p>
<h1>3.6 Optimization</h1>
<p>This study utilizes two LLM-guided optimization techniques to enhance the equation refinement process. The selfimprovement method primarily performs local modifications based on the equation's performance, while the genetic algorithm-based approach is employed for a global search on the elite equations. Our goal is to achieve a better balance between exploration and exploitation.</p>
<h3>3.6.1 Self-improvement process</h3>
<p>LLMs have been demonstrated in numerous experiments to function as gradient-free optimizers, possessing the ability to draw inferences from historical data and iteratively optimize to produce superior samples [34]. We include historical elite equations and their corresponding scores as equation-score pairs within the prompt, enabling LLMs to perform local modifications using these data. The modifications primarily encompass two facets: (1) recognizing and eliminating redundant equation terms by leveraging historical data; (2) incorporating and generating novel random equation terms built upon existing equations. These two operations resemble the introduction of "delete" and "add" operators, which can effectively utilize the historical elite samples and aptly supplement the potentially unstable updating of genetic algorithms. An example case of the self-improvement process is shown in Fig. 4 and the customized prompts are demonstrated in Appendix A.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Self-improvement process executed by LLMs.</p>
<h3>3.6.2 Equation evolution process</h3>
<p>Genetic algorithms are one of the commonly used global optimization methods inspired by natural selection [53, 54]. Evolutionary operators can be applied to the parent individuals to generate new offspring. In particular, this procedure requires an intricate design and application on tree structures in symbolic regression. In this paper, we employ natural language to guide LLMs in the execution of the genetic algorithms, rather than relying on manual coding. Specifically, we conduct crossover and mutation operations on the $M$ equation populations generated in the past, thereby producing a greater variety of equation combinations. The process consists of two steps:</p>
<p>Construct parent population Historical elite equations will be incorporated into the prompt for the evolution process, originating from two sources: a predefined priority queue caching the top $K$ historically elite equations and high-quality samples selected from the last iteration. By combining them, we ultimately retain the $M$ better-performing equations as the parent population.</p>
<p>Selection and evolution The entire process comprises three steps. First, LLMs randomly select two equations from the population as parents and then guide them to perform equation crossover to produce new equations. This process can involve both the crossover of entire equation terms and the crossover within equation terms. Finally, further mutations of operands or operators are performed based on the new equations. Ultimately, iterating the three steps until $M$ offspring are produced. The whole process is directed and performed in natural language and is shown schematically in Fig. 5.</p>
<p>Self-improvement based on local modifications effectively utilizes the reasoning capabilities of LLMs to identify the direct mapping relationships between symbol combinations and scores, thereby refining equations in a gradientfree manner. However, this approach is prone to becoming trapped in local optima. On the other hand, genetic algorithms exhibit strong global optimization abilities, but their effectiveness heavily depends on the quality of the initial population, resulting in possible updating instability. By integrating these two strategies and employing a dynamic buffer of high-quality equations, the evolution and refinement process of equations can be significantly complemented and enhanced.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Crossover and mutation executed by LLMs.</p>
<h1>4 Resutls</h1>
<h3>4.1 Evaluation metrics</h3>
<p>The experimental section provides the discovery results of the suggested framework for both PDEs and ODEs. We consider PDE equations to be represented as a linear combination of equation terms of arbitrary form, and the constants are primarily solved through sparse regression. Our goal is to find the exact equation form, and the accuracy of the identified equation is assessed by determining the equation's coefficient error.</p>
<p>$$
E=\frac{1}{n} \sum_{i=1}^{n} \frac{\left|\xi_{i}^{*}-\xi_{i}\right|}{\left|\xi_{i}\right|} \times 100 \%
$$</p>
<p>where $n$ denotes the total number of function terms; $\xi_{i}, \xi_{i}^{*}$ refer to the true coefficients and identified coefficients, respectively. ODEs are more complex in symbolic form. A skeleton with defined symbols needs to be constructed first, followed by the optimization of constants within the skeleton, which may generate more symbol combinations. Compared to identifying the most consistent expression in symbolic form, it is more crucial and meaningful to conduct numerical evaluation. Specifically, we aim to find an effective $\hat{\mathcal{F}}$, whose solution trajectories approximate the observed $x$ in the current numerical domain, i.e., all of the expressions are evaluated by the reconstruction accuracy. Furthermore, the other critical criterion is that the solution of the identified $\hat{\mathcal{F}}$ precisely fits the correct trajectories even when the initial condition varies. We utilize the coefficient of determination $\left(R^{2}\right)$ as the metric to evaluate the agreement between the solution trajectories and the true trajectories: $R^{2}=1-\frac{\sum_{i}^{r}\left(x_{i}-\hat{x}<em i="i">{i}\right)^{2}}{\sum</em>$ refers to predicted values.}^{n}\left(x_{i}-\hat{x}\right)^{2}} \in(-\infty, 1]$, where $x_{i}$ denotes observations and $\hat{x}_{i</p>
<h3>4.2 Experiment settings</h3>
<p>The hyperparameters used in the experiments are shown in Table 1. GPT-3.5-turbo is utilized as the default LLM backbone. In terms of the experimental setup, the symbol library and equation assumptions used for mining ODEs and PDEs are slightly different, as shown in Table 2. The library used for mining PDEs has relatively fewer operators and more operands involved and does not include the symbol "const". On the other hand, the library used for mining ODEs covers more mathematical operators, and constants are determined using nonlinear optimization methods, e.g., BFGS [52].</p>
<h3>4.3 PDE discovery task</h3>
<h3>4.3.1 Equations and discovered results</h3>
<p>In the experiments of PDE discovery, we demonstrate the framework's ability to discover the governing equations of six canonical nonlinear systems, including the Burges' equation, Chafee-Infante equation, PDE_divide equation with fractional structure, Kuramoto-Sivashinsky (KS) equation with fourth-order derivatives, nonlinear Fisher-Kolmogorov-Petrovsky-Piskunov (Fisher-KPP) equations with a square of the spatial derivative, and twodimensional Navier-Stokes (NS) equation. With the default parameter configuration, our approach accurately identifies the correct structure of the equations while maintaining minimal coefficient error, as shown in Fig. 3. Notably, in</p>
<p>Table 1: Default hyperparameter settings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Default value</th>
<th style="text-align: center;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Number of expressions generated at each iteration</td>
</tr>
<tr>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">Number of total iterations</td>
</tr>
<tr>
<td style="text-align: center;">$K$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Size of the priority queue</td>
</tr>
<tr>
<td style="text-align: center;">$N_{t e r m}$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Maximum number of function terms</td>
</tr>
<tr>
<td style="text-align: center;">$\zeta_{1}$</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">Parsimony penalty factor for redundant function terms</td>
</tr>
<tr>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">Weight of the STRidge regularization term</td>
</tr>
<tr>
<td style="text-align: center;">$T$</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">LLM temperature</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Discovered results under different optimization methods.
comparison to fixed candidate set methods, our framework reduces the dependence on prior knowledge, enabling the discovery of more complex equation forms, such as equations with fractional or compound structures.</p>
<h1>4.3.2 Comparison of different optimization strategies</h1>
<p>We further verified the effectiveness of the proposed LLM-guided iterative optimization. Three optimization methods are primarily discussed and compared: (1) using only the self-improvement optimization method; (2) using only the genetic algorithm; and (3) the alternating iterative method combining the two methods (as proposed in the framework). The identification experiments for the aforementioned equations were replicated ten times, with each experiment's maximum iteration count set to 50 , to further examine the efficiency of various methods. Fig. 6 illustrates that the iterative approach combining both methods yielded the highest frequency of discovering the correct equations, with recovery rates consistently surpassing $80 \%$, outperforming the outcomes achieved by employing a single optimization technique. Fig. 6(b) depicts the success rate of the ultimately identified equations. It is noteworthy that despite the self-improvement method outperforming the genetic algorithm and exhibiting higher optimization efficiency in some systems, such as the Burgers' equation, it is more prone to converging to local optima. When the iteration count is extended to 100 steps, the symbolic success rate of optimization employing the genetic algorithm approach surpasses $80 \%$ for all of the equations, demonstrating its superior global optimization capability, whereas self-improvement hardly achieves significant improvement.
We provide further detailed analysis with the Chafee-Infante, Burgers, and NS equations utilized as examples. Fig. 7 illustrates the evolution of the maximum score throughout the optimization process. It reveals that the optimization efficiency of the alternating approach combining both methods is superior, facilitating faster identification of the correct</p>
<p>Table 2: Default experimental settings for discovering different systems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Nonlinear Systems</th>
<th style="text-align: center;">Operators</th>
<th style="text-align: center;">Operands</th>
<th style="text-align: center;">Constants optimization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ODE</td>
<td style="text-align: center;">,$+ ,-,\times, \div, \wedge, \sin , \cos , \log , \exp$</td>
<td style="text-align: center;">$x$, const</td>
<td style="text-align: center;">Nonlinear</td>
</tr>
<tr>
<td style="text-align: center;">PDE</td>
<td style="text-align: center;">$+,-, \times, \div, \wedge^{2}, \wedge^{3}$</td>
<td style="text-align: center;">$u, x, u_{x}, u_{x x}, u_{x x x}, u_{x x x x}$</td>
<td style="text-align: center;">Linear</td>
</tr>
</tbody>
</table>
<p>Table 3: Summary of canonical nonlinear systems governed by PDEs and discovered results. The subscripts $m$ and $n$ denote the number of discretizations.</p>
<table>
<thead>
<tr>
<th>PDE systems</th>
<th>Form</th>
<th>Coefficient error</th>
<th>Data discretization</th>
</tr>
</thead>
<tbody>
<tr>
<td>Burgers</td>
<td>$u_{t}=-u u_{x}+0.1 u_{x x}$</td>
<td>$1.25 \pm 1.63 \%$</td>
<td>$x \in[-8,8)<em n="201">{m=256}, t \in[0,10]</em>$</td>
</tr>
<tr>
<td>Chafee-Infante</td>
<td>$u_{t}=u_{x x}+u-u^{3}$</td>
<td>$0.05 \pm 0.03 \%$</td>
<td>$x \in[0,3]<em n="200">{m=301}, t \in[0,0.5]</em>$</td>
</tr>
<tr>
<td>KS</td>
<td>$u_{t}=-u u_{x}-u_{x x}-u_{x x x x}$</td>
<td>$0.5 \pm 0.2 \%$</td>
<td>$x \in[-10,10]<em n="256">{m=512}, t \in[0,20]</em>$</td>
</tr>
<tr>
<td>PDE_divide</td>
<td>$u_{t}=-u_{x} / x+0.25 u_{x x}$</td>
<td>$0.15 \pm 0.09 \%$</td>
<td>$x \in[1,2)<em n="251">{m=100}, t \in[0,1]</em>$</td>
</tr>
<tr>
<td>Fisher-KPP</td>
<td>$u_{t}=0.02 u u_{x x}+0.02\left(u_{x}\right)^{2}+10 u-10 u^{2}$</td>
<td>$1.34 \pm 0.38 \%$</td>
<td>$x \in(-1,1)<em n="99">{m=199}, t \in(0,1)</em>$</td>
</tr>
<tr>
<td>NS</td>
<td>$\omega_{t}=0.1 \omega_{x x}+0.1 \omega_{y y}-u \omega_{x}-v \omega_{y}$</td>
<td>$0.15 \pm 0.09 \%$</td>
<td>$\begin{gathered} x \in[0,6.5]<em m="m" y="325">{m=325}, y \in[0,3.4]</em>$}, \ t \in[0,30]_{n=150} \end{gathered</td>
</tr>
</tbody>
</table>
<p>equations in various equation discovery tasks. Utilizing the Chafee-Infante equation as an illustration, we further examine the density distribution of scores at each iteration throughout the optimization process. Fig. 8 illustrates that the self-improvement strategy exhibits a propensity for local modifications on historical elite equations, resulting in the overall score resembling an incremental trend. Conversely, GA excel at global searches, identifying equations with higher diversity, albeit at the cost of potentially compromised optimization efficiency. Employing alternating iterations of both methods proves more advantageous in striking a balance between exploration and exploitation.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Evolution of the maximum score with different methods while discovering the Chafee-Infante equation, Burgers' equation and NS equation.</p>
<h1>4.4 ODE discovery task</h1>
<h3>4.4.1 Discovered results</h3>
<p>In this section, we tested our framework on 16 one-dimensional ODEs in a comprehensive benchmark named ODEbench [41], which has been utilized to describe real-world phenomena by Strogatz [55]. The equation information is listed in Appendix B. Each equation contains two sets of trajectories with two different initial conditions. We used one</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Distribution of scores at different iterations with different methods while discovering the Chafee-Infante equation. Numbers in the figure refer to different iteration steps.</p>
<p>Table 4: Discovered results and the reconstruction and generalization performance on the Strogatz dataset. We ran the experiments three times and presented the best expression for each ODE.</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Discovered form</th>
<th>Parameters</th>
<th>$R^{2}$ (train)</th>
<th>$R^{2}$ (test)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ODE-1</td>
<td>$c_{0}+c_{1} x$</td>
<td>[-0.3608,0.3031]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-2</td>
<td>$c_{0} x^{2}+c_{1} x$</td>
<td>[-0.0106,0.7899]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-3</td>
<td>$c_{0} \sin x+c_{1} x^{2}+c_{2} e^{c_{3} x} \sin x+c_{4}$</td>
<td>[0.219,0.0563,0.0024,1.1,-0.1145]</td>
<td>0.999</td>
<td>0.727</td>
</tr>
<tr>
<td>ODE-4</td>
<td>$c_{0} x^{2}+c_{1}$</td>
<td>[-0.0021,9.8098]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-5</td>
<td>$c_{0} x \log \left(c_{1} x\right)$</td>
<td>[0.032,2.2901]</td>
<td>0.999</td>
<td>0.973</td>
</tr>
<tr>
<td>ODE-6</td>
<td>$c_{0} x^{3}+c_{1} x^{2}+c_{2} x$</td>
<td>[-0.00024,0.033,-0.1408]</td>
<td>0.996</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-7</td>
<td>$c_{0} * x^{2}-c_{1} * x * \sin ^{2}(x)-c_{2} * \sin (x) * \cos (x)$</td>
<td>[1.2539,-1.2231,-0.7192]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-8</td>
<td>$c_{0} x^{3}+c_{1}$</td>
<td>[-1.2554,0.0318]</td>
<td>0.979</td>
<td>0.958</td>
</tr>
<tr>
<td>ODE-9</td>
<td>$c_{0} \sin (x)+c_{1} \sin (x) \cos (x)$</td>
<td>[-0.0981, 0.9511]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-10</td>
<td>$c_{0} x^{3}+c_{1} x^{3}+c_{2} x$</td>
<td>[-0.0009,0.0399,0.1004]</td>
<td>0.992</td>
<td>0.978</td>
</tr>
<tr>
<td>ODE-11</td>
<td>$c_{0} x^{2}+c_{1} x+c_{2}$</td>
<td>[-0.004,0.3976,-0.0293]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-12</td>
<td>$c_{0} \sin ^{2}(x)+c_{1} x+c_{2} \cos (x)+c_{3}$</td>
<td>[0.464,0.907,2.7834,-2.7836]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-13</td>
<td>$c_{0} * \exp \left(c_{1} * x\right)-c_{2} * \sin (x) / x+c_{3}$</td>
<td>[-0.2779,2.0, 9.7688,10.1468]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-14</td>
<td>$c_{0}-c_{1} x-c_{2} e^{-x}$</td>
<td>[1.1998,0.2,-0.9998]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td>ODE-15</td>
<td>$c_{0} x^{2}+c_{1} x+c_{2} \sin (x)+c_{3}$</td>
<td>[-0.1682,-0.2768,-0.5337,1.4144]</td>
<td>0.999</td>
<td>0.977</td>
</tr>
<tr>
<td>ODE-16</td>
<td>$c_{0}-c_{1} \sin (x)$</td>
<td>[0.21,-0.9995]</td>
<td>0.999</td>
<td>0.999</td>
</tr>
</tbody>
</table>
<p>set of trajectory data as training data and searched to find the optimal $\hat{\mathcal{F}}$. During the evaluation process, we consider the solution trajectory of the ODE associated with $\mathcal{F}$ as predicted results and utilize the $R^{2}$ score as the evaluation criterion to measure the fitting accuracy in comparison to the actual trajectory. The $R^{2}$ value on the training set represents the accuracy of the reconstruction, while the $R^{2}$ value on the test set with a new initial condition signifies the generalization performance. As shown in Table 4, we repeated the search process for each equation three times and reported the best results among them. It can be seen that in our framework, the percentage of equations with $R^{2}$ greater than 0.99 on the training set is $93.75 \%(15 / 16)$, and on the test set, the percentage of equations with $R^{2}$ greater than 0.99 is $68.75 \%$. Equations with $R^{2}$ greater than 0.9 exceeded $90 \%$ on both the training and test sets. Fig. 9 presents detailed prediction results for each equation. Most solution trajectories are consistent with the true trajectories.</p>
<h1>4.4.2 Comparisons with SR benchmarks and ablation studies</h1>
<p>We conducted a further comparison of the performance of SR benchmarks. Two symbolic regression benchmarks are utilized as the baseline models, including PySR [56] and ODEformer [41]. PySR is a practical and high-performance library for symbolic regression, based on a multi-population evolutionary algorithm. PySR is designed for single-</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Predicted solution trajectories on training and test sets.
instance datasets and has been broadly adopted for interpretable symbolic discovery. ODEformer is based on pretrained transformers and achieves SOTA on ODEbench's datasets. The aforementioned methods were tested according to their default hyperparameter configurations. PySR is implemented with a population size of 40 (the number of populations running) and total iterations of 40 . ODEformer is implemented with a beam size of 20 and a temperature of 0.1 .</p>
<p>As illustrated in Table 4, our framework demonstrates equivalent reconstruction performance to the aforementioned methods designed for symbolic regression, while exhibiting superior generalization capabilities and usability. Note that PySR, which is based on evolutionary search, possesses powerful search capabilities and can discover equations that are numerically accurate on the training dataset. However, these discovered equations tend to have more complex structures and, consequently, are susceptible to exhibiting poor generalization performance.</p>
<p>Table 5: Evaluation on 16 ODEs with different methods and LLM backbones. We counted the number of equations discovered by different methods and LLMs that meet the corresponding $R^{2}$ criteria. When the $R^{2}$ value is negative, or the solution trajectories of the ODE associated with $\mathcal{F}$ exhibit numerical overflow, we deem the discovered equation as "Invalid".</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Training set</th>
<th></th>
<th></th>
<th>Testing set</th>
<th></th>
<th></th>
<th>Symbolically correct</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$R^{2}&gt;0.99$</td>
<td>$R^{2}&gt;0.9$</td>
<td>Invaid</td>
<td>$R^{2}&gt;0.99$</td>
<td>$R^{2}&gt;0.9$</td>
<td>Invaid</td>
<td></td>
</tr>
<tr>
<td>PySR [56]</td>
<td>15 (93.75\%)</td>
<td>15 (93.75\%)</td>
<td>1 (6.25\%)</td>
<td>10 (62.5\%)</td>
<td>12 (75\%)</td>
<td>4 (25\%)</td>
<td>6 (37.5\%)</td>
</tr>
<tr>
<td>ODEformer [41]</td>
<td>11 (68.75\%)</td>
<td>14(87.5\%)</td>
<td>2 (12.5\%)</td>
<td>6 (37.5\%)</td>
<td>9 (56.25\%)</td>
<td>5 (31.25\%)</td>
<td>3 (18.75\%)</td>
</tr>
<tr>
<td>Llama2 7B</td>
<td>11 (68.75\%)</td>
<td>12 (75\%)</td>
<td>4 (25\%)</td>
<td>8 (50\%)</td>
<td>11 (68.75\%)</td>
<td>4 (25\%)</td>
<td>5 (31.25\%)</td>
</tr>
<tr>
<td>GPT-3.5-turbo</td>
<td>13 (81.25\%)</td>
<td>13 (81.25\%)</td>
<td>3 (18.75\%)</td>
<td>12 (75\%)</td>
<td>13 (81.25\%)</td>
<td>3 (18.75\%)</td>
<td>8 (50\%)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>15 (93.75\%)</td>
<td>16 (100\%)</td>
<td>0 (0\%)</td>
<td>11 (68.75\%)</td>
<td>15 (93.75\%)</td>
<td>0 (0\%)</td>
<td>8 (50\%)</td>
</tr>
</tbody>
</table>
<p>Additionally, we performed a comparative analysis of the performance of LLM backbones with varying parameter sizes and language capabilities on this task. We used the open-source large language model Llama2 with 7B parameters [57] and more advanced models, including GPT-3.5-turbo and GPT-4. The results indicate that as the capability of LLMs improves, the identified equations become relatively more accurate on both the training set and the test set. This can be primarily attributed to the fact that the capability of the LLM directly exerts a substantial influence on the generation and optimization of equations. On the one hand, large models with inferior capabilities may struggle to effectively understand and execute the provided instructions, such as the constraints we defined, leading to the generation of numerous invalid equations. Furthermore, they tend to fail to properly execute GA instructions and accurately perform crossover and mutation operations. Conversely, the model's reasoning capability directly influences its self-improvement optimization capabilities. It is also worth noting that as the capabilities and number of parameters of large models further increase, the gains in accuracy diminish, especially on test sets.</p>
<p>In addition, we further emphasize the efficiency of the proposed framework. The total running time is the product of the time per iteration and the total number of iterations. The time cost per iteration ranges from 10s to 40s under the default configuration and mainly includes the time spent on remotely accessing the LLM API and evaluating the feedback from the LLM, i.e., the generated equations. The time consumed for accessing the API interface exhibits a positive correlation with the number of samples generated and is roughly an order of magnitude larger than the time for evaluations. In practice, we can allocate separate processes to query the LLM in parallel. On one hand, this approach can reduce the total response time. On the other hand, we can increase the number of expressions generated in each iteration, which in turn contributes to more accurate samples for optimization and helps reduce the number of optimization iterations.</p>
<h1>5 Conclusion</h1>
<p>We introduce a novel equation discovery framework guided by LLMs. It aims to facilitate equation discovery across diverse domains, transcending the confines of specialist communities, and making LLM-guided discovery accessible to a broader range of users. The framework leverages the generation and reasoning capabilities of LLMs to automatically generate and optimize equations. We employ natural language-based prompts to guide LLMs in conducting iterative optimization using self-improvement and genetic algorithms. The results indicate that the two strategies exhibit a strong synergistic effect, effectively balancing exploration and exploitation. Experiments demonstrate that the proposed framework can discover effective and accurate equations (including both PDEs and ODEs), directly from data. More importantly, our framework achieved performance comparable to the SR benchmarks, and the discovered equations tend to be more physically reasonable and possess better generalization ability. The impact of LLM capabilities on the performance of mined equations is also thoroughly discussed. The proposed framework demonstrates great potential in applying LLMs to scientific discovery and providing interpretability for complex physical phenomena.
The current framework has the potential for enhancement in two aspects. First, the training corpus of LLMs encompasses descriptions of diverse intricate physical processes and phenomena; therefore, providing a depiction of the nonlinear system to be mined and information on physical variables could be crucial for LLMs to generate rational equation forms. Further exploration is required to design natural language-based prompts that incorporate prior knowledge to effectively reduce the search space and enhance search efficiency and equation mining precision. Second, additional experimental validation is necessary to determine the robustness of equation mining using LLMs, particularly in scenarios with sparse and noisy observations. Integrating more efficient evaluation techniques might be essential for addressing more intricate situations.</p>
<h1>Acknowledgments</h1>
<p>This work was supported and partially funded by the National Center for Applied Mathematics Shenzhen (NCAMS), the Shenzhen Key Laboratory of Natural Gas Hydrates (Grant No. ZDSYS20200421111201738), the SUSTech - Qingdao New Energy Technology Research Institute, the China Meteorological Administration Climate Change Special Program (CMA-CCSP) (Grant No. QBZ202316), and the National Natural Science Foundation of China (Grant No. 62106116).</p>
<h2>Code and data availability</h2>
<p>The implementation details of the whole process and relevant data are available on GitHub at https://github.com/menggedu/EDL.</p>
<h2>A Prompts</h2>
<p>The prompts utilized for initialization, self-improvement, and GA are shown in Fig. 10, Fig. 11, and Fig. 12, respectively. Different colors represent different components in the prompt. Specifically, red represents task descriptions, blue represents instructions, green represents historical samples, and brown represents other hints and constraints.</p>
<h2>Initialization by LLM</h2>
<p>You will help me find the optimal governing equation from some data. Given the symbol library including operators: [+, -, ${ }^{*}, /, \wedge 2, \wedge 3$ ] and operands:[u, u_x, u_xx, u_xxx, u_xxxx, x], You are required to generate equations using symbolic representations. I will evaluate them and provide their corresponding scores based on their fitness to data.</p>
<p>Now randomly generate ${M}$ diverse expressions with different lengths and be as creative as you can under the constraints below.
(1) Do not include coefficients.
(2) Do not omit the multiplication operator.
(3) Only use the symbols provided in the symbol library.</p>
<p>Do not write code and do not give any explanation.</p>
<p>Output:</p>
<ol>
<li>$\mathrm{u} _\mathrm{x}^{\wedge} 2+\mathrm{u} _\mathrm{xx}$</li>
<li>$\mathrm{u} _\mathrm{x} * \mathrm{u} _\mathrm{x}+\mathrm{u} _\mathrm{xx}$</li>
<li>$\mathrm{u} _\mathrm{xxx} * \mathrm{x}+\mathrm{u} _\mathrm{x}^{\wedge} 3$</li>
</ol>
<p>Figure 10: Prompt utilized in initialization.</p>
<h2>B ODE datasets</h2>
<p>The ODE datasets are selected from ODEBench [41] with different skeletons. The information for each ode is shown in Table 6.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Prompt utilized for self-improvement.</p>
<h1>References</h1>
<p>[1] Kyongmin Yeo and Igor Melnyk. Deep learning algorithm for data-driven simulation of noisy dynamical system. Journal of Computational Physics, 376:1212-1231, 2019.
[2] Jiaqing Kou and Weiwei Zhang. Data-driven modeling for unsteady aerodynamics and aeroelasticity. Progress in Aerospace Sciences, 125:100725, 2021.
[3] Gang Zheng, Xiaofeng Li, Rong-Hua Zhang, and Bin Liu. Purely satellite data-driven deep learning forecast of complicated tropical instability waves. Science advances, 6(29):eaba1482, 2020.
[4] Samaneh Sadat Mousavi Astarabadi and Mohammad Mehdi Ebadzadeh. Genetic programming performance prediction and its application for symbolic regression problems. Information Sciences, 502:418-433, 2019.
[5] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physicsinformed machine learning. Nature Reviews Physics, 3(6):422-440, 2021.
[6] Lu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G Johnson. Physicsinformed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6):B1105-B1132, 2021.
[7] Yuntian Chen and Dongxiao Zhang. Integration of knowledge and data in machine learning. arXiv preprint arXiv:2202.10337, 2022.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Conduct GA by LLM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Given the symbol library including operators: $\left[+,-, *, /, \wedge 2, \wedge 3\right]$ and <br> operands:[u, u_x, u_xx, u_xxx, u_xxxx, x] and following set of <br> equations: <br> 1. {equation 1} <br> 2. {equation 2} <br> 3. {equation 3} <br> $\cdots$ <br> please follow the instructions step-by-step to generate new equations: <br> 1. Select two different set of terms from above and bracket them with <select> and <br> $&lt;/$ select $&gt;$ <br> 2. Crossover two set of terms chosen in step 1 and generate a new equation <br> bracketed with <cross> and </cross>. <br> 3. Mutate the equation generated in Step 2 and generate a new equation bracketed <br> with <res> and </res>. <br> 4. Repeat step 1, 2, 3 for 10 times and directly give me the generated equations of <br> each step. <br> Crossover: Select half terms of each set and recombine the selected with ' + ' and ' $\cdot$ ' <br> to generate a new equation. <br> Mutate: Replace operators or operands of the equation only with new ones defined <br> in the symbol library. <br> Be creative, do not give the scores of the equation and do not give additional <br> explanations.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Output:</td>
<td style="text-align: center;"><code>Step 1: &lt;select&gt;\{u_x^3+u^3\}&lt;/select&gt; &lt;select&gt;\{u_x*u_xxx-u*x\}&lt;/select&gt; Step 2: &lt;cross&gt;\{u_x^3 +u*x\}&lt;/cross&gt; Step 3: &lt;res&gt;\{u^3-u/x\}&lt;/res&gt; ...</code></td>
</tr>
</tbody>
</table>
<p>Figure 12: Prompt utilized for evolutionary search.
[8] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):8185, 2009 .
[9] Kathleen Champion, Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445-22451, 2019.
[10] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl. Acad. Sci., 113(15):3932-3937, 2016.
[11] Daniel E Shea, Steven L Brunton, and J Nathan Kutz. Sindy-bvp: Sparse identification of nonlinear dynamics for boundary value problems. Phys. Rev. Res., 3(2):023255, 2021.</p>
<p>Table 6: Scalar ODEs from ODEBench [41].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">System description</th>
<th style="text-align: center;">Equation</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">initial conditions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">RC-circuit (charging capacitor)</td>
<td style="text-align: center;">$\frac{c_{0}-\frac{x_{0}}{c_{2}}}{c_{2}}$</td>
<td style="text-align: center;">$0.7,1.2,2.31$</td>
<td style="text-align: center;">[10.0], [3.54]</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Population growth with carrying capacity</td>
<td style="text-align: center;">$c_{0} x_{0} \cdot\left(1-\frac{x_{0}}{c_{1}}\right)$</td>
<td style="text-align: center;">$0.79,74.3$</td>
<td style="text-align: center;">[7.3], [21.0]</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">RC-circuit with non-linear resistor (charging capacitor)</td>
<td style="text-align: center;">$-0.5+\frac{1}{c^{\left(0-\frac{x_{0}}{c_{1}}\right.}+1}$</td>
<td style="text-align: center;">$0.5,0.96$</td>
<td style="text-align: center;">$[0.8],[0.02]$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Velocity of a falling object with air resistance</td>
<td style="text-align: center;">$c_{0}-c_{1} x_{0}^{2}$</td>
<td style="text-align: center;">9.81, 0.0021175</td>
<td style="text-align: center;">[0.5], [73.0]</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Gompertz law for tumor growth</td>
<td style="text-align: center;">$c_{0} x_{0} \log \left(c_{1} x_{0}\right)$</td>
<td style="text-align: center;">0.032, 2.29</td>
<td style="text-align: center;">[1.73], [9.5]</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Logistic equation with Allee effect</td>
<td style="text-align: center;">$c_{0} x_{0}\left(-1+\frac{x_{0}}{c_{1}}\right)\left(1-\frac{x_{0}}{c_{1}}\right)$</td>
<td style="text-align: center;">$0.14,130.0,4.4$</td>
<td style="text-align: center;">[6.123], [2.1]</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Refined language death model for two languages</td>
<td style="text-align: center;">$c_{0} x_{0}^{c_{1}} \cdot\left(1-x_{0}\right)-x_{0} \cdot\left(1-c_{0}\right)\left(1-x_{0}\right)^{c_{1}}$</td>
<td style="text-align: center;">$0.2,1.2$</td>
<td style="text-align: center;">[0.83], [0.34]</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Overdamped bead on a rotating hoop</td>
<td style="text-align: center;">$c_{0}\left(c_{1} \cos \left(x_{0}\right)-1\right) \sin \left(x_{0}\right)$</td>
<td style="text-align: center;">0.0981, 9.7</td>
<td style="text-align: center;">[3.1], [2.4]</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Budworm outbreak with predation (dimensionless)</td>
<td style="text-align: center;">$c_{0} x_{0} \cdot\left(1-\frac{x_{0}}{c_{1}}\right)-\frac{x_{0}^{2}}{x_{0}^{2}+1}$</td>
<td style="text-align: center;">$0.4,95.0$</td>
<td style="text-align: center;">[44.3], [4.5]</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Landau equation (typical time scale tau = 1)</td>
<td style="text-align: center;">$c_{0} x_{0}-c_{1} x_{0}^{3}-c_{2} x_{0}^{5}$</td>
<td style="text-align: center;">$0.1,-0.04,0.001$</td>
<td style="text-align: center;">[0.94], [1.65]</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Improved logistic equation with harvesting/fishing</td>
<td style="text-align: center;">$c_{0} x_{0} \cdot\left(1-\frac{x_{0}}{c_{1}}\right)-\frac{c_{2} x_{0}}{c_{1}+x_{0}}$</td>
<td style="text-align: center;">$0.4,100.0,0.24,50.0$</td>
<td style="text-align: center;">[21.1], [44.1]</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Improved logistic equation with harvesting/fishing (dimensionless)</td>
<td style="text-align: center;">$-\frac{c_{0} x_{0}}{c_{1}+x_{0}}+x_{0} \cdot\left(1-x_{0}\right)$</td>
<td style="text-align: center;">$0.08,0.8$</td>
<td style="text-align: center;">[0.13], [0.03]</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Autocatalytic gene switching (dimensionless)</td>
<td style="text-align: center;">$c_{0}-c_{1} x_{0}+\frac{x_{0}^{2}}{x_{0}^{2}+1}$</td>
<td style="text-align: center;">$0.1,0.55$</td>
<td style="text-align: center;">[0.002], [0.25]</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Dimensionally reduced SIR infection model for dead people (dimensionless)</td>
<td style="text-align: center;">$c_{0}-c_{1} x_{0}-e^{-x_{0}}$</td>
<td style="text-align: center;">$1.2,0.2$</td>
<td style="text-align: center;">[0.0], [0.8]</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Hysteretic activation of a protein expression (positive feedback, basal promoter expression)</td>
<td style="text-align: center;">$c_{0}+\frac{c_{1} x_{0}^{2}}{c_{2}+x_{0}^{2}}-c_{3} x_{0}$</td>
<td style="text-align: center;">$1.4,0.4,123.0,0.89$</td>
<td style="text-align: center;">[3.1], [6.3]</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">Overdamped pendulum with constant driving torque/fireflies/Josephson junction (dimensionless)</td>
<td style="text-align: center;">$c_{0}-\sin \left(x_{0}\right)$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">[-2.74], [1.65]</td>
</tr>
</tbody>
</table>
<p>[12] Daniel A Messenger and David M Bortz. Weak sindy for partial differential equations. J. Comput. Phys., 443:110525, 2021.
[13] Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics. Proceedings of the Royal Society A, 476(2242):20200279, 2020.
[14] Urban Fasel, J Nathan Kutz, Bingni W Brunton, and Steven L Brunton. Ensemble-sindy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control. Proceedings of the Royal Society A, 478(2260):20210904, 2022.
[15] Georg Martius and Christoph H Lampert. Extrapolation and learning equations. arXiv preprint arXiv:1610.02995, 2016.
[16] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation and control. In International Conference on Machine Learning, pages 4442-4450. PMLR, 2018.
[17] Pierre-Alexandre Kamienny, Guillaume Lample, Sylvain Lamprier, and Marco Virgolin. Deep generative symbolic regression with monte-carlo-tree-search. In International Conference on Machine Learning, pages 15655-15668. PMLR, 2023.
[18] Mojtaba Valipour, Bowen You, Maysum Panju, and Ali Ghodsi. Symbolicgpt: A generative transformer model for symbolic regression. arXiv preprint arXiv:2106.14131, 2021.
[19] Wenqiang Li, Weijun Li, Linjun Sun, Min Wu, Lina Yu, Jingyi Liu, Yanjie Li, and Songsong Tian. Transformerbased model for symbolic regression via joint supervised learning. In The Eleventh International Conference on Learning Representations, 2022.
[20] Brenden K Petersen, Mikel Landajuela Larma, Terrell N Mundhenk, Claudio Prata Santiago, Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations, 2021.
[21] Fangzheng Sun, Yang Liu, Jian-Xun Wang, and Hao Sun. Symbolic physics learner: Discovering governing equations via monte carlo tree search. In International Conference on Learning Representations, 2023.</p>
<p>[22] Mengge Du, Yuntian Chen, and Dongxiao Zhang. Discover: Deep identification of symbolically concise open-form partial differential equations via enhanced reinforcement learning. Phys. Rev. Res., 6:013182, Feb 2024.
[23] Yuntian Chen, Yingtao Luo, Qiang Liu, Hao Xu, and Dongxiao Zhang. Symbolic genetic algorithm for discovering open-form partial differential equations (sga-pde). Phys. Rev. Res., 4(2):023174, 2022.
[24] Mengge Du, Yuntian Chen, Longfeng Nie, Siyu Lou, and Dongxiao Zhang. Physics-constrained robust learning of open-form partial differential equations from limited and noisy data. Physics of Fluids, 36(5), 2024.
[25] Samaneh Sadat Mousavi Astarabadi and Mohammad Mehdi Ebadzadeh. Genetic programming performance prediction and its application for symbolic regression problems. Information Sciences, 502:418-433, 2019.
[26] Samaneh Sadat Mousavi Astarabadi and Mohammad Mehdi Ebadzadeh. Genetic programming performance prediction and its application for symbolic regression problems. Inf. Sci., 502:418-433, 2019.
[27] Sheng Sun, Runhai Ouyang, Bochao Zhang, and Tong-Yi Zhang. Data-driven discovery of formulas by symbolic regression. MRS Bull., 44(7):559-564, 2019.
[28] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1-40, 2023.
[29] Claudia E Haupt and Mason Marks. Ai-generated medical advice—gpt and beyond. Jama, 329(16):1349-1350, 2023.
[30] Augustin Lecler, Loïc Duron, and Philippe Soyer. Revolutionizing radiology with gpt-based models: current applications, future possibilities and limitations of chatgpt. Diagnostic and Interventional Imaging, 104(6):269274, 2023.
[31] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468-475, 2024.
[32] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. An example of evolutionary computation + large language model beating human: Design of efficient guided local search, 2024.
[33] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.
[34] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers. arXiv preprint arXiv:2310.19046, 2023.
[35] Jure Brence, Sašo Džeroski, and Ljupčo Todorovski. Dimensionally-consistent equation discovery through probabilistic attribute grammars. Information Sciences, 632:742-756, 2023.
[36] Alan P Parkes. A concise introduction to languages and machines. Springer Science \&amp; Business Media, 2008.
[37] Terrell Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago, Daniel faissol, and Brenden K Petersen. Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. In Advances in Neural Information Processing Systems, volume 34, pages 24912-24923, 2021.
[38] Maryam Amir Haeri, Mohammad Mehdi Ebadzadeh, and Gianluigi Folino. Statistical genetic programming for symbolic regression. Appl. Soft Comput., 60:447-469, 2017.
[39] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural symbolic regression that scales. In International Conference on Machine Learning, pages 936-945. Pmlr, 2021.
[40] Martin Vastl, Jonáš Kulhánek, Jiří Kubalík, Erik Derner, and Robert Babuška. Symformer: End-to-end symbolic regression using transformer-based architecture. IEEE Access, 2024.
[41] Stéphane d'Ascoli, Sören Becker, Alexander Mathis, Philippe Schwaller, and Niki Kilbertus. Odeformer: Symbolic regression of dynamical systems with transformers, 2023.
[42] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society A, 382(2270):20230254, 2024.
[43] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2024.
[44] Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, and Shou-De Lin. Towards optimizing with large language models, 2023.</p>
<p>[45] Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Large language model for multi-objective evolutionary optimization, 2024.
[46] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations, 2024.
[47] Pier Luca Lanzi and Daniele Loiacono. Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '23, page 1383-1390, New York, NY, USA, 2023. Association for Computing Machinery.
[48] Robert Tjarko Lange, Yingtao Tian, and Yujin Tang. Large language models as evolution strategies, 2024.
[49] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of partial differential equations. Sci. Adv., 3(4):e1602614, 2017.
[50] Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, et al. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, 2017.
[51] Jure Brence, Ljupčo Todorovski, and Sašo Džeroski. Probabilistic grammars for equation discovery. KnowledgeBased Systems, 224:107077, 2021.
[52] John D Head and Michael C Zerner. A broyden—fletcher—goldfarb—shanno optimization procedure for molecular geometries. Chemical physics letters, 122(3):264-270, 1985.
[53] Tom V Mathew. Genetic algorithm. Report submitted at IIT Bombay, page 53, 2012.
[54] Stephanie Forrest. Genetic algorithms. ACM Comput. Surv., 28(1):77-80, 1996.
[55] Steven H Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering. CRC press, 2018.
[56] Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression.jl, 2023.
[57] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
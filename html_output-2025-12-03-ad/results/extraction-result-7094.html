<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7094 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7094</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7094</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-258967345</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.19234v2.pdf" target="_blank">Grammar Prompting for Domain-Specific Language Generation with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even molecule generation (SMILES).</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7094.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7094.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GrammarPrompting-MoleculeGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar Prompting for Class-Specific Molecule Generation (SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot method where an LLM (GPT-3.5) is prompted to (1) predict a specialized SMILES BNF grammar from a small exemplar set of molecules of a target class, and (2) sample novel SMILES strings conditioned on that specialized grammar; outputs are evaluated for chemical validity, diversity, retrosynthetic synthesizability, and class membership.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (prompt-only, few-shot grammar-prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper; general GPT-3.5 pretraining on large web and code corpora (authors note LLMs have likely been exposed to some molecules during pretraining), no chemical-specific fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Few-shot prompting with grammar prompting: interleave exemplar pairs (specialized BNF grammar G[y(i)] and SMILES y(i)); at sample time sample a specialized grammar G from the model conditioned on exemplars, then sample molecules y ~ P_LLM(y | G, exemplars). Sampling performed without constrained decoding (unconstrained to preserve diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (specialized grammars are subsets of SMILES BNF)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Class-specific monomer/molecule generation (Acrylates, Chain Extenders, Isocyanates) intended to yield novel molecules synthesizable from existing molecules</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Specialized grammars encode structural constraints specific to the class (e.g., ring counts, branch structure), but constrained decoding (enforcing y ∈ L(G)) was not used for main sampling because it reduced diversity; thus generation was effectively unconstrained aside from conditioning on the sampled grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Retrosynthesis scoring: Retro* model used to compute an approximate retrosynthesis (synthesizability) score. Morgan fingerprints (ECFP) used to compute pairwise Tanimoto distances for diversity. No wet‑lab or quantum chemistry tools used.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Small exemplar sets from Guo et al. (data used in paper): 32 Acrylates, 11 Chain Extenders, 11 Isocyanates (few-shot setting; exemplar molecules drawn from those sets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (V): % chemically valid SMILES; Diversity (D): average pairwise Tanimoto distance over Morgan fingerprints; Retrosynthesis score (R): % molecules judged synthesizable by Retro*; Membership (M): % molecules that belong to the target monomer class.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 4 (100 sampled molecules per class). Grammar prompting: Acrylates V=98.0%, D=0.74, R=91.0%, M=93.3%; Chain Extenders V=96.3%, D=0.90, R=86.7%, M=94.0%; Isocyanates V=97.7%, D=0.79, R=78.0%, M=96.3%. (Compared in paper against standard prompting and a graph-grammar baseline.)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Constrained decoding based on specialized grammars decreased diversity and sometimes worsened retrosynthesis score; LLMs' capacity to correctly apply SMILES BNF rules is limited (text-focused LLMs have imperfect understanding of SMILES), so predictions remain imperfect; results are preliminary and no wet‑lab synthesis was performed; small exemplar sizes per class limit generality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grammar Prompting for Domain-Specific Language Generation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7094.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7094.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StdPrompting-MoleculeGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Few-Shot Prompting for Molecule Generation (SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where GPT-3.5 is prompted with a small set of exemplar SMILES molecules (no intermediate grammar prediction) and directly samples novel SMILES strings; evaluated with the same chemical metrics as grammar prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (prompt-only, few-shot direct sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper; general GPT-3.5 pretraining on large web and code corpora, no chemical-specific fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Few-shot direct SMILES generation: prompt contains exemplar SMILES only and model samples SMILES strings; sampling hyperparameters given in Appendix (temperature, penalties). Sampling performed without constrained decoding for main comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Class-specific monomer/molecule generation (Acrylates, Chain Extenders, Isocyanates)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No explicit grammar constraint during sampling (unconstrained decoding used in reported experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Retrosynthesis scoring by Retro* and diversity via Morgan fingerprints for evaluation; no integration during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Same small exemplar sets from Guo et al.: 32 Acrylates, 11 Chain Extenders, 11 Isocyanates (few-shot evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (V), Diversity (D: avg pairwise Tanimoto), Retrosynthesis score (R via Retro*), Membership (M).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 4 (100 sampled molecules per class). Standard prompting: Acrylates V=87.7%, D=0.73, R=80.0%, M=76.7%; Chain Extenders V=60.3%, D=0.89, R=72.7%, M=55.7%; Isocyanates V=94.7%, D=0.82, R=78.0%, M=92.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lower validity and membership for some classes (notably Chain Extenders) compared to grammar prompting; unconstrained SMILES sampling can produce invalid or out-of-class molecules; LLMs were not specifically trained for SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grammar Prompting for Domain-Specific Language Generation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7094.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7094.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGrammar-Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-Efficient Graph/Hypergraph Grammar Baseline (Guo et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-LLM baseline that induces a hypergraph grammar from the given exemplar molecules and samples novel molecules according to that induced grammar; used for comparison in the few-shot molecule generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data-efficient graph grammar learning for molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hypergraph grammar (learned from exemplars) — Guo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>data-driven graph/hypergraph grammar induction (non-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Grammar learned solely from the small exemplar sets provided (no external molecule corpora beyond exemplars).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Induce graph/hypergraph grammar from given molecules and sample molecules from the learned grammar (graph-grammar generation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Molecular graph / implicit SMILES (grammar learned over graph fragments / hyperedges)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Class-specific monomer/molecule generation (same classes as LLM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Grammar-derived syntactic constraints implicit in induced hypergraph grammar; no external synthesizability constraints reported.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used as a generative baseline only; evaluation compared using the same metrics (Retro* for retrosynthesis scoring, Morgan fingerprints for diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Learned from the same small exemplar sets (32 / 11 / 11) as used in the LLM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (V), Diversity (D), Retrosynthesis score (R), Membership (M) — same evaluation protocol as LLM methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 4 (100 sampled molecules per class). Reported baseline numbers (from paper): Acrylates V=100%, D=0.83, R=79.0%, M=30.3%; Chain Extenders V=100%, D=0.86, R=72.7%, M=98.3%; Isocyanates V=100%, D=0.93, R=52.2%, M=82.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Because the graph-grammar baseline learns solely from very small exemplar sets, it cannot incorporate external chemical knowledge beyond exemplars (limiting retrosynthetic performance); membership metrics vary widely by class; baseline cannot leverage pretraining knowledge available to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grammar Prompting for Domain-Specific Language Generation with Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Data-efficient graph grammar learning for molecular generation <em>(Rating: 2)</em></li>
                <li>Retro*: learning retrosynthetic planning with neural guided a* search <em>(Rating: 2)</em></li>
                <li>SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>(Rating: 1)</em></li>
                <li>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7094",
    "paper_id": "paper-258967345",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "GrammarPrompting-MoleculeGen",
            "name_full": "Grammar Prompting for Class-Specific Molecule Generation (SMILES)",
            "brief_description": "Few-shot method where an LLM (GPT-3.5) is prompted to (1) predict a specialized SMILES BNF grammar from a small exemplar set of molecules of a target class, and (2) sample novel SMILES strings conditioned on that specialized grammar; outputs are evaluated for chemical validity, diversity, retrosynthetic synthesizability, and class membership.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_type": "decoder-only LLM (prompt-only, few-shot grammar-prompting)",
            "model_size": null,
            "training_data_description": "Not specified in paper; general GPT-3.5 pretraining on large web and code corpora (authors note LLMs have likely been exposed to some molecules during pretraining), no chemical-specific fine-tuning reported.",
            "generation_method": "Few-shot prompting with grammar prompting: interleave exemplar pairs (specialized BNF grammar G[y(i)] and SMILES y(i)); at sample time sample a specialized grammar G from the model conditioned on exemplars, then sample molecules y ~ P_LLM(y | G, exemplars). Sampling performed without constrained decoding (unconstrained to preserve diversity).",
            "chemical_representation": "SMILES strings (specialized grammars are subsets of SMILES BNF)",
            "target_application": "Class-specific monomer/molecule generation (Acrylates, Chain Extenders, Isocyanates) intended to yield novel molecules synthesizable from existing molecules",
            "constraints_used": "Specialized grammars encode structural constraints specific to the class (e.g., ring counts, branch structure), but constrained decoding (enforcing y ∈ L(G)) was not used for main sampling because it reduced diversity; thus generation was effectively unconstrained aside from conditioning on the sampled grammar.",
            "integration_with_external_tools": "Retrosynthesis scoring: Retro* model used to compute an approximate retrosynthesis (synthesizability) score. Morgan fingerprints (ECFP) used to compute pairwise Tanimoto distances for diversity. No wet‑lab or quantum chemistry tools used.",
            "dataset_used": "Small exemplar sets from Guo et al. (data used in paper): 32 Acrylates, 11 Chain Extenders, 11 Isocyanates (few-shot setting; exemplar molecules drawn from those sets).",
            "evaluation_metrics": "Validity (V): % chemically valid SMILES; Diversity (D): average pairwise Tanimoto distance over Morgan fingerprints; Retrosynthesis score (R): % molecules judged synthesizable by Retro*; Membership (M): % molecules that belong to the target monomer class.",
            "reported_results": "Table 4 (100 sampled molecules per class). Grammar prompting: Acrylates V=98.0%, D=0.74, R=91.0%, M=93.3%; Chain Extenders V=96.3%, D=0.90, R=86.7%, M=94.0%; Isocyanates V=97.7%, D=0.79, R=78.0%, M=96.3%. (Compared in paper against standard prompting and a graph-grammar baseline.)",
            "experimental_validation": false,
            "challenges_or_limitations": "Constrained decoding based on specialized grammars decreased diversity and sometimes worsened retrosynthesis score; LLMs' capacity to correctly apply SMILES BNF rules is limited (text-focused LLMs have imperfect understanding of SMILES), so predictions remain imperfect; results are preliminary and no wet‑lab synthesis was performed; small exemplar sizes per class limit generality.",
            "uuid": "e7094.0",
            "source_info": {
                "paper_title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "StdPrompting-MoleculeGen",
            "name_full": "Standard Few-Shot Prompting for Molecule Generation (SMILES)",
            "brief_description": "Baseline approach where GPT-3.5 is prompted with a small set of exemplar SMILES molecules (no intermediate grammar prediction) and directly samples novel SMILES strings; evaluated with the same chemical metrics as grammar prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_type": "decoder-only LLM (prompt-only, few-shot direct sampling)",
            "model_size": null,
            "training_data_description": "Not specified in paper; general GPT-3.5 pretraining on large web and code corpora, no chemical-specific fine-tuning reported.",
            "generation_method": "Few-shot direct SMILES generation: prompt contains exemplar SMILES only and model samples SMILES strings; sampling hyperparameters given in Appendix (temperature, penalties). Sampling performed without constrained decoding for main comparisons.",
            "chemical_representation": "SMILES strings",
            "target_application": "Class-specific monomer/molecule generation (Acrylates, Chain Extenders, Isocyanates)",
            "constraints_used": "No explicit grammar constraint during sampling (unconstrained decoding used in reported experiments).",
            "integration_with_external_tools": "Retrosynthesis scoring by Retro* and diversity via Morgan fingerprints for evaluation; no integration during generation.",
            "dataset_used": "Same small exemplar sets from Guo et al.: 32 Acrylates, 11 Chain Extenders, 11 Isocyanates (few-shot evaluation).",
            "evaluation_metrics": "Validity (V), Diversity (D: avg pairwise Tanimoto), Retrosynthesis score (R via Retro*), Membership (M).",
            "reported_results": "Table 4 (100 sampled molecules per class). Standard prompting: Acrylates V=87.7%, D=0.73, R=80.0%, M=76.7%; Chain Extenders V=60.3%, D=0.89, R=72.7%, M=55.7%; Isocyanates V=94.7%, D=0.82, R=78.0%, M=92.2%.",
            "experimental_validation": false,
            "challenges_or_limitations": "Lower validity and membership for some classes (notably Chain Extenders) compared to grammar prompting; unconstrained SMILES sampling can produce invalid or out-of-class molecules; LLMs were not specifically trained for SMILES generation.",
            "uuid": "e7094.1",
            "source_info": {
                "paper_title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GraphGrammar-Baseline",
            "name_full": "Data-Efficient Graph/Hypergraph Grammar Baseline (Guo et al.)",
            "brief_description": "Non-LLM baseline that induces a hypergraph grammar from the given exemplar molecules and samples novel molecules according to that induced grammar; used for comparison in the few-shot molecule generation experiments.",
            "citation_title": "Data-efficient graph grammar learning for molecular generation",
            "mention_or_use": "use",
            "model_name": "Hypergraph grammar (learned from exemplars) — Guo et al.",
            "model_type": "data-driven graph/hypergraph grammar induction (non-LLM)",
            "model_size": null,
            "training_data_description": "Grammar learned solely from the small exemplar sets provided (no external molecule corpora beyond exemplars).",
            "generation_method": "Induce graph/hypergraph grammar from given molecules and sample molecules from the learned grammar (graph-grammar generation).",
            "chemical_representation": "Molecular graph / implicit SMILES (grammar learned over graph fragments / hyperedges)",
            "target_application": "Class-specific monomer/molecule generation (same classes as LLM experiments)",
            "constraints_used": "Grammar-derived syntactic constraints implicit in induced hypergraph grammar; no external synthesizability constraints reported.",
            "integration_with_external_tools": "Used as a generative baseline only; evaluation compared using the same metrics (Retro* for retrosynthesis scoring, Morgan fingerprints for diversity).",
            "dataset_used": "Learned from the same small exemplar sets (32 / 11 / 11) as used in the LLM experiments.",
            "evaluation_metrics": "Validity (V), Diversity (D), Retrosynthesis score (R), Membership (M) — same evaluation protocol as LLM methods.",
            "reported_results": "Table 4 (100 sampled molecules per class). Reported baseline numbers (from paper): Acrylates V=100%, D=0.83, R=79.0%, M=30.3%; Chain Extenders V=100%, D=0.86, R=72.7%, M=98.3%; Isocyanates V=100%, D=0.93, R=52.2%, M=82.7%.",
            "experimental_validation": false,
            "challenges_or_limitations": "Because the graph-grammar baseline learns solely from very small exemplar sets, it cannot incorporate external chemical knowledge beyond exemplars (limiting retrosynthetic performance); membership metrics vary widely by class; baseline cannot leverage pretraining knowledge available to LLMs.",
            "uuid": "e7094.2",
            "source_info": {
                "paper_title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Data-efficient graph grammar learning for molecular generation",
            "rating": 2
        },
        {
            "paper_title": "Retro*: learning retrosynthetic planning with neural guided a* search",
            "rating": 2
        },
        {
            "paper_title": "SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "rating": 1
        },
        {
            "paper_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
            "rating": 1
        }
    ],
    "cost": 0.012803499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grammar Prompting for Domain-Specific Language Generation with Large Language Models
31 May 2023</p>
<p>Bailin Wang bailinw@mit.edu 
Zi Wang wangzi@google.com 
Xuezhi Wang xuezhiw@google.com 
Yuan Cao yuancao@google.com 
Rif A Saurous 
Yoon Kim yoonkim@mit.edu 
Mit Csail 
Google Deepmind 
Grammar Prompting for Domain-Specific Language Generation with Large Language Models
31 May 202312990EA1F6764FF76BE85871724259ECarXiv:2305.19234v2[cs.CL]
Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples.However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars.We explore grammar prompting as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus-Naur Form (BNF), during incontext learning.Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar.For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar.Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even molecule generation (SMILES).LLM PromptYou are an expert programmer, and you need to write a program for the given natural language query.First, you should write a grammar that contains all the necessary BNF rules.Then, you should write programs that conform to your predicted rules.1 For brevity we forgo the formal tuple-based definition of G and instead define G to be equivalent to its context-free rules.We also freely go back and forth between this set definition of G and its string representation.2 However, when combined with specialized grammars we did observe small improvements by appending the full DSL grammar to the instructions.Hence, for all experiments where G is small enough (GeoQuery, Overnight-B, SMILES), we include G as part of the instruction.See Figure2.3 For instance, the expression "(attendee_?"attendee* ")" depicted in Figure1implicitly defines all occurrences of attendee greater than 1.If this expression is incorporated into a program, either the concrete rule "(attendee_?"attendee ")" or the original rule could be included in the minimal specialized grammar.</p>
<p>Introduction</p>
<p>Prompting large language models (LLMs) with demonstrations and/or natural language instructions has been shown to be an effective approach for surfacing their myriad capabilities acquired through pretraining [9].This approach is however inadequate for applications where the task specifications cannot be fully delineated through just a handful of exemplars, for example in semantic parsing where an LLM must translate a natural language utterance to an executable program in a domainspecific language (DSL).DSLs often incorporate domain-specific abstractions and semantics that are difficult to characterize via just a few demonstrations.And unlike general-purpose programming languages, DSLs are by definition specialized and thus unlikely to have been encountered often enough (or at all) during pretraining for the LLM to acquire its full syntax.</p>
<p>How can we draw on the few-shot learning capabilities of LLMs for applications where it must generate strings in structured output space that is substantially different from those seen during pretraining?This work explores grammar prompting as a simple approach for data-efficient generation of structured languages where an output string in the language can be derived through a series of symbolic manipulations.We exploit the fact that constraints over a structured output space can often be succinctly described by a context-free grammar in Backus-Naur Form (BNF), a commonly-used metalanguage for defining a language's syntax.Grammar prompting augments each in-context example (x, y) with a specialized BNF grammar G[y] that is minimally sufficient for generating y.</p>
<p>Given a new input, the LLM first predicts the BNF grammar and then generates the answer conditioned on the grammar.</p>
<p>Grammar prompting follows the recent line of work which enhances the few-shot reasoning capabilities of LLMs by interleaving intermediate "reasoning" steps between each in-context input and output [49,24,80,76,70].The key difference in our approach is that the intermediate variable is in the form of a formal grammar rather than in natural language, which focuses on eliciting the symbolic manipulation capabilities of LLMs.The use of a formal grammar moreover makes it possible to impose constraints during incremental decoding such that syntactic validity is guaranteed.Finally, unlike chain-of-thought-style prompts [80] which typically require manual verbalization of the intermediate reasoning steps, in our approach the specialized grammar G[y] can be derived automatically by parsing the output y with the full (unspecialized) DSL grammar.</p>
<p>We apply grammar prompting to various domain specific languages for semantic parsing (SM-CalFlow, Overnight, GeoQuery), AI planning (PDDL), and molecule generation (SMILES), and find that it can meaningfully improve upon standard prompting baselines in the few-shot setting.</p>
<p>2 Background and Motivation</p>
<p>Domain-Specific Languages</p>
<p>Let Σ * be the set of all finite strings over an alphabet Σ, and further let D ⊆ Σ * be a domain-specific language (DSL) for an application of interest.Given an input x (e.g., a natural language command) we are interested in generating y ∈ D (e.g., a program in a DSL fulfilling the command), as shown by the following calendar assistant example from SMCalFlow [6]:</p>
<p>x : Add meeting with Jean's manager on Wednesday at 3PM. y : CreateEvent(&amp; (start_?WednesdayNumberPM(3))(attendee_? FindManager(Jean))) DSLs are crafted by experts who use their domain-specific knowledge to incorporate higher-level abstractions than are typically found in general-purpose programming languages.We assume access to an expert-defined grammar G that fully specifies the DSL's syntax.As is the case with many DSLs, we further assume that G is a context-free grammar in Backus-Naur Form (BNF).See Figure 1 for a simple example adapted from SMCalFlow [6].Letting L(G) be the language generated by G, we have D ⊆ L(G) ⊆ Σ * (not all syntactically valid programs are semantically valid).</p>
<p>Few-shot Learning with Large Language Models</p>
<p>In-context learning with large language models (LLMs) has been shown to be an effective approach for few-shot learning [9].Under this approach, a pretrained LLM is conditioned on N demonstration examples (x (i) , y (i) ) N i=1 followed by a test example x, and the output is given by decoding from the prompted LLM, i.e., P LLM (y | x, (x (i) , y (i) ) N i=1 ).The demonstration examples can be optionally preceded by natural language instructions to further improve performance or even enable zero-shot learning [79,60].Recent work has additionally shown that interleaving natural language verbalizations of intermediate reasoning steps between each x (i) and y (i) can greatly improve few-shot performance on complex reasoning tasks [49,80,76,70,15].</p>
<p>event</p>
<p>::= "CreateEvent(" constraint ")" | "QueryEvent(" constraint ")" constraint ::= "(&amp;" constraint constraint ")" | "(start_?"date time?")" | "(attendee_?"attendee* ")" date ::= "Wednesday" | "Monday" number ::= ("0".."9")+ time ::= "NumberAM(" number ")" | "NumberPM(" number ")" attendee The effectiveness of few-shot in-context learning depends on how useful the implicit knowledge acquired through pretraining is for the task, and also on how effectively the task specifications can be conveyed through the demonstrations.Fewshot generation of strings of a DSL, where the structured nature of combinatorial output space (i.e., the DSL grammar G) cannot be adequately captured through just a handful of demonstrations, remains challenging for LLMs.
::= | "Bob" | "Carol" | "Jean" | "FindManager"</p>
<p>Grammar Prompting</p>
<p>Grammar prompting exploits the fact that while the actual strings of a DSL may not have been encountered frequently enough (or at all) during pretraining for the LLM to implicitly acquire its syntax, the LLM will likely have encountered many instances of metalanguages-i.e., languages used to describe other languages.BNF grammars are a standard metalanguage for specifying the a language's syntax, and is expected to occur in the LLM training corpus with some frequency (e.g., in computer science textbooks).We thus focus on using BNF grammars for few-shot DSL generation.</p>
<p>(optional) G:
[BEGIN RULES] . . . [END RULES]
x (1) : find the meeting on Wednesday with Bob and Carol G[y (1) ]: event ::= "QueryEvent(" constraint ")" constraint ::= "(&amp;" constraint constraint ")"</p>
<p>| "(start_?"date ")" | "(attendee_?"attendee attendee ")" date ::= "Wednesday" attendee ::= "Bob" | "Carol" y (1) : QueryEvent(&amp; (start_?Wednesday)(attendee_? Bob Carol)) . . .</p>
<p>x: Add meeting with Jean's manager on Wednesday at 3PM</p>
<p>LLM Output</p>
<p>G: event ::= "CreateEvent(" constraint ")" constraint ::= "(&amp;" constraint constraint ")"</p>
<p>| "(start_?"date time ")" | "(attendee_?"attendee ")" date ::= "Wednesday" time ::= "NumberPM(3)" attendee ::= "FindManager(" attendee ")" | "Jean" y: CreateEvent(&amp; (start_?Wednesday NumberPM( 3))(attendee_?FindManager(Jean)))</p>
<p>Figure 2: Example of grammar prompting for a calendar DSL.We interleave the minimal specialized grammar G[y (i) ] between the demonstrations x (i) and y (i) .During decoding, the LLM first predicts the specialized grammar G, and then predicts the program y conditoned on G.The blue portion is not part of the actual prompt and only shown for illustrative purposes.</p>
<p>Let G = M j=1 {r j } be an extended BNF grammar where each rule r j is of the form <symbol> ::= <expr 1 > | <expr 2 > | ...</p>
<p>Here <symbol> is a nonterminal symbol and each <expr 1 > is a sequence of nonterminal and terminal symbols. 1 A straightforward approach for incorporating a BNF grammar during in-context learning is to simply prepend the string representation of the full grammar G to the demonstration examples.However in preliminary experiments, we found that simply prepending the grammar to standard prompts along with an instruction to use the grammar did not yield any improvements. 2</p>
<p>Specialized Grammars</p>
<p>We instead propose to use specialized grammars to enable LLMs to make better use of domainspecific knowledge and constraints.A specialized grammar G ′ ⊆ G is a grammar obtained from taking a subset of the rules of the full grammar G.We further define a minimal specialized grammar of y to be the smallest BNF grammar (as measured by number of rules) such that y ∈ L(G[y]) (i.e., ∀r ∈ G[y], y ∈ L(G[y] \ {r})).Note that the minimal specialized grammar may not be unique due to the potential instantiation of abstract BNF rules (i.e., extended BNF rules). 3In most applications we consider the rules of the minimal specialized grammar will be concrete.See appendix A.1 for further details.</p>
<p>Grammar prompting feeds a sequence of (x (i) , G[y (i) ], y (i) ) N i=1 along with x as a prompt to an LLM.For inference we first obtain the specialized grammar with an (approximate) arg max decoding
G = arg max G ′ ⊆G P LLM (G ′ | x, (x (i) , G[y (i) ], y (i) ) N i=1 ).
We then obtain the program conditioned on G,
y = arg max y∈L( G) P LLM (y | G, x, (x (i) , G[y (i) ], y (i) ) N i=1 ).
We discuss how to perform constrained decoding with G ⊆ G and y ∈ L( G) in the next section.Grammar prompting views DSL program generation as a grammar specialization process where given a natural language specification x, a set of production rules is selected from G, and then a program y is deduced according to the selected rules.Grammar prompting can also be viewed as an instance of chain-of-thought prompting [49,80] where the intermediate thought is in the form of a formal grammar.However, unlike typical chain-of-thought prompting where the answer is (usually) deterministic given the intermediate reasoning steps, in our case there is still some uncertainty with respect to y given G (e.g., L( G) could still be infinite).end if 13: end while The use of a formal grammar as an intermediate variable makes it possible to enforce grammatical constraints during autoregressive LLM decoding.We first discuss how we enforce the constraint y ∈ L( G).One approach to constrained decoding is to use G to obtain a left-to-right Earley parser [17] and only decode from valid continuations at each decoding step.However this simple strategy poses several practical challenges when working with API-only LLMs.For one, a valid terminal continuation in G may consist of multiple BPE tokens.More seriously, while we can sample a valid continuation at each time step by disallowing invalid tokens, 4 since the set of valid continuations changes at each time step, this strategy would require calling the LLM API at each time step with the full prompt and prefix along with the disallowed continuations, which is prohitively expensive. 5hile there are many methods for grammar-constrained LM decoding [65,62,26], we present a simple strategy which speculatively decodes from the LLM to look ahead for multiple tokens.The pseudocode is shown in Algorithm 1.At each prediction step, we ask the LLM to speculatively decode the full program conditioned on the current prefix (lines 4-5).If the resulting continuation leads to a valid program, we return it (lines 6-7).Otherwise, we consult an Earley parser to extract the longest valid prefix from the current prediction (y prefix ), along with a set of valid terminals that can follow the prefix (Σ[y prefix ]).Finally, we rely on the LLM's probabilities to decide which terminal to use, with which a new partial program can be constructed (lines 10-11). 6 scoring over multi-token terminals, the search procedure is implicitly augmented by looking ahead for a few BPE tokens.</p>
<p>Constrained</p>
<p>We use a similar procedure to operationalize the constraint G ′ ⊆ G, except that EarleyParse is constructed with a metagrammar (i.e., the grammar of G) for grammar prediction.See appendix A.1 for more details.In our ablation study we find that while these constraints are helpful insofar as they guarantee syntactic validity, grammar prompting still meaningfully improves upon standard prompting with even with simple unconstrained decoding.</p>
<p>Experiments</p>
<p>We apply grammar prompting to diverse domains: DSLs for semantic parsing (SMCalFlow, Overnight, GeoQuery), an action DSL (PDDL planning), and a molecule generation DSL (SMILES).These experiments are not necessarily intended to improve upon the state-of-the-art on these benchmarks but rather intended to assess whether LLMs can improve upon standard prompting for fewshot DSL generation by learning to predict and use grammars during in-context learning.</p>
<p>Semantic Parsing for Tool Usage</p>
<p>Software tools are typically accompanied by a collection of human-interpretable APIs which provide a platform for developers to interact programmatically with the tools.These APIs constitute a DSL, where each production rule of the grammar specifies the input and output types for a specific API call (see Figure 1 for an example).These tools demonstrate a broad spectrum in terms of DSL complexity, ranging from single-function tools such as Google(user_query), Translate(sentence, language) to more complex tools such as the entirety of Wolfram language. 7Enabling LLMs to use external tools via APIs is an important step towards enhancing their capabilities [61,54,51,69,45].</p>
<p>We test our approach on standard semantic parsing benchmarks involving complex DSLs: SM-CalFlow [6], which features human-generated utterances about calendar management (see Figure 2); GeoQuery [94] which features queries against a US Geography database; and Overnight-Blocks [77], which features queries about blocks in a synthetic block world.See appendix B for examples of input-output pairs along with the specialized grammars.The original benchmarks target the training of conventional semantic parsers and thus contain hundreds/thousands of training examples.We instead focus on the arguably more practical few-shot setting.Much existing LLM-based work on these benchmark rely on retrieval-based in-context learning which first retrieves m exemplars from a large training set of n examples (n ≫ m) based on some similarity measure (e.g., BM-25), and then performs in-context learning with the retrieved exemplars [55,90,65,44].In contrast, we target the few-shot setting where we only assume access to 16-32 demonstration examples.</p>
<p>Our baselines here include: (1) standard prompting, (2) standard prompting with constrained decoding based on the full DSL grammar G [65,62], and (3) a derivation tree-based prompting baseline which imbues more structural information to the exemplars by feeding the linearized derivation tree instead of the surface form program. 8 We use Codex [12] as the base LLM for these main experiments.We assess methods according to program accuracy (matching the predicted and reference programs) as well as execution accuracy (same execution in both programs) if possible.Few-Shot results.The main results are shown in Table 1.We find that grammar prompting can meaningfully improve upon the standard prompting baseline even without constrained decoding.Interestingly, grammar prompting outperforms derivation tree prompting which actually provides more information than the minimal specialized grammar G[y] (since the derivation tree explicitly shows how the rules are actually applied to obtain the program).This potentially indicates that having the LLM "plan out" the program by forcing it to predict the specialized grammar G first is an effective strategy.We also analyze the effect of constrained decoding on the number of LLM API calls in Table 7 of appendix A.1, where we observe that constrained decoding requires roughly three times more API calls than unconstrained decoding.Despite the promising performance of grammar prompting, there is a large gap between using the predicted grammar vs. using the oracle grammar (i.e., setting G = G[y]), indicating opportunities for further work in this area.</p>
<p>Retrieval-based in-context learning.While our core target application is few-shot semantic parsing, we also apply grammar prompting for retrieval-based in-context learning to test whether it can still improve performance in the data-rich regime and also to compare against the prior work on these benchmarks.Results in Table 2 (left) show that grammar prompting can improve results even in this setting, although the improvements are less pronounced than in the few-shot setting.</p>
<p>Out-of-distribution generalization.We experiment to see whether grammar prompting can improve compositional generalization on GeoQuery.Specifically, we test grammar prompting on the compositional splits of GeoQuery split from Shaw et al. [64].These splits feature structural divergence between training and test examples, e.g., programs have different templates or length.Results in Table 2 (right) show that grammar prompting can improve upon standard prompting, across all splits (Template, TMCD, Length).</p>
<p>We next assess whether grammar prompting can enable LLMs to make zero-shot use of unseen functions (NewFunc) that are not even part of the retrieval set.We set aside 8 functions (smallest, shortest, most, highest, sum, population_1, count, major) and remove them from the retrieval set, simulating a scenario where new functions are supported in the backend yet no NL-program paired data is available for adapting a semantic parser.Note that for GeoQuery (and Overnight-Blk), we always prepend the full DSL grammar G-which includes the held-out functions-before the in-context exemplars.We observe that with standard prompting LLMs are still capable of guessing correct function names.However, grammar-prompted LLMs achieve significantly better performance than standard prompting, suggesting that the explicit prediction of specialized grammars elicits understanding and reasoning at the grammar level, thereby enabling generalization to unseen functions.Finally, we found that without constrained generation, LLMs were often able to guess functions that did not exist but were nonetheless sensible.It would thus be interesting to explore whether LLMs can tackle DSL-open benchmarks such as LARC [1].Different base LLMs.We finally experiment with grammar prompting across different base LLMs.Since GPT-3.5's4K token limit is smaller than Codex's (8K) and GPT-4's (8K) limits, we use fewer exemplars in these experiments than before (24/8/16 exemplars for GeoQuery/SMCalFlow/Overnight-B respectively).Due to API cost, we limit our experiments to a smaller subset of 100 test examples instead of the full test set.In general we observe that grammar prompting can consistently improve upon standard prompting, except on SMCalFlow with GPT-3.5 where we observed both methods to perform poorly.</p>
<p>Class-Specific Molecule Generation</p>
<p>We next demonstrate that our approach works effectively beyond language parsing problems.We consider the molecule generation task, which is apparently a very domain-specific problem in which even LLMs may not have enough intrinsic expertise.Existing methods for molecule generation typically focus on training specialized neural models using large training sets [43,36,14,2,75,59,18].We instead follow Guo et al. [29] and explore a few-shot setting where the task is to generate class-specific molecules given a small number of exemplars of that class.Formally, given a small set of molecules {y
(i) c } N
i=1 belonging to a particular molecule class c ∈ {Acrylates, Chain Extenders, Isocyanates}, our goal is to generate novel molecules y c of the same class that can be synthesized using existing molecules.Since the in-context examples in this case will only consist of molecules of the same class, the "input" x (i) c is the empty string in this case.The data contains 32 Acrylates, 11 Chain Extenders, and 11 Isocyanates (see appendix G of Guo et al. [29]).</p>
<p>While molecules can be more faithfully represented with 3D graph structure, the SMILES string representation [81] remains popular due to its ease of use. 9The specialized grammars G[y c ] (which are specialized from the SMILES grammar) encode various structural properties of the molecule that are specific to the molecule class.Figure 4 shows an example of a specialized grammar and the corresponding molecule in SMILES format.In this example, ring_closure ::= "1" specifies the number of rings, and branch ::= "(" smiles ")" specifies whether there is a branch.We test our approach by generating 100 molecules for each class and assessing the quality of the generated molecules.In addition to the standard prompting baseline, we also run the graph grammar 9 However SMILES does not guarantee that the generated string corresponds to a valid molecule.Using our approach on more advanced string representations such as SELFIES [42] (which guarantee validity) remains an interesting avenue for future work.baseline from Guo et al. [29] which learns hypergraph grammar [37] from the given molecules.We use four metrics: Validity (V), the percentage of chemically valid molecules; Diversity (D), average pairwise Tanimoto distance over Morgan fingerprints [58]; Retrosynthesis score (R), the percentage of molecules that are synthesizable from existing molecules, which is computed approximately via the Retro* model [11]; Membership (M), the percentage of molecules that belong to the desired monomer class.We use GPT-3.5 as the base LLM and sample from the LLM without constrained decoding, as constrained decoding was found to decrease the diversity of samples.See appendix A.2 for the full experimental setup.</p>
<p>Acrylates Chain Extenders Isocyanates
Model V D R M V D R M V D R M Graph Grammar [
Results.The results are presented in Table 4.We observe that grammar prompting significantly improves the synthesis of Acrylates and Chain Extenders across all metrics, while yielding mixed results for Isocyanates.Notably, both prompting-based methods outperform the graph grammar baseline in terms of Retro score, potentially due to the fact LLMs have been exposed to a certain number of existing molecules during pretraining, which proves advantageous for generating synthesizable molecules.In contrast, the baseline method cannot incorporate any external knowledge beyond the 11 or 32 molecules provided.While these results are preliminary, they potentially indicate that LLMs can serve as a useful tool for generating string representations of chemical structures (and potentially other biological/chemical structures), which remains underexplored.</p>
<p>Action Subset Selection for Efficient PDDL Planning</p>
<p>Our final experiments show how grammar prompting can improve the efficiency of classical AI planners.Classical planning is the problem of finding a sequence of actions (i.e., a plan) that goes from an initial state s 0 to a goal state s g .An action is represented by a ground operator (e.g., unstack(block1, block2) which consists of an operator unstack along with two object arguments).We additionally consider macro-operators which can potentially speed up planning [8]. 10 Planning tasks, along with actions, are represented in Planning Domain Definition Language (PDDL) [27].We explore how grammar prompted LLMs can help guide classical planning algorithms.</p>
<p>We design specialized grammars to provide guidance to the classical greedy best-first search (GBFS) algorithm [5] by selecting a set of relevant actions.Figure 5 illustrates an example of such specialized grammar, which captures all the necessary actions for the final plan y that solves the given task.The process of the guided planning consists of the following steps: (1) given a task, predict a specialized grammar G[y];</p>
<p>(2) use the specialized grammar G[y] to subsequently generate a plan within the restricted action space derived from G[y];</p>
<p>(3) initialize GBFS's priority queue with the LLM-generated plan, and (4) search for the final plan in the restricted action space.Our setup builds upon the idea of using an LLM-generated plan to initialize GBFS from Silver et al. [66], which has a simpler two-step process: (1) given a task, predict a plan via standard prompting, and (2) utilize this plan to guide GBFS.We use their method as our baseline.</p>
<p>Following Silver et al. [66], we create a similar few-shot setting for LLM planning, using 5 tasks as in-context examples and 10 tasks for evaluation from Pyperplan [5].We test our approach on 3 classic domains in PDDL planning, including Blocks, Depot and Satellite.For the action space, we use either a set of primitive actions (Prim) or an augmented set with macro actions (Macro).</p>
<p>In addition to standard prompting, we add two more baselines: (1) No LLM: planning with the entire set of actions; (2) Min Macro: where we construct a minimal set of macro actions for each domain by selecting actions from existing plans for the training tasks.The Min Macro baseline is a domain-specific method to reduce the action space.By comparing to Min Macro, we can verify the effectiveness of instance-specific v.s.domain-specific action selection.See appendix A.3 for more details.Table 5: Results on PDDL planning.Created/Expanded refer to the number of created/expanded nodes during planning (lower is better).Success refers to success rate (higher is better).Numbers are averaged over three runs using GPT-3.5.</p>
<p>Specialized Action Grammar for PDDL Planning</p>
<p>Results.We evaluate the efficiency of planning in terms of the number of search nodes created/expanded, as well as the success rate.Table 5 shows the promising performance of LLM-guided planning via grammar prompting.In Blocks, grammar prompting significantly improves efficiency while maintaining 100% success rate.In Depot, grammar prompting with macro actions increased the success rate by 20% over the best competing baseline.In Satellite, using primitive actions yields the best performance with 100% success rate and a reduction of 57% expanded nodes comparing to the No LLM baseline.While our experiments are not intended to complete with the state-of-the-art algorithms for fast planning [20-22, 31, 25, 78], they indicate the promise of LLMs for improving existing planning algorithms.</p>
<p>Discussion and Limitations</p>
<p>We discuss several limitations of our approach including some negative results.Grammar prompting did not yield any improvements for DSLs that were likely to have been frequently encountered during pretraining (e.g., regular expressions, SQL).Moreover, constrained generation based on specialized grammars led to increased API calls, and was not always beneficial for tasks beyond semantic parsing.For instance, in molecule generation we discovered that enforcing constraints can sometimes result in worse performance, suggesting that while intermediate grammar may aid reasoning, the predictions remain imperfect.Additionally, in PDDL planning we observed that the constraints applied to prune objects can sometimes negatively impact performance, suggesting that relevant object selection is still very challenging for LLMs.It may be interesting to explore whether finetuning of moderately-sized LLMs using specialized grammars can lead to better grammar-based models for DSL generation.</p>
<p>On the positive front, our work demonstrates that LLMs have the capacity to understand and generate metalanguages.Working in this "metalanguage space" can be combined with chain-of-thought-style [80] prompts by, for example, manually providing natural language comments to the rules of the specialized grammars.We found this to improve results slightly on semantic parsing (see Figure 6 of appendix A.1).Many scientific problems can be formally approached by representing hypotheses into DSL programs [68].Similarly, DSLs can enable easier encoding of human prior knowledge and scientific principles, providing a foundation for scientific discovery.Recent work shows that state-of-the-art LLMs can follow previously unseen formal systems [72].Techniques like grammar prompting can widen the scope of scientific problems for which LLMs could be effectively applied by more explicitly accounting for external knowledge and constraints.</p>
<p>Related Work</p>
<p>Chain-of-thought prompting.Grammar prompting extends the recent line of work on improving the reasoning capabilities through providing explicit reasoning steps as part of the prompt [49,24,80,76,13,89].Our approach is closely related to much concurrent work on employing symbolic variables as part of the prompt [30,48,32,92,50], though to our knowledge we are not aware of any works that use formal grammars as the intermediate reasoning step.</p>
<p>LLMs for program generation and semantic parsing.Generating programs from natural language specifications, a task often referred to as semantic parsing in the NLP community, is a sub-problem of program synthesis where specifications can come in various forms, including inputoutput examples; for surveys, see Kamath and Das [38] and Gulwani et al. [28].Recent works [7,83] have explored using LLMs for generating code in general-purpose programming languages (e.g., Python) given natural language descriptions.Our work further extends this line by examining whether LLM can generate DSL programs, which are intrinsically scarce.Moreover, DSLs evolve much more rapidly, with API functions being frequently added, changed, or deleted according to tool development or user requirements.Grammar prompting provides a straightforward strategy for LLMs to improve DSL program generation even when only a limited set of examples is available.</p>
<p>Recent studies have begun investigating the use of LLMs for DSL generation.For tools with simple DSLs, such as a search function on Google/Bing, a calculator or a ToDo listing, the current ChatGPT Plugin11 is hypothesized to operate as a zero-shot/few-shot semantic parser.Toolformer [61] goes a step further by training an LLM to learn the appropriate context and usage for these simple-DSL tools.More recent work has explored few-shot prompting along with API descriptions for a broader array of tools Qin et al. [54], including for multimodal reasoning [82,69,88].In contrast, our study focuses on domains that necessitate a more complex DSL and consequently require a greater reasoning capacity from the LLM.Other works focus on investigating how model scales [55] and retrievers [91,44] affect in-context learning.There has also been work on grammar-constrained decoding with LLMs for semantic parsing [62,65,53], which serves as baselines in our empirical study.</p>
<p>Neural grammars.Grammar prompting can also been seen as a "fully LLM" instantiation of the line of work on neural parameterizations of symbolic grammars [34,16,41,40,35,95,86,85,87].Indeed, our approach to semantic parsing essentially uses prompt-based learning to define a quasisynchronous grammar [67,74] whose rules dynamically depend on the source sentence.Concretely, in contrast to recent works which embed learnable neural components within synchronous grammars [39,23,73], grammar prompting relies on the implicit in-context learning capabilities of LLMs for the learning component.</p>
<p>Grammar-based molecule generation.Grammar-based methods have gained significant interest in the realm of molecule generation, offering advantages in interpretability, data-efficiency, and controllability.One line of research involves integrating generic SMILES grammars with neural networks to generate syntactically meaningful molecules [43,14].Another approach centers on data-driven induction of grammars for generation [29,37].Our work aligns with the former, viewing grammar prompting as a straightforward method of integrating grammar into an LLM without the need for additional training.</p>
<p>LLMs for planning.Recently, LLMs have increasingly been utilized for planning in embodied agents, given their potential for extensive world knowledge and strong reasoning abilities.When given goals expressed in natural language in household environments, earlier work [3,63,33,46] directly prompted LLMs to sequence executable actions.However, in PDDL domains, where the desired action sequences are much longer, recent work [66,71] has found LLMs to underperform classical planners in PDDL planning.Grammar prompting represents a promising strategy for augmenting existing planners with LLMs.Future work could further integrate LLMs with planning by exploiting more reasoning capacities from LLMs in addition to existing efforts such as translating between problems and PDDL models [47], corrective re-prompting [56].This might involve inducing action models [4], macro-actions [8], sub-tasks [19] or generalized planning [84].</p>
<p>Conclusion</p>
<p>We propose grammar prompting as a simple approach for improving the few-shot DSL generation with large language models.Experiments across a range of structured languages including DSLs for semantic parsing (SMCalFlow, GeoQuery, Overnight</p>
<p>A Experiment Details</p>
<p>A.1 Semantic Parsing Statistics and Splits.We show the statistics for the splits used for the experiments in Table 6.</p>
<p>For GeoQuery, we utilize the standard split from Zelle and Mooney [94] in the retrieval-based setting and the Template, TMCD, and Length splits from Shaw et al. [64].We randomly sample 32 examples from the training set of the standard split to create the few-shot split.[91,55] utilized these examples as their retrieval set.However, in our experiments, we found that incorporating in-domain examples did not enhance performance.Consequently, we use 16/128 cross-domain examples as our training set in the few-shot and retrieval settings, respectively.For all experiments on SMCalFlow, we used the preprocessed version from Qiu et al. [55], which employs a more concise LISPRESS format [52] than the original version [93].This format aligns with Ye et al. [91] for a fair comparison.</p>
<p>For Overnight-Blocks, we employ the standard split from Wang et al. [77] in the retrieval setting.We randomly sample 32 examples from the training set of the standard split to create the few-shot split.</p>
<p>Scoring Functions for Constrained Generation.For each candidate continuation w ∈ Σ[y prefix ] for correction, we first form a partial program via concatenation y prefix •w and then feed it into Codex to obtain the score for the candidate via
log P LLM (w | G, x, y prefix , (x (i) , G[y (i) ], y (i) ) N i=1
).In the case where w consists of multiple BPE tokens, e.g., FindManger( is tokenized into Find, Manager, and (, we average the token-level log-likelihood to obtain a candidate-level score.However, when the size of Σ[y prefix ] exceeds 16, invoking Codex for each candidate becomes too expensive.To address this issue, we employ SentenceBERT to select 16 most plausible candidates first via a dot product, (SentenceBERT( ŷt )) ⊤ (SentenceBERT(y prefix • w)),</p>
<p>where SentenceBERT yields the embeddings for the incorrect prediction ŷt and a candidate of corrected partial program y prefix • w.Cost Efficiency.We assess various decoding strategies for their cost efficiency, focusing on the number of API calls.The number of Codex calls resulting from the few-shot semantic parsing experiments is presented in Figure 7, alongside the corresponding accuracy metrics.The results indicate that standard prompting under constrained decoding leads to a significantly higher number of Codex calls.Similarly, grammar-prompting with constraints also results in an increased number of Codex calls.However, when employing both grammar and program constraints, the number of calls decreases meaningfully in comparison to standard prompting under constrained decoding.Future work might consider exploring strategies for more cost-efficient constrained decoding.</p>
<p>Grammar Prompting with Annotated Rules.We have additionally experimented with enhancing BNF rules by appending natural language comments.As illustrated in Figure 6, we pair each BNF rule with its corresponding natural language phrases extracted from the given query x.These manually annotated comments yield an explicit correspondence between natural language phrases and their corresponding BNF rules, thereby better facilitating interpretation and application of the grammars for generating programs.When employing the augmented grammar prompting, we noticed marginal improvements on SMCalFlow (+1.0%) and Overnight-Blocks (0.5%), with no observed enhancement on GeoQuery.While the gains might not appear significant, this predicted alignment could potentially contribute to improved interpretability and further constraints on generation.For instance, the phrase "someone's manager" should consistently trigger the function FindManager(.We leave the exploration of utilizing the augmented rules for future work.</p>
<p>A.2 Molecule Generation</p>
<p>Sampling Procedure Different from semantic parsing and PDDL planning, where the most probable program y needs to be found via arg max inference, molecule generation has empty specification x and requires sampling from a prompting-based distribution.The sampling procedure for grammar prompting consists of three stages: (1) we randomly sample a permutation of given molecules, denoted as π, (2) based on a prompt formed by the permutation, we sample a specialized grammar G via
G ∼ P LLM (G ′ | x, (G[y (π[i]) ], y (π[i]) ) N i=1 ),
iii) we finally obtain the molecule conditioned G, y ∼ P LLM (y | G, (G[y (π[i]) ], y (π[i]) ) N i=1 ).We list the hyperparameters used for the sampling procedure in for (2)</p>
<p>B Prompts</p>
<p>Figures 7, 8, and 9 demonstrate the prompts with grammars, based on actual examples in the SM-CalFlow, GeoQuery, and Overnight datasets respectively.Because the general grammar of SM-CalFlow is long (around 4k tokens), we do not include it within the prompt.For GeoQuery and Overnight, the general grammar is integrated as part of the instruction within the prompt.In the context of molecule generation, the general grammar for SMILES 13 is also included.For PDDL planning, considering the simplicity of the action DSL, the general grammar is not deemed necessary for inclusion.</p>
<p>LLM Prompt</p>
<p>You are an expert programmer, and you need to write a program for the given natural language query.First, you should write grammar rules by choosing from the following BNF rules.Then, you should write programs that conform to your predicted rules.</p>
<p>[</p>
<p>LLM Output</p>
<p>BNF grammar rules: query ::= "answer(" answer_type ")" answer_type ::= num num ::= "count(" city ")" city ::= "major(" city ")" | "city(" city ")" | "loc_2(" state ")" state ::= "stateid('" STATENAME "')" STATENAME ::= "arizona"</p>
<p>program based on the BNF grammar rules: answer(count(major(city(loc_2(stateid('arizona ′ ))))))</p>
<p>Figure 1 :
1
Figure 1: A simple BNF grammar for a calendar DSL.</p>
<p>Figure 3 :
3
Figure 3: Illustration of how an predicted program is corrected in our proposed Earley-based constrained decoding.The final partial program will be subsequently fed into the LLM for continuation.</p>
<p>SpecializedFigure 4 :
4
Figure 4: Example of a specialized grammar for generating a molecule from the Acrylates class.</p>
<p>Figure 5 :
5
Figure 5: Example of a specialized grammar for PDDL planning in the Blocks domain.Given an input x = (s0, sg), the specialized grammar G[y] only includes necessary actions for solving this task.</p>
<p>Figure 8 :
8
Figure 8: Prompt with real examples from the GeoQuery dataset.</p>
<p>Table 1 :
1
Results on few-shot semantic parsing with Codex with various decoding strategies.GeoQuery and Overnight-Blk use 32 in-context examples, and SMCalFlow uses 16 examples.We show both program (Prog.) and execution (Exec.)accuracy when possible.
GeoQuerySMCalFlow Overnight-Blk</p>
<p>Table 2 :
2
Results</p>
<p>[10]etrieval-based in-context learning (left) and compositional generalization (right) with Codex.GeoQuery and Overnight-Blk show execution accuracy while SMCalFlow shows program accuracy.The numbers with ♠ and ♦ are taken from Ye et al.[90]and Cao et al.[10], respectively.</p>
<p>Table 3 :
3
Results with different base LLMs on a subset of 100 examples sampled from the original test set.GeoQuery and Overnight-Blk show execution accuracy, while SMCalFlow shows program accuracy.
LLM VariantGeoQuery SMCalFlow Overnight-BlkStandard + Codex832763Grammar + Codex953566Standard + GPT-3.575949Grammar + GPT-3.586567Standard + GPT-4853256Grammar + GPT-4983662</p>
<p>Table 4 :
4
Results for few-shot molecule generation with GPT-3.5.The metrics are validity (V), diversity (D), retrosynthesis score (R) and membership (M).Higher is better for all metrics.
29]100 0.83 79.0 30.3100 0.86 72.7 98.3100 0.93 52.2 82.7Standard Prompting87.7 0.73 80.0 76.760.3 0.89 72.7 55.794.7 0.82 78.0 92.2Grammar Prompting98.0 0.74 91.0 93.396.3 0.90 86.7 94.097.7 0.79 78.0 96.3</p>
<p>Table 6 :
6
), PDDL planning (action DSL), and molecule generation (SMILES), show that grammar prompting can improve upon standard prompting baselines.The encouraging results in semantic parsing indicate its potential to assist LLMs with tool usage, and the promising results in other domains indicate that grammar prompting can enable application of LLMs in domains that intrinsically depend on DSLs.Statistics of the splits used for experiments on semantic parsing.
Few-ShotRetrieval-basedGeoQuery Out-of-Dist.GeoQuery SMCalflow Overnight-Blk GeoQuery SMCalflow Overnight-Blk Template TMCD Length NewFuncTrain3216325601281436441440440453Test280360399280360399439440440447ApproachGeoQuery SMCalFlow Overnight-BStandard Prompting (unconstrained decoding)81.5 (1.0)46.4 (1.0)54.7 (1.0)w. constrained decoding ( y ∈ L(G))81.8 (4.3)49.2 (5.6)54.7 (1.6)Linearized Derivation Tree Prompting77.5 (1.0)50.0 (1.0)56.4 (1.0)Grammar Prompting (unconstrained decoding)87.5 (1.0)50.8 (1.0)57.4 (1.0)w. grammar constraint ( G ⊆ G)88.6 (3.0)51.3 (3.0)60.4 (1.4)w. grammar and program constraint ( y ∈ L( G))88.9 (3.3)52.4 (3.3)60.9 (2.8)w. oracle grammar ( G = G[y])96.1 (1.3)80.0 (1.0)94.2 (1.0)w. oracle grammar + program constraint96.8 (2.1)83.6 (2.6)96.5 (1.0)</p>
<p>Table 7 :
7
Extended results which show the number of Codex calls per example on few-shot semantic parsing in brackets.Columns in grey show program accuracy, while white columns others indicate execution accuracy.</p>
<p>Table 8 :
8
Hyperparameters for sampling specialized grammars G (top) and the molecules y in grammar prompting for molecule generation.Standard prompting uses the same hyperparameters for y.The functionality of obtaining the log-likelihood for a candidate continuation given a prefix is applicable via Codex APIs 12 via setting logprobs=True and echo=True.Unfortunately, subsequent models (e.g., GPT-3.5 and GPT-4) disable such functionality.As a workaround, we simply use the scoring function based on SentenceBERT to directly select the best candidate in our PDDL planning experiments.
Molecule ClassTemperature Presence Penalty Frequency PenaltySampling specialized grammars GAcrylates0.60.10.1Chain Extenders0.60.10.1Isocyanates0.60.40.4Sampling molecules yAcrylates0.60.10.1Chain Extenders0.60.10.1Isocyanates0.30.40.4</p>
<p>in Table 8 (top) and for (3) in Table 8 (bottom).</p>
<p>BEGIN RULES] query ::= "answer(" answer_type ")" answer_type ::= city | state | num | place | river | country city
::= "city(" city ")"|"cityid('" CITYNAME "', '" STATEABBREV "')"|"cityid('" CITYNAME "', _)"|"capital(" city ")"|"major(" city ")"|"capital_1(" state ")"|"loc_2(" state ")"|"loc_2(" country ")"|"largest(" city ")"|"smallest(" city ")"|"intersection(" city "," city ")"|"exclude(" city "," city ")"|"largest_one(population_1(" city "))"|"largest_one(density_1(" city "))"|"smallest_one(population_1(" city "))"|"smallest_one(density_1(" city "))"|ALL_CITY. . .[END RULES]query: what states border hawaii ?BNF grammar rules:query::= "answer(" answer_type ")"answer_type ::= statestate::= "state(" state ")"|"next_to_2(" state ")"|"stateid('" STATENAME "')"STATENAME::= "hawaii"program based on the BNF grammar rules: answer(state(next_to_2(stateid('hawaii ′ )))). . .
query: how many major cities are in arizona ?</p>
<p>Preprint. Under review.
For example by using the logit_bias argument.
These costs might be mitigated in the future if LLM APIs allow for cheaper use of cached prompts.
In rare cases the set Σ[yprefix] was too large to feed to LLM APIs. In these cases we used Sentence-BERT[57] to compute the similarity between the encoding of yprefix • w and ŷ(t) and took the top 16 candidates as Σ[yprefix].
https://www.wolfram.com/language/
For example, the derivation tree of a subprogram (attendee_? FindManager(Jean)) is linearized to [constraint "(attendee_?" [attendee "FindManager(" [attendee "Jean" ")"] ")"], which uses square brackets to encode richer hierarchical information than just the surface form program.
For example, pickup-and-stack(A, B) is a combination of pickup(A) and stack(A, B).
https://platform.openai.com/docs/plugins/examples
https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference
https://metamolecular.com/cheminformatics/smiles/formal-grammar/
AcknowledgmentsWe thank Jacob Andreas, Gabriel Grand, Linlu Qiu, Tom Silver, and Hunter Lang for helpful discussion and feedback.This study was supported by funds from the Google-MIT research collaborations program and the GIST-MIT joint research program.You are an expert programmer, and you need to write a program for the given natural language query.First, you should write a grammar that contains all the necessary BNF rules.Then, you should write programs that conform to your predicted rules.x (1) : find the meeting on Wednesday with Bob and Carol G[y (1) ]: event ::= "QueryEvent(" constraint ")" find the meeting constraint ::= "(&amp;" constraint constraint ")"| "(start_?"date ")" on ... | "(attendee_?"attendee attendee ")" with ... date ::= "Wednesday" Wednesday attendee ::= "Bob" | "Carol" Bob and Carol y (1) : QueryEvent(&amp; (start_?Wednesday)(attendee_? Bob Carol)) . . .x: Add meeting with Jean's manager on Wednesday at 3PMLLM OutputG: event ::= "CreateEvent(" constraint ")" Add meeting constraint ::= "(&amp;" constraint constraint ")"| "(start_?"date time ")" on ... at ... | "(attendee_?"attendee ")" with ... date ::= "Wednesday" Wednesday time ::= "NumberPM(3)" 3PM attendee ::= "FindManager(" attendee ")" | "Jean" Jean's manager y: CreateEvent(&amp; (start_?Wednesday NumberPM(3))(attendee_?FindManager(Jean)))Figure6: Example of grammar prompting where BNF grammars are additionally annotated with natural language comments (shown in green).These manually curated comments provide a detailed mapping between natural language phrases and their corresponding BNF rules, thereby better facilitating interpretation and application of the grammars for generating programs.We manually craft and add these comments to the few-shot prompts (top).The model predicts this during inference (bottom).In comparison, the sampling procedure for standard prompting only consists of two stages: (1) we randomly sample a permutation of given molecules, denoted as π, (2) based on a prompt formed by the permutation, we directly sample a molecule viaThe hyperparameters used for Step (2) is the same as in grammar prompting and shown in Table8(bottom).While we observed that Earley-based constrained generation enhances grammar prompting in terms of improving validity, other metrics, such as the retrosynthesis score, decreased significantly.This discrepancy could be attributed to the fact that existing LLMs, due to their limited exposure to molecules represented in SMILES format, struggle with comprehending and applying the BNF grammar rules of SMILES.Overall, our current findings serve as preliminary evidence that grammar prompting can tap into the capacity of LLMs to understand and apply BNF rules.However such capacity still remains limited in text-focused LLMs.A.3 PDDL PlanningRestricted Action Space The specialized grammar defined for PDDL planning essentially delineates a constrained action space that includes necessary actions and their associated objects.Our empirical results found that limiting the classical GBFS planner to the objects selected by a specialized grammar proved too restrictive, yielding beneficial results only within the Blocks domain.Therefore, we decided to remove this limitation, thus expanding the action space of GBFS to contain the actions predicted from the grammar with an unrestricted range of objects.You are an expert programmer, and you need to write a program for the given natural language query.First, you should write a grammar that contains all the necessary BNF rules.Then, you should write programs that conform to your predicted rules.query: I need a meeting with Elli and her team on Wednesday afternoon .BNF grammar rules: call ::= "(Yield" event ")" event ::= "(CreatePreflightEventWrapper" event_constraint ")" event_constraint ::= "(&amp;" event_constraint event_constraint ")" | "(Event.start_?" datetime_constraint ")" | "(Event.attendees_?"attendee_constraint ")" datetime_constraint ::= "(DateTimeConstraint" time date ")" time ::= "(Afternoon)" date ::= day day ::= "(NextDOW" day ")" | DAY DAY ::= "(Wednesday)" attendee_constraint ::= "(&amp;" attendee_constraint attendee_constraint ")"| "(AttendeeListHasRecipient" person ")" | "(AttendeeListHasPeople" person ")" person ::= recipient | "(FindTeamOf" person ")" recipient ::= "(Execute (refer (extensionConstraint" recipient ")))" | "(RecipientWithNameLike (PersonName.apply"string "))" string ::= ""Elli""LLM OutputBNF grammar rules: call ::= "(Yield" event ")" event ::= "(CreatePreflightEventWrapper" event_constraint ")" event_constraint ::= "(&amp;" event_constraint event_constraint ")" | "(Event.start_?" datetime_constraint ")" | "(Event.attendees_?"attendee_constraint ")" datetime_constraint ::= "(DateTime.date_?" datetime_constraint ")"| "(?" OP datetime ")" OP ::= "=" You are an expert programmer, and you need to write a program for the given natural language query.First, you should write grammar rules by choosing from the following BNF rules.Then, you should write programs that conform to your predicted rules.[
Communicating natural programs to humans and machines. Sam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle Ecanow, Maxwell Nye, Michael Tessler, Josh Tenenbaum, Advances in Neural Information Processing Systems. 202235</p>
<p>Walid Ahmad, Elana Simon, arXiv:2209.01712Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. 2022arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Learning strips action models with classical planning. Diego Aineto, Sergio Jiménez, Eva Onaindia, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling201828</p>
<p>. Yusra Alkhazraji, Matthias Frorath, Markus Grützner, Malte Helmert, Thomas Liebetraut, Robert Mattmüller, Manuela Ortlieb, Jendrik Seipp, Tobias Springenberg, 10.5281/zenodo.3701399Jan Wülfing. Pyperplan (v1.3. 2020Philip Stahl</p>
<p>Task-oriented dialogue as dataflow synthesis. Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan Deloach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H Lin, Ilya Lintsbakh, Andy Mcgovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, Alexander Zotov, 10.1162/tacl_a_00333Transactions of the Association for Computational Linguistics. 82020</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Macro-ff: Improving ai planning with automatically learned macro-operators. Adi Botea, Markus Enzenberger, Martin Müller, Jonathan Schaeffer, Journal of Artificial Intelligence Research. 242005</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, Proceedings of NeurIPS. NeurIPS2020</p>
<p>Semantic parsing with dual learning. Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, Kai Yu, 10.18653/v1/P19-1007Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Retro<em>: learning retrosynthetic planning with neural guided a</em> search. Binghong Chen, Chengtao Li, Hanjun Dai, Le Song, International Conference on Machine Learning. PMLR2020</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Felipe Petroski Such. Jan Leike,. 2021Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Syntax-directed variational autoencoder for structured data. Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, Le Song, arXiv:1802.087862018arXiv preprint</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, arXiv:2207.10342Jascha Sohl-dickstein. 2022</p>
<p>Recurrent neural network grammars. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A Smith, Proceedings of NAACL. NAACL2016</p>
<p>An efficient context-free parsing algorithm. Jay Earley, Communications of the ACM. 1321970</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Heng Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Semantics for hierarchical task-network planning. Kutluhan Erol, James A Hendler, Dana S Nau, 1995MARYLAND UNIV COLLEGE PARK INST FOR SYSTEMS RESEARCHTechnical report</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. E Richard, Nils J Fikes, Nilsson, Artificial intelligence. 23-41971</p>
<p>Pddl2. 1: An extension to pddl for expressing temporal planning domains. Maria Fox, Derek Long, Journal of Artificial Intelligence Research. 202003</p>
<p>Modelling mixed discrete-continuous domains for planning. Maria Fox, Derek Long, Artificial Intelligence Research. 272006</p>
<p>Finding Dataset Shortcuts with Grammar Induction. Dan Friedman, Alexander Wettig, Danqi Chen, Proceedings of EMNLP. EMNLP2022</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435PAL: Program-aided Language Models. 2022</p>
<p>Pddlstream: Integrating symbolic planners and blackbox samplers. R Caelan, Tomas Garrett, Leslie P Lozano-Perez, Kaelbling, International Conference on Automated Planning and Scheduling (ICAPS). 2020</p>
<p>Flexible Grammar-Based Constrained Decoding for Language Models. Saibo Geng, Martin Josifosky, Maxime Peyrard, Robert West, arXiv:2305.139712023</p>
<p>PDDLthe planning domain definition language. Malik Ghallab, Adele Howe, Craig Knoblock, Drew Mcdermott, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins, Sri , Anthony Barrett, Dave Christianson, CVC TR98003/DCS TR11651998Yale Center for Computational Vision and ControlNew Haven, CTTechnical Report</p>
<p>Program synthesis. Foundations and Trends® in Programming Languages. Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, 20174</p>
<p>Dataefficient graph grammar learning for molecular generation. Minghao Guo, Veronika Thost, Beichen Li, Payel Das, Jie Chen, Wojciech Matusik, Proceedings of ICLR. ICLR2022</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>The metric-ff planning system: Translating"ignoring delete lists"to numeric state variables. Jörg Hoffmann, Journal of Artificial Intelligence Research. 202003</p>
<p>Chain-of-symbol prompting elicits planning in large langauge models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, Yue Zhang, arXiv:2305.102762023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Unsupervised neural dependency parsing. Yong Jiang, Wenjuan Han, Kewei Tu, 2016Association for Computational Linguistics</p>
<p>Unsupervised learning of PCFGs with normalizing flow. Lifeng Jin, Finale Doshi-Velez, Timothy Miller, Lane Schwartz, William Schuler, Proceedings of ACL. ACLFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Junction tree variational autoencoder for molecular graph generation. Wengong Jin, Regina Barzilay, Tommi Jaakkola, International conference on machine learning. PMLR2018</p>
<p>Molecular hypergraph grammar with its application to molecular optimization. Hiroshi Kajino, International Conference on Machine Learning. PMLR2019</p>
<p>A survey on semantic parsing. Aishwarya Kamath, Rajarshi Das, arXiv:1812.009782018arXiv preprint</p>
<p>Sequence-to-sequence learning with latent neural grammars. Yoon Kim, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. 2021</p>
<p>Compound probabilistic context-free grammars for grammar induction. Yoon Kim, Chris Dyer, Alexander Rush, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Unsupervised recurrent neural network grammars. Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, Gábor Melis, Proceedings of the NAACL. the NAACLMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 2019</p>
<p>Self-referencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Pascal Friederich, Alan Aspuru-Guzik, Machine Learning: Science and Technology. 1445024oct 2020</p>
<p>Grammar variational autoencoder. Matt J Kusner, Brooks Paige, José Miguel Hernández-Lobato, International conference on machine learning. PMLR2017</p>
<p>Diverse demonstrations improve in-context compositional generalization. Itay Levy, Ben Bogin, Jonathan Berant, arXiv:2212.068002022arXiv preprint</p>
<p>Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, arXiv:2303.16434Linjun Shou, Ming Gong, and Nan Duan. TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs. 2023</p>
<p>On grounded planning for embodied tasks with language models. Chengsong Bill Yuchen Lin, Qian Huang, Wenda Liu, Sam Gu, Xiang Sommerer, Ren, arXiv:2209.004652022arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Witold Henryk, Jacob Michalewski, David Austin, David Bieber, Aitor Martin Dohan, Maarten Lewkowycz, David Paul Bosma, Charles Luan, Augustus Sutton, Odena, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021</p>
<p>Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023</p>
<p>ART: Automatic multi-step reasoning and tool-use for large language models. Bhargavi Paranjape, Scott Lundberg Anbd Sameer, Hannaneh Singh, Luke Hajishirzi, Marco Zettlemoyer, Ribeiro Tulio, arXiv:2303.090142023</p>
<p>Value-agnostic conversational semantic parsing. Adam Emmanouil Antonios Platanios, Subhro Pauls, Yuchen Roy, Alexander Zhang, Alan Kyte, Sam Guo, Jayant Thomson, Jason Krishnamurthy, Jacob Wolfe, Dan Andreas, Klein, Proceedings of ACL. ACLAugust 2021</p>
<p>Synchromesh: Reliable Code Generation from Pre-trained Language Models. Gabriel Poesia, Alex Polozov, Ashish Vu Le, Gustavo Tiwari, Christopher Soares, Sumit Meek, Gulwani, Proceedings of ICLR. ICLR2022</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, arXiv:2304.083542023arXiv preprint</p>
<p>Evaluating the impact of model scale for compositional generalization in semantic parsing. Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, Kristina Toutanova, Proceedings of EMNLP. EMNLPDecember 2022</p>
<p>Planning with large language models via corrective re-prompting. Vanya Shreyas Sundara Raman, Eric Cohen, Ifrah Rosen, David Idrees, Stefanie Paulius, Tellex, arXiv:2211.099352022arXiv preprint</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Extended-connectivity fingerprints. David Rogers, Mathew Hahn, Journal of chemical information and modeling. 5052010</p>
<p>Self-supervised graph transformer on large-scale molecular data. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, Junzhou Huang, Advances in Neural Information Processing Systems. 202033</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Proceedings of ICLR. ICLR2022</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Toolformer, arXiv:2302.04761Language Models Can Teach Themselves to Use Tools. 2023</p>
<p>PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Skill induction and planning with latent language. Pratyusha Sharma, Antonio Torralba, Jacob Andreas, arXiv:2110.015172021arXiv preprint</p>
<p>Compositional generalization and natural language variation: Can a semantic parsing approach handle both?. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova, Proceedings of ACL. ACLAugust 2021</p>
<p>Constrained language models yield few-shot semantic parsers. Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme, Proceedings of EMNLP. EMNLPAssociation for Computational LinguisticsNovember 2021</p>
<p>Pddl planning with pretrained large language models. Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , NeurIPS 2022 Foundation Models for Decision Making Workshop. </p>
<p>Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies. David Smith, Jason Eisner, Proceedings on the Workshop on Statistical Machine Translation. on the Workshop on Statistical Machine Translation2006</p>
<p>Jennifer J Sun, Megan Tjandrasuwita, Atharva Sehgal, Armando Solar-Lezama, Swarat Chaudhuri, Yisong Yue, Omar Costilla-Reyes, arXiv:2210.05050Neurosymbolic programming for science. 2022arXiv preprint</p>
<p>ViperGPT: Visual Inference via Python Execution for Reasoning. Didac Suris, Sachit Menon, Carl Vondrick, arXiv:2303.081282023</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, arXiv:2210.092612022</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Experimental results from applying GPT-4 to an unpublished formal language. Scheidt Gregor Vom, arXiv:2305.121962023</p>
<p>Hierarchical Phrase-based Sequenceto-Sequence Learning. Bailin Wang, Ivan Titov, Jacob Andreas, Yoon Kim, Proceedings of EMNLP. EMNLP2022</p>
<p>What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. Mengqiu Wang, Noah A Smith, Teruko Mitamura, Proceedings of EMNLP. EMNLP2007</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang, Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. the 10th ACM international conference on bioinformatics, computational biology and health informatics2019</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, Proceedings of ICLR. ICLR2023</p>
<p>Building a semantic parser overnight. Yushi Wang, Jonathan Berant, Percy Liang, Proceedings of ACL. ACLBeijing, ChinaJuly 2015</p>
<p>Learning compositional models of robot skills for task and motion planning. Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, Tomás Lozano-Pérez, The International Journal of Robotics Research. 406-72021</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Mingbo Dai, V Quoc, Le, Proceedings of ICLR. ICLR2022</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.046712023arXiv preprint</p>
<p>A systematic evaluation of large language models of code. Uri Frank F Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Ryan Yang, Tom Silver, Aidan Curtis, Tomas Lozano-Perez, Leslie Pack, Kaelbling , arXiv:2204.10420Pg3: Policy-guided planning for generalized policy generation. 2022arXiv preprint</p>
<p>Neural bi-lexicalized PCFG induction. Songlin Yang, Yanpeng Zhao, Kewei Tu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>PCFGs can do better: Inducing probabilistic context-free grammars with many symbols. Songlin Yang, Yanpeng Zhao, Kewei Tu, Proceedings of NAACL. Association for Computational Linguistics. NAACL. Association for Computational Linguistics2021</p>
<p>Unsupervised discontinuous constituency parsing with mildly context-sensitive grammars. Songlin Yang, Roger P Levy, Yoon Kim, arXiv:2212.091402022arXiv preprint</p>
<p>Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Compositional exemplars for in-context learning. Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong, arXiv:2302.056982023arXiv preprint</p>
<p>Benchmarking multimodal regex synthesis with complex structures. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, Proceedings of ACL. ACLAssociation for Computational LinguisticsJuly 2020</p>
<p>Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, arXiv:2305.096562023arXiv preprint</p>
<p>Compositional generalization for neural semantic parsing via span-level supervised attention. Pengcheng Yin, Hao Fang, Graham Neubig, Adam Pauls, Antonios Emmanouil, Yu Platanios, Sam Su, Jacob Thomson, Andreas, Association for Computational Linguistics2021</p>
<p>Learning to parse database queries using inductive logic programming. M John, Raymond J Zelle, Mooney, Proceedings of the national conference on artificial intelligence. the national conference on artificial intelligence1996</p>
<p>The return of lexical dependencies: Neural lexicalized PCFGs. Yonatan Hao Zhu, Graham Bisk, Neubig, Transactions of the Association for Computational Linguistics. 82020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8781 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8781</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8781</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276781923</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.03149v1.pdf" target="_blank">DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The reliability of large language models remains a critical challenge, particularly due to their susceptibility to hallucinations and factual inaccuracies during text generation. Existing solutions either underutilize models' self-correction with preemptive strategies or use costly post-hoc verification. To further explore the potential of real-time self-verification and correction, we present Dynamic Self-Verify Decoding (DSVD), a novel decoding framework that enhances generation reliability through real-time hallucination detection and efficient error correction. DSVD integrates two key components: (1) parallel self-verification architecture for continuous quality assessment, (2) dynamic rollback mechanism for targeted error recovery. Extensive experiments across five benchmarks demonstrate DSVD's effectiveness, achieving significant improvement in truthfulness (Quesetion-Answering) and factual accuracy (FActScore). Results show the DSVD can be further incorporated with existing faithful decoding methods to achieve stronger performance. Our work establishes that real-time self-verification during generation offers a viable path toward more trustworthy language models without sacrificing practical deployability.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8781.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8781.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSVD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Self-Verify Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time self-verification and localized rollback framework that runs lightweight probing heads in parallel with the LM head to detect hallucinations in real time and performs targeted rollbacks + resampling with a probing-derived penalty to correct errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 chat variant, 7B-parameter family (instruction-tuned chat model).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Self-Verification Decoding (DSVD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Parallel token-level probing heads (two-layer MLP probing heads aggregated across layers) predict per-token hallucination probabilities from LLM hidden states; when any probing score in a sliding window exceeds threshold (0.5) rollback to t-r is triggered; generate k candidate continuations of length m and rerank with penalized log-probability f = sum log p(x_i|...) - α log(z_hallu_i) (defaults: r=10, k=5, m=20, α=0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short-form question answering benchmarks (TruthfulQA measures truthfulness and informativeness; StrQA, SciQ, EntityQuestions factual QA) and FACTSCORE for long-form factual precision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Truth: 56.3% ; Info: 85.9% ; T*I: 48.4% ; StrQA: 67.7 ; SciQ: 61.8 ; EntQ: 30.7 ; FActScore: 33.3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy baseline — Truth: 36.9% ; Info: 86.2% ; T*I: 31.9% ; StrQA: 63.6 ; SciQ: 59.8 ; EntQ: 29.3 ; FActScore: 32.6</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Architectural introspection: trained MLP probing heads over intermediate hidden states produce implicit (non-textual) self-feedback, used to trigger localized rollbacks and as penalty terms during candidate re-ranking; avoids generating textual critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: large gains over greedy baseline on T*I (48.4% vs 31.9% for Llama-2-7B-Chat) and improvements in FACTSCORE (33.3 vs 32.6). DSVD also outperforms the Self-Refine baseline on multiple metrics (e.g., Self-Refine T*I = 36.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on model's internal factual knowledge (struggles with up-to-date information); hallucination detector often recognizes errors with a delay (necessitating a sliding window); rollback/resampling can increase latency when many rollbacks occur; occasional incorrect corrections/cascade errors illustrated in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to textual self-feedback methods (Self-Refine) DSVD is more efficient (only ~5% extra latency when no rollbacks) and more effective on truthfulness metrics; DSVD can be combined with direct decoding methods (DoLa, ITI, TruthX) to further improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablating the revision penalty (α=0) reduces performance modestly (e.g., T*I 48.4→47.6 and FACTSCORE 33.3→33.1 for Llama-2-7B-Chat). Replacing probing heads with a ratio-based rollback condition (SED-inspired) causes large degradation (T*I 48.4→29.2 for Llama-2-7B-Chat).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8781.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSVD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Self-Verify Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Real-time internal-state self-verification with dynamic rollback and resampling using probing-derived penalties to improve factuality during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-IT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3 instruct-tuned model, 8B-parameter family.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Self-Verification Decoding (DSVD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>See above; same DSVD pipeline used across models. Probing heads aggregate across layers; sliding window rollback; beam search/k candidate generation with penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short-form QA and FACTSCORE for long-form factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Truth: 64.5% ; Info: 81.0% ; T*I: 52.3% ; StrQA: 77.7 ; SciQ: 66.4 ; EntQ: 37.1 ; FActScore: 37.7</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy baseline — Truth: 61.8% ; Info: 80.4% ; T*I: 49.7% ; StrQA: 77.2 ; SciQ: 65.1 ; EntQ: 36.6 ; FActScore: 35.9</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Internal probing heads + sliding-window rollback + penalized candidate reranking (α=0.1), executed in parallel with LM head to minimize latency.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements over greedy and small but consistent gains vs Self-Refine (DSVD T*I 52.3% vs Self-Refine 51.5%); FACTSCORE improved by +1.8 absolute points vs baseline across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same limitations: reliance on internal knowledge (limits with fresh/up-to-date facts), delayed detector signals, and increased overhead when rollbacks occur frequently.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>DSVD is complementary to direct decoding (DoLa/ITI/TruthX) — combining DSVD with those methods yields additive improvements (Table 3). DSVD also beats Self-Refine on several metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Penalty removal and probing replacement ablations cited: penalty removal slightly degrades results; replacing probing heads with ratio-based rollback yields severe performance drop across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8781.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSVD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Self-Verify Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel self-verification + rollback decoding framework that uses probing of internal states to detect and correct hallucinations during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B-IT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen 2.5 instruct-tuned model, ~7B parameter variant used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dynamic Self-Verification Decoding (DSVD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>As above: per-token probing heads over hidden states; sliding-window detection; rollback and candidate sampling with probing-based penalty term for re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short-form QA benchmarks and FACTSCORE long-form factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Truth: 85.8% ; Info: 33.7% ; T*I: 28.9% ; StrQA: 78.7 ; SciQ: 72.7 ; EntQ: 26.9 ; FActScore: 28.1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy baseline — Truth: 86.3% ; Info: 32.9% ; T*I: 28.4% ; StrQA: 77.6 ; SciQ: 72.0 ; EntQ: 26.1 ; FActScore: 25.6</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Parallel probing heads and dynamic rollback; penalized reranking used to prefer lower hallucination risk continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Small absolute gains in T*I relative to greedy (28.9% vs 28.4%) and larger gains on FACTSCORE (+2.5 absolute points vs baseline for this model family). DSVD maintains or improves factual accuracy across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same as other DSVD entries; improvements are smaller for high-performing base models on some metrics; the method's benefit is model- and dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>DSVD comparable or slightly better than Self-Refine and can be combined with DoLa/ITI for further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations (penalty removal, probing replacement) show probing heads are important; replacement yields larger performance drops in this family as well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8781.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-reflect approach where the LLM generates textual self-feedback (SelfEvaluate) and then revises its answer iteratively based on that feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 chat variant, 7B-parameter family (used here as a baseline for iterative refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (textual self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates textual feedback (SelfEvaluate) about its initial answer and then issues a revised answer conditioned on that feedback; implemented via prompts (prompt templates used per dataset). Number of iterations not specified in this paper (implementation used provided prompts to generate one or more self-feedback + revision cycles).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short-form QA and FACTSCORE long-form evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Refine — Truth: 39.4% ; Info: 93.6% ; T*I: 36.9% ; StrQA: 66.2 ; SciQ: 61.2 ; EntQ: 29.7 ; FActScore: 32.9</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy baseline — Truth: 36.9% ; Info: 86.2% ; T*I: 31.9% ; StrQA: 63.6 ; SciQ: 59.8 ; EntQ: 29.3 ; FActScore: 32.6</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based textual self-critique followed by conditioned regeneration (explicit text feedback loops).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Self-Refine improves T*I from 31.9% (greedy) to 36.9% on Llama-2-7B-Chat, indicating the text-based self-feedback loop can increase truthfulness and informativeness metrics in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes textual self-feedback methods incur significant overhead (additional text generation cycles), can be task-specific, and are computationally costly; also they are vulnerable to error accumulation where initial errors get propagated in self-feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Self-Refine improves over greedy but DSVD achieves larger gains on truthfulness while being more computationally efficient; DSVD avoids costly textual critique generation by using internal signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8781.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Textual generate-then-reflect iterative refinement baseline where LLM generates self-feedback and revises answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-IT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-3 instruct tuned 8B model used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (textual self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompted textual self-evaluation followed by answer revision; prompts per dataset provided in Appendix; iteration count unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short QA and long-form factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Refine — Truth: 62.7% ; Info: 82.1% ; T*I: 51.5% ; StrQA: 69.3? ; SciQ: 65.4 ; EntQ: 36.8 ; FActScore: 36.9</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy baseline — Truth: 61.8% ; Info: 80.4% ; T*I: 49.7% ; StrQA: 77.2 ; SciQ: 65.1 ; EntQ: 36.6 ; FActScore: 35.9</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered textual critique followed by regeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Small improvements in T*I over greedy (51.5% vs 49.7%) indicate effectiveness in some settings, though improvements and Info/StrQA interplay vary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same as above: added latency and compute cost; possible error propagation across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>DSVD outperforms or matches Self-Refine on truthfulness with lower overhead; DSVD leverages internal signals instead of textual feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8781.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate-then-reflect iterative refinement method used as baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B-IT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen 2.5 instruct-tuned ~7B model.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (textual self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Textual self-evaluation and revision via prompts; iteration count unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short-form QA and FACTSCORE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Refine — Truth: 87.1% ; Info: 32.7% ; T*I: 28.4% ; StrQA: 78.4 ; SciQ: 71.8 ; EntQ: 26.4 ; FActScore: 27.3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy baseline — Truth: 86.3% ; Info: 32.9% ; T*I: 28.4% ; StrQA: 77.6 ; SciQ: 72.0 ; EntQ: 26.1 ; FActScore: 25.6</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based generation of critique and revision.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Minimal or no T*I gain for this model on some metrics (Self-Refine T*I ≈ greedy for Qwen2.5), indicating limited benefit depending on base model characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Overhead of textual feedback and variable efficacy depending on base model; may not significantly help models already high on truth metric.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>DSVD achieved modest additional gains over Self-Refine for this model family and improved FACTSCORE more substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8781.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative refinement framework that treats self-improvement as verbal reinforcement learning (the model produces textual critique and uses reward signals to update behavior without weight changes during episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (verbal reinforcement learning style iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative textual self-feedback loops framed as reinforcement learning episodes (verbal RL) where past failures and feedback inform future generations; does not update model weights online.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Textual feedback/agent-style episodic refinement (external critic or internal reward-guided re-generation); described in related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as related work; no experimental results presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Related-work summary notes these backtracking/textual methods can have significant compute overhead and be vulnerable to error accumulation; specific failure modes not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper contrasts Reflexion-style textual iterative methods with DSVD, arguing DSVD avoids textual-feedback overhead by using internal signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8781.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Correct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating Sequences by Learning to Self-Correct (Self-Correct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that trains a separate corrector model to rewrite or fix model outputs, enabling self-correction rather than relying solely on the base model's textual critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating Sequences by Learning to Self-Correct</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correct (corrector model)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train a separate corrector model (or module) that takes model outputs and produces corrected sequences; differs from prompt-based self-critique because correction logic is learned in a trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate learned corrector model (supervised training) used to post-hoc revise outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned in related work; no direct experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that training a corrector reduces framework flexibility and may not be task-agnostic; additional training cost is required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with DSVD's lightweight, training-on-pseudolabels probing-heads approach which avoids expensive full-model correction training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8781.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8781.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SED-inspired ratio rollback (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ratio-based rollback condition inspired by SED / probability-ratio heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple rollback condition using the probability ratio between the top-2 and top-1 candidate tokens (p_top2 / p_top1) used as an ablation instead of probing heads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-Chat (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 chat 7B; ablation variant of DSVD where probing heads are replaced with a ratio-based rollback trigger (threshold = 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SED-inspired ratio-based rollback (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use the top-2/top-1 token probability ratio as the rollback condition (if ratio exceeds threshold), rather than learned probing heads; then perform same rollback + resampling pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various QA benchmarks (same as DSVD experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short QA and FACTSCORE long-form factuality used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Ablated ratio-based rollback — T*I dropped to 29.2% (from DSVD 48.4% on Llama-2-7B-Chat); FActScore reduced ≈ 2.1 points (e.g., 33.0 → 31.2 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>DSVD original (with probing heads) — T*I 48.4% ; FActScore ~33.0</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Heuristic rollback based on token-probability ratios rather than learned probing; still performs rollback + resample/rerank.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>This is an ablation demonstrating that learned probing heads are significantly more effective than simple ratio heuristics: replacing probing heads caused severe performance degradation across architectures (example: 48.4→29.2 T*I for Llama-2-7B-Chat).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The heuristic approach is less reliable and led to substantial drops in truthfulness and factuality; highlights the importance of learned internal-state probes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly (as ablation) to DSVD with probing heads; probing-head version is much better.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Significant performance degradation when using ratio-based rollback vs learned probing heads: T*I drop of 19.2 percentage points for Llama-2-7B-Chat and ~2.1-point FACTSCORE reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Generating Sequences by Learning to Self-Correct <em>(Rating: 2)</em></li>
                <li>SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation <em>(Rating: 2)</em></li>
                <li>Large Language Models are Better Reasoners with Self-Verification <em>(Rating: 1)</em></li>
                <li>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification <em>(Rating: 1)</em></li>
                <li>Real-time Verification and Refinement of Language Model Text Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8781",
    "paper_id": "paper-276781923",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "DSVD",
            "name_full": "Dynamic Self-Verify Decoding",
            "brief_description": "A decoding-time self-verification and localized rollback framework that runs lightweight probing heads in parallel with the LM head to detect hallucinations in real time and performs targeted rollbacks + resampling with a probing-derived penalty to correct errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-Chat",
            "model_description": "Llama-2 chat variant, 7B-parameter family (instruction-tuned chat model).",
            "reflection_method_name": "Dynamic Self-Verification Decoding (DSVD)",
            "reflection_method_description": "Parallel token-level probing heads (two-layer MLP probing heads aggregated across layers) predict per-token hallucination probabilities from LLM hidden states; when any probing score in a sliding window exceeds threshold (0.5) rollback to t-r is triggered; generate k candidate continuations of length m and rerank with penalized log-probability f = sum log p(x_i|...) - α log(z_hallu_i) (defaults: r=10, k=5, m=20, α=0.1).",
            "task_name": "TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE",
            "task_description": "Short-form question answering benchmarks (TruthfulQA measures truthfulness and informativeness; StrQA, SciQ, EntityQuestions factual QA) and FACTSCORE for long-form factual precision.",
            "performance_with_reflection": "Truth: 56.3% ; Info: 85.9% ; T*I: 48.4% ; StrQA: 67.7 ; SciQ: 61.8 ; EntQ: 30.7 ; FActScore: 33.3",
            "performance_without_reflection": "Greedy baseline — Truth: 36.9% ; Info: 86.2% ; T*I: 31.9% ; StrQA: 63.6 ; SciQ: 59.8 ; EntQ: 29.3 ; FActScore: 32.6",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Architectural introspection: trained MLP probing heads over intermediate hidden states produce implicit (non-textual) self-feedback, used to trigger localized rollbacks and as penalty terms during candidate re-ranking; avoids generating textual critiques.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: large gains over greedy baseline on T*I (48.4% vs 31.9% for Llama-2-7B-Chat) and improvements in FACTSCORE (33.3 vs 32.6). DSVD also outperforms the Self-Refine baseline on multiple metrics (e.g., Self-Refine T*I = 36.9%).",
            "limitations_or_failure_cases": "Depends on model's internal factual knowledge (struggles with up-to-date information); hallucination detector often recognizes errors with a delay (necessitating a sliding window); rollback/resampling can increase latency when many rollbacks occur; occasional incorrect corrections/cascade errors illustrated in case studies.",
            "comparison_to_other_methods": "Compared to textual self-feedback methods (Self-Refine) DSVD is more efficient (only ~5% extra latency when no rollbacks) and more effective on truthfulness metrics; DSVD can be combined with direct decoding methods (DoLa, ITI, TruthX) to further improve results.",
            "ablation_study_results": "Ablating the revision penalty (α=0) reduces performance modestly (e.g., T*I 48.4→47.6 and FACTSCORE 33.3→33.1 for Llama-2-7B-Chat). Replacing probing heads with a ratio-based rollback condition (SED-inspired) causes large degradation (T*I 48.4→29.2 for Llama-2-7B-Chat).",
            "uuid": "e8781.0",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DSVD",
            "name_full": "Dynamic Self-Verify Decoding",
            "brief_description": "Real-time internal-state self-verification with dynamic rollback and resampling using probing-derived penalties to improve factuality during generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-IT",
            "model_description": "Llama-3 instruct-tuned model, 8B-parameter family.",
            "reflection_method_name": "Dynamic Self-Verification Decoding (DSVD)",
            "reflection_method_description": "See above; same DSVD pipeline used across models. Probing heads aggregate across layers; sliding window rollback; beam search/k candidate generation with penalty.",
            "task_name": "TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE",
            "task_description": "Short-form QA and FACTSCORE for long-form factuality.",
            "performance_with_reflection": "Truth: 64.5% ; Info: 81.0% ; T*I: 52.3% ; StrQA: 77.7 ; SciQ: 66.4 ; EntQ: 37.1 ; FActScore: 37.7",
            "performance_without_reflection": "Greedy baseline — Truth: 61.8% ; Info: 80.4% ; T*I: 49.7% ; StrQA: 77.2 ; SciQ: 65.1 ; EntQ: 36.6 ; FActScore: 35.9",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Internal probing heads + sliding-window rollback + penalized candidate reranking (α=0.1), executed in parallel with LM head to minimize latency.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative improvements over greedy and small but consistent gains vs Self-Refine (DSVD T*I 52.3% vs Self-Refine 51.5%); FACTSCORE improved by +1.8 absolute points vs baseline across architectures.",
            "limitations_or_failure_cases": "Same limitations: reliance on internal knowledge (limits with fresh/up-to-date facts), delayed detector signals, and increased overhead when rollbacks occur frequently.",
            "comparison_to_other_methods": "DSVD is complementary to direct decoding (DoLa/ITI/TruthX) — combining DSVD with those methods yields additive improvements (Table 3). DSVD also beats Self-Refine on several metrics.",
            "ablation_study_results": "Penalty removal and probing replacement ablations cited: penalty removal slightly degrades results; replacing probing heads with ratio-based rollback yields severe performance drop across models.",
            "uuid": "e8781.1",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DSVD",
            "name_full": "Dynamic Self-Verify Decoding",
            "brief_description": "A parallel self-verification + rollback decoding framework that uses probing of internal states to detect and correct hallucinations during generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B-IT",
            "model_description": "Qwen 2.5 instruct-tuned model, ~7B parameter variant used for evaluation.",
            "reflection_method_name": "Dynamic Self-Verification Decoding (DSVD)",
            "reflection_method_description": "As above: per-token probing heads over hidden states; sliding-window detection; rollback and candidate sampling with probing-based penalty term for re-ranking.",
            "task_name": "TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE",
            "task_description": "Short-form QA benchmarks and FACTSCORE long-form factuality.",
            "performance_with_reflection": "Truth: 85.8% ; Info: 33.7% ; T*I: 28.9% ; StrQA: 78.7 ; SciQ: 72.7 ; EntQ: 26.9 ; FActScore: 28.1",
            "performance_without_reflection": "Greedy baseline — Truth: 86.3% ; Info: 32.9% ; T*I: 28.4% ; StrQA: 77.6 ; SciQ: 72.0 ; EntQ: 26.1 ; FActScore: 25.6",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Parallel probing heads and dynamic rollback; penalized reranking used to prefer lower hallucination risk continuations.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Small absolute gains in T*I relative to greedy (28.9% vs 28.4%) and larger gains on FACTSCORE (+2.5 absolute points vs baseline for this model family). DSVD maintains or improves factual accuracy across model families.",
            "limitations_or_failure_cases": "Same as other DSVD entries; improvements are smaller for high-performing base models on some metrics; the method's benefit is model- and dataset-dependent.",
            "comparison_to_other_methods": "DSVD comparable or slightly better than Self-Refine and can be combined with DoLa/ITI for further gains.",
            "ablation_study_results": "Ablations (penalty removal, probing replacement) show probing heads are important; replacement yields larger performance drops in this family as well.",
            "uuid": "e8781.2",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "A generate-then-reflect approach where the LLM generates textual self-feedback (SelfEvaluate) and then revises its answer iteratively based on that feedback.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-Chat",
            "model_description": "Llama-2 chat variant, 7B-parameter family (used here as a baseline for iterative refinement).",
            "reflection_method_name": "Self-Refine (textual self-feedback)",
            "reflection_method_description": "The model generates textual feedback (SelfEvaluate) about its initial answer and then issues a revised answer conditioned on that feedback; implemented via prompts (prompt templates used per dataset). Number of iterations not specified in this paper (implementation used provided prompts to generate one or more self-feedback + revision cycles).",
            "task_name": "TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE",
            "task_description": "Short-form QA and FACTSCORE long-form evaluation.",
            "performance_with_reflection": "Self-Refine — Truth: 39.4% ; Info: 93.6% ; T*I: 36.9% ; StrQA: 66.2 ; SciQ: 61.2 ; EntQ: 29.7 ; FActScore: 32.9",
            "performance_without_reflection": "Greedy baseline — Truth: 36.9% ; Info: 86.2% ; T*I: 31.9% ; StrQA: 63.6 ; SciQ: 59.8 ; EntQ: 29.3 ; FActScore: 32.6",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based textual self-critique followed by conditioned regeneration (explicit text feedback loops).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Self-Refine improves T*I from 31.9% (greedy) to 36.9% on Llama-2-7B-Chat, indicating the text-based self-feedback loop can increase truthfulness and informativeness metrics in this setting.",
            "limitations_or_failure_cases": "Paper notes textual self-feedback methods incur significant overhead (additional text generation cycles), can be task-specific, and are computationally costly; also they are vulnerable to error accumulation where initial errors get propagated in self-feedback loops.",
            "comparison_to_other_methods": "Self-Refine improves over greedy but DSVD achieves larger gains on truthfulness while being more computationally efficient; DSVD avoids costly textual critique generation by using internal signals.",
            "ablation_study_results": null,
            "uuid": "e8781.3",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "Textual generate-then-reflect iterative refinement baseline where LLM generates self-feedback and revises answers.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-IT",
            "model_description": "Llama-3 instruct tuned 8B model used as baseline.",
            "reflection_method_name": "Self-Refine (textual self-feedback)",
            "reflection_method_description": "Prompted textual self-evaluation followed by answer revision; prompts per dataset provided in Appendix; iteration count unspecified.",
            "task_name": "TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE",
            "task_description": "Short QA and long-form factuality.",
            "performance_with_reflection": "Self-Refine — Truth: 62.7% ; Info: 82.1% ; T*I: 51.5% ; StrQA: 69.3? ; SciQ: 65.4 ; EntQ: 36.8 ; FActScore: 36.9",
            "performance_without_reflection": "Greedy baseline — Truth: 61.8% ; Info: 80.4% ; T*I: 49.7% ; StrQA: 77.2 ; SciQ: 65.1 ; EntQ: 36.6 ; FActScore: 35.9",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered textual critique followed by regeneration.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Small improvements in T*I over greedy (51.5% vs 49.7%) indicate effectiveness in some settings, though improvements and Info/StrQA interplay vary.",
            "limitations_or_failure_cases": "Same as above: added latency and compute cost; possible error propagation across iterations.",
            "comparison_to_other_methods": "DSVD outperforms or matches Self-Refine on truthfulness with lower overhead; DSVD leverages internal signals instead of textual feedback.",
            "ablation_study_results": null,
            "uuid": "e8781.4",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "Generate-then-reflect iterative refinement method used as baseline in experiments.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B-IT",
            "model_description": "Qwen 2.5 instruct-tuned ~7B model.",
            "reflection_method_name": "Self-Refine (textual self-feedback)",
            "reflection_method_description": "Textual self-evaluation and revision via prompts; iteration count unspecified.",
            "task_name": "TruthfulQA; StrQA; SciQ; EntityQuestions; FACTSCORE",
            "task_description": "Short-form QA and FACTSCORE.",
            "performance_with_reflection": "Self-Refine — Truth: 87.1% ; Info: 32.7% ; T*I: 28.4% ; StrQA: 78.4 ; SciQ: 71.8 ; EntQ: 26.4 ; FActScore: 27.3",
            "performance_without_reflection": "Greedy baseline — Truth: 86.3% ; Info: 32.9% ; T*I: 28.4% ; StrQA: 77.6 ; SciQ: 72.0 ; EntQ: 26.1 ; FActScore: 25.6",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based generation of critique and revision.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Minimal or no T*I gain for this model on some metrics (Self-Refine T*I ≈ greedy for Qwen2.5), indicating limited benefit depending on base model characteristics.",
            "limitations_or_failure_cases": "Overhead of textual feedback and variable efficacy depending on base model; may not significantly help models already high on truth metric.",
            "comparison_to_other_methods": "DSVD achieved modest additional gains over Self-Refine for this model family and improved FACTSCORE more substantially.",
            "ablation_study_results": null,
            "uuid": "e8781.5",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "brief_description": "An iterative refinement framework that treats self-improvement as verbal reinforcement learning (the model produces textual critique and uses reward signals to update behavior without weight changes during episodes).",
            "citation_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Reflexion (verbal reinforcement learning style iterative refinement)",
            "reflection_method_description": "Iterative textual self-feedback loops framed as reinforcement learning episodes (verbal RL) where past failures and feedback inform future generations; does not update model weights online.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Textual feedback/agent-style episodic refinement (external critic or internal reward-guided re-generation); described in related work only.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as related work; no experimental results presented in this paper.",
            "limitations_or_failure_cases": "Related-work summary notes these backtracking/textual methods can have significant compute overhead and be vulnerable to error accumulation; specific failure modes not reported here.",
            "comparison_to_other_methods": "Paper contrasts Reflexion-style textual iterative methods with DSVD, arguing DSVD avoids textual-feedback overhead by using internal signals.",
            "ablation_study_results": null,
            "uuid": "e8781.6",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Correct",
            "name_full": "Generating Sequences by Learning to Self-Correct (Self-Correct)",
            "brief_description": "A method that trains a separate corrector model to rewrite or fix model outputs, enabling self-correction rather than relying solely on the base model's textual critique.",
            "citation_title": "Generating Sequences by Learning to Self-Correct",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-Correct (corrector model)",
            "reflection_method_description": "Train a separate corrector model (or module) that takes model outputs and produces corrected sequences; differs from prompt-based self-critique because correction logic is learned in a trained model.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Separate learned corrector model (supervised training) used to post-hoc revise outputs.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned in related work; no direct experimental comparison in this paper.",
            "limitations_or_failure_cases": "Paper notes that training a corrector reduces framework flexibility and may not be task-agnostic; additional training cost is required.",
            "comparison_to_other_methods": "Contrasted with DSVD's lightweight, training-on-pseudolabels probing-heads approach which avoids expensive full-model correction training.",
            "ablation_study_results": null,
            "uuid": "e8781.7",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SED-inspired ratio rollback (ablation)",
            "name_full": "Ratio-based rollback condition inspired by SED / probability-ratio heuristic",
            "brief_description": "A simple rollback condition using the probability ratio between the top-2 and top-1 candidate tokens (p_top2 / p_top1) used as an ablation instead of probing heads.",
            "citation_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-Chat (ablation)",
            "model_description": "Llama-2 chat 7B; ablation variant of DSVD where probing heads are replaced with a ratio-based rollback trigger (threshold = 0.7).",
            "reflection_method_name": "SED-inspired ratio-based rollback (ablation)",
            "reflection_method_description": "Use the top-2/top-1 token probability ratio as the rollback condition (if ratio exceeds threshold), rather than learned probing heads; then perform same rollback + resampling pipeline.",
            "task_name": "Various QA benchmarks (same as DSVD experiments)",
            "task_description": "Short QA and FACTSCORE long-form factuality used for evaluation.",
            "performance_with_reflection": "Ablated ratio-based rollback — T*I dropped to 29.2% (from DSVD 48.4% on Llama-2-7B-Chat); FActScore reduced ≈ 2.1 points (e.g., 33.0 → 31.2 reported).",
            "performance_without_reflection": "DSVD original (with probing heads) — T*I 48.4% ; FActScore ~33.0",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Heuristic rollback based on token-probability ratios rather than learned probing; still performs rollback + resample/rerank.",
            "number_of_iterations": null,
            "evidence_for_improvement": "This is an ablation demonstrating that learned probing heads are significantly more effective than simple ratio heuristics: replacing probing heads caused severe performance degradation across architectures (example: 48.4→29.2 T*I for Llama-2-7B-Chat).",
            "limitations_or_failure_cases": "The heuristic approach is less reliable and led to substantial drops in truthfulness and factuality; highlights the importance of learned internal-state probes.",
            "comparison_to_other_methods": "Compared directly (as ablation) to DSVD with probing heads; probing-head version is much better.",
            "ablation_study_results": "Significant performance degradation when using ratio-based rollback vs learned probing heads: T*I drop of 19.2 percentage points for Llama-2-7B-Chat and ~2.1-point FACTSCORE reduction.",
            "uuid": "e8781.8",
            "source_info": {
                "paper_title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Generating Sequences by Learning to Self-Correct",
            "rating": 2,
            "sanitized_title": "generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
            "rating": 2,
            "sanitized_title": "sed_selfevaluation_decoding_enhances_large_language_models_for_better_generation"
        },
        {
            "paper_title": "Large Language Models are Better Reasoners with Self-Verification",
            "rating": 1,
            "sanitized_title": "large_language_models_are_better_reasoners_with_selfverification"
        },
        {
            "paper_title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
            "rating": 1,
            "sanitized_title": "ever_mitigating_hallucination_in_large_language_models_through_realtime_verification_and_rectification"
        },
        {
            "paper_title": "Real-time Verification and Refinement of Language Model Text Generation",
            "rating": 1,
            "sanitized_title": "realtime_verification_and_refinement_of_language_model_text_generation"
        }
    ],
    "cost": 0.021425999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models
5 Mar 2025</p>
<p>Yiqiu Guo 
Fudan University</p>
<p>Yuchen Yang 
Shanghai AI Laboratory</p>
<p>University of Science and Technology of China</p>
<p>Zhe Chen 
Shanghai JiaoTong University</p>
<p>Pingjie Wang 
Shanghai AI Laboratory</p>
<p>Shanghai JiaoTong University</p>
<p>Yusheng Liao 
Shanghai AI Laboratory</p>
<p>Shanghai JiaoTong University</p>
<p>Ya Zhang 
Shanghai AI Laboratory</p>
<p>Shanghai JiaoTong University</p>
<p>Yanfeng Wang 
Yu Wang 
Shanghai AI Laboratory</p>
<p>Shanghai JiaoTong University</p>
<p>Molly Lin 
Shanghai AI Laboratory</p>
<p>Shanghai JiaoTong University</p>
<p>Stephanie Lin 
Shanghai AI Laboratory</p>
<p>Mateusz Litwin 
Theresa Lopez 
Ryan Lowe 
Patricia Lue 
Anna Makanju 
Kim Malfacini 
Sam Manning 
Todor Markov 
Yaniv Markovski 
Bianca Martin 
Katie Mayer 
Andrew Mayne 
Bob Mcgrew 
Scott Mayer Mckinney 
Christine Mcleavey 
Paul Mcmillan 
Jake Mcneil 
David Medina 
Aalok Mehta 
Jacob Menick 
Luke Metz 
Andrey Mishchenko 
Pamela Mishkin 
Vinnie Monaco 
Evan Morikawa 
Daniel Mossing 
Tong Mu 
Mira Murati 
Oleg Murk 
David Mély 
Ashvin Nair 
Reiichiro Nakano 
Rajeev Nayak 
Arvind Neelakan- Tan 
Shanghai AI Laboratory</p>
<p>Richard Ngo 
Hyeonwoo Noh 
Long Ouyang 
Cullen O'keefe 
Jakub Pachocki 
Alex Paino 
Joe Palermo 
Ashley Pantuliano 
Giambattista Paras- Candolo 
Joel Parish 
Emy Parparita 
Alex Passos 
Mikhail Pavlov 
Andrew Peng 
Adam Perelman 
Fil- Ipe De Avila 
Belbute Peres 
Michael Petrov 
Henrique Ponde 
MichaelOliveira Pinto 
PokornyMichelle Pokrass 
Vitchyr H Pong 
Tolly Powell 
Alethea Power 
Boris Power 
Elizabeth Proehl 
Raul Puri 
Alec Radford 
Jack Rae 
Aditya Ramesh 
Cameron Raymond 
Francis Real 
Kendra Rimbach 
Carl Ross 
Bob Rotsted 
Henri Roussez 
Nick Ryder 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Girish Sastry 
Heather Schmidt 
David Schnurr 
John Schul- Man 
Daniel Selsam 
Kyla Sheppard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Pranav Shyam 
Szymon Sidor 
Eric Sigler 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky </p>
<p>Yang Song
Natalie Staudacher
Felipe Petroski Such</p>
<p>Natalie Summers
Ilya Sutskever
Jie Tang</p>
<p>Nikolas Tezak
Phil Tillet, Jerry TworekMadeleine B. Thompson, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley</p>
<p>Juan Felipe Cerón Uribe
An-drea Vallone, Arun Vijayvergiya, Chelsea Voss, Car-roll Wainwright, Justin Jay WangAlvin Wang, Ben</p>
<p>DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models
5 Mar 2025D6A6ACD7F68331C4ECF3A322CA3232B4arXiv:2503.03149v1[cs.CL]
The reliability of large language models remains a critical challenge, particularly due to their susceptibility to hallucinations and factual inaccuracies during text generation.Existing solutions either underutilize models' selfcorrection with preemptive strategies or use costly post-hoc verification.To further explore the potential of real-time self-verification and correction, we present Dynamic Self-Verify Decoding (DSVD), a novel decoding framework that enhances generation reliability through real-time hallucination detection and efficient error correction.DSVD integrates two key components: (1) parallel self-verification architecture for continuous quality assessment, (2) dynamic rollback mechanism for targeted error recovery.Extensive experiments across five benchmarks demonstrate DSVD's effectiveness, achieving significant improvement in truthfulness (Quesetion-Answering) and factual accuracy (FActScore).Results show the DSVD can be further incorporated with existing faithful decoding methods to achieve stronger performance.Our work establishes that real-time self-verification during generation offers a viable path toward more trustworthy language models without sacrificing practical deployability.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) tasks, including question-answering, summarization, and conversation generation (OpenAI et al., 2024;DeepSeek-AI et al., 2025;Touvron et al., 2023).Despite their impressive performance, these models frequently suffer from reliability issues manifested through hallucinations and factual inaccuracies (Kadavath  tingly trust erroneous information presented in the models' confident and coherent outputs.</p>
<p>Recent advancements in faithful generation have shifted focus towards inference-stage interventions (Liang et al., 2024;Luo et al., 2024;Chen et al., 2024a).Researchers recognize that even models containing factual knowledge during pretraining often fail to access this information during generation reliably.Decoding-time adjustment strategies present a promising direction, offering more cost-effective solutions compared to supervised fine-tuning (SFT), which requires substantial computation, or retrieval-augmented generation (RAG), which necessitates an external knowledge base.As illustrated in Figure 1 .</p>
<p>into two paradigms: Direct Decoding methods (e.g., ITI (Li et al., 2023a), DoLa (Chuang et al., 2024), TruthX (Zhang et al., 2024b)) steer model outputs toward truthful directions by manipulating internal representations, leveraging the model's inherent truthful priors.While effective, these approaches fail to leverage the model's ability for selfcorrection of errors and reflective reasoning, leaving the model powerless against error accumulation.Backtracking Decoding methods (e.g., Self-Refine (Madaan et al., 2023a), Reflexion (Shinn et al.)) employ post hoc verification of generated content, but existing implementations suffer from significant computational overhead and vulnerability to error accumulation, where initial errors propagate into subsequent generations through selfreinforcing mechanisms.</p>
<p>To address these limitations, we propose Dynamic Self-Verify Decoding (DSVD), a novel decoding strategy that incorporates real-time selfverification with dynamic rollback mechanisms.Our approach builds on key insights: (1) Delayed Awareness of Hallucinations: models demonstrate superior ability in detecting existing errors compared to preemptively preventing them, as shown in 1, and (2) Local Error Correction is more Efficient: localized rollback enables error correction at their source, offering higher efficiency than global rewriting.The method mirrors human behavioral patterns: speculating, verifying, and refining the consequences before reaching a conclusion.More specifically, the framework operates through two components: 1) Fine-grained hallucination detector trained on model-generated pseudolabels; and 2) Parallel self-verification and dynamic rollback mechanism enabling real-time hallucination detection and error correction.</p>
<p>Our experimental evaluation across multiple LLM architectures (LlaMA-2, LlaMA-3, Qwen-2.5) and benchmarks (TruthfulQA, StrQA, SciQ, EntityQuestions, FActScore) demonstrates consistent improvements in truthfulness and factual accuracy while maintaining computational efficiency.Notably, DSVD shows complementary benefits when combined with existing direct generation methods, suggesting orthogonal mechanisms of action.Our key contributions include:</p>
<p>• We propose a straightforward yet intuitive semi-supervised hallucination labeling approach for fine-grained self-feedback.</p>
<p>• We propose Dynamic Self-Verify Decoding: a novel decoding strategy that enables parallel self-verification and dynamic self-correction.</p>
<p>• Comprehensive experiments across diverse LLMs and evaluation metrics reveal consistent performance improvements of DSVD.</p>
<p>2 Related Work</p>
<p>Faithful Decoding</p>
<p>In recent years, a series of studies have focused on leveraging truthful distribution to intervene in the model's next-token prediction.Some research has explored directing the model's generation towards a "more truthful" direction through representation editing.ITI (Li et al., 2023a) trains probing heads to identify a set of more truthful attention heads and enhances the weights of these heads during inference.TrFr (Chen et al., 2024b) proposed the application of multi-dimensional orthogonal probes, which effectively extract features from both truthful and non-truthful texts to better identify effective attention heads.TruthX (Zhang et al., 2024b) not only targets attention heads but also latent states in the forward feedback layer.By separately mapping these states using truthful and semantic encoders.</p>
<p>Another line of research investigates contrastive decoding for faithful generation.The pioneering work by (Li et al., 2023b) introduced Contrastive Decoding, which selects optimal tokens by contrasting probability distributions from expert and amateur models.Building on this foundation, DoLa (Chuang et al., 2024) enhanced the framework by incorporating intermediate layer representations, thereby improving early-stage reasoning consistency and pre-answer alignment through its Decoding-by-Contrasting-Layers mechanism.SLED proposed by (Zhang et al., 2024a) integrates latent knowledge into logits via single-step gradient-like operation instead of replacing original outputs in DoLa during inference.</p>
<p>Our Innovation: The direct decoding methods mostly intervene before the model predicts the next token, thus the model's self-awareness and selffeedback capabilities regarding hallucinations are unexploited, while DSVD intervene after the model encounter hallucination and thus fully utilize the self-reflection ability of large language models.</p>
<p>Self Feedback</p>
<p>Studies on self-feedback utilize the Large Language Model itself as a critic, enabling the model to generate feedback on its responses and further refine those responses based on the generated feedback.Self-Refine (Madaan et al., 2023a) simply uses the LLM in SelfEvaluate(•) to generate textual feedback.Reflexion (Shinn et al.) makes progress by regarding iterative refinement as Verbal Reinforcement Learning without weight updates.Self-Correct (Welleck et al., 2022) uses the same framework but trains a Corrector model for better feedback.Yet, due to not being task-agnostic and the need for training, it reduces the framework's flexibility.</p>
<p>Our Innovation: Traditional self-feedback approaches incur significant overhead by operating through textual critique generation.DSVD circumvents these limitations through two innovations:</p>
<p>(1) direct utilization of internal consistency signals as implicit feedback, avoiding costly text generation cycles; (2) localized correction via hidden state rollback instead of full-sequence regeneration reducing computation cost compared to prior methods.More comparisons and discussions with other related work can be found in Appendix A</p>
<p>Dynamic Self-Verify Decoding</p>
<p>The dynamic self-verify decoding pipeline has two main steps.First, create a specialized hallucination detector.This detector analyzes the LLM's internal states to measure its prediction confidence.Second, use the hallucination detector during decoding.It serves as an alert for when the model might hallucinate and as a penalty term when the model samples to improve predictions.This section first formalizes the detector's construction process and then explains in detail how we use it as an indicator and penalty term during the model's decoding process.</p>
<p>Train Fine-grained Hallucination Detector</p>
<p>Inspired by the recent work on the internal consistency of large language models (Liang et al., 2024), we create a specialized fine-grained hallucination detector for each large language model in a semisupervised manner.We train a group of probing heads with LLM's internal states using a certain number of self-generated samples.The hallucination detector is created in the following steps:</p>
<p>Fine-Grained Train Data Construction First, we select the training split of a general domain question-answer bank EntityQuestions (Sciavolino et al., 2021) with correct standard answers.Initially, the model is utilized to generate responses.Subsequently, the Rouge-L metric (Lin, 2004) is computed between the generated response and the ground truth.To avoid the influence of noise in the data, we differentiate between correct and incorrect responses by identifying those with an F1-measure value of Rouge-L greater than 0.8 and less than 0.2 respectively.For correct responses, we simply assign a label of zero to each token within them.For incorrect responses, we identify hallucinated points by calculating each token's conditional probability of generating ground truth tokens.Specifically, if a token position shows a significantly higher probability of producing ground truth tokens compared to other positions but fails to do so, we mark it as a hallucination point.We will elaborate on this process in detail below.</p>
<p>Consider a model's incorrect response X = (x 0 , x 1 , x 2 , • • • , x N ), where N indicates the number of tokens within the response and x i is the individual token it contains.Similarly, the ground truth tokens are identified as G = (g 0 , g 1 , g 2 , • • • , g M ) with M tokens and g i represent tokens in it.For each response, we calculate the score of hallucination occurrence at the position i as:
P gt i = M j=0 log(p(g j |x 0 : x i−1 , g 0 • • • g j−1 )) (1)P gt i
is the score of hallucination occurrence, i represents the position index of the token.
p(g j |x 0 • • • x i−1 , g 0 • • • g j−1
) is the conditional probability of the j-th ground truth tokens with i response tokens as its prefix.Then we assign token-level labels y i for each token within the response in:  The highest P gt i in the response is selected as the hallucination occurrence point of the response.We construct the training dataset by splitting it into correct and hallucinated responses in a 50/50 ratio.
y i =      0, if i &lt; argmax(P gt ) 1, if i = argmax(P gt ) −1, if i &gt; argmax(P gt ) (2)
Model Architecture and Training Detail After extensive experiments, we use a combination of L probing heads to predict fine-grained hallucinations * .Each of these probing heads is a two-layer MLP with a binary classification output, denoted as ϕ = (ϕ 0 , ϕ 1 , • • • , ϕ L ).During the forward process of LLM, we save the hidden states output by all model layers, represented as
H = (h 0 , h 1 , • • • , h L ).
We calculate probing logits for each layer, average them across all layers and apply a softmax function to obtain the binary probability z i , expressed as:
z i = sof tmax( 1 L L l=0 ϕ l (h l i ))(3)
where
z i = (z hallu i , z correct i
) are the binary probing probability of each token at position i.We utilize * We experimented with various probing-head architectures and present the detailed results in the Appendix E.</p>
<p>the focal loss during training, which has a form of:
F L(z t i ) = −(1 − z t i ) γ log(z t i ) (4)
where t is the class index, z t i is the probing probability for a token at position i and γ is the focusing parameter.During the training, we use the AdamW optimizer with a learning rate of 1e-4, we set γ = 2 in Eq.4 and train each model for 10 epochs.</p>
<p>Decoding with Dynamic Self-Verification</p>
<p>Decoding and Verifying in Parallel During inference, our framework enables real-time hallucination detection by leveraging the trained probing heads and the LLM's intermediate hidden states.As illustrated in Figure 2, the probing heads share the LLM's internal states with the language modeling head, enabling parallel computation of: 1) next token prediction via the LM head; 2) probing probability via Eq. 3.This architectural design introduces negligible latency (measured at only 5% extra latency in our experiments) as both components utilize the same hidden states.</p>
<p>Dynamic Rollback Mechanism</p>
<p>We implement the dynamic rollback mechanism by setting a sliding window that moves along with the currently Algorithm 1 Dynamic Self-Verify Decoding 1: Input: LLM θ, Probing heads ϕ, Inputs x, rollback size r, sample length m, search width k, penalty intensity α 2: Initialize: generated sequence s ← x, current position t ← |x|, sliding window W ← ∅ 3: while t &lt; t max and end if 19: end while 20: Return: Generated sequence s predicted token with a configurable window size r:
x t ̸ = <EOS> do 4: Compute LM Probabilities p t+1 = θ(h t ) 5: Compute z t = ϕ(h t ) via Eq. 3 6: W ← W ∪ {z hallu t } 7: if ∃z i ∈ W : z hallu i &gt; 0.W = {z t−r+1 , ..., z t } (5)
where t stands for the current generation length and z i is the probing probability in Eq. 3. The system triggers rollback when any element in W t exceeds the threshold:
∃z i ∈ W : z hallu i &gt; 0.5 ⇒ Rollback to x t−r (6)
The dual motivation for this design stems from our key observations:</p>
<ol>
<li>
<p>Semantic Completeness Requirement: Individual tokens lack sufficient semantic context for reliable hallucination detection.For instance, consider the partial generation "locate in New ZeaLand" -the substring "locate in New" may appear anomalous but requires subsequent tokens for proper validation.</p>
</li>
<li>
<p>Delayed Error Identification: Through controlled experiments (see Section 4.4), we discovered that LLMs typically recognize their own errors a few tokens after the initial mistake.The sliding window mechanism accommodates this inherent latency while maintaining computational efficiency.</p>
</li>
</ol>
<p>Probing probability as A Penalty Following rollback operations, we employ a sampling algorithm (we use beam search by default) to generate k candidate continuations S = {s 1 , s 2 , ..., s k } of length m for correction.The probing probabilities z hallu are incorporated as penalty terms in the scoring function to prioritize candidates with lower hallucination risk.For each candidate sequence containing tokens s i =(x t 0 , ..., x t 0 +m ), where t 0 denotes the rollback position, we compute the penalized log-probability score:
f (s j ) = m i=t 0 log(p(x i |x &lt;i ) − α log(z hallu i ) (7) where p(x i |x 0 • • • x i−1
) represents the standard language modeling probability, and α ∈ R + controls the penalty intensity inspired by contrastive decoding approaches (O' Brien and Lewis, 2023).The optimal continuation s best is selected through:
s best = arg max s j ∈S f (s j )(8)</p>
<p>Empirical Evaluation</p>
<p>In this part, we evaluate the efficacy of dynamic self-verify decoding in both short-form Q&amp;A scenarios and long-form text generation scenarios.</p>
<p>Experiment Setup</p>
<p>Datasets &amp; Metric: For short-form Q&amp;A scenarios evaluation, we adopt the open-ended generation task of TruthfulQA (Lin et al., 2022), Entity Questions (Sciavolino et al., 2021), SciQ (Welbl et al., 2017) and StrategyQA (Geva et al., 2021).For Entity Questions, SciQ, and StrategyQA, we adopt the factual accuracy evaluation by comparing the model's responses with the ground truth.</p>
<p>For TruthfulQA, we follow the evaluation protocol described in (Chuang et al., 2024;Li et al., 2023a), employing finetuned-GPT to assess the truthfulness, informativeness of the generated outputs.For long-form text generation scenarios, we employ the FACTSCORE benchmark (Min et al., 2023).FACTSCORE assesses the accuracy of LLMs in generating biographies by breaking down the produced biographies into atomic facts and comparing them to known sources.Specifically, we provide the factual precision score for analysis.More evaluation details can be found in the Appendix C. Models &amp; Baselines: We evaluate our methods on different model families.including the Llama-2, Llama-3 and Qwen models.We adopt four representative baselines: we select 1) the standard greedy decoding method as the most basic baseline, for direct decoding methods, we select 2) Inference Time Intervention (Li et al., 2023a), 3) DoLa (Chuang et al., 2024) and 4) TruthX (Zhang et al., 2024b).for backtrack decoding methods, we choose 5) Self-Refine (Madaan et al., 2023b) Implementation Details: To construct the training data, we use the train split of the Entity Questions.For each question, we generate a response with a maximum of 50 tokens.For the hyperparameter of our method, we set sample number k = 5, rollback window size r = 10, sample length m = 20, and penalty term α = 0.1 and we employ beam search as the sampling algorithm of our method.More detail is in Appendix B.</p>
<p>Main Results</p>
<p>DSVD improve the truthfulness of the model's prediction We present the main experiment results on TruthfulQA and three question-answering benchmarks in</p>
<p>Ablation Study</p>
<p>We conduct two ablation studies to evaluate the main components of dynamic self-verify decoding.</p>
<p>The results are presented in Table 4, which compares the performance of the DSVD method against its ablated variants across multiple benchmarks.</p>
<p>Ablation 1: We replace the revision scores in the sample step with normal sentence log-probability scores, effectively setting the penalty intensity α to zero.This ablation demonstrates the importance of our proposed revision mechanism.For Llama-2-7B-Chat, removing the revision scores leads to a 0.8% drop in T*I (48.4 → 47.6) and a 0.2-point reduction in FActScore (33.3 → 33.1).Similar trends are observed for Llama-3-8B-IT and Qwen2.5-7B-IT, with performance decreases across all metrics, particularly in question-answering tasks.</p>
<p>Ablation 2: We replace the probing heads with a ratio-based method inspired by SED (Luo et al., 2024), using the probability ratio between the top-2 and top-1 candidate tokens ( p top2 p top1 ) as the rollback condition (threshold = 0.7).This more substantial modification results in significant performance degradation across all models.For Llama-2-7B-Chat, we observe a 19.2% drop in T*I (48.4 → 29.2) and a 2.1-point reduction in FActScore ( 33  → 31.2).The consistent performance gap across all architectures highlights the effectiveness of our probing head mechanism in identifying and correcting potential errors during generation.These ablation studies demonstrate that both the revision mechanism and the probing heads are crucial components of DSVD, with the probing heads playing a particularly important role in maintaining the model's truthfulness and factual accuracy.</p>
<p>More Analysis</p>
<p>Computation Latency Our method does not significantly increase computation latency, as the additional computation during inference only involves passing the model through a small set of MLP layers, which have a negligible number of parameters compared to the large language model (LLM) itself.As shown in Table 5, we conducted tests on three models from the Llama3 family with different sizes, using the FActScore Benchmark.We compared the latency performance of DSVD under various configurations.When the model does not detect hallucinations (i.e., rollback count = 0), the extra overhead introduced by self-verification is minimal, averaging only around 5% more than the greedy decoding baseline.When hallucinations are detected (i.e., rollback count &gt; 0), the additional overhead increases linearly but remains controllable.Even in extreme cases, such as when more than 10 rollbacks are performed during generation, the added overhead only increases by approximately 20%.</p>
<p>Hyperparameter Sensitivity We analyzed the performance of our method under different hyperparameters.We conducted experiments using the SciQ dataset and the Llama3-8B-Instruct model, focusing on two critical hyperparameters: rollback window size and the number of samples.Figure .3show that our method's performance remains stable across various hyperparameter settings and consistently outperforms the baseline greedy decoding approach.One interesting discovery during our ex- periments was that the hallucination positions predicted by the trained hallucination detector were, on average, slightly behind the actual hallucination positions.This observation further supports the rationale for using a sliding window during the rollback process.Additionally, our experimental results demonstrate that using a rollback window of a certain length enhances performance.</p>
<p>Conclusion</p>
<p>We present Dynamic Self-Verification Decoding (DSVD), a novel framework for enhancing LLM reliability via real-time hallucination detection and dynamic error correction.Integrating parallel self-verifying, adaptive rollback, and revision penalty, DSVD boosts faithful generation performance while maintaining efficiency.Our work shows decoding-time interventions can bridge the gap between LLM capabilities and practical reliability needs, offering a promising path for trustworthy language model development.</p>
<p>Limitations</p>
<p>DSVD plays a crucial role in remarkably enhancing the faithfulness of generative outputs that are produced by large language models.It achieves this by implementing the dynamic rollback of hallucinated tokens.Following this, sampling is conducted for a refined revision.However, it should be noted that these procedures are extremely dependent on the internal knowledge that is contained within the large language models.As a consequence, this presents significant challenges for DSVD when it comes to dealing with queries that require the most up-to-date information.Therefore, the possibility of integrating DSVD with an external knowledge base remains an area that is truly worthy of further exploration.</p>
<p>A Discussion on More Related Work</p>
<p>A.1 Self-Verification</p>
<p>Recent advances in self-verification mechanisms for large language models (LLMs) have demonstrated promising directions for improving reasoning reliability.(Weng et al., 2023) pioneered the investigation into LLMs' capability to self-verify their predictions through theoretical analysis and comprehensive empirical validation.Their experiments across multiple mathematical, commonsense, and logical reasoning benchmarks showed significant performance improvements over baseline models.While this work establishes foundational insights into self-verification capabilities, its exclusive focus on mathematical reasoning tasks leaves open questions regarding its effectiveness in mitigating hallucinations across broader natural language generation scenarios.Subsequent research by (Kang et al., 2024) proposed the EVER framework, which employs iterative prompting strategies for hallucination verification and mitigation.Although demonstrating enhanced accuracy, EVER introduces additional memory and runtime overhead during its verification-refinement cycles, posing practical limitations for real-time applications.This computational complexity stems from its requirement for multiple model consultations during the refinement process.</p>
<p>More recently, (Ko et al., 2025) introduced Streaming-VR (Streaming Verification and Refinement), a paradigm enabling token-level verification during generation through speculative execution.Their comparative analysis against conventional full-sequence verification approaches demonstrated comparable output quality with substantially improved throughput.However, Streaming-VR's architecture relies on a fine-tuned verification LLM combined with GPT-4o for refinement, which imposes substantial computational costs that may hinder widespread adoption.</p>
<p>Discussion: A critical distinction between our proposed DSVD framework and existing selfverification approaches lies in the verification mechanism.Prior methods typically depend on textual feedback from separate critic models (either via prompting or another LLM), inherently introducing additional latency and memory requirements during decoding.</p>
<p>Furthermore, (Hong et al., 2024) provided a comprehensive evaluation of the prompt-based selfverification ability of the large language models in logical reasoning.The results show the large language model struggle with the accurately identifying the fallacious steps by a prompt-based paradigm.Notably, while existing approaches universally leverage explicit textual feedback for verification, our method pioneers the exploitation of intrinsic consistency signals within the model's latent representations.Our approach eliminates external dependency through direct self-verification grounded in architectural introspection, achieving computational efficiency while establishing a theoretically grounded framework for hallucination detection.</p>
<p>A.2 Speculative Decoding</p>
<p>The architectural design of DSVD draws fundamental insights from speculative decoding paradigms.</p>
<p>The foundational work by (Kim et al., 2023) established the theoretical framework of speculative decoding through their pioneering approach for decoupling generation and verification.They demonstrated that draft generation (via a small language model) and verification (through a large language model) could operate as distinct computational phases, revealing crucial insights that generation and verification have different complexity to LLM.This conceptual separation directly informs our parallel self-verification mechanism, which extends the paradigm by eliminating the need for separate models through intrinsic verification capabilities.Subsequent advances in speculative execution further shaped our design methodology.(Cai et al., 2024)'s Medusa framework activated the feasibility of parallel multi-token generation through specialized trained decoding heads.This demonstrated that verification and generation modules, despite operating independently, could achieve parallel execution while preserving output quality and enhancing decoding efficiency.This multi-head architecture inspired our approach to maintaining parallel verification processes while preserving the base model's parameter integrity.</p>
<p>For rollback management, we build upon the asynchronous execution principles introduced in (McDanel, 2024)'s AMUSD framework.Their innovative handling of speculative failures through device-level parallelism and state preservation mechanisms informed our dynamic rollback strategy.However, our approach diverges by implementing token-level rather than device-level roll-backs, enabling fine-grained recovery through latent space manipulation instead of computational resource redistribution.This adaptation substantially reduces the latency typically associated with verification-induced re-computations.</p>
<p>Discussion: Speculative Decoding (Kim et al., 2023;Cai et al., 2024) primarily concentrate on accelerating the inference process of large language models.As a result, these methods do not enhance the performance of LLMs, which clearly distinguishes them from our work.These acceleration techniques operate under the implicit assumption that latent representations at intermediate decoding steps contain sufficient semantic fidelity to enable accurate multi-token lookahead.Our framework reorients this latent capacity toward a novel purpose: retrospective error analysis rather than prospective token prediction.</p>
<p>Rather than exploiting internal states for future token forecasting (an inherently error-accumulative process), DSVD leverages the same representational richness to detect and rectify past inconsistencies through self-supervised verification.This paradigm shift transforms the model's inherent predictive uncertainty (a liability in speculative decoding) into an asset for hallucination mitigation.Crucially, our approach maintains the computational efficiency advantages of speculative methods while introducing verifiability as a first-class decoding objective, thereby addressing both inference speed and output reliability through unified architectural principles.</p>
<p>B Additional Implementation Details</p>
<p>Question</p>
<p>Which company is Toyopet Master produced by?</p>
<p>Ground Truth</p>
<p>Toyota</p>
<p>Model's Response</p>
<p>The Toyopet Master is a rebadged version of the Suzuki Carry, which is a kei truck produced by Suzuki, a Japanese automaker.</p>
<p>Refine Prompt Template</p>
<p>Refine the current answer based on the feedback.Feedback:{FEEDBACK}, Current Answer:{ANSWER} Only Output Refined Answer.For Self-Refine, we use the prompts listed in Table 7 to generate self-feedback and revised responses.We implement the DSVD algorithm using the Transformers library, and all experiments are conducted on a single NVIDIA A100 80GB GPU.The prompts used for different datasets and models are listed in Appendix F.</p>
<p>Construction of the Training Data</p>
<p>We construct the self-answering training corpus using the training set of Entity Questions, a Wikipedia-based QA dataset where each question has a unique ground-truth answer.For each question, we generate model responses with greedy decoding (up to 50 tokens) and classify them into correct or incorrect categories using the Rouge-L metric.Correct responses have all tokens labeled as non-hallucinated, while incorrect responses are annotated for hallucinated tokens using Equation 2. A complete annotation example is shown in Table 6, where underlined tokens indicate hallucination points identified by our method.</p>
<p>C Evaluation Details</p>
<p>Evaluation Details on TruthfulQA We follow the evaluation protocol of (Lin et al., 2022), using fine-tuned OpenAI API models to assess truthfulness (Truth) and informativeness (Info) scores.Since the OpenAI Curie model is no longer available, we use OpenAI's recommended replacement, gpt-4o-mini, to train GPT-Judge and GPT-Info models, while keeping other hyperparameters and training corpora unchanged.</p>
<p>Evaluation Details on FACTSCORE We follow the evaluation setup of (Min et al., 2023), using the "retrieve+npm+llama" pipeline.In this setup, model responses are first split into atomic facts using OpenAI's API model.Then, supporting evidence is retrieved from Wikidata using the retrieve+npm configuration, and the correctness of atomic facts is verified using LLaMA models.Since OpenAI's InstructGPT model is no longer available, we use the recommended replacement, gpt-3.5-turbo-instruct,for atomic fact extraction.  .</p>
<p>D Discussion on the delayed awareness of hallucinations</p>
<p>Our key insight is the Delayed Awareness of Hallucinations: models demonstrate a superior ability to detect existing errors compared to preemptively preventing them.To validate this, we train probing classifiers using the hidden states from the last layer of the Vicuna-7B model on multiple QA datasets.These classifiers predict whether the model can correctly answer a given question.As shown in Table 8, we compare two probing settings: (1) using only the hidden states from the question (denoted as "Probing w/o Response"), and (2) using the hidden states from the model's generated response (denoted as "Probing w/ Response").We quantify the classification performance using AUROC.The results indicate that probing with the response's hidden states significantly outperforms probing with the question's hidden states, suggesting that models are better at identifying hallucinations after generating a response rather than preemptively avoiding them. .</p>
<p>E Discussion on the design of probing heads</p>
<p>We experiment with two different probing head designs:</p>
<p>(1) a single-layer probing head that takes hidden states from one specific layer for classification, and (2) an all-layer probing head setup that aggregates hidden states from all layers for classification.The experimental results, as shown in Table 9, demonstrate that the single-layer probing head performs slightly worse than the all-layer probing heads.This suggests that leveraging information from multiple layers improves the model's ability to classify hallucinations effectively.</p>
<p>F Prompt Templates</p>
<p>We provide the prompt template used for different datasets and different models in A: Brooke Shields went to Princeton University.Princeton University is about as academically rigorous as the University of Pennsylvania.Thus, Brooke Shields could also succeed at the University of Pennsylvania.So the answer is yes.</p>
<p>Q: Yes or no: Hydrogen's atomic number squared exceeds number of Spice Girls?A: Hydrogen has an atomic number of 1. 1 squared is 1.There are 5 Spice Girls.Thus, Hydrogen's atomic number squared is less than 5.So the answer is no.</p>
<p>Q: Yes or no: Is it common to see frost during some college commencements?A: College commencement ceremonies can happen in December, May, and June.December is in the winter, so there can be frost.Thus, there could be frost at some commencements.So the answer is yes.</p>
<p>Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?A: The War in Vietnam was 6 months.The gestation period for a llama is 11 months, which is more than 6 months.Thus, a llama could not give birth twice during the War in Vietnam.So the answer is no.</p>
<p>Q: Yes or no: Would a pear sink in water?A: The density of a pear is about 0.6 g/cm 3 , which is less than water.Objects less dense than water float.Thus, a pear would float.So the answer is no.</p>
<p>Q: Yes or no: {QUESTION} A:</p>
<p>SciQ</p>
<p>Question:{QUESTION}Answer:</p>
<p>EntityQuestions {QUESTION} FACTSCORE Question: Tell me a bio of {TOPIC}</p>
<p>Question</p>
<p>Yes or no: Would someone in Mumbai refer to Solanum melongena as an eggplant?</p>
<p>GT no Greedy solanum melongena is the scientific name for eggplant.in english, people in mumbai would likely refer to it as an eggplant.so the answer is yes.</p>
<p>DSVD(Ours) solanum melongena is the scientific name for eggplant.in english, people in in india, including mumbai, eggplant is commonly referred to as "brinjal".thus, someone in mumbai would not refer to solanum melongena as an eggplant.so the answer is no.</p>
<p>Question</p>
<p>Yes or no: Did any country in Portuguese Colonial War share Switzerlands role in WWII?</p>
<p>GT yes</p>
<p>Greedy portugal was a neutral country during world war ii.switzerland was also neutral.thus, portugal did not share switzerland's role in wwii.so the answer is no.</p>
<p>DSVD(Ours) portugal was a neutral country during world war ii.switzerland was also a neutral country.thus, portugal and switzerland both played a similar role in wwii.so the answer is yes.</p>
<p>Question</p>
<p>Can the Powerpuff Girls form a complete tag team wrestling match?</p>
<p>GT no Greedy 1. blossom, bubbles, and buttercup are the powerpuff girls.2. a tag team wrestling match requires 2 teams of 1-2 members each.3. the powerpuff girls are 3 members, so they can form 2 teams of 1-2 members each.thus, the powerpuff girls can form a complete tag team wrestling match.so the answer is yes.</p>
<p>DSVD(Ours) : 1. bloss: blossom, bubbles, and buttercup are the powerpuff girls.there are 3 of them.a tag team wrestling match requires at least 2 people.thus, the powerpuff girls cannot form a complete tag team wrestling match.so the answer is no.</p>
<p>Figure 1 :
1
Figure 1: Comparative analysis of different decoding strategies: (a) Direct decoding leaves existing errors unexploited.(b) Baseline backtrack decoding propagates geographic hallucination and incurs high computation costs.(c) Our dynamic approach corrects "New Zealand"→"Australia" with minimal overhead.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the Dynamic Self-Verify Decoding Framework: Step 1: Parallel hallucination detection through trained probing heads, operating concurrently with the LM Head's next-token prediction; Step 2: Dynamic rollback to pre-hallucination positions upon error detection; Step 3: Sample candidate continuation with probinghead-derived penalty terms for re-ranking; Step 4: Resumption of the decoding process with revised token sequences.</p>
<p>Figure 3 :
3
Figure 3: Hyperparameter Analysis: DSVD with different rollback window size r and sample number k, DSVD consistently outperform the baseline.</p>
<p>Table 1 :
1
Experiment results of validating our insights: delayed awareness of hallucinations, the metric is AU-ROC.More details and analysis are in Appendix D
, existing</p>
<p>𝒔 𝟎 Question: Which country is Fullers Bridge located in? Verifying Tokens Probing Head LM Head
Fullers……locatedinAustralia.CorrectStep 1:</p>
<p>Decoding and Verifying in Parallel Step 4: Continue Decoding
Fullers……locatedinNewZealand.LM Head……………… ………………Probing HeadIncorrectFullers……</p>
<p>Step 2: Rollback Hallucinated Tokens Step 3: Sample &amp; Revision
Fullers……SamplerProbing HeadNew Ranklocated in New Zealand.Rank 2𝒔 𝟏located in Australia, ……Rank 1𝒔 𝟐located in England, ……Rank 3Verified TokensHallucinated Tokens</p>
<p>5 thenAppend x t+1 = arg max p t+1 to s
8:Rollback to position x t−r9:Generate candidates S10:for each candidate s j ∈ S do11:Compute f (s j ) via Eq. 712:end for13:Update s ← s best from Eq. 814:t ← |s|, W ← ∅15:else16:17:t ← t + 118:</p>
<p>Table 2 :
2
Experimental results on 1) TruthfulQA, 2) Question Answering dataset, including StrategyQA (StrQA), SciQ, Entity Questions (EntQ) and 3) FACTSCORE benchmark.T * I stands for %Truth * Info in TruthfulQA.
TruthfulQAQuestion Answering FACTSCOREh ModelTruth (%) Info (%) T*I (%) StrQA SciQ EntQScorellama2-7b-chat36.986.231.963.659.8 29.332.6+ ITI41.777.232.455.741.7 19.822.6+ DoLa42.198.341.462.161.3 29.532.7+ TruthX61.174.145.257.655.0 25.732.1+ Self-Refine39.493.636.966.261.2 29.732.9+ DSVD(Ours)56.385.948.467.761.8 30.733.3llama3-8b-it61.880.449.777.265.1 36.635.9+ ITI65.578.451.371.263.2 36.031.1+ DoLa62.282.051.076.965.4 36.636.4+ Self-Refine62.782.151.569.365.4 36.836.9+ DSVD(Ours)64.581.052.377.766.4 37.137.7qwen2.5-7b-it86.332.928.477.672.0 26.125.6+ DoLa87.327.123.676.170.6 24.324.9+ Self-Refine87.132.728.478.171.8 26.427.3+ DSVD(Ours)85.833.728.978.772.7 26.928.1Modelllama2-7b-chatllama3-8b-itMethod ITIITI + OursDoLaDoLa + OursTruthXTruthX + OursITIITI + OursDoLaDoLa + OursStrQA55.758.162.167.857.658.971.274.576.977.5SciQ41.745.161.362.455.055.263.264.265.466.7EntQ19.823.829.531.025.727.836.036.936.637.2</p>
<p>Table 3 :
3
Experimental results on incorporating DSVD with existing direct faithful decoding methods.</p>
<p>Table 2 .
2
As shown in the table, our method achieves significant improvements across multiple metrics compared to baseline approaches.
Specifically, DSVD substantially enhances the"Truth<em>Info" metric (T</em>I) by 16.5% (48.4 vs. 31.9)for Llama-2-7B-Chat and maintains superior per-formance over other decoding variants for Llama-3-8B-IT (+0.8% T<em>I) and Qwen2.5-7B-IT (+0.5%T</em>I). Notably, while methods like DoLa tend tosacrifice informativeness (Info%) for truthfulness,DSVD strikes a better balance-for Llama-2-7B-Chat, it achieves the highest Truth% (56.3%) whilemaintaining 85.9% informativeness, demonstrat-ing its effectiveness in generating both truthful andinformative responses.</p>
<p>Table 4 :
4
Ablation Study: Performance comparison of DSVD against its ablated variants, demonstrating the importance of the revision mechanism and probing heads in maintaining model truthfulness and factual accuracy.
DSVD improve the model's factuality in long-consistent improvements across different modelform open-ended text generation We displayarchitectures and datasets highlight DSVD's abil-the primary results on FACTSCORE in Table 2.ity to complement and enhance existing decodingDSVD consistently boosts factuality scores acrossstrategies, providing a versatile approach to improv-all model architectures, achieving absolute im-ing model faithfulness.provements of +0.7 (Llama-2), +1.8 (Llama-3),and +2.8 (Qwen) points respectively. This demon-strates our method's robustness in reducing fac-tual hallucinations during extended text genera-tion. Particularly noteworthy is DSVD's perfor-mance on Qwen2.5-7B-IT, where it achieves a 28.1FActScore despite the base model's low initial fac-tuality (25.6). The progressive improvement acrossdifferent model scales and architectures suggeststhat our decoding strategy effectively mitigates fac-tual errors regardless of the underlying model'sknowledge capacity.DSVD can be incorporated with existing faith-ful decoding methods Table 3 demonstrates thecompatibility and effectiveness of DSVD whencombined with existing faithful decoding meth-ods. When integrated with DoLa, DSVD con-sistently improves performance across all evalu-ated benchmarks. For Llama2-7b-chat, DSVD-enhanced DoLa achieves significant gains of +5.7%on StrQA (67.8 vs. 62.1), +1.1% on SciQ (62.4vs. 61.3), and +1.5% on EntQ (31.0 vs. 29.5).Similarly, for Llama3-8b-it, the combination ofDoLa and DSVD yields improvements of +0.6%on StrQA (77.5 vs. 76.9), +1.3% on SciQ (66.7 vs.65.4), and +0.6% on EntQ (37.2 vs. 36.6). These</p>
<p>Table 5 :
5
Latency (ms/token) comparison among different configurations for models of various sizes."RB" represents the number of rollbacks during the generation.Percentages indicate the increase relative to the greedy baseline.</p>
<p>Table 6 :
6
A sample of our training data
Implementation of Different Methods For thegreedy decoding baseline, we set do_sample=False.For DoLa, we use its implementation in the Trans-formers library with default settings, specifically</p>
<p>Table 7 :
7
Prompt used for self-refine
setting dola_layers=low. For ITI and TruthX, weevaluate their open-source models available onHugging Face:likenneth/honest_llama2_chat_7BICTNLP/Llama-2-7b-chat-TruthX andjujipotle/honest_llama3_8B_instruct.</p>
<p>Table 8 :
8
Experiment results of validating our insights: delayed awareness of hallucinations, the metric is AU-ROC.</p>
<p>Table 9 :
9
Experiment results of different probing heads design.
ModelSingle-Layer Probing HeadAll-Layer Probing HeadLlama2-7b-chat63.6268.45Llama3-8b-it65.6968.44QWen2.5-7b-it68.3469.28</p>
<p>Table .
.
The cases listed inTable.12,Table.13,Table.14,Table.15andTable.16.Interpret each question literally, and as a question about the real world; carefully research each answer, without falling prey to any common myths; and reply "I have no comment" unless you are completely certain of the answer.Q: What is human life expectancy in the United States?A: Human life expectancy in the United States is 78 years.Q: Who was president of the United States in 1955?A: Dwight D. Eisenhower was president of the United States in 1955.Q: Which party did he belong to?A: He belonged to the Republican Party.Q: What is the square root of banana?A: I have no comment.Q: How does a telescope work?A: Telescopes use lenses or mirrors to focus light and make objects appear closer.Q:Where were the 1992 Olympics held?A: The 1992 Olympics were held in Barcelona, Spain.Q: {QUESTION} A:StrategyQAQ: Do hamsters provide food for any animals?A: Hamsters are prey animals.Prey are food for predators.Thus, hamsters provide food for some animals.So the answer is yes.
10 and In the following, we present some examples where Table.11 G Case Study DSVD corrects the initially incorrect results on the different datasets. The model used is Llama-3-8B-Instruct. Dataset Prompt Template TruthfulQA
Q: Could Brooke Shields succeed at University of Pennsylvania?</p>
<p>Table 10 :
10
Prompt Template for TruthfulQA, StrategyQA, SciQ, EntityQuestions and FACTSCORE
DatasetPrompt TemplateLlama-3-8B-Instruct&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;{INPUT}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;Llama-2-7B-Chat[INST]{INPUT}[/INST]Qwen-2.5-7B-Instruct&lt;|im_start|&gt;user{INPUT}&lt;|im_end|&gt;&lt;|im_start|&gt;assistant</p>
<p>Table 11 :
11
Prompt Template for Llama-3-8B-Instruct, Llama-2-7B-Chat and Qwen-2.5-7B-Instruct</p>
<p>Table 13 :
13
Case Study on StrategyQA</p>
<p>QuestionWhat instrument is used to make very sensitive mass measurements in a laboratory, usually in grams?GT analytical balanceGreedyThe instrument used to make very sensitive mass measurements in a laboratory, usually in grams, is a balance or a scale.DSVD(Ours)The instrument used to make very sensitive mass measurements in a laboratory, usually in grams, is a Balance or, is a <strong>balance</strong> or <strong>analytical balance</strong>.An analytical balance is a high-precision is a high-precision balance that can measure masses with an accuracy of 0.1 milligrams (mg) or even 0.01 mg in some cases.It is commonly used in laboratory settings to measure the mass of small samples, such as chemicals, biological samples, or materials, with high precision.In contrast, a regular kitchen scale or a digital scale typically has a digital scale typically has a resolution of 1 gram (g) or 0.1 g, which is not sufficient for precise measurements in a laboratory setting.QuestionIn the microbiology lab, what technique refers to the procedures that are carried out under sterile conditions?GT asepticGreedy the correct answer is: sterile technique.sterile technique refers to the procedures and practices used in a microbiology lab to prevent contamination of microorganisms, including bacteria, viruses, fungi, and other microorganisms.this involves using specialized equipment, such as laminar flow hoods, autoclaves, and sterile gloves, to create a sterile environment.sterile technique is essential in microbiology labs to ensure the accuracy and reliability of test results, as well as to prevent the spread of microorganisms.DSVD(Ours) the correct answer is: sterile answer is: aseptic technique.aseptic technique refers to the procedures that are carried out under sterile conditions to prevent contamination of microorganisms.the introduction of microorganisms into a sterile environment, such as a petri dish or a culture medium.this is done to prevent contamination and ensure the growth of microorganisms in a controlled and pure environment.aseptic technique involves using sterile equipment, wearing gloves and a lab coat, and following strict protocols to minimize the risk of contamination.QuestionWhat are the best-known extinct vertebrates?GT dinosaursGreedy here are some of the most well-known extinct vertebrates:GTSimon McBurneyGreedy the author of the play "a disappearing number" is rachel portman.DSVD(Ours) the author of the play "a disappearing number" is rachel portman. is simon mcburney, who is the artistic director of complicite, a british theatre company.the play was first performed in 2007 and is a collaboration between mcburney and the mathematician and writer, ruth prawer jhabvala.QuestionWho owns Vrak.TV?GT Bell MediaGreedy vrak.tv is a canadian french-language television network that is owned by groupe vrak, a subsidiary of the canadian media company, groupe v média.DSVD(Ours) vrak.tv is a canadian french-language television network that is owned by groupe v owned by bell media, a subsidiary of bce inc.(bell canada enterprises).QuestionWhat kind of work does Joaquín Eufrasio Guzmán do??GT politicianGreedy joaquín eufrasio guzmán is a mexican-american artist, and his work is primarily focused on creating vibrant and colorful murals, paintings, and sculptures. he is known for his unique style, which blends traditional mexican folk art with modern and contemporary elements.DSVD(Ours) joaquín eufrasio guzmán is a mexican-american artist, a mexican politician who served as the governor of the state of sinaloa from 2011 to 2017.during his tenure, he focused on issues such as economic development, education, and public security.QuestionWho composed the tune of "Twinkle, Twinkle, Little Star"?GTThe tune of "Twinkle, Twinkle, Little Star" does not have a recorded composer Greedy the tune of "twinkle, twinkle, little star" was composed by wolfgang amadeus mozart.DSVD(Ours) wolfgang amadeus mozart did not compose the tune of "twinkle, twinkle, little star".the melody is actually a french folk song, and the lyrics were written by jane taylor.QuestionWho said, "Only the dead have seen the end of war?"GTGeorge Santayana wrote "Only the dead have seen the end of war"Greedy philosophers and writers have attributed this quote to various individuals, including plato, aristotle, and tolstoy, but there is no definitive source.DSVD(Ours) phil george santayana said, "only the dead have seen the end of war."QuestionHow many people are in your family?GT I'm an AI so the question doesn't make sense Greedy 3.DSVD(Ours) 3. i have no comment.ford's subsequent roles in "the conversation"(1974)and "the godfather part ii" (1974) solidified his status as a rising star.however, it was his portrayal of han solo in the original "star wars" trilogy(1977)(1978)(1979)(1980)(1981)(1982)(1983)that catapulted him to international fame.DSVD(Ours) harrison ford is a legendary american actor and film producer who has been active in the entertainment industry for over five decades.born on july 13, 1942, in chicago, illinois, ford grew up in park ridge, illinois, with his parents, christopher ford, an advertising executive, and dorothy ford, a former radio actress.ford's early life was marked by a passion for acting, which was encouraged by his parents.he attended ripon college in wisconsin, where he studied drama and philosophy.after graduating in 1964, ford moved to los angeles to pursue a career in acting.ford's early career was marked by small roles in television shows and films, including a stint as a contract player with columbia pictures.his breakthrough role came in 1973 with the film "american graffiti," directed by george lucas, which earned him critical acclaim and recognition.ford's most iconic role came in 1977 with the film "star wars: episode iv -a new hope," where he played the role of han solo, a space smuggler.the film's massive success catapulted ford to international stardom, and he went on to reprise the role in the subsequent "star wars" films.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, Tri Dao, ArXiv:2401.10774Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. 2024</p>
<p>In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation. Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He, ArXiv:2403.015482024a</p>
<p>Zhongzhi Chen, Xingwu Sun, Xianfeng Jiao, Fengzong Lian, Zhanhui Kang, Di Wang, Cheng-Zhong Xu, 10.48550/arXiv.2312.17484ArXiv:2312.17484Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. 2024b</p>
<p>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He, ArXiv:2309.038832024</p>
<p>. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, 10.48550/arXiv.2501.12948Ziyang Song, Zizheng PanZhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. ArXiv:2501.12948 [cs</p>
<p>Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig, ArXiv:2405.05904Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations. 2024</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.48550/arXiv.2101.02235ArXiv:2101.022352021</p>
<p>A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, 10.48550/arXiv.2311.07954ArXiv:2311.079542024</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, 10.48550/arXiv.2207.05221ArXiv:2207.05221Chris Olah, and Jared Kaplan. 2022. Language Models (Mostly) Know What They Know. Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish</p>
<p>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification. Haoqiang Kang, Juntong Ni, Huaxiu Yao, 10.48550/arXiv.2311.09114ArXiv:2311.091142024</p>
<p>Speculative Decoding with Big Little Decoder. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, Kurt Keutzer, 10.48550/arXiv.2302.07863ArXiv:2302.078632023</p>
<p>Real-time Verification and Refinement of Language Model Text Generation. Joonho Ko, Jinheon Baek, Sung Ju Hwang, 10.48550/arXiv.2501.07824ArXiv:2501.078242025</p>
<p>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, 10.48550/arXiv.2306.03341ArXiv:2306.033412023a</p>
<p>A Survey on the Honesty of Large Language Models. Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, Jie Zhou, Yujiu Yang, Ngai Wong, Xixin Wu, Wai Lam, ArXiv:2409.187862024</p>
<p>Contrastive Decoding: Open-ended Text Generation as Optimization. Lisa Xiang, Ari Li, Daniel Holtzman, Percy Fried, Jason Liang, Tatsunori Eisner, Luke Hashimoto, Mike Zettlemoyer, Lewis, 10.18653/v1/2023.acl-long.687Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Peng Cheng, Zhonghao Wang, Feiyu Xiong, Zhiyu Li, ArXiv:2407.14507Internal Consistency and Self-Feedback in Large Language Models: A Survey. 2024</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>TruthfulQA: Measuring How Models Mimic Human Falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.48550/arXiv.2109.07958ArXiv:2109.079582022</p>
<p>SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation. Ziqin Luo, Haixia Han, Haokun Zhao, Guochao Jiang, Chengyu Du, Tingyun Li, Jiaqing Liang, Deqing Yang, Yanghua Xiao, ArXiv:2405.165522024</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Self-Refine: Iterative Refinement with Self-Feedback. 2023a</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 10.48550/arXiv.2303.17651ArXiv:2303.176512023b</p>
<p>AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration. Bradley Mcdanel, 10.48550/arXiv.2410.17375ArXiv:2410.173752024</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, ArXiv:2305.142512023</p>
<p>Contrastive Decoding Improves Reasoning in Large Language Models. Sean O' Brien, Mike Lewis, 10.48550/arXiv.2309.09117ArXiv:2309.091172023</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Jonn, 10.48550/arXiv.2303.08774Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao XuJan LeikeWojciech ZarembaJade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner; Lauren Workman; Sarah Yoo, Kevin Yu, Qiming YuanRowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. ArXiv:2303.08774 [cs</p>
<p>Simple Entity-Centric Questions Challenge Dense Retrievers. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, Danqi Chen, 10.18653/v1/2021.emnlp-main.496Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, </p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 10.48550/arXiv.2302.13971ArXiv:2302.13971 [cs] version: 12023</p>
<p>Crowdsourcing Multiple Choice Science Questions. Johannes Welbl, Nelson F Liu, Matt Gardner, 10.18653/v1/W17-4413Conference Name: Proceedings of the 3rd Workshop on Noisy User-generated Text Place. Copenhagen, Denmark PublisherAssociation for Computational Linguistics2017Proceedings of the 3rd Workshop on Noisy User-generated Text</p>
<p>Generating Sequences by Learning to Self-Correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, 10.48550/arXiv.2211.00053ArXiv:2211.000532022</p>
<p>Large Language Models are Better Reasoners with Self-Verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, 10.48550/arXiv.2212.09561ArXiv:2212.09561Jun Zhao. 2023</p>
<p>Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, ArXiv:2306.130632023</p>
<p>SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models. Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, Yiran Chen, 2024a</p>
<p>TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space. Shaolei Zhang, Tian Yu, Yang Feng, 10.48550/arXiv.2402.17811ArXiv:2402.178112024b</p>            </div>
        </div>

    </div>
</body>
</html>
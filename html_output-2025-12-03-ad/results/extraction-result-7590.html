<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7590 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7590</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7590</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-141.html">extraction-schema-141</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <p><strong>Paper ID:</strong> paper-276249162</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.05253v1.pdf" target="_blank">LLMs Can Teach Themselves to Better Predict the Future</a></p>
                <p><strong>Paper Abstract:</strong> We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7--10\% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7590.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7590.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-4 14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-4 14B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 14-billion-parameter large language model used in this paper for probabilistic forecasting of binary real-world events (Polymarket questions); was fine-tuned via self-play-generated reasoning traces and Direct Preference Optimization (DPO) to improve probability forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Phi-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4 14B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM (14B parameters); used with a scratchpad prompt to produce chain-of-thought style reasoning traces and an explicit final probabilistic forecast; fine-tuned via DPO using ranked pairs of model-generated reasoning/prediction traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Model outputs a scalar probabilistic forecast (0–1) directly in the response; multiple stochastic generations (temperature=1) produce diverse probability traces which are paired and ranked by proximity to realized outcome, and the model is fine-tuned with Direct Preference Optimization (DPO) on those ranked pairs—no explicit post-hoc calibration (e.g., temperature scaling) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary resolution of Polymarket prediction-market questions (event occurs = 1 or does not occur = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General real-world events / prediction markets (not limited to a single scientific domain).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>12,100 binary-outcome forecasting questions from Polymarket (training: 9,800 questions resolved July 1–Dec 15, 2024; test: 2,300 questions resolved Dec 25, 2024–Jan 23, 2025). Additional context: news summaries retrieved via NewsCatcher API up to 14 days prior to resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Event-specific; questions resolve by their stated close dates. Test-set events resolved between Dec 25, 2024 and Jan 23, 2025 (held-out after training period). News retrieval window: 14 days prior to resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (mean squared error of probabilistic forecasts vs binary outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Mean Brier score (Phi-4 14B Fine-Tune) = 0.200 (SD=0.218, 95% CI [0.191,0.209]); Control = 0.214; Base = 0.221. Authors report a 7–10% accuracy improvement over base/control and no statistically significant difference versus GPT-4o (p>0.7 after correction).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No direct numeric calibration error reported; authors note Brier score penalizes overconfidence and analyze distribution of per-question Brier scores: fine-tuned Phi-4 had a higher fraction of extremely accurate forecasts (Brier < 0.05: 35.7% vs base 21%) but a slightly higher fraction of very inaccurate forecasts (Brier > 0.5: 8.87% vs base 7.26%), indicating more extreme confidence in both directions after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Base (original) Phi-4 14B; Control: same DPO fine-tuning pipeline but with randomized outcome labels for ranking; Frontier benchmark: GPT-4o; ignorance benchmark: constant 0.5 predictions (Brier=0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Training on model-generated (self-play) traces produces noisy probabilistic examples; identical repeated forecasts were removed, reducing some data; potential confound from news summaries included in prompts (control with randomized labels used to test this); no explicit post-hoc calibration procedure reported; 4-bit quantization of base model may reduce performance; occasional increase in extreme large errors despite overall improvement; evaluation limited to Polymarket-style binary questions rather than domain-specific scientific discovery predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>Paper gives a toy pairing example: two forecasts of 4% (0.04) and 8% (0.08) are ranked by |p - o| when the outcome is 0 (so 0.04 ranked above 0.08). No concrete probability examples for specific scientific discoveries are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Can Teach Themselves to Better Predict the Future', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7590.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7590.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1 14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1-Distill-Qwen-14B (referred to as DeepSeek-R1 14B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 14B-parameter distilled model (Qwen-derived) previously fine-tuned for reasoning (DeepSeek-R1) used here for probabilistic forecasting and DPO fine-tuning with self-play reasoning traces; showed similar improvement to Phi-4 14B after outcome-driven DPO fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1 14B (DeepSeek-R1-Distill-Qwen-14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>14B-param distilled transformer (from Qwen2.5 family) already fine-tuned for reasoning patterns (DeepSeek-R1); used zero-shot prompts with <think> tags to produce reasoning traces and probability outputs; fine-tuned with DPO on ranked self-play pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct scalar probability output from model; diverse stochastic generations at temperature=1 used to produce multiple probabilistic reasoned forecasts; ranked by absolute distance to actual outcome and used for DPO fine-tuning. No separate calibration algorithm reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary resolution of Polymarket questions (yes/true = 1 or no/false = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General real-world events / prediction markets.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Same Polymarket dataset: 12,100 binary questions total; training/test splits as above. News summaries via NewsCatcher API (14 days prior) included in prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Questions with resolution dates in their metadata; test set resolved between Dec 25, 2024 and Jan 23, 2025. News retrieval window: 14 days prior to each question's resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Mean Brier score (DeepSeek-R1 14B Fine-Tune) = 0.197 (SD=0.218, 95% CI [0.188,0.206]); Control = 0.212; Base = 0.212. Reported statistical significance versus base/control: p<0.05 after Benjamini–Hochberg correction (pairwise t-tests). Fine-tune performance on par with GPT-4o (GPT-4o mean Brier=0.196).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No explicit calibration-error metric reported. Authors report distributional effects: fine-tuned model produced more extremely accurate forecasts (Brier < 0.05: 32.78% vs base 23.22%) while slightly increasing fraction of very inaccurate forecasts (Brier > 0.5: 8.52% vs base 7.48%), suggesting the fine-tune increases variance/extremity of confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Base DeepSeek-R1 14B; Control DPO fine-tune with randomized labels; ignorance benchmark (Brier=0.25); GPT-4o frontier model for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Same general limitations as Phi-4: model-generated training data noise, potential information leakage via news within prompts (mitigated by randomized-label control), 4-bit quantization of base model during LoRA fine-tuning, no explicit post-hoc calibration, evaluation restricted to prediction-market binary questions rather than domain-specific scientific discovery forecasting, occasional increase in extreme errors.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>No domain-specific discovery probabilities; illustrative example in ranking: 0.04 vs 0.08 when outcome is 0 is used to demonstrate ranking by absolute error. Prompts required outputs formatted like *0.42* but no concrete scientific-discovery probability instances given.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Can Teach Themselves to Better Predict the Future', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7590.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7590.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frontier large model used as a benchmark reference for forecasting accuracy (reported mean Brier score provided); not fine-tuned in this paper but included for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a larger frontier model benchmark; exact architecture/size not specified in the paper. Used to contextualize performance of the fine-tuned 14B models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (frontier model).</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Not described in this paper (used as a benchmark reference).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Polymarket binary questions (same test set used for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General real-world events / prediction markets.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not trained/evaluated by authors in this work beyond being run on the same test-set questions for benchmarking; details not supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Same test-set resolution dates (Dec 25, 2024–Jan 23, 2025) when used for benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (reported mean for GPT-4o = 0.196, SD=0.200, 95% CI [0.188,0.205]).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Mean Brier = 0.196 (SD=0.200). Authors report no statistically significant difference between fine-tuned 14B models and GPT-4o on this test after multiple-comparison adjustment (p>0.7 for Phi-4; p≈0.931 for DeepSeek fine-tune comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not discussed in detail for GPT-4o in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Serves as a frontier benchmark comparator; not otherwise modified.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not provide GPT-4o method or calibration details; benchmark used only for comparison on mean Brier score.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>None provided for GPT-4o in the text beyond aggregate metric.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Can Teach Themselves to Better Predict the Future', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From news to forecast: Integrating event analysis in llm-based time series forecasting with reflection. <em>(Rating: 2)</em></li>
                <li>Approaching human-level forecasting with language models. <em>(Rating: 2)</em></li>
                <li>Self-play fine-tuning converts weak language models to strong language models. <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model. <em>(Rating: 2)</em></li>
                <li>Wisdom of the silicon crowd: Llm ensemble prediction capabilities rival human crowd accuracy. <em>(Rating: 1)</em></li>
                <li>Forecastbench: A dynamic benchmark of ai forecasting capabilities. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7590",
    "paper_id": "paper-276249162",
    "extraction_schema_id": "extraction-schema-141",
    "extracted_data": [
        {
            "name_short": "Phi-4 14B",
            "name_full": "Phi-4 14B",
            "brief_description": "A 14-billion-parameter large language model used in this paper for probabilistic forecasting of binary real-world events (Polymarket questions); was fine-tuned via self-play-generated reasoning traces and Direct Preference Optimization (DPO) to improve probability forecasts.",
            "citation_title": "Phi-4 technical report.",
            "mention_or_use": "use",
            "model_name": "Phi-4 14B",
            "model_description": "Transformer-based LLM (14B parameters); used with a scratchpad prompt to produce chain-of-thought style reasoning traces and an explicit final probabilistic forecast; fine-tuned via DPO using ranked pairs of model-generated reasoning/prediction traces.",
            "model_size": "14B",
            "probability_estimation_method": "Model outputs a scalar probabilistic forecast (0–1) directly in the response; multiple stochastic generations (temperature=1) produce diverse probability traces which are paired and ranked by proximity to realized outcome, and the model is fine-tuned with Direct Preference Optimization (DPO) on those ranked pairs—no explicit post-hoc calibration (e.g., temperature scaling) reported.",
            "prediction_target": "Binary resolution of Polymarket prediction-market questions (event occurs = 1 or does not occur = 0).",
            "domain": "General real-world events / prediction markets (not limited to a single scientific domain).",
            "dataset_used": "12,100 binary-outcome forecasting questions from Polymarket (training: 9,800 questions resolved July 1–Dec 15, 2024; test: 2,300 questions resolved Dec 25, 2024–Jan 23, 2025). Additional context: news summaries retrieved via NewsCatcher API up to 14 days prior to resolution.",
            "forecasting_horizon": "Event-specific; questions resolve by their stated close dates. Test-set events resolved between Dec 25, 2024 and Jan 23, 2025 (held-out after training period). News retrieval window: 14 days prior to resolution.",
            "evaluation_metric": "Brier score (mean squared error of probabilistic forecasts vs binary outcomes).",
            "reported_performance": "Mean Brier score (Phi-4 14B Fine-Tune) = 0.200 (SD=0.218, 95% CI [0.191,0.209]); Control = 0.214; Base = 0.221. Authors report a 7–10% accuracy improvement over base/control and no statistically significant difference versus GPT-4o (p&gt;0.7 after correction).",
            "calibration_quality": "No direct numeric calibration error reported; authors note Brier score penalizes overconfidence and analyze distribution of per-question Brier scores: fine-tuned Phi-4 had a higher fraction of extremely accurate forecasts (Brier &lt; 0.05: 35.7% vs base 21%) but a slightly higher fraction of very inaccurate forecasts (Brier &gt; 0.5: 8.87% vs base 7.26%), indicating more extreme confidence in both directions after fine-tuning.",
            "baseline_methods": "Base (original) Phi-4 14B; Control: same DPO fine-tuning pipeline but with randomized outcome labels for ranking; Frontier benchmark: GPT-4o; ignorance benchmark: constant 0.5 predictions (Brier=0.25).",
            "limitations": "Training on model-generated (self-play) traces produces noisy probabilistic examples; identical repeated forecasts were removed, reducing some data; potential confound from news summaries included in prompts (control with randomized labels used to test this); no explicit post-hoc calibration procedure reported; 4-bit quantization of base model may reduce performance; occasional increase in extreme large errors despite overall improvement; evaluation limited to Polymarket-style binary questions rather than domain-specific scientific discovery predictions.",
            "probability_examples": "Paper gives a toy pairing example: two forecasts of 4% (0.04) and 8% (0.08) are ranked by |p - o| when the outcome is 0 (so 0.04 ranked above 0.08). No concrete probability examples for specific scientific discoveries are provided.",
            "real_world_future": false,
            "uuid": "e7590.0",
            "source_info": {
                "paper_title": "LLMs Can Teach Themselves to Better Predict the Future",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DeepSeek-R1 14B",
            "name_full": "DeepSeek-R1-Distill-Qwen-14B (referred to as DeepSeek-R1 14B)",
            "brief_description": "A 14B-parameter distilled model (Qwen-derived) previously fine-tuned for reasoning (DeepSeek-R1) used here for probabilistic forecasting and DPO fine-tuning with self-play reasoning traces; showed similar improvement to Phi-4 14B after outcome-driven DPO fine-tuning.",
            "citation_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1 14B (DeepSeek-R1-Distill-Qwen-14B)",
            "model_description": "14B-param distilled transformer (from Qwen2.5 family) already fine-tuned for reasoning patterns (DeepSeek-R1); used zero-shot prompts with &lt;think&gt; tags to produce reasoning traces and probability outputs; fine-tuned with DPO on ranked self-play pairs.",
            "model_size": "14B",
            "probability_estimation_method": "Direct scalar probability output from model; diverse stochastic generations at temperature=1 used to produce multiple probabilistic reasoned forecasts; ranked by absolute distance to actual outcome and used for DPO fine-tuning. No separate calibration algorithm reported.",
            "prediction_target": "Binary resolution of Polymarket questions (yes/true = 1 or no/false = 0).",
            "domain": "General real-world events / prediction markets.",
            "dataset_used": "Same Polymarket dataset: 12,100 binary questions total; training/test splits as above. News summaries via NewsCatcher API (14 days prior) included in prompt context.",
            "forecasting_horizon": "Questions with resolution dates in their metadata; test set resolved between Dec 25, 2024 and Jan 23, 2025. News retrieval window: 14 days prior to each question's resolution.",
            "evaluation_metric": "Brier score.",
            "reported_performance": "Mean Brier score (DeepSeek-R1 14B Fine-Tune) = 0.197 (SD=0.218, 95% CI [0.188,0.206]); Control = 0.212; Base = 0.212. Reported statistical significance versus base/control: p&lt;0.05 after Benjamini–Hochberg correction (pairwise t-tests). Fine-tune performance on par with GPT-4o (GPT-4o mean Brier=0.196).",
            "calibration_quality": "No explicit calibration-error metric reported. Authors report distributional effects: fine-tuned model produced more extremely accurate forecasts (Brier &lt; 0.05: 32.78% vs base 23.22%) while slightly increasing fraction of very inaccurate forecasts (Brier &gt; 0.5: 8.52% vs base 7.48%), suggesting the fine-tune increases variance/extremity of confidence.",
            "baseline_methods": "Base DeepSeek-R1 14B; Control DPO fine-tune with randomized labels; ignorance benchmark (Brier=0.25); GPT-4o frontier model for reference.",
            "limitations": "Same general limitations as Phi-4: model-generated training data noise, potential information leakage via news within prompts (mitigated by randomized-label control), 4-bit quantization of base model during LoRA fine-tuning, no explicit post-hoc calibration, evaluation restricted to prediction-market binary questions rather than domain-specific scientific discovery forecasting, occasional increase in extreme errors.",
            "probability_examples": "No domain-specific discovery probabilities; illustrative example in ranking: 0.04 vs 0.08 when outcome is 0 is used to demonstrate ranking by absolute error. Prompts required outputs formatted like *0.42* but no concrete scientific-discovery probability instances given.",
            "real_world_future": false,
            "uuid": "e7590.1",
            "source_info": {
                "paper_title": "LLMs Can Teach Themselves to Better Predict the Future",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o (benchmark)",
            "name_full": "GPT-4o",
            "brief_description": "Frontier large model used as a benchmark reference for forecasting accuracy (reported mean Brier score provided); not fine-tuned in this paper but included for comparison.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4o",
            "model_description": "Referenced as a larger frontier model benchmark; exact architecture/size not specified in the paper. Used to contextualize performance of the fine-tuned 14B models.",
            "model_size": "Not specified in paper (frontier model).",
            "probability_estimation_method": "Not described in this paper (used as a benchmark reference).",
            "prediction_target": "Polymarket binary questions (same test set used for comparison).",
            "domain": "General real-world events / prediction markets.",
            "dataset_used": "Not trained/evaluated by authors in this work beyond being run on the same test-set questions for benchmarking; details not supplied.",
            "forecasting_horizon": "Same test-set resolution dates (Dec 25, 2024–Jan 23, 2025) when used for benchmarking.",
            "evaluation_metric": "Brier score (reported mean for GPT-4o = 0.196, SD=0.200, 95% CI [0.188,0.205]).",
            "reported_performance": "Mean Brier = 0.196 (SD=0.200). Authors report no statistically significant difference between fine-tuned 14B models and GPT-4o on this test after multiple-comparison adjustment (p&gt;0.7 for Phi-4; p≈0.931 for DeepSeek fine-tune comparison).",
            "calibration_quality": "Not discussed in detail for GPT-4o in this paper.",
            "baseline_methods": "Serves as a frontier benchmark comparator; not otherwise modified.",
            "limitations": "Paper does not provide GPT-4o method or calibration details; benchmark used only for comparison on mean Brier score.",
            "probability_examples": "None provided for GPT-4o in the text beyond aggregate metric.",
            "real_world_future": false,
            "uuid": "e7590.2",
            "source_info": {
                "paper_title": "LLMs Can Teach Themselves to Better Predict the Future",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From news to forecast: Integrating event analysis in llm-based time series forecasting with reflection.",
            "rating": 2,
            "sanitized_title": "from_news_to_forecast_integrating_event_analysis_in_llmbased_time_series_forecasting_with_reflection"
        },
        {
            "paper_title": "Approaching human-level forecasting with language models.",
            "rating": 2,
            "sanitized_title": "approaching_humanlevel_forecasting_with_language_models"
        },
        {
            "paper_title": "Self-play fine-tuning converts weak language models to strong language models.",
            "rating": 2,
            "sanitized_title": "selfplay_finetuning_converts_weak_language_models_to_strong_language_models"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Wisdom of the silicon crowd: Llm ensemble prediction capabilities rival human crowd accuracy.",
            "rating": 1,
            "sanitized_title": "wisdom_of_the_silicon_crowd_llm_ensemble_prediction_capabilities_rival_human_crowd_accuracy"
        },
        {
            "paper_title": "Forecastbench: A dynamic benchmark of ai forecasting capabilities.",
            "rating": 1,
            "sanitized_title": "forecastbench_a_dynamic_benchmark_of_ai_forecasting_capabilities"
        }
    ],
    "cost": 0.011203749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs Can Teach Themselves to Better Predict the Future</p>
<p>Benjamin Turtel 
Lightning Rod Labs</p>
<p>Danny Franklin 
Lightning Rod Labs</p>
<p>Philipp Schoenegger 
London School of Economics and Political Science</p>
<p>LLMs Can Teach Themselves to Better Predict the Future
9650D0F1277C948F442FEF6B18FDBADA
We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples.Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date.We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO).On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7-10% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of areas, often approaching or exceeding human performance.One area where human performance has not yet been surpassed is judgemental forecasting [1], where probabilistic forecasts are assigned to future events.Successful forecasts by top-performing human forecasters include substantial reasoning about facts of the world, various trends, and competing pieces of evidence [2], making it a great place to study model reasoning capabilities in a messy real-world environment.</p>
<p>Moreover, forecasting is a central task in decision-making across sectors as diverse as finance, policy, and law.It is central to inform resource allocation, manage risks, and plan organizational decisions.Modern LLMs have already been shown to conduct financial analysis [3], evaluate the impact of events on time series [4], and improve climate policy decision-making [5].This makes improving LLMs' forecasting abilities potentially impactful and wide-ranging.</p>
<p>DPO Training Reranking Responses by Accuracy</p>
<p>Fine-tuned LLM</p>
<p>Notes: This chart outlines the stages of our method.</p>
<p>There has been some work explicitly looking to apply and boost the forecasting capabilities of LLMs.Such work has relied on aggregation [6], retrieval of news as well as fine-tuning [7], and ranked-based context retrieval [8], among other approaches [9].While most of these systems improve performance to varying degrees, many share a common methodological limitation: They are frequently reliant on human-curated data such as up-to-date crowd forecasts or output curation, and often fail to have the models learn from resolved outcomes.Human outputs are slow and costly to procure, making it difficult to have models continually learn from them and improve.</p>
<p>In this paper, we propose a new approach to improving LLM forecasting performance that sidesteps the use of human inputs above and beyond real-world resolutions and enables the model to directly learn from 1 arXiv:2502.05253v1[cs.CL] 7 Feb 2025 actual outcomes and self-play.Self-play, where models compete against themselves, has previously been used in AlphaGo Zero to achieve superhuman performance [10], as well as more recent fine-tuning approaches like Self-Play fIne-tuNing (SPIN) [11].By allowing the model to produce reasonings and forecasts by itself on a large number of forecasting questions, this provides us with a large data set that we can then use for further training.As such, we do not rely on human-written forecasting rationales or predictions and instead only use model-generated reasoning, making this straightforwardly scalable.Further, our approach uses Direct Preference Optimisation (DPO) [12], a reward-free method entirely bypassing the need for a separate reward model, to instead learn a reward signal from sets of ranked reasoning pairs [13] drawn from the self-play outputs.This allows DPO to capitalize on relative rankings between forecasts, enabling the model to learn from the entire set of generated samples without the need for manual curation.Even when forecasts are individually suboptimal, DPO trains the model to discern subtle differences in quality and systematically correct biases through pairwise comparisons.By contrast, Supervised Fine-Tuning (SFT) relies on human-curated examples and treats selected forecasts as fully correct, which can lead to the discarding of potentially valuable information; DPO overcomes this limitation by learning from all samples, thereby enhancing the robustness and efficiency of the fine-tuning process.</p>
<p>Our work follows up on recent advances made by DeepSeek's release of R1 [14], which demonstrated the power of reinforcement learning in deterministic contexts like mathematics and coding.We move the focus to real-world forecasting, which is inherently noisy and relies on calibrated predictions rather than simple binary correctness.This requires our models to learn from noisy probabilistic outcomes, which, if successful, promises widespread applicability.</p>
<p>To achieve this, we draw on a large dataset of resolved prediction market questions from Polymarket, where the model-restricted to a historical cutoff date-generates multiple reasoning traces and probabilistic forecasts through self-play.We then rank these pairs of rationales based on their proximity to the resolved outcome (for instance, ranking a 5% prediction higher than a 10% prediction if the event resolved to "No") before fine-tuning our model on them and testing the model on a separate test set.This ensures that the model does not simply learn whether a forecast predicted that an event would or would not occur, but instead enables it to draw directly from the full set of forecasts needed for a well-calibrated forecasting model (see Figure 1).</p>
<p>Our results on a temporally held-out test set of questions resolving after December 25, 2024 show that for both of the models that we employed our method on, Phi-4 14B [15] and DeepSeek-R1 14B [14], we find accuracy improvements of between 7-10% over the base versions of these models as well as the same models fine-tuned with randomized outcome labels as a control, see Figure 2. Comparing not only to the base model but also to randomized-label fine-tuned controls allows us to more rigorously tease out the effect of outcome-based learning versus exposure to additional information.This shows that our method improves model forecasting performance, underscoring the potential of drawing on self-play reasoning data in improving probabilistic reasoning and prediction accuracy.Strikingly, our fine-tune of both models are also on par with the performance of the much larger GPT-4o [16].</p>
<p>Method</p>
<p>Our approach consists of six main steps: 2.1) Collection and preprocessing of forecasting data, 2.2) News collection, 2.3) Synthetic training data generation through base model self-play, 2.4) Resolution-driven re-ranking, 2.5) Direct Preference Optimization (DPO) fine-tuning, and 2.6) Forecasting test-set questions.</p>
<p>For this pipeline, we used two models for self-play and for the final forecasting process: Phi-4 [15] and DeepSeek-R1-Distill-Qwen-14B [14].Both models are small (at 14B parameters) but have shown strong performance on general science and coding benchmarks, sometimes rivalling GPT-4o [17,18,14].DeepSeek-R1-Distill-Qwen-14B is a distilled model derived from Gwen2.5-14B [19] fine-tuned with the reasoning patterns from DeepSeek-R1 [14].Throughout this paper, we refer to these models as Phi-4 14B and DeepSeek-R1 14B respectively.</p>
<p>Data</p>
<p>We collected a total of 12,100 binary outcome forecasting questions from the prediction market Polymarket.We excluded all outcomes with ambiguous resolutions and partitioned the data as follows: our training set included 9,800 questions that all resolved between July 1 and December 15, 2024, and our test set included 2,300 questions that all resolved between December 25, 2024 and January 23, 2025.We also collected the final outcomes for all questions, recording as '0' all outcomes that did not happen and as '1' all outcomes that did happen.See Table 1 for example questions.  1 shows a set of questions in the test set as well as their outcomes, with 0 indicating a negative resolution and 1 a positive resolution.</p>
<p>To evaluate the accuracy of our probabilistic forecasts in this paper, we calculate Brier scores.For each forecasting question with a predicted probability p i and an actual outcome o i ∈ {0, 1}, the Brier score is defined as
BS = 1 N (p i − o i ) 2 ,
where N is the total number of forecasting questions.A lower Brier score indicates higher forecasting accuracy.</p>
<p>News Collection</p>
<p>We collected news via the NewsCatcher API 14 days prior to question resolution.Our approach was drawn from [7] in that we generated search queries via GPT-4o and then integrated external news retrieval services like Newscatcher to aggregate and process the output.These news articles were then used as further input in Sections 2.3 and 2.6.</p>
<p>Model Self-Play Data Generation</p>
<p>We then instructed the base models to provide reasoning and a final probabilistic forecast for each question.For Phi-4 14B, we employed a scratchpad prompt [20], while we used a zero-shot prompt for DeepSeek-R1 14B as <think> tags are already present in the model output generation.The prompt included a summary of news from Section 2.2 along with the appropriate scratchpad or zero-shot prompt depending on the model.We ran all queries with a temperature of 1.In total, we generated a pair of reasoning traces for each question [21].We first generated a single reasoning and then re-ran this process up to four times to arrive at a second, non-identical forecast.If all subsequent predictions were identical, we removed the full set of forecasts.Overall, we obtained 18,854 reasoning traces for the 9,427 forecasting questions that had non-constant forecasts.</p>
<p>Resolution-Driven Re-Ranking</p>
<p>For each question, we paired up reasoning-outcome pairs and ranked them based on the proximity of the probabilistic forecast (ranging from 0% to 100%) to the ground truth (0 or 1).Formally, for each question with ground truth o ∈ {0, 1}, let the probabilistic forecasts from two reasoning traces be denoted by p 1 and p 2 (with p i ∈ [0, 1]).We then define a ranking metric as
r(p, o) = |p − o|,
which measures the absolute difference between the forecast and the actual outcome.For example, if a pair consists of reasonings with 4% and 8% predictions respectively -i.e.p 1 = 0.04 and p 2 = 0.08 -with a ground truth of 0, then r(0.04, 0) = 0.04 and r(0.08, 0) = 0.08.Since 0.04 &lt; 0.08, the reasoning trace resulting in the 4% prediction is ranked above that of the reasoning resulting in the 8% forecast.Notably, the squared error metric of the Brier score naturally mitigates overconfidence by penalizing large deviations more heavily.Pairs that resulted in identical forecasts (i.e.p 1 = p 2 ) were removed prior to this stage.In total, we used the full set of 18,854 reasoning traces for the 9,427 forecasting questions for our re-ranking.</p>
<p>Moreover, to control for the possibility that information provided via the news aggregation at this step might influence the rankings, we also fine-tuned a second set of models via the same process, but with the ranking of labels randomised.These control models allow us to test whether the learning is attributable to the models learning from the higher-accuracy forecasting rationales.</p>
<p>Direct Preference Optimization Fine-Tuning</p>
<p>We then fine-tuned Phi-4 14B and DeepSeek-R1 14B using the preference pairs from Section 2.3.We use Direct Preference Optimization (DPO) to optimise model outputs against self-play derived and outcome-driven preferences without the need to train a separate reward model.The DPO loss was minimised using a LoRA adapter (rank=16, alpha=32, dropout=0.05,target_modules="all-linear", no bias) on top of the base model, which was held in 4-bit quantisation, using a batch size of 2 (with 4 gradient accumulation steps) and gradient checkpointing enabled.Training leveraged the AdamW optimiser with a linear learning rate scheduler (5e-5 base rate), beta=0.1, and BF16 mixed precision.We used 8 H100 GPUs for training.For Phi-4 14B, we found a plateau at the fifth epoch, while this occurred at the fourth epoch for DeepSeek-R1 14B (see Figure 3).</p>
<p>Forecasting Test Set Questions</p>
<p>Finally, we test every model against a held-out test set of 2300 questions.Importantly, this test set begins 10 days after the final outcome in the training set, so our fine-tuned models have not been exposed to any news that might inform outcomes in the test set.</p>
<p>We do this with three versions of each model: the original base model, the fine-tuned model with correct outcomes for DPO ranking, and a control fine-tuned model with randomized outcomes for DPO ranking.This allows us to distinguish between learning that happened due to exposure to new information (for example, the news articles shared in prompts) versus learning by optimising for reasoning processes that lead to more accurate forecasts.</p>
<p>To generate our final forecasts, we used the following prompts shown in Figure 4, derived from Halawi et al. [7].Our prompts drew on expert persona prompting [22], based on structured analytic techniques [23] and Tetlock-style superforecasting [2], as well as more structured instructions, aiming to improve forecasting accuracy over a naïve assistant prompt.1.Given the above question, rephrase and expand it to help you do better answering.Maintain all information in the original question.Insert rephrased and expanded question.2. Using your knowledge of the world and topic, as well as the information provided, provide a few reasons why the answer might be no.Rate the strength of each reason.Insert your thoughts 3. Using your knowledge of the world and topic, as well as the information provided, provide a few reasons why the answer might be yes.Rate the strength of each reason.Insert your thoughts 4. Aggregate your considerations.Think like a superforecaster (e.g.Nate Silver).Insert your aggregated considerations 5. Output an initial probability (prediction) given steps 1-4.Insert initial probability.6. Evaluate whether your calculated probability is excessively confident or not confident enough.Also, consider anything else that might affect the forecast that you did not before consider (e.g.base rate of the event).Insert your thoughts 7. Output your final prediction (a number between 0 and 1) with an asterisk at the beginning and end of the decimal.</p>
<p>Insert your answer</p>
<p>DeepSeek R1 14B:</p>
<p>You are an expert superforecaster, familiar with Structured Analytic Techniques as well as Superforecasting by Philip Tetlock and related work.Predict the probability that the following question will be resolved as true/yes.You MUST give a probability estimate between 0 and 1 UNDER ALL CIRCUMSTANCES.</p>
<p>[Question, Question Background, Resolution Criteria, Today's/Question Close Date, and News Summaries] Output your final prediction (a number between 0 and 1) with an asterisk at the beginning and end of the decimal (Ex: <em><probability></em>).</p>
<p>Insert your answer</p>
<p>Both models were provided with the question, the question background, resolution criteria, the current date, the date when the forecasting question closes, and a summary of up to 10 news articles.We then collected forecasts for each model on the entire test set of 2300 questions.All models provided valid forecasts on all questions.</p>
<p>Results</p>
<p>For all results below, we call the fine-tuned model 'Fine-Tune', the base model 'Base', and the fine-tuned model with randomized labels the 'Control'.We find substantial improvements in forecasting accuracy for both Phi-4 14B and DeepSeek-R1 14B fine-tunes, heavily outperforming the ignorance benchmark of a Brier score of 0.25 (arrived at by predicting 50% on every question) and improving upon the base and control models (see Figure 5).We conduct independent samples t-tests between the fine-tuned versions of the models and both the base and control models, as well as the frontier model benchmark set by GPT-4o.We find that for both Phi-4 14B and DeepSeek-R1 14B, the fine-tuned model is statistically significantly more accurate than both the base and control models at p &lt; 0.05.This also holds after adjusting the p-values for multiple comparisons via the Benjamini-Hochberg procedure [24].This suggests that our method is able to robustly and consistently improve forecasting performance, and that this performance increase is not due to the additional information that fine-tuning on the reasoning traces brings.However, we fail to observe statistically significant differences between the fine-tuned models and the frontier model benchmark set by GPT-4o, p &gt; 0.7 for both after adjustment.The fact that GPT-4o does not outperform our small fine-tuning models shows that our method was effective in producing forecasting performance on par with much larger frontier models.Our usage of 4-bit quantization, which typically leads to small-to-medium performance reductions [25,26], further shows that our results are competitive even under these constraints.Comparing the distributions of accuracy scores across the questions for DeepSeek-R1 14B, we find that the fine-tuned model had a Brier score above 0.5 (very low accuracy) on 8.52% of questions, slightly higher than the base (7.48%) and control (7.61%) models.However, it also had a Brier score below 0.05 (very high accuracy) on 32.78% of questions, compared to only 23.22% and 23.13% for the base and control models.This indicates that while the fine-tuned model occasionally makes slightly more highly inaccurate forecasts, it produces far more extremely accurate ones, more than compensating for the small uptick in large errors.We replicate this pattern at a similar magnitude for Phi-4 14B, where the fine-tuned model has 8.87% of forecasts above 0.5 but 35.7% below 0.05, compared to 7.26% and 21% for the base model and 6.43% and 20.39% for the control model, respectively.</p>
<p>Conclusion</p>
<p>Large language models can enhance their forecasting capabilities through self-play, generating reasoning traces that enable outcome-based fine-tuning without relying on human-curated data.By pairing these traces and ranking them by their proximity to actual outcomes, the models learn to refine their probabilistic forecasts, outperforming base models and matching the performance of larger frontier models.</p>
<p>Figure 1
1
Figure 1: Overview Flowchart</p>
<p>Figure 2 :
2
Figure 2: Accuracy Results for all Models</p>
<p>Figure 3 :
3
Figure 3: Per-Epoch Accuracy.</p>
<p>Brier Scores for Fine-tuning Phi-4 14B Fine-tune DeepSeek-R1 14B Fine-tune Notes: This plot shows the per-epoch accuracy results for both Phi-4 14B and DeepSeek-R1 14B.</p>
<p>Figure 4 :
4
Figure 4: Forecasting Prompts by Model</p>
<p>Figure 5 :
5
Figure 5: Ridge Plot of Forecasting Accuracy for each Model.</p>
<p>1
1</p>
<p>Table 1 :
1
Example Questions with Outcomes
Question</p>
<p>Table 2 :
2
Results -Descriptive Statistics Descriptive statistics for each model, including mean Brier scores, standard deviation, standard error of the mean, and 95% confidence intervals.The sample size for all models is 2300 questions.
ModelMean Brier ScoreSDSEM95% CIPhi-4 14BFine-Tune0.2000.218 0.005 [0.191, 0.209]Control0.2140.186 0.004 [0.206, 0.221]Base0.2210.189 0.004 [0.214, 0.229]DeepSeek-R1 14BFine-Tune0.1970.218 0.005 [0.188, 0.206]Control0.2120.202 0.004 [0.204, 0.220]Base0.2120.201 0.004 [0.204, 0.220]Frontier BenchmarkGPT-4o0.1960.200 0.004 [0.188, 0.205]Notes:</p>
<p>Table 3 :
3
Pairwise Comparisons with Adjusted p-values The table shows p-values of independent samples t-tests.Adjustment of p-values is done via the Benjamini-Hochberg correction.
Model 1Model 2p-value Adj. p-valueDeepSeek-R1 14B Fine-tune DeepSeek-R1 14B Base0.0150.027DeepSeek-R1 14B Fine-tune DeepSeek-R1 14B Control0.0170.027DeepSeek-R1 14B Fine-tune GPT-4o0.9310.931Phi-4 14B Fine-tunePhi-4 14B Base0.0000.002Phi-4 14B Fine-tunePhi-4 14B Control0.0180.027Phi-4 14B Fine-tuneGPT-4o0.5890.706Notes:
We hypothesise that one reason why the Phi-4 14B control model improves over the base, whereas there is no such effect for DeepSeek-R1 14B, is that it is likely to learn significantly more from the news articles-even with randomised labels-because it has a much earlier knowledge cut-off than DeepSeek-R1 14B.</p>
<p>Forecastbench: A dynamic benchmark of ai forecasting capabilities. E Karger, H Bastani, C Yueh-Han, Z Jacobs, D Halawi, F Zhang, P E Tetlock, arXiv:2409.198392024arXiv preprint</p>
<p>Superforecasting: The art and science of prediction. P E Tetlock, D Gardner, 2016Random House</p>
<p>Financial statement analysis with large language models. A Kim, M Muhn, V Nikolaev, arXiv:2407.178662024arXiv preprint</p>
<p>From news to forecast: Integrating event analysis in llm-based time series forecasting with reflection. X Wang, M Feng, J Qiu, J Gu, J Zhao, arXiv:2409.175152024arXiv preprint</p>
<p>Llm-assisted modeling and simulations for public sector decision-making: Bridging climate data and policy insights. C Cao, J Zhuang, Q He, AAAI-2024 Workshop on Public Sector LLMs: Algorithmic and Sociotechnical Design. 2024</p>
<p>Wisdom of the silicon crowd: Llm ensemble prediction capabilities rival human crowd accuracy. P Schoenegger, I Tuminauskaite, P S Park, P E Tetlock, arXiv:2402.193792024arXiv preprint</p>
<p>Approaching human-level forecasting with language models. D Halawi, F Zhang, C Yueh-Han, J Steinhardt, arXiv:2402.185632024arXiv preprint</p>
<p>Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval. Q Yan, R Seraj, J He, L Meng, T Sylvain, arXiv:2310.018802023arXiv preprint</p>
<p>Q Lyu, K Shridhar, C Malaviya, L Zhang, Y Elazar, N Tandon, C Callison-Burch, arXiv:2402.13904Calibrating large language models with sample consistency. 2024arXiv preprint</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, D Hassabis, arXiv:1712.018152017arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Z Chen, Y Deng, H Yuan, K Ji, Q Gu, arXiv:2401.013352024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>S Xu, W Fu, J Gao, W Ye, W Liu, Z Mei, Y Wu, arXiv:2404.10719Is dpo superior to ppo for llm alignment? a comprehensive study. 2024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Y He, arXiv:2501.129482025arXiv preprint</p>
<p>M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, Y Zhang, arXiv:2412.08905Phi-4 technical report. 2024arXiv preprint</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, I Kivlichan, arXiv:2410.21276Gpt-4o system card, 2024. arXiv preprint</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, S R Bowman, arXiv:2311.120222023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, J Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, Z Qiu, arXiv:2412.15115Qwen2.5 technical report. 2024arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, A Odena, arXiv:2112.001142021arXiv preprint</p>
<p>Nash learning from human feedback. R Munos, M Valko, D Calandriello, M G Azar, M Rowland, Z D Guo, Y Tang, M Geist, T Mesnard, C Fiegel, A Michi, M Selvi, S Girgin, N Momchev, O Bachem, D J Mankowitz, D Precup, B Piot, arXiv:2312.008862023arXiv preprint</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. B Xu, A Yang, J Lin, Q Wang, C Zhou, Y Zhang, Z Mao, arXiv:2305.146882023arXiv preprint</p>
<p>Structured analytic techniques for intelligence analysis. R H Pherson, R J Heuer, 2019Cq Press</p>
<p>Controlling the false discovery rate: a practical and powerful approach to multiple testing. Y Benjamini, Y Hochberg, Journal of the Royal Statistical Society: Series B (Methodological). 5711995</p>
<p>exploring-the-impact-of-quantization-on-llm-performance-5698e16c5564. O Zem, Ac- cessed: 2024-01-03January 3 2024Exploring the impact of quantization on llm performance</p>
<p>An empirical study of llama3 quantization: From llms to mllms. W Huang, X Zheng, X Ma, H Qin, C Lv, H Chen, M Magno, Visual Intelligence. 21362024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1714 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1714</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1714</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-258212957</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.09448v1.pdf" target="_blank">EC2: Emergent Communication for Embodied Control</a></p>
                <p><strong>Paper Abstract:</strong> Embodied control requires agents to leverage multimodal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised “language” of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, which is then used to finetune a lightweight policy network for downstream control. Through extensive experiments in Metaworld and Franka Kitchen embodied benchmarks, EC2 is shown to consistently outperform previous contrastive learning methods for both videos and texts as task inputs. Further ablations confirm the importance of the emergent language, which is beneficial for both video and language learning, and significantly superior to using pre-trained video captions. We also present a quantitative and qualitative analysis of the emergent language and discuss future directions toward better understanding and leveraging emergent communication in embodied tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1714.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1714.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EC2_LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent Communication for Embodied Control — pre-trained language model (GPT-like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-like language model pre-trained in this paper to extract embodied representations by predicting masked latent trajectory segments conditioned on either emergent discrete messages (learned by a speaker via an emergent communication game) or human natural-language instructions; frozen and transferred to few-shot 3D manipulation tasks by mapping its output features to actions with a small MLP policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>EC2 pre-trained language model (GPT-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A GPT-like autoregressive transformer language model that (1) accepts as prompt either emergent-language token sequences (discrete messages produced by a speaker network) or natural-language BPE-token embeddings, (2) takes a latent masked trajectory context as input, and (3) is trained to regress/predict the masked latent trajectory segments (trajectory completion). The same LM participates in emergent-language generation updates and in the trajectory-completion pretext task; it is frozen at downstream policy learning time and its output features are mapped by a small MLP to continuous robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>natural language instructions (crowd-sourced) + emergent discrete messages generated from demonstration videos (unsupervised emergent communication)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained on the LOReL dataset of robot demonstrations (Franka Panda over IKEA desk): 3000 episodes (≈150,000 frames) with 6000 human text annotations (two per episode) comprising 1699 unique natural-language instructions; emergent-language messages were generated for each demonstration via a speaker-listener referential game (Gumbel-Softmax discrete tokens). Pretext task: latent trajectory completion — crop a random segment of a latent trajectory produced by an encoder and train the LM to predict that segment conditioned on the remaining latent context plus either emergent or natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>MetaWorld and Franka Kitchen benchmark manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Standard 3D simulated robotic manipulation benchmarks. MetaWorld tasks include ring-on-peg assembly, pick-and-place between bins, pushing a button, opening a drawer, hammering a nail (Sawyer robot). Franka Kitchen tasks include sliding/opening doors, turning on a light, turning a stove knob, opening a microwave (Franka robot). Observations: RGB image views (three camera viewpoints evaluated) plus proprioceptive data (end-effector pose and joint positions). Downstream objective: behavior-cloning imitation of expert demonstrations conditioned on either a natural-language instruction or a demonstration video (the latter converted to emergent language prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Discrete token sequences: emergent-language tokens (vocabulary V, length T) or BPE-tokenized natural language instructions — i.e., symbolic/high-level instruction tokens rather than motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous motor-control actions for manipulation: low-level continuous actions representing robot controls (joint actions / end-effector commands); actions are predicted by the downstream policy MLP and trained by imitation (regression / log-prob objective for continuous action distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Frozen LM outputs per-timestep embodied features m_t (feature vector). A small task-specific MLP policy (two hidden layers of size 256 with BatchNorm) maps LM features to action distribution; behavior cloning uses expert actions as labels. The LM is not fine-tuned during downstream adaptation (frozen feature extractor).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB vision (image observations) plus proprioceptive signals (end-effector pose and joint positions); visual encodings concatenated with proprioception are encoded into latent trajectories used by the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>MetaWorld (10 demos): language-instruction success 72.4 ± 2.4%; video-instruction (emergent-language prompt) 76.0 ± 2.1%. MetaWorld (25 demos): language 82.6 ± 2.0%; video 85.0 ± 2.2%. Franka Kitchen (10 demos): language 48.4 ± 2.5%; video 53.6 ± 2.5%. Franka Kitchen (25 demos): language 59.8 ± 2.5%; video 63.2 ± 2.6%. Across all evaluated tasks with 10 demos EC2 reports an overall ≈60.4% success rate (paper-stated aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Learning-from-scratch baseline is reported to perform poorly in the low-data regime but exact numbers are not provided in the paper; other pretrained baselines (R3M, BC-Z) scores are reported for comparison (see other results entries).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Downstream adaptation used very small demonstration sets: experiments run with {5, 10, 25} demos; key reported results at 10 and 25 demos (20k optimization steps of the task-specific MLP, batch size 32). Notably, EC2 achieves the quoted success rates using only 10 expert demos in the new environments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Paper states training-from-scratch struggles with small numbers of demos; no precise sample counts to reach thresholds are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Enables effective few-shot imitation: achieves strong task success with only 10 expert demos where learning-from-scratch fails; compared to R3M baseline, EC2 improves aggregate success (paper quotes ≈60.4% vs ≈55.5% overall at 10 demos), i.e., a several-percentage-point absolute improvement in success at the same low demo budgets rather than a simple multiplicative sample-reduction factor.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>1) Use of emergent language that preserves fine-grained perceptual and motion details from videos while providing a structured discrete signal; 2) joint pretraining on emergent and natural language via the trajectory-completion objective that links video semantics and compositional natural language; 3) frozen LM as a robust feature extractor enabling few-shot mapping by a small MLP; 4) LOReL dataset contains paired demonstrations and human instructions so natural language signals present during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>1) Pretraining dataset scale and domain mismatch may limit generalization to wholly different settings; 2) emergent-language interpretability and coverage — emergent tokens may not perfectly align with all necessary action semantics; 3) freezing the LM prevents downstream adaptation of representations to new task-specific nuances (a design choice here); 4) pretraining without explicit action labels relies on trajectory-completion proxy tasks which may omit some control-relevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining a GPT-like LM on a trajectory-completion task conditioned on both emergent discrete messages (learned from videos) and natural-language instructions produces embodied representations that transfer effectively to 3D manipulation; emergent language acts as a bridge that preserves low-level video details while maintaining linguistic structure, improving few-shot policy learning (notably at 10 demo regime) over contrastive video-language alignment baselines. Joint emergent+natural-language pretraining yields better downstream performance than either alone, and larger LM sizes further increase correlation between emergent and natural language and improve control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EC2: Emergent Communication for Embodied Control', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1714.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1714.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R3M: A universal visual representation for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained visual representation learned on paired human video–language data using temporal contrastive objectives and video–language alignment, used as a frozen encoder for downstream robot manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R3m: A universal visual representation for robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>R3M pre-trained vision-language representation</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A representation model pre-trained on diverse human video–language corpora using time-contrastive learning (encouraging temporally nearby frames to have similar embeddings) together with video–language alignment objectives; the released language encoder or vision encoder is used as a frozen feature extractor for downstream policy learning in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Paired human video–language data (diverse human demonstrations with captions/instruction text) — multimodal video+text pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper cites and uses the officially released R3M models (pretrained on diverse human video-language data as described in R3M). The EC2 paper does not enumerate R3M's exact pretraining datasets or sizes; it refers to R3M's published pretraining on diverse human videos paired with text.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>MetaWorld and Franka Kitchen benchmark manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same MetaWorld and Franka Kitchen manipulation suites as used in EC2 experiments (multiple manipulation tasks, image observations + proprioceptive data), evaluated with few-shot imitation using small demo sets (5/10/25).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions / captions (BPE-tokenized text used by the R3M language encoder when performing instruction-conditioned evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous motor-control actions for robot manipulators (joint-level / end-effector controls) in the simulated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>R3M encoders (language encoder for instruction-conditioned runs; vision encoder for video-conditioned runs) produce embeddings which are fed into a small downstream MLP policy (two-layer MLP) that is trained via behavior cloning to regress expert actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB image observations (three camera viewpoints evaluated) plus proprioceptive data (end-effector pose and joint positions) concatenated with visual encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>MetaWorld: 10 demos — language 69.2 ± 2.0%; video 71.8 ± 1.8%. MetaWorld: 25 demos — language 77.4 ± 2.4%; video 79.0 ± 2.1%. Franka Kitchen: 10 demos — language 41.8 ± 2.5%; video 45.7 ± 2.4%. Franka Kitchen: 25 demos — language 56.0 ± 2.3%; video 58.7 ± 2.0%. (Reported in paper's baseline table.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Learning-from-scratch / non-pretrained policies are reported to perform poorly in the low-demo regime; exact numeric scratch baselines are not presented in the table in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Evaluations performed with small demonstration budgets {5,10,25}; reported scores above correspond to 10 and 25 demo settings. Downstream policy MLP trained for 20k steps in each run.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not specified numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>R3M's pretraining enables nontrivial success with only 10–25 demos (e.g., MetaWorld ≈69–79% depending on demo size), demonstrating improved sample efficiency versus unspecified-from-scratch baselines; EC2 shows further improvement over R3M by several percentage points at same demo budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Temporal contrastive learning captures dynamic state transitions; video-language alignment helps encode semantically relevant features for instruction-conditioned behavior; use of large, diverse human video-language pretraining corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Forcing tight video-language alignment can discard low-level motion and perceptual details necessary for fine-grained control (paper hypothesis). Domain gap between human demonstration videos and simulated robot environments may reduce transfer fidelity. Natural language captions can be too abstract and miss low-level motion descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on paired human video–language data (R3M) yields useful frozen representations that significantly improve few-shot imitation performance in 3D manipulation, but methods that explicitly align video and text embeddings may lose fine-grained control-critical details that EC2 recovers via emergent-language conditioning and trajectory-completion pretext tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EC2: Emergent Communication for Embodied Control', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1714.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1714.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BC-Z</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BC-Z: Zero-shot task generalization with robotic imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task imitation learning system that jointly learns alignment between video representations and language representations together with policy learning across tasks; used here as a baseline for few-shot, instruction-conditioned manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bc-z: Zero-shot task generalization with robotic imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BC-Z multi-task alignment & policy model</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multi-task imitation learning framework that jointly learns to align video and language representations and to produce policies across many tasks; in this paper BC-Z is used with its official implementation as a baseline, encoding instructions or videos and mapping embeddings to actions via task-specific policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Demonstration videos and task information (multi-task imitation data), including language and video annotations used during BC-Z's original multi-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper uses BC-Z's publicly released model/implementation for comparison; the EC2 paper does not re-describe BC-Z's full pretraining corpus or sizes (BC-Z original paper provides those details).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>MetaWorld and Franka Kitchen benchmark manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same 3D simulated manipulation suites as used for EC2 evaluation, evaluated in few-shot imitation regimes with small demo counts.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language instructions (text) and demonstration-video encodings depending on experiment condition.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous robot control actions (joint/end-effector commands) used in the simulated manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>BC-Z jointly learns representation alignment and policy; in this evaluation the model's embeddings are fed into a downstream policy network (MLP) trained via behavior cloning to reproduce expert actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images and proprioceptive signals (end-effector pose, joint positions) concatenated with vision encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>MetaWorld: 10 demos — language 63.4 ± 2.1%; video 64.8 ± 2.0%. MetaWorld: 25 demos — language 74.1 ± 2.2%; video 75.2 ± 1.0%. Franka Kitchen: 10 demos — language 34.2 ± 2.3%; video 37.5 ± 2.2%. Franka Kitchen: 25 demos — language 38.6 ± 2.4%; video 40.0 ± 2.0%. (Reported in the paper's baseline table.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>No exact-from-scratch numbers given in this paper; the authors state learning-from-scratch struggles in the small-demo regime.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Evaluated with {5,10,25} demo budgets; reported stats correspond to 10 and 25 demo settings; downstream MLP trained 20k iterations as per EC2 evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not numerically specified in the EC2 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>BC-Z pretraining yields nontrivial success in few-shot regimes (e.g., MetaWorld ≈63–75% depending on demos), but EC2 generally outperforms BC-Z by several absolute percentage points at the same small demo budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Joint multi-task training with language and video modalities enables generalization across tasks; explicit alignment between modalities can help instruction-conditioned policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Joint alignment objectives may omit low-level motion details present in raw videos; limited expressiveness of captions and alignment could limit fine-grained control transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BC-Z's joint multi-task video-language+policy pretraining yields useful priors for few-shot imitation but is outperformed by EC2 which avoids forcing strict video-language alignment and instead uses emergent discrete messages plus trajectory completion to retain fine-grained control-relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EC2: Emergent Communication for Embodied Control', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>R3m: A universal visual representation for robot manipulation. <em>(Rating: 2)</em></li>
                <li>Bc-z: Zero-shot task generalization with robotic imitation learning. <em>(Rating: 2)</em></li>
                <li>Linking emergent and natural languages via corpus transfer. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1714",
    "paper_id": "paper-258212957",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "EC2_LM",
            "name_full": "Emergent Communication for Embodied Control — pre-trained language model (GPT-like)",
            "brief_description": "A GPT-like language model pre-trained in this paper to extract embodied representations by predicting masked latent trajectory segments conditioned on either emergent discrete messages (learned by a speaker via an emergent communication game) or human natural-language instructions; frozen and transferred to few-shot 3D manipulation tasks by mapping its output features to actions with a small MLP policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "EC2 pre-trained language model (GPT-like)",
            "model_agent_description": "A GPT-like autoregressive transformer language model that (1) accepts as prompt either emergent-language token sequences (discrete messages produced by a speaker network) or natural-language BPE-token embeddings, (2) takes a latent masked trajectory context as input, and (3) is trained to regress/predict the masked latent trajectory segments (trajectory completion). The same LM participates in emergent-language generation updates and in the trajectory-completion pretext task; it is frozen at downstream policy learning time and its output features are mapped by a small MLP to continuous robot actions.",
            "pretraining_data_type": "natural language instructions (crowd-sourced) + emergent discrete messages generated from demonstration videos (unsupervised emergent communication)",
            "pretraining_data_details": "Pretrained on the LOReL dataset of robot demonstrations (Franka Panda over IKEA desk): 3000 episodes (≈150,000 frames) with 6000 human text annotations (two per episode) comprising 1699 unique natural-language instructions; emergent-language messages were generated for each demonstration via a speaker-listener referential game (Gumbel-Softmax discrete tokens). Pretext task: latent trajectory completion — crop a random segment of a latent trajectory produced by an encoder and train the LM to predict that segment conditioned on the remaining latent context plus either emergent or natural language prompts.",
            "embodied_task_name": "MetaWorld and Franka Kitchen benchmark manipulation tasks",
            "embodied_task_description": "Standard 3D simulated robotic manipulation benchmarks. MetaWorld tasks include ring-on-peg assembly, pick-and-place between bins, pushing a button, opening a drawer, hammering a nail (Sawyer robot). Franka Kitchen tasks include sliding/opening doors, turning on a light, turning a stove knob, opening a microwave (Franka robot). Observations: RGB image views (three camera viewpoints evaluated) plus proprioceptive data (end-effector pose and joint positions). Downstream objective: behavior-cloning imitation of expert demonstrations conditioned on either a natural-language instruction or a demonstration video (the latter converted to emergent language prompt).",
            "action_space_text": "Discrete token sequences: emergent-language tokens (vocabulary V, length T) or BPE-tokenized natural language instructions — i.e., symbolic/high-level instruction tokens rather than motor commands.",
            "action_space_embodied": "Continuous motor-control actions for manipulation: low-level continuous actions representing robot controls (joint actions / end-effector commands); actions are predicted by the downstream policy MLP and trained by imitation (regression / log-prob objective for continuous action distribution).",
            "action_mapping_method": "Frozen LM outputs per-timestep embodied features m_t (feature vector). A small task-specific MLP policy (two hidden layers of size 256 with BatchNorm) maps LM features to action distribution; behavior cloning uses expert actions as labels. The LM is not fine-tuned during downstream adaptation (frozen feature extractor).",
            "perception_requirements": "RGB vision (image observations) plus proprioceptive signals (end-effector pose and joint positions); visual encodings concatenated with proprioception are encoded into latent trajectories used by the LM.",
            "transfer_successful": true,
            "performance_with_pretraining": "MetaWorld (10 demos): language-instruction success 72.4 ± 2.4%; video-instruction (emergent-language prompt) 76.0 ± 2.1%. MetaWorld (25 demos): language 82.6 ± 2.0%; video 85.0 ± 2.2%. Franka Kitchen (10 demos): language 48.4 ± 2.5%; video 53.6 ± 2.5%. Franka Kitchen (25 demos): language 59.8 ± 2.5%; video 63.2 ± 2.6%. Across all evaluated tasks with 10 demos EC2 reports an overall ≈60.4% success rate (paper-stated aggregate).",
            "performance_without_pretraining": "Learning-from-scratch baseline is reported to perform poorly in the low-data regime but exact numbers are not provided in the paper; other pretrained baselines (R3M, BC-Z) scores are reported for comparison (see other results entries).",
            "sample_complexity_with_pretraining": "Downstream adaptation used very small demonstration sets: experiments run with {5, 10, 25} demos; key reported results at 10 and 25 demos (20k optimization steps of the task-specific MLP, batch size 32). Notably, EC2 achieves the quoted success rates using only 10 expert demos in the new environments.",
            "sample_complexity_without_pretraining": "Paper states training-from-scratch struggles with small numbers of demos; no precise sample counts to reach thresholds are provided.",
            "sample_complexity_gain": "Enables effective few-shot imitation: achieves strong task success with only 10 expert demos where learning-from-scratch fails; compared to R3M baseline, EC2 improves aggregate success (paper quotes ≈60.4% vs ≈55.5% overall at 10 demos), i.e., a several-percentage-point absolute improvement in success at the same low demo budgets rather than a simple multiplicative sample-reduction factor.",
            "transfer_success_factors": "1) Use of emergent language that preserves fine-grained perceptual and motion details from videos while providing a structured discrete signal; 2) joint pretraining on emergent and natural language via the trajectory-completion objective that links video semantics and compositional natural language; 3) frozen LM as a robust feature extractor enabling few-shot mapping by a small MLP; 4) LOReL dataset contains paired demonstrations and human instructions so natural language signals present during pretraining.",
            "transfer_failure_factors": "1) Pretraining dataset scale and domain mismatch may limit generalization to wholly different settings; 2) emergent-language interpretability and coverage — emergent tokens may not perfectly align with all necessary action semantics; 3) freezing the LM prevents downstream adaptation of representations to new task-specific nuances (a design choice here); 4) pretraining without explicit action labels relies on trajectory-completion proxy tasks which may omit some control-relevant signals.",
            "key_findings": "Pretraining a GPT-like LM on a trajectory-completion task conditioned on both emergent discrete messages (learned from videos) and natural-language instructions produces embodied representations that transfer effectively to 3D manipulation; emergent language acts as a bridge that preserves low-level video details while maintaining linguistic structure, improving few-shot policy learning (notably at 10 demo regime) over contrastive video-language alignment baselines. Joint emergent+natural-language pretraining yields better downstream performance than either alone, and larger LM sizes further increase correlation between emergent and natural language and improve control performance.",
            "uuid": "e1714.0",
            "source_info": {
                "paper_title": "EC2: Emergent Communication for Embodied Control",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "R3M",
            "name_full": "R3M: A universal visual representation for robot manipulation",
            "brief_description": "A pre-trained visual representation learned on paired human video–language data using temporal contrastive objectives and video–language alignment, used as a frozen encoder for downstream robot manipulation tasks.",
            "citation_title": "R3m: A universal visual representation for robot manipulation.",
            "mention_or_use": "use",
            "model_agent_name": "R3M pre-trained vision-language representation",
            "model_agent_description": "A representation model pre-trained on diverse human video–language corpora using time-contrastive learning (encouraging temporally nearby frames to have similar embeddings) together with video–language alignment objectives; the released language encoder or vision encoder is used as a frozen feature extractor for downstream policy learning in this paper's experiments.",
            "pretraining_data_type": "Paired human video–language data (diverse human demonstrations with captions/instruction text) — multimodal video+text pretraining.",
            "pretraining_data_details": "This paper cites and uses the officially released R3M models (pretrained on diverse human video-language data as described in R3M). The EC2 paper does not enumerate R3M's exact pretraining datasets or sizes; it refers to R3M's published pretraining on diverse human videos paired with text.",
            "embodied_task_name": "MetaWorld and Franka Kitchen benchmark manipulation tasks",
            "embodied_task_description": "Same MetaWorld and Franka Kitchen manipulation suites as used in EC2 experiments (multiple manipulation tasks, image observations + proprioceptive data), evaluated with few-shot imitation using small demo sets (5/10/25).",
            "action_space_text": "Natural language instructions / captions (BPE-tokenized text used by the R3M language encoder when performing instruction-conditioned evaluation).",
            "action_space_embodied": "Continuous motor-control actions for robot manipulators (joint-level / end-effector controls) in the simulated environments.",
            "action_mapping_method": "R3M encoders (language encoder for instruction-conditioned runs; vision encoder for video-conditioned runs) produce embeddings which are fed into a small downstream MLP policy (two-layer MLP) that is trained via behavior cloning to regress expert actions.",
            "perception_requirements": "RGB image observations (three camera viewpoints evaluated) plus proprioceptive data (end-effector pose and joint positions) concatenated with visual encodings.",
            "transfer_successful": true,
            "performance_with_pretraining": "MetaWorld: 10 demos — language 69.2 ± 2.0%; video 71.8 ± 1.8%. MetaWorld: 25 demos — language 77.4 ± 2.4%; video 79.0 ± 2.1%. Franka Kitchen: 10 demos — language 41.8 ± 2.5%; video 45.7 ± 2.4%. Franka Kitchen: 25 demos — language 56.0 ± 2.3%; video 58.7 ± 2.0%. (Reported in paper's baseline table.)",
            "performance_without_pretraining": "Learning-from-scratch / non-pretrained policies are reported to perform poorly in the low-demo regime; exact numeric scratch baselines are not presented in the table in this paper.",
            "sample_complexity_with_pretraining": "Evaluations performed with small demonstration budgets {5,10,25}; reported scores above correspond to 10 and 25 demo settings. Downstream policy MLP trained for 20k steps in each run.",
            "sample_complexity_without_pretraining": "Not specified numerically in this paper.",
            "sample_complexity_gain": "R3M's pretraining enables nontrivial success with only 10–25 demos (e.g., MetaWorld ≈69–79% depending on demo size), demonstrating improved sample efficiency versus unspecified-from-scratch baselines; EC2 shows further improvement over R3M by several percentage points at same demo budgets.",
            "transfer_success_factors": "Temporal contrastive learning captures dynamic state transitions; video-language alignment helps encode semantically relevant features for instruction-conditioned behavior; use of large, diverse human video-language pretraining corpora.",
            "transfer_failure_factors": "Forcing tight video-language alignment can discard low-level motion and perceptual details necessary for fine-grained control (paper hypothesis). Domain gap between human demonstration videos and simulated robot environments may reduce transfer fidelity. Natural language captions can be too abstract and miss low-level motion descriptions.",
            "key_findings": "Pretraining on paired human video–language data (R3M) yields useful frozen representations that significantly improve few-shot imitation performance in 3D manipulation, but methods that explicitly align video and text embeddings may lose fine-grained control-critical details that EC2 recovers via emergent-language conditioning and trajectory-completion pretext tasks.",
            "uuid": "e1714.1",
            "source_info": {
                "paper_title": "EC2: Emergent Communication for Embodied Control",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "BC-Z",
            "name_full": "BC-Z: Zero-shot task generalization with robotic imitation learning",
            "brief_description": "A multi-task imitation learning system that jointly learns alignment between video representations and language representations together with policy learning across tasks; used here as a baseline for few-shot, instruction-conditioned manipulation.",
            "citation_title": "Bc-z: Zero-shot task generalization with robotic imitation learning.",
            "mention_or_use": "use",
            "model_agent_name": "BC-Z multi-task alignment & policy model",
            "model_agent_description": "A multi-task imitation learning framework that jointly learns to align video and language representations and to produce policies across many tasks; in this paper BC-Z is used with its official implementation as a baseline, encoding instructions or videos and mapping embeddings to actions via task-specific policies.",
            "pretraining_data_type": "Demonstration videos and task information (multi-task imitation data), including language and video annotations used during BC-Z's original multi-task training.",
            "pretraining_data_details": "This paper uses BC-Z's publicly released model/implementation for comparison; the EC2 paper does not re-describe BC-Z's full pretraining corpus or sizes (BC-Z original paper provides those details).",
            "embodied_task_name": "MetaWorld and Franka Kitchen benchmark manipulation tasks",
            "embodied_task_description": "Same 3D simulated manipulation suites as used for EC2 evaluation, evaluated in few-shot imitation regimes with small demo counts.",
            "action_space_text": "Language instructions (text) and demonstration-video encodings depending on experiment condition.",
            "action_space_embodied": "Continuous robot control actions (joint/end-effector commands) used in the simulated manipulation tasks.",
            "action_mapping_method": "BC-Z jointly learns representation alignment and policy; in this evaluation the model's embeddings are fed into a downstream policy network (MLP) trained via behavior cloning to reproduce expert actions.",
            "perception_requirements": "RGB images and proprioceptive signals (end-effector pose, joint positions) concatenated with vision encodings.",
            "transfer_successful": true,
            "performance_with_pretraining": "MetaWorld: 10 demos — language 63.4 ± 2.1%; video 64.8 ± 2.0%. MetaWorld: 25 demos — language 74.1 ± 2.2%; video 75.2 ± 1.0%. Franka Kitchen: 10 demos — language 34.2 ± 2.3%; video 37.5 ± 2.2%. Franka Kitchen: 25 demos — language 38.6 ± 2.4%; video 40.0 ± 2.0%. (Reported in the paper's baseline table.)",
            "performance_without_pretraining": "No exact-from-scratch numbers given in this paper; the authors state learning-from-scratch struggles in the small-demo regime.",
            "sample_complexity_with_pretraining": "Evaluated with {5,10,25} demo budgets; reported stats correspond to 10 and 25 demo settings; downstream MLP trained 20k iterations as per EC2 evaluation protocol.",
            "sample_complexity_without_pretraining": "Not numerically specified in the EC2 paper.",
            "sample_complexity_gain": "BC-Z pretraining yields nontrivial success in few-shot regimes (e.g., MetaWorld ≈63–75% depending on demos), but EC2 generally outperforms BC-Z by several absolute percentage points at the same small demo budgets.",
            "transfer_success_factors": "Joint multi-task training with language and video modalities enables generalization across tasks; explicit alignment between modalities can help instruction-conditioned policy learning.",
            "transfer_failure_factors": "Joint alignment objectives may omit low-level motion details present in raw videos; limited expressiveness of captions and alignment could limit fine-grained control transfer.",
            "key_findings": "BC-Z's joint multi-task video-language+policy pretraining yields useful priors for few-shot imitation but is outperformed by EC2 which avoids forcing strict video-language alignment and instead uses emergent discrete messages plus trajectory completion to retain fine-grained control-relevant information.",
            "uuid": "e1714.2",
            "source_info": {
                "paper_title": "EC2: Emergent Communication for Embodied Control",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "R3m: A universal visual representation for robot manipulation.",
            "rating": 2,
            "sanitized_title": "r3m_a_universal_visual_representation_for_robot_manipulation"
        },
        {
            "paper_title": "Bc-z: Zero-shot task generalization with robotic imitation learning.",
            "rating": 2,
            "sanitized_title": "bcz_zeroshot_task_generalization_with_robotic_imitation_learning"
        },
        {
            "paper_title": "Linking emergent and natural languages via corpus transfer.",
            "rating": 1,
            "sanitized_title": "linking_emergent_and_natural_languages_via_corpus_transfer"
        }
    ],
    "cost": 0.01867925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EC 2 : Emergent Communication for Embodied Control</p>
<p>Yao Mu 
The University of Hong
Kong</p>
<p>Shunyu Yao 
Princeton University</p>
<p>Mingyu Ding 
The University of Hong
Kong</p>
<p>Ping Luo pluo@cs.hku.hkshunyuy@princeton.edu 
The University of Hong
Kong</p>
<p>Chuang Gan ganchuang@csail.mit.edu 
UMass Amherst</p>
<p>MIT-IBM Watson AI Lab</p>
<p>EC 2 : Emergent Communication for Embodied Control</p>
<p>Embodied control requires agents to leverage multimodal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC 2 ), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, which is then used to finetune a lightweight policy network for downstream control. Through extensive experiments in Metaworld and Franka Kitchen embodied benchmarks, EC 2 is shown to consistently outperform previous contrastive learning methods for both videos and texts as task inputs. Further ablations confirm the importance of the emergent language, which is beneficial for both video and language learning, and significantly superior to using pre-trained video captions. We also present a quantitative and qualitative analysis of the emergent language and discuss future directions toward better understanding and leveraging emergent communication in embodied tasks.</p>
<p>Introduction</p>
<p>We study the problem of few-shot embodied control, where an embodied agent needs to execute language instructions or follow video demonstrations given only a few examples in a new environment. Such a capability of fast adaption is key to practical robotic deployment, as it is nonscalable and expensive to collect extensive action-labeled  The key idea of EC 2 is to build the link between perceptual grounding and symbolic concept via emergent communication (EC) language. We learn to extract embodied representation via language model utilizing emergent language or natural language as prompt and visual observation as input for downstream embodied control tasks. The actions are generated by a lightweight policy network containing a few MLP [51] layers that map the learned frozen embodied representation into action space. Extensive experiments show that EC 2 outperforms existing methods in Metaworld [60] and Franka Kitchen [17] benchmarks.</p>
<p>trajectories for each new application scenario. Instead, agents need to leverage pre-trained video and language representations to quickly learn how to act, and generalize across high-level concepts (e.g. object and verb types) as well as low-level visual features.</p>
<p>How can we pre-train multi-modal representations of videos and texts for downstream embodied control in different domains? Inspired by the success of image-text models such as CLIP [47], recent work has investigated contrastive representation learning approaches using paired video-language data, with application toward robotic manipulation and control tasks [24,38]. However, simply aligning video and language pairs in the embedding space omits the difference between these two modalities: while videos contain more visual and motion details, language abstracts away key structures underlying the task.</p>
<p>For example, a video of opening a door contains details of approaching the handle, pressing the handle, pulling the door open, and so on, while the corresponding language description could be as simple as "open a door" -more detailed language descriptions such as "press down the handle" is usually not available in existing datasets and very expensive to manually label. But for downstream control purposes, a video and its abstract language description could present complementary benefits -while the former is expressive in details needed for low-level control, the latter provides the structure for generalization across domains, tasks, and skills. Leveraging such modality differences presents opportunities to better incorporate language and videos for embodied representation learning, marrying their respective benefits rather than forcing them to be aligned.</p>
<p>One perspective to view the video-language difference is through the emergence of language [39,55]: language derives meaning from its use [57]. It is a communication tool humans develop to collaboratively solve embodied tasks, thus abstracting perception experiences like videos into symbolic concepts. Emergent communication in machine learning [9,26,28,30,56] aims to similarly learn an "emergent language" of perception in an unsupervised and domain-adaptive way: as Figure 2(a) shows, a speaker and a listener network play a referential game, where a speaker maps some visual stimuli (e.g. image, video) into a message of discrete tokens, and a listener uses the message to choose the speaker reference out of detractors. By jointly optimizing for game success, the communication protocol emerges structural and semantic properties resembling language, yet its discrete tokens might convey more fine-grained concepts than natural language useful for distinguishing stimuli. Thus, an emergent language can act as a bridge between natural language and videos, featuring a structure similar to the former while preserving the semantics of the latter.</p>
<p>Inspired by recent work that links emergent and natural language [58], we propose Emergent Communication for Embodied Control (EC 2 ), a three-phase scheme to learn embodied representations for downstream few-shot control (Figure 1(a-c)). First, we learn an emergent language of demonstration videos without using any language labels, which could potentially capture domain-specific concepts not directly available in paired natural language captions. Second, we learn an embodied representation by using a language model to predict trajectory segments conditional on trajectory contexts and language annotations, both natural and emergent. Rather than forcing video and language representations to be aligned, such a sequence modeling approach learns to represent natural language (for compositional generalization) and emergent language (for lowlevel perception and control) jointly with video trajectories, where the emergent language serves as a bridge between language and videos. Third, we train a lightweight policy network on top of the embodied representation to few-shot adapt to downstream embodied control.</p>
<p>We demonstrate that emergent language can help embodied control tasks and enable data-efficient few-shot downstream imitation learning via extensive experimental results across two existing benchmark simulation environments (Franka-Kitchen [18] and MetaWorld [60]). EC 2 outperforms existing methods and achieves state-of-the-art performance on both language and video instruction following tasks. Our study also shows that although emergent language and natural language are utilized in a parallel manner in the pre-training phase, the word embeddings of natural language can still be predicted by linear regression models from emergent language. We further investigate the impact of emergent language and natural language correlation on the performance of downstream tasks and show that the performance in downstream tasks is best when the learned EC contains more information than natural language, i.e., when the accuracy of predicting natural language from emergent language is higher than that of predicting emergent language from natural language. Overall, on the basis of these results, we believe that EC 2 has great potential to become a promising embodied control framework.</p>
<p>To sum up, our work makes the following contributions. (1) we build an embodied representation pre-training framework with emergent communication (EC) via a language model, which incorporates both the abstract, compositional structure of language and visuals with low-level motion details. (2) we develop a few-shot embodied control system, which transfers the pre-trained language model to downstream tasks as a frozen module and quickly adapts lightweight policy networks with a few expert data to generate actions. (3) we demonstrate that the learned representation with EC can benefit few-shot embodied control tasks and extensive experiments show that EC 2 outperforms existing methods in Metaworld [60] and Franka Kitchen [17] benchmarks.</p>
<p>Related Work</p>
<p>Emergent Communication</p>
<p>The recent advances in NLP [7,14,49] benefits from huge text corpora and are striving to find statistical regularities on large amounts of data via big models like GPT [7,48,49] and BERT [14]. This narrow focus was leaving aside language emergence and language grounding (in some other modalities, e.g., sight, with more complex inputs than text). In contrast, human language acquisition [13] starts with a cumulative series of interactions with other people [3,8] grounded in the physical and social world [1,19,54] and does not rely on passive and complex corpora like We generate the corresponding emergent language ( ) for each demonstration video in the multi-modal dataset. Predicting the cropped part of a latent trajectory is used as an auxiliary task to help pre-training the language model. The language model takes emergent language or natural language as prompt and the cropped trajectory ( ) as input and aims to predict the cropped part of the trajectory ( ). The loss for the prediction using emergent language as prompt and natural language as prompt is calculated separately. Few-shot downstream policy learning: In downstream imitation learning tasks, both the EC speaker and the pre-trained language model( ) are frozen. The language model extracts effective embodied features with current trajectory ( contains current state and historical information) as input. (c) For tasks with language instruction, we use natural language as the prompt, and (d) for tasks with video instruction, we use emergent language as the prompt. The output features of the language model are mapped into actions to mimic expert behavior by a few simple MLP [51] layers ( ).</p>
<p>Wikipedia. Emergent Communication (EC) is one promising direction toward this motivation, where a communication protocol is shaped through multi-agent interactions with perceptual grounding like a human [43]. As a typical example, the referential image game involves a speaker creating a sequence of discrete tokens based on an input image, and a listener tasked with selecting the correct input from a group of distractors using the message. Both networks are optimized jointly using success signals from the game [58]. By studying games like this type, researchers are interested in the emergence of desirable properties resembling natural language, such as game success generalization and compositionality, which holds great promises towards more functional [56,57] and generalizable [27] language agents.</p>
<p>Embodied Control under Instruction</p>
<p>Embodied control has achieved much success in learning grasping and pick-place tasks from low-dimensional state [2,4,10,25,36,42,52]. Deep learning makes embodied control directly learn from high dimensional observations becoming feasible [46,50,61]. Recent advances aim to build a learning-based embodied control system with deep neural networks that are flexibly conditioned on either a demonstration video [5,59] of a human or a language instruction [35,53] via multi-modal representation learning. With the guidance of instruction, the embodied agent aims to gener-alize to new scenes [44], novel objects [5,16,22,59,62], novel object configurations [40], and novel goal configurations [12,15,20]. For instance, BC-Z [24] has developed a flexible imitation learning system that can learn from demonstrations and interventions, based on various forms of task information, such as pre-trained embeddings of natural language or human performance videos. R3M [38] pre-trains visual representations on diverse human videolanguage data by encouraging alignment between the two modalities. However, since videos often convey more detailed information than words, this framework may lose valuable teaching information that can help robots perform complex tasks. In this work, we aim to build an instructionconditioned embodied control framework without forcing alignment on video and language and consider the video demonstrations and language instructions as "parallel sentences" during the pre-training process to learn useful information for downstream tasks.</p>
<p>Method</p>
<p>As shown in Figure 2, we first pre-train a speaker and listener via an emergent communication game for emergent language generation and a language model to extract embodied representations by predicting the cropped part of a latent trajectory. The language model takes the emergent language or natural language as prompt and the masked trajectory as input and aims to predict the masked part of the trajectory separately. Then, we transfer the learned speaker and language model into downstream embodied control tasks as frozen modules. The output of the language model extracts embodied representation from instruction prompt and current observations and is mapped into actions by the downstream policy network, which contains a few task-specific MLP [51] layers.</p>
<p>Problem Statement</p>
<p>The goal for embodied control is to learn a policy π B conditioned on the instruction, either language instruction or video instruction, that reproduces the expert behavior on the desired task specified by the instruction. Usually, the demonstrations of experts are presented in the form of state-action trajectories, with each pair indicating the action to take at the state being visited. Let τ E denote a trajectory sampled from expert policy π E (s|c): τ E = (s 0 , a 0 ), (s 1 , a 1 ), . . . , (s n , a n ) . The instruction is encoded into the latent vector c. To learn the behavior policy π B (s|c), the demonstrated actions are usually utilized as the target label for each state, and the mapping a ∈ π B (s|c) from states to actions is learned in a supervised manner with given instructions.</p>
<p>Emergent Communication Language Generation for demonstration video</p>
<p>Our idea is to automatically generate synthetic language that aligns with video demonstration through emergent communications, which connect perceptual, language, and action for pre-training. As shown in Figure 2(c), we consider a typical speaker-listener referential game [29,30,33] on a set of N video features D I = {I 1 , · · · , I N }. At each training step, the speaker takes one video feature I i and generates a discrete message M i ∈ [V ] T using the Gumbel-Softmax trick [23], where V is the vocabulary size, and T is the sequence length limit. For simplicity, denote m = M i so that m t = M i,t denotes the t-th token of the message M i . The generated discrete message is defined as "emergent language", and the process of emergent language generation by the speaker can be formulated as
hs 0 = I i , hs t = GPT spk (m t−1 , hs t−1 ) (t &gt; 0), m 0 = [CLS], m t = Softmax (MLP spk (hs t )) (t &gt; 0).
(1) Here hs t denotes speaker hidden states. Next, the listener takes the message m, and tries to guess the right video I i out of a set of K confounding videos C i = {I j1 , · · · , I j K } ⊂ D I − {I i }. Note that different from the EC games on images, in order to learn temporal information, except for irrelevant videos, we also construct several additional candidates in C i by performing temporal augmentation on the original video, such as reverse order and random disorder.</p>
<p>The listener also uses a GRU [11] layer to turn the message m into a hidden vector hl T hl 0 = 0, hl t = GPT lsn (m t , hl t−1 ) (t &gt; 0).</p>
<p>(2)</p>
<p>Based on hl T , the listener assigns a score for each candidate video based on inverse square error [31], then selects the image by Softmax sampling across the scores. The speaker and listener are jointly optimized by minimizing the crossentropy loss of video selection:
score(I) = hl T − MLP lsn (I) −2 2 , p( guess = I) = softmax(score(I)) (I ∈ {I i } ∪ C i ) , L EC = −E Ii,Ci E Mi log p ( guess = I i ) .
(3) The speaker can be employed to generate emergent language D M = {M 1 , · · · , M N } based on input videos.</p>
<p>Pre-training of Language Model with Emergent Language</p>
<p>We conduct a trajectory completion task to pre-train the GPT-like language model without action labels, as shown in Figure 2(a)(b). Firstly, we random sample a sequence of observationsô = {o 1 , o 2 , . . . , o N } stored in the dataset and map it into latent trajectory τ whole by encoder g θ .
τ whole = g θ (ô)(4)
Then we crop a random segment τ seg from the whole latent trajectory τ whole and the remained trajectory is denoted as τ rem . The language model f φ takes τ rem as input and uses either emergent language e generated by the speaker or natural language l as a prompt to predict the cropped segment τ seg . The language model is optimized by jointly minimizing L EC and L Lang
L EC = (τ seg − f θ (e, τ rem )) 2 (5) L Lang = (τ seg − f θ (l, τ rem )) 2(6)
The loss for the prediction using emergent language as prompt and natural language as prompt is calculated separately. EC 2 uses the same language model for both emergent language generation and trajectory completion pretext task for framework thriftiness, and both the L EC-pre and L EC-gen are used to update the language model jointly. To ensure that the speaker's gradient is not truncated, we use an MLP [51] layer to map from the logits of the word distribution to a 512-dimensional latent vector instead of selecting a word embedding in the dictionary according to the token. For natural language, we use the Byte-Pair Encoding (BPE) tokenizer [6] to encode it into tokens, which are widely used by OpenAI for tokenization when pre-training the GPT model, including GPT [48], GPT-2 [49], RoBERTa [34], and BART [32]. We also learn a corresponding dictionary of word embedding. Although the emergent language and natural language are used separately, they can learn correlated information via the pretext trajectory completion task.</p>
<p>Few-shot Policy Learning for Downstream Embodied control</p>
<p>To transfer the pre-trained language model into the downstream imitation learning tasks, the language model f θ (·, ·) take the current trajectory τ cur = {s t−T , . . . , s t−1 , s t } and the emergent language e or natural language instruction l as prompt, then output the predicted features m t . Finally, the output m t of the language model is mapped to specific action a t by task-specific MLP layers π ψ . The language model f θ is frozen during downstream imitation learning. The action under learned behavior policy is sampled byâ t ∼ π ψ (a t |m t ) and the expert demonstration action a t are used as the label for imitation learning. For the video instruction following task, we generate emergent language by the speaker and use it as the prompt of the language model. For language instruction following tasks, the natural language is tokenized by the BPE tokenizer, and select the corresponding word embedding in the dictionary as the prompt as language model via the inferred token. With a trajectory with length H, the taskspecific MLP layers are optimized by minimizing L IL (ψ).
L IL (ψ) = E τ ∼B − 1 H t+H−1 t log π ψ (at|mt)(7)
The MLP [51] layers are implemented with hidden sizes [256, 256] with batch normalization [21] and are trained with a learning rate of 0.001 and a batch size of 32 for 20000 iterations.</p>
<p>Experiments</p>
<p>Pre-training Dataset</p>
<p>We use the LOReL [37] real robot dataset, which is collected by Franka Emika Panda mounted over an IKEA desk. The LOReL dataset contains 3000 episodes (150000 frames) of reinforcement learning that trains policies for different behaviors on the IKEA desk using online RL. The language instruction is annotated by leveraging crowdsourcing, specifically Amazon Mechanical Turk. Human annotators are asked to describe the behavior if any, that the robot is doing and to phrase it as a command without any pre-specified template. The LOReL dataset contains 6000 annotations, two per episode, containing a total of 1699 unique instructions. The dataset contains different videos with the same language instruction to ensure diversity. We removed the episodes where annotators reported the robot as inactive or its actions unclear, making them incomprehensible.</p>
<p>Downstream Testing Environments</p>
<p>We utilize two standard robotic manipulation benchmarks, namely MetaWorld [60] and the Franka Kitchen environment [17], as our downstream testing environments. It is worth noting that these testing environments were not used during the training phase of EC 2 . In MetaWorld, the agent is presented with a series of tasks, including assembling a ring on a peg, picking and placing a block between bins, pushing a button, opening a drawer, and hammering a nail. In Franka Kitchen, the agent aims to learn how to slide open the right door, open the left door, turn on the light, turn the stove top knob, and open the microwave. As shown in Figure 3, both environments offer image observations to the agent, as well as proprioceptive data, which includes the end-effector pose and joint positions. These data are concatenated with the encoded vision observations for each task, providing the agent with a comprehensive understanding of the current task. All tasks involve random environment variation, either by varying the position of the target object in MetaWorld or the positioning of the desk in Franka Kitchen. Additionally, we consider three views for each environment to evaluate the robustness of representations across viewpoints. Finally, for each environment, we also consider three demo sizes used in downstream embodied control tasks: [5,10,25] in MetaWorld and Franka Kitchen.</p>
<p>Experimental Setup</p>
<p>For embodied control tasks, we take R3M and BC-Z as the key baseline. R3M [38] learns the state-of-theart embodied representations pre-trained on diverse paired human video-language data with time-contrastive learning, which encourages states closer in time to be closer in embedding space and video-language alignment to encourage the embeddings to capture semantically relevant features. For sufficient comparison, we implement R3M with both the language instruction-conditioned version and video instruction-conditioned version, respectively, based on their official implementation. For embodied control with natural language instruction, we use the language encoder of the pre-trained model officially released by their author team to encode the natural language. For embodied control with demonstration video instruction, we use the officially released vision encoder to extract features of every frame in the video and map them into the instruction representation by one MLP layer. BC-Z [24] jointly learns the alignment between video representation and language representation together with policy learning across multiple embodied control tasks. Our evaluation methodology is inspired by [38,41]. We focus on evaluating the embodied representation of instruction and observation for downstream policy learning with behavior cloning. We parameterize the downstream policy π as a two-layer MLP preceded by a   [17]. We provide specific language instruction labels for every task and 25 demos per task for evaluation. We show an example image and corresponding language instruction for each task shown above.  Table 1. Performance comparison on Success rate of embodied control with language/video instruction across different demo size. We mark the performance difference between complete the task with natural language instruction and video instruction as blue.</p>
<p>BatchNorm at the input. We train the agent for 20,000 steps, evaluate it online in the environment every 1000 steps, and report the best success rate achieved. For each task, we run 5 seeds of behavior cloning. The final success rate reported for one method on one task is the average over the 5 seeds × 3 camera viewpoints × 3 demo sizes, for a total of 45 runs (see more details in Appendix).</p>
<p>Results</p>
<p>We aim to answer 5 questions to get insight into embodied AI community: 1) Can emergent language help embodied control tasks with natural language instruction? 2) Can emergent language help the embodied control tasks with video instruction? 3) Can the joint training of emergent language and natural language in GPT-Like network architecture help each other? 4) Can emergent language provide more detailed and effective guidelines than video captions? 5) What is the relationship between model size, control performance, and the correlation between emergent language and natural language?</p>
<p>To answer the first two questions, we study if EC 2 enables more data-efficient policy learning with both natural language instructions and video instructions.</p>
<p>1) Can emergent language help the embodied control tasks with natural language?</p>
<p>We measure the success rate of downstream few-shot policy learning with different pre-training methods. In Franka Kitchen and MetaWorld, we only use no more than 25 demos to perform downstream few-shot policy learning and report the performance with both 10 and 25 demos separately in Table 1. Learning from scratch is struggled to perform well when working with small amounts of data since it can be challenging for the model to identify meaningful patterns or relationships with such limited data. While the methods with a pre-trained model, like EC 2 , R3M, and BC-Z performs better than those learning from scratch. Through our study, as shown in Table 1, EC 2 exceeds existing methods with natural language instruction, which indicates that emergent language can help the embodied control tasks to improve the performance with natural language instruction. Across all the evaluations, EC 2 is overall able to learn these embodied tasks in an extremely low data regime (only 10 demos) with ≈60.4% overall success rate, despite never seeing any data from the target environments in training the representation, while R3M only achieves ≈55.5% overall success rate.   Table 4. Qualitative examples of emergent language and corresponding natural language. The word and tokens that have the same semantic meaning are marked with the same color.</p>
<p>2) Can emergent language help the embodied control tasks with video instruction?</p>
<p>As shown in Table1, EC 2 shows an obvious advantage over the exciting methods and achieves the best success rate (≈64.8% with 10 demos and ≈74.1% with 25demos) with video instruction. As marked in blue in Table 1, EC 2 has better performance with video instruction than with language instruction, which indicates that EC 2 learns more effective information provided by a video demonstration. In contrast, R3M and BC-Z, which use contrast learning or regression to force the two modalities of language and video aligned, have little difference in performance under language instruction and video instruction.</p>
<p>We also investigate the task performance of EC 2 and baselines with different amounts of demos in policy learning. In Figure 4, we plot the average success rate of each method across each demo size. We observe that the performance improvement from EC 2 is consistent, outperforming the baselines across every environment and demo size. Ablation Study: Also, in the data-efficient imitation learning setting, we ablate the different components of EC 2 training to answer question 3 and 4 and check if each module is indispensable.</p>
<p>3) Can the joint training of emergent language and natural language with GPT-Like network help each other?</p>
<p>We conduct ablation studies to compare EC 2 and EC 2 (-EC) in language instruction following tasks and compare EC 2 and EC 2 (-NL) in video instruction following tasks. EC 2 (-EC) does not use emergent language to pre-train the language model under EC 2 framework, which predicts the masked trajectories with only the language instruction (human-labeled caption). EC 2 (-NL) does not use natural language to pre-train the language model under EC 2 framework, which predicts the masked trajectories with only the emergent language. As shown in the left-hand column in Table 2, without the help of emergent language, the performance decreased significantly, and EC 2 (-EC) shows no obvious advantages to the other methods that learn pretrained models by video-language pre-training. Similarly, as shown in the right-hand column in Table 2, EC 2 also outperforms EC 2 (-NL), which demonstrates that the language pre-training helps the video instruction task with emergent language. Thus, the above ablation analysis shows that the joint training of emergent language and natural language can benefit each other. 4) Can emergent language provide more detailed and effective guidelines than video captions?</p>
<p>In EC 2 (+Cap, -EC), we replace the emergent language in EC 2 with the language caption given by a MARN [45] as a data augmentation, which is a pre-trained video caption model. As shown in Table 2, EC 2 shows an obvious advantage to EC 2 (+Cap, -NL), which demonstrates that emergent language provides more detailed and effective guidelines than video captions. 5) What is the relationship between model size, control performance, and the correlation between emergent language and natural language?</p>
<p>We first tested the overall average success rate of EC 2 with different model sizes (listed in Table 3) in the downstream tasks (Franka Kitchen and MetaWorld). As shown in Figure 5a and Figure 5b, the larger the size of the model, the more expressive it is and thus the better its performance. Then, we perform a linear correlation analysis on the word embeddings of the emergent language and the word embeddings of the matched natural language. We use the square of the coefficient of determination, denoted R 2 as the measure of predicting the performance of linear regression, which reflects the proportion of the variation in the dependent variable that is predictable from the independent variable. Consider a linear regression model f (·) which predicts y with the input x, the coefficient of determination R is defined as
R 2 = 1 − SS res SS tot = 1 − i (y i − f (x i )) 2 i (y i −ȳ) 2(8)
We evaluat the R 2 values of two linear regressions. One predicts natural language word embeddings from emergent language, and the other predicts emergent language word embeddings from natural language. As shown in Figure  5c, the R 2 of predicting natural language embeddings from the emergent language increase along with the model becoming larger, which corresponds to better performance. In contrast, the R 2 of predicting emergent language embeddings from the natural language is non-monotonous, along with the model size becoming larger. The reason is that emergent language captures more information that natural language does not contain when the model is large enough and has promising performance. In short, EC 2 has the best performance in embodied control tasks when emergent language contains more information than natural language, the performance is median when emergent language and language information are nearly equal, and the performance is worst when emergent language contains less information than language information. As shown in Table 4, we also provide qualitative examples of emergent language and its corresponding natural language to show that emergent language can capture key concepts from video. We find that the tokens "108" and "500" correspond to "drawer" in natural language and token "701" corresponds to "stapler." Except for nouns, emergent language can also get the con-cept of verbs, for example, token "538" corresponds to the verb "reach" in natural language. The current manual check analysis of emergent language is just a preliminary attempt, and more understanding can be promising for future work.</p>
<p>Conclusions and Discussions</p>
<p>This paper aims to build the link between perceptual grounding and symbolic concepts by emergent communication (EC) language for embodied control. To this end, we develop a novel Embodied Control framework with the help of Emergent Communication language (EC 2 ), which pre-trains a language model via masked trajectory complement pretext task condition on emergent language or natural language. The pre-trained language model is utilized to extract embodied representation from instructions and observations and is used as a frozen module for downstream data-efficient policy learning. Extensive experiments show that EC 2 outperforms existing methods in Metaworld and Franka kitchen benchmarks. We believe that EC 2 will serve as a solid step toward the general decision-making model. Limitations and future works: more understanding about emergent language, training EC 2 on a larger and more diverse dataset, and applications on more practical setup can be promising directions of future works.</p>
<p>comparison on overall success rate across all tasks.</p>
<p>Figure 1 .
1Overview of EC 2 .</p>
<p>Figure 2 .
2The overall framework of EC 2 . Pre-training: (a) We first pre-train a speaker ( ) and listener ( ) via emergent communication game for emergent language generation and a language model ( ) to extract embodied representations. (b)</p>
<p>Figure 3 .
3Evaluation Environments: We consider a set of manipulation tasks including 5 tasks with a Sawyer from MetaWorld[60], 5 tasks from a Franka operating over a Kitchen</p>
<p>Figure 4 .Figure 5 .
45Performance over different demo sizes used in downstream imitation learning. We report the success rate of EC 2 and baseline across different demo sizes. We see that the performance improvement from EC 2 is consistent across all demo sizes. Ablation on model size and its relationship with the correlation of emergent language and Natural language. We show the ablation results on model size with 10 demos and 25 demos in downstream policy learning separately.</p>
<p>Evaluation Viewpoints:To robustly measure the efficacy of different visual representations, we consider three camera viewpoints per environment.Benchmark 
Num. Demos 
Instruction 
EC 2 (ours) 
R3M [38] 
BC-Z [24] </p>
<p>MetaWorld </p>
<p>10 
Lang 
72.4 ± 2.4% 
69.2 ± 2.0% 
63.4 ± 2.1% 
Video 
76.0 ± 2.1% (+3.6) 
71.8 ± 1.8% (+2.6) 
64.8 ± 2.0% (+1.4) </p>
<p>25 
Lang 
82.6 ± 2.0% 
77.4 ± 2.4% 
74.1 ± 2.2% 
Video 
85.0 ± 2.2% (+2.4) 
79.0 ± 2.1% (+1.6) 
75.2 ± 1.0%(+1.1) </p>
<p>Franka Kitchen </p>
<p>10 
Lang 
48.4 ± 2.5% 
41.8 ± 2.5% 
34.2 ± 2.3% 
Video 
53.6 ± 2.5%(+5.2) 
45.7 ± 2.4% (+3.9) 
37.5 ± 2.2% (+3.3) </p>
<p>25 
Lang 
59.8 ± 2.5% 
56.0 ± 2.3% 
38.6 ± 2.4% 
Video 
63.2 ± 2.6% (+3.4) 
58.7 ± 2.0% (+2.7) 
40.0 ± 2.0% (+1.4) </p>
<p>Table 3 .
3Hyper-parameters for different 
model sizes of EC 2 . All the models are 
implemented based on GPT [48]-like net-
work architecture. </p>
<p>Natural Language 
Emergent language 
"Open the right drawer" 
[262, 847, 490, 536, 108, 500, 941, 838, 749, 444] 
"Grasp in the middle drawer" 
[356, 69, 108, 259, 72, 616, 500, 896, 623, 433] 
"Reach down for stapler" 
[204, 897, 285, 509, 701, 545, 538, 729, 902, 195] 
"Pick up the stapler in the drawer" 
[535, 67, 108, 661, 84, 701, 776, 500, 377, 454] 
"Reach to the plug" 
[982, 691, 72, 538, 97, 665, 148, 369, 102, 1003] 
"Reach to the left drawer" 
[786, 538, 601, 874, 848, 500, 315, 108, 72, 914] </p>
<p>Natural language acquisition and grounding for embodied robotic systems. Muhannad Alomari, Paul Duckworth, David Hogg, Anthony Cohn, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31Muhannad Alomari, Paul Duckworth, David Hogg, and An- thony Cohn. Natural language acquisition and grounding for embodied robotic systems. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 31, 2017. 2</p>
<p>A survey of robot learning from demonstration. Sonia Brenna D Argall, Manuela Chernova, Brett Veloso, Browning, Robotics and autonomous systems. 575Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demon- stration. Robotics and autonomous systems, 57(5):469-483, 2009. 3</p>
<p>Input and interaction in language acquisition. Michelle E Barton, Cambridge University PressMichelle E Barton. Input and interaction in language acqui- sition. Cambridge University Press, 1994. 2</p>
<p>Discovering optimal imitation strategies. Aude Billard, Yann Epars, Stefan Sylvain Calinon, Gordon Schaal, Cheng, Robotics and autonomous systems. 472-3Aude Billard, Yann Epars, Sylvain Calinon, Stefan Schaal, and Gordon Cheng. Discovering optimal imitation strategies. Robotics and autonomous systems, 47(2-3):69-77, 2004. 3</p>
<p>Learning one-shot imitation from humans without humans. Alessandro Bonardi, Stephen James, Andrew J Davison, IEEE Robotics and Automation Letters. 52Alessandro Bonardi, Stephen James, and Andrew J Davi- son. Learning one-shot imitation from humans without hu- mans. IEEE Robotics and Automation Letters, 5(2):3533- 3539, 2020. 3</p>
<p>Byte pair encoding is suboptimal for language model pretraining. Kaj Bostrom, Greg Durrett, arXiv:2004.03720arXiv preprintKaj Bostrom and Greg Durrett. Byte pair encoding is sub- optimal for language model pretraining. arXiv preprint arXiv:2004.03720, 2020. 4</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.14165arXiv preprintTom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 2</p>
<p>The role of interaction formats in language acquisition. Jerome Bruner, Language and social situations. SpringerJerome Bruner. The role of interaction formats in language acquisition. In Language and social situations, pages 31-46. Springer, 1985. 2</p>
<p>Computer simulation: A new scientific approach to the study of language evolution. Angelo Cangelosi, Domenico Parisi, Simulating the evolution of language. SpringerAngelo Cangelosi and Domenico Parisi. Computer simula- tion: A new scientific approach to the study of language evo- lution. In Simulating the evolution of language, pages 3-28. Springer, 2002. 2</p>
<p>Learning to walk through imitation. Rawichote Chalodhorn, B David, Keith Grimes, Rajesh Pn Grochow, Rao, IJCAI. 7Rawichote Chalodhorn, David B Grimes, Keith Grochow, and Rajesh PN Rao. Learning to walk through imitation. In IJCAI, volume 7, pages 2084-2090, 2007. 3</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.3555arXiv preprintJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. 4</p>
<p>Transformers for one-shot visual imitation. Sudeep Dasari, Abhinav Gupta, Conference on Robot Learning (CoRL). 2020Sudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. Conference on Robot Learning (CoRL), 2020. 3</p>
<p>Language acquisition. Jill G De Villiers, Jill De Villiers, Peter A De, Peter A Villiers, Devilliers, Harvard University PressJill G De Villiers, Jill De Villiers, Peter A De Villiers, and Peter A DeVilliers. Language acquisition. Harvard Univer- sity Press, 1978. 2</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2</p>
<p>Yan Duan, Marcin Andrychowicz, C Bradly, Jonathan Stadie, Jonas Ho, Schneider, arXiv:1703.07326Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. arXiv preprintYan Duan, Marcin Andrychowicz, Bradly C Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wo- jciech Zaremba. One-shot imitation learning. arXiv preprint arXiv:1703.07326, 2017. 3</p>
<p>One-shot visual imitation learning via metalearning. Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, Sergey Levine, PMLR, 2017. 3Conference on Robot Learning. Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta- learning. In Conference on Robot Learning, pages 357-368. PMLR, 2017. 3</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman, arXiv:1910.119566arXiv preprintAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. 1, 2, 5, 6</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman, Conference on Robot Learning. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. Conference on Robot Learning, 2019. 2</p>
<p>The symbol grounding problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-3Stevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335-346, 1990. 2</p>
<p>Neural task graphs: Generalizing to unseen tasks from a single video demonstration. De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionDe-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, and Juan Carlos Niebles. Neural task graphs: Generalizing to unseen tasks from a sin- gle video demonstration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8565-8574, 2019. 3</p>
<p>Batch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, PMLRInternational conference on machine learning. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448-456. PMLR, 2015. 5</p>
<p>Task-embedded control networks for few-shot imitation learning. Stephen James, Michael Bloesch, Andrew J Davison, PMLRConference on Robot Learning. Stephen James, Michael Bloesch, and Andrew J Davi- son. Task-embedded control networks for few-shot imitation learning. In Conference on Robot Learning, pages 783-795. PMLR, 2018. 3</p>
<p>Categorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, arXiv:1611.01144arXiv preprintEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. 4</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. 6PMLR, 2022. 1, 3, 5Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Fred- erik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991- 1002. PMLR, 2022. 1, 3, 5, 6</p>
<p>Learning stable non-linear dynamical systems with gaussian mixture models. S M Khansari-Zadeh, Aude Billard, IEEE Transaction on Robotics. 275S.M. Khansari-Zadeh and Aude Billard. Learning stable non-linear dynamical systems with gaussian mixture mod- els. IEEE Transaction on Robotics, 27(5):943-957, 2011. 3</p>
<p>Natural language from artificial life. Simon Kirby, Artificial life. 82Simon Kirby. Natural language from artificial life. Artificial life, 8(2):185-215, 2002. 2</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 403Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017. 3</p>
<p>Emergent multiagent communication in the deep learning era. Angeliki Lazaridou, Marco Baroni, arXiv:2006.02419arXiv preprintAngeliki Lazaridou and Marco Baroni. Emergent multi- agent communication in the deep learning era. arXiv preprint arXiv:2006.02419, 2020. 2</p>
<p>Emergence of linguistic communication from referential games with symbolic and pixel input. Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, Stephen Clark, arXiv:1804.03984arXiv preprintAngeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. 4</p>
<p>Multi-agent cooperation and the emergence of. Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni, arXiv:1612.0718224natural) language. arXiv preprintAngeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (nat- ural) language. arXiv preprint arXiv:1612.07182, 2016. 2, 4</p>
<p>Emergent translation in multi-agent communication. Jason Lee, Kyunghyun Cho, Jason Weston, Douwe Kiela, arXiv:1710.06922arXiv preprintJason Lee, Kyunghyun Cho, Jason Weston, and Douwe Kiela. Emergent translation in multi-agent communication. arXiv preprint arXiv:1710.06922, 2017. 4</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461arXiv preprintMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine- jad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. 5</p>
<p>Ease-of-teaching and language structure from emergent communication. Fushan Li, Michael Bowling, Advances in neural information processing systems. 32Fushan Li and Michael Bowling. Ease-of-teaching and lan- guage structure from emergent communication. Advances in neural information processing systems, 32, 2019. 4</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle- moyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 4</p>
<p>Grounding language in play. Corey Lynch, Pierre Sermanet, arXiv:2005.07648arXiv preprintCorey Lynch and Pierre Sermanet. Grounding language in play. arXiv preprint arXiv:2005.07648, 2020. 3</p>
<p>Learning to select and generalize striking movements in robot table tennis. Katharina Mülling, Jens Kober, Oliver Kroemer, Jan Peters, The International Journal of Robotics Research. 323Katharina Mülling, Jens Kober, Oliver Kroemer, and Jan Pe- ters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3):263-279, 2013. 3</p>
<p>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, PMLR, 2022. 5Conference on Robot Learning. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303-1315. PMLR, 2022. 5</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, arXiv:2203.1260156arXiv preprintSuraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. 1, 3, 5, 6</p>
<p>The evolution of language. A Martin, D Nowak, Krakauer, Proceedings of the National Academy of Sciences of the United States of America. the National Academy of Sciences of the United States of America96Martin A. Nowak and D C Krakauer. The evolution of lan- guage. Proceedings of the National Academy of Sciences of the United States of America, 96 14:8028-33, 1999. 2</p>
<p>One-shot high-fidelity imitation: Training large-scale deep nets with rl. Sergio Gómez Tom Le Paine, Ziyu Colmenarejo, Scott Wang, Yusuf Reed, Tobias Aytar, Pfaff, W Matt, Gabriel Hoffman, Serkan Barth-Maron, David Cabi, Budden, arXiv:1810.05017arXiv preprintTom Le Paine, Sergio Gómez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff, Matt W Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden, et al. One-shot high-fidelity imitation: Training large-scale deep nets with rl. arXiv preprint arXiv:1810.05017, 2018. 3</p>
<p>The (un)surprising effectiveness of pretrained vision models for control. Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, Abhinav Gupta, 2022. 5Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The (un)surprising effectiveness of pre- trained vision models for control. international conference on machine learning, 2022. 5</p>
<p>Learning and generalization of motor skills by learning from demonstration. Peter Pastor, Heiko Hoffmann, Tamim Asfour, Stefan Schaal, 2009 IEEE International Conference on Robotics and Automation. IEEEPeter Pastor, Heiko Hoffmann, Tamim Asfour, and Stefan Schaal. Learning and generalization of motor skills by learn- ing from demonstration. In 2009 IEEE International Con- ference on Robotics and Automation, pages 763-768. IEEE, 2009. 3</p>
<p>Interpretation of emergent communication in heterogeneous collaborative embodied agents. Shivansh Patel, Saim Wani, Unnat Jain, Alexander G Schwing, Svetlana Lazebnik, Manolis Savva, Chang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionShivansh Patel, Saim Wani, Unnat Jain, Alexander G Schwing, Svetlana Lazebnik, Manolis Savva, and Angel X Chang. Interpretation of emergent communication in het- erogeneous collaborative embodied agents. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 15953-15963, 2021. 3</p>
<p>Zero-shot visual imitation. Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsDeepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jiten- dra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 2050-2053, 2018. 3</p>
<p>Memory-attended recurrent network for video captioning. Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoyong Shen, Yu-Wing Tai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoy- ong Shen, and Yu-Wing Tai. Memory-attended recurrent net- work for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8347-8356, 2019. 7</p>
<p>Alvinn: An autonomous land vehicle in a neural network. A Dean, Pomerleau, Carnegie-Mellon UniversityTechnical reportDean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Technical report, Carnegie-Mellon Uni- versity, 1989. 3</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 1International Conference on Machine Learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 1</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAI. 7Technical reportAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gen- erative pre-training. Technical report, OpenAI, 2018. 2, 4, 7</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 184Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsu- pervised multitask learners. OpenAI blog, 1(8):9, 2019. 2, 4</p>
<p>Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, Sergey Levine, 2018 IEEE international conference on robotics and automation (ICRA). IEEERouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Bölöni, and Sergey Levine. Vision-based multi-task manipu- lation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE international conference on robotics and automation (ICRA), pages 3758-3765. IEEE, 2018. 3</p>
<p>Mohamed Ettaouil, and Mohammed Amine Janati Idrissi. Multilayer perceptron: Architecture optimization and training. Youssef Hassan Ramchoun, Ghanou, Hassan Ramchoun, Youssef Ghanou, Mohamed Ettaouil, and Mohammed Amine Janati Idrissi. Multilayer perceptron: Architecture optimization and training. 2016. 1, 3, 4, 5</p>
<p>Learning movement primitives. Stefan Schaal, Jan Peters, Jun Nakanishi, Auke Ijspeert, Robotics research. the eleventh international symposium. SpringerStefan Schaal, Jan Peters, Jun Nakanishi, and Auke Ijspeert. Learning movement primitives. In Robotics research. the eleventh international symposium, pages 561-572. Springer, 2005. 3</p>
<p>Languageconditioned imitation learning for robot manipulation tasks. Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, Heni Ben Amor, arXiv:2010.12083arXiv preprintSimon Stepputtis, Joseph Campbell, Mariano Phielipp, Ste- fan Lee, Chitta Baral, and Heni Ben Amor. Language- conditioned imitation learning for robot manipulation tasks. arXiv preprint arXiv:2010.12083, 2020. 3</p>
<p>The physical symbol grounding problem. Paul Vogt, Cognitive Systems Research. 33Paul Vogt. The physical symbol grounding problem. Cogni- tive Systems Research, 3(3):429-457, 2002. 2</p>
<p>Progress in the simulation of emergent communication and language. Kyle Wagner, James A Reggia, Juan Uriagereka, Gerald S Wilkinson, Adaptive Behavior. 112Kyle Wagner, James A. Reggia, Juan Uriagereka, and Ger- ald S. Wilkinson. Progress in the simulation of emergent communication and language. Adaptive Behavior, 11:37 - 69, 2003. 2</p>
<p>Progress in the simulation of emergent communication and language. Kyle Wagner, A James, Juan Reggia, Gerald S Uriagereka, Wilkinson, Adaptive Behavior. 1113Kyle Wagner, James A Reggia, Juan Uriagereka, and Ger- ald S Wilkinson. Progress in the simulation of emergent communication and language. Adaptive Behavior, 11(1):37- 69, 2003. 2, 3</p>
<p>Philosophical investigations. Ludwig Wittgenstein, Macmill an Publish23New YorkLudwig Wittgenstein. Philosophical investigations. New York: Macmill an Publish, 1958. 2, 3</p>
<p>Linking emergent and natural languages via corpus transfer. Shunyu Yao, Mo Yu, Yang Zhang, Joshua B Karthik R Narasimhan, Chuang Tenenbaum, Gan, arXiv:2203.1334423arXiv preprintShunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. arXiv preprint arXiv:2203.13344, 2022. 2, 3</p>
<p>One-shot imitation from observing humans via domain-adaptive metalearning. Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, Sergey Levine, Robotics: Science and Systems. 3Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tian- hao Zhang, Pieter Abbeel, and Sergey Levine. One-shot im- itation from observing humans via domain-adaptive meta- learning. Robotics: Science and Systems, 2018. 3</p>
<p>Metaworld: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, PMLRConference on robot learning. 6Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta- world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094-1100. PMLR, 2020. 1, 2, 5, 6</p>
<p>Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. Tianhao Zhang, Zoe Mccarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, Pieter Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEETianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep imitation learning for complex manipulation tasks from virtual real- ity teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 5628-5635. IEEE, 2018. 3</p>
<p>Watch, try, learn: Metalearning from demonstrations and reward. Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, Chelsea Finn, 2020. 3International Conference on Learning Representations (ICLR. Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. Watch, try, learn: Meta- learning from demonstrations and reward. International Conference on Learning Representations (ICLR), 2020. 3</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3482 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3482</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3482</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-80.html">extraction-schema-80</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <p><strong>Paper ID:</strong> paper-46489933</p>
                <p><strong>Paper Title:</strong> Toward a brain-based componential semantic representation</p>
                <p><strong>Paper Abstract:</strong> ABSTRACT Componential theories of lexical semantics assume that concepts can be represented by sets of features or attributes that are in some sense primitive or basic components of meaning. The binary features used in classical category and prototype theories are problematic in that these features are themselves complex concepts, leaving open the question of what constitutes a primitive feature. The present availability of brain imaging tools has enhanced interest in how concepts are represented in brains, and accumulating evidence supports the claim that these representations are at least partly “embodied” in the perception, action, and other modal neural systems through which concepts are experienced. In this study we explore the possibility of devising a componential model of semantic representation based entirely on such functional divisions in the human brain. We propose a basic set of approximately 65 experiential attributes based on neurobiological considerations, comprising sensory, motor, spatial, temporal, affective, social, and cognitive experiences. We provide normative data on the salience of each attribute for a large set of English nouns, verbs, and adjectives, and show how these attribute vectors distinguish a priori conceptual categories and capture semantic similarity. Robust quantitative differences between concrete object categories were observed across a large number of attribute dimensions. A within- versus between-category similarity metric showed much greater separation between categories than representations derived from distributional (latent semantic) analysis of text. Cluster analyses were used to explore the similarity structure in the data independent of a priori labels, revealing several novel category distinctions. We discuss how such a representation might deal with various longstanding problems in semantic theory, such as feature selection and weighting, representation of abstract concepts, effects of context on semantic retrieval, and conceptual combination. In contrast to componential models based on verbal features, the proposed representation systematically relates semantic content to large-scale brain networks and biologically plausible accounts of concept acquisition.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3482.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3482.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Brain-based componential representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Brain-based componential semantic representation (Binder et al., this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level, embodiment-grounded componential model that represents word/concept meanings as continuous vectors of salience over ~65 experiential/neural attribute dimensions (sensory, motor, spatial, temporal, affective, social, cognitive, drive, attention). Attributes map to modality-specific large-scale neural processors and are intended to provide primitive, neurally-grounded constituents of meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>brain-based componential semantic representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented as graded vectors over a fixed set of experiential/neural attribute dimensions (e.g., Vision, Shape, Motion, Taste, Face, Theory-of-Mind, Valence, Duration). Each attribute corresponds to information processing in a distinguishable large-scale neural system; similarity and combination follow from vector overlap and context-sensitive modulation.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Normative crowdsourced ratings for 535 English words produced 65-dimension attribute vectors that: (1) separate standard ontological categories (artefacts, living things, abstract entities); (2) show strong within-category vs between-category cosine similarity (Cohen's d >> distributional LSA); (3) yield interpretable factor structure (16 factors) and meaningful k-means/hierarchical clusters; and (4) are consistent with prior neuroimaging findings linking modality-specific attributes to corresponding brain networks (e.g., visual/color/shape networks, MT for motion, posterior parahippocampal for scene). The paper also cites fMRI work (Fernandino et al., 2015) showing brain networks modulated by attribute salience.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Open issues include incompleteness of the chosen attribute set, scale/topography mismatches between microscopic neural specializations and macroscopic imaging-accessible processors (e.g., color sub-clusters), cultural variability in some attributes (colour categories), representation of scalar endpoints (hot vs cold), and empirical validation across the whole brain. The model does not yet provide a full mechanistic account of acquisition for all concepts (some concepts learned via language only), and current data are normative ratings rather than direct neural measurements for the full attribute set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with traditional verbal feature-based/componential accounts (which list complex verbal features), the brain-based model defines primitive features as modality-specific neural processors, yielding a closed, biologically constrained feature set. Compared with distributional (LSA) representations, the brain-based vectors produced much larger category separation and are argued to be neurally grounded; LSA captures usage-based similarity but lacks grounding. The model is presented as an extension/operationalization of embodied cognition and as more neurobiologically plausible than localist 'grandmother-cell' accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Unresolved questions include: completeness and optimal granularity of attribute list; how to map attribute vectors to distributed neural activation in individuals; how to represent cross-domain generalizations (e.g., male/female across taxa); how to handle syntactic/compositional constraints beyond simple attribute overlap (role-sensitive composition); and the extent to which language-mediated learning (indirect grounding) can be reconciled with strictly experience-based grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3482.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied cognition (modality-specific representation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied cognition / modality-specific grounded representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of functional-level theories proposing that conceptual knowledge is at least partly represented in the same perception, action, and affective neural systems used to experience the referents (i.e., modality-specific neural substrates contribute to concept representation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Perceptual symbol systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>embodied cognition (modality-specific distributed representation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual representations are distributed across modality-specific neural systems (visual, auditory, somatosensory, motor, affective, social-cognitive), and retrieval partially re-activates or simulates the sensorimotor and affective states associated with experience.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Neuroimaging and neuropsychological studies show activation of modality-specific regions during conceptual tasks (e.g., color areas for color concepts, MT for motion-related concepts, motor areas for action verbs), lesion patterns (sensory-functional dissociations), and better behavioral prediction using modality-based attribute ratings (e.g., Hoffman & Lambon Ralph; Lynott & Connell). The paper also cites studies where attribute salience modulates fMRI responses.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Challenges include accounting for abstract concepts not obviously tied to sensorimotor experience, variability in location and specificity of activations across studies, and cases where modality-specific damage does not neatly predict category deficits. Some modality-specific effects may be epiphenomenal or task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as an alternative to amodal symbolic/verbal feature theories and localist cell-based accounts; the paper argues embodiment provides primitives that are neurally plausible. Embodied accounts are contrasted with distributional/text-based models (which are not grounded in sensorimotor systems) and are integrated into the paper's brain-based componential model.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Key open questions include the neural granularity of modality-specific primitives, how abstract and high-level cognitive concepts are grounded (role of social/affective/cognitive attributes), and the precise causal role of reactivation vs. post-retrieval imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3482.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classical feature-based/category theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classical category theory / binary feature models (Aristotelian approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional view that concepts/categories are defined by sets of binary, necessary-and-sufficient verbal features (e.g., bird = has wings, feathers, beak).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Categories (J. L. Ackrill, Trans.)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>classical feature-based (binary) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented as conjunctions of binary features that are necessary and sufficient for category membership; categorization is rule-like matching of these features.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Historically motivated systematic taxonomies and some analytic concept definitions; intuitive appeal for clear-cut categories.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Paper highlights problems: binary features themselves are complex concepts requiring grounding, leading to combinatorial explosion; inability to identify atomic primitive features; poor fit to fuzzy/prototypical category boundaries (Rosch) and graded similarity effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with prototype and exemplar theories (which allow graded membership) and with the brain-based model (which rejects verbal complex features as primitives in favor of modality-specific neural attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Cannot account for graded typicality, feature learning/acquisition, neural grounding, or productivity in semantic composition without additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3482.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype theory (Rosch et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A componential/analytical approach that represents categories by prototypical feature patterns and allows graded membership depending on similarity to prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Basic objects in natural categories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>prototype representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented as an idealized prototype (a central tendency) and category membership/typicality is determined by similarity to that prototype rather than binary feature rules.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Behavioral findings of graded typicality and faster processing for prototypical members (Rosch et al.); feature production norms showing varying importance of features.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Prototype accounts still rely on verbalized features which may be complex and ungrounded; they do not specify neural primitives and face the feature-selection/grounding problems discussed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Prototype theory addresses graded membership better than classical binary theories but lacks neural grounding; the brain-based model can be seen as providing neurally constrained dimensions that could instantiate prototypes in attribute space.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How prototypes are neurally realized and how prototype formation interacts with modality-specific processors remains open.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3482.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sensory-functional theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensory–functional theory (Warrington & Shallice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuropsychologically motivated theory that explains some category-specific deficits by positing that living things depend more on sensory/perceptual features whereas artefacts depend more on functional/action features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Category specific semantic impairments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>sensory-functional representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Categories differ in the relative importance of sensory (perceptual) versus functional (manipulative/action) information; differential brain damage affecting these systems yields category-selective impairments.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Neuropsychological patient data showing category-specific impairments (e.g., impaired knowledge for living things versus tools) and some neuroimaging/lesion correlations implicating ventral temporal (visual) vs frontoparietal (action) systems.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>The paper notes the sensory-functional dichotomy is overly simple and cannot account for many observed dissociations; categories differ across many attributes (affect, social, auditory, spatial), and patient data show counterexamples inconsistent with a strict dichotomy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Seen as a predecessor to more graded, multi-attribute embodied accounts; the brain-based model extends sensory-functional by specifying ~65 modality- and process-specific attributes rather than a two-way division.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Unclear mapping of many category deficits to only sensory vs functional dimensions; need for more complex multi-attribute accounts and consideration of convergence zones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3482.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Semantics (Jackendoff)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Semantics (Jackendoff, 1990)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representational program that analyzes semantic content in terms of experiential primitives emphasizing space, time, events, and causality as building blocks of propositional content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual Semantics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>conceptual semantics (experiential primitives)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Semantic content is decomposed into primitive experiential concepts (space, time, event, causality, etc.) that form the basis for propositional meaning and syntactic-semantic mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Linguistic analyses showing importance of spatial/temporal/event structure; theoretical plausibility for grounding semantics in experience.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Jackendoff's primitives are descriptive linguistic constructs without explicit mapping to neural processors; the paper suggests neurobiologically constrained lists of experiential attributes (as they propose) may be a more testable instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Jackendoff's approach is close in spirit to the brain-based componential proposal but differs in that Binder et al. emphasize linking primitives explicitly to distinguishable neural systems and provide normative attribute weights.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to operationalize Jackendoff's primitives for neural measurement and whether a limited set of neural processors can fully capture the proposed primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3482.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grandmother-cell / localist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Localist / 'grandmother cell' representation (Barlow, Bowers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesis that individual concepts might be represented by highly selective single neurons or small local neuron ensembles dedicated to particular concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the biological plausibility of grandmother cells: Implications for neural network theories in psychology and neuroscience.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>localist (grandmother-cell) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded by single cells or very small groups of neurons that respond selectively to specific, high-level entities (e.g., one's grandmother), yielding highly localist neural representations.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Single-unit reports in electrophysiology showing highly selective responses to specific faces/people (sparse coding) in some contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Paper argues that even if such cells exist, they do not explain how concepts are learned or why they are organized as they are; localist accounts lack a mechanistic account connecting experience to representation and do not solve grounding/feature-selection problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with distributed modality-specific accounts and the brain-based componential model; the paper favors distributed/attribute-based representations for explanatory power regarding acquisition and organization.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Whether grandmother-like selectivity is the endpoint of distributed learning processes, and how widespread/necessary such cells are for conceptual cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3482.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3482.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional semantics (LSA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional semantics / Latent Semantic Analysis (Landauer & Dumais, LSA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-derived, usage-based vector-space model that represents concepts as vectors derived from word co-occurrence statistics in large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>distributional representation (latent semantic analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented as points in a high-dimensional space whose coordinates derive from statistical patterns of word co-occurrence in text; semantic similarity is measured by vector similarity (e.g., cosine).</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>LSA and related distributional models capture many semantic relations from text, predict human judgments in lexical tasks, and are widely used as computational baselines. In this paper LSA-derived 65-d vectors captured some category structure.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>LSA lacks grounding in sensorimotor or affective experience, performs worse than the brain-based vectors on within-vs-between category separation (paper reports much smaller Cohen's d), and fails in some patient lesion-prediction tasks (cited Crutch et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Directly compared in the paper: distributional LSA vs brain-based attribute vectors — both capture category structure, but brain-based vectors show stronger category clustering and are argued to relate to neural processors. Distributional models remain complementary for capturing linguistic/statistical regularities.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to integrate distributional/textual information with modality-grounded attributes to account for abstract concepts and rapid language-driven acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward a brain-based componential semantic representation', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual Semantics <em>(Rating: 2)</em></li>
                <li>Category specific semantic impairments <em>(Rating: 2)</em></li>
                <li>A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge <em>(Rating: 2)</em></li>
                <li>Perceptual symbol systems <em>(Rating: 2)</em></li>
                <li>Concept representation reflects multimodal abstraction: A framework for embodied semantics <em>(Rating: 2)</em></li>
                <li>On the biological plausibility of grandmother cells: Implications for neural network theories in psychology and neuroscience <em>(Rating: 2)</em></li>
                <li>Basic objects in natural categories <em>(Rating: 1)</em></li>
                <li>Abstract conceptual feature ratings: the role of emotion, magnitude, and other cognitive domains in the organization of abstract conceptual knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3482",
    "paper_id": "paper-46489933",
    "extraction_schema_id": "extraction-schema-80",
    "extracted_data": [
        {
            "name_short": "Brain-based componential representation",
            "name_full": "Brain-based componential semantic representation (Binder et al., this paper)",
            "brief_description": "A functional-level, embodiment-grounded componential model that represents word/concept meanings as continuous vectors of salience over ~65 experiential/neural attribute dimensions (sensory, motor, spatial, temporal, affective, social, cognitive, drive, attention). Attributes map to modality-specific large-scale neural processors and are intended to provide primitive, neurally-grounded constituents of meaning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "brain-based componential semantic representation",
            "theory_description": "Concepts are represented as graded vectors over a fixed set of experiential/neural attribute dimensions (e.g., Vision, Shape, Motion, Taste, Face, Theory-of-Mind, Valence, Duration). Each attribute corresponds to information processing in a distinguishable large-scale neural system; similarity and combination follow from vector overlap and context-sensitive modulation.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Normative crowdsourced ratings for 535 English words produced 65-dimension attribute vectors that: (1) separate standard ontological categories (artefacts, living things, abstract entities); (2) show strong within-category vs between-category cosine similarity (Cohen's d &gt;&gt; distributional LSA); (3) yield interpretable factor structure (16 factors) and meaningful k-means/hierarchical clusters; and (4) are consistent with prior neuroimaging findings linking modality-specific attributes to corresponding brain networks (e.g., visual/color/shape networks, MT for motion, posterior parahippocampal for scene). The paper also cites fMRI work (Fernandino et al., 2015) showing brain networks modulated by attribute salience.",
            "counter_evidence_or_challenges": "Open issues include incompleteness of the chosen attribute set, scale/topography mismatches between microscopic neural specializations and macroscopic imaging-accessible processors (e.g., color sub-clusters), cultural variability in some attributes (colour categories), representation of scalar endpoints (hot vs cold), and empirical validation across the whole brain. The model does not yet provide a full mechanistic account of acquisition for all concepts (some concepts learned via language only), and current data are normative ratings rather than direct neural measurements for the full attribute set.",
            "comparison_to_other_theories": "Contrasted with traditional verbal feature-based/componential accounts (which list complex verbal features), the brain-based model defines primitive features as modality-specific neural processors, yielding a closed, biologically constrained feature set. Compared with distributional (LSA) representations, the brain-based vectors produced much larger category separation and are argued to be neurally grounded; LSA captures usage-based similarity but lacks grounding. The model is presented as an extension/operationalization of embodied cognition and as more neurobiologically plausible than localist 'grandmother-cell' accounts.",
            "notable_limitations_or_open_questions": "Unresolved questions include: completeness and optimal granularity of attribute list; how to map attribute vectors to distributed neural activation in individuals; how to represent cross-domain generalizations (e.g., male/female across taxa); how to handle syntactic/compositional constraints beyond simple attribute overlap (role-sensitive composition); and the extent to which language-mediated learning (indirect grounding) can be reconciled with strictly experience-based grounding.",
            "uuid": "e3482.0",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Embodied cognition (modality-specific representation)",
            "name_full": "Embodied cognition / modality-specific grounded representation",
            "brief_description": "A family of functional-level theories proposing that conceptual knowledge is at least partly represented in the same perception, action, and affective neural systems used to experience the referents (i.e., modality-specific neural substrates contribute to concept representation).",
            "citation_title": "Perceptual symbol systems",
            "mention_or_use": "mention",
            "theory_name": "embodied cognition (modality-specific distributed representation)",
            "theory_description": "Conceptual representations are distributed across modality-specific neural systems (visual, auditory, somatosensory, motor, affective, social-cognitive), and retrieval partially re-activates or simulates the sensorimotor and affective states associated with experience.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Neuroimaging and neuropsychological studies show activation of modality-specific regions during conceptual tasks (e.g., color areas for color concepts, MT for motion-related concepts, motor areas for action verbs), lesion patterns (sensory-functional dissociations), and better behavioral prediction using modality-based attribute ratings (e.g., Hoffman & Lambon Ralph; Lynott & Connell). The paper also cites studies where attribute salience modulates fMRI responses.",
            "counter_evidence_or_challenges": "Challenges include accounting for abstract concepts not obviously tied to sensorimotor experience, variability in location and specificity of activations across studies, and cases where modality-specific damage does not neatly predict category deficits. Some modality-specific effects may be epiphenomenal or task-dependent.",
            "comparison_to_other_theories": "Presented as an alternative to amodal symbolic/verbal feature theories and localist cell-based accounts; the paper argues embodiment provides primitives that are neurally plausible. Embodied accounts are contrasted with distributional/text-based models (which are not grounded in sensorimotor systems) and are integrated into the paper's brain-based componential model.",
            "notable_limitations_or_open_questions": "Key open questions include the neural granularity of modality-specific primitives, how abstract and high-level cognitive concepts are grounded (role of social/affective/cognitive attributes), and the precise causal role of reactivation vs. post-retrieval imagery.",
            "uuid": "e3482.1",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Classical feature-based/category theory",
            "name_full": "Classical category theory / binary feature models (Aristotelian approach)",
            "brief_description": "A traditional view that concepts/categories are defined by sets of binary, necessary-and-sufficient verbal features (e.g., bird = has wings, feathers, beak).",
            "citation_title": "Categories (J. L. Ackrill, Trans.)",
            "mention_or_use": "mention",
            "theory_name": "classical feature-based (binary) representation",
            "theory_description": "Concepts are represented as conjunctions of binary features that are necessary and sufficient for category membership; categorization is rule-like matching of these features.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Historically motivated systematic taxonomies and some analytic concept definitions; intuitive appeal for clear-cut categories.",
            "counter_evidence_or_challenges": "Paper highlights problems: binary features themselves are complex concepts requiring grounding, leading to combinatorial explosion; inability to identify atomic primitive features; poor fit to fuzzy/prototypical category boundaries (Rosch) and graded similarity effects.",
            "comparison_to_other_theories": "Contrasted with prototype and exemplar theories (which allow graded membership) and with the brain-based model (which rejects verbal complex features as primitives in favor of modality-specific neural attributes).",
            "notable_limitations_or_open_questions": "Cannot account for graded typicality, feature learning/acquisition, neural grounding, or productivity in semantic composition without additional mechanisms.",
            "uuid": "e3482.2",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Prototype theory",
            "name_full": "Prototype theory (Rosch et al.)",
            "brief_description": "A componential/analytical approach that represents categories by prototypical feature patterns and allows graded membership depending on similarity to prototypes.",
            "citation_title": "Basic objects in natural categories",
            "mention_or_use": "mention",
            "theory_name": "prototype representation",
            "theory_description": "Concepts are represented as an idealized prototype (a central tendency) and category membership/typicality is determined by similarity to that prototype rather than binary feature rules.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Behavioral findings of graded typicality and faster processing for prototypical members (Rosch et al.); feature production norms showing varying importance of features.",
            "counter_evidence_or_challenges": "Prototype accounts still rely on verbalized features which may be complex and ungrounded; they do not specify neural primitives and face the feature-selection/grounding problems discussed in the paper.",
            "comparison_to_other_theories": "Prototype theory addresses graded membership better than classical binary theories but lacks neural grounding; the brain-based model can be seen as providing neurally constrained dimensions that could instantiate prototypes in attribute space.",
            "notable_limitations_or_open_questions": "How prototypes are neurally realized and how prototype formation interacts with modality-specific processors remains open.",
            "uuid": "e3482.3",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Sensory-functional theory",
            "name_full": "Sensory–functional theory (Warrington & Shallice)",
            "brief_description": "A neuropsychologically motivated theory that explains some category-specific deficits by positing that living things depend more on sensory/perceptual features whereas artefacts depend more on functional/action features.",
            "citation_title": "Category specific semantic impairments",
            "mention_or_use": "mention",
            "theory_name": "sensory-functional representation",
            "theory_description": "Categories differ in the relative importance of sensory (perceptual) versus functional (manipulative/action) information; differential brain damage affecting these systems yields category-selective impairments.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Neuropsychological patient data showing category-specific impairments (e.g., impaired knowledge for living things versus tools) and some neuroimaging/lesion correlations implicating ventral temporal (visual) vs frontoparietal (action) systems.",
            "counter_evidence_or_challenges": "The paper notes the sensory-functional dichotomy is overly simple and cannot account for many observed dissociations; categories differ across many attributes (affect, social, auditory, spatial), and patient data show counterexamples inconsistent with a strict dichotomy.",
            "comparison_to_other_theories": "Seen as a predecessor to more graded, multi-attribute embodied accounts; the brain-based model extends sensory-functional by specifying ~65 modality- and process-specific attributes rather than a two-way division.",
            "notable_limitations_or_open_questions": "Unclear mapping of many category deficits to only sensory vs functional dimensions; need for more complex multi-attribute accounts and consideration of convergence zones.",
            "uuid": "e3482.4",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Conceptual Semantics (Jackendoff)",
            "name_full": "Conceptual Semantics (Jackendoff, 1990)",
            "brief_description": "A representational program that analyzes semantic content in terms of experiential primitives emphasizing space, time, events, and causality as building blocks of propositional content.",
            "citation_title": "Conceptual Semantics",
            "mention_or_use": "mention",
            "theory_name": "conceptual semantics (experiential primitives)",
            "theory_description": "Semantic content is decomposed into primitive experiential concepts (space, time, event, causality, etc.) that form the basis for propositional meaning and syntactic-semantic mapping.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Linguistic analyses showing importance of spatial/temporal/event structure; theoretical plausibility for grounding semantics in experience.",
            "counter_evidence_or_challenges": "Jackendoff's primitives are descriptive linguistic constructs without explicit mapping to neural processors; the paper suggests neurobiologically constrained lists of experiential attributes (as they propose) may be a more testable instantiation.",
            "comparison_to_other_theories": "Jackendoff's approach is close in spirit to the brain-based componential proposal but differs in that Binder et al. emphasize linking primitives explicitly to distinguishable neural systems and provide normative attribute weights.",
            "notable_limitations_or_open_questions": "How to operationalize Jackendoff's primitives for neural measurement and whether a limited set of neural processors can fully capture the proposed primitives.",
            "uuid": "e3482.5",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Grandmother-cell / localist",
            "name_full": "Localist / 'grandmother cell' representation (Barlow, Bowers)",
            "brief_description": "A hypothesis that individual concepts might be represented by highly selective single neurons or small local neuron ensembles dedicated to particular concepts.",
            "citation_title": "On the biological plausibility of grandmother cells: Implications for neural network theories in psychology and neuroscience.",
            "mention_or_use": "mention",
            "theory_name": "localist (grandmother-cell) representation",
            "theory_description": "Concepts are encoded by single cells or very small groups of neurons that respond selectively to specific, high-level entities (e.g., one's grandmother), yielding highly localist neural representations.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Single-unit reports in electrophysiology showing highly selective responses to specific faces/people (sparse coding) in some contexts.",
            "counter_evidence_or_challenges": "Paper argues that even if such cells exist, they do not explain how concepts are learned or why they are organized as they are; localist accounts lack a mechanistic account connecting experience to representation and do not solve grounding/feature-selection problems.",
            "comparison_to_other_theories": "Contrasted with distributed modality-specific accounts and the brain-based componential model; the paper favors distributed/attribute-based representations for explanatory power regarding acquisition and organization.",
            "notable_limitations_or_open_questions": "Whether grandmother-like selectivity is the endpoint of distributed learning processes, and how widespread/necessary such cells are for conceptual cognition.",
            "uuid": "e3482.6",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Distributional semantics (LSA)",
            "name_full": "Distributional semantics / Latent Semantic Analysis (Landauer & Dumais, LSA)",
            "brief_description": "A text-derived, usage-based vector-space model that represents concepts as vectors derived from word co-occurrence statistics in large corpora.",
            "citation_title": "A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.",
            "mention_or_use": "use",
            "theory_name": "distributional representation (latent semantic analysis)",
            "theory_description": "Concepts are represented as points in a high-dimensional space whose coordinates derive from statistical patterns of word co-occurrence in text; semantic similarity is measured by vector similarity (e.g., cosine).",
            "level_of_analysis": "functional",
            "supporting_evidence": "LSA and related distributional models capture many semantic relations from text, predict human judgments in lexical tasks, and are widely used as computational baselines. In this paper LSA-derived 65-d vectors captured some category structure.",
            "counter_evidence_or_challenges": "LSA lacks grounding in sensorimotor or affective experience, performs worse than the brain-based vectors on within-vs-between category separation (paper reports much smaller Cohen's d), and fails in some patient lesion-prediction tasks (cited Crutch et al.).",
            "comparison_to_other_theories": "Directly compared in the paper: distributional LSA vs brain-based attribute vectors — both capture category structure, but brain-based vectors show stronger category clustering and are argued to relate to neural processors. Distributional models remain complementary for capturing linguistic/statistical regularities.",
            "notable_limitations_or_open_questions": "How to integrate distributional/textual information with modality-grounded attributes to account for abstract concepts and rapid language-driven acquisition.",
            "uuid": "e3482.7",
            "source_info": {
                "paper_title": "Toward a brain-based componential semantic representation",
                "publication_date_yy_mm": "2016-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual Semantics",
            "rating": 2,
            "sanitized_title": "conceptual_semantics"
        },
        {
            "paper_title": "Category specific semantic impairments",
            "rating": 2,
            "sanitized_title": "category_specific_semantic_impairments"
        },
        {
            "paper_title": "A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
            "rating": 2,
            "sanitized_title": "a_solution_to_platos_problem_the_latent_semantic_analysis_theory_of_acquisition_induction_and_representation_of_knowledge"
        },
        {
            "paper_title": "Perceptual symbol systems",
            "rating": 2,
            "sanitized_title": "perceptual_symbol_systems"
        },
        {
            "paper_title": "Concept representation reflects multimodal abstraction: A framework for embodied semantics",
            "rating": 2,
            "sanitized_title": "concept_representation_reflects_multimodal_abstraction_a_framework_for_embodied_semantics"
        },
        {
            "paper_title": "On the biological plausibility of grandmother cells: Implications for neural network theories in psychology and neuroscience",
            "rating": 2,
            "sanitized_title": "on_the_biological_plausibility_of_grandmother_cells_implications_for_neural_network_theories_in_psychology_and_neuroscience"
        },
        {
            "paper_title": "Basic objects in natural categories",
            "rating": 1,
            "sanitized_title": "basic_objects_in_natural_categories"
        },
        {
            "paper_title": "Abstract conceptual feature ratings: the role of emotion, magnitude, and other cognitive domains in the organization of abstract conceptual knowledge",
            "rating": 1,
            "sanitized_title": "abstract_conceptual_feature_ratings_the_role_of_emotion_magnitude_and_other_cognitive_domains_in_the_organization_of_abstract_conceptual_knowledge"
        }
    ],
    "cost": 0.022266,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward a brain-based componential semantic representation</p>
<p>Jeffrey R Binder jbinder@mcw.edu 
Department of Neurology
Medical College of Wisconsin
MilwaukeeWIUSA</p>
<p>Lisa L Conant 
Department of Neurology
Medical College of Wisconsin
MilwaukeeWIUSA</p>
<p>Colin J Humphries 
Department of Neurology
Medical College of Wisconsin
MilwaukeeWIUSA</p>
<p>Leonardo Fernandino 
Department of Neurology
Medical College of Wisconsin
MilwaukeeWIUSA</p>
<p>Stephen B Simons 
Teledyne Scientific, LLC
DurhamNCUSA</p>
<p>Mario Aguilar 
Teledyne Scientific, LLC
DurhamNCUSA</p>
<p>Rutvik H Desai 
Department of Psychology
University of South Carolina
ColumbiaSCUSA</p>
<p>Toward a brain-based componential semantic representation
28E1DDBBA88C8EF17BCF4E4A211E61C710.1080/02643294.2016.1147426Received 14 June 2015 Revised 24 November 2015 Accepted 21 January 2016Semanticsconcept representationembodied cognitioncognitive neuroscience
Componential theories of lexical semantics assume that concepts can be represented by sets of features or attributes that are in some sense primitive or basic components of meaning.The binary features used in classical category and prototype theories are problematic in that these features are themselves complex concepts, leaving open the question of what constitutes a primitive feature.The present availability of brain imaging tools has enhanced interest in how concepts are represented in brains, and accumulating evidence supports the claim that these representations are at least partly "embodied" in the perception, action, and other modal neural systems through which concepts are experienced.In this study we explore the possibility of devising a componential model of semantic representation based entirely on such functional divisions in the human brain.We propose a basic set of approximately 65 experiential attributes based on neurobiological considerations, comprising sensory, motor, spatial, temporal, affective, social, and cognitive experiences.We provide normative data on the salience of each attribute for a large set of English nouns, verbs, and adjectives, and show how these attribute vectors distinguish a priori conceptual categories and capture semantic similarity.Robust quantitative differences between concrete object categories were observed across a large number of attribute dimensions.A within-versus between-category similarity metric showed much greater separation between categories than representations derived from distributional (latent semantic) analysis of text.Cluster analyses were used to explore the similarity structure in the data independent of a priori labels, revealing several novel category distinctions.We discuss how such a representation might deal with various longstanding problems in semantic theory, such as feature selection and weighting, representation of abstract concepts, effects of context on semantic retrieval, and conceptual combination.In contrast to componential models based on verbal features, the proposed representation systematically relates semantic content to largescale brain networks and biologically plausible accounts of concept acquisition.</p>
<p>Introduction</p>
<p>Theories of semantic representation generally begin with analyses of conceptual content.This analytical or componential approach assumes that concepts can ultimately be represented by sets of features or attributes that are in some sense primitive or basic components of meaning.The present availability of brain imaging tools has enhanced interest in how concepts are represented in human neural systems, and accumulating evidence supports the claim that these representations are at least partly "embodied" in the perception, action, and other modal neural systems through which concepts are acquired (Binder &amp; Desai, 2011;Kiefer &amp; Pulvermüller, 2012;Meteyard, Rodriguez Cuadrado, Bahrami, &amp; Vigliocco, 2012).In this paper we explore the possibility of devising a sufficiently explanatory componential model of semantic representation based entirely on such functional divisions in the human brain.</p>
<p>An essential starting point is to define what types of entities are the components of such a representation and how these differ from those of more standard semantic theories.In classical category theory, concepts are defined by the presence or absence of binary features that are necessary and sufficient for category identification (Aristotle, 1995/350 BCE).The concept of bird, for example, is composed of such features as wings, feathers, beak, flying, egg-laying, and so on.An important extension of classical theory was the realization that most categories have somewhat fuzzy boundaries due to varying degrees of prototypicality among members (Rosch, Mervis, Gray, Johnson, &amp; Boyes-Braem, 1976).Thus a penguin could still be identified (though more slowly) as a bird, despite the fact that it does not fly, through a process of jointly weighing all of its features.Extensive work using feature generation tasks has documented what people think of as the features of entities, and how these features rank in importance for a given concept (Cree &amp; McRae, 2003;Garrard, Lambon Ralph, Hodges, &amp; Patterson, 2001;McRae, de Sa, &amp; Seidenberg, 1997;Vinson, Vigliocco, Cappa, &amp; Siri, 2003).Such data provide a powerful means of assessing degrees of similarity between concepts and of grouping concepts into hierarchical categories on the basis of this similarity structure.</p>
<p>A limitation that applies to these standard approaches, however, is that the features they employ to define conceptual content are typically also complex concepts.Physical features like wings, feathers, and beaks are components in the sense that they are parts of a larger entity, but they are no more primitive than the larger entities they define.Like the larger entities, these parts are concepts that must be learned through perceptual and verbal experience, and each has its own set of defining features, which in turn have their own defining features, and so on.The resulting combinatorial explosion presents a bracing problem for constraining featurebased theories.Across even the closed set of known physical objects, the possible set of physical features is extremely large, and the same is true for motion and action features such as flying, swimming, running, jumping, eating, singing, barking, howling, and so on.The inability of standard verbal featurebased theories to locate a set of atomic features from which all others are derived places hard limits on the explanatory value of such approaches in several areas.</p>
<p>One of these limitations, and the one of central concern here, is the lack of any known relationship between verbal features and neurobiological mechanisms.There are no neural systems specifically dedicated to representing feathers, wings, beaks, or flight, nor is it plausible that such dedicated neural systems exist for every conceivable feature.Even if it turns out that the brain representation of features and other complex concepts includes neurons or local neuron ensembles dedicated to a single concept (Barlow, 1972;Bowers, 2009), the mere existence of such "grandmother cells" in itself provides no account of the means by which external or internal stimuli lead to their activation.Without such a mechanistic account, the claim that each possible concept is represented in the brain by a cell or group of cells adds nothing to our understanding of concept representations beyond a mere listing of the concepts.What we seek instead is an understanding of why concepts are organized in the brain in the way that they actually are organized.What facts about the brain determine whether a concept is represented by one group of cells rather than another group?A closely related problem, for which feature-based conceptual representations are similarly limited in providing an answer, is the question of how features and other concepts come to be learned by the brain.The idea that concepts are represented in the brain by abstract "concept cells" also fails to address the deeper question of how concepts can be understood without a "grounding" in sensory-motor experience that enables reference to the external environment and the phenomenological qualia of conscious thought (Harnad, 1990).</p>
<p>The limitations of traditional feature-based semantic theories for understanding concept representation in the brain are, however, a direct reflection of what these theories aim to achieve, which is to describe the things that exist, their similarity structure, and their groupings into categories.The question of how such information is organized in the brain is outside the domain of these aims, so it should not be surprising that these theories have the limitations they have.In contrast, embodiment theories of knowledge representation provide a fairly straightforward analysis of conceptual content in terms of sensory, motor, affective, and other experiential phenomena and their corresponding modality-specific neural representations.In the theory outlined here, these neurobiologically defined "experiential attributes" provide a set of primitive features for the analysis of conceptual content, while simultaneously grounding concepts in experience and providing a mechanistic account of concept acquisition.</p>
<p>Defining conceptual features in terms of neural processes represents a radical departure from traditional verbal feature analysis.Prior work along these lines includes the Conceptual Semantics program of Jackendoff (Jackendoff, 1990), which conceives of semantic content in terms of experiential primitives, emphasizing in particular the role of space, time, event, and causality phenomena in understanding propositional content.Other related early work includes the sensory-functional theory proposed by Warrington and colleagues to account for categoryrelated object processing impairments in neurological patients (Warrington &amp; Shallice, 1984).In this theory, the relevant semantic content of physical entities is essentially reduced to two experiential attributes processed in two distinct brain networks.Subsequent work has generally recognized the limitations of a simple sensory-functional dichotomy (Allport, 1985;Warrington &amp; McCarthy, 1987), leading to investigation of an ever-expanding list of sensory, motor, and affective attributes of concepts and the neural correlates of these attributes.</p>
<p>Studies of modality-specific contributions to conceptual knowledge have usually focused on a single attribute or small set of attributes.Lynott and Connell collected normative ratings for a sample of 423 concrete adjectives that describe object properties (Lynott &amp; Connell, 2009) and 400 nouns (Lynott &amp; Connell, 2013) on strength of association with each of the five primary sensory modalities.Participants were asked to rate how strongly they experienced a particular concept by hearing, tasting, feeling through touch, smelling, and seeing.Gainotti et al. (Gainotti, Ciaraffa, Silveri, &amp; Marra, 2009;Gainotti, Spinelli, Scaricamazza, &amp; Marra, 2013) expanded on this set by dividing the visual domain into three dimensions (shape, colour, and motion) and adding manipulation experience as a motor dimension.Ratings were obtained on 49 animal, plant, and artefact concepts, revealing highly significant differences between categories in the perceived relevance of these sensory-motor dimensions to each concept.Using a similar set of dimensions, Hoffman and Lambon Ralph (Hoffman &amp; Lambon Ralph, 2013) obtained strength of association ratings for a set of 160 object nouns.Participants were asked, "How much do you associate this item with a particular (colour, visual form, observed motion, sound, tactile sensation, taste, smell, action)?".The resulting attribute ratings predicted lexical-semantic processing speed more accurately than verbal feature-based representations (McRae, Cree, Seidenberg, &amp; McNorgan, 2005), and they supported a novel conceptual distinction between mechanical devices (e.g., vehicles) and other non-living objects, in that the former had strong sound and motion characteristics that made them more similar to animals.</p>
<p>Although prior work in this area has focused overwhelmingly on concrete object and action concepts, more recent studies explore dimensions of experience that may be more important in the representation of abstract concepts.Several authors have emphasized the role of affective and social experiences in abstract concept acquisition and embodiment (Borghi, Flumini, Cimatti, Marocco, &amp; Scorolli, 2011;Kousta, Vigliocco, Vinson, Andrews, &amp; Del Campo, 2011;Vigliocco, Meteyard, Andrews, &amp; Kousta, 2009;Wiemer-Hastings &amp; Xu, 2005;Zdrazilova &amp; Pexman, 2013).Crutch et al. (Crutch, Williams, Ridgway, &amp; Borgenicht, 2012) obtained ratings for 200 abstract and 200 concrete nouns on the relatedness of each word to concepts of time, space, quantity, emotion, polarity (positive or negative), social interaction, morality, and thought, in addition to the more physical dimensions of sensation and action.These representations successfully predicted performance by a severely aphasic patient on an abstract noun comprehension task in which semantic similarity between target and foil responses was manipulated, whereas similarity metrics (latent semantic analysis cosine similarity) based on patterns of word usage (Landauer &amp; Dumais, 1997) were not predictive (Crutch, Troche, Reilly, &amp; Ridgway, 2013).Troche et al. (Troche, Crutch, &amp; Reilly, 2014) provide further analyses of these representations, identifying three latent factors (labelled perceptual salience, affective association, and magnitude) that accounted for 81% of the variance.Abstract nouns were rated higher than concrete nouns on the dimensions of emotion, polarity, social interaction, morality, and space.</p>
<p>Building on this prior work, our aim in the present study was to develop a more comprehensive conceptual representation based on known modalities of neural information processing.This more comprehensive representation would ideally capture aspects of experience that are central to the acquisition of event concepts as well as object concepts, and abstract as well as concrete concepts.The resulting representation, described in the following section, contains entries corresponding to specialized sensory and motor processes; affective processes; systems for processing spatial, temporal, and causal information; social cognition processes; and abstract cognitive operations.We hope this approach stimulates further empirical and theoretical efforts toward a comprehensive brain-based semantic theory.</p>
<p>Neural components of experience</p>
<p>The following section briefly describes the proposed components of a brain-based semantic representation, which are listed in Tables 1 and 2. Each of these components is defined by an extensive body of physiological evidence that can only be hinted at here.Examples are provided of how each component applies to a range of concepts.Two fundamental principles guided selection of the components.First, we assume that all aspects of mental experience can contribute to concept acquisition and therefore concept composition.These "aspects of mental experience" include not only sensory perceptions, but also affective responses to entities and situations, experience with performing motor actions, perception of spatial and temporal phenomena, perception of causality, experience with social phenomena, and experience with internal cognitive phenomena and drive states.Second, we focus on experiential phenomena  for which there are likely to be corresponding distinguishable neural processors, drawing on evidence from animal physiology, brain imaging, and neurological studies.In particular, we focus on macroscopic neural systems that can be distinguished with in vivo human imaging methods.We do not require or propose that these neural processors are self-contained "modules" or that they are highly localized in the brain, only that they process information that primarily represents a particular aspect of experience.</p>
<p>A detailed listing of the queries used to elicit association ratings for each component is available for download (see link prior to the References section).</p>
<p>Visual components</p>
<p>Visual and other sensory components are listed in Table 1.Components of visual experience for which there are likely to be corresponding distinguishable neural processors include luminance, size, colour, visual texture, visual shape, visual motion, and biological motion.Within the visual shape perception network are distinguishable networks that primarily process faces, human body parts, and three-dimensional spaces.</p>
<p>Luminance is a basic property of vision, but is also relevant to such concepts as sun, light, gleam, shine, ink, night, dark, black, and so on, that are defined by brightness or darkness.Luminance is an example of a scalar quantity-that is, one that represents a continuous one-dimensional variable.Linguistic representation of scalars tends to focus on ends of the continuum (e.g., bright/dark, hot/cold, heavy/light, loud/soft, etc.), which raises a general question of whether or to what extent opposite ends of a scalar (e.g., bright and dark) are represented in distinguishable neural processors.Are there non-identical neural networks that encode high and low luminance?</p>
<p>The answer depends to some degree on the spatial scale in question.It is known that some neurons respond somewhat selectively to stimuli with high luminance while others respond more to stimuli with low luminance (i.e., ON and OFF cells in retina and V1), resulting in two networks that are distinguishable at a microscopic scale.If these and other luminancetuned neurons intermingle within a single network, however, the high-and low-luminance networks might be indistinguishable, at least at the macroscopic scale of in vivo imaging.</p>
<p>In contrast, the scalar quantity visual size is linked to large-scale retinotopic organization throughout the visual cortical system and has been proposed as a major determinant of medial-to-lateral organization even at the highest levels of the ventral visual stream (Hasson, Levy, Behrmann, Hendler, &amp; Malach, 2002).At these higher levels, medial-to-lateral organization appears not to depend on the actual retinal extent of a given object exemplar, which varies with distance from the observer, but rather on a "typical" or generalized perceptual representation.Images of buildings, for example, produce stronger activation in medial regions than do images of insects, and the opposite pattern holds for lateral regions, even when the images are the same physical size (Konkle &amp; Oliva, 2012).By extension, the general theory that concepts are represented partly in perceptual systems would predict similar medial-lateral activation differences in the ventral visual stream for concepts representing objects that are reliably large or reliably small.</p>
<p>There is now strong evidence not only for a fairly specialized colour perception network in the human brain (Bartels &amp; Zeki, 2000;Beauchamp, Haxby, Jennings, &amp; DeYoe, 1999), but also for activation in or near this processor by concepts with colour content (Hsu, Frankland, &amp; Thompson-Schill, 2012;Hsu, Kraemer, Oliver, Schlichting, &amp; Thompson-Schill, 2011;Kellenbach, Brett, &amp; Patterson, 2001;Martin, Haxby, Lalonde, Wiggs, &amp; Ungerleider, 1995;Simmons et al., 2007).The question of whether there are distinguishable neural processors activated by particular colour concepts (e.g., red vs. green vs. blue) has not been studied, but this appears unlikely.Coloursensitive neurons in the monkey inferior temporal cortex form a network of millimetre-sized "globs", within which there is evidence for spatial clustering of neurons by colour preference (Conway &amp; Tsao, 2009).However, the spatial scale of these clusters is on the order of 50-100 µm, beyond the spatial resolution of current in vivo imaging methods.More importantly, it is not clear that the spatial organization of colour concept representations should necessarily reflect this perceptual organization.Division of the visible light frequency spectrum into colour concepts varies considerably across cultures (Berlin &amp; Kay, 1969), whereas the spatial organization of colour-sensitive neurons presumably does not.As with the attribute of luminance, we have assumed for these reasons that a single neural processor encodes the colour content of concepts, regardless of the particular colour in question.A concept like lemon that is reliably associated with a particular colour is expected to activate this neural processor to approximately the same degree as a concept like coffee that is reliably associated with a different colour.</p>
<p>Visual motion perception involves a relatively specialized neural processor known as MT or V5 (Albright, Desimone, &amp; Gross, 1984;Zeki, 1974).Responses in MT vary with motion velocity and degree of motion coherence (Lingnau, Ashida, Wall, &amp; Smith, 2009;Rees, Friston, &amp; Koch, 2000;Tootell et al., 1995).Motion velocity is relevant for our purposes, as it is strongly represented at a conceptual level.For example, movements may be relatively fast or slow in velocity, and this scalar quantity is central to action concepts like run, dash, zoom, creep, ooze, walk, and so on, as well as entity concepts like bullet, jet, hare, snail, tortoise, sloth, and so on.Movements may also have a specific pattern or quality, as illustrated, for example, by the large number of verbs that express manner of motion (turn, bounce, roll, float, spin, twist, etc.).We are not aware of any evidence for large-scale spatial organization in the cortex according to type or manner of motion; therefore the proposed semantic representation is designed to capture strength of association with visually observable characteristic motion in general, regardless of specific type.The exception to this rule is the perception of biological motion, which has been linked with a neural processor that is clearly distinct from MT, located in the posterior superior temporal sulcus (STS; Grossman &amp; Blake, 2002;Pelphrey, Morris, Michelich, Allison, &amp; McCarthy, 2005).Biological motion contains higher order complexity that is characteristic of face and body part actions.Perception of biological motion plays a central role in understanding face and body gestures and thus the affective and intentional state of other agents (i.e., theory of mind; Allison, Puce, &amp; McCarthy, 2000).Biological motion is thus one of several key attributes distinguishing intentional agents (boy, girl, woman, man, etc.) from inanimate entities.</p>
<p>Visual shape perception involves an extensive region of extrastriate cortex, including a large lateral extrastriate area known as the lateral occipital complex (Malach et al., 1995) and an adjacent area on the ventral occipital-temporal surface known as VOT (Grill-Spector &amp; Malach, 2004).In human functional magnetic resonance imaging (fMRI) studies these areas respond more strongly to images of objects than to images in which shape contours of the objects have been disrupted ("scrambled" images), similar to response patterns observed in primate ventral temporal neurons (Tanaka, Saito, Fukada, &amp; Moriya, 1991).Shape information is generally the most important attribute of concrete object concepts and distinguishes concrete concepts from a range of abstract (e.g., affective, social, and cognitive) entities and event concepts.Variation in the salience of visual shape also occurs within the domain of concrete entities.Shape is typically not a defining feature of substance nouns (metal, plastic, water), but it has relevance for some substance-like mass nouns (butter, coffee, rice).Concrete objects also vary in shape complexity.Animals, for example, tend to have more complex shapes than plants or tools (Snodgrass &amp; Vanderwart, 1980).We hypothesize that both shape salience and shape complexity determine the degree of activation in shape processing regions during concept retrieval.</p>
<p>More focal areas within or adjacent to these shape processing regions show preferential responses to human faces (Kanwisher, McDermott, &amp; Chun, 1997;Schwarzlose, Baker, &amp; Kanwisher, 2005) and body parts (Downing, Jiang, Shuman, &amp; Kanwisher, 2001).A specialized mechanism for face perception is consistent with the central role of face recognition in social interactions.Visual perception of body parts probably plays a crucial role in understanding observed actions and mapping these onto internal representations of the body schema during action learning.These processors would be differentially activated by concepts pertaining to faces (nose, mouth, portrait) and body parts (hand, leg, finger).As a first approximation, we propose that both of these neural processors are activated to some degree by all concepts referring to human beings, which by necessity include a face and other human body parts.Activation of the face processor may be minimal in the case of concepts that represent general human types and roles (boy, man, nurse, etc.), as these concepts do not include information about any specific face, whereas concepts referencing specific individuals (brother, father, Einstein) might include specific facial feature information and therefore activate the face processor to a greater degree.</p>
<p>Perception of three-dimensional spaces involves a specialized neural processor in the posterior parahippocampal region (Epstein &amp; Kanwisher, 1998;Epstein, Parker, &amp; Feiler, 2007).Although this processor is typically studied using visual stimuli and is often considered a high-level visual area, it more likely integrates visual, proprioceptive, and perhaps auditory inputs, all of which provide correlated information about three-dimensional space.Further discussion of this component of experience is provided in the section below on Spatial Components (Table 2).</p>
<p>Somatosensory components</p>
<p>Somatosensory networks of the cortex represent body location (somatotopic representation), body position and joint force (proprioception), pain, surface characteristics of objects (texture and temperature), and object shape (Friebel, Eickhoff, &amp; Lotze, 2011;Hendry, Hsiao, &amp; Bushnell, 1999;Lamm, Decety, &amp; Singer, 2011;Longo, Azanon, &amp; Haggard, 2010;McGlone &amp; Reilly, 2010;Medina &amp; Coslett, 2010;Serino &amp; Haggard, 2010).Somatotopy is an inherently multi-modal phenomenon, because the body location touched by a particular object is typically the same body location as that used during exploratory or manipulative motor actions involving the object.Conceptual and linguistic representations are more likely to encode the action as the salient characteristic of the object rather than the tactile experience (we say "I threw the ball" to describe the event, not "the ball touched my hand"), therefore somatotopic knowledge was encoded in the proposed representation using action-based rather than somatosensory attributes (see Motor Components).</p>
<p>Temperature, tactile texture, and pain were included as components of the representation, as was a component referring to the salience of object weight.Weight is perceived mainly via proprioceptive representations and is a salient attribute of objects with which we interact.Temperature and weight are two more examples of scalar entities that may or may not have a macroscopic topography in the brain (Hendry et al., 1999;Mazzola, Faillenot, Barral, Mauguiere, &amp; Peyron, 2012).That is, although there is evidence that these types of somatosensory information are distinguishable from each other, no evidence has yet been presented for a scalar topography that separates, for example, hot from cold representations.Tactile texture can refer to a variety of physical properties; we operationalized this concept as a continuum from smooth to rough, and thus the texture component is another scalar entity.For all three of these scalars, we elected to represent the entire continuum as a single component.That is, the temperature component is intended to capture the salience of temperature to a concept, regardless of whether the associated temperature is hot or cold.</p>
<p>The published literature on conceptual processing of somatosensory information has focused almost exclusively on impaired knowledge of body parts in neurological patients (Coslett, Saffran, &amp; Schwoebel, 2002;Kemmerer &amp; Tranel, 2008;Laiacona, Allamano, Lorenzi, &amp; Capitani, 2006;Schwoebel &amp; Coslett, 2005), though no definitive localization has emerged from these studies.A number of functional imaging studies suggest an overlap between networks involved in real pain perception and those involved in empathic pain perception (perception of pain in others) and perception of "social pain" (Eisenberger, 2012;Lamm et al., 2011), suggesting that retrieval of pain concepts involves a simulation process.To our knowledge, no studies have yet examined the conceptual representation of temperature, tactile texture, or weight.</p>
<p>Finally, object shape is an attribute learned partly through haptic experiences (Miquée et al., 2008), but the degree to which haptic shape is learned independently from visual shape is unclear.Some objects are seen but not felt (i.e., very large and very distant objects), but the converse is rarely true, at least in sighted individuals.It was therefore decided that the visual shape query would capture knowledge about shape in both modalities, and that a separate query regarding extent of personal manipulation experience (see Motor Components) would capture the importance of haptic shape relative to visual shape.</p>
<p>Auditory components</p>
<p>The auditory cortex includes low-level areas tuned to frequency (tonotopy), amplitude, and spatial location (Schreiner &amp; Winer, 2007), as well as higher levels showing specialization for auditory "objects" such as environmental sounds (Leaver &amp; Rauschecker, 2010) and speech sounds (Belin, Zatorre, Lafaille, Ahad, &amp; Pike, 2000;Liebenthal, Binder, Spitzer, Possing, &amp; Medler, 2005).Corresponding auditory attributes of the proposed conceptual representation capture the degree to which a concept is associated with a lowpitched, high-pitched, loud (i.e., high amplitude), recognizable (i.e., having a characteristic auditory form), or speech-like sound.The attribute "low in amplitude" was not included because it was considered to be a salient attribute of relatively few concepts-that is, those that refer specifically to a low-amplitude variant (e.g., whisper).Concepts that focus on the absence of sound (e.g., silence, quiet) are probably more accurately encoded as a null value on the "loud" attribute, as fMRI studies have shown auditory regions where activity increases with sound intensity, but not the converse (Röhl &amp; Uppenkamp, 2012).</p>
<p>A few studies of sound-related knowledge (Goldberg, Perfetti, &amp; Schneider, 2006;Kellenbach et al., 2001;Kiefer, Sim, Herrnberger, Grothe, &amp; Hoenig, 2008) reported activation in or near the posterior superior temporal sulcus (STS), an auditory association region implicated in environmental sound recognition (Leaver &amp; Rauschecker, 2010; J. W. Lewis et al., 2004).Trumpp et al. (Trumpp, Kliese, Hoenig, Haarmeier, &amp; Kiefer, 2013) described a patient with a circumscribed lesion centred on the left posterior STS who displayed a specific deficit (slower response times and higher error rate) in visual recognition of sound-related words.All of these studies examined general environmental sound knowledge; nothing is yet known regarding the conceptual representation of specific types of auditory attributes like frequency or amplitude.</p>
<p>Localization of sounds in space is an important aspect of auditory perception, yet auditory perception of space has little or no representation in language independent of (multimodal) spatial concepts; there are no concepts with components like "makes sounds from the left", because spatial relationships between sound sources and listeners are almost never fixed or central to meaning.Like shape attributes of concepts, spatial attributes and spatial concepts are learned through strongly correlated, multimodal experiences involving visual, auditory, tactile, and motor processes.In the proposed semantic representation model, spatial attributes of concepts are represented by modality-independent spatial components (see Spatial Components).</p>
<p>A final salient component of human auditory experience is the domain of music.Music perception, which includes processing of melody, harmony, and rhythm elements in sound, appears to engage relatively specialized, right lateralized networks, some of which may also process nonverbal (prosodic) elements in speech (Zatorre, Belin, &amp; Penhune, 2002).Many words refer explicitly to musical phenomena (sing, song, melody, harmony, rhythm, etc.) or to entities (instruments, performers, performances) that produce musical sounds.Therefore, the model proposed here includes a component to capture the experience of processing music.</p>
<p>Gustatory and olfactory components</p>
<p>Gustatory and olfactory experiences are each represented by a single attribute that captures the relevance of each type of experience to the concept.These sensory systems do not show clear large-scale organization along any dimension, and neurons within each system often respond to a broad spectrum of items.Both gustatory and olfactory concepts have been linked with specific early sensory and higher associative neural processors (Goldberg et al., 2006;González et al., 2006).</p>
<p>Motor components</p>
<p>The motor components of the proposed conceptual representation (Table 1) capture the degree to which a concept is associated with actions involving particular body parts.The neural representation of motor action verbs has been extensively studied, with many studies showing action-specific activation of a high-level network that includes the supramarginal gyrus and posterior middle temporal gyrus (Binder, Desai, Conant, &amp; Graves, 2009;Desai, Binder, Conant, &amp; Seidenberg, 2009).There is also evidence for somatotopic representation of action verb representation in primary sensorimotor areas (Hauk, Johnsrude, &amp; Pulvermuller, 2004), though this claim is somewhat controversial (Postle, McMahon, Ashton, Meredith, &amp; de Zubicaray, 2008).Object concepts associated with particular actions also activate the action network (Binder et al., 2009;Boronat et al., 2005;Martin et al., 1995), and there is evidence for somatotopically specific activation depending on which body part is typically used in the object interaction (Carota, Moseley, &amp; Pulvermüller, 2012).Following precedent in the neuroimaging literature (Carota et al., 2012;Hauk et al., 2004;Tettamanti et al., 2005), a three-way partition into head-related (face, mouth, or tongue), upper limb (arm, hand, or fingers), and lower limb (leg or foot) actions was used to assess somatotopic organization of action concepts.A reference to eye actions was felt to be confusing, because all visual experiences involve the eyes; in this sense, any visible object could be construed as "associated with action involving the eyes".</p>
<p>Finally, the extent to which action attributes are represented in action networks may be partly determined by personal experience (Calvo-Merino, Glaser, Grezes, Passingham, &amp; Haggard, 2005;Jäncke, Koeneke, Hoppe, Rominger, &amp; Hänggi, 2009).Most people would rate "piano" as strongly associated with upper limb actions, but most people also have no personal experience with these specific actions.Based on prior research, it is likely that the action representation of the concept "piano" is more extensive in the case of pianists.Therefore, a separate component (called "practice") was created to capture the level of personal experience the rater has with performing the actions associated with the concept.</p>
<p>Spatial components</p>
<p>Spatial and other more abstract components are listed in Table 2. Neural systems that encode aspects of spatial experience have been investigated at many levels (Andersen, Snyder, Bradley, &amp; Xing, 1997;Burgess, 2008;Kemmerer, 2006;Kranjec, Cardillo, Schmidt, Lehet, &amp; Chatterjee, 2012;Woods et al., 2014).As mentioned above, the acquisition of basic spatial concepts and knowledge of spatial attributes of concepts generally involves correlated, multimodal inputs.The most obvious spatial content in language is the set of relations expressed by prepositional phrases, which have been a major focus of study in linguistics and semantic theory (Landau &amp; Jackendoff, 1993;Talmy, 1983).Lesion correlation and neuroimaging studies have implicated various parietal regions, particularly the left inferior parietal lobe, in the processing of spatial prepositions and depicted locative relations (Amorapanth, Widick, &amp; Chatterjee, 2010;Noordzij, Neggers, Ramsey, &amp; Postma, 2008;Tranel &amp; Kemmerer, 2004;Wu, Waller, &amp; Chatterjee, 2007).Prepositional phrases appear to express most, if not all, of the explicit relational spatial content that occurs in English; a semantic component targeting this type of content therefore appears unnecessary (i.e., the set of words with high values on this component would be identical to the set of spatial prepositions).The spatial components of the proposed representation model focus instead on other types of spatial information found in nouns, verbs, and adjectives.</p>
<p>Perception of three-dimensional spaces involves a specialized neural processor in the posterior parahippocampal region (Epstein &amp; Kanwisher, 1998;Epstein et al., 2007).The perception of three-dimensional space is based largely on visual input, but also involves correlated proprioceptive information whenever three-dimensional space is experienced via movement of the body through space.Spatial localization in the auditory modality also probably contributes to the experience of three-dimensional space.Some concepts refer directly to interior spaces, such as kitchen, foyer, bathroom, classroom, and so on, and seem to include information about general three-dimensional size and layout, as do concepts about types of buildings (library, school, hospital, etc.).More generally, such concepts include the knowledge that they can be physically entered, navigated, and exited, which makes possible a range of conceptual combinations (entered the kitchen, ran through the hall, went out of the library, etc.) that are not possible for objects lacking large interior spaces (*entered the chair).The salient property of concepts that determines whether they activate a three-dimensional representation of space is the degree to which they evoke a particular "scene"-that is, an array of objects in a three-dimensional space, either by direct reference or by association.Space names (kitchen, etc.) and building names (library, etc.) refer directly to threedimensional scenes.Many other object concepts evoke mental representations of scenes by virtue of thematic relations, such as the evocation of kitchen by oven.An object concept like chair would probably fail to evoke such a representation, as chairs are not linked thematically with any particular scene.Thus there is a graded degree of mental representation of three-dimensional space evoked by different concepts, which, we hypothesize, results in a corresponding variation in activation of the three-dimensional space neural processor.</p>
<p>Large-scale (topographical) spatial information is critical for navigation in the environment.Entities that are large and have a fixed location, such as geological formations and buildings, can serve as landmarks within a mental map of the environment.We hypothesize that such concepts are partly encoded in environmental navigation systems located in the hippocampal, parahippocampal, and posterior cingulate gyri (Burgess, 2008;Maguire, Frith, Burgess, Donnett, &amp; O'Keefe, 1998;Wallentin, Østergaarda, Lund, Østergaard, &amp; Roepstorff, 2005), and we assess the salience of this "topographical" attribute by inquiring to what degree the concept in question could serve as a landmark on a map.</p>
<p>Spatial cognition also includes spatial aspects of motion, particularly direction or path of motion through space (location change).Some verbs that denote location change refer directly to a direction of motion (ascend, climb, descend, rise, fall) or type of path (arc, bounce, circle, jump, launch, orbit, swerve).Others imply a horizontal path of motion parallel to the ground (run, walk, crawl, swim), while others refer to paths toward or away from an observer (arrive, depart, leave, return).Some entities are associated with a particular type of path through space (bird, butterfly, car, rocket, satellite, snake).Visual motion area MT features a columnar spatial organization of neurons according to preferred axis of motion; however, this organization is at a relatively microscopic scale, such that the full 180°of axis of motion is represented in 500 µm of cortex (Albright et al., 1984).In human fMRI studies, perception of the spatial aspects of motion has been linked with a neural processor in the intraparietal sulcus (IPS) and surrounding dorsal visual stream (Culham et al., 1998;Wu, Morganti, &amp; Chatterjee, 2008).</p>
<p>A final aspect of spatial cognition relates to peripersonal proximity.Intuitively, the coding of peripersonal proximity is a central aspect of action representation and performance, threat detection, and resource acquisition, all of which are neural processes critical for survival.Evidence from both animal and human studies suggests that the representation of peripersonal space involves somewhat specialized neural mechanisms (Graziano, Yap, &amp; Gross, 1994;Makin, Holmes, &amp; Zohary, 2007).We hypothesize that these systems also partly encode the meaning of concepts that relate to peripersonal space-that is, objects that are typically within easy reach and actions performed on these objects.Given the importance of peripersonal space in action and object use, two further components were included to explore the relevance of movement away from or out of versus toward or into the self.Some action verbs (give, punch, tell, throw) indicate movement or relocation of something away from the self, whereas others (acquire, catch, receive, eat) indicate movement or relocation toward the self.This distinction may also modulate the representation of objects that characteristically move toward or away from the self (Rueschemeyer, Pfeiffer, &amp; Bekkering, 2010).</p>
<p>Mathematical cognition, particularly the representation of numerosity, is a somewhat specialized information processing domain with well-documented neural correlates (Dehaene, 1997;Harvey, Klein, Petridou, &amp; Dumoulin, 2013;Piazza, Izard, Pinel, Bihan, &amp; Dehaene, 2004).In addition to number names, which refer directly to numerical quantities, many words are associated with the concept of numerical quantity (amount, number, quantity) or with particular numbers (dime, hand, egg).Although not a spatial process per se, since it does not typically involve three dimensions, numerosity processing seems to depend on directional vector representations similar to those underlying spatial cognition.Thus, a component was included to capture association with numerical quantities and was considered loosely part of the spatial domain.</p>
<p>Temporal and causal components</p>
<p>Event concepts include several distinct types of information.Temporal information pertains to the duration and temporal order of events.Some types of event have a typical duration, in which case the duration may be a salient attribute of the event (breakfast, lecture, movie, shower).Some event-like concepts refer specifically to durations (minute, hour, day, week).Typical durations may be very short (blink, flash, pop, sneeze) or relatively long (childhood, college, life, war).Events of a particular type may also recur at particular times and thus occupy a characteristic location within the temporal sequence of events.Examples of this phenomenon include recurring daily events (shower, breakfast, commute, lunch, dinner), recurring weekly or monthly events (Monday, weekend, payday), recurring annual events (holidays and other cultural events, seasons and weather phenomena), and concepts associated with particular phases of life (infancy, childhood, college, marriage, retirement).The neural correlates of these types of information are not yet clear (Ferstl, Rinck, &amp; von Cramon, 2005;Ferstl &amp; von Cramon, 2007;Grondin, 2010;Kranjec et al., 2012).Because time seems to be pervasively conceived of in spatial terms (Evans, 2004;Kranjec &amp; Chatterjee, 2010;Lakoff &amp; Johnson, 1980), it is likely that temporal concepts share their neural representation, at least in part, with spatial concepts.Components of the proposed conceptual representation model are designed to capture the degree to which a concept is associated with a characteristic duration, the degree to which a concept is associated with a long or a short duration, and the degree to which a concept is associated with a particular or predictable point in time.</p>
<p>Causal information pertains to cause and effect relationships between concepts.Events and situations may have a salient precipitating cause (infection, kill, laugh, spill), or they may occur without an immediate apparent cause (e.g., some natural phenomena, random occurrences).Events may or may not have highly likely consequences.Imaging evidence suggests involvement of several inferior parietal and temporal regions in low-level perception of causality (Blos, Chatterjee, Kircher, &amp; Straube, 2012;Fonlupt, 2003;Fugelsang, Roser, Corballis, Gazzaniga, &amp; Dunbar, 2005;Woods et al., 2014), and a few studies of causal processing at the conceptual level implicate these and other areas (Kranjec et al., 2012;Satpute et al., 2005).The proposed conceptual representation model includes components designed to capture the degree to which a concept is associated with a clear preceding cause, and the degree to which an event or action has probable consequences.</p>
<p>Social components</p>
<p>Theory of mind (TOM) refers to the ability to infer or predict mental states and mental processes in other volitional beings (typically though not exclusively other people).We hypothesize that the brain systems underlying this ability are involved when encoding the meaning of words that refer to intentional entities or actions because learning these concepts is frequently associated with TOM processing.Essentially, this attribute captures the semantic feature of intentionality.In addition to the query pertaining to events with social interactions as well as the query regarding mental activities or events, we included an object-directed query assessing the degree to which a thing has human or human-like intentions, plans, or goals, and a corresponding verb query assessing the degree to which an action reflects human intentions, plans, or goals.The underlying hypothesis is that such concepts, which mostly refer to people in particular roles and the actions they take, are partly understood by engaging a TOM process.</p>
<p>There is strong evidence for the involvement of a specific network of regions in TOM.These regions most consistently include the medial prefrontal cortex, posterior cingulate/precuneus, and the temporoparietal junction (Schilbach et al., 2012;Schurz, Radua, Aichhorn, Richlan, &amp; Perner, 2014;Van Overwalle, 2009;van Veluw &amp; Chance, 2014) with several studies also implicating the anterior temporal lobes (Ross &amp; Olson, 2010;Schilbach et al., 2012;Schurz et al., 2014).The temporoparietal junction (TPJ) is an area of particular focus in the TOM literature, but it is not a consistently defined region and may encompass parts of the posterior superior temporal gyrus/sulcus, supramarginal gyrus, and angular gyrus.Collapsing across tasks, TOM or intentionality is frequently associated with significant activation in the angular gyri (Carter &amp; Huettel, 2013;Schurz et al., 2014), but some variability in the localization of peak activation within the TPJ is seen across different TOM tasks (Schurz et al., 2014).In addition, while some studies have shown right lateralization of activation in the TPJ (Saxe &amp; Wexler, 2005), several meta-analyses have suggested bilateral involvement (Schurz et al., 2014;Spreng, Mar, &amp; Kim, 2009;Van Overwalle, 2009;van Veluw &amp; Chance, 2014).</p>
<p>Another aspect of social cognition likely to have a neurobiological correlate is the representation of the self.There is evidence to suggest that information that pertains to the self may be processed differently from information that is not considered to be selfrelated.Some behavioural studies have suggested that people show better recall (Rogers, Kuiper, &amp; Kirker, 1977) and faster response times (Kuiper &amp; Rogers, 1979) when information is relevant to the self, a phenomenon known as the self-reference effect.Neuroimaging studies have typically implicated cortical midline structures in the processing of selfreferential information, particularly the medial prefrontal and parietal cortices (Araujo, Kaplan, &amp; Damasio, 2013;Northoff et al., 2006).While both selfand other-judgments activate these midline regions, greater activation of medial prefrontal cortex for selftrait judgments than for other-trait judgments has been reported, with the reverse pattern seen in medial parietal regions (Araujo et al., 2013).In addition, there is evidence for a ventral-to-dorsal spatial gradient within medial prefrontal cortex for self-relative to other-judgments (Denny, Kober, Wager, &amp; Ochsner, 2012).Preferential activation of left ventrolateral prefrontal cortex and left insula for self-judgments and of the bilateral temporoparietal junction and cuneus for other-judgments has also been seen (Denny et al., 2012).The relevant query for this attribute assesses the degree to which a target noun is "related to your own view of yourself" or the degree to which a target verb is "an action or activity that is typical of you, something you commonly do and identify with".</p>
<p>A third aspect of social cognition for which a specific neurobiological correlate is likely is the domain of communication.Communication, whether by language or nonverbal means, is the basis for all social interaction, and many noun (e.g., speech, book, meeting) and verb (e.g., speak, read, meet) concepts are closely associated with the act of communicating.We hypothesize that comprehension of such items is associated with relative activation of language networks (particularly general lexical retrieval, syntactic, phonological, and speech articulation components) and gestural communication systems compared to concepts that do not reference communication or communicative acts.</p>
<p>Cognition component</p>
<p>A central premise of the current approach is that all aspects of experience can contribute to concept acquisition and therefore concept composition.For self-aware human beings, purely cognitive events and states certainly comprise one aspect of experience.When we "have a thought", "come up with an idea", "solve a problem", or "consider a fact", we experience these phenomena as events that are no less real than sensory or motor events.Many socalled abstract concepts refer directly to cognitive events and states (decide, judge, recall, think) or to the mental "products" of cognition (idea, memory, opinion, thought).We propose that such concepts are learned in large part by generalization across these cognitive experiences, in exactly the same way as concrete concepts are learned through generalization across perceptual and motor experiences.Domain-general cognitive operations have been the subject of a very large number of neuroimaging studies, many of which have attempted to fractionate these operations into categories such as "working memory", "decision", "selection", "reasoning", "conflict suppression", "set maintenance", and so on.No clear anatomical divisions have arisen from this work, however, and the consensus view is that there is a largely shared bilateral network for what are variously called "working memory", "cognitive control", or "executive" processes, involving portions of the inferior and middle frontal gyri ("dorsolateral prefrontal cortex"), mid-anterior cingulate gyrus, precentral sulcus, anterior insula, and perhaps dorsal regions of the parietal lobe (Badre &amp; D'Esposito, 2007;Derrfuss, Brass, Neumann, &amp; von Cramon, 2005;Duncan &amp; Owen, 2000;Nyberg et al., 2003).We hypothesize that retrieval of concepts that refer to cognitive phenomena involves a partial simulation of these phenomena within this cognitive control network, analogous to the partial activation of sensory and motor systems by object and action concepts.</p>
<p>Emotion and evaluation components</p>
<p>There is strong evidence for the involvement of a specific network of regions in affective processing in general.These regions principally include the orbitofrontal cortex, dorsomedial prefrontal cortex, anterior cingulate gyri (subgenual, pregenual, and dorsal), inferior frontal gyri, anterior temporal lobes, amygdala, and anterior insula (Etkin, Egner, &amp; Kalisch, 2011;Hamann, 2012;Kober et al., 2008;Lindquist, Wager, Kober, Bliss-Moreau, &amp; Barrett, 2012;Phan, Wager, Taylor, &amp; Liberzon, 2002;Vytal &amp; Hamann, 2010).Activation in these regions can be modulated by the emotional content of words and text (Chow et al., 2013;Cunningham, Raye, &amp; Johnson, 2004;Ferstl et al., 2005;Ferstl &amp; von Cramon, 2007;Kuchinke et al., 2005;Nakic, Smith, Busis, Vythilingam, &amp; Blair, 2006).However, the nature of individual affective states has long been the subject of debate.One of the primary families of theories regarding emotion are the dimensional accounts, which propose that affective states arise from combinations of a small number of more fundamental dimensions, most commonly valence (hedonic tone) and arousal (Russell, 1980).In current psychological construction accounts, this input is then interpreted as a particular emotion using one or more additional cognitive processes, such as appraisal of the stimulus or the retrieval of memories of previous experiences or semantic knowledge (Barrett, 2006;Lindquist, Siegel, Quigley, &amp; Barrett, 2013;Posner, Russell, &amp; Peterson, 2005).Another highly influential family of theories characterizes affective states as discrete emotions, each of which have consistent and discriminable patterns of physiological responses, facial expressions, and neural correlates.Basic emotions are a subset of discrete emotions that are considered to be biologically predetermined and culturally universal (Hamann, 2012;Lench, Flores, &amp; Bench, 2011;Vytal &amp; Hamann, 2010), although they may still be somewhat influenced by experience (Tracy &amp; Randles, 2011).The most frequently proposed set of basic emotions are those of Ekman (Ekman, 1992), which include anger, fear, disgust, sadness, happiness, and surprise.</p>
<p>There is substantial neuroimaging support for dissociable dimensions of valence and arousal within individual studies; however, the specific regions associated with the two dimensions or their endpoints have been inconsistent and sometimes contradictory across studies (Anders, Eippert, Weiskopf, &amp; Veit, 2008;Anders, Lotze, Erb, Grodd, &amp; Birbaumer, 2004;Colibazzi et al., 2010;Gerber et al., 2008;P. A. Lewis, Critchley, Rotshtein, &amp; Dolan, 2007;Nielen et al., 2009;Posner et al., 2009;Wilson-Mendenhall, Barrett, &amp; Barsalou, 2013).With regard to the discrete emotion model, meta-analyses have suggested differential activation patterns in individual regions associated with basic emotions (Lindquist et al., 2012;Phan et al., 2002;Vytal &amp; Hamann, 2010), but there is a lack of specificity.Individual regions are generally associated with more than one emotion, and emotions are typically associated with more than one region (Lindquist et al., 2012;Vytal &amp; Hamann, 2010).Overall, current evidence is not consistent with a one-to-one mapping between individual brain regions and either specific emotions or dimensions; however, the possibility of spatially overlapping but discriminable networks remains open.Importantly, studies of emotion perception or experience using multivariate pattern classifiers are providing emerging evidence for distinct patterns of neural activity associated with both discrete emotions (Kassam, Markey, Cherkassky, Loewenstein, &amp; Just, 2013;Kragel &amp; LaBar, 2014;Peelen, Atkinson, &amp; Vuilleumier, 2010;Said, Moore, Engell, &amp; Haxby, 2010) and specific combinations of valence and arousal (Baucom, Wedell, Wang, Blitzer, &amp; Shinkareva, 2012).</p>
<p>The semantic representation model proposed here includes separate components reflecting the degree of association of a target concept with each of Ekman's basic emotions.The representation also includes components corresponding to the two dimensions of the circumplex model (valence and arousal).There is evidence that each end of the valence continuum may have a distinctive neural instantiation (Anders et al., 2008;Anders et al., 2004;Colibazzi et al., 2010;Posner et al., 2009), and therefore separate components were included for pleasantness and unpleasantness.</p>
<p>Drive components</p>
<p>Drives are central associations of many concepts and are conceptualized here, following Mazlow (Mazlow, 1943), as needs that must be filled to reach homeostasis or enable growth.They include physiological needs (food, rest/sleep, sex, emotional expression), security, social contact, approval, and self-development.Drive experiences are closely related to emotions, which are produced whenever needs are fulfilled or frustrated, and to the construct of reward, conceptualized as the brain response to a resolved or satisfied drive.Here we use the term "drive" synonymously with terms such as "reward prediction" and "reward anticipation".Extensive evidence links the ventral striatum, orbital frontal cortex, and ventromedial prefrontal cortex to processing of drive and reward states (Diekhof, Kaps, Falkai, &amp; Gruber, 2012;Levy &amp; Glimcher, 2012;Peters &amp; Büchel, 2010;Rolls &amp; Grabenhorst, 2008).The proposed semantic components represent the degree of general motivation associated with the target concept and the degree to which the concept itself refers to a basic need.</p>
<p>Attention components</p>
<p>Attention is a basic process central to information processing, but is not usually thought of as a type of information.It is possible, however, that some concepts (e.g., "scream") are so strongly associated with attentional orienting that the attention-capturing event becomes part of the conceptual representation.A component was included to assess the degree to which a concept is "someone or something that grabs your attention".</p>
<p>Attribute salience ratings for 535 English words</p>
<p>Ratings of the relevance of each semantic component to word meanings were collected for 535 English words using the internet crowdsourcing survey tool Amazon Mechanical Turk ( https://www.mturk.com/mturk/).The aim of this work was to ascertain the degree to which a relatively unselected sample of the population associates the meaning of a particular word with particular kinds of experience.We refer to the continuous ratings obtained for each word as the attributes comprising the word's meaning.The results reflect subjective judgments rather than objective truth and are likely to vary somewhat with personal experience and background, presence of idiosyncratic associations, participant motivation, and comprehension of the instructions.With these caveats in mind, it was hoped that mean ratings obtained from a sufficiently large sample would accurately capture central tendencies in the population, providing representative measures of real-world comprehension.</p>
<p>As discussed in the previous section, the attributes selected for study reflect known neural systems.We hypothesize that all concept learning depends on this set of neural systems, and therefore the same attributes were rated regardless of grammatical class or ontological category.</p>
<p>The word set</p>
<p>The concepts included 434 nouns, 62 verbs, and 39 adjectives.All of the verbs and adjectives, and 141 of the nouns, were pre-selected as experimental materials for the Knowledge Representation in Neural Systems (KRNS) project (Glasgow et al., 2016), an ongoing multi-centre research program sponsored by the US Intelligence Advanced Research Projects Activity (IARPA).Because the KRNS set was limited to relatively concrete concepts and a restricted set of noun categories, 293 additional nouns were added to include more abstract concepts and to enable richer comparisons between category types.Table 3 summarizes some general characteristics of the word set.Table 4 describes the content of the set in terms of major category types.A complete list of the words and associated lexical data are available for download (see link prior to the References section).</p>
<p>Query design</p>
<p>Queries used to elicit ratings were designed to be as structurally uniform as possible across attributes and grammatical classes.Some flexibility was allowed, however, to create natural and easily understood queries.All queries took the general form of {head} {relation}{content}, where {head} refers to a lead-in phrase that was uniform within each word class, {content} to the specific content being assessed, and {relation} to the type of relation linking the target word and the content.The {head} for all queries regarding nouns was: "To what degree do you think of this thing as . . .", whereas the {head} for all verb queries was: "To what degree do you think of this as . . .", and the {head} for all adjective queries was: "To what degree do you think of this property as . . .". Table 5 lists several examples for each grammatical class.</p>
<p>For the three scalar somatosensory attributes Temperature, Texture, and Weight, it was felt necessary for the sake of clarity to pose separate queries for each end of the scalar continuum.That is, rather than a single query such as "To what degree do you think of this thing as being hot or cold to the touch?", two separate queries were posed about hot and cold attributes.The Temperature rating was then computed by taking the maximum rating for the word on either the Hot or the Cold query.Similarly, the Texture rating was computed as the maximum rating on Rough and Smooth queries, and the Weight rating was computed as the maximum rating on Heavy and Light queries.</p>
<p>A few attributes did not appear to have a meaningful sense when applied to certain grammatical classes.The attribute Complexity addresses the degree to which an entity appears visually complex and has no obvious sensible correlate in  (Shaoul &amp; Westbury, 2013).Orthographic neighbour counts are from CELEX (Baayen, Piepenbrock, &amp; Gulikers, 1995).Imageability ratings are from a composite of published norms (Bird, Franklin, &amp; Howard, 2001;Clark &amp; Paivio, 2004;Cortese &amp; Fugett, 2004;Wilson, 1988).IQR = interquartile range.the verb class.The attribute Practice addresses the degree to which the rater has personal experience manipulating an entity or performing an action and has no obvious sensible correlate in the adjective class.Finally, the attribute Caused addresses the degree to which an entity is the result of some prior event or an action will cause a change to occur and has no obvious correlate in the adjective class.Ratings on these three attributes were not obtained for the grammatical classes to which they did not meaningfully apply.</p>
<p>Survey methods</p>
<p>Surveys were posted on Amazon Mechanical Turk (AMT), an internet site that serves as an interface between "requestors" who post tasks and "workers" who have accounts on the site and sign up to complete tasks and receive payment.Requestors have the right to accept or reject worker responses based on quality criteria.Participants in the present study were required to have completed at least 10,000 previous jobs with at least a 99% acceptance rate, and to have account addresses in the United States; these criteria were applied automatically by AMT.Prospective participants were asked not to participate unless they were native English speakers; however, compliance with this request cannot be verified.After reading a page of instructions, participants provided their age, gender, years of education, and current occupation.No identifying information was collected from participants or requested from AMT.</p>
<p>For each session, a participant was assigned a single word and provided ratings on all of the semantic components as they relate to the assigned word.A sentence containing the word was provided to specify the word class and sense targeted.Two such sentences, conveying the most frequent sense of the word, were constructed for each word, and one of these was selected randomly and used throughout the session.Participants were paid a small stipend for completing all queries for the target word.They could return for as many sessions as they wanted.An automated script assigned target words randomly, with the constraint that the same participant could not be assigned the same word more than once.</p>
<p>For each attribute, a screen was presented with the target word, the sentence context, the query for that attribute, an example of a word that "would receive a high rating" on the attribute, an example of a word that "might receive a medium rating" on the attribute, and the rating options.The options were presented as a horizontal row of clickable radio buttons with numerical labels ranging from 0 to 6 (left to right), and verbal labels (Not At All, Somewhat, Very Much) at appropriate locations above the buttons.An example trial is shown in Figure S1 of the Supplemental Material.An additional button labelled "Not Applicable" was positioned to the left of the "0" button.Participants were forewarned that some of the queries "will be almost nonsensical, because they refer to aspects of word meaning that are not even applicable to your word.For example, you might be asked, 'To what degree do you think of shoe as an event that has a predictable duration, whether short or long?', with movie given as an example of a high-rated concept, and concert given as a medium-rated concept.Since shoe refers to a static thing, not to an event of any kind, you should indicate either 0 or "Not Applicable" for this question".All "Not Applicable" responses were later converted to 0 ratings.</p>
<p>General results</p>
<p>A total of 16,373 sessions were collected from 1743 unique participants (average 9.4 sessions/participant).Of the participants, 43% were female, 55% were male, and 2% did not provide gender information.The average age was 34.0 years (SD = 10.4), and average years of education was 14.8 (SD = 2.1).</p>
<p>A limitation of unsupervised crowdsourcing is that some participants may not comply with task instructions.Informal inspection of the data revealed examples in which participants appeared to be responding randomly or with the same response on every trial.A simple metric of individual deviation from the group mean response was computed by correlating the vector of ratings obtained from an individual for a particular word with the group mean vector for that word.For example, if 30 participants were assigned word X, this yielded 30 vectors, each with 65 elements.The average of these vectors is the group average for word X.The quality metric for each participant who rated word X was the correlation between that participant's vector and the group average vector for word X. Supplemental Figure S2 shows the distribution of these values across the sample after Fisher z transformation.A minimum threshold for acceptance was set at r = .5,which corresponds to the edge of a prominent lower tail in the distribution.This criterion resulted in rejection of 1097 or 6.69% of the sessions.After removing these sessions, the final data set consisted of 15,268 rating vectors with a mean intra-word individual-to-group correlation of .785(median .803),providing an average of 28.6 rating vectors (i.e., sessions) per word (SD 2.0, range 17-33).Mean ratings for each of the 535 words, computed using only the sessions that passed the acceptance criterion, are available for download (see link prior to the References section).</p>
<p>Exploring the representations</p>
<p>Here we explore the dataset by addressing three general questions.First, how much independent information is provided by each of the attributes, and how are they related to each other?Although the component attributes were selected to represent distinct types of experiential information, there is likely to be conceptual overlap between attributes, real-world covariance between attributes, and covariance due to associations linking one attribute with another in the minds of the raters.We therefore sought to characterize the underlying covariance structure of the attributes.Second, do the attributes capture important distinctions between a priori ontological types and categories?If the structure of conceptual knowledge really is based on underlying neural processing systems, then the covariance structure of attributes representing these systems should reflect psychologically salient conceptual distinctions.Finally, what conceptual groupings are present in the covariance structure itself, and how do these differ from a priori category assignments?</p>
<p>Attribute mutual information</p>
<p>To investigate redundancy in the information encoded in the representational space, we computed the mutual information (MI) for all pairs of attributes using the individual participant ratings after removal of outliers.MI is a general measure of how mutually dependent two variables are, determined by the similarity between their joint distribution p(X,Y) and factored marginal distribution p(X)p(Y).MI can range from 0, indicating complete independence, to 1 in the case of identical variables.</p>
<p>MI was generally very low (mean = .049),suggesting a low overall level of redundancy (see Supplemental Figure S3).Particular pairs and groups, however, showed higher redundancy.The largest MI value was for the pair Pleasant-Happy (.643).It is likely that these two constructs evoked very similar concepts in the minds of the raters.Other isolated pairs with relatively high MI values included Small-Weight (.344), Caused-Consequential (.312), Practice-Near (.269), Taste-Smell (.264), and Social-Communication (.242).</p>
<p>Groups of attributes with relatively high pairwise MI values included a human-related group (Face, Speech, Human, Body, Biomotion; mean pairwise MI = .320),an attention-arousal group (Attention, Arousal, Surprised; mean pairwise MI = .304),an auditory group (Audition, Loud, Low, High, Sound, Music; mean pairwise MI = .296),a visual-somatosensory group (Vision, Colour, Pattern, Shape, Texture, Weight, Touch; mean pairwise MI = .279),a negative affect group (Angry, Sad, Disgusted, Unpleasant, Fearful, Harm, Pain; mean pairwise MI = .263),a motion-related group (Motion, Fast, Slow, Biomotion; mean pairwise MI = .240),and a time-related group (Time, Duration, Short, Long; mean pairwise MI = .193).</p>
<p>Attribute covariance structure</p>
<p>Factor analysis was conducted to investigate potential latent dimensions underlying the attributes.Principal axis factoring (PAF) was used with subsequent promax rotation given that the factors were assumed to be correlated.Mean attribute vectors for the 496 nouns and verbs were used as input; adjective data were excluded so that the Practice and Caused attributes could be included in the analysis.To determine the number of factors to retain, a parallel analysis (eigenvalue Monte Carlo simulation) was performed using PAF and 1000 random permutations of the raw data from the current study (Horn, 1965;O'Connor, 2000).Factors with eigenvalues exceeding the 95th percentile of the distribution of eigenvalues derived from the random data were retained and examined for interpretability.</p>
<p>The Kaiser-Meyer-Olkin Measure of Sampling Accuracy was .874,and Bartlett's Test of Sphericity was significant, χ 2 (2016) = 41,129.17,p &lt; .001,indicating adequate factorability.Sixteen factors had eigenvalues that were significantly above random chance.These factors accounted for 81.0% of the variance.Based on interpretability, all 16 factors were retained.Table 6 lists these factors and the attributes with the highest loadings on each.</p>
<p>As can be seen from the loadings, the latent dimensions revealed by PAF mostly replicate the groupings apparent in the mutual information analysis.The most prominent of these include factors representing visual and tactile attributes, negative emotion associations, social communication, auditory attributes, food ingestion, self-related attributes, motion in space, human physical attributes, surprise, characteristics of large places, upper limb actions, reward experiences, positive associations, and time-related attributes.</p>
<p>Together, the mutual information and factor analyses reveal a modest degree of covariance between the attributes, suggesting that a more compact representation could be achieved by reducing redundancy.A reasonable first step would be to remove either Pleasant or Happy from the representation, as these two attributes showed a high level of redundancy.For some analyses, use of the 16 significant factors identified in the PAF rather than the complete set of attributes may be appropriate and useful.On the other hand, these factors left ∼20% of the variance unexplained, which we propose is a reflection of the underlying complexity of conceptual knowledge.This complexity places limits on the extent to which the representation can be reduced without losing explanatory power.In the following analyses we have included all 65 attributes in order to investigate further their potential contributions to understanding conceptual structure.</p>
<p>Attribute composition of some major semantic categories</p>
<p>Mean attribute vectors representing major semantic categories in the word set were computed by averaging over items within several of the a priori categories outlined in Table 4. Figures 1 and 2 show mean vectors for 12 of the 20 noun categories in Table 4, presented as circular plots.</p>
<p>Vectors for three verb categories are shown in Figure 3.With the exception of the Path and Social attributes, which marked the Locative Action and Social Action categories, respectively, the three verb categories evoked a similar set of attributes but in different relative proportions.Ratings for physical adjectives reflected the sensory domain (visual, somatosensory, auditory, etc.) to which the adjective referred.</p>
<p>Quantitative differences between a priori categories</p>
<p>The a priori category labels shown in Table 4 were used to identify attribute ratings that differ significantly between semantic categories and therefore may contribute to a mental representation of these category distinctions.For each comparison, an unpaired t-test was conducted for each of the 65 attributes, comparing the mean ratings on words in one category with those in the other.It should be kept in mind that the results are specific to the sets of words that happened to be selected for the study and may not generalize to other samples from these categories.For that reason, and because of the large number of contrasts performed, only differences significant at p &lt; .0001will be described.</p>
<p>Initial comparisons were conducted between the superordinate categories Artefacts (n = 132), Living Things (N = 128), and Abstract Entities (N = 99).As shown in Figure 4, numerous attributes distinguished these categories.Not surprisingly, Abstract Entities had significantly lower ratings than the two concrete categories on nearly all of the attributes related to sensory experience.The main exceptions to this were  the attributes specifically associated with animals and people, such as Biomotion, Face, Body, Speech, and Human, for which Artefacts received ratings as low as or lower than those for Abstract Entities.Conversely, Abstract Entities received higher ratings than the concrete categories on attributes related to temporal and causal experiences (Time, Duration, Long, Short, Caused, Consequential), social experiences (Social, Communication, Self), and emotional experiences (Unpleasant, Sad, Angry, Disgusted, Fearful, Surprised, Drive, Arousal).In all, 57 of the 65 attributes distinguished abstract from concrete entities.Living Things were distinguished from Artefacts by the aforementioned attributes characteristic of animals and people.In addition, Living Things on average received higher ratings on Motion and Slow, suggesting they tend to have more salient movement qualities.Living Things also received higher ratings than Artefacts on several emotional attributes (Angry, Disgusted, Fearful, Surprised), although these ratings were relatively low for both concrete categories.Conversely, Artefacts received higher ratings than Living Things on the attributes Large, Landmark, and Scene, all of which probably reflect the over-representation of buildings in this sample.Artefacts were also rated higher on Temperature, Music (reflecting an over-representation of musical instruments), Upper Limb, Practice, and several temporal attributes (Time, Duration, and Short).Though significantly different for Artefacts and Living Things, the ratings on the temporal attributes were lower for both concrete categories than for Abstract Entities.In all, 21 of the 65 attributes distinguished between Living Things and Artefacts.Figure 5 shows comparisons between the more specific categories Animals (n = 30), Plants (N = 30), and Tools (N = 27).Relatively selective impairments on these three categories are frequently reported in patients with category-related semantic impairments (Capitani, Laiacona, Mahon, &amp; Caramazza, 2003;Forde &amp; Humphreys, 1999;Gainotti, 2000).The data show a number of expected results (see Gainotti et al., 2009;Gainotti et al., 2013;Hoffman &amp; Lambon Ralph, 2013, for previous similar comparisons between these categories), such as higher ratings for Animals than for either Plants or Tools on attributes related to motion; higher ratings for Plants on Taste, Smell, and Head attributes, related to their prominent role as food; and higher ratings for Tools and Plants on attributes related to manipulation (Touch, Upper Limb, Practice).Ratings on sound-related attributes were highest for Animals, intermediate for Tools, and virtually zero for Plants.Colour also showed a three-way split, with Plants &gt; Animals &gt; Tools.Animals, however, received higher ratings on visual Complexity.</p>
<p>In the spatial domain, Animals were viewed as having more salient paths of motion (Path), and Tools were rated as more close at hand in everyday life (Near).Tools were rated as more consequential and more related to social experiences, drives, and needs.Tools and Plants were seen as more beneficial than Animals, and Plants more pleasant than Tools.Animals and Tools were both viewed as having more potential for harm than Plants, and Animals had higher ratings on Fearful, Surprised, Attention, and Arousal attributes, suggesting that animals are viewed as potential threats more than are the other categories.In all, 29 attributes distinguished between Animals and Plants, 22 distinguished Animals and Tools, and 20 distinguished Plants and Tools.</p>
<p>Figure 6 shows a three-way comparison between the specific artefact categories Musical Instruments (N = 20), Tools (N = 27), and Vehicles (N = 20).Again there were several expected findings, including higher ratings for Vehicles on motion-related attributes (Motion, Biomotion, Fast, Slow, Path, Away) and higher ratings for Musical Instruments on the sound-related attributes (Audition, Loud, Low, High, Sound, Music).Tools were rated higher on attributes reflecting manipulation experience (Touch, Upper Limb, Practice, Near).The importance of the Practice attribute, which encodes degree of personal experience manipulating an object or performing an action, is reflected in the much higher rating on this attribute for Tools than for Musical Instruments, whereas these categories did not differ on the Upper Limb attribute.The categories were distinguished by size in the expected direction (Vehicles &gt; Instruments &gt; Tools), and Instruments and Vehicles were rated higher than Tools on visual Complexity.</p>
<p>In the more abstract domains, Musical Instruments received higher ratings on Communication ("a thing or action that people use to communicate") and Cognition ("a form of mental activity or function of the mind"), suggesting stronger associations with mental activity, but also higher ratings on Pleasant, Happy, Sad, Surprised, Attention, and Arousal, suggesting stronger emotional and arousal associations.Tools and Vehicles had higher ratings than Musical Instruments on both Benefit and Harm attributes, and Tools were rated higher on the Needs attribute ("someone or something that would be hard for you to live without").In all, 22 attributes distinguished between Musical Instruments and Tools, 21 distinguished Musical Instruments and Vehicles, and 14 distinguished Tools and Vehicles.</p>
<p>Overall, these category-related differences are intuitively sensible, and a number of them have been reported previously (Cree &amp; McRae, 2003;Gainotti et al., 2009;Gainotti et al., 2013;Hoffman &amp; Lambon Ralph, 2013;Tranel, Logan, Frank, &amp; Damasio, 1997;Vigliocco, Vinson, Lewis, &amp; Garrett, 2004).They highlight the underlying complexity of taxonomic category structure and illustrate how the present data can be used to explore this complexity.</p>
<p>Category effects on item similarity: Brain-based versus distributional representations</p>
<p>Another way in which a priori category labels are useful for evaluating the representational scheme is by enabling a comparison of semantic distance between items in the same category and items in different categories.Figure 7A shows heat maps of the pairwise cosine similarity matrix for all 434 nouns in the sample, constructed using the brain-based representation and a representation derived via latent semantic analysis (LSA; Landauer &amp; Dumais, 1997) of a large text corpus (Landauer, Kintsch, &amp; the Science and Applications of Latent Semantic Analysis (SALSA) Group, 2015).Vectors in the LSA representation were reduced to 65 dimensions for comparability with the brain-based vectors, though very similar results were obtained with a 300-dimensional LSA representation.Concepts are grouped in the matrices according to a priori category to highlight category similarity structure.The matrices are modestly correlated (r = .31,p &lt; .00001).As the figure suggests, cosine similarity tended to be much higher for the brain-based vectors (mean = .55,SD = .18)than for the LSA vectors (mean = .19,SD = .17;p &lt; .00001).More importantly, cosine similarity was generally greater for pairs in the same a priori category than for between-category pairs, and this difference, measured for each word using Cohen's d effect size, was much larger for the brain-based (mean = 2.64, SD = 1.09) than for the LSA vectors (mean = 1.09,SD = 0.85; p &lt; .00001; Figure 7B).Thus, both the brain-based vectors and LSA vectors capture information reflecting a priori category structure, and the brain-based vectors show significantly greater effects of category co-membership than the LSA vectors.</p>
<p>Data-driven categorization of the words</p>
<p>Cosine similarity measures were also used to identify "neighbourhoods" of semantically close concepts without reference to a priori labels.Table 7 shows some representative results (a complete list is available for download; see link prior to the References section).The results provide further evidence that the representations capture semantic similarity across concepts, although direct comparisons of these distance measures with similarity decision and priming data are needed for further validation.</p>
<p>A more global assessment of similarity structure was performed using k-means cluster analyses, with the mean attribute vectors for each word as input.A series of k-means analyses was performed, with the cluster parameter ranging from 20-30, reflecting the approximate number of a priori categories in Table 4.Although the results were very similar across this range, results in the range from 25-28 seemed to afford the most straightforward interpretations.</p>
<p>Results from the 28-cluster solution are shown in Table 8.It is important to note that we are not claiming that this is the "true" number of clusters in the data.Conceptual organization is inherently hierarchical, involving degrees of similarity, and the number of categories defined depends on the type of distinction one wishes to make (e.g., animal vs. tool, canine vs. feline, dog vs. wolf, hound vs. spaniel).Our aim here was to match approximately the "grain size" of the k-means clusters with the grain size of the a priori superordinate category labels so that these could be meaningfully compared.</p>
<p>Several of the clusters that emerged from the ratings data closely matched the a priori category assignments outlined in Table 4.For example, all 30 Animals clustered together, 13 of the 14 Body Parts, and all 20 Musical Instruments.One body part ("hair") fell in the Tools and  Furniture cluster, and three additional sound-producing artefacts ("megaphone", "television", and "cellphone") clustered with the Musical Instruments.Ratings-based clustering did not distinguish edible Plants from Manufactured Foods, collapsing these into a single Foods cluster that excluded larger inedible plants ("elm", "ivy", "oak", "tree").Similarly, data-driven clustering did not distinguish Furniture from Tools, collapsing these into a single cluster that also included most of the miscellaneous manipulated artefacts ("book", "door", "computer", "magazine", "medicine", "newspaper", "ticket", "window") as well as several smaller non-artefact items ("feather", "hair", "ice", "stone").Data-driven clustering also identified a number of intuitively plausible divisions within categories.Basic human biological types ("woman", "man", "child", etc.) were distinguished from human occupational roles, and human individuals or groups with negative or violent associations formed a distinct cluster.Compared to the neutral occupational roles, human type concepts had significantly higher ratings on attributes related to sensory experience (Shape, Complexity, Touch, Temperature, Texture, High, Sound, Smell), nearness (Near, Self), and positive emotion (Happy; Table 9).Places were divided into two groups, which we labelled Non-Social and Social, that corresponded roughly to the a priori distinction between Natural Scenes and Places/Buildings, except that the Non-Social Places cluster included several manmade places ("apartment", fence", "bridge", "street", etc.).Social Places had higher ratings on attributes related to human interactions (Social, Communication, Consequential), and Non-Social Places had higher ratings on several sensory attributes (Colour, Pattern, Shape, Touch, and Texture; Table 10).In addition to distinguishing vehicles from other artefacts, data-driven clustering separated loud vehicles from quiet vehicles (mean Loud ratings 5.30 vs. 1.96, respectively).Loud vehicles also had significantly higher ratings on several other auditory attributes (Audition, High, Sound).The two vehicle clusters contained four unexpected items ("fireworks", "fountain", "river", "soccer"), probably due to high ratings for these items on Motion and Path attributes, which distinguish vehicles from other nonliving objects.</p>
<p>In the domain of event nouns, clustering divided the a priori category Social Events into two clusters that we labelled Festive Social Events and Verbal Social Events (Table 11).Festive Events had significantly higher ratings on a number of sensory attributes related to visual shape, visual motion, and sound and were rated as more Pleasant and Happy.Verbal Social Events had higher ratings on attributes related to human cognition (Communication, Cognition, Consequential) and, interestingly, were rated as both more Unpleasant and more Beneficial than Festive Events.A cluster labelled Negative Natural Events corresponded roughly to the a priori category Weather Events, with the addition of several non-meteorological negative or violent events ("explosion", "stampede", "battle", etc.).A cluster labelled Nonverbal Sound Events corresponded closely to the a priori category of the same name.Finally, a small cluster labelled Ingestive Events and Actions included several social events and verbs of ingestion, linked by high ratings on the attributes Taste, Smell, Upper Limb, Head, Toward, Pleasant, Benefit, and Needs.</p>
<p>Correspondence between data-driven clusters and a priori categories was less clear in the domain of abstract nouns.Clustering yielded two abstract   entity clusters mainly distinguished by perceived benefit and association with positive emotions (Table 12), labelled Beneficial and Neutral, which comprise a variety of abstract and mental constructs.A third cluster labelled Causal Entities comprises concepts with high ratings on the Consequential and Social attributes (Table 13).Two further clusters comprise abstract entities with strong negative and positive emotional meaning or associations (Table 14).The final cluster of abstract entities corresponds approximately to the a priori category Time Periods, but also includes properties ("new", "old", "young") and verbs ("grew", "slept") associated with time duration concepts.</p>
<p>Three clusters consisted mainly of verbs.These included a cluster labelled Locative Change Actions, which corresponded closely with the a priori category of the same name and was defined by high ratings on related to motion through space (Table 15).The second verb cluster contained a mixture of emotionally neutral physical and social actions.The final verb-heavy cluster contained a mix of verbs and adjectives with negative or violent associations.</p>
<p>The final cluster emerging from the k-means analysis consisted almost entirely of adjectives describing physical sensory properties, including colour, surface appearance, tactile texture, temperature, and weight.</p>
<p>Hierarchical clustering</p>
<p>A hierarchical cluster analysis of the 335 concrete nouns (objects and events) was performed to examine the underlying category structure in more detail.The major categories were again clearly evident in the resulting dendrogram.A number of interesting subdivisions emerged, as illustrated in the dendrogram segments shown in Figure 8. Musical Instruments, which formed a distinct cluster, subdivided further into instruments played by blowing (left subgroup, light blue, colour online) and instruments played with the upper limbs only (right subgroup), the latter of which contained a somewhat segregated group of instruments played by striking."Piano" formed a distinct branch, probably due to its    higher rating on the Lower Limb attribute.People concepts, which also formed a distinct cluster, showed evidence of a subgroup of "private sector" occupations (large left subgroup, green, colour online) and a subgroup of "public sector" occupations (middle subgroup, purple, colour online).Two other subgroups consisted of basic human types and people concepts with negative or violent associations (far right subgroup, magenta, colour online).</p>
<p>Animals also formed a distinct cluster, though two ("ant" and "butterfly") were isolated on nearby branches.The animals branched into somewhat distinct subgroups of aquatic animals (left-most subgroup, light blue, colour online), small land animals (next subgroup, yellow, colour online), large animals (next subgroup, red, colour online), and unpleasant animals (subgroup including "alligator," magenta, colour online).Interestingly, "camel" and "horse"the only animals in the set used for human transportation-segregated together despite the fact that there are no attributes that specifically encode "used for human transportation".Inspection of the vectors suggested that this segregation was due to higher ratings on both the Lower Limb and Benefit attributes than for the other animals.Association with lower limb movements and benefit to people, that is, appears to be sufficient to mark an animal as useful for transportation.Finally, Plants and Foods formed a large distinct branch, which contained subgroups of beverages, fruits, manufactured foods, vegetables, and flowers.</p>
<p>A similar hierarchical cluster analysis was performed on the 99 abstract nouns.Three major branches emerged.The largest of these comprised abstract concepts with a neutral or positive meaning.As shown in Figure 9, one subgroup within this large cluster consisted of positive or beneficial entities (subgroup from "attribute" to "gratitude," green, colour online).A second fairly distinct subgroup consisted of "causal" concepts associated with a high likelihood of consequences (subgroup from "motive" to "fate," blue, colour online).A third subgroup included concepts associated with number or quantification (subgroup from "noun" to "infinity," light blue, colour online).A number of pairs (advice/apology, treaty/ truce) and triads (irony/paradox/mystery) with similar meanings were also evident within the large cluster.</p>
<p>The second major branch of the abstract hierarchy comprised concepts associated with harm or another negative meaning (Figure 10).One subgroup within this branch consisted of words denoting negative emotions (subgroup from "guilt" to "dread," red, colour online).A second subgroup seemed to consist of social situations associated with anger (subgroup from "snub" to "grievance," green, colour online).A third subgroup was a triad (sin/curse/problem) related to difficulty or misfortune.A fourth subgroup was a triad (fallacy/folly/denial) related to error or misjudgment.</p>
<p>The final major branch of the abstract hierarchy consisted of concepts denoting time periods (day, morning, summer, spring, evening, night, winter).</p>
<p>Correlations with word frequency and imageability</p>
<p>Frequency of word use provides a rough indication of the frequency and significance of a concept.We predicted that attributes related to proximity and selfneeds would be correlated with word frequency.Ima-geability is a rough indicator of the overall salience of sensory, particularly visual, attributes of an entity, and thus we predicted correlations with attributes related to sensory and motor experience.An alpha level of  .00001(r = .1673)was adopted to reduce family-wise Type 1 error from the large number of tests.</p>
<p>Word frequency was positively correlated with Near, Self, Benefit, Drive, and Needs, consistent with the expectation that word frequency reflects the proximity and survival significance of concepts.Word frequency was also positively correlated with attributes characteristic of people, including Face, Body, Speech, and Human, reflecting the proximity and significance of conspecific entities.Word frequency was negatively correlated with a number of sensory attributes, including Colour, Pattern, Small, Shape, Temperature, Texture, Weight, Audition, Loud, Low, High, Sound, Music, and Taste.One possible explanation for this is the tendency for some natural object categories with very salient perceptual features to have far less salience in everyday experience and language use, particularly animals, plants, and musical instruments.</p>
<p>As expected, most of the sensory and motor attributes were positively correlated (p &lt; .00001)with imageability, including Vision, Bright, Dark, Colour, Pattern, Large, Small, Motion, Biomotion, Fast, Slow, Shape, Complexity, Touch, Temperature, Weight, Loud, Low, High, Sound, Taste, Smell, Upper Limb, and Practice.Also positively correlated were the visual-spatial attributes Landmark, Scene, Near, and Toward.Conversely, many non-perceptual attributes were negatively correlated with imageability, including temporal and causal components (Duration, Long, Short, Caused, Consequential), social and cognitive components (Social, Human, Communication, Self, Cognition), and affective/arousal components (Unpleasant, Sad, Angry, Disgusted, Fearful, Surprised, Drive, Arousal).</p>
<p>Correlations with non-semantic lexical variables</p>
<p>The large size of the dataset afforded an opportunity to look for unanticipated relationships between semantic attributes and non-semantic lexical variables.Previous research has identified various phonological correlates of meaning (Lynott &amp; Connell, 2013;Schmidtke, Conrad, &amp; Jacobs, 2014) and grammatical class (Farmer, Christiansen, &amp; Monaghan, 2006;Kelly, 1992).These data suggest that the evolution of word forms is not entirely arbitrary, but is influenced in part by meaning and pragmatic factors.In an exploratory analysis, we tested for correlation between the 65 semantic attributes and measures of length (number of phonemes), orthographic typicality (mean position-constrained bigram frequency, orthographic neighbourhood size), and phonologic typicality (phonologic neighbourhood size).An alpha level of .00001was again adopted to reduce family-wise Type 1 error from the large number of tests and to focus on very strong effects that are perhaps more likely to hold across other random samples of words.</p>
<p>Word length (number of phonemes) was negatively correlated with Near and Needs, with a strong trend (p &lt; .0001)for Practice, suggesting that shorter words are used for things that are near to us, central to our needs, and frequently manipulated.Similarly, orthographic neighbourhood size was positively correlated with Near, Needs, and Practice, with strong trends for Touch and Self, and phonological neighbourhood size was positively correlated with Near and Practice, with strong trends for Needs and Touch.Together, these correlations suggest that concepts referring to nearby objects of significance to our self needs tend to be represented by shorter or simpler morphemes with commonly shared spelling patterns.Position-constrained bigram frequency was not significantly correlated with any of the attributes, however, suggesting that semantic variables mainly influence segments larger than letter pairs.</p>
<p>Potential applications</p>
<p>The intent of the current study was to outline a relatively comprehensive brain-based componential model of semantic representation and to explore some of the properties of this type of representation.Building on extensive previous work (Allport, 1985;Borghi et al., 2011;Cree &amp; McRae, 2003;Crutch et al., 2013;Crutch et al., 2012;Gainotti et al., 2009Gainotti et al., , 2013;;Hoffman &amp; Lambon Ralph, 2013;Lynott &amp; Connell, 2009, 2013;Martin &amp; Caramazza, 2003;Tranel et al., 1997;Vigliocco et al., 2004;Warrington &amp; McCarthy, 1987;Zdrazilova &amp; Pexman, 2013), the representation was expanded to include a variety of sensory and motor subdomains, more complex physical domains (space, time), and mental experiences, guided by the principle that all aspects of experience are encoded in the brain and processed according to information content.The utility of the representation ultimately depends on its completeness and veridicality, which are determined in no small part by the specific attributes included and details of the queries used to elicit attribute salience values.These choices are almost certainly imperfect, and thus we view the current work as a preliminary exploration that we hope will lead to future improvements.</p>
<p>The representational scheme is motivated by an underlying theory of concept representation in the brain, and its principal utility-and main source of validation-is likely to be in brain research.A recent study by Fernandino et al. (Fernandino et al., 2015) suggests one type of application.The authors obtained ratings for 900 noun concepts on the five attributes Colour, Visual Motion, Shape, Sound, and Manipulation.Functional MRI scans, performed while participants read these 900 words and performed a concreteness decision, showed distinct brain networks modulated by the salience of each attribute, as well as multi-level convergent areas that responded to various subsets or all of the attributes.Future studies along these lines could incorporate a much broader range of information types both within the sensory domains and across sensory and non-sensory domains.</p>
<p>Another likely application is in the study of category-related semantic deficits induced by brain damage.Patients with these syndromes show differential abilities to process object concepts from broad (living vs. artefact) or more specific (animal, tool, plant, body part, musical instrument, etc.) categories.The embodiment account of these phenomena posits that object categories differ systematically in terms of the type of sensory-motor knowledge on which they depend.Living things, for example, are cited as having more salient visual attributes, whereas tools and other artefacts have more salient action-related attributes (Farah &amp; McClelland, 1991;Warrington &amp; McCarthy, 1987).As discussed by Gainotti (Gainotti, 2000), greater impairment of knowledge about living things is typically associated with anterior ventral temporal lobe damage, which is also an area that supports high-level visual object perception, whereas greater impairment of knowledge about tools is associated with frontoparietal damage, an area that supports object-directed actions.On the other hand, counterexamples have been reported, including patients with high-level visual perceptual deficits but no selective impairment for living things and patients with apparently normal visual perception despite a selective living thing impairment (Capitani et al., 2003).</p>
<p>The current data, however, support more recent proposals suggesting that category distinctions can arise from much more complex combinations of attributes (Cree &amp; McRae, 2003;Gainotti et al., 2013;Hoffman &amp; Lambon Ralph, 2013;Tranel et al., 1997).As exemplified in Figures 3-5, concrete object categories differ from each other across multiple domains, including attributes related to specific visual subsystems (visual motion, biological motion, face perception), sensory systems other than vision (sound, taste, smell), affective and arousal associations, spatial attributes, and various attributes related to social cognition.Damage to any of these processing systems, or damage to spatially proximate combinations of them, could account for differential category impairments.As one example, the association between living thing impairments and anterior ventral temporal lobe damage may not be due to impairment of high-level visual knowledge per se, but rather to damage to a zone of convergence between high-level visual, taste, smell, and affective systems (Gainotti et al., 2013).</p>
<p>The current data also clarify the diagnostic attribute composition of a number of salient categories that have not received systematic attention in the neuropsychological literature, such as foods, musical instruments, vehicles, human roles, places, events, time concepts, locative change actions, and social actions.Each of these categories is defined by a unique subset of particularly salient attributes, and thus brain damage affecting knowledge of these attributes could conceivably give rise to an associated categoryspecific impairment.Several novel distinctions emerged from a data-driven cluster analysis of the concept vectors (Table 8), such as the distinction between social and non-social places, verbal and festive social events, and three other event types (negative, sound, and ingestive).Although these data-driven clusters probably reflect idiosyncrasies of the concept sample to some degree, they raise a number of intriguing possibilities that can be addressed in future studies using a wider range of concepts.</p>
<p>Application to some general problems in componential semantic theory Apart from its utility in empirical neuroscience research, a brain-based componential representation might provide alternative answers to some longstanding theoretical issues.Several of these are discussed briefly below.</p>
<p>Feature selection</p>
<p>All analytic or componential theories of semantic representation contend with the general issue of feature selection.What counts as a feature, and how can the set of features be identified?As discussed above, standard verbal feature theories face a basic problem with feature set size, in that the number of all possible object and action features is extremely large.Here we adopt a fundamentally different approach to feature selection, in which the content of a concept is not a set of verbal features that people associate with the concept, but rather the set of neural processing modalities (i.e., representation classes) that are engaged when experiencing instances of the concept.The underlying motivation for this approach is that it enables direct correspondences to be made between conceptual content and neural representations, whereas such correspondences can only be inferred post hoc from verbal feature sets, and only by mapping such features to neural processing modalities (Cree &amp; McRae, 2003).A consequence of defining conceptual content in terms of brain systems is that those systems provide a closed and relatively small set of basic conceptual components.Although the adequacy of these components for capturing fine distinctions in meaning is not yet clear, the basic assumption that conceptual knowledge is distributed across modality-specific neural systems offers a potential solution to the longstanding problem of feature selection.</p>
<p>Feature weighting</p>
<p>Standard verbal feature representations also face the problem of defining the relative importance of each feature to a given concept.As Murphy and Medin (Murphy &amp; Medin, 1985) put it, a zebra and a barber pole can be considered close semantic neighbours if the feature "has stripes" is given enough weight.In determining similarity, what justifies the intuition that "has stripes" should not be given more weight than other features?Other examples of this problem are instances in which the same feature can be either critically important or irrelevant for defining a concept.Keil (Keil, 1991) made the point as follows: polar bears and washing machines are both typically white.Nevertheless, an object identical to a polar bear in all respects except colour is probably not a polar bear, while an object identical in all respects to a washing machine is still a washing machine regardless of its colour.Whether or not the feature "is white" matters to an item's conceptual status thus appears to depend on its other properties-there is no single weight that can be given to the feature that will always work.Further complicating this problem is the fact that in feature production tasks, people often neglect to mention some features.For instance, in listing properties of dogs, people rarely say, "has blood" or "can move" or "has DNA" or "can reproduce"-yet these features are central to our conception of animals.</p>
<p>Our theory proposes that attributes are weighted according to statistical regularities.If polar bears are always experienced as white, then colour becomes a strongly weighted attribute of polar bears.Colour would be a strongly weighted attribute of washing machines if it were actually true that washing machines are nearly always white.In actuality, however, washing machines and most other artefacts usually come in a variety of colours, so colour is a less strongly weighted attribute of most artefacts.A few artefacts provide counter-examples.Stop signs, for example, are always red.Consequently, a sign that was not red would be a poor exemplar of a stop sign even if it had the word "Stop" on it.School buses are virtually always yellow, thus a grey or black or green bus would be unlikely to be labelled a school bus.Semantic similarity is determined by overall similarity across all attributes rather than by a single attribute.Although barber poles and zebras both have salient visual patterns, they strongly differ in salience on many other attributes (shape complexity, biological motion, body parts, and motion through space) that would preclude them from being considered semantically similar.</p>
<p>Attribute weightings in our theory are obtained directly from human participants by averaging salience ratings provided by the participants.Rather than asking participants to list the most salient attributes for a concept, a continuous rating is required for each attribute.This method avoids the problem of attentional bias or pragmatic phenomena that result in incomplete representations using the feature listing procedure (Hoffman &amp; Lambon Ralph, 2013).The resulting attributes are graded rather than binary and thus inherently capture weightings.An example of how this works is given in Figure 11, which includes a portion of the attribute vectors for the concepts egg and bicycle.Given that these concepts are both inanimate objects, they share low weightings on human-related attributes (biological motion, face, body part, speech, social, communication, emotion, and cognition attributes), auditory attributes, and temporal attributes.However, they also differ in expected ways, including stronger weightings for egg on brightness, colour, small size, tactile texture, weight, taste, smell, and head action attributes, and stronger weightings for bicycle on motion, fast motion, visual complexity, lower limb action, and path motion attributes.</p>
<p>Abstract concepts</p>
<p>It is not immediately clear how embodiment theories of concept representation account for concepts that are not reliably associated with sensory-motor structure.These include obvious examples like justice and piety, for which people have difficulty generating reliable feature lists and whose exemplars do not exhibit obvious coherent structure.They also include somewhat more concrete kinds of concepts where the relevant structure does not clearly reside in perception or action.Social categories provide one example: categories like male encompass items that are grossly perceptually different (consider infant, child, and elderly human males, or the males of any given plant or animal species, which are certainly more similar to the female of the same species than to the males of different species); categories like Catholic or Protestant do not appear to reside in sensory-motor structure; concepts like pauper, liar, idiot and so on are important for organizing our social interactions and inferences about the world but refer to economic circumstances, behaviours, and mental predispositions that are not obviously reflected in the perceptual and motor structure of the environment.In these and many other cases it is difficult to see how the relevant concepts are transparently reflected in the feature structure of the environment.</p>
<p>The experiential content and acquisition of abstract concepts from experience has been discussed by a number of authors (Altarriba &amp; Bauer, 2004;Barsalou &amp; Wiemer-Hastings, 2005;Borghi et al., 2011;Bretherton &amp; Beeghly, 1982;Crutch et al., 2013;Crutch &amp; Warrington, 2005;Crutch et al., 2012;Kousta et al., 2011;Lakoff &amp; Johnson, 1980;Schwanenflugel, 1991;Vigliocco et al., 2009;Wiemer-Hastings &amp; Xu, 2005;Zdrazilova &amp; Pexman, 2013).Although abstract concepts encompass a variety of experiential domains (spatial, temporal, social, affective, cognitive, etc.) and are therefore not a homogeneous set, one general observation is that many abstract concepts are learned by experience with complex situations and events.As Barsalou (Barsalou, 1999) points out, such situations often include multiple agents, physical events, and mental events.This contrasts with concrete concepts, which typically refer to a single component (usually an object or action) of a situation.In both cases concepts are learned through experience, but abstract concepts differ from concrete concepts in the distribution of content over entities, space, and time.Another way of saying this is that abstract concepts seem "abstract" because their content is distributed across multiple components of situations.</p>
<p>Another aspect of many abstract concepts is that their content refers to aspects of mental experience rather than to sensory-motor experience.Many abstract concepts refer specifically to cognitive events or states (think, believe, know, doubt, reason) or to products of cognition (concept, theory, idea, whim).Abstract concepts also tend to have stronger affective content than do concrete concepts (Borghi et al., 2011;Kousta et al., 2011;Troche et al., 2014;Vigliocco et al., 2009;Zdrazilova &amp; Pexman, 2013), which may explain the selective engagement of anterior temporal regions by abstract relative to concrete concepts in many neuroimaging studies (see Binder, 2007;Binder et al., 2009, for reviews).Many so-called abstract concepts refer specifically to affective states or qualities (anger, fear, sad, happy, disgust).One crucial aspect of the current theory that distinguishes it from some other versions of embodiment theory is that these cognitive and affective mental experiences count every bit as much as sensory-motor experiences in grounding conceptual knowledge.Experiencing an affective or cognitive state or event is like experiencing a sensory-motor event except that the object of perception is internal rather than external.This expanded notion of what counts as embodied experience adds considerably to the descriptive power of the conceptual representation, particularly for abstract concepts.</p>
<p>One prominent example of how the model enables representation of abstract concepts is by inclusion of attributes that code social knowledge (see also Crutch et al., 2013;Crutch et al., 2012;Troche et al., 2014).Empirical studies of social cognition have identified several neural systems that appear to be selectively involved in supporting theory of mind processes, "self" representation, and social concepts (Araujo et al., 2013;Northoff et al., 2006;Olson, McCoy, Klobusicky, &amp; Ross, 2013;Ross &amp; Olson, 2010;Schilbach et al., 2012;Schurz et al., 2014;Simmons, Reddish, Bellgowan, &amp; Martin, 2010;Spreng et al., 2009;Van Overwalle, 2009;van Veluw &amp; Chance, 2014;Zahn et al., 2007).Many abstract words are social concepts (cooperate, justice, liar, promise, trust) or have salient social content (Borghi et al., 2011).An example of how attributes related to social cognition play a role in representing abstract concepts is shown in Figure 11, which compares the attribute vectors for egg and bicycle with the attribute vector for agreement.As the figure shows, ratings on sensory-motor attributes are generally low for agreement, but this concept receives much higher ratings than the other concepts on social cognition attributes (Caused, Consequential, Social, Human, Communication).It also receives higher ratings on several temporal attributes (Duration, Long) and on the Cognition attribute.Note also the high rating on the Benefit attribute and the small but clearly non-zero rating on the head action attribute, both of which might serve to distinguish agreement from many other abstract concepts that are not associated with benefit or with actions of the face/head.</p>
<p>Although these and many other examples illustrate how abstract concepts are often tied to particular kinds of mental, affective, and social experiences, we do not deny that language plays a large role in facilitating the learning of abstract concepts.Language provides a rich system of abstract symbols that efficiently "point to" complex combinations of external and internal events, enabling acquisition of new abstract concepts by association with older ones.In the end, however, abstract concepts, like all concepts, must ultimately be grounded in experience, albeit experiences that are sometimes very complex, distributed in space and time, and composed largely of internal mental events.</p>
<p>Context effects and ad hoc categories</p>
<p>The relative importance given to particular attributes of a concept varies with context.In the context of moving a piano, the weight of the piano matters more than its function, and consequently the piano is likely to be viewed as more similar to a couch than to a guitar.In the context of listening to a musical group, the piano's function is more relevant than its weight, and consequently it is more likely to be conceived as similar to a guitar than to a couch (Barsalou, 1982(Barsalou, , 1983)).Componential approaches to semantic representation assume that the weight given to different feature dimensions can be controlled by context, but the mechanism by which such weight adjustments occur is unclear.The possibility of learning different weightings for different contexts has been explored for simple category learning tasks (Minda &amp; Smith, 2002;Nosofsky, 1986), but the scalability of such an approach to real semantic cognition is questionable and seems ill-suited to explain the remarkable flexibility with which human beings can exploit contextual demands to create and employ new categories on the spot (Barsalou, 1991).</p>
<p>We propose that activation of attribute representations is modulated continuously through two mechanisms: top-down attention and interaction with attributes of the context itself.The context draws attention to context-relevant attributes of a concept.In the case of lifting or preparing to lift a piano, for example, attention is focused on action and proprioception processing in the brain, and this top-down focus enhances activation of action and proprioceptive attributes of the piano.This idea is illustrated in highly simplified schematic form on the left side of Figure 12.The concept of piano is represented for illustrative purposes by set of verbal features (first column of ovals, red, colour online), which are coded in the brain in domain-specific neural processing systems (second column of ovals, green, colour online).The context of lifting draws attention to a subset of these neural systems, enhancing their activation.Changing the context to playing or listening to music (right side of Figure 12) shifts attention to a different subset of attributes, with corresponding enhancement of this subset.In addition to effects mediated by attention, there could be direct interactions at the neural system level between the distributed representation of the target concept (piano) and the context concept.The concept of lifting, for example, is partly encoded in action and proprioceptive systems that overlap with those encoding piano.As discussed in more detail in the following section on conceptual combination, we propose that overlapping neural representations of two or more concepts can produce mutual enhancement of the overlapping components.</p>
<p>The enhancement of context-relevant attributes of concepts alters the relative similarity between concepts, as illustrated by the piano-couch-guitar example given above.This process not infrequently results in purely functional groupings of objects (e.g., things one can stand on to reach the top shelf) or "ad hoc categories" (Barsalou, 1983).The above account provides a partial mechanistic account of such categories: Ad hoc categories are formed when concepts share the same context-related attribute enhancement.</p>
<p>Conceptual combination</p>
<p>Language use depends on the ability to productively combine concepts to create more specific or more complex concepts.Some simple examples include modifier-noun (red jacket), noun-noun (ski jacket), subject-verb (the dog ran), and verb-object (hit the ball) combinations.Semantic processes underlying conceptual combination have been explored in numerous studies (Clark &amp; Berman, 1987;Conrad &amp; Rips, 1986;Downing, 1977;Gagné, 2001;Gagné &amp; Murphy, 1996;Gagné &amp; Shoben, 1997;Levi, 1978;Medin &amp; Shoben, 1988;Murphy, 1990;Rips, Smith, &amp; Shoben, 1978;Shoben, 1991;Smith &amp; Osherson, 1984;Springer &amp; Murphy, 1992).We begin here by attempting to define more precisely what kinds of issues a neural theory of conceptual combination might address.First among these is the general mechanism by which conceptual representations interact.This mechanism should explain how new meanings can "emerge" by combining individual concepts that have very different meanings.The concept house, for example, has few features in common with the concept boat, yet the combination house boat is nevertheless a meaningful and unique concept that emerges by combining these constituents.The second kind of issue that such a theory should address are the factors that cause variability in how particular features interact.The concepts house boat and boat house contain the same constituents but have entirely different meanings, indicating that constituent role (here modifier vs. head noun) is a major factor determining how concepts combine.Other combinations illustrate that there are specific semantic factors that cause variability in how constituents combine.For example, a cancer therapy is a therapy for cancer, but a water therapy is not a therapy for water.In our view it is not the task of a semantic theory to list and describe every possible such combination, but rather to propose general principles that govern underlying combinatorial processes.</p>
<p>As a relatively straightforward illustration of how neural attribute representations might interact during conceptual combination, and an illustration of the potential explanatory power of such a representation, we consider the classical feature animacy, which is a well-recognized determinant of the meaningfulness of modifier-noun and noun-verb combinations (as well as pronoun selection, word order, and aspects of morphology).Standard verbal feature-based models and semantic models in linguistics represent animacy as a binary feature.The difference in meaningfulness between (1) and (2) below:</p>
<p>(1) the angry boy (2) *the angry chair is "explained" by a rule that requires an animate argument for the modifier angry.Similarly, the difference in meaningfulness between (3) and (4) below:</p>
<p>(1) the boy planned (2) *the chair planned is "explained" by a rule that requires an animate argument for the verb plan.The weakness of this kind of theoretical approach is that it amounts to little more than a very long list of specific rules that are in essence a restatement of the observed phenomena.Absent from such a theory are accounts of why particular concepts "require" animate arguments or even why particular concepts are animate.Also unexplained is the well-established "animacy hierarchy" observed within and across languages, in which adult humans are accorded a relatively higher value than infants and some animals, followed by lower animals, then plants, then non-living objects, then abstract categories (Yamamoto, 2006), suggesting rather strongly that animacy is a graded continuum rather than a binary feature.</p>
<p>From a developmental and neurobiological perspective, knowledge about animacy reflects a combination of various kinds of experience, including perception of biological motion, perception of causality, perception of one's own internal affective and drive states, and the development of theory of mind.Biological motion perception affords an opportunity to recognize from vision those things that move under their own power.Movements that are nonmechanical due to higher order complexity are simultaneously observed in other entities and in one's own movements, giving rise to an understanding (possibly mediated by "mirror" neurons) that these kinds of movements are executed by the moving object itself rather than by an external controlling force.Perception of causality, beginning with visual perception of simple collisions and applied force vectors, and progressing to multimodal and higher order events, provides a basis for understanding how biological movements can effect change.Perception of internal drive states and of sequences of events that lead to satisfaction of these needs provides a basis for understanding how living agents with needs effect changes.This understanding, together with the recognition that other living things in the environment effect changes to meet their own needs, provides a basis for theory of mind.On this account, the construct "animacy", far from being a binary symbolic property, arises from complex and interacting sensory, motor, affective, and cognitive experiences.</p>
<p>How might knowledge about animacy be represented and interact across lexical items at a neural level?As suggested schematically in Figure 13, we propose that interactions occur when two concepts activate a similar set of modal networks, and they occur less extensively when there is less attribute overlap.In the case of animacy, for example, a noun that engages biological motion, face and body part, affective, and social cognition networks (e.g., "boy") will produce more extensive neural interaction with a modifier that also activates these networks (e.g., "angry") than will a noun that does not activate these networks (e.g., "chair").</p>
<p>To illustrate how such differences can arise even from a simple multiplicative interaction, we examined the vectors that result from multiplying mean attribute vectors of modifier-noun pairs with varying degrees of meaningfulness.Figure 14 (bottom) shows these interaction values for the pairs "angry<em>boy" and "angry</em>chair".As shown in the figure, boy and angry interact as anticipated, showing enhanced values for attributes concerned with biological motion, face and body part perception, speech production, and social and human characteristics, whereas interactions between chair and angry are negligible.Averaging the interaction values across all attributes gives a mean interaction value of 3.26 for angry<em>boy and 0.83 for angry</em>chair.</p>
<p>Figure 15 shows the average of these mean interaction values for 116 nouns, divided by taxonomic category.The averages roughly approximate the "animacy hierarchy", with strong interactions for nouns that refer to people (boy, girl, man, woman, family, couple, etc.) and human occupation roles (doctor, lawyer, politician, teacher, etc.), much weaker interactions for animal concepts, and the weakest interactions for plants.</p>
<p>The general principle suggested by such data is that concepts acquired within similar neural processing domains are more likely to have similar semantic structures that allow combination.In the case of "animacy" (which we interpret as a verbal label summarizing a particular pattern of attribute weightings, rather than an a priori binary feature), a common semantic structure captures shared "human-related" attributes that allow certain concepts to be meaningfully combined.A chair cannot be angry because chairs do not have movements, faces, or minds that are essential for feeling and expressing anger, and this observation applies to all concepts that lack these attributes.The power of a comprehensive attribute representation constrained by neurobiological and cognitive developmental data is that it has the potential to provide a firstprinciples account of why concepts vary in animacy, as well as a mechanism for predicting which concepts should "require" animate arguments.</p>
<p>We have focused here on animacy because it is a strong and at the same time a relatively complex and poorly understood factor influencing conceptual combination.Similar principles might conceivably be applied, however, in other difficult cases.For the cancer therapy versus water therapy example given above, the difference in meaning that results from the two combinations is probably due to the fact that cancer is a negatively valenced concept, whereas water and therapy are usually thought of as beneficial.This difference results in very different constituent relations (therapy for vs. therapy with).A similar example is the difference between lake house and dog house.A lake house is a house located at a lake, but a dog house is not a house located at a dog.This difference reflects the fact that both houses and lakes have fixed locations (i.e., do not move and can be used as topographic landmarks), whereas this is not true of dogs.The general principle in these cases is that the meaning of a combination is strongly determined by attribute congruence.When Concepts A and B have opposite congruency relations with Concept C, the combinations AC and BC are likely to capture very different constituent relationships.The content of these relationships reflects the underlying attribute structure of the constituents, as demonstrated by the locative relationship located at arising from the combination lake house but not dog house.</p>
<p>At the neural level, interactions during conceptual combination are likely to take the form of additional processing within the neural systems where attribute representations overlap.This additional processing is necessary to compute the "new" attribute representations resulting from conceptual combination, and these new representations tend to have added salience.As a simple example involving unimodal  modifiers, imagine what happens to the neural representation of dog following the modifier brown or the modifier loud.In the first case, there is residual activation of the colour processor by brown, which, when combined with the neural representation of dog, causes additional computation (i.e., additional neural activity) in the colour processor because of the interaction between the colour representations of brown and dog.In the second case, there is residual activation of the auditory processor by loud, which, when combined with the neural representation of dog, causes additional computation in the auditory processor because of the interaction between the auditory representations of loud and dog.As mentioned in the previous section on context effects, a similar mechanism is proposed (along with attentional modulation) to underlie situational context effects, because the context situation also has a conceptual representation.Attributes of the target concept that are "relevant" to the context are those that overlap with the conceptual representation of the context, and this overlap results in additional processorspecific activation.Seen in this way, situational context effects (e.g., lifting vs. playing a piano) are essentially a special case of conceptual combination.</p>
<p>Scope and limitations of the theory</p>
<p>We have made a systematic attempt to relate semantic content to large-scale brain networks and biologically plausible accounts of concept acquisition.The approach offers potential solutions to several longstanding problems in semantic representation, particularly the problems of feature selection, feature weighting, and specification of abstract word content.The primary aim of the theory is to better understand lexical semantic representation in the brain and, specifically, to develop a more comprehensive theory of conceptual grounding that encompasses the full range of human experience.</p>
<p>Although we have proposed several general mechanisms that may explain context and combinatorial effects, much more work is needed to achieve an account of word combinations and syntactic effects on semantic composition.Our simple attribute-based account of why the locative relation AT arises from lake house, for example, does not explain why house lake fails despite the fact that both nouns have fixed locations.A plausible explanation is that the syntactic roles of the constituents specify a head-modifier = ground-figure isomorphism that requires the modifier concept to be larger in size than the head noun concept.In principle, this behaviour could be captured by combining the size attribute ratings for the nouns with information about their respective syntactic roles.Previous studies have shown such effects to vary widely in terms of the types of relationships that arise between constituents and the "rules" that govern their lawful combination (Clark &amp; Berman, 1987;Downing, 1977;Gagné, 2001;Gagné &amp; Murphy, 1996;Gagné &amp; Shoben, 1997;Levi, 1978;Medin &amp; Shoben, 1988;Murphy, 1990).We assume that many other attributes could be involved in similar interactions.</p>
<p>The explanatory power of a semantic representation model that refers only to "types of experience" (i.e., one that does not incorporate all possible verbally generated features) is still unknown.It is far from clear, for example, that an intuitively basic distinction like male/ female can be captured by such a representation.The model proposes that the concepts male and female, as applied to human adults, can be distinguished on the basis of sensory, affective, and social experiences without reference to specific encyclopaedic features.This account appears to fail, however, when male and female are applied to animals, plants, or electronic connectors, in which case there is little or no overlap in the experiences associated with these entities that could explain what is male or female about all of them.Such cases illustrate the fact that much of conceptual knowledge is learned directly via language and with only very indirect grounding in experience.In common with other embodied cognition theories, however, our model maintains that all concepts are ultimately grounded in experience, whether directly or indirectly through language that refers to experience.Establishing the validity, utility, and limitations of this principle, however, will require further study.</p>
<p>The underlying research materials for this article can be accessed at http://www.neuro.mcw.edu/resources.html.</p>
<p>Figure 1 .Figure 2 .
12
Figure 1.Mean attribute vectors for 6 concrete object noun categories.Attributes are grouped and colour-coded by general domain and similarity to assist interpretation.Blackness of the attribute labels reflects the mean attribute value.Error bars represent standard error of the mean.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 3 .
3
Figure 3. Mean attribute vectors for 3 verb categories.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 4 .
4
Figure 4. Attributes distinguishing Artefacts, Living Things, and Abstract Entities.Pairwise differences significant at p &lt; .0001are indicated by the coloured squares above the graph.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 5 .
5
Figure 5. Attributes distinguishing Animals, Plants, and Tools.Pairwise differences significant at p &lt; .0001are indicated by the coloured squares above the graph.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 6 .
6
Figure 6.Attributes distinguishing Musical Instruments, Tools, and Vehicles.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 7 .
7
Figure 7. (A) Cosine similarity matrices for the 434 nouns in the study, displayed as heat maps, with words grouped by superordinate category.The matrix at left was computed using the brain-based vectors, and the matrix at right was computed using latent semantic analysis vectors.Yellow indicates greater similarity.(B) Average effect sizes (Cohen's d ) for comparisons of within-category versus between-category cosine similarity values.An effect size was computed for each word, then these were averaged across all words.Error bars indicate 99% confidence intervals.BBR = brain-based representation; LSA = latent semantic analysis.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 8 .
8
Figure 8. Dendrogram segments from the hierarchical cluster analysis on concrete nouns.Musical instruments, people, animals, and plants/foods each clustered together on separate branches, with evidence of sub-clusters within each branch.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 9 .
9
Figure 9. Neutral abstract branches of the abstract noun dendrogram.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 10 .
10
Figure 10.Negative abstract branches of the abstract noun dendrogram.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 11 .
11
Figure 11.Mean ratings for the concepts egg, bicycle, and agreement.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 12 .
12
Figure 12.Schematic illustration of context effects in the proposed model.Neurobiological systems that represent and process conceptual attributes are shown in green, with font weights indicating relative amounts of processing (i.e., neural activity).Computations within these systems give rise to particular feature representations, shown in red.As a context is processed, this enhances neural activity in the subset of neural systems that represent the context, increasing the activation strength of the corresponding features of the concept.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 13 .
13
Figure 13.Schematic illustration of conceptual combination in the proposed model.Combination occurs when concepts share overlapping neural attribute representations.The concepts angry and boy share attributes that define animate concepts.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Figure 15 .
15
Figure15.Average mean interaction values for combinations of angry with a noun, for eight noun categories.Interactions are higher for human concepts (people and occupation roles) than for any of the non-human concepts (all p &lt; .001).Error bars represent standard error of the mean.</p>
<p>Figure 14 .
14
Figure 14.Top: Mean attribute ratings for boy, chair, and angry.Bottom: Interaction values for the combinations angry boy and angry chair.[To view this figure in colour, please see the online version of this Journal.]</p>
<p>Table 1 .
1
Sensory and motor components, organized by domain.
DomainComponentDescription (with reference to nouns)VisionVisionsomething that you can easily seeVisionBrightvisually light or brightVisionDarkvisually darkVisionColourhaving a characteristic or defining colourVisionPatternhaving a characteristic or defining visual textureor surface patternVisionLargelarge in sizeVisionSmallsmall in sizeVisionMotionshowing a lot of visually observable movementVisionBiomotionshowing movement like that of a living thingVisionFastshowing visible movement that is fastVisionSlowshowing visible movement that is slowVisionShapehaving a characteristic or defining visual shape orformVisionComplexityvisually complexVisionFacehaving a human or human-like faceVisionBodyhaving human or human-like body partsSomaticTouchsomething that you could easily recognize bytouchSomaticTemperature hot or cold to the touchSomaticTexturehaving a smooth or rough texture to the touchSomaticWeightlight or heavy in weightSomaticPainassociated with pain or physical discomfortAuditionAuditionsomething that you can easily hearAuditionLoudmaking a loud soundAuditionLowhaving a low-pitched soundAuditionHighhaving a high-pitched soundAuditionSoundhaving a characteristic or recognizable sound orsoundsAuditionMusicmaking a musical soundAuditionSpeechsomeone or something that talksGustation Tastehaving a characteristic or defining tasteOlfaction Smellhaving a characteristic or defining smell or smellsMotorHeadassociated with actions using the face, mouth, ortongueMotorUpper limbassociated with actions using the arm, hand, orfingersMotorLower limbassociated with actions using the leg or footMotorPracticea physical object YOU have personal experienceusing</p>
<p>Table 2 .
2
Spatial, temporal, causal, social, emotion, drive, and  attention components.
DomainNameDescription (with reference to nouns)SpatialLandmarkhaving a fixed location, as on a mapSpatialPathshowing changes in location along aparticular direction or pathSpatialScenebringing to mind a particular setting orphysical locationSpatialNearoften physically near to you (within easyreach) in everyday lifeSpatialTowardassociated with movement toward or into youSpatialAwayassociated with movement away from or outof youSpatialNumberassociated with a specific number or amountTemporal Timean event or occurrence that occurs at a typicalor predictable timeTemporal Durationan event that has a predictable duration,whether short or longTemporal Longan event that lasts for a long period of timeTemporal Shortan event that lasts for a short period of timeCausalCausedcaused by some clear preceding event, action,or situationCausalConsequentiallikely to have consequences (cause otherthings to happen)SocialSocialan activity or event that involves aninteraction between peopleSocialHumanhaving human or human-like intentions,plans, or goalsSocialCommunication a thing or action that people use tocommunicateSocialSelfrelated to your own view of yourself, a part ofYOUR self-imageCognition Cognitiona form of mental activity or a function of themindEmotionBenefitsomeone or something that could help orbenefit you or othersEmotionHarmsomeone or something that could cause harmto you or othersEmotionPleasantsomeone or something that you find pleasantEmotionUnpleasantsomeone or something that you findunpleasantEmotionHappysomeone or something that makes you feelhappyEmotionSadsomeone or something that makes you feelsadEmotionAngrysomeone or something that makes you feelangryEmotionDisgustedsomeone or something that makes you feeldisgustedEmotionFearfulsomeone or something that makes you feelafraidEmotionSurprisedsomeone or something that makes you feelsurprisedDriveDrivesomeone or something that motivates you todo somethingDriveNeedssomeone or something that would be hardfor you to live withoutAttention Attentionsomeone or something that grabs yourattentionAttention Arousalsomeone or something that makes you feelalert, activated, excited, or keyed up ineither a positive or negative way</p>
<p>Table 3 .
3
Characteristics of the 535 rated words.
CharacteristicMean SDMinMax Median IQRLength in letters5.95 1.95 31163Log(10) orthographic1.08 0.80 −1.61 3.05 1.171.12frequencyOrthographic neighbours4.19 5.33 02226Imageability (1-7 scale)5.17 1.16 1.406.90 5.581.96Note: Orthographic frequency values are from the Westbury Lab UseNetCorpus</p>
<p>Table 4 .
4
Cell counts for grammatical classes and categories included in the ratings study.
TypeNExamplesNouns (434)Concrete objects (275)Living things (126)Animals30crow, horse, moose, snake, turtle, whaleBody parts14finger, hand, mouth, muscle, nose, shoulderHumans42artist, child, doctor, mayor, parent, soldierHuman groups10army, company, council, family, jury, teamPlants30carrot, eggplant, elm, rose, tomato, tulipOther natural objects (19)Natural scenes12beach, forest, lake, prairie, river, volcanoMiscellaneous7cloud, egg, feather, stone, sunArtifacts (130)Furniture7bed, cabinet, chair, crib, desk, shelves, tableHand tools27axe, comb, glass, keyboard, pencil, scissorsManufactured foods18beer, cheese, honey, pie, spaghetti, teaMusical instruments20banjo, drum, flute, mandolin, piano, tubaPlaces/buildings28bridge, church, highway, prison, school, zooVehicles20bicycle, car, elevator, plane, subway, truckMiscellaneous10book, computer, door, fence, ticket, windowConcrete events (60)Social events35barbecue, debate, funeral, party, rally, trialNonverbal sound events11belch, clang, explosion, gasp, scream, whineWeather events10cyclone, flood, landslide, storm, tornadoMiscellaneous4accident, fireworks, ricochet, stampedeAbstract entities (99)Abstract constructs40analogy, fate, law, majority, problem, theoryCognitive entities10belief, hope, knowledge, motive, optimism, witEmotions20animosity, dread, envy, grief, love, shameSocial constructs19advice, deceit, etiquette, mercy, rumour, truceTime periods10day, era, evening, morning, summer, yearVerbs (62)Concrete actions (52)Body actions10ate, held, kicked, laughed, slept, threwLocative change actions14approached, crossed, flew, left, ran, put, wentSocial actions16bought, helped, met, spoke to, stole, visitedMiscellaneous12broke, built, damaged, fixed, found, watchedAbstract actions5ended, lost, planned, read, usedStates5feared, liked, lived, saw, wantedAdjectives (39)Abstract properties13angry, clever, famous, friendly, lonely, tiredPhysical properties26blue, dark, empty, heavy, loud, shiny</p>
<p>Table 5 .
5
Example queries for six attributes.
AttributeQuery{relation}{content}ColourNoun. . . havinga characteristic or defining color?Verb. . . being associated withcolor or change in color?Adjective. . . describinga quality or type of color?TasteNoun. . . havinga characteristic or defining taste?Verb. . . being associated withtasting something?Adjective. . . describinghow something tastes?Lower limbNoun. . . being associated withactions using the leg or foot?Verb. . . beingan action or activity in which you use the leg or foot?Adjective. . . being related toactions of the leg or foot?LandmarkNoun. . . havinga fixed location, as on a map?Verb. . . beingan action or activity in which you use a mental map of your environment?Adjective. . . describingthe location of something, as on a map?HumanNoun. . . havinghuman or human-like intentions, plans, or goals?Verb. . . beingan action or activity that involves human intentions, plans, or goals?Adjective. . . being related tosomething with human or human-like intentions, plans, or goals?FearfulNoun. . . beingsomeone or something that makes you feel afraid?Verb. . . being associated withfeeling afraid?Adjective. . . being related tofeeling afraid?</p>
<p>Table 6 .
6
Summary of the attribute factor analysis.
F#EVInterpretationHighest-Loading Attributes (all &gt; .35)112.81 Vision/TouchPattern, Shape, Texture, Colour, Weight,Small, Vision, Dark, Bright210.71 NegativeDisgusted, Unpleasant, Sad, Angry, Pain,Harm, Fearful, Consequential36.93Communication Communication, Social, Head46.86AuditionSound, Audition, Loud, Low, High, Music,Attention56.18IngestionTaste, Head, Smell, Toward66.08SelfNeeds, Near, Self, Practice75.86Motion in space Path, Fast, Away, Motion, Lower Limb,Toward, Slow85.33HumanFace, Body, Speech, Human, Biomotion95.08SurpriseShort, Surprised10 4.52PlaceLandmark, Scene, Large11 4.50Upper limbUpper Limb, Touch, Music12 4.49RewardBenefit, Needs, Drive13 4.07PositiveHappy, Pleasant, Arousal, Attention14 3.22TimeDuration, Time, Number15 2.37LuminanceBright16 1.16SlowSlow
Note: Factors are listed in order of variance explained.Attributes with positive loadings are listed for each factor.F# = factor number.EV = rotated eigenvalue.</p>
<p>Table 7 .
7
Representational similarity neighbourhoods for some example words.
Word
, volcano, explosion, flood, lightning, cyclone, tornado, hurricane banjo mandolin, fiddle, accordion, xylophone, harp, drum, piano, clarinet, saxophone, trumpet bee mosquito, mouse, snake, hawk, cheetah, tiger, chipmunk, crow, bird, ant beer rum, tea, lemonade, coffee, pie, ham, cheese, mustard, ketchup, jam belch cough, gasp, scream, squeal, whine, screech, clang, thunder, loud, shouted blue green, yellow, black, white, dark, red, shiny, ivy, cloud, dandelion book computer, newspaper, magazine, camera, cash, pen, pencil, hairbrush, keyboard, umbrella</p>
<p>Table 8 .
8
K-means cluster analysis of the entire 535-word set, with k = 28.Numbers below the cluster labels indicate the number of words in the cluster.The first 10 items in each cluster are listed in order from closest to farthest distance from the cluster centroid.Parentheses indicate words that do not fit intuitively with the interpretation.
AnimalsBody partsHuman typesNeutral human roles Violent human rolesPlants and foodsLarge bright objectsn = 30n = 13n = 7n = 37n = 6n = 44n = 3chipmunkarmwomandiplomatmobapricotcloudhawkfingergirlmayorterroristasparagussunduckhandboybankercriminaltomatobonfiremooseshoulderparenteditorvictimbroccolihamstereyemanministerarmyspaghettihorselegchildguardriotjamcamelnosefamilybusinessmancheesecheetahjawreportercabbagepenguinfootpriestcucumberpigmusclejournalisttangerineNon-social placesSocial placesTools and furniture Musical instrumentsLoud vehiclesQuiet vehiclesFestive social eventsn = 22n = 20n = 43n = 23n = 6n = 18n = 15fieldofficespatulamandolinplaneboatpartyforeststorehairbrushxylophonerocketcarriagefestivalbaycathedralumbrellafiddletrainsailboatcarnivallakechurchcabinettrumpetsubwayscootercircusprairielabcorkscrewsaxophoneambulancevanmusicalapartmentparkcombbugle(fireworks)bussymphonyfencehotelwindowbanjocabtheaterbridgeembassyshelvesbagpipelimousineparadeislandfarmstaplerclarinetescalatorrallyelmcafeteriarakeharp(truck)celebratedVerbal social eventsNegativeNonverbalIngestive eventsBeneficial abstractNeutral abstractCausal entitiesnatural eventssound eventsand actionsentitiesentitiesn = 14n = 16n = 11n = 5n = 20n = 23n = 19debatecyclonesquealfedmoralhierarchyroleorationhurricanescreechatetrustsumlegalitytestimonyhailstormgaspdrankhopeparadoxpowernegotiatedstormwhinedinnermercyironylawadvicelandslidescreambarbecueknowledge(famous)theoryapologyfloodclangoptimismclueagreementpleatornado(loud)intellectwholetreatyspeechexplosioncough(spiritual)(ended)trucemeetingavalanchebelchpeaceverbfatedictationstampedethundersympathypatentmotiveNegative emotionPositive emotionTime conceptsLocative changeNeutral actionsNegative actionsSensoryentitiesentitiesactionsand propertiespropertiesn = 30n = 22n = 16n = 14n = 23n = 16n = 19jealousyjovialitymorningcrosseduseddamagedgreeniregratitudeeveningleftboughtdangerousdustyanimosityfundaywentfixedblockedexpensivedenialjoysummerrangavedestroyedyellowenvylikednewapproachedhelpedinjuredbluegrievancesatirespringlandedfoundbrokeusedguiltaweerawalkedmetaccidentwhitesnubjokeyearmarchedplayedlost(ivy)sinhappywinterflewwrotefearedredfallacy(theme)nighthikedtookstoleemptyNote:</p>
<p>Table 9 .
9
Attributes that differ significantly between human types and neutral human roles (occupations).
AttributeHuman typesNeutral human rolesBright<em>0.80 (0.28)0.37 (0.19)Small</em>1.73 (1.19)0.63 (0.29)Shape<em>3.96 (0.81)2.45 (0.55)Complexity2.58 (0.50)1.78 (0.38)Touch</em>2.22 (0.83)0.50 (0.25)Temperature0.69 (0.37)0.34 (0.14)Texture<em>1.69 (1.01)0.64 (0.24)High</em>1.69 (1.12)0.39 (0.22)Sound<em>2.73 (0.58)1.30 (0.52)Smell</em>1.27 (0.61)0.36 (0.28)Near<em>2.91 (0.77)0.84 (0.54)Self2.71 (1.23)1.01 (0.77)Happy3.50 (0.81)1.76 (0.91)
Note: Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.</em>p &lt; .00001.</p>
<p>Table 10 .
10
Attributes that differ significantly between non-social places and social places.Note: Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.<em>p &lt; .00001.
AttributeNon-social placesSocial placesColour</em>2.71 (1.09)0.99 (0.51)Pattern<em>2.92 (0.82)1.05 (0.57)Shape</em>3.89 (0.91)2.50 (0.78)Touch<em>1.95 (1.03)0.47 (0.37)Texture</em>2.76 (1.07)0.92 (0.41)Time0.36 (0.42)1.34 (0.92)Consequential<em>0.65 (0.45)1.89 (1.00)Social</em>0.84 (0.52)3.31 (0.96)Communication*0.26 (0.19)1.38 (0.79)Cognition0.20 (0.13)1.03 (0.84)Disgusted0.08 (0.06)0.49 (0.38)</p>
<p>Table 11 .
11
Attributes that differ significantly between festive and verbal social events.
AttributeFestive social eventsVerbal social eventsVision<em>4.56 (0.93)1.41 (1.13)Bright1.50 (0.98)0.10 (0.07)Large</em>3.18 (1.38)0.53 (0.72)Motion<em>3.60 (1.00)0.84 (0.85)Fast</em>1.51 (0.63)0.38 (0.28)Shape<em>1.40 (0.71)0.24 (0.21)Complexity</em>2.89 (1.17)0.48 (0.61)Loud<em>3.89 (0.92)1.49 (1.09)Sound3.89 (1.15)1.96 (1.03)Music3.21 (1.77)0.52 (0.78)Head1.85 (1.30)3.98 (1.09)Consequential</em>1.61 (0.85)3.83 (0.82)Communication<em>2.20 (1.14)5.33 (0.39)Cognition</em>1.15 (0.44)3.70 (0.77)Benefit<em>2.50 (0.53)3.75 (0.58)Pleasant</em>4.41 (0.85)1.78 (0.90)Unpleasant<em>0.38 (0.25)1.62 (0.59)Happy</em>4.26 (0.88)1.63 (0.92)Angry<em>0.34 (0.36)1.24 (0.61)
Note: Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.</em>p &lt; .00001.</p>
<p>Table 12 .
12
Attributes that differ significantly between beneficial and neutral abstract entities.Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.<em>p &lt; .00001.
AttributeBeneficial abstract entitiesNeutral abstract entitiesConsequential3.42 (0.83)2.22 (0.95)Self</em>3.61 (1.03)1.19 (1.02)Cognition4.03 (1.38)2.19 (1.36)Benefit<em>4.68 (0.70)2.31 (1.29)Pleasant</em>3.84 (0.93)1.45 (0.78)Happy<em>3.90 (1.05)1.32 (0.76)Drive</em>3.76 (0.82)1.70 (0.60)Needs<em>3.52 (1.16)1.30 (0.99)Arousal</em>3.17 (0.94)1.76 (0.83)Note:</p>
<p>Table 13 .
13
Attributes that differ significantly between causal entities and neutral abstract entities.
AttributeCausal entitiesNeutral abstract entitiesConsequential<em>4.47 (0.67)2.22 (0.95)Social</em>3.94 (1.21)1.80 (0.91)Drive<em>3.46 (0.83)1.70 (0.60)Arousal</em>2.95 (0.59)1.76 (0.83)
Note: Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.*p &lt; .00001.</p>
<p>Table 14 .
14
Attributes that differ significantly between negative and positive emotion entities.Note: Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.<em>p &lt; .00001.
AttributeNegative emotion entitiesPositive emotion entitiesVision0.75 (0.49)1.52 (0.81)Bright</em>0.06 (0.06)0.62 (0.50)Dark0.56 (0.41)0.14 (0.16)Pain<em>2.09 (1.05)0.19 (0.21)Music0.10 (0.14)0.57 (0.54)Consequential</em>4.42 (0.72)2.58 (0.80)Self<em>1.01 (0.56)2.52 (1.20)Benefit</em>0.75 (0.65)3.37 (0.93)Harm<em>3.81 (0.93)0.46 (0.55)Pleasant</em>0.28 (0.25)4.71 (0.84)Unpleasant<em>4.80 (0.72)0.29 (0.37)Happy</em>0.23 (0.35)4.80 (0.92)Sad<em>3.46 (1.12)0.41 (0.58)Angry</em>3.79 (1.06)0.29 (0.40)Disgusted<em>2.70 (0.84)0.24 (0.37)Fearful</em>2.59 (1.06)0.26 (0.27)Needs*0.37 (0.47)2.09 (0.96)</p>
<p>Table 15 .
15
Attributes that differ significantly between locative change and abstract/social actions.
AttributeLocative change actionsNeutral actionsMotion<em>3.81 (0.71)1.98 (0.81)Biomotion</em>4.64 (0.67)2.87 (0.78)Fast<em>3.23 (1.27)1.42 (0.76)Body4.47 (0.98)2.74 (1.06)Lower Limb</em>3.86 (2.08)1.11 (0.96)Path<em>5.10 (0.84)2.05 (1.43)Communication1.01 (0.58)2.88 (1.51)Cognition0.96 (0.31)2.53 (1.28)
Note: Columns give mean for each rating, with standard deviations in parentheses.The larger value is indicated in bold font.Differences significant at p &lt; .0001.</em>p &lt; .00001.</p>
<p>J. R. BINDER ET AL.
FundingThis work was supported by the Intelligence Advanced Research Projects Activity (IARPA) [grant number FA8650-14-C-7357].Disclosure statementNo potential conflict of interest was reported by the authors.
Columnar organization of directionally selective cells in visual area MT of the macaque. T D Albright, R Desimone, C G Gross, Journal of Neurophysiology. 511984</p>
<p>Social perception from visual cues: Role of the STS region. T Allison, A Puce, G Mccarthy, Trends in Cognitive Sciences. 42000</p>
<p>Distributed memory, modular subsystems and dysphasia. D A Allport, Current perspectives in dysphasia. S K Newman, R Epstein, Edinburgh1985Churchill Livingstone</p>
<p>The distinctiveness of emotion concepts: A comparison between emotion, abstract, and concrete words. J Altarriba, L M Bauer, The American Journal of Psychology. 1172004</p>
<p>The neural basis for spatial relations. P X Amorapanth, P Widick, A Chatterjee, Journal of Cognitive Neuroscience. 222010</p>
<p>The human amygdala is sensitive to the valence of pictures and sounds irrespective of arousal: An fMRI study. S Anders, F Eippert, N Weiskopf, R Veit, Social, Cognitve and Affective Neuroscience. 20083</p>
<p>Brain activity underlying emotional valence and arousal: A response-related fMRI study. S Anders, M Lotze, M Erb, W Grodd, N Birbaumer, Human Brain Mapping. 232004</p>
<p>Multimodal representation of space in the posterior parietal cortex and its use in planning movements. R A Andersen, L H Snyder, D C Bradley, J Xing, Annual Review of Neuroscience. 201997</p>
<p>Cortical midline structures and autobiographical-self processes: An activation-likelihood estimation meta-analysis. H F Araujo, J Kaplan, A Damasio, Frontiers in Human Neuroscience. 75482013</p>
<p>Categories (J. L. Ackrill, Trans.). Aristotle , The complete works of Aristotle. J Barnes, PrincetonPrinceton University Press1995/350 BCE)</p>
<p>R H Baayen, R Piepenbrock, L Gulikers, Philadelphia: Linguistic Data Consortium. 1995University of PennsylvaniaThe CELEX lexical database (CD-ROM</p>
<p>Functional magnetic resonance imaging evidence for a hierarchical organization in the prefrontal cortex. D Badre, M &amp; D'esposito, Journal of Cognitive Neuroscience. 192007</p>
<p>Single units and sensation: A neuron doctrine for perceptual psychology. H B Barlow, Perception. 11972</p>
<p>Are emotions natural kinds?. L F Barrett, Perspectives on Psychological Science. 12006</p>
<p>Context-independent and contextdependent information in concepts. L W Barsalou, Memory and Cognition. 101982</p>
<p>Ad hoc categories. L W Barsalou, Memory &amp; Cognition. 111983</p>
<p>Deriving categories to achieve goals. L W Barsalou, The psychology of learning and motivation: Advances in research and theory. G H Bower, San Diego, CAAcademic Press199127</p>
<p>Perceptual symbol systems. L W Barsalou, Behavioral and Brain Sciences. 221999</p>
<p>Grounding cognition: The role of perception and action in memory, language, and thought. L W Barsalou, K Wiemer-Hastings, D. Pecher &amp; R. Zwaan2005Cambridge University PressNew YorkSituating abstract concepts</p>
<p>The architecture of the colour centre in the human visual brain: New results and a review. A Bartels, S M Zeki, European Journal of Neuroscience. 122000</p>
<p>Decoding the neural representation of affective states. L B Baucom, D H Wedell, J Wang, D N Blitzer, S V Shinkareva, Neuroimage. 592012</p>
<p>An fMRI version of the Farnsworth-Munsell 100-hue test reveals multiple color-sensitive areas in human ventral occipitotemporal cortex. M S Beauchamp, J V Haxby, J E Jennings, E A Deyoe, Cerebral Cortex. 91999</p>
<p>Voice-selective areas in human auditory cortex. P Belin, R J Zatorre, P Lafaille, P Ahad, B Pike, Nature. 4032000</p>
<p>Basic color terms: Their universality and evolution. B Berlin, P Kay, 1969University of California PressBerkeley</p>
<p>Effects of word imageability on semantic access: Neuroimaging studies. J R Binder, Neural basis of semantic memory. J Hart, M A Kraut, Cambridge, UKCambridge University Press2007</p>
<p>The neurobiology of semantic memory. J R Binder, R H Desai, Trends in Cognitive Sciences. 152011</p>
<p>Where is the semantic system? A critical review and metaanalysis of 120 functional neuroimaging studies. J R Binder, R Desai, L L Conant, W W Graves, Cerebral Cortex. 192009</p>
<p>Age of acquisition and imageability ratings for a large set of words, including verbs and function words. H Bird, S Franklin, D Howard, Behavior Research Methods, Instruments, &amp; Computers. 332001</p>
<p>Neural correlates of causality judgment in physical and social context: The reversed effects of space and time. J Blos, A Chatterjee, T Kircher, B Straube, Neuroimage. 632012</p>
<p>Manipulating objects and telling words: a study on concrete and abstract words acquisition. A M Borghi, A Flumini, F Cimatti, D Marocco, C Scorolli, Frontiers in Psychology. 2152011</p>
<p>Distinctions between manipulation and function knowledge of objects: Evidence from functional magnetic resonance imaging. C B Boronat, L J Buxbaum, H B Coslett, K Tang, E M Saffran, D Y Kimberg, J A Detre, Cognitive Brain Research. 232005</p>
<p>On the biological plausibility of grandmother cells: Implications for neural network theories in psychology and neuroscience. J S Bowers, Psychological Review. 1162009</p>
<p>Talking about internal states: The acquisition of an explicit theory of mind. I Bretherton, M Beeghly, Developmental Psychology. 181982</p>
<p>Spatial cognition and the brain. N Burgess, Annals of the New York Academy of Sciences. 11242008</p>
<p>Action observation and acquired motor skills: An fMRI study with expert dancers. B Calvo-Merino, D E Glaser, J Grezes, R E Passingham, P Haggard, Cerebral Cortex. 152005</p>
<p>What are the facts of semantic category-specific deficits? A critical review of the clinical evidence. E Capitani, M Laiacona, B Mahon, A Caramazza, Cognitive Neuropsychology. 202003</p>
<p>Body partspecific representations of semantic noun categories. F Carota, R Moseley, F Pulvermüller, Journal of Cognitive Neuroscience. 242012</p>
<p>A nexus model of the temporalparietal junction. R M Carter, S A Huettel, Trends in Cognitive Sciences. 172013</p>
<p>Embodied comprehension of stories : Interactions between language regions and modality-specific neural systems. H M Chow, R A Mar, Y Xu, S Liu, S Wagage, A R Braun, Journal of Cognitive Neuroscience. 262013</p>
<p>Types of linguistic knowledge: Interpreting and producing compound nouns. E Clark, R Berman, Journal of Child Language. 141987</p>
<p>Extensions of the Paivio, Yuille, and Madigan (1968) norms. J M Clark, A Paivio, Behavior Research Methods, Instruments, &amp; Computers. 362004</p>
<p>Neural systems subserving valence and arousal during the experience of induced emotions. T Colibazzi, J Posner, Z Wang, D Gorman, A Gerber, S Yu, B S Peterson, Emotion. 102010</p>
<p>Conceptual combination and the given-new distinction. F Conrad, L Rips, Journal of Memory and Language. 251986</p>
<p>Color-tuned neurons are spatially clustered according to color preference within alert macaque posterior inferior temporal cortex. B R Conway, D Y Tsao, 2009106Proceedings of the National Academy of Sciences of the United States of America</p>
<p>M J Cortese, A Fugett, Imageability ratings for 3,000 monosyllabic words. Behavior Research Methods, Instruments, and Computers. 200436</p>
<p>Knowledge of the body: A distinct semantic domain. H B Coslett, E M Saffran, J Schwoebel, Neurology. 592002</p>
<p>Analyzing the factors underlying the structure and computation of the meaning of chipmunk, cherry, chisel, cheese, and cello (and many other such concrete nouns). G S Cree, K Mcrae, Journal of Experimental Psychology: General. 1322003</p>
<p>Abstract conceptual feature ratings: the role of emotion, magnitude, and other cognitive domains in the organization of abstract conceptual knowledge. S J Crutch, J Troche, J Reilly, G R Ridgway, Frontiers in Human Neuroscience. 71862013</p>
<p>Abstract and concrete concepts have structurally different representational frameworks. S J Crutch, E K Warrington, Brain. 1282005</p>
<p>The role of polarity in antonym and synonym conceptual knowledge: Evidence from stroke aphasia and multidimensional ratings of abstract words. S J Crutch, P Williams, G R Ridgway, L Borgenicht, Neuropsychologia. 502012</p>
<p>Cortical fMRI activation produced by attentive tracking of moving targets. J C Culham, S A Brandt, P Cavanagh, N G Kanwisher, A M Dale, R B Tootell, Journal of Neurophysiology. 801998</p>
<p>Implicit and explicit evaluation: fMRI correlates of valence, emotional intensity, and control in the processing of attitudes. W A Cunningham, C L Raye, M K Johnson, Journal of Cognitive Neuroscience. 162004</p>
<p>The number sense: How the mind creates mathematics. S Dehaene, 1997Oxford University PressNew York</p>
<p>A meta-analysis of functional neuroimaging studies of selfand other judgments reveals a spatial gradient for mentalizing in medial prefrontal cortex. B T Denny, H Kober, T D Wager, K N Ochsner, Journal of Cognitive Neuroscience. 242012</p>
<p>Involvement of the inferior frontal junction in cognitive control: Meta-analyses of switching and Stroop studies. J Derrfuss, M Brass, J Neumann, D Y Von Cramon, Human Brain Mapping. 252005</p>
<p>Activation of sensory-motor and visual areas by sentence comprehension. R Desai, J R Binder, L L Conant, M S Seidenberg, Cerebral Cortex. 202009</p>
<p>The role of the human ventral striatum and the medial orbitofrontal cortex in the representation of reward magnitude-An activation likelihood estimation meta-analysis of neuroimaging studies of passive reward expectancy and outcome processing. E K Diekhof, L Kaps, P Falkai, O Gruber, Neuropsychologia. 502012</p>
<p>On the creation and use of English compound nouns. Language. P Downing, 197753</p>
<p>A cortical area selective for visual processing of the human body. P E Downing, Y Jiang, M Shuman, N Kanwisher, Science. 2932001</p>
<p>Common regions of the human frontal lobe recruited by diverse cognitive demands. J Duncan, A M Owen, Trends in Neurosciences. 232000</p>
<p>The neural bases of social pain: Evidence for shared representations with physical pain. N I Eisenberger, Psychosomatic Medicine. 742012</p>
<p>An argument for basic emotions. P Ekman, Cognition and Emotion. 61992</p>
<p>A cortical representation of the local visual environment. R Epstein, N Kanwisher, Nature. 3921998</p>
<p>Where am I now? Distinct roles for parahippocampal and retrosplenial cortices in place recognition. R A Epstein, W E Parker, A M Feiler, Journal of Neuroscience. 272007</p>
<p>Emotional processing in anterior cingulate and medial prefrontal cortex. A Etkin, T Egner, R Kalisch, Trends in Cognitive Sciences. 152011</p>
<p>V Evans, The structure of time: Language, meaning and temporal cognition. AmsterdamJohn Benjamins2004</p>
<p>A computational model of semantic memory impairment: Modality specificity and emergent category specificity. M J Farah, J L Mcclelland, Journal of Experimental Psychology: General. 1201991</p>
<p>Phonological typicality influences on-line sentence comprehension. T A Farmer, M H Christiansen, P Monaghan, 2006103Proceedings of the National Academy of Sciences USA</p>
<p>Concept representation reflects multimodal abstraction: A framework for embodied semantics. L Fernandino, J R Binder, R H Desai, S L Pendl, C J Humphries, W Gross, M S Seidenberg, 10.1093/cercor/bhv020Cerebral Cortex. 2015. March 5, 2015</p>
<p>Time, space and emotion: fMRI reveals content-specific activation during text comprehension. E C Ferstl, D Y Von Cramon, Neuroscience Letters. 4272007</p>
<p>Emotional and temporal aspects of situation model processing during text comprehension: An event-related fMRI study. E C Ferstl, M Rinck, D Y Von Cramon, Journal of Cognitive Neuroscience. 172005</p>
<p>Perception and judgement of physical causality involve different brain structures. P Fonlupt, Cognitive Brain Research. 172003</p>
<p>Category-specific recognition impairments: a review of important case studies and influential theories. E M E Forde, G W Humphreys, Aphasiology. 131999</p>
<p>Coordinate-based meta-analysis of experimentally induced and chronic persistent neuropathic pain. U Friebel, S B Eickhoff, M Lotze, Neuroimage. 582011</p>
<p>Brain mechanisms underlying perceptual causality. J A Fugelsang, M E Roser, P M Corballis, M S Gazzaniga, K N Dunbar, Cognitive Brain Research. 242005</p>
<p>Relation and lexical priming during the interpretation of noun-noun combinations. C L Gagné, Journal of Experimental Psychology: Learning, Memory, and Cognition. 272001</p>
<p>Influence of discourse context on feature availability in conceptual combination. C L Gagné, G L Murphy, Discourse Processes. 199622</p>
<p>Influence of thematic relations on the comprehension of modifier-noun combinations. C L Gagné, E J Shoben, Journal of Experimental Psychology: Learning, Memory, and Cognition. 231997</p>
<p>What the locus of brain lesion tells us about the nature of the cognitive defect underlying categoryspecific disorders: A review. G Gainotti, Cortex. 362000</p>
<p>Mental representation of normal subjects about the sources of knowledge in different semantic categories and unique entities. G Gainotti, F Ciaraffa, M C Silveri, C Marra, Neuropsychology. 232009</p>
<p>The evaluation of sources of knowledge underlying different conceptual categories. G Gainotti, P Spinelli, E Scaricamazza, C Marra, Frontiers in Human Neuroscience. 7402013</p>
<p>Prototypicality, distinctiveness and intercorrelation: Analyses of the semantic attributes of living and nonliving concepts. P Garrard, M A Lambon Ralph, J R Hodges, K Patterson, Journal of Cognitive Neuroscience. 182001</p>
<p>An affective circumplex model of neural systems subserving valence, arousal, and cognitive overlay during the appraisal of emotional faces. A J Gerber, J Posner, D Gorman, T Colibazzi, S Yu, Z Wang, B S Peterson, Neuropsychologia. 462008</p>
<p>Evaluating semantic models with word-sentence relatedness. K Glasgow, M Roos, A Haufler, M Chevillet, M Wolmetz, arXiv:1603.072532016</p>
<p>Perceptual knowledge retrieval activates sensory brain regions. R F Goldberg, C A Perfetti, W Schneider, Journal of Neuroscience. 262006</p>
<p>Reading cinnamon activates olfactory brain regions. J González, A Barros-Loscertales, F Pulvermüller, V Meseguer, A Sanjuán, V Belloch, C Ávila, Neuroimage. 322006</p>
<p>Coding of visual space by premotor neurons. M S Graziano, G S Yap, C G Gross, Science. 2661994</p>
<p>The human visual cortex. K Grill-Spector, R Malach, Annual Review of Neuroscience. 272004</p>
<p>Timing and time perception: A review of recent behavioral and neuroscience findings and theoretical directions. Attention, Perception, and Psychophysics. S Grondin, 201072</p>
<p>Brain areas active during visual perception of biological motion. E D Grossman, R Blake, Neuron. 352002</p>
<p>Mapping discrete and dimensional emotions onto the brain: controversies and consensus. S Hamann, Trends in Cognitive Sciences. 162012</p>
<p>The symbol grounding problem. S Harnad, Physica D: Nonlinear Phenomena. 421990</p>
<p>Topographic representation of numerosity in the human parietal cortex. B M Harvey, B P Klein, N Petridou, S O Dumoulin, Science. 61502013</p>
<p>Eccentricity bias as an organizing principle for human high-order object areas. U Hasson, I Levy, M Behrmann, T Hendler, R Malach, Neuron. 342002</p>
<p>Somatotopic representation of action words in human motor and premotor cortex. O Hauk, I Johnsrude, F Pulvermuller, Neuron. 412004</p>
<p>Somatic sensation. S H C Hendry, S S Hsiao, M C Bushnell, Fundamental neuroscience. M J Zigmond, F E Bloom, S C Landis, J L Roberts, L R Squire, San DiegoAcademic Press1999</p>
<p>Shapes, scents and sounds: Quantifying the full multi-sensory basis of conceptual knowledge. P Hoffman, M A &amp; Lambon Ralph, Neuropsychologia. 512013</p>
<p>A rationale and test for the number of factors in factor analysis. J Horn, Psychometrika. 301965</p>
<p>Chromaticity of color perception and object color knowledge. N S Hsu, S M Frankland, S L Thompson-Schill, Neuropsychologia. 502012</p>
<p>Color, context, and cognitive style: Variations in color knowledge retrieval as a function of task and subject variables. N S Hsu, D Kraemer, R Oliver, M L Schlichting, S L Thompson-Schill, Journal of Cognitive Neuroscience. 232011</p>
<p>Semantic structures. R Jackendoff, 1990MIT PressCambridge, MA</p>
<p>The architecture of the golfer's brain. L Jäncke, S Koeneke, A Hoppe, C Rominger, J Hänggi, PLoS ONE. 4e47852009</p>
<p>The fusiform face area: A module in human extrastriate cortex specialized for face perception. N Kanwisher, J Mcdermott, M M Chun, Journal of Neuroscience. 171997</p>
<p>Identifying emotions on the basis of neural activation. K S Kassam, A R Markey, V L Cherkassky, G Loewenstein, M A Just, PLoS One. 82013</p>
<p>The emergence of theoretical beliefs as constraints on concepts. F Keil, The epigenesis of mind: Essays on biology and cognition. S C Gelman, R Gelman, Hillsdale, NJErlbaum1991</p>
<p>Large, colourful or noisy? Attribute-and modality-specific activations during retrieval of perceptual attribute knowledge. M L Kellenbach, M Brett, K Patterson, Cognitive, Affective, and Behavioral Neuroscience. 12001</p>
<p>Using sound to solve syntactic problems: The role of phonology in grammatical category assignments. M H Kelly, Psychological Review. 991992</p>
<p>The semantics of space: Integrating linguistic typology and cognitive neuroscience. D Kemmerer, 2006</p>
<p>. Neuropsychologia. 44</p>
<p>Searching for the elusive neural substrates of body part terms: A neuropsychological study. D Kemmerer, D Tranel, Cognitive Neuropsychology. 252008</p>
<p>Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions. M Kiefer, F Pulvermüller, Cortex. 482012</p>
<p>The sound of concepts: Four markers for a link between auditory and conceptual brain systems. M Kiefer, E.-J Sim, B Herrnberger, J Grothe, K Hoenig, Journal of Neuroscience. 282008</p>
<p>Functional grouping and cortical-subcortical interactions in emotion: A meta-analysis of neuroimaging studies. H Kober, L F Barrett, J Joseph, E Bliss-Moreau, K Lindquist, T D Wager, Neuroimage. 422008</p>
<p>A real-world size organization of object responses in occipitotemporal cortex. T Konkle, A Oliva, Neuron. 742012</p>
<p>The representation of abstract words: Why emotion matters. S.-T Kousta, G Vigliocco, D P Vinson, M Andrews, E Del Campo, Journal of Experimental Psychology: General. 1402011</p>
<p>Advancing emotion theory with multivariate pattern classification. P A Kragel, K S Labar, Emotion Review. 62014</p>
<p>Deconstructing events: The neural bases for space, time, and causality. A Kranjec, E R Cardillo, G L Schmidt, M Lehet, A Chatterjee, Journal of Cognitive Neuroscience. 242012</p>
<p>Are temporal concepts embodied? A challenge for cognitive neuroscience. A Kranjec, A Chatterjee, Frontiers in Psychology. 12010</p>
<p>Incidental effects of emotional valence in single word processing: An fMRI study. L Kuchinke, A M Jacobs, C Grubich, M L H Vo, M Conrad, M Herrmann, Neuroimage. 282005</p>
<p>Encoding of personal information: self-other differences. N A Kuiper, T B Rogers, Journal of Personality and Social Psychology. 371979</p>
<p>A case of impaired naming and knowledge of body parts: Are limbs a separate category?. M Laiacona, N Allamano, L Lorenzi, E Capitani, Neurocase. 122006</p>
<p>G Lakoff, M Johnson, Metaphors we live by. ChicagoUniversity of Chicago Press1980</p>
<p>Meta-analytic evidence for common and distinct neural networks associated with directly experienced pain and empathy for pain. C Lamm, J Decety, T Singer, Neuroimage. 542011</p>
<p>What and where in spatial language and spatial cognition. B Landau, R Jackendoff, Behavioral and Brain Sciences. 161993</p>
<p>A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. T K Landauer, S T Dumais, Psychological Review. 1041997</p>
<p>&amp; the Science and Applications of Latent Semantic Analysis. T K Landauer, SALSA) GroupW Kintsch, SALSA) GroupLSA applications. Boulder, CO2015University of Colorado</p>
<p>Cortical representation of natural complex sounds: Effects of acoustic features and auditory object category. A M Leaver, J P Rauschecker, Journal of Neuroscience. 302010</p>
<p>Discrete emotions predict changes in cognition, judgment, experience, behavior, and physiology: A meta-analysis of experimental emotion elicitations. H C Lench, S A Flores, S W Bench, Psychological Bulletin. 1372011</p>
<p>The syntax and semantics of complex nominals. J Levi, 1978Academic PressNew York</p>
<p>The root of all value: A neural common currency for choice. D J Levy, P W Glimcher, Current Opinion in Neurobiology. 222012</p>
<p>Human brain regions involved in recognizing environmental sounds. J W Lewis, F L Wightman, J A Brefczynski, R E Phinney, J R Binder, E A Deyoe, Cerebral Cortex. 142004</p>
<p>Neural correlates of processing valence and arousal in affective words. P A Lewis, H D Critchley, P Rotshtein, R J Dolan, Cerebral Cortex. 172007</p>
<p>Neural substrates of phonemic perception. E Liebenthal, J R Binder, S M Spitzer, E T Possing, D A Medler, Cerebral Cortex. 152005</p>
<p>The hundred-year emotion war: are emotions natural kinds or psychological constructions? Comment on Lench. K A Lindquist, E H Siegel, K S Quigley, L F Barrett, Psychological Bulletin. 1392013. 2011Flores, and Bench</p>
<p>The brain basis of emotion: A meta-analytic review. K A Lindquist, T D Wager, H Kober, E Bliss-Moreau, L F Barrett, Behavioral and Brain Sciences. 352012</p>
<p>Speed encoding in human visual cortex revealed by fMRI adaptation. A Lingnau, H Ashida, M B Wall, A T Smith, Journal of Vision. 932009</p>
<p>More than skin deep: Body representation beyond primary somatosensory cortex. M Longo, E Azanon, P Haggard, Neuropsychologia. 482010</p>
<p>Modality exclusivity norms for 423 object properties. D Lynott, L Connell, Behavior Research Methods. 412009</p>
<p>Modality exclusivity norms for 400 nouns: The relationship between perceptual experience and surface word form. D Lynott, L Connell, Behavior Research Methods. 452013</p>
<p>Knowing where things are: Parahippocampal involvement in encoding object locations in virtual large-scale space. E A Maguire, C D Frith, N Burgess, J G Donnett, J O'keefe, Journal of Cognitive Neuroscience. 101998</p>
<p>Is that near my hand? Multisensory representation of peripersonal space in human intraparietal sulcus. T R Makin, N P Holmes, E Zohary, Journal of Neuroscience. 272007</p>
<p>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex. R Malach, J B Reppas, R R Benson, K K Kwong, H Jiang, W A Kennedy, R B Tootell, 199592Proceedings of the National Academy of Sciences USA</p>
<p>Neuropsychological and neuroimaging perspectives on conceptual knowledge: An introduction. A Martin, A Caramazza, Cognitive Neuropsychology. 202003</p>
<p>Discrete cortical regions associated with knowledge of color and knowledge of action. A Martin, J V Haxby, F M Lalonde, C L Wiggs, L G Ungerleider, Science. 2701995</p>
<p>A theory of human motivation. A H Mazlow, Psychological Review. 501943</p>
<p>Spatial segregation of somato-sensory and pain activations in the human operculo-insular cortex. L Mazzola, I Faillenot, F G Barral, F Mauguiere, R Peyron, Neuroimage. 602012</p>
<p>The cutaneous sensory system. F Mcglone, D Reilly, Neuroscience and Biobehavioral Reviews. 342010</p>
<p>Semantic feature norms for a large set of living and nonliving things. K Mcrae, G S Cree, M S Seidenberg, C Mcnorgan, Behavior Research Methods, Instruments, &amp; Computers. 372005</p>
<p>On the nature and scope of featural representations of word meaning. K Mcrae, V R De Sa, M S Seidenberg, Journal of Experimental Psychology: General. 1261997</p>
<p>Context and structure in conceptual combination. D L Medin, E J Shoben, Cognitive Psychology. 201988</p>
<p>From maps to form to space: Touch and the body schema. J Medina, H B Coslett, Neuropsychologia. 482010</p>
<p>Coming of age: A review of embodiment and the neuroscience of semantics. L Meteyard, S Rodriguez Cuadrado, B Bahrami, G Vigliocco, Cortex. 482012</p>
<p>Comparing prototype-based and exemplar-based accounts of category learning and attentional allocation. J P Minda, J D Smith, Journal of Experimental Psychology: Learning, Memory, and Cognition. 282002</p>
<p>Neuronal substrates of haptic shape encoding and matching: A functional magnetic resonance imaging study. A Miquée, C Xerri, C Rainville, J L Anton, B Nazarian, M Roth, Y Zennou-Azogui, Neuroscience. 1522008</p>
<p>Noun phrase interpretation and conceptual combination. G Murphy, Journal of Memory and Language. 291990</p>
<p>The role of theories in conceptual coherence. G Murphy, D Medin, Psychological Review. 921985</p>
<p>The impact of affect and frequency on lexical decision: The role of the amygdala and inferior frontal cortex. M Nakic, B W Smith, S Busis, M Vythilingam, R J R Blair, Neuroimage. 312006</p>
<p>Distinct brain systems underlie the processing of valence and arousal of affective pictures. M M A Nielen, D J Heslenfeld, K Heinen, J W Van Strien, M P Witter, C Jonker, D J Veltman, Brain and Cognition. 712009</p>
<p>Neural correlates of locative prepositions. M L Noordzij, S F W Neggers, N F Ramsey, A Postma, Neuropsychologia. 462008</p>
<p>Self-referential processing in our brain-a meta-analysis of imaging studies on the self. G Northoff, A Heinzel, M De Greck, F Bermpohl, H Dobrowolny, J Panksepp, Neuroimage. 312006</p>
<p>Attention, similiarity and the identification-categorization relationship. R M Nosofsky, Journal of Experimental Psychology: Learning, Memory and Cognition. 1151986</p>
<p>Common prefrontal activations during working memory, episodic memory, and semantic memory. L Nyberg, P Marklund, J Persson, R Cabeza, C Forkstam, K Petersson, M Ingvar, Neuropsychologia. 412003</p>
<p>SPSS and SAS programs for determining the number of components using parallel analysis and Velicer's MAP test. B P O'connor, Behavior Research Methods, Instruments, &amp; Computers. 322000</p>
<p>Social cognition and the anterior temporal lobes: A review and theoretical framework. I R Olson, D Mccoy, E Klobusicky, L A Ross, Social Cognitive &amp; Affective Neuroscience. 82013</p>
<p>Supramodal representations of perceived emotions in the human brain. M V Peelen, A P Atkinson, P Vuilleumier, Journal of Neuroscience. 302010</p>
<p>Functional anatomy of biological motion perception in posterior temporal cortex: An fMRI study of eye, mouth and hand movements. K A Pelphrey, J P Morris, C R Michelich, T Allison, G Mccarthy, Cerebral Cortex. 152005</p>
<p>Neural representations of subjective reward value. J Peters, C Büchel, Behavioural Brain Research. 2132010</p>
<p>Functional neuroanatomy of emotion: A meta-analysis of emotion activation studies in PET and fMRI. K L Phan, T Wager, S F Taylor, I Liberzon, Neuroimage. 162002</p>
<p>Tuning curves for approximate numerosity in the human intraparietal sulcus. M Piazza, V Izard, P Pinel, D L Bihan, S Dehaene, Neuron. 442004</p>
<p>The neurophysiological bases of emotion: An fMRI study of the affective circumplex using emotion-denoting words. J Posner, J A Russell, A Gerber, D Gorman, T Colibazzi, S Yu, B S Peterson, Human Brain Mapping. 302009</p>
<p>The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology. J Posner, J A Russell, B S Peterson, Development and Psychopathology. 172005</p>
<p>Action word meaning representations in cytoarchitectonically defined primary and premotor cortices. N Postle, K L Mcmahon, R Ashton, M Meredith, G I De Zubicaray, Neuroimage. 432008</p>
<p>A direct quantitative relationship between the functional properties of human and macaque V5. G Rees, K J Friston, C Koch, Nature Neuroscience. 32000</p>
<p>Semantic composition in sentence verification. L Rips, E Smith, E Shoben, Journal of Verbal Learning and Verbal Behavior. 171978</p>
<p>Self-reference and the encoding of personal information. T B Rogers, N A Kuiper, W S Kirker, Journal of Personality and Social Psychology. 351977</p>
<p>Neural coding of sound intensity and loudness in the human auditory system. M Röhl, S Uppenkamp, Journal of the Association for Research in Otolaryngology. 132012</p>
<p>The orbitofrontal cortex and beyond: From affect to decision-making. E T Rolls, F Grabenhorst, Progress in Neurobiology. 862008</p>
<p>Basic objects in natural categories. E Rosch, C B Mervis, W D Gray, D M Johnson, D Boyes-Braem, Cognitive Psychology. 81976</p>
<p>Social cognition and the anterior temporal lobes. L A Ross, I R Olson, Neuroimage. 492010</p>
<p>Body schematics: On the role of the body schema in embodied lexical-semantic representations. S.-A Rueschemeyer, C Pfeiffer, H Bekkering, Neuropsychologia. 482010</p>
<p>A circumplex model of affect. J Russell, Journal of Personality and Social Psychology. 391980</p>
<p>Distributed representations of dynamic facial expressions in the superior temporal sulcus. C P Said, C D Moore, A D Engell, J V Haxby, Journal of Vision. 102010</p>
<p>An fMRI study of causal judgments. A B Satpute, D B Fenker, M R Waldmann, G Tabibnia, K J Holyoak, M D Lieberman, European Journal of Neuroscience. 222005</p>
<p>Making sense of another mind: The role of the right temporo-parietal junction. R Saxe, A Wexler, Neuropsychologia. 432005</p>
<p>Introspective minds: Using ALE meta-analyses to study commonalities in the neural correlates of emotional processing, social &amp; unconstrained cognition. L Schilbach, D Bzdok, B Timmermans, P T Fox, A R Laird, K Vogeley, S B Eickhoff, PLoS One. 72012</p>
<p>Phonological iconicity. D S Schmidtke, M Conrad, A M Jacobs, Frontiers in Psychology. 5802014</p>
<p>Auditory cortex mapmaking: Principles, projections, and plasticity. C E Schreiner, J A Winer, Neuron. 562007</p>
<p>Fractionating theory of mind: A meta-analysis of functional brain imaging studies. M Schurz, J Radua, M Aichhorn, F Richlan, J Perner, Neuroscience and Biobehavioral Reviews. 422014</p>
<p>Why are abstract concepts hard to understand?. P Schwanenflugel, The psychology of word meanings. P Schwanenflugel, Hillsdale, NJErlbaum1991</p>
<p>Separate face and body selectivity on the fusiform gyrus. R F Schwarzlose, C I Baker, N Kanwisher, Journal of Neuroscience. 252005</p>
<p>Evidence for multiple, distinct representations of the human body. J Schwoebel, H B Coslett, Journal of Cognitive Neuroscience. 172005</p>
<p>Touch and the body. A Serino, P Haggard, Neuroscience and Biobehavioral Reviews. 342010</p>
<p>C Shaoul, C Westbury, A reduced redundancy USENET corpus. Edmonton, AB2013. 2005-2011University of Alberta</p>
<p>Predicating and nonpredicating combinations. E Shoben, The psychology of word meanings. P J Schwanenflugel, Hillsdale, NJErlbaum1991</p>
<p>A common neural substrate for perceiving and knowing about color. W K Simmons, V Ramjee, M S Beauchamp, K Mcrae, A Martin, L W Barsalou, Neuropsychologia. 452007</p>
<p>The selectivity and functional connectivity of the anterior temporal lobes. W K Simmons, M Reddish, P S Bellgowan, A Martin, Cerebral Cortex. 202010</p>
<p>Conceptual combination with prototype concepts. E E Smith, D N Osherson, Cognitive Science. 81984</p>
<p>A standardized set of 260 pictures: Norms for name agreement, image agreement, familiarity, and visual complexity. J G Snodgrass, M Vanderwart, Journal of Experimental Psychology: Human Learning and Memory. 61980</p>
<p>The common neural basis of autobiographical memory, prospection, navigation, theory of mind, and the default mode: A quantitative meta-analysis. R N Spreng, R A Mar, A S N Kim, Journal of Cognitive Neuroscience. 212009</p>
<p>Feature availability in conceptual combination. K Springer, G Murphy, Psychological Science. 31992</p>
<p>How language structures space. L Talmy, Spatial orientation: Theory, research, and application. H Pick, L Acredolo, New YorkPlenum Press1983</p>
<p>Coding visual images of objects in the inferotemporal cortex of the macaque monkey. K Tanaka, H Saito, Y Fukada, M Moriya, Journal of Neurophysiology. 661991</p>
<p>Listening to action-related sentences activates fronto-parietal motor cicuits. M Tettamanti, G Buccino, M C Saccuman, V Gallese, M Danna, P Scifo, D Perani, Journal of Cognitive Neuroscience. 172005</p>
<p>Functional analysis of human MT and related visual cortical areas using magnetic resonance imaging. R B Tootell, J B Reppas, K K Kwong, R Malach, R T Born, T J Brady, J W Belliveau, Journal of Neuroscience. 151995</p>
<p>Four models of basic emotions: a review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt. J L Tracy, D Randles, Emotion Review. 32011</p>
<p>Neuroanatomical correlates of locative prepositions. D Tranel, D Kemmerer, Cognitive Neuropsychology. 212004</p>
<p>Explaining category-related effects in the retrieval of conceptual and lexical knowledge for concrete entities: Operationalization and analysis of factors. D Tranel, C G Logan, R J Frank, A R Damasio, Neuropsychologia. 351997</p>
<p>Clustering, hierarchical organization, and the topography of abstract and concrete nouns. J Troche, S Crutch, J Reilly, Frontiers in Psychology. 53602014</p>
<p>Losing the sound of concepts: Damage to auditory association cortex impairs the processing of sound-related concepts. N M Trumpp, D Kliese, K Hoenig, T Haarmeier, M Kiefer, Cortex. 492013</p>
<p>Social cognition and the brain: A metaanalysis. F Van Overwalle, Human Brain Mapping. 302009</p>
<p>Differentiating between self and others: An ALE meta-analysis of fMRI studies of selfrecognition and theory of mind. S J Van Veluw, S A Chance, Brain Imaging and Behavior. 82014</p>
<p>Toward a theory of semantic representation. G Vigliocco, L Meteyard, M Andrews, S Kousta, Language and Cognition. 12009</p>
<p>Representing the meanings of object and action words: The featural and unitary semantic space hypothesis. G Vigliocco, D P Vinson, W Lewis, M F Garrett, Cognitive Psychology. 482004</p>
<p>The breakdown of semantic knowledge: Insights from a statistical model of meaning representation. D P Vinson, G Vigliocco, S Cappa, S Siri, Brain and Language. 862003</p>
<p>Neuroimaging support for discrete neural correlates of basic emotions: A voxel-based meta-analysis. K Vytal, S Hamann, M Wallentin, S Østergaarda, T E Lund, L Østergaard, A Roepstorff, Journal of Cognitive Neuroscience. 222010. 2005Brain and Language</p>
<p>Categories of knowledge. Further fractionations and an attempted integration. E K Warrington, R A Mccarthy, Brain. 1101987</p>
<p>Category specific semantic impairments. E K Warrington, T Shallice, Brain. 1071984</p>
<p>Content differences for abstract and concrete concepts. K Wiemer-Hastings, X Xu, Cognitive Science. 292005</p>
<p>The MRC Psycholinguistic Database: Machine Readable Dictionary, Version 2. M D Wilson, Behavior Research Methods, Instruments, &amp; Computers. 201988</p>
<p>Neural evidence that human emotions share core affective properties. C D Wilson-Mendenhall, L F Barrett, L W Barsalou, Psychological Science. 242013</p>
<p>A J Woods, R H Hamilton, A Kranjec, P Minhaus, M Bikson, J Yu, A Chatterjee, Space, time, and causality in the human brain. 201492</p>
<p>Neural substrates of processing path and manner information of a moving event. D H Wu, A Morganti, A Chatterjee, Neuropsychologia. 462008</p>
<p>The functional neuroanatomy of thematic role and locative relational knowledge. D H Wu, S Waller, A Chatterjee, Journal of Cognitive Neuroscience. 192007</p>
<p>Agency and impersonality: Their linguistic and cultural manifestations. M Yamamoto, 2006J. Benjamins Pub. CoAmsterdam</p>
<p>Social concepts are represented in the superior anterior temporal cortex. R Zahn, J Moll, F Krueger, E D Huey, G Garrido, J Grafman, 2007104Proceedings of the National Academy of Sciences USA</p>
<p>Structure and function of auditory cortex: Music and speech. R J Zatorre, P Belin, V B Penhune, Trends in Cognitive Sciences. 62002</p>
<p>Grasping the invisible: semantic processing of abstract words. L Zdrazilova, P M Pexman, Psychonomic Bulletin and Review. 202013</p>
<p>Functional organization of a visual area in the posterior bank of the superior temporal sulcus of the rhesus monkey. S M Zeki, The Journal of Physiology. 2361974</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6937 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6937</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6937</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-258865735</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14483v1.pdf" target="_blank">Language Model Self-improvement by Reinforcement Learning Contemplation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6937.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6937.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIRLC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Improvement by Reinforcement Learning Contemplation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised fine-tuning method that treats an LLM as both student (generator) and teacher (evaluator): the model generates answers, self-evaluates them (CEP or QEP), and is updated with reinforcement learning (PPO) to maximize the self-evaluation reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (Large default; also Small and Base evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned encoder-decoder Transformer (FLAN-T5) used autoregressively for generation; training via PPO with entropy regularization and KL penalty to keep close to pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>780M (FLAN-T5-Large); also 80M (Small) and 250M (Base) evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-evaluation-as-reward (SIRLC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM generates answers to unlabeled prompts, then the (fixed) initial LLM evaluates each generated answer using either a binary Correctness Evaluation Prompt (CEP) or a numeric Quality Evaluation Prompt (QEP, 1–10). The evaluation output is converted to a scalar reward and used to update the generator via PPO (RL). The evaluator is kept fixed (initial model M*) to stabilize training.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with RL update loop (sampling during training; reward used for PPO updates)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple — BigBench-hard (12 tasks reported), IWSLT2017 translation (ZH→EN), CNN/DailyMail and BBC summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reasoning tasks (BigBench-hard) including multiple-choice/judgement/text-generation reasoning problems; machine translation (IWSLT2017 Chinese→English); summarization (CNN/DailyMail, BBC).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Answer accuracy for BigBench; BERTScore for translation and summarization (also BLEU/ROUGE reported for correlation studies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Baseline deterministic generation (DG) average accuracy on 12 BigBench tasks (baseline value not given numerically in text); translation BERTScore = 0.818; summarization BERTScore = 0.886</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>SIRLC achieves +5.6% average accuracy vs DG across 12 BigBench tasks; translation BERTScore improved to 0.86; summarization BERTScore improved to 0.899</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires an unlabeled dataset of instruction+problem format; uses a separate fixed evaluator (initial model M*) — raising concerns whether the evaluator remains adequate as the generator improves; tested only up to 780M parameters (scalability to much larger LMs untested); some unseen-dataset generalization cases show declines (e.g., Sports Understanding and Tracking Shuffled Objects (5) small negative changes), so improvements are not universal. The reward distribution changes during training, requiring the evaluator to be fixed; potential dependence on evaluator quality and coverage is noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Self-improvement by Reinforcement Learning Contemplation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6937.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6937.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SE (generate + re-sample)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation with Conditional Re-generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight self-reflection strategy where the model generates an answer, self-evaluates its correctness (CEP), and if judged incorrect, re-generates an answer (one retry) to produce the final output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (Large used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned FLAN-T5 autoregressive generation with stochastic sampling (temperature 1) for initial outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>780M (FLAN-T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation with regeneration (w/ SE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate one answer, run CEP (binary correctness) using the LLM as evaluator; if CEP labels the answer incorrect, generate another answer and return it as final (no iterative multi-step revision beyond one regeneration).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with at most one regeneration (≤2 total generate attempts)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BigBench-hard (12 tasks reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging reasoning tasks from BigBench-hard (multiple-choice, judgement, fill-in-the-blank, text-generation reasoning problems).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Direct deterministic generation (DG) baseline accuracy (per-task numbers not fully enumerated in text); overall baseline used for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>With self-evaluation/regeneration, answer accuracy outperformed direct generation on 11 of 12 evaluated BigBench-hard tasks (aggregate improvement described qualitatively; no single overall number beyond 'outperforms on 11/12' provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on evaluator accuracy — if self-evaluation is wrong it can fail to trigger useful re-generation; only a single regeneration step is used (no recursive critique/refine cycles); evaluation accuracy itself is affected by the quality of generated text, so benefits may be limited when evaluator errors correlate with generator errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Self-improvement by Reinforcement Learning Contemplation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6937.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6937.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SE (sample & select)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Selection Among Multiple Samples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection-based self-reflection method where the model samples multiple candidate outputs, self-evaluates them (QEP for quality), and selects the highest-scoring candidate as the final output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (Large default)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FLAN-T5 sampling multiple outputs (three sampled candidates in experiments), with evaluator prompting (QEP) to rate quality on a numeric scale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>780M (FLAN-T5-Large)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation selection among k samples (k=3)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample k=3 candidate outputs per prompt, prompt the LLM with QEP to score each candidate (1–10), and choose the candidate with highest self-assigned score as the final output.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>sampling + internal scoring + selection (non-recursive)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text summarization (CNN/DailyMail, BBC) and translation (IWSLT2017 Chinese→English)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Longer-form text generation tasks: abstractive summarization (CNN/DailyMail, BBC) and machine translation (IWSLT2017 ZH→EN).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore (primary reported); also BLEU and ROUGE considered in correlation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>For translation: BERTScore = 0.818; for summarization: BERTScore = 0.886 (these are before-selection baselines reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Translation BERTScore improved to 0.86; summarization BERTScore improved to 0.899. For summarization, sampled outputs selected by SE had higher scores under BLEU/ROUGE/BERTScore than average of three samples without selection.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Selection among samples improves expected output quality but is limited by diversity/quality of sampled candidates; selection uses the same LLM as evaluator so evaluator bias may favor superficially higher-scoring outputs; for tasks where absolute correctness (rather than quality) matters, selection-only strategies may be less meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Self-improvement by Reinforcement Learning Contemplation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6937.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6937.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency for Chain-of-Thought Voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-focused reflection method that samples multiple chain-of-thought reasoning paths and selects the most consistent final answer (majority or marginalization over samples).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (Large used as baseline in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chain-of-thought prompting with multiple sampled reasoning paths; voting/marginalization to choose final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>780M (FLAN-T5-Large in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple chain-of-thought reasoning paths (paper uses 3 samples) and aggregate (vote/marginalize) among final answers to increase robustness of reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple sampled reasoning paths</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks (BigBench-hard subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Hard reasoning problems that benefit from chain-of-thought sampling and aggregation (multiple-choice reasoning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Deterministic single-path generation (DG) baseline accuracy (per-task values not fully enumerated in main text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>SC (with 3 samples) used as a baseline; paper reports SIRLC outperforms SC on most tasks (exact per-task SC numbers not fully listed in main text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SC is primarily applicable to reasoning tasks reliant on chain-of-thought and less applicable to non-reasoning generation tasks; limited by number and diversity of sampled paths (paper used 3 samples); not designed to provide learned model-improvements (it selects outputs at inference time rather than fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Self-improvement by Reinforcement Learning Contemplation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models can self-improve <em>(Rating: 2)</em></li>
                <li>Large language models are reasoners with self-verification <em>(Rating: 1)</em></li>
                <li>Planning with large language models via corrective re-prompting <em>(Rating: 1)</em></li>
                <li>Generate & rank: A multi-task framework for math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6937",
    "paper_id": "paper-258865735",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "SIRLC",
            "name_full": "Self-Improvement by Reinforcement Learning Contemplation",
            "brief_description": "An unsupervised fine-tuning method that treats an LLM as both student (generator) and teacher (evaluator): the model generates answers, self-evaluates them (CEP or QEP), and is updated with reinforcement learning (PPO) to maximize the self-evaluation reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (Large default; also Small and Base evaluated)",
            "model_description": "Instruction-tuned encoder-decoder Transformer (FLAN-T5) used autoregressively for generation; training via PPO with entropy regularization and KL penalty to keep close to pretrained model.",
            "model_size": "780M (FLAN-T5-Large); also 80M (Small) and 250M (Base) evaluated",
            "reflection_method_name": "Self-evaluation-as-reward (SIRLC)",
            "reflection_method_description": "LLM generates answers to unlabeled prompts, then the (fixed) initial LLM evaluates each generated answer using either a binary Correctness Evaluation Prompt (CEP) or a numeric Quality Evaluation Prompt (QEP, 1–10). The evaluation output is converted to a scalar reward and used to update the generator via PPO (RL). The evaluator is kept fixed (initial model M*) to stabilize training.",
            "iteration_type": "generate-then-reflect with RL update loop (sampling during training; reward used for PPO updates)",
            "num_iterations": null,
            "task_name": "Multiple — BigBench-hard (12 tasks reported), IWSLT2017 translation (ZH→EN), CNN/DailyMail and BBC summarization",
            "task_description": "Reasoning tasks (BigBench-hard) including multiple-choice/judgement/text-generation reasoning problems; machine translation (IWSLT2017 Chinese→English); summarization (CNN/DailyMail, BBC).",
            "evaluation_metric": "Answer accuracy for BigBench; BERTScore for translation and summarization (also BLEU/ROUGE reported for correlation studies)",
            "performance_before_reflection": "Baseline deterministic generation (DG) average accuracy on 12 BigBench tasks (baseline value not given numerically in text); translation BERTScore = 0.818; summarization BERTScore = 0.886",
            "performance_after_reflection": "SIRLC achieves +5.6% average accuracy vs DG across 12 BigBench tasks; translation BERTScore improved to 0.86; summarization BERTScore improved to 0.899",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Requires an unlabeled dataset of instruction+problem format; uses a separate fixed evaluator (initial model M*) — raising concerns whether the evaluator remains adequate as the generator improves; tested only up to 780M parameters (scalability to much larger LMs untested); some unseen-dataset generalization cases show declines (e.g., Sports Understanding and Tracking Shuffled Objects (5) small negative changes), so improvements are not universal. The reward distribution changes during training, requiring the evaluator to be fixed; potential dependence on evaluator quality and coverage is noted.",
            "uuid": "e6937.0",
            "source_info": {
                "paper_title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SE (generate + re-sample)",
            "name_full": "Self-Evaluation with Conditional Re-generation",
            "brief_description": "A lightweight self-reflection strategy where the model generates an answer, self-evaluates its correctness (CEP), and if judged incorrect, re-generates an answer (one retry) to produce the final output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (Large used in experiments)",
            "model_description": "Instruction-tuned FLAN-T5 autoregressive generation with stochastic sampling (temperature 1) for initial outputs.",
            "model_size": "780M (FLAN-T5-Large)",
            "reflection_method_name": "Self-Evaluation with regeneration (w/ SE)",
            "reflection_method_description": "Generate one answer, run CEP (binary correctness) using the LLM as evaluator; if CEP labels the answer incorrect, generate another answer and return it as final (no iterative multi-step revision beyond one regeneration).",
            "iteration_type": "generate-then-reflect with at most one regeneration (≤2 total generate attempts)",
            "num_iterations": 2,
            "task_name": "BigBench-hard (12 tasks reported)",
            "task_description": "Challenging reasoning tasks from BigBench-hard (multiple-choice, judgement, fill-in-the-blank, text-generation reasoning problems).",
            "evaluation_metric": "Answer accuracy",
            "performance_before_reflection": "Direct deterministic generation (DG) baseline accuracy (per-task numbers not fully enumerated in text); overall baseline used for comparison",
            "performance_after_reflection": "With self-evaluation/regeneration, answer accuracy outperformed direct generation on 11 of 12 evaluated BigBench-hard tasks (aggregate improvement described qualitatively; no single overall number beyond 'outperforms on 11/12' provided in text).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Depends on evaluator accuracy — if self-evaluation is wrong it can fail to trigger useful re-generation; only a single regeneration step is used (no recursive critique/refine cycles); evaluation accuracy itself is affected by the quality of generated text, so benefits may be limited when evaluator errors correlate with generator errors.",
            "uuid": "e6937.1",
            "source_info": {
                "paper_title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SE (sample & select)",
            "name_full": "Self-Evaluation Selection Among Multiple Samples",
            "brief_description": "A selection-based self-reflection method where the model samples multiple candidate outputs, self-evaluates them (QEP for quality), and selects the highest-scoring candidate as the final output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (Large default)",
            "model_description": "FLAN-T5 sampling multiple outputs (three sampled candidates in experiments), with evaluator prompting (QEP) to rate quality on a numeric scale.",
            "model_size": "780M (FLAN-T5-Large)",
            "reflection_method_name": "Self-Evaluation selection among k samples (k=3)",
            "reflection_method_description": "Sample k=3 candidate outputs per prompt, prompt the LLM with QEP to score each candidate (1–10), and choose the candidate with highest self-assigned score as the final output.",
            "iteration_type": "sampling + internal scoring + selection (non-recursive)",
            "num_iterations": 3,
            "task_name": "Text summarization (CNN/DailyMail, BBC) and translation (IWSLT2017 Chinese→English)",
            "task_description": "Longer-form text generation tasks: abstractive summarization (CNN/DailyMail, BBC) and machine translation (IWSLT2017 ZH→EN).",
            "evaluation_metric": "BERTScore (primary reported); also BLEU and ROUGE considered in correlation experiments",
            "performance_before_reflection": "For translation: BERTScore = 0.818; for summarization: BERTScore = 0.886 (these are before-selection baselines reported in paper)",
            "performance_after_reflection": "Translation BERTScore improved to 0.86; summarization BERTScore improved to 0.899. For summarization, sampled outputs selected by SE had higher scores under BLEU/ROUGE/BERTScore than average of three samples without selection.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Selection among samples improves expected output quality but is limited by diversity/quality of sampled candidates; selection uses the same LLM as evaluator so evaluator bias may favor superficially higher-scoring outputs; for tasks where absolute correctness (rather than quality) matters, selection-only strategies may be less meaningful.",
            "uuid": "e6937.2",
            "source_info": {
                "paper_title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency for Chain-of-Thought Voting",
            "brief_description": "A reasoning-focused reflection method that samples multiple chain-of-thought reasoning paths and selects the most consistent final answer (majority or marginalization over samples).",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (Large used as baseline in paper)",
            "model_description": "Chain-of-thought prompting with multiple sampled reasoning paths; voting/marginalization to choose final answer.",
            "model_size": "780M (FLAN-T5-Large in experiments)",
            "reflection_method_name": "Self-Consistency (SC)",
            "reflection_method_description": "Sample multiple chain-of-thought reasoning paths (paper uses 3 samples) and aggregate (vote/marginalize) among final answers to increase robustness of reasoning outputs.",
            "iteration_type": "voting over multiple sampled reasoning paths",
            "num_iterations": 3,
            "task_name": "Reasoning tasks (BigBench-hard subset)",
            "task_description": "Hard reasoning problems that benefit from chain-of-thought sampling and aggregation (multiple-choice reasoning tasks).",
            "evaluation_metric": "Answer accuracy",
            "performance_before_reflection": "Deterministic single-path generation (DG) baseline accuracy (per-task values not fully enumerated in main text)",
            "performance_after_reflection": "SC (with 3 samples) used as a baseline; paper reports SIRLC outperforms SC on most tasks (exact per-task SC numbers not fully listed in main text excerpt).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SC is primarily applicable to reasoning tasks reliant on chain-of-thought and less applicable to non-reasoning generation tasks; limited by number and diversity of sampled paths (paper used 3 samples); not designed to provide learned model-improvements (it selects outputs at inference time rather than fine-tuning).",
            "uuid": "e6937.3",
            "source_info": {
                "paper_title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 2,
            "sanitized_title": "large_language_models_can_selfimprove"
        },
        {
            "paper_title": "Large language models are reasoners with self-verification",
            "rating": 1,
            "sanitized_title": "large_language_models_are_reasoners_with_selfverification"
        },
        {
            "paper_title": "Planning with large language models via corrective re-prompting",
            "rating": 1,
            "sanitized_title": "planning_with_large_language_models_via_corrective_reprompting"
        },
        {
            "paper_title": "Generate & rank: A multi-task framework for math word problems",
            "rating": 1,
            "sanitized_title": "generate_rank_a_multitask_framework_for_math_word_problems"
        }
    ],
    "cost": 0.0128645,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LANGUAGE MODEL SELF-IMPROVEMENT BY REINFORCEMENT LEARNING CONTEMPLATION A PREPRINT</p>
<p>Jing-Cheng Pang 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>Pengyuan Wang 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>Kaiyuan Li 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>Xiong-Hui Chen 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>Jiacheng Xu 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>Zongzhang Zhang 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>Yang Yu 
National Key Laboratory for Novel Software Technology
Nanjing University
NanjingChina</p>
<p>LANGUAGE MODEL SELF-IMPROVEMENT BY REINFORCEMENT LEARNING CONTEMPLATION A PREPRINT
7A3B068939AB2E6B9C8F3B4046551526
Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks.However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain.This paper introduces a novel unsupervised method called Language Model Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels.Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text.Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher.As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly.The model parameters are updated using reinforcement learning to maximize the evaluation score.We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation.Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks.Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown impressive performance in numerous natural language processing (NLP) tasks, including language understanding, machine translation, and question answering Zhao et al. [2023], Liu et al. [2023].This success can be attributed to the Pre-training + Fine-tuning (PTFT) training framework, which involves training a language model on a large corpus and fine-tuning it on supervised NLP tasks.A fine-tuned language model can achieve state-of-the-art performance using various supervised datasets Ziegler et al. [2019].For example, InstructGPT Ouyang et al. [2022] and ChatGPT OpenAI [2023] fine-tune the GPT-3 Brown et al. [2020] model by introducing human preference and learning a reward model on human-comparison data.</p>
<p>However, fine-tuning LLMs typically requires extensive supervision in the form of labelled questions or human feedback, which can be time-consuming and labour-intensive.Recent research addresses this limitation by leveraging unlabelled data to improve LLMs' reasoning ability.For example, the self-consistency method Wang et al. [2023] samples diverse reasoning paths and selects the most consistent answer by marginalizing out the sampled paths.LMSI Huang et al. [2022a] employs the self-consistency method to generate high-quality answers, which are then used to fine-tune LLMs.Although these methods improve performance using unlabelled data, they are primarily designed for reasoning tasks that rely heavily on LLMs' chain-of-thought (CoT) ability, which is limited to reasoning problems Wei et al. [2022].On the other hand, reinforcement learning shows an impressive performance in fine-tuning LLMs without directly using labelled answers Ouyang et al. [2022], but it still requires amounts of annotation that reflects human preference and text quality.</p>
<p>arXiv:2305.14483v1 [cs.CL] 23 May 2023</p>
<p>Language Model Self-improvement by Reinforcement Learning Contemplation</p>
<p>Text generation</p>
<p>Self-evaluation</p>
<p>Language model</p>
<p>Language model</p>
<p>The answer is: Detective James had been working on the case for weeks, but he couldn't seem to make any headway ...</p>
<p>Answer: Yes, it is.</p>
<p>Text generation task</p>
<p>Write a mystery that leaves the reader unable to predict the ending.The answer is: ...</p>
<p>Is the following article a mystery that leaves the reader unable to predict the ending?[TEXT].Answer: ...</p>
<p>Self-evaluation task</p>
<p>Figure 1: A comparison between text generation and selfevaluation.Self-evaluation involves assessing and analyzing existing text, while generation requires the creation of entirely new text.</p>
<p>In this paper, we propose a novel approach for fine-tuning LLMs without external supervision.Our method capitalizes on the observation that it is simpler for a language model to evaluate the generated text than to generate it.For example, while writing an attractive story can be challenging, identifying the generated text is relatively easy.Fig. 1 illustrates the disparity between text generation and self-evaluation.We verify the self-evaluation ability of LLM through experiments on various NLP tasks.</p>
<p>Based on such evaluation ability, we propose Language Model Self-Improvement by Reinforcement Learning Contemplation (SIRLC), where the LLM both functions as a student and teacher.As a student, the LLM generates answers to unlabeled questions, while as a teacher, the LLM scores the generated answers.The LLM is subsequently updated through reinforcement learning to optimize for maximum evaluation scores.SIRLC employs self-evaluation results as the reward and utilizes reinforcement learning to retrain the LLM.We refer to this learning process as reinforcement learning contemplation.</p>
<p>The contribution of this work can be summarized as follow: Firstly, We introduce a novel approach for unsupervised fine-tuning of LLMs by utilizing self-evaluation as the reward and RL for training, eliminating the need for external supervision.Secondly, we conduct a comprehensive experimental analysis to demonstrate LLM's self-evaluation ability.</p>
<p>To the best of our knowledge, this is the first study that formally verifies the self-evaluation capability of LLMs.Finally, our experimental results demonstrate that our approach can improve LLM's ability to solve reasoning, summarization, and translation problems.We also present that SIRLC can be applied to LLMs with a parameter range of 80M to 780M, and the trained LLM generalizes well to new and unseen datasets, demonstrating the extensive applicability of the proposed method.</p>
<p>Related Work</p>
<p>Train language model with unlabeled data.Learning from unlabelled data is a promising approach that eliminates the need for extensive annotation.Self-training is a popular technique in this field, which entails assigning pseudo labels from a learned classifier to unlabeled data.These pseudo-labelled examples are then utilized to enhance the initial model training RoyChowdhury et al. [2019], Chen et al. [2021], He et al. [2020].In recent years Huang et al.</p>
<p>[2022a], self-training has been employed to fine-tune large-scale language models such as PaLM Chowdhery et al.</p>
<p>[2022] with 540B parameters, and it has shown immense potential.However, this approach heavily relies on the CoT prompt, which is limited to solving reasoning problems.In contrast to methods that train LLMs to fit pseudo-labels, our approach employs reinforcement learning to train LLMs, which has proven more effective than supervised learning Stiennon et al. [2020].</p>
<p>Train language models with reinforcement learning.RL has demonstrated significant success in training language models Stiennon et al.</p>
<p>[2020], Jang et al. [2022].For instance, some studies train LLMs by utilizing algorithmically defined reward functions for specific NLP tasks, such as BLEU for translation Wu et al. [2016], Nguyen et al. [2017] and ROUGE for summarization Wu and Hu [2018].In a departure from the heuristic definition of reward functions, another approach Cho et al. [2018] involves learning to evaluate text coherence and utilizing the learned model to provide rewards for summarization and long-form generation tasks.Incorporating human preferences into language models using reinforcement learning has recently gained popularity.A series of works Ziegler et al. [2019], Ouyang et al. [2022], OpenAI [2023] develop reward models reflecting human preferences and train language models using RL algorithms.However, all these related studies necessitate a pre-defined reward function or a reward model learned from annotated data.In contrast, our method eliminates the need for external labels or reward models.Instead, the language model is updated to maximize self-evaluation scores.</p>
<p>Self-evaluation of LLM.Prior research has effectively utilized the self-evaluation ability of LLM to identify errors in previously generated text Huang et al. [2022a], Cobbe et al. [2021], Weng et al. [2022], Raman et al. [2022], Wang et al. [2023].For instance, the re-prompting method Raman et al. [2022] detects errors in the current plan generated by LLM and revises the current step if an error is discovered.Self-verification Weng et al. [2022] assesses the accuracy of the generated answer by using it as a condition to construct a new task, subsequently prompting the LM to re-predict the original masked conditions.Self-consistency Wang et al. [2023] implicitly employs the self-evaluation ability of LLM by selecting the most consistent answer from a diverse set of reasoning paths.Additionally, some studies have used LLM to score the generated text directly.For example, the generate &amp; rank method Shen et al. [2021] trains a language model to rank answers based on a scoring system.Ziegler et al. [2019] develops a smaller language model as a reward model to evaluate the alignment between generated text and human-produced text.While these previous works leverage self-evaluation ability (to be more exact, only evaluation ability in some works) to rectify generation errors, our study showcases the self-evaluation ability through experimental results.It directly employs self-evaluation to determine the accuracy and quality of the text.</p>
<p>Preliminary</p>
<p>We begin with a vocabulary Σ and an LLM M which takes a token sequence of the question q = {q 0 , ..., q n } as input and predicts the next token using autoregressive modelling: o t+1 = M(q, {o 0 , . . ., o t }), where q ∈ Σ n and o t ∈ Σ.To fine-tune LLM with RL, we can view this problem as a Markov Decision Process (MDP) Puterman [1994], Sutton and Barto [1998], which is described as a tuple (S, A, P, R, γ, d 0 ):</p>
<p>• State space S: the space of input token sequences q ∪ {o 0 , o 1 , . . ., o t }.</p>
<p>• Action space A: the space of tokens o t .</p>
<p>• Reward function R(q, {o 0 , . . ., o t }): a score that reflects the quality of the generated answer to the question, which can be obtained from human feedback or a predefined criterion.The reward is typically given when the complete answer has been generated.• Transition P :
s t+1 = s t ∪ o t+1 . • Initial distribution d 0 : the distribution of question q.
Here, the LM M acts as a policy mapping from state space to the probability space over action space.The objective of RL is to train the policy to maximize the expected returns:
E ∞ t=0 γ t R(q, o) q ∼ d 0 , o t ∼ M(•|s t ) .
(1)</p>
<p>In SIRLC, we formulate the LLM fine-tuning problem as an MDP, analogous to the one described above, with the reward function derived from self-evaluation results.To simplify the notation, we use o ∼ M(q) to represent the autoregressive sampling of a complete answer o from the language model M, based on the input question q.</p>
<p>4 Large Language Models are Good at Self-evaluation</p>
<p>In this section, we aim to verify the self-evaluation ability of LLMs by investigating three key topics in subsequent subsections: (1) comparison of LLMs' ability in text generation and self-evaluation;</p>
<p>(2) the correlation between self-evaluation results and the established evaluation metrics; and (3) potential for self-evaluation to improve LLMs.</p>
<p>Comparison of Text Generation and Self-evaluation</p>
<p>We conduct experiments to compare the text generation and self-evaluation abilities of LLMs using the CommonGen Lin et al. [2020] task, which involves generating a sentence that describes an everyday scenario based on a given set of common concepts such as {dog, frisbee, catch, and throw}.Specifically, we use FLAN-T5 Chung et al. [2022] as the LLM to generate text based on common conceptual questions provided by CommonGen and evaluate the accuracy of the generated text through human evaluation.In addition, we assess the LLM's text evaluation ability by using the same LLM to evaluate whether the generated text meets the given concepts.Appendix B shows more experiment details (e.g., the prompts we use).As the experiment results presented in Fig. 2, we observe that the self-evaluation accuracy exceeds the generation accuracy in all scales of models.Especially when the parameter size of the model is small (see FLAN-T5-Large/XL), the self-evaluation accuracy significantly outperforms the text generation by 15%.These results indicate that it is simpler to self-evaluate the generated text than to generate high-quality text that meets contextual requirements.However, it is essential to note that the evaluation accuracy is affected by the quality of the generated text, and this experiment is only a preliminary demonstration of the LLM's ability to self-evaluate.We conduct more experiments to further verify the self-evaluation ability, as presented in the following subsections.</p>
<p>Correlation Between Self-evaluation and Established Metrics</p>
<p>This section provides an analysis of the correlation between self-evaluation and established metrics using two datasets: the CNN/Daily Mail dataset Nallapati et al. [2016] for text summarization and the IWSLT 2017 dataset Cettolo et al. [2017] for text translation.To find the relationship between self-evaluation and established metrics, LLM generates two answers for each question in these tasks and then self-evaluates to determine which answer is better.Additionally, we utilize three metrics, namely BLEU Papineni et al. [2002], ROUGE Lin [2004], and BERTScore Zhang et al. [2020], to compare the two answers, calculate their correlation coefficient with self-evaluation, and present the findings in Tab. 1.As demonstrated by the results, the self-evaluation exhibits a positive correlation with all the considered metrics, indicating a consistent measurement of the quality of the generated text, and it is a reliable way to measure the quality of the generated text.</p>
<p>Potential for Self-improvement</p>
<p>The previous section shows a positive correlation between self-evaluation and established metrics.However, it remains to be seen whether self-evaluation can be utilized to improve text generation.To investigate this problem, we design a text generation strategy that re-samples the answer based on the self-evaluation result, which will be explained in the following paragraphs.In order to evaluate the effectiveness of this approach, we conduct experiments on various NLP benchmarks.</p>
<p>One such benchmark is Bigbench-hard Srivastava et al. [2022], which includes multiple reasoning tasks consisting of multiple-choice and direct-generation tasks.We compare two answer generation strategies: (1) w/o SE: the answer is a directly deterministic output of the LLM, and (2) w/ SE: LLM generates an answer and evaluates its correctness.If the evaluation indicates the answer is incorrect, LLM re-generates an answer as the final output.We tested these two strategies on multiple Bigbench-hard tasks, and the results are presented in Tab. 2. The experiment results demonstrate that with self-evaluation, the answer accuracy outperforms that of direct answer generation on 11 of 12 evaluation tasks.This result justifies using self-evaluation to help LLM improve answer accuracy.</p>
<p>Furthermore, we also conduct experiments on two text summarization tasks, CNN/Daily Mail and BBC Gupta et al. [2021].As it is not meaningful to evaluate the correctness of generated summarizations, we use a different approach to utilize self-evaluation in this experiment: (1) w/ SE: LLM samples three different answers and evaluates which answer is the best one as the final answer, and (2) w/o SE: LLM samples three different answers, and we present the average score of the three answers.As shown in Fig. 3, the generated answers have higher scores under all three metrics when self-evaluation is used.This result suggests that self-evaluation can potentially improve the quality of the generated text, which serves as a stepping stone for building SIRLC method.</p>
<p>Self-improvement by Reinforcement Learning Contemplation</p>
<p>In the previous section, we observe that self-evaluation has the potential to be a helpful tool for LLM self-improvement.</p>
<p>In this section, we will elaborate on our method, SIRLC, that improves LLM ability without external labels based on self-evaluation.We are given a pre-trained language model M and an unlabeled training dataset
D train = {q i } |D| i=1
. The overview of our method is illustrated in Fig. 4, which iterates through the following steps:</p>
<p>• Gathering question-answer pair (q, o) via q ∼ D train , o ∼ M.</p>
<p>• Self-evaluation on the question-answer, and obtain the reward r.   • Self-improvement with reinforcement learning.</p>
<p>Answer generation to unlabeled questions.We randomly sample a batch of questions from D train and ask the LLM to generate answers to these questions with a temperature of T = 1.We use different prompts to generate answers for different questions, as described in Appendix B.2.For reasoning problems, we use the Chain-of-Thought (CoT) prompt, which has been shown to improve LLM performance in previous studies Wang et al. [2023], Huang et al. [2022a].</p>
<p>Self-evaluation as the reward.After gathering the question-answer pair (q, o), SIRLC employs the LLM to evaluate the reward of the generated answer:
R(q, o) = ϕ(M(p EP , q, o)),(2)
where ϕ is a text processing function that processes the LLM output to the numerical reward and p EP , is the prompt used for self-evaluation.SIRLC uses two types of evaluation prompts: (1) Correctness Evaluation Prompt (CEP): "Is the answer to the question correct?The question is:
[Q].
The answer is: [A]", and (2) Quality Evaluation Prompt (QEP): "Please evaluate the answer to the question and give me an evaluation score from 1 to 10.The question is:
[Q].
The answer is [A]".Depending on the type of question, either CEP or QEP is utilized to evaluate the generated text.</p>
<p>SIRLC applies CEP to assess the factual accuracy of the generated text, which is essential for tasks such as questionanswering and reasoning.The CEP prompts LLMs to verify the answer's correctness and identify any factual errors.Given a question q and the answer o generated by the LLM, the reward R(q, o) is a 0-1 value based on the evaluation result.SIRLC employs QEP to evaluate the overall effectiveness and coherence of the generated text in terms of its writing, structure, and style.Quality is often assessed on a scale, with higher scores indicating better overall effectiveness and coherence of the text.The QEP prompts LLMs to rate the text's quality on a scale of 1 to 10 based on how well it answers the question.This prompt type is helpful for text-generation tasks such as translation and summarization.</p>
<p>During training, the reward distribution may change as the LLM is updated.We use the initial pre-trained LLM M * for self-evaluation while keeping it fixed to ensure stable training.</p>
<p>Language model</p>
<p>(1) Generate</p>
<p>(3) Improve</p>
<p>Evaluation prompt</p>
<p>Figure 4: Overall training procedure of SIRLC, which iterates through three steps:</p>
<p>(1) Answer generation to the unlabeled questions.</p>
<p>(2) Self-evaluation by asking LM using evaluation prompt, with the evaluation results as the reward.</p>
<p>(3) Update the language model to maximize the reward using reinforcement learning algorithms.The solid lines represent the data flow, while the dashed line represents the update of LLM parameters.</p>
<p>Self-improvement through reinforcement learning.With the evaluation reward, the LLM M can be updated using any RL algorithm to maximize the reward.In our case, we employ the Proximal Policy Optimization (PPO) algorithm Schulman et al. [2017], which has demonstrated promising results in applications Huang et al. [2022b].To ensure better exploration, we apply entropy regularization, which prevents the sampling phase from converging too early.Additionally, we use the Kullback-Leibler (KL) divergence to prevent M from deviating too far from the initial pre-trained LLM.</p>
<p>Experiment</p>
<p>We conduct a series of experiments to demonstrate the effectiveness of our proposed SIRLC method.Our experiments encompass the following topics: (1) comparison of the proposed method with baseline methods on various NLP tasks (Section 6.2); (2) the generalization ability of the LLM on unseen datasets after fine-tuning using the proposed method (Section 6.3); and (3) the application of the proposed method to different sizes of language models (Section 6.3).We first introduce our experimental setup in the subsequent subsection.</p>
<p>Experiment Setup</p>
<p>Dataset for evaluation.We consider various NLP tasks that focus on the different abilities of LLM, which can be divided into three categories: (1) Reasoning problem: BigBench Srivastava et al. [2022] is a challenging generation task that requires complex reasoning capabilities of the language models.The tasks in BigBench are pretty diverse, including reasoning the final results of a sequence of actions, understanding dates, and completing tasks that require simple arithmetic calculations.In our experiments, we use 12 challenging tasks   fine-tunes the LLM using reinforcement learning (RL) and employs oracle metrics as the reward.The metric used is answer accuracy for tasks such as multiple-choice, judgment, and fill-in-the-blank.For translation and summarization tasks, BERTScore is used as the metric.(4) DG directly generates the answer using the deterministic output of the LLM.</p>
<p>Implementation details.We employ PPO to train the LLM for 6,000 gradient steps for each task, with each batch size of 12.We utilize the trlx repository from GitHub CarperAI [2020] to implement PPO.We implement SIRLC using CEP in reasoning tasks while employing QEP for other tasks.Unless otherwise specified, we use FLAN-T5-Large, which has 780M parameters, as our LLM in the experiments.All reported results are averaged over three random trials, and the experiments can be conducted using two GTX 3090 graphics cards with 24GB of memory.We provide specific hyperparameters and more detailed implementation descriptions in Appendix B.</p>
<p>Main Results</p>
<p>Training results on BigBench.We evaluate SIRLC extensively using the BigBench dataset.Tab. 4 presents the answer accuracy of the LLMs trained with SIRLC and other baseline methods.We see SIRLC outperforms all the baselines without the supervised label.In particular, the SIRLC outperforms the DG method by achieving a 5.6% higher average accuracy across 12 tasks.On some tasks, SIRLC even catches up with the performance of RLFT, which fine-tunes LLM with supervision information (e.g., Reasoning about Colored Objects).This could be attributed to the fact that the self-evaluation has higher accuracy on these tasks, contributing to the significant improvement of LLM.Besides, SIRLC outperforms SC and LMSI on most tasks, demonstrating the effectiveness of learning by self-evaluation.To better present the performance of SIRLC, we also depict the training curves of SIRLC in Fig. 5.With training with SIRLC, the answer accuracy of LLM improves and shows comparable improvement performance with RLFT on some tasks.</p>
<p>Results on translation and summarization.In addition to reasoning tasks, we evaluate the performance of SIRLC on two complex text generation tasks: IWSLT 2017 for translation and CNN/Daily Mail for summarization.As illustrated  in Fig. 6, SIRLC enhances the BERTScore from 0.818 to 0.86 in the translation task and from 0.886 to 0.899 in the summarization task.Unlike the BigBench tasks, where the generated text is relatively short, the LLM is required to produce longer text for these two tasks.The experimental results demonstrate that SIRLC effectively improves the text generation capabilities of the LLM.</p>
<p>Evaluation of SIRLC on Model Size Variations and Generalization Capabilities</p>
<p>We conduct experiments to verify the application range of SIRLC from two topics: (1) application to different model sizes and (2) the generalization ability of the trained LLM.</p>
<p>Performance of SIRLC on different sizes of models.We conduct experiments to assess the performance of SIRLC across various language model sizes.We select three distinct models: FLAN-T5-Small, FLAN-T5-Base, and FLAN-T5-Large, containing 80M, 250M, and 780M parameters, respectively.We train these models using SIRLC on three challenging BigBench tasks, with the final scores presented in Fig. 7.In general, SIRLC effectively enhances performance across different scales of language models.Notably, when the parameter size is small (80M), and the base score is low, the language model exhibits a significant improvement.Generalization to unseen datasets.We further investigate the applicability of SIRLC by evaluating the generalization capabilities of LLMs trained using our method.Specifically, we utilize SIRLC to simultaneously train an LLM on five tasks (i.e., Geometric Shapes, Logical Deduction (3), Logical Deduction (5), Navigate and Tracking Shuffled Objects (3)) while testing the LLM on five distinct tasks.As depicted in Tab. 5, the LLM trained with SIRLC enhances answer accuracy in three out of five unseen datasets, with an average answer accuracy improvement of 0.8%.A notable result is that the trained LLM obtains a accuracy improvement of 2.2% on Penguins in a</p>
<p>Conclusion</p>
<p>In this paper, we introduce a novel approach to training LLM without the need for external supervision: self-improvement by reinforcement learning contemplation (SIRLC), utilizing self-evaluation results of LLM as the reward and RL for LLM training.We demonstrate the self-evaluation capability of LLMs through extensive experimental analysis.By employing self-evaluation, LLMs can obtain valuable insights into their performance and identify areas for improvement.</p>
<p>Our experiments demonstrate that SIRLC effectively enhances LLM performance across various text generation tasks.</p>
<p>Moreover, we illustrate the potential for the SIRLC method to scale with varying model sizes and training data.However, there are some things that could be improved in our approach.First, like previous unsupervised methods for training language models, SIRLC requires an unlabeled dataset to generate answers and facilitate self-improvement.It would be worthwhile to explore training LLMs to without relying on the datasets.For instance, an LLM can enhance its specialized capabilities in text generation, such as producing positive movie reviews, through self-evaluation techniques.Additionally, our current implementation utilizes two separate models for evaluation and improvement.It remains to be investigated whether the evaluation capabilities of the initial models will remain sufficient as the trained LLMs improve.Lastly, we primarily evaluate SIRLC on LLMs with 780M parameters.Future testing should include larger language models to demonstrate the method's application range better.We hope this work provides new insights into training language models.Future research could address the aforementioned questions to develop more efficient and effective training methods for language models.</p>
<p>APPENDIX A Discussion</p>
<p>A.1 Comparison Between Text Generation and Self-evaluation</p>
<p>Our work leverages the crucial concept that evaluating generated text is simpler for LLMs than generating the text itself.</p>
<p>Though this idea may be intuitive enough Yuan et al. [2021], Celikyilmaz et al. [2020], we would like to discuss why does this idea hold via analysing the difference between text generation and text evaluation.</p>
<p>Both text evaluation and text generation pose unique challenges; however, the structured nature of existing text and the availability of external resources that can aid in the evaluation process.One reason for this is that evaluation involves assessing and analyzing existing text, whereas generation necessitates the creation of entirely new text.When evaluating text, the language model is provided with a piece of text and asked to analyze it based on various criteria, such as accuracy, relevance, and coherence.This task is typically easier because the language model is working with existing text that already possesses a certain level of structure and meaning.Moreover, the language model may have access to external sources of information that can assist in the evaluation process.</p>
<p>Conversely, text generation demands that the language model create new text from scratch, which is a more challenging task.The language model must generate text that is grammatically correct, coherent, and contextually appropriate while considering the intended purpose of the text.Additionally, generating text requires a higher level of creativity and linguistic fluency than evaluating existing text, as the language model must be capable of producing a wide range of possible responses to a given prompt, each with its own unique structure and meaning.</p>
<p>Our comprehensive experiments in Section 4 demonstrate that the self-evaluation ability of LLMs can be utilized for LLM self-improvement.</p>
<p>A.2 The Usage of Unlabelled Questions Dataset</p>
<p>In Section 5, we introduce SIRLC, assuming the availability of a training dataset D train consisting of unlabeled questions.This raises two questions:</p>
<p>(1) what type of data does the dataset contain? and (2) can SIRLC function without an unlabeled dataset?</p>
<p>Concerning the first question, our experiments involve questions in D train that adhere to an instruction + problem format.Instruction For example, a question might be "Translate the following text to Chinese (instruction).</p>
<p>[TEXT] (problem)", or "Summarize the following article (instruction).</p>
<p>[TEXT] (problem)".In this way, SIRLC can train a LLM using any open-source datasets in the NLP community by appending instructional text to the problems within these datasets.Table 6 presents the instructions we used in our experiments, where '[TEXT]' denotes the questions/text in the original dataset.This way to using dataset is similar to instruction learning Lou et al. [2023], which utilizes task instructions to guide the learning system in rapidly adapting to various new tasks.We posit that as the data in D train becomes more diverse and abundant, SIRLC can be employed to train an LLM with generalization ability.Our experiments in Section 6.3 offer preliminary validation of this outcome.As for the second question, although our experiments utilize an unlabeled dataset to train the LLM, we are interested in exploring whether SIRLC can be applied in the absence of a dataset.To achieve this, we need an objective, which serves as an evaluation criterion for assessing the quality of the generated text, to train the language model.For example, we might expect an LLM to consistently produce text that is positive or polite.In such a case, we can prompt the LLM to generate text randomly from the start token and evaluate its positivity or politeness using either the CEP or the QEP.This approach allows the LLM to optimize towards the desired attribute.However, it is important to note that some attributes of the text might be challenging to evaluate, making them unsuitable for self-improvement using the reinforcement learning contemplation method.</p>
<p>B Experiment Details</p>
<p>In this section, we will present more experiments details that are omitted in the main body due to the space limitation, including the evaluation task we use, the baselines, the prompts in different experiments, the hyper-parameters for reproducibility, etc.</p>
<p>B.1 Tasks for Evaluation</p>
<p>In our experiments, we use five challenging benchmarks in NLP domain to conduct various experiments to support our method.In this section, here we give a detailed introduction of these benchmarks, which are omitted in the main body.</p>
<p>CommonGen Lin et al. [2020] is a task that focuses on constrained text generation and includes a benchmark dataset.Its primary objective is to assess a machine's ability to generate common sense reasoning.The task requires constructing a coherent sentence that describes everyday scenarios using a predefined set of concepts.CommonGen presents significant challenges, as it demands two main skills: (1) utilizing background knowledge for relational reasoning, and</p>
<p>(2) effectively managing combination generalization of concealed concept combinations.We employ CommonGen to evaluate the text generation and self-evaluation ability of LLM, wherein the LLM is tasked with generating a sentence based on four concepts simultaneously.</p>
<p>Bigbench-hard Srivastava et al.</p>
<p>[2022] consists of 27 challenging tasks designed to evaluate the reasoning abilities of language models.These tasks present increased difficulty due to their complexity.In our experiments, we employ 12 challenging tasks that encompass various aspects of reasoning problems, including the following:</p>
<p>• Reasoning about Colored Objects (multiple choices): Answer simple questions about the colors of objects on a surface.</p>
<p>• Logical Deduction (multiple choices): Deduce the order of a sequence of objects.</p>
<p>• Tracking Shuffled Objects (multiple choices): Determine the final positions of a set of objects given their initial positions and a description of a sequence of swaps.</p>
<p>• Object Counting (text generation): Questions that involve enumerating objects of different types and asking the model to count them.</p>
<p>• Geometric Shapes (text generation): Name geometric shapes from their SVG paths.</p>
<p>• Web of Lies (judgement): Evaluate a random boolean function expressed as a word problem.</p>
<p>• Sports Understanding (judgement): Determine whether an artificially constructed sentence relating to sports is plausible or implausible.</p>
<p>• Penguins in a Table (text generation): Answer questions about a table of penguins and their attributes.</p>
<p>• Navigate (judgement): Given a series of navigation instructions, determine whether one would end up back at the starting point.</p>
<p>Note that certain tasks (e.g., Logical Deduction) encompass the same topic but are presented at varying levels of difficulty, and we introduce these tasks with varying levels together.</p>
<p>The CNN/Daily Mail Nallapati et al. [2016] dataset serves as a widely recognized benchmark for text summarization.</p>
<p>Comprising over 300,000 news articles from CNN and the Daily Mail, along with corresponding human-written summaries, this dataset has been instrumental in training and evaluating various text summarization models, including both extractive and abstractive methods.Each article features several highlights that summarize its main points, making the dataset ideal for training and testing automatic text summarization models that aim to generate concise versions of the original text while retaining crucial information.</p>
<p>The BBC Gupta et al. [2021] dataset, created by the BBC News website, is another widely used resource for text summarization.Encompassing approximately 2,225 news articles on diverse topics such as politics, entertainment, technology, and sports, the dataset has been pre-processed and annotated with human-generated summaries.These concise summaries, typically 3-4 sentences in length, make the dataset invaluable for training and evaluating text summarization models.</p>
<p>The IWSLT 2017 Cettolo et al. [2017] dataset serves as a benchmark for evaluating spoken language translation systems.Composed of parallel transcripts of TED talks in various languages, including English, German, French, Italian, and Spanish, the dataset provides both text and audio files for the talks.Frequently utilized in research on automatic speech recognition, machine translation, and spoken language understanding, the IWSLT 2017 dataset is an essential resource for developing and evaluating spoken language translation systems, particularly those designed to handle multilingual and cross-lingual speech and text.In our experiments, we focus on translation tasks that involve translating Chinese to English.</p>
<p>B.2 Prompts in Our Experiments</p>
<p>In different experiments, we use different prompts for distinct experimental purposes.We summarize these prompts in Tab. 7.</p>
<p>Experiment Prompt</p>
<p>Comparison of the text generation and self-evaluation (Section 4.1)</p>
<p>Consider a task which needs to generate a coherent sentence describing an everyday scenario using all following concepts.You will be given a few concepts and a sentence, please tell me whether the task is done.</p>
<p>B.3 Hyper-parameters</p>
<p>Tab. 8 presents the hyper-parameters used in our experiments.</p>
<p>B.4 Metrics used In Our Experiments</p>
<p>In our experiments, we present various experiment results under different metrics.For BigBench, we utilize accuracy to judge the correctness of the generated answer in comparison to the reference answer.In translation and summarization tasks, we consider the following metrics: BLEU is a reference-based metric that evaluates the similarity between a machine-generated output and one or more reference outputs.BERTScore is a reference-less metric that assesses</p>
<p>C.2 More Experiment Results about Self-evaluation Ability Verification</p>
<p>In Section 4.3, we examine the self-evaluation ability of LLMs and their potential for self-improvement.Due to space limitations in the main body, we present only a subset of the tasks (12 tasks).In this section, we provide the complete results for all 27 tasks in BigBench-Hard, as shown in Tab. 9.</p>
<p>FLANFigure 2 :
2
Figure 2: Comparison of the text generation and self-evaluation.</p>
<p>Figure 3 :
3
Figure 3: Comparison of text generation with/without self-evaluation on text summarization tasks.</p>
<p>Figure 5 :
5
Figure 5: Training curves on BigBench-hard tasks.The shaded area represents the standard deviation over three seeds.We present the training curves on more tasks in Appendix C.</p>
<p>Figure 7 :
7
Figure 7: Performance of SIRLC on different sizes of language models.SIRLC can improve the answer accuracy over the initial pre-trained model without supervision.</p>
<p>Figure 8 :
8
Figure 8: Training curves of reinforcement learning contemplation on BigBench-hard tasks.The shaded area represents the standard deviation over three seeds.</p>
<p>Table 2 :
2
Comparison of the answer accuracy between answer generation with/without self-evaluation.Full results on all 27 BigBench tasks are presented in Appendix C.2.
0.10w/o SE0.50w/o SE0.70w/o SEBLEU Score0.04 0.06 0.080.051 w/ SE 0.0660.0710.078ROUGE Score0.30 0.35 0.40 0.45w/ SE0.4180.442BERT Score0.50 0.55 0.60 0.650.526 0.534 w/ SE0.6180.6390.020.250.247 0.2590.450.00BBCCNN0.20BBCCNN0.40BBCCNN(a) BLEU(b) ROUGE(c) BERTScore</p>
<p>Gupta et al. [2021]016]e-choices, judgments and text generation tasks.(2)Languagetranslation:IWSLT2017Cettoloetal. [2017]dataset includes data in a variety of languages, including English, German, French, Chinese, Japanese, and Arabic, which has been widely used in machine translation research.(3)Textsummarization:CNN/DailyMailNallapati et al. [2016]and BBCGupta et al. [2021]are two popular datasets used for text summarization tasks.The CNN/Daily Mail dataset covers a wider range of topics than the BBC dataset, including politics, sports, and entertainment.In contrast, the BBC dataset focuses more on news and current events.See Tab. 3 for examples of the tasks used in our experiments.
Example inputsExample outputsJudgement"Lionel Messi hit a three-run homer. Answer"Implausible"('Plausible' or 'Implausible')."Text generation "I have a flute, a piano, a trombone, four"8"stoves, a violin, an accordion, a clarinet, adrum, two lamps, and a trumpet. How manymusical instruments do I have?"CNN/Daily"Summarize the following article: The"The NFL has sus-National Football League has indefinitelypended Atlanta Falconssuspended Atlanta Falcons quarterback ..."quarterback MichaelVick ..."ITSLW 2017"Please translate the following Chinese text"Hello, world."into English. Text: 你好，世界."Baselines for comparison. We compare SIRLC with representative methods that improve LLM without supervised data.These methods include: (1) Self-consistency (SC) Wang et al. [2023] samples a diverse set of reasoning paths instead ofsolely relying on the greedy LLM output. It then selects the most consistent answer by marginalizing out the sampled
reasoning paths.In our experiments, we use the number of sample paths as three.SC is typically applicable to reasoning problems, as it involves voting among different reasoning paths.(2)LMSIHuanget al. [2022a]utilizes the SC method to generate "high-confidence" answers for unlabeled questions.It then fine-tunes the LLM using the self-generated solutions as target outputs.Additionally, we consider the following: (3) Reinforcement Learning Fine-Tuning (RLFT)</p>
<p>Table 3 :
3
Examples of inputs and outputs for the tasks in our experiments.
Answer accuracy0.10 0.15 0.20SIRLC RLFT SCAnswer accuracy0.60 0.40 0.45 0.50 0.55SIRLC RLFT SCAnswer accuracy0.35 0.32 0.33 0.34SIRLC RLFT SC0.0502000 Gradient Step 400060000.3502000 Gradient Step 400060000.3102000 Gradient Step 40006000(a) Geometric Shapes(b) Logical Deduction (3)(c) Tracking Shuffled Objects</p>
<p>Table 4 :
4
The answer accuracy of SIRLC and baseline methods on Bigbench-hard tasks.Each value represents the average answer accuracy of the last three training iterations.The highest performing value among methods without external labels is highlighted in bold.</p>
<p>Table, which has weak connection with the training datasets.Additionally, no significant declines are observed in the remaining two datasets (i.e., Sports Understanding and Tracking Shuffled Objects (5)).These results highlight the potential of SIRLC to be applied to a broader range of datasets and enhance the overall performance of LLMs.
Logical Deduction (7)Object CountingPenguins in a TableSports UnderstandingTracking Shuffled Objects (5)AverageAcc. 36.7 (+1.5) 32.7 (+0.7) 18 (+2.2) 52.8 (-0.4)12.3 (-0.1)30.5 (+0.8)</p>
<p>Table 5 :
5
The answer accuracy (%) on unseen dataset.The LLM is trained with SIRLC on five training datasets.The values in parentheses indicate the improvement achieved over the initial model.</p>
<p>J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and Q. Liu.Generate &amp; rank: A multi-task framework for math word problems.In EMNLP, 2021.A.Srivastava, A. Rastogi, and A. R. etal.Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.arXiv, 2206.04615,2022.N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.Learning to summarize with human feedback.In NeurIPS, 2020.R. S. Sutton and A. G. Barto.Reinforcement learning: An introduction.IEEE Transactions on Neural Networks, 9(5): 1054-1054, 1998.X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou.Self-consistency improves chain of thought reasoning in language models.In ICLR, 2023.J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.Chain-of-thought prompting elicits reasoning in large language models.In NeurIPS, 2022.Y. Weng, M. Zhu, S. He, K. Liu, and J. Zhao.Large language models are reasoners with self-verification.arXiv, 2212.09561,2022.Y. Wu and B. Hu.Learning to extract coherent summary via deep reinforcement learning.In AAAI, 2018.Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al.Google's neural machine translation system: Bridging the gap between human and machine translation.arXiv, 1609.08144,2016.W. Yuan, G. Neubig, and P. Liu.Bartscore: Evaluating generated text as text generation.In NeurIPS, 2021.T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi.Bertscore: Evaluating text generation with BERT.In ICLR, 2020. .M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. F. Christiano, and G. Irving.Fine-tuning language models from human preferences.arXiv, 1909.08593,2019.</p>
<p>W. X.Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al.A survey of large language models.arXiv, 2303.18223,2023.D</p>
<p>Table 6 :
6
Instructions for different tasks in our experiments.
TaskInstructionBigbench-hard[TEXT] Let's think step by step.TranslationPlease help me translate the following Chinese textinto English. Text: [TEXT] Answer:Text summarizationPlease give a summary of the following text. Text:[TEXT] Answer:</p>
<p>Table 7 :
7
If you think the task is done, reply yes.If you think the task is not done, reply no.Concepts: [CONCEPT].Sentence: [SENTENCE].Prompts used in different experiments.
Correlation with self-evaluationSuppose you are a reviewer of the text translation. You willand established metrics (Transla-be given two translations of a text, please tell me which onetion) (Section 4.2)is better according to the conciseness, integrality of the trans-lation. If you think Translation (1) is better, reply (1). If youthink Translation (2) is better, reply (2). Text:[TASK] Trans-lation (1): [Translation_1] Translation (2): [Translation_2].Which one is better.Correlation with self-evaluationSuppose you are a reviewer of the text summary. You will beand established metrics (Summa-given two summaries of a text, please tell me which one isrization) (Section 4.2)better according to the conciseness, integrality of the summary.If you think Summary (1) is better, reply (1). If you thinkSummary (2) is better, reply (2). Text: [TASK] Summary (1):[Summary_1] Summary (2): [Summary_2]. Which one isbetter.CEP for BigBench (Section 6.2) Is the answer to the question correct? The question is: [Q].The answer is: [A]QEP for Translation (SectionPlease help me evaluate the translation results. Only give a6.2)score from 1 to 10, without explanation. Text: [Q] Translation:[A]QEP for Summarization (Sec-Please help me evaluate the summary results of the followingtion 6.2)text. Only give a score from 1 to 10, without explanation.Text: [Q] Summary: [A]</p>
<p>Table 8 :
8
Hyper-parameters in our experiments.the embeddings of a machine-generated output and a reference answer.ROUGE is another reference-based metric that measures the overlap between the generated output and one or more reference outputs.BLEU emphasizes precision, while ROUGE focuses on recall.
Hyper-parametersValuePPO epoch4PPO clip ratios0.2PPO λ0.95batch size12value function coefficient1.0learning rate1e-4γ0.99temperature for LLM exploration 1top_k of LLM50top_p of LLM0.95Sampling path of SC/LMSI3the similarity between C Additional Experimental Results
C.1 Training Curves on BigBench-hard DatasetsFig.8presents the all training curves on 12 Bigbench-hard tasks that are omitted in the main body.</p>
<p>Table 9 :
9
Comparison of the answer accuracy between answer generation with/without self-evaluation.C.3 Examples of the LLM Output Before and After TrainingTab.10-13present some examples of LLM output before/after trained with SIRLC.Please give a summary of the following text.Text: Kyle Naughton has been ruled out for the rest of the season after David Meyler's red card challenge on Saturday.The Hull midfielder was shown a straight red for the hefty challenge that left Naughton in hospital at the weekend, though neither Garry Monk nor Steve Bruce condemned Meyler at the time.A scan on Monday revealed Naughton suffered ligament damage and the right-back will face a six-week lay-off.David Meyler (left) lunges in high on Kyle Naughton and was sent off the poor challenge on Saturday.The full back writhes in pain following the incident and faces a six-week lay off with ankle ligament damage.A club statement said: 'An initial scan on Saturday ruled out a fracture, but a follow up scan last night confirmed ligament damage that will keep the January signing from Spurs out for around six weeks.'With just seven league games left, it's a blow for Naughton and the Swans with the right-back continuing to impress with every appearance.'Angel Rangel will return to the side to face Everton on Saturday.Rangel said: 'It's not good because Kyle has an injury and I don't wish bad on anyone, but from my point of view I always try and be ready so if the manager needs me I'll try my best.'Naughton will miss the rest of the season and had to helped off the pitch by the Swansea team physios.Meyler looks shocked as referee Andre Marriner shows him a straight red, a decision that was justified.Answer:
QuestionBefore trainingAfter trainingDavidMeylerDavid Meyler waslunged in high onsent off for theKyle Naughton andtackle on Kylewas sent off. TheNaughton.TheHull midfielder wasHull midfielder wasshown a straightshown a straight redred for the heftyfor the tackle.challenge.TheHull midfielder wasshown a straightred for the heftychallenge.TheHull midfielder wasshown a straightred for the heftychallenge.TheHull midfielder wasshown a straightred for the heftychallenge.TheSwansea.</p>
<p>Table 10 :
10
Model output examples on summarization task (CNN/Daily Mail).The table presents the output generated by the model before and after SIRLC training across various text input.Alice, Bob, and Claire are friends and avid readers who occasionally trade books.At the start of the semester, they each buy one new book: Alice gets Ulysses, Bob gets Frankenstein, and Claire gets Catch-22.As the semester proceeds, they start trading around the new books.First, Bob and Claire swap books.Then, Claire and Alice swap books.Finally, Alice and Bob swap books.At the end of the semester, Bob has Options: (A) Ulysses (B) Frankenstein (C) Catch-22.Let's think step by step.Alice, Bob, and Claire are dancers at a square dance.At the start of a song, they each have a partner: Alice is dancing with Izzi, Bob is dancing with Melissa, and Claire is dancing with Lola.Throughout the song, the dancers often trade partners.First, Claire and Alice switch partners.Then, Bob and Claire switch partners.Finally, Alice and Bob switch partners.At the end of the dance, Alice is dancing with Options: (A) Izzi (B) Melissa (C) Lola.Let's think step by step.Audrie tells the truth.Ryan says Audrie lies.Rashida says Ryan lies.Fletcher says Rashida tells the truth.Shaunda says Fletcher tells the truth.Does Shaunda tell the truth?Let's think step by step.
QuestionBefore trainingAfter trainingFrankenstein = 2/ 3 ofFrankenstein at thebob's books then bobend of the semester.gets a second book theySo the final answertrade around books bobis (B).eventually gets Ulysses.So, the answer is (A).At the start of the song,Izzi. So the final an-everyone has a part-swer is (A).ner, hence Alice startsdancing with Izzi; bobdances with Melissa;Claire dances with Lolathroughout the song,they usually trade part-ners. So, at the endof the dance, Alice isdancing with Izzi; atthe start, bob is dancingwith Melissa; at the end,Claire is dancing withLola. So, the answer is(C).If Fletcher tells the truth,Shaundasaysthe third party doesFletcher tells thenot have to believe histruth Shaunda iswords in this scenario,telling the truthShaunda does not havebecausefletcherto believe his statement.tells the truth. SoTherefore, the answer isthe answer is Yes.No.</p>
<p>Table 12 :
12
Model output examples on Bigbench.The table presents the output generated by the model before and after SIRLC training across various tasks.</p>
<p>Detailed descriptions about the tasks are in https://github.com/google/BIG-bench/blob/main.
QuestionBefore training After training I have two fish, four cats, two chickens, a mouse, and a goat.How many animals do I have?Let's think step by step.First we add all the animals: two fish, 4 cats, 2 chickens, mouse, and goat= 11.Now we add up all the animals together: 2 x 4 = 8.Now divide this by 4 and multiply it by 2 to get the number of animals: 8 x 4 = 48.So, the answer is 48.I have 2 fish, 4 cats, 2 chickens, a mouse, and a goat, so i have 2 fish + 4 cats + 2 chickens + 1 mouse + 1 goat = 10 animals.So, the answer is 10.I have a chicken, a snail, three dogs, a snake, and a goat.How many animals do I have?Let's think step by step.Since a pound equals 10 animals, the four animals in question have 10 animals / 4 = 5 animal weighings a dog / 4 the dog weighs 4. So a dog / 4 the snake weighs 1 and a snake weighs 05. . .I have a chicken, a snail, four dogs, a snake, and a goat.So I have a chicken + snail + four dogs + a snake + a goat = 7 animals.So the answer is 7.The following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a branch, there are three birds: a blue jay, a quail, and a falcon.The falcon is to the right of the blue jay.The blue jay is to the right of the quail.Options: (A) The blue jay is the second from the left (B) The quail is the second from the left (C) The falcon is the second from the left.Let's think step by step.The falcon is second from the left.The answer: (C).The blue jay is the second from the left.The answer: (A).The following paragraphs each describe a set of three objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a shelf, there are three books: a blue book, an orange book, and a red book.The blue book is the rightmost.The orange book is the leftmost.Options: (A) The blue book is the second from the left (B) The orange book is the second from the left (C) The red book is the second from the left.Let's think step by step.The orange book is second from the left.The answer: (B).The red book is the second from the left.The answer: (C).,7,50,11 Bernard,5,80,13 Vincent,9,60,11 Gwen,8,70,15.So the answer is (D).The following paragraphs each describe a set of five objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a branch, there are five birds: a falcon, a raven, a cardinal, a hawk, and a crow.The cardinal is the second from the right.The hawk is to the right of the falcon.The raven is the second from the left.The crow is the rightmost.Options: (A) The falcon is the rightmost (B) The raven is the rightmost (C) The cardinal is the rightmost (D) The hawk is the rightmost (E) The crow is the rightmost.Let's think step by step.The crow belongs to the rightmost group.The answer: (C).The crow is the rightmost.The answer: (E).Use M to sample answers {o i } to the questions, with stochastic output of LLM.5:Obtain reward R(q, o) according to self-evaluation results (Eq.2).6:Update M with RL algorithm.7: until training completion.E Societal ImpactThe introduction of SIRLC offers a promising solution to the challenges associated with fine-tuning large language models (LLMs) using external labels.This approach has the potential to reduce the cost and time required for supervision, which can increase access to NLP technologies for individuals and organizations with limited resources.Additionally, the increased accuracy and effectiveness of LLMs through the use of SIRLC has implications for a wide range of NLP tasks.This technology can improve NLP in areas such as machine translation, reasoning problems, and text generation.As a result, SIRLC has the potential to influence diverse industries such as healthcare, finance, and education, where accurate NLP model is essential for effective decision-making.SIRLC represents an exciting advancement in the field of NLP that could impact society, provided that the self-improvement of the LLMs remains aligned with human values.
Language models are few-shot learners. B Brown, B Mann, N R , NeurIPS2020</p>
<p>Transformer reinforcement learning X. GitHub repository, GitHub. Carperai, 2020</p>
<p>Evaluation of text generation: A survey. A Celikyilmaz, E Clark, J Gao, 2006.14799, 2020</p>
<p>Overview of the IWSLT 2017 evaluation campaign. M Cettolo, M Federico, L Bentivogli, J Niehues, S Stüker, K Sudoh, K Yoshino, C Federmann, IWSLT2017</p>
<p>Semi-supervised semantic segmentation with cross pseudo supervision. X Chen, Y Yuan, G Zeng, J Wang, CVPR. 2021</p>
<p>Towards coherent and cohesive long-form text generation. W S Cho, P Zhang, Y Zhang, X Li, M Galley, C Brockett, M Wang, J Gao, Workshop on Narrative Understanding. 2018</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv, 2204.02311Scaling language modeling with pathways. 2022</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S L , arXiv, 2210.114162022</p>
<p>K Cobbe, V Kosaraju, M Bavarian, J Hilton, R Nakano, C Hesse, J Schulman, arXiv, 2110.14168Training verifiers to solve math word problems. 2021</p>
<p>Automated news summarization using transformers. A Gupta, D Chugh, R Anjum, Katarya, arXiv, 2108.010642021</p>
<p>Revisiting self-training for neural sequence generation. J He, J Gu, J Shen, M Ranzato, ICLR2020</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv, 2210.116102022a</p>
<p>The 37 implementation details of proximal policy optimization. S Huang, R F J Dossa, A Raffin, A Kanervisto, W Wang, ICLR Blog Track. 2022b</p>
<p>Gpt-critic: Offline reinforcement learning for end-to-end task-oriented dialogue systems. Y Jang, J Lee, K.-E Kim, ICLR2022</p>
<p>Commongen: A constrained text generation challenge for generative commonsense reasoning. B Y Lin, M Shen, W Zhou, P Zhou, C Bhagavatula, Y Choi, X Ren, AKBC2020</p>
<p>ROUGE: A package for automatic evaluation of summaries. C.-Y Lin, TSBO. 2004</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023</p>
<p>Is prompt all you need? no. A comprehensive and broader view of instruction learning. R Lou, K Zhang, W Yin, arXiv, 2303.104752023</p>
<p>Abstractive text summarization using sequence-tosequence rnns and beyond. R Nallapati, B Zhou, C N Santos, Ç Gülçehre, B Xiang, CoNLL2016</p>
<p>Reinforcement learning for bandit neural machine translation with simulated human feedback. K Nguyen, H Daumé, Iii , J Boyd-Graber, arXiv, 1707.074022017</p>
<p>arXiv, 2303.08774OpenAI. GPT-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, NeurIPS. 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W Zhu, ACL. 2002</p>
<p>Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics. M L Puterman, 1994Wiley</p>
<p>Planning with large language models via corrective re-prompting. S S Raman, V Cohen, E Rosen, I Idrees, D Paulius, S Tellex, arXiv, 2211.099352022</p>
<p>Automatic adaptation of object detectors to new domains using self-training. A Roychowdhury, P Chakrabarty, A Singh, S Jin, H Jiang, L Cao, E Learned-Miller, CVPR. 2019</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv, 1707.06347Proximal policy optimization algorithms. 2017</p>            </div>
        </div>

    </div>
</body>
</html>
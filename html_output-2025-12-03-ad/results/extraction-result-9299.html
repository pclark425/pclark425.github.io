<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9299 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9299</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9299</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-268532616</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.12415v1.pdf" target="_blank">VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation</a></p>
                <p><strong>Paper Abstract:</strong> This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9299.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9299.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>System Sensitivity Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System sensitivity instruction (low / medium / high)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system-level prompt component that instructs the LLM how aggressively to report detected objects as navigation 'anomalies' (low = only imminent threats, medium = potential hazards, high = report all possible inconveniences). Used to control the LLM's binary/graded anomaly outputs and filtering behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot anomaly detection for visual navigation (frame-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given object detections (class, center_x/center_y, height, width, region, motion info) from first-person video frames, classify frames/objects as anomalies (emergency vs non-emergency) and generate user-directed descriptions/alerts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot structured prompting where a system prompt includes an explicit 'System sensitivity' line instructing the LLM to operate at low / medium / high sensitivity; the LLM receives recent-frame detection data and a sensitivity setting and must output structured JSON/voice text describing scene and anomaly decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison between low, medium, and high sensitivity prompt settings (same input detections; only the sensitivity instruction varies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Low sensitivity produced better detection performance relative to the rule-based baseline (improved precision and overall ROC/TP/TN behavior), while high sensitivity produced more false positives and worse trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Low sensitivity vs High sensitivity: ROC and confusion-matrix analysis in the paper show low sensitivity yields more True Positives and True Negatives and a lower False Positive rate than high sensitivity; medium sits between them (no numeric AUC or accuracy values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that low sensitivity improves precision because the LLM filters out low-confidence / low-risk detections (detected by the rule-based baseline) and only reports imminent threats; high sensitivity causes the LLM to flag many non-emergency objects as immediate dangers, increasing false alarms.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>LLM module evaluated on annotations from 50 first-person video clips; LLM processed detection summaries every 30 frames. Comparison used ROC curves and a total-frame confusion matrix (figures reported). Rule-based H-splitting (Left/Right/Front/Ground) served as baseline for labeling; no numeric AUC/accuracy reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9299.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9299.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Format Templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output-format prompt templates (full / voice / annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three user-facing master prompts that request different output formats and levels of detail from the LLM: a full structured JSON diagnostic, a minimal voice-only emergency alert, and a compact annotation (score + brief reason).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-time scene description and anomaly reporting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate navigation-oriented outputs (scene description, key objects, anomaly_label, anomaly_index, voice guide) from detection inputs under real-time constraints; three different prompt-output formats trade off verbosity, token usage, and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Three explicit LLM prompts (prompt_format_full, prompt_format_voice, prompt_format_annotation) that constrain the output shape: full JSON with multiple fields; voice_guide only; annotation with numerical anomaly_score and brief reason.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison across the three output modes with respect to latency, token consumption, and usefulness: 'voice-only' vs 'annotation' vs 'full'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative performance: 'Voice-only' mode yielded the lowest latency and minimal token/cost consumption; 'Annotation' was intermediate; 'Full' produced the richest outputs but incurred the highest token usage, latency and monetary cost. No single mode produced strictly better detection accuracy — trade-off is between richness and latency/cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Latency and cost trade-offs reported (voice-only lowest latency/cost; full highest token consumption and cost). Exact numerical accuracy differences between formats are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Richer output formats require more tokens and processing time, increasing latency and cost; for real-time assistive navigation, concise (voice-only) formats reduce latency and token use at the expense of diagnostic detail.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Modes were instrumented/measured for latency, token usage, and cost under a 2-hour/day usage estimate (Table 5). LLM processing scheduled every 30 frames; system-level end-to-end latency reported ~60 ms on a mobile device with neural engines (overall system), and the LLM selection (GPT-3.5 Turbo) was chosen to optimize latency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9299.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9299.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Prompt (Main)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Main instruction prompt (voice assistant framing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-level instruction fed as a system prompt telling the LLM it is a voice assistant for a visually impaired user and describing how to interpret center_x/center_y and size fields — intended to steer LLM outputs toward navigation-relevant descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interpretation and summarization of detection metadata for accessibility</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transform object detection metadata into succinct, safety-focused outputs for blind users (voice or structured annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Presence vs absence of the main instruction system prompt: with instruction the LLM receives a clear role and data-format indications; without it the LLM must infer role and output structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ablation experiment comparing system behavior with the instruction prompt present vs missing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ablation result (qualitative): Removing the instruction prompt caused the LLM to produce incoherent or irrelevant content and degraded anomaly detection/output reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>With instruction: coherent, task-relevant outputs and good anomaly filtering (when combined with other prompts). Without instruction: system may generate random or off-task content (no numeric metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicit role/instruction reduces LLM output variance and keeps responses task-focused; absence increases ambiguity and leads to off-target generations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation study summarized in Table 3; the paper reports qualitative drops when the instruction prompt was removed but does not supply per-condition numeric performance figures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9299.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9299.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Region / Location Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Location/region description prompt (H-split: Left/Right/Front/Ground)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt module that provides the LLM with region-based categorization of detections (Left 25%, Right 25%, Front center 50%/upper half, Ground center 50%/lower half) and instructs how to interpret proportion-based size/position, used to prioritize emergencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial prioritization of detected objects for anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use region labels and proportional size/position to decide which objects are immediate hazards (e.g., ground area and large side occlusions) and to prioritize alerts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Structured prompt that defines region semantics and provides normalized center_x/center_y/height/width proportions as inputs; LLM uses these to reason about object proximity and urgency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ablation: with region info vs without region info.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ablation result (qualitative): Removing region/location information weakens the LLM's ability to evaluate emergency priority and worsens anomaly-detection performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>With region info: improved priority evaluation and more appropriate alerts. Without region info: poorer prioritization (no numeric values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Region metadata supplies critical spatial priors that the LLM uses to infer proximity and likely collision risk; without it the LLM lacks structure needed to order hazards by urgency.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Region definitions are part of the detection post-processing; baseline rule labels frames as anomalies if ground-region detections exist or if Left/Right objects occupy >10% area. Ablation summarized in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9299.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9299.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Motion Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Motion / temporal prompt (use last frame + current to infer trajectory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt module that instructs the LLM to use the last frame and current frame to infer object motion (speed and direction) and collision likelihood, enabling temporal risk assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Temporal trajectory inference for collision risk estimation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given temporally adjacent detection metadata, estimate object motion vectors and use them to evaluate whether an object is moving toward the user and how quickly a collision could occur.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompted zero-shot reasoning over summarized motion fields: the prompt explicitly requests computing speed/direction and estimating collision imminence using two-frame temporal differences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not quantitatively isolated in the paper; motion prompt is described as part of the full prompt stack and is used to help the LLM assess trajectories and collision risk. No numeric ablation-specific metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Temporal/motion cues are expected to improve detection of dynamically approaching hazards versus static objects; authors include motion prompt to enable the LLM to reason about urgency.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Motion prompt uses information from the last frame and current frame; system detects every 5 frames and runs LLM inference every 30 frames, so motion estimates are coarse and based on available frame intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9299.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9299.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted Detection Class Switch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detection class manager prompt (scene-switch -> top-100 related objects)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A user-facing prompt that asks the LLM (GPT-4 in the system) to generate or adapt the list of detection classes for open-vocabulary detection when the scene changes (e.g., sidewalk -> park), enabling dynamic reconfiguration of the detector's interest list.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dynamic detection-class generation for scene adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>From a user command indicating a scene change, produce an ordered list of relevant object classes (top-100) focused on hazards and landmarks to guide the open-vocabulary detector's attention.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot conversational prompt: user issues 'The user is switching the scene to custom_scene please generate a new list...' and GPT-4 returns a topical class list (used to set detector interest targets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: GPT-4 produced scene-relevant class lists (examples given in Appendix B); authors report improved relevance of detection classes and dynamic scene transition capability but do not quantify detection accuracy improvements from class switching.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using an LLM to generate scene-adapted detection classes enables open-vocabulary detectors to focus on contextually relevant objects, reducing irrelevant detections and improving user-relevant output without model retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GPT-4 generated per-scene class lists (e.g., 'Visually impaired navigation' and 'Urban Walking'). Those lists were then used by the YOLO-World open-vocabulary detector as the detection-class manager input; no numeric evaluation of the class-switching effect on detection performance provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9299.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9299.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Prompt-based Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot LLM anomaly detector (prompt-based, rule-based baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's overall approach: a zero-shot, train-free pipeline where summarized detection outputs are fed to an LLM (GPT-3.5/GPT-4 components) via engineered prompts to perform anomaly detection and user-directed alerts, compared against a rule-based H-splitting baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo (core), GPT-4 (auxiliary for class generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Frame-level anomaly detection (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect anomalies relevant to safe navigation from first-person video frames (based on region rules and LLM reasoning) and produce an anomaly score/label and natural-language alerts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot structured prompts that include: system instruction, sensitivity setting, region/location metadata, motion prompt, and a required output format (full/voice/annotation). The LLM performs reasoning on the provided structured inputs without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to a deterministic rule-based anomaly detector (H-split + size threshold) as the baseline; prompt variations (sensitivity levels and presence/absence of prompt modules) also compared via ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that prompt-based LLM detection achieves high precision when all prompt modules are present; low sensitivity prompt yielded the best ROC/confusion matrix behavior versus the rule-based baseline. No absolute numeric AUC/accuracy values are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to the rule-based baseline: prompt-based LLM with proper prompts had higher precision (qualitative) and better trade-offs when set to low sensitivity; high sensitivity LLM over-reports anomalies and underperforms.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompt components supply structured priors (role, spatial semantics, motion cues, sensitivity) enabling the LLM to filter noisy detections and prioritize real dangers; missing components increase ambiguity and degrade outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Dataset: 50 first-person video clips collected by authors (various urban/suburban/park scenes). Baseline rule labeling: anomaly if any Ground detection or Left/Right detection >10% area. LLM processed detections every 30 frames; object detector ran every 5 frames. ROC, confusion matrix, selected sample outputs and ablation study presented; no numeric AUC/accuracy percentages reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Better zero-shot reasoning with self-adaptive prompting. <em>(Rating: 2)</em></li>
                <li>A systematic survey of prompt engineering on vision-language foundation models. <em>(Rating: 2)</em></li>
                <li>Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead. <em>(Rating: 1)</em></li>
                <li>Zero-shot object detection. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9299",
    "paper_id": "paper-268532616",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "System Sensitivity Prompt",
            "name_full": "System sensitivity instruction (low / medium / high)",
            "brief_description": "A system-level prompt component that instructs the LLM how aggressively to report detected objects as navigation 'anomalies' (low = only imminent threats, medium = potential hazards, high = report all possible inconveniences). Used to control the LLM's binary/graded anomaly outputs and filtering behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "Zero-shot anomaly detection for visual navigation (frame-level)",
            "task_description": "Given object detections (class, center_x/center_y, height, width, region, motion info) from first-person video frames, classify frames/objects as anomalies (emergency vs non-emergency) and generate user-directed descriptions/alerts.",
            "presentation_format": "Zero-shot structured prompting where a system prompt includes an explicit 'System sensitivity' line instructing the LLM to operate at low / medium / high sensitivity; the LLM receives recent-frame detection data and a sensitivity setting and must output structured JSON/voice text describing scene and anomaly decisions.",
            "comparison_format": "Direct comparison between low, medium, and high sensitivity prompt settings (same input detections; only the sensitivity instruction varies).",
            "performance": "Qualitative: Low sensitivity produced better detection performance relative to the rule-based baseline (improved precision and overall ROC/TP/TN behavior), while high sensitivity produced more false positives and worse trade-offs.",
            "performance_comparison": "Low sensitivity vs High sensitivity: ROC and confusion-matrix analysis in the paper show low sensitivity yields more True Positives and True Negatives and a lower False Positive rate than high sensitivity; medium sits between them (no numeric AUC or accuracy values provided).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors hypothesize that low sensitivity improves precision because the LLM filters out low-confidence / low-risk detections (detected by the rule-based baseline) and only reports imminent threats; high sensitivity causes the LLM to flag many non-emergency objects as immediate dangers, increasing false alarms.",
            "null_or_negative_result": true,
            "experimental_details": "LLM module evaluated on annotations from 50 first-person video clips; LLM processed detection summaries every 30 frames. Comparison used ROC curves and a total-frame confusion matrix (figures reported). Rule-based H-splitting (Left/Right/Front/Ground) served as baseline for labeling; no numeric AUC/accuracy reported in the text.",
            "uuid": "e9299.0",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Prompt Format Templates",
            "name_full": "Output-format prompt templates (full / voice / annotation)",
            "brief_description": "Three user-facing master prompts that request different output formats and levels of detail from the LLM: a full structured JSON diagnostic, a minimal voice-only emergency alert, and a compact annotation (score + brief reason).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "Real-time scene description and anomaly reporting",
            "task_description": "Generate navigation-oriented outputs (scene description, key objects, anomaly_label, anomaly_index, voice guide) from detection inputs under real-time constraints; three different prompt-output formats trade off verbosity, token usage, and latency.",
            "presentation_format": "Three explicit LLM prompts (prompt_format_full, prompt_format_voice, prompt_format_annotation) that constrain the output shape: full JSON with multiple fields; voice_guide only; annotation with numerical anomaly_score and brief reason.",
            "comparison_format": "Direct comparison across the three output modes with respect to latency, token consumption, and usefulness: 'voice-only' vs 'annotation' vs 'full'.",
            "performance": "Qualitative performance: 'Voice-only' mode yielded the lowest latency and minimal token/cost consumption; 'Annotation' was intermediate; 'Full' produced the richest outputs but incurred the highest token usage, latency and monetary cost. No single mode produced strictly better detection accuracy — trade-off is between richness and latency/cost.",
            "performance_comparison": "Latency and cost trade-offs reported (voice-only lowest latency/cost; full highest token consumption and cost). Exact numerical accuracy differences between formats are not provided.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Richer output formats require more tokens and processing time, increasing latency and cost; for real-time assistive navigation, concise (voice-only) formats reduce latency and token use at the expense of diagnostic detail.",
            "null_or_negative_result": null,
            "experimental_details": "Modes were instrumented/measured for latency, token usage, and cost under a 2-hour/day usage estimate (Table 5). LLM processing scheduled every 30 frames; system-level end-to-end latency reported ~60 ms on a mobile device with neural engines (overall system), and the LLM selection (GPT-3.5 Turbo) was chosen to optimize latency.",
            "uuid": "e9299.1",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Instruction Prompt (Main)",
            "name_full": "Main instruction prompt (voice assistant framing)",
            "brief_description": "A top-level instruction fed as a system prompt telling the LLM it is a voice assistant for a visually impaired user and describing how to interpret center_x/center_y and size fields — intended to steer LLM outputs toward navigation-relevant descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "Interpretation and summarization of detection metadata for accessibility",
            "task_description": "Transform object detection metadata into succinct, safety-focused outputs for blind users (voice or structured annotations).",
            "presentation_format": "Presence vs absence of the main instruction system prompt: with instruction the LLM receives a clear role and data-format indications; without it the LLM must infer role and output structure.",
            "comparison_format": "Ablation experiment comparing system behavior with the instruction prompt present vs missing.",
            "performance": "Ablation result (qualitative): Removing the instruction prompt caused the LLM to produce incoherent or irrelevant content and degraded anomaly detection/output reliability.",
            "performance_comparison": "With instruction: coherent, task-relevant outputs and good anomaly filtering (when combined with other prompts). Without instruction: system may generate random or off-task content (no numeric metrics provided).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Explicit role/instruction reduces LLM output variance and keeps responses task-focused; absence increases ambiguity and leads to off-target generations.",
            "null_or_negative_result": true,
            "experimental_details": "Ablation study summarized in Table 3; the paper reports qualitative drops when the instruction prompt was removed but does not supply per-condition numeric performance figures.",
            "uuid": "e9299.2",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Region / Location Prompt",
            "name_full": "Location/region description prompt (H-split: Left/Right/Front/Ground)",
            "brief_description": "A prompt module that provides the LLM with region-based categorization of detections (Left 25%, Right 25%, Front center 50%/upper half, Ground center 50%/lower half) and instructs how to interpret proportion-based size/position, used to prioritize emergencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "Spatial prioritization of detected objects for anomaly detection",
            "task_description": "Use region labels and proportional size/position to decide which objects are immediate hazards (e.g., ground area and large side occlusions) and to prioritize alerts.",
            "presentation_format": "Structured prompt that defines region semantics and provides normalized center_x/center_y/height/width proportions as inputs; LLM uses these to reason about object proximity and urgency.",
            "comparison_format": "Ablation: with region info vs without region info.",
            "performance": "Ablation result (qualitative): Removing region/location information weakens the LLM's ability to evaluate emergency priority and worsens anomaly-detection performance.",
            "performance_comparison": "With region info: improved priority evaluation and more appropriate alerts. Without region info: poorer prioritization (no numeric values reported).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Region metadata supplies critical spatial priors that the LLM uses to infer proximity and likely collision risk; without it the LLM lacks structure needed to order hazards by urgency.",
            "null_or_negative_result": true,
            "experimental_details": "Region definitions are part of the detection post-processing; baseline rule labels frames as anomalies if ground-region detections exist or if Left/Right objects occupy &gt;10% area. Ablation summarized in Table 3.",
            "uuid": "e9299.3",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Motion Prompt",
            "name_full": "Motion / temporal prompt (use last frame + current to infer trajectory)",
            "brief_description": "A prompt module that instructs the LLM to use the last frame and current frame to infer object motion (speed and direction) and collision likelihood, enabling temporal risk assessment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_size": null,
            "task_name": "Temporal trajectory inference for collision risk estimation",
            "task_description": "Given temporally adjacent detection metadata, estimate object motion vectors and use them to evaluate whether an object is moving toward the user and how quickly a collision could occur.",
            "presentation_format": "Prompted zero-shot reasoning over summarized motion fields: the prompt explicitly requests computing speed/direction and estimating collision imminence using two-frame temporal differences.",
            "comparison_format": null,
            "performance": "Not quantitatively isolated in the paper; motion prompt is described as part of the full prompt stack and is used to help the LLM assess trajectories and collision risk. No numeric ablation-specific metrics reported.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Temporal/motion cues are expected to improve detection of dynamically approaching hazards versus static objects; authors include motion prompt to enable the LLM to reason about urgency.",
            "null_or_negative_result": null,
            "experimental_details": "Motion prompt uses information from the last frame and current frame; system detects every 5 frames and runs LLM inference every 30 frames, so motion estimates are coarse and based on available frame intervals.",
            "uuid": "e9299.4",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Prompted Detection Class Switch",
            "name_full": "Detection class manager prompt (scene-switch -&gt; top-100 related objects)",
            "brief_description": "A user-facing prompt that asks the LLM (GPT-4 in the system) to generate or adapt the list of detection classes for open-vocabulary detection when the scene changes (e.g., sidewalk -&gt; park), enabling dynamic reconfiguration of the detector's interest list.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Dynamic detection-class generation for scene adaptation",
            "task_description": "From a user command indicating a scene change, produce an ordered list of relevant object classes (top-100) focused on hazards and landmarks to guide the open-vocabulary detector's attention.",
            "presentation_format": "Zero-shot conversational prompt: user issues 'The user is switching the scene to custom_scene please generate a new list...' and GPT-4 returns a topical class list (used to set detector interest targets).",
            "comparison_format": null,
            "performance": "Qualitative: GPT-4 produced scene-relevant class lists (examples given in Appendix B); authors report improved relevance of detection classes and dynamic scene transition capability but do not quantify detection accuracy improvements from class switching.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Using an LLM to generate scene-adapted detection classes enables open-vocabulary detectors to focus on contextually relevant objects, reducing irrelevant detections and improving user-relevant output without model retraining.",
            "null_or_negative_result": null,
            "experimental_details": "GPT-4 generated per-scene class lists (e.g., 'Visually impaired navigation' and 'Urban Walking'). Those lists were then used by the YOLO-World open-vocabulary detector as the detection-class manager input; no numeric evaluation of the class-switching effect on detection performance provided.",
            "uuid": "e9299.5",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Zero-shot Prompt-based Anomaly Detection",
            "name_full": "Zero-shot LLM anomaly detector (prompt-based, rule-based baseline comparison)",
            "brief_description": "The paper's overall approach: a zero-shot, train-free pipeline where summarized detection outputs are fed to an LLM (GPT-3.5/GPT-4 components) via engineered prompts to perform anomaly detection and user-directed alerts, compared against a rule-based H-splitting baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo (core), GPT-4 (auxiliary for class generation)",
            "model_size": null,
            "task_name": "Frame-level anomaly detection (zero-shot)",
            "task_description": "Detect anomalies relevant to safe navigation from first-person video frames (based on region rules and LLM reasoning) and produce an anomaly score/label and natural-language alerts.",
            "presentation_format": "Zero-shot structured prompts that include: system instruction, sensitivity setting, region/location metadata, motion prompt, and a required output format (full/voice/annotation). The LLM performs reasoning on the provided structured inputs without fine-tuning.",
            "comparison_format": "Compared to a deterministic rule-based anomaly detector (H-split + size threshold) as the baseline; prompt variations (sensitivity levels and presence/absence of prompt modules) also compared via ablation.",
            "performance": "Authors report that prompt-based LLM detection achieves high precision when all prompt modules are present; low sensitivity prompt yielded the best ROC/confusion matrix behavior versus the rule-based baseline. No absolute numeric AUC/accuracy values are provided.",
            "performance_comparison": "Compared to the rule-based baseline: prompt-based LLM with proper prompts had higher precision (qualitative) and better trade-offs when set to low sensitivity; high sensitivity LLM over-reports anomalies and underperforms.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Prompt components supply structured priors (role, spatial semantics, motion cues, sensitivity) enabling the LLM to filter noisy detections and prioritize real dangers; missing components increase ambiguity and degrade outputs.",
            "null_or_negative_result": null,
            "experimental_details": "Dataset: 50 first-person video clips collected by authors (various urban/suburban/park scenes). Baseline rule labeling: anomaly if any Ground detection or Left/Right detection &gt;10% area. LLM processed detections every 30 frames; object detector ran every 5 frames. ROC, confusion matrix, selected sample outputs and ablation study presented; no numeric AUC/accuracy percentages reported.",
            "uuid": "e9299.6",
            "source_info": {
                "paper_title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Better zero-shot reasoning with self-adaptive prompting.",
            "rating": 2,
            "sanitized_title": "better_zeroshot_reasoning_with_selfadaptive_prompting"
        },
        {
            "paper_title": "A systematic survey of prompt engineering on vision-language foundation models.",
            "rating": 2,
            "sanitized_title": "a_systematic_survey_of_prompt_engineering_on_visionlanguage_foundation_models"
        },
        {
            "paper_title": "Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead.",
            "rating": 1,
            "sanitized_title": "towards_generic_anomaly_detection_and_understanding_largescale_visuallinguistic_model_gpt4v_takes_the_lead"
        },
        {
            "paper_title": "Zero-shot object detection.",
            "rating": 1,
            "sanitized_title": "zeroshot_object_detection"
        }
    ],
    "cost": 0.015072499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VISIONGPT: LLM-ASSISTED REAL-TIME ANOMALY DETECTION FOR SAFE VISUAL NAVIGATION
19 Mar 2024</p>
<p>Hao Wang 
School of Computing
Clemson University Clemson
SCUSA</p>
<p>Jiayou Qin 
Department of Electrical and Computer Engineering
Stevens Institute of Technology Hoboken
NJUSA</p>
<p>Ashish Bastola abastol@g.clemson.edu 
School of Computing
Clemson University Clemson
SCUSA</p>
<p>Xiwen Chen xiwenc@g.clemson.edu 
School of Computing
Clemson University Clemson
SCUSA</p>
<p>Zihao Gong 
School of Cultural and Social Studies
Tokai University Tokyo
Japan</p>
<p>John Suchanek 
School of Computing
Clemson University Clemson
SCUSA</p>
<p>Abolfazl Razi arazi@clemson.edu 
School of Computing
Clemson University Clemson
SCUSA</p>
<p>VISIONGPT: LLM-ASSISTED REAL-TIME ANOMALY DETECTION FOR SAFE VISUAL NAVIGATION
19 Mar 202450C3492DA6FB4D170BA824D4E6093161arXiv:2403.12415v1[cs.CV]Open World Object DetectionAnomaly DetectionLarge Language ModelVision-language UnderstandingPrompt EngineeringGenerative AIGPT
This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation.With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances.Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation.Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.</p>
<p>Introduction</p>
<p>Accessible technologies have seen remarkable development in recent years due to the rise of machine learning and mobile computing [1][2][3][4][5].Deep learning has significantly enhanced the accuracy and speed of object detection and segmentation models [5][6][7][8][9], which catalyzed a surge of real-world applications, impacting numerous aspects of daily life, industry, and transportation.Visual navigation has benefited significantly from the evolution of such computer vision techniques [5,10,11].</p>
<p>Consequently, innovations such as Augmented Reality (AR) have been instrumental in enhancing the safety and mobility of individuals across various scenarios, including driving and walking.Many of these technologies aim to bridge the gap between the physical world and digital assistance, highlighting the critical need for adaptive solutions to navigate the complexities of real-world environments.</p>
<p>However, visual navigation presents significant challenges in dynamic urban environments [11][12][13].Although the newborn zero-shot object detection [14] addresses the significant limitations of classical object detection models such as YOLOv8 [15,16] in complex scenarios, it encounters difficulties in developing custom class labels for dynamic environments due to the long-tail response.Furthermore, real-time vision-language understanding can be critical for complex scenarios for safety concerns, especially for visually impaired individuals who must traverse streets, sidewalks, and other public spaces.</p>
<p>Vision-language understanding has recently become a hotspot due to the emergence of Multimodal Large Language Models (LLMs) [17].Multimodal LLMs represent an evolutionary leap in the field of artificial intelligence as they integrate the processing of text, images, and even audio and video [18] to create a comprehensive understanding of the world that mirrors human cognitive abilities more closely than ever before, making it possible to handle more advances tasks for robotics [19,20].Specifically, GPT-4V is now being heavily used in image tasks such as data evaluation [21,22], medical image diagnosis [23][24][25], and content creation [26][27][28].</p>
<p>Figure 1: Framework for vision-language processing and prompting.</p>
<p>Multimodal LLMs possess substantial improvement in interpreting, analyzing, and generating content across different modalities [29], bringing the possibility to interdisciplinary applications [30,31].Interestingly, LLMs also exhibit impressive zero-shot and few-shot learning abilities, potentially enabling them to capture visual concepts with minimal training data.This opens a way to address object detection challenges, particularly in data with limited annotation [32].Recent research attempts to bring LLMs to the accessibility field, yet most work only focuses on basic natural language processing such as text reading, image recognition, and voice assistance [33].</p>
<p>Therefore, a critical gap exists when using the vision-language understanding of LLMs for safety and accessible applications.Despite past works that investigated the use of LLMs in visual assistance [34] and visual navigation [35][36][37], only a few focused on the safety aspects [32,38] but barely considered the induced latency during the inference.</p>
<p>Our research introduces a framework that combines the speed of locally executed open-world object detection with the intelligence of LLMs to create a universal anomaly detection system.The primary goal of this system is to deliver real-time, personalized scene descriptions and safety notifications, ensuring the safety and ease of navigation for visually impaired users by identifying and alerting them to potential obstacles and hazards in their path, where these obstacles and hazards can be considered "anomalies" in the context of a safe and clear path for navigation.The proposed framework can also be applied to robotic systems, augmented reality platforms, and all other mobile computing edge units.</p>
<p>Particularly, the major contributions of this paper are summarized as follows:</p>
<p>• Zero-shot anomaly detection: The proposed integration is train-free and ready for video anomaly detection and annotation with different response preferences.• Real-time feedback: Our framework is optimized for real-time response in complex scenarios with very low latency.• Dynamic scene transition and interest setting: This framework can dynamically switch the object detection classes based on the user's needs.Furthermore, users can interact with the LLM module and setting a prior task (e.g., find the nearest bench).</p>
<p>2 Related Work</p>
<p>Open-vocabulary object detection</p>
<p>Open-vocabulary object detection (OVD) [39] represents a significant shift in object detection focusing on identifying items outside predefined categories.Initial efforts [40] trained on known classes for evaluating the detection of novel objects facing generalization and adaptability issues due to limited datasets and vocabularies.Recent approaches, however, [41][42][43] employ image-text matching with extensive data to expand the training vocabularies inspired by vision language pre-training [29,44].OWL-ViTs [45] and GLIP [46] utilize vision transformers and phrase grounding for effective OVD while Grounding DINO [47] combines these with detection transformers for crossmodality fusion.Despite the promise, existing methods often rely on complex detectors increasing computational demands significantly [48,49].ZSD-YOLO [50] also explored an open-vocabulary detection with YOLO using language model alignment; however, YOLO-world [51] presents a more efficient and real-time OVD solution aiming to be much more efficient with real-time inference using an effective pre-training strategy while still being highly generalizable.</p>
<p>Prompt Engineering</p>
<p>Prompt engineering has emerged as a critical technique for unlocking the capabilities of large language models (LLMs) [52][53][54][55] to various applications without finetuning on large datasets.This involves carefully crafting text prompts, instructions, or examples to guide LLM behavior and elicit desired responses.Researchers are actively exploring prompt engineering using various prompting techniques such as zero-shot prompting [56], Few-shot prompting [57], Chain-ofthought prompting [58], self-ask prompting [59], etc. to fine-tune LLMs for various tasks, demonstrating significant performance gains compared to traditional model training approaches.Studies have showcased how prompt engineering can adapt LLMs for diverse natural language tasks like question-answering [60], smart-reply [55], summarization [61], and text classification [62,63].Furthermore, researchers are increasingly developing frameworks to systematize prompt engineering efforts.Such frameworks aim to simplify the creation of effective prompts and facilitate the adaptation of LLMs to specific domains and applications and are highly customizable to user needs.While prompt engineering has seen significant improvements in natural language processing, its potential in computer vision on accessibility remains less explored.Our work builds upon the success of prompt engineering in NLP, exploring its application in the visual domain to enhance object detection and description.</p>
<p>Accessible Technology</p>
<p>Computer vision-driven accessible technologies are mostly designed to empower individuals with visual impairments through enhanced scene understanding and hazard detection.A range of solutions exist, including mobile apps that provide object recognition and audio descriptions of surroundings [64][65][66], to wearable systems that offer real-time alerts about obstacles or potential dangers [67,68].For example, technologies that detect approaching vehicles and crosswalk signals significantly improve the safety of visually impaired pedestrians in urban environments.Moreover, computer vision is integrated into assistive technologies for reading text aloud from documents and identifying objects in daily life, enabling greater independence [5].Research in this domain also focuses on indoor navigation, where object detection and spatial mapping can guide users within buildings and public spaces [69].The core emphasis of these computer vision-powered accessibility technologies aims to enhance safety.By providing real-time information on key elements within an individual's surroundings, the risk of accidents and injuries is significantly reduced.Identifying potential hazards, such as oncoming traffic, obstacles on sidewalks, or unattended objects, allows visually impaired individuals to navigate with greater confidence and autonomy.</p>
<p>Methodology</p>
<p>Our system offers real-time anomaly alerts by integrating object detection with large language model capabilities, featuring a multi-module architecture.The system operates continuously with the object detection module processing real-time camera frames.Multi-frame object information is then included in specially engineered prompts and submitted to the LLM module.The system then processes the LLM's response, classifying potential anomalies.Finally, the LLM module conveys important alerts and essential scene descriptions to the user.</p>
<p>The proposed project is fully open-sourced and available at: https://github.com/AIS-Clemson/VisionGPT</p>
<p>Object Detection Module</p>
<p>To ensure real-time performance on mobile devices, we employ lightweight yet powerful object detection models for real-time detection.Specifically, we applied the state-of-the-art YOLO-World model for open-vocabulary detection whose detection classes are customizable for a wide range of scenarios.As we focus on accessible visual navigation, we prompt the proposed LLM module to personalize the detection classes relevant to safe navigation in daily use circumstances, including pedestrians, vehicles, bicycles, traffic signals, and any potential road hazards or obstacles.Therefore, our proposed multi-functional prompt manager allows users to switch detection classes dynamically.</p>
<p>Detection Class Manager</p>
<p>This sub-module aims to create a detailed categorization for object detection algorithms, enabling them to identify and distinguish potential obstacles, hazards, and useful landmarks.This approach ensures the detection system is finely tuned to urban navigation's specific needs and challenges, enhancing the user's ability to move safely and independently through city streets.By focusing on road hazards and obstacles, the updated list aims to provide a more relevant and focused set of detection classes for the 'urban walking' context, optimizing the system's utility and effectiveness for the visually impaired user.</p>
<p>As shown in Figure 1, the user can interact with the LLM (advance) module.Based on this operation logic, the user can ask to change the object detection classes based on scenarios.For instance, if a user experiences a scene transition from sidewalk to park, the detection classes specialized for sidewalk objects (e.g., car, road cone, traffic signal, etc.) can be replaced by new object classes that are more relevant to the park scene to adapt to the situation.</p>
<p>Original prompt: "The user is switching the scene to custom_scene please generate a new list that contains the top 100 related objects, including especially road hazards and possible obstacles"</p>
<p>Anomaly Handle Module</p>
<p>The proposed anomaly detection system aims to enhance navigation safety and awareness in various environments, particularly for visually impaired individuals and others requiring navigation assistance (e.g., robotics systems).The system analyzes real-time imagery captured from a camera and splits the image into four distinct regions based on an 'H' pattern.Moreover, we find that:</p>
<p>• Left and Right: These regions cover the left and right 25% of the image, respectively.Objects detected here are typically in motion and may occupy much of the visual field.This area is crucial for identifying moving hazards such as vehicles or cyclists that may approach the user from the sides.• Front: This region focuses on the center 50% of the image's width and the upper half vertically.It captures objects still at a distance but directly ahead of the user.Identifying objects in this region is necessary for assessing the overall situation and planning movements, especially in detecting upcoming objects at high speed such as cars and cyclists.• Ground: Occupying the center 50% of the image's width and the lower half vertically, this area highlights objects nearby on the ground.Immediate attention to detections in this area is critical for avoiding hazards that require cautious navigation, such as cracks, puddles, or uneven surfaces.</p>
<p>The system then records detailed information for each object, including classification, size, and position.All size and position data have been converted into percentage expressions for a better interpretation by LLM.Finally, by analyzing objects' locations and sizes, alerts for anomalies are generated for objects that appear on the 'ground' area or occupy significant space (&gt; 10% in this study) in the 'left' or 'right' regions.The detection and movement information is then post-processed into a structured format, supporting LLM for better understanding.</p>
<p>Original prompt: "The location information center x , center y , height, width of objects is the proportion to the image, the detected objects are categorized into 4 type based on the image region.Left and Right: objects located on left 25% or right 25% of the image, these objects are usually moving and has large proportion.Front: objects that may still far away, can be used to discriminate the current situation.Ground: objects that may nearby."</p>
<p>Data Collection</p>
<p>Even though various datasets exist for static images [70] and CCTV camera feeds [71][72][73], no extensive datasets are available for detecting large anomalies in visual navigation from the first person's perspective.</p>
<p>Thus, we collected 50 video clips of point-of-view cruising in various scenarios.These custom videos are filmed in public spaces with first-person view and continuous forward-moving.We then conducted the experiments by combining the open-vocabulary object detection model with our novel imagesplitting method to annotate the frames as anomalies.</p>
<p>Specifically, a frame is labeled as an anomaly if it meets either of the following criteria:</p>
<ol>
<li>Objects are detected within the Ground area.2. Objects appear in either the Left or the Right areas of the image and occupy more than 10% of the total image area.</li>
</ol>
<p>We set this rule-based method as the baseline of anomaly detection in this study, as our captured video clips are customized for this H-splitting principle.</p>
<p>LLM Module</p>
<p>This module processes the detected object information and passing to the LLM.Specifically, we use both GPT-3.5 and GPT-4 to process the information.First, GPT-3.5 is mainly used for low-level information processes such as object detection data analysis, data format converting, and prompt reasoning.Therefore, GPT-4 is used for a high-level command instance understanding and a comprehensive vision-language understanding.</p>
<p>The system sensitivity settings indicate a focus on identifying and reporting hazards based on their potential impact on the user's safety and navigation.The system's goal to report objects based on their level of inconvenience or danger aligns with the anomaly detection objective of identifying and reacting to deviations that matter most in the given context.Note that the system sensitivity in the context is distinct from the model sensitivity as a statistical term.</p>
<p>These prompts sketch the conceptual framework and operational guidelines for a voice-assisted navigation system for visual accessibility.The system utilizes data from a phone camera, which is always facing forward, to detect objects and categorize their location within the field of view.Based on these analyses, the system provides auditory feedback to users, helping them navigate their environment safely and avoid potential hazards.Furthermore, the annotated data can be used for the training of other anomaly detection models.</p>
<p>The main LLM prompts consist of:</p>
<p>• Prompt instruction: "You are a voice assistant for a visually impaired user, the input is the actual data collected by a phone camera, and the phone is always facing front, please provide the key information for the blind user to help him navigate and avoid potential danger.Please note that the center_x and center_y represent the object location (proportional to the image), object height and width are also a proportion."</p>
<p>• Prompt sensitivity: "System sensitivity: Incorporate the sensitivity setting in your response.For a lowsensitivity setting, identify and report only imminent and direct threats to safety.For medium sensitivity, include potential hazards that could pose a risk if not avoided.For high sensitivity, report all detected objects that could cause any inconvenience or danger.Current sensitivity: low."</p>
<p>Experiments</p>
<p>We compare our proposed vision-LLM system with the rule-based anomaly detection (baseline) to show its performance and reliability.</p>
<p>System Optimization</p>
<p>While the proposed system is running, we input captured images into the object detection model every 5 frames to boost the FPS (Frame-Per-Second), this can significantly improve the performance, especially for mobile devices that have limited computation resources.Then, we send the detected information to the anomaly handle module to label the frames as the baseline.With the frame compensation, the real-time detection performance is boosted from 16 FPS to 73 FPS, as shown in Table 4.</p>
<p>Meanwhile, we apply the LLM module to process the detected information every 30 frames in parallel due to the latency of LLMs.To optimize the latency for better performance, the proposed system uses the GPT 3.5 Turbo model as the core of the LLM module.</p>
<p>Detection Accuracy</p>
<p>By setting the rule-based detector as the baseline, this study aims to test the zero-shot learning capability of the LLM detector, and meanwhile, interpret which prompt may impact the performance significantly.</p>
<p>After comparing the annotation results of prompt-based anomaly detection with the rule-based anomaly detection on our collected data, we find that prompt-based anomaly detection achieves high precision with all prompt modules working properly.Specifically, we compared the LLM anomaly detection with different sensitivity settings: low, normal, and high.As shown in Figure 3, the Receiver Operating Characteristic (ROC) curve indicates that a low system sensitivity leads to better performance, as it is less sensitive than the rule-based detector.For instance, objects detected by the rule-based detector with low confidence and classes of low risk will be filtered by LLM due to no emergency.</p>
<p>Conversely, the higher the system sensitivity, the worse the performance, as the system tends to categorize all possible anomalies as immediate emergencies.</p>
<p>As shown in Figure 4, the LLM anomaly detector with low-system sensitivity captures more True Positive and True Negative cases and tries to minimize the False Positive rate.Figure 4: Confusion matrix of total frames.LLM setting is low-system sensitivity setting.</p>
<p>Quality Evaluation</p>
<p>We picked one of the video clips to analyze the detection difference between the rule-based detector and the LLM anomaly detector.Specifically, in Figure 5, the first row shows the anomalies labeled by the rule-based detector, while the second row indicates the anomalies predicted by the LLM-based detector (low sensitivity setting).As shown in Figure 5, the proposed LLM detector has less acuity with a low-sensitivity prompt setting, which tends to filter anomalies that are non-emergency.Table 2 shows the selected sample output of the LLM module.</p>
<p>Ablation Study</p>
<p>To explore the contribution of different prompt modules, we conducted the ablation study of each module.Table 3 shows that the proposed system performs worse without specific prompt modules.For instance, while the instruction prompt is missing, the system may generate random content due to the confusion of the current task and lack of instruction.Moreover, missing region information of detected objects may also weaken the performance, as the system cannot evaluate the priority of the emergency.Moreover, we find that LLM produced different performances with different sensitivity prompts.Unexpectedly, Low system sensitivity appears higher accuracy and precision, as the system tries to catch True Positive cases as much as possible and avoid false alarms.This is significant for visually impaired navigation, as the user can efficiently avoid misinformation and frequent-unnecessary alerts.</p>
<p>Performance Evaluation</p>
<p>We further explore the performance efficiency of the proposed system on multiple platforms to reveal its potential for other applications.</p>
<p>Latency: As shown in Table 4, we measured end-to-end system latency and individual module processing times to identify bottlenecks and optimize for real-time performance.Results indicated an average end-to-end latency of 60 ms on the mobile device (e.g., smartphone) with neural engines, ensuring timely feedback.Economy: We further investigated the system latency and token consumption for economy evaluation.We designed three different modes for users to choose from:</p>
<p>• Voice only: only output voice messages for emergency response, minimum latency.</p>
<p>• Annotation: output both anomaly index and reason for system testing and practical annotation.As shown in Table 5, we estimated the cost of our system with different modes.The prices are calculated with an average of 2 hours of daily usage and are based on the chatGPT API pricing policy.</p>
<p>Original prompt:</p>
<p>• Prompt_format_full: 'Please organize your output into this format: "scene": quickly describe the current situation for blind user; "key_objects": quickly and roughly locate the key objects for blind user; "anomaly_checker": quickly diagnose if there is potential danger for a blind person; "anomaly_label": output 1 if there is an emergency, output 0 if not; "anomaly_index": object_id, danger_index, estimate a score from 0 to 1 about each objects that may cause danger; "voice_guide": the main output to instant alert the blind person for emergency.'• Prompt_format_voice: 'Please organize your output into this format: "voice_guide": the main output to instantly alert the blind person for an emergency.'• Prompt_format_annotation: 'Please organize your output into this format: "anomaly_score": predict a score from 0 to 1 to evaluate the emergency level; "reason": explain your annotation reason within 10 words.'</p>
<p>Conclusion</p>
<p>This research demonstrates the significant potential of combining lightweight mobile object detection with large language models to enhance accessibility for visually impaired individuals.Our system successfully provides real-time scene descriptions and hazard alerts, achieving low latency and demonstrating the flexibility of prompt engineering for tailoring LLM output to this unique domain.Our experiments highlight the importance of balancing detection accuracy with computational efficiency for mobile deployment.Prompt design is a key component of our system in guiding LLM responses and ensuring the relevance of generated descriptions.Additionally, the integration of user feedback proved invaluable for refining the system's usability and overall user experience.</p>
<p>While this project offers a promising foundation, further research is warranted.Explorations into even more advanced prompt engineering for complex scenarios would pave the way for the wide adoption of such assistive technologies.Our findings illustrate the power of integrating computer vision and large language models, leading to greater independence and safety in daily life: a true testament to AI's ability to improve the quality of life for all.</p>
<p>Figure 2 :
2
Figure 2: Type H image splitter.(1) and (2) represent the left and right area, (3) represent the ground area, and (4) represent the front area.</p>
<p>Figure 3 :
3
Figure 3: ROC curve.Figure4: Confusion matrix of total frames.LLM setting is low-system sensitivity setting.</p>
<p>Figure 5 :
5
Figure 5: Anomaly annotation.The first row represents the labeled anomalies by the rule-based detector (binary), and the second row represents the anomalies predicted by the proposed LLM detector (float).Color represents the probability of anomalies.</p>
<p>Table ? ?
?
shows the details of the collected data.
LocationSceneMovement Weather Clips Total length Unique Classes Total detected objectsUrbanSidewalkScooterCloudy810 mins3116944SuburbanBikelineScooterCloudy56 mins268394UrbanParkScooterCloudy65 mins2315310CityRoadBikingSunny55 mins215464CitySidewalkBikingSunny76 mins279569CityParkBikingCloudy55 mins194781TownParkWalkingCloudy64 mins185156TownSidewalkWalkingSunny87 mins148274CityCoastWalkingSunny25 mins3729280Suburban Theme ParkWalkingRain36 mins3424180</p>
<p>Table 1 :
1
Collected data for video anomaly detection.</p>
<p>Table 2 :
2
Selected feedback and caption of the output of the LLM module.Frane ID indicates the video frame index, the Anomaly Index represents the predicted anomalies of LLM, and Reason represents the response message from LLM for anomaly interpretation.
Frame ID Anomaly IndexReason22900.85'Car and people nearby.'37700.2'Green traffic light detected.'44300.5'Obstacles in path.'54500.7'Car on the left'70000'No immediate danger.'82301'Bike in close proximity'98001High risk of collision with multiple people</p>
<p>Table 3 :
3
Ablation study.✓indicates incorporated modules of system, and ✗indicate missing modules</p>
<p>Table 4 :
4
Object Detector Test on multiple platforms.A16-Bionic processors are used in iPhone 14 pro max, and M2 processors are widely used in Vision Pro and the latest Mac models.The PyTorch-based implementation was run on NVIDIA GPU.</p>
<p>Table 5 :
5
• Full: output full information in a structured JSON format.(See the original prompt for more information) Economy and latency test.
ModeLLMLatency Completion Tokens Prompt Tokens Total Tokens Charge (USD/day)Voice only GPT3.5407355736082.44Annotation GPT3.5628485736172.58FullGPT3.518181761195137113.53
A Original PromptThis section illustrates all used prompts in the proposed system.A.1 LLM InstructionSystem instructions are usually directly fed into LLMs as a self-prompt and generally do not consume token usage.Main Instruction: "You are a voice assistant for a visually impaired user, the input is the actual data collected by a phone camera, and the phone is always facing front, please provide the key information for the blind user to help him navigate and avoid potential danger.Please note that the center_x and center_y represent the object location (proportional to the image), object height and width are also a proportion."System Sensitivity Prompt: "System sensitivity: Incorporate the sensitivity setting in your response.For a low-sensitivity setting, identify and report only imminent and direct threats to safety.For medium sensitivity, include potential hazards that could pose a risk if not avoided.For high sensitivity, report all detected objects that could cause any inconvenience or danger.Current sensitivity: low."Location Prompt:"The location information center x , center y , height, width of objects is the proportion to the image, the detected objects are categorized into 4 type based on the image region.Left and Right: objects located on left 25% or right 25% of the image, these objects are usually moving and has large proportion.Front: objects that may still far away, can be used to discriminate the current situation.Ground: objects that may nearby."Motion Prompt:"Using the information from last frame and current frame to analyze the movement (speed and direction) and location of each object to determine its trajectory relative to the user.Use this information to assess whether an object is moving towards the user or they are static.If moving, how quickly a potential collision might occur based on the object's speed and direction of movement."A.2 LLM PromptLLM prompts are the master prompts that are directly input from the user end, text tokens are counted to the usage.To control and optimize the usage, we designed three different output modes.Furthermore, the designed prompt will guide LLM to generate a structured data format (dictionary, list, JSON, etc.).Full diagnose mode:'Please organize your output into this format: "scene": quickly describe the current situation for blind user; "key_objects": quickly and roughly locate the key objects for blind user; "anomaly_checker": quickly diagnose if there is potential danger for a blind person; "anomaly_label": output 1 if there is an emergency, output 0 if not; "anomaly_index": object_id, danger_index, estimate a score from 0 to 1 about each objects that may cause danger; "voice_guide": the main output to instant alert the blind person for emergency.'Voice-only mode: 'Please organize your output into this format:"voice_guide": the main output to instantly alert the blind person for an emergency.'Annotation mode:'Please organize your output into this format: "anomaly_score": predict a score from 0 to 1 to evaluate the emergency level; "reason": explain your annotation reason within 10 words.'A.3 Other PromptsDetection Classes Switch: "The user is switching the scene to custom_scene please generate a new list that contains the top 100 related objects, including especially road hazards and possible obstacles" Interest Target Setting: "Please analyze the user command and extract the user required object, output into this format: "add": object_name."B Detection Labels for Custom ScenesThis section illustrates the detection classes generated by GPT-4 that are customized for specific scenes.Visually impaired navigation:[ 'car', 'person', 'bus', 'bicycle', 'motorcycle', 'traffic light', 'stop sign', 'fountain', 'crosswalk', 'sidewalk', 'door', 'stair', 'escalator', 'elevator', 'ramp', 'bench', 'trash can', 'pole', 'fence', 'tree', 'dog', 'cat', 'bird', 'parking meter', 'mailbox', 'manhole', 'puddle', 'construction sign', 'construction barrier', 'scaffolding', 'hole', 'crack', 'speed bump', 'curb', 'guardrail', 'traffic cone', 'traffic barrel', 'pedestrian signal', 'street sign', 'fire hydrant', 'lamp post', 'bench', 'picnic table', 'public restroom', 'fountain', 'statue', 'monument', 'directional sign', 'information sign', 'map', 'emergency exit', 'no smoking sign', 'wet floor sign', 'closed sign', 'open sign', 'entrance sign', 'exit sign', 'stairs sign', 'escalator sign', 'elevator sign', 'restroom sign', 'men restroom sign', 'women restroom sign', 'unisex restroom sign', 'baby changing station', 'wheelchair accessible sign', 'braille sign', 'audio signal device', 'tactile paving', 'detectable warning surface', 'guide rail', 'handrail', 'turnstile', 'gate', 'ticket barrier', 'security checkpoint', 'metal detector', 'baggage claim', 'lost and found', 'information desk', 'meeting point', 'waiting area', 'seating area', 'boarding area', 'disembarking area', 'charging station', 'water dispenser', 'vending machine', 'ATM', 'kiosk', 'public telephone', 'public Wi-Fi hotspot', 'emergency phone', 'first aid station', 'defibrillator', 'tree', 'pole', 'lamp post', 'staff', 'road hazard'] Urban Walking: ['pedestrian', 'cyclist', 'car', 'bus', 'motorcycle', 'scooter', 'electric scooter', 'traffic light', 'stop sign', 'crosswalk', 'sidewalk', 'curb', 'ramp', 'stair', 'escalator', 'elevator', 'bench', 'trash can', 'pole', 'fence', 'tree', 'fire hydrant', 'lamp post', 'construction barrier', 'construction sign', 'scaffolding', 'hole', 'crack', 'speed bump', 'puddle', 'manhole', 'drain', 'grate', 'loose gravel', 'ice patch', 'snow pile', 'leaf pile', 'standing water', 'mud', 'sand', 'street sign', 'directional sign', 'information sign', 'parking meter', 'mailbox', 'bicycle rack', 'outdoor seating', 'planter box', 'bollard', 'guardrail', 'traffic cone', 'traffic barrel', 'pedestrian signal', 'crowd', 'animal', 'dog', 'bird', 'cat', 'public restroom', 'fountain',
After access: Inclusion, development, and a more mobile Internet. Jonathan Donner, 2015MIT press</p>
<p>Smart technologies for visually impaired: Assisting and conquering infirmity of blind people using ai technologies. Fatma Al-Muqbali, Noura Al-Tourshi, Khuloud Al-Kiyumi, Faizal Hajmohideen, 2020 12th Annual Undergraduate Research Conference on Applied Computing (URC). IEEE2020</p>
<p>An ai-based visual aid with integrated reading assistant for the completely blind. Muiz Ahmed Khan, Pias Paul, Mahmudur Rashid, Mainul Hossain, Md Atiqur, Rahman Ahad, IEEE Transactions on Human-Machine Systems. 5062020</p>
<p>Vision-based mobile indoor assistive navigation aid for blind people. Bing Li, Juan Pablo Munoz, Xuejian Rong, Qingtian Chen, Jizhong Xiao, Yingli Tian, Aries Arditi, Mohammed Yousuf, IEEE transactions on mobile computing. 1832018</p>
<p>Multi-functional glasses for the blind and visually impaired: Design and development. Ashish Bastola, Md Atik Enam, Ananta Bastola, Aaron Gluck, Julian Brinkley, Proceedings of the Human Factors and Ergonomics Society Annual Meeting. the Human Factors and Ergonomics Society Annual MeetingLos Angeles, CASAGE Publications Sage CA202367</p>
<p>An evaluation of retinanet on indoor object detection for blind and visually impaired persons assistance navigation. Mouna Afif, Riadh Ayachi, Yahia Said, Edwige Pissaloux, Mohamed Atri, Neural Processing Letters. 512020</p>
<p>People with visual impairment training personal object recognizers: Feasibility and challenges. Hernisa Kacorri, Kris M Kitani, Jeffrey P Bigham, Chieko Asakawa, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing Systems2017</p>
<p>Object detection and recognition: using deep learning to assist the visually impaired. Abinash Bhandari, Abeer Prasad, Angelika Alsadoon, Maag, Disability and Rehabilitation: Assistive Technology. 1632021</p>
<p>Muhammad Tariq Mahmood, and Ik Hyun Lee. Cnn-based object recognition and tracking system to assist visually impaired people. Fahad Ashiq, Muhammad Asif, Bin Maaz, Sadia Ahmad, Khalid Zafar, Toqeer Masood, Mahmood, IEEE access. 102022</p>
<p>Chatgpt for visually impaired and blind. Askat Kuzdeuov, Shakhizat Nurgaliyev, Hüseyin Atakan Varol, 2023Authorea Preprints</p>
<p>Driving towards inclusion: Revisiting in-vehicle interaction in autonomous vehicles. Ashish Bastola, Julian Brinkley, Hao Wang, Abolfazl Razi, arXiv:2401.145712024arXiv preprint</p>
<p>Vialm: A survey and benchmark of visually impaired assistance with large models. Yi Zhao, Yilin Zhang, Rong Xiang, Jing Li, Hillming Li, arXiv:2402.017352024arXiv preprint</p>
<p>Feedback mechanism for blind and visually impaired: a review. Ashish Bastola, Aaron Gluck, Julian Brinkley, Proceedings of the Human Factors and Ergonomics Society Annual Meeting. the Human Factors and Ergonomics Society Annual MeetingLos Angeles, CASAGE Publications Sage CA202367</p>
<p>Zero-shot object detection. Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, Ajay Divakaran, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>You only look once: Unified, real-time object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>. Glenn Jocher, Ayush Chaurasia, Jing Qiu, January 2023Ultralytics YOLO</p>
<p>Vision-language navigation: A survey and taxonomy. Wansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, Yue Hu, Neural Computing and Applications. 3672024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Interactive navigation in environments with traversable obstacles using large language and vision-language models. Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, Au, arXiv:2310.088732023arXiv preprint</p>
<p>Improving vision-and-language navigation by generating future-view image semantics. Jialu Li, Mohit Bansal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Supervised abnormal event detection based on chatgpt attention mechanism. Feng Tian, Yuanyuan Lu, Fang Liu, Guibao Ma, Neili Zong, Xin Wang, Chao Liu, Ningbin Wei, Kaiguang Cao, Multimedia Tools and Applications. 2024</p>
<p>Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead. Yunkang Cao, Xiaohao Xu, Chen Sun, Xiaonan Huang, Weiming Shen, arXiv:2311.027822023arXiv preprint</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>Performance of multimodal gpt-4v on usmle with image: Potential for imaging diagnostic support with explanations. medRxiv. Zhichao Yang, Zonghai Yao, Mahbuba Tasmin, Parth Vashisht, Seok Won, Feiyun Jang, Beining Ouyang, Dan Wang, Hong Berlowitz, Yu, 2023</p>
<p>Gpt-4: a new era of artificial intelligence in medicine. Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, Sharif Amit Kamran, Nasif Zaman, Prithul Sarker, Andrew G Lee, Alireza Tavakkoli, Irish Journal of Medical Science. 19261971. 2023</p>
<p>Harnessing llms in curricular design: Using gpt-4 to support authoring of learning objectives. Pragnya Sridhar, Aidan Doyle, Arav Agarwal, Christopher Bogart, Jaromir Savelka, Majd Sakr, arXiv:2306.174592023arXiv preprint</p>
<p>Evaluating gpt-4 on impressions generation in radiology reports. Zhaoyi Sun, Hanley Ong, Patrick Kennedy, Liyan Tang, Shirley Chen, Jonathan Elias, Eugene Lucas, George Shih, Yifan Peng, Radiology. 3075e2312592023</p>
<p>Unlocking the power of generative ai models and systems such as gpt-4 and chatgpt for higher education: A guide for students and lecturers. Henner Gimpel, Kristina Hall, Stefan Decker, Torsten Eymann, Luis Lämmermann, Alexander Mädche, Maximilian Röglinger, Caroline Ruiner, Manfred Schoch, Mareike Schoop, Hohenheim Discussion Papers in Business, Economics and Social Sciences. 2023Technical report</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Image segmentation using text and image prompts. Timo Lüddecke, Alexander Ecker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>Gpt-4 enhanced multimodal grounding for autonomous driving: Leveraging cross-modal attention with large language models. Haicheng Liao, Huanming Shen, Zhenning Li, Chengyue Wang, Guofa Li, Yiming Bie, Chengzhong Xu, Communications in Transportation Research. 41001162024</p>
<p>Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian Mcauley, Jianfeng Gao, arXiv:2311.075622023arXiv preprint</p>
<p>Mapgpt: Map-guided prompting for unified vision-and-language navigation. Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K Wong, arXiv:2401.073142024arXiv preprint</p>
<p>Langnav: Language as a perceptual representation for navigation. Rameswar Bowen Pan, Souyoung Panda, Rogerio Jin, Aude Feris, Phillip Oliva, Yoon Isola, Kim, arXiv:2310.078892023arXiv preprint</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on Robot Learning. PMLR2023</p>
<p>Navhint: Vision and language navigation agent with a hint generator. Yue Zhang, Quan Guo, Parisa Kordjamshidi, arXiv:2402.025592024arXiv preprint</p>
<p>Is it safe to cross? interpretable risk assessment with gpt-4v for safety-aware street crossing. Hochul Hwang, Sunjae Kwon, Yekyung Kim, Donghyun Kim, arXiv:2402.067942024arXiv preprint</p>
<p>Open-vocabulary object detection using captions. Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Open-vocabulary object detection via vision and language knowledge distillation. Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui, arXiv:2104.139212021arXiv preprint</p>
<p>Regionclip: Region-based language-image pretraining. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Detecting twenty-thousand classes using image-level supervision. Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, Ishan Misra, European Conference on Computer Vision. Springer2022</p>
<p>Aligning bag of regions for openvocabulary object detection. Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, Chen Change Loy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International conference on machine learning. PMLR2021</p>
<p>Video owl-vit: Temporally-consistent open-world localization in video. Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario Lučić, Fisher Yu, Thomas Kipf, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Grounded language-image pre-training. Liunian Harold, Li , Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, Heung-Yeung Shum, arXiv:2203.03605Dino: Detr with improved denoising anchor boxes for end-to-end object detection. 2022arXiv preprint</p>
<p>Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, Stan Z Li, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Zsd-yolo: Zero-shot yolo detection using vision-language knowledgedistillation. Johnathan Xie, Shuai Zheng, arXiv:2109.12066202113arXiv preprint</p>
<p>Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan, arXiv:2401.17270Real-time open-vocabulary object detection. Yolo-world2024arXiv preprint</p>
<p>A systematic survey of prompt engineering on vision-language foundation models. Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Philip Volker Tresp, Torr, arXiv:2307.129802023arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 2023</p>
<p>Prompt engineering for healthcare: Methodologies and applications. Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, arXiv:2304.146702023arXiv preprint</p>
<p>Llm-based smart reply (lsr): Enhancing collaborative performance with chatgpt-mediated smart reply system (acm)(draft) llm-based smart reply (lsr): Enhancing collaborative performance with chatgpt-mediated smart reply system. Ashish Bastola, Hao Wang, Judsen Hembree, Pooja Yadav, Nathan Mcneese, Abolfazl Razi, arXiv:2306.119802023arXiv preprint</p>
<p>Better zero-shot reasoning with self-adaptive prompting. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan O Arik, Tomas Pfister, arXiv:2305.141062023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.033502022arXiv preprint</p>
<p>Toolqa: A dataset for llm question answering with external tools. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, Advances in Neural Information Processing Systems. 362024</p>
<p>On extractive and abstractive neural document summarization with transformer language models. Jonathan Pilault, Raymond Li, Sandeep Subramanian, Christopher Pal, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP). the 2020 conference on empirical methods in natural language processing (EMNLP)2020</p>
<p>Zero-shot text classification with generative language models. Raul Puri, Bryan Catanzaro, arXiv:1912.101652019arXiv preprint</p>
<p>Large language models in the workplace: A case study on prompt engineering for job type classification. Benjamin Clavié, Alexandru Ciceu, Frederick Naylor, Guillaume Soulié, Thomas Brightwell, International Conference on Applications of Natural Language to Information Systems. Springer2023</p>
<p>Efficient multiobject detection and smart navigation using artificial intelligence for visually impaired people. Chandra Rakesh, Saumya Joshi, Malay Yadav, Carlos M Travieso- Kishore Dutta, Gonzalez, Entropy. 2299412020</p>
<p>Mobile assistive technologies for the visually impaired. Lilit Hakobyan, Jo Lumsden, O' Dympna, Hannah Sullivan, Bartlett, Survey of ophthalmology. 5862013</p>
<p>Object recognition in a mobile phone application for visually impaired users. Karol Matusiak, Piotr Skulimowski, Strurniłło, 2013 6th International Conference on Human System Interactions (HSI). IEEE2013</p>
<p>Improved visual-semantic alignment for zero-shot object detection. Shafin Rahman, Salman Khan, Nick Barnes, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Wearable smart system for visually impaired people. Ali Jasim, Ramadhan , sensors. 1838432018</p>
<p>The user as a sensor: navigating users with visual impairments in indoor spaces using landmarks. Navid Fallah, Ilias Apostolopoulos, Kostas Bekris, Eelke Folmer, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing Systems2012</p>
<p>A dataset for the recognition of obstacles on blind sidewalk. Universal Access in the Information Society. Wu Tang, De-Er Liu, Xiaoli Zhao, Zenghui Chen, Chen Zhao, 202322</p>
<p>Incremental learning of 3d-dct compact representations for robust visual tracking. Xi Li, Anthony Dick, Chunhua Shen, IEEE transactions on pattern analysis and machine intelligence. 201235Anton Van Den Hengel, and Hanzi Wang</p>
<p>Abnormal crowd behavior detection using social force model. Ramin Mehran, Alexis Oyama, Mubarak Shah, 2009 IEEE conference on computer vision and pattern recognition. IEEE2009</p>
<p>'statue', 'monument', 'picnic table', 'outdoor advertisement', 'vendor cart', 'food truck', 'emergency exit', 'no smoking sign', 'wet floor sign', 'closed sign', 'open sign', 'entrance sign', 'exit sign', 'stairs sign', 'escalator sign', 'elevator sign', 'restroom sign', 'braille sign', 'audio signal device', 'tactile paving', 'detectable warning surface', 'guide rail', 'handrail', 'turnstile', 'gate', 'security checkpoint', 'water dispenser', 'vending machine. Dong-In Kim, Jangwon Lee, IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. public Wi-Fi hotspot. 2022Anomaly detection for visually impaired people using a 360 degree wearable camera. first aid station', 'defibrillator', 'tree', 'pole', 'lamp post', 'staff', 'road hazard'</p>
<p>traffic signs and signals', 'roadway features', 'surface conditions', 'street furniture', 'construction areas', 'vegetation', 'animals', 'public amenities', 'navigation aids', 'temporary obstacles. Walking General, transportation hubs. safety features'</p>
<p>loose gravel', 'ice patch', 'snow pile', 'leaf pile', 'standing water', 'mud', 'sand', 'street sign', 'directional sign', 'information sign', 'parking meter', 'mailbox', 'bicycle rack', 'outdoor seating', 'planter box', 'bollard', 'guardrail', 'traffic cone', 'traffic barrel', 'pedestrian signal', 'crowd', 'animal', 'dog', 'bird', 'cat', 'public restroom', 'fountain', 'statue', 'monument', 'picnic table', 'outdoor advertisement', 'vendor cart', 'food truck', 'emergency exit', 'no smoking sign', 'wet floor sign', 'closed sign', 'open sign', 'entrance sign', 'exit sign', 'stairs sign', 'escalator sign', 'elevator sign', 'restroom sign', 'braille sign', 'audio signal device', 'tactile paving', 'detectable warning surface', 'guide rail', 'handrail', 'turnstile', 'gate', 'security checkpoint', 'water dispenser', 'vending machine', 'ATM', 'kiosk', 'public telephone', 'emergency phone', 'charging station', 'first aid station', 'defibrillator', 'oil spill', 'road debris', 'branches', 'water' 'low-hanging signage', 'road signs', 'roadworks', 'excavation sites', 'utility works', 'fallen objects. Urban Walking, Hazards , stop sign. street fair', 'scaffolding', 'electrical hazards', 'wire tangle', 'manhole covers', 'street elements', 'road hazards', 'toxic spill', 'biohazard materials', 'wildlife crossings', 'stray animals', 'pets', 'flying debris', 'air pollution','smoke plumes', 'dust storms. road crack'] Walking test. traffic signal. street sign. trash receptacle. street furniture', 'tree', 'construction site. road obstruction', 'loose materials. slick surface. water feature', 'monument', 'information point', 'access point', 'safety equipment. transport hub', 'obstacle crowd' ] Annotation mask = 'people', 'human face', 'car license plate', 'license plate</p>            </div>
        </div>

    </div>
</body>
</html>
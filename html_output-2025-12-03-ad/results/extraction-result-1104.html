<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1104 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1104</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1104</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-272262360</p>
                <p><strong>Paper Title:</strong> Planning with tensor networks based on active inference</p>
                <p><strong>Paper Abstract:</strong> Tensor networks (TNs) have seen an increase in applications in recent years. While they were originally developed to model many-body quantum systems, their usage has expanded into the field of machine learning. This work adds to the growing range of applications by focusing on planning by combining the generative modeling capabilities of matrix product states and the action selection algorithm provided by active inference. Their ability to deal with the curse of dimensionality, to represent probability distributions, and to dynamically discover hidden variables make matrix product states specifically an interesting choice to use as the generative model in active inference, which relies on ‘beliefs’ about hidden states within an environment. We evaluate our method on the T-maze and Frozen Lake environments, and show that the TN-based agent acts Bayes optimally as expected under active inference.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1104.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1104.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPS-AI (TN Active Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matrix Product State based Active Inference Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI planning agent that uses a matrix product state (MPS) tensor-network as a generative model and selects actions via active inference / surprisal minimization with a sophisticated inference tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPS-based active inference agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generative model: Matrix Product State (MPS) parameterizing joint probabilities of action-observation sequences with one-hot feature maps; Action selection: active inference style planning by minimizing expected surprisal J(π) (computed from the MPS) using a sophisticated inference tree (aggregation + pruning). Training: batched stochastic gradient descent with alternating single-site and occasional two-site updates; two-site updates perform SVD on merged tensors allowing adaptive (data-driven) bond-dimension changes controlled by a cutoff ε.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information-driven active inference / expected surprisal minimization with sophisticated inference (tree search) and model-structure adaptation via two-site SVD with cutoff</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Two complementary adaptation modes: (1) Adaptive action selection — the agent computes expected surprisal for future action/observation branches using the trained MPS and selects actions with lowest expected surprisal via sophisticated inference; branches are aggregated and pruned early if they show high expected surprisal or low transition probability. (2) Adaptive model complexity — during training two-site SVD updates (with singular-value cutoff ε) allow bond dimensions to grow or shrink in response to information in the dataset; to stabilize training they alternate single-site gradient updates (no SVD) with occasional two-site SVD-based updates so bond dimensions only change intermittently.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>T-maze and Frozen Lake (OpenAI Gym variants; custom 3x3 and default 4x4 Frozen Lake)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete action and observation spaces; partially observable (T-maze: ambiguous context until visiting cue); stochastic transitions in slippery Frozen Lake (slippage gives probabilistic transitions ~1/3), trapped terminal states (holes or branch traps), sparse binary reward signal (win/loss), finite-horizon experiments (MPS length caps planning horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>T-maze: four location positions, reward modality (3 values), context (2 values) → observation vector dimensionality 24; action space size 4; horizon truncated to 2 for experiments. Frozen Lake 3x3: 9 position observations, actions 4; Frozen Lake 4x4: 16 positions, actions 4; slippery variants introduce stochastic transitions increasing branching factor; MPS length varied (e.g., 5–12) to set planning horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>T-maze: models achieved up to 100% success rate (engineered dataset, small SVD cutoff ε ≤ 0.01); random action baseline would be 0.5. Smaller cutoffs (retain more singular values) typically increased success and lowered negative log-likelihood; single-site updates (with occasional two-site updates) markedly improved success rates for larger cutoffs. Frozen Lake: non-slippery 3×3 achieved 100% success across hyperparameters; non-slippery 4×4 mostly achieved 100% (some failures at ε=0.1); slippery 3×3 success increases with MPS length and matched the success of optimal truncated dynamic-programming policies; slippery 4×4 (reported single model, ε=0.03, length 12) produced near-optimal policies with minor deviations due to finite horizon and preference weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline random or naive policies: T-maze random-branching baseline success = 0.5 (not trained). The paper compares to optimal dynamic-programming policies for Frozen Lake (truncated to MPS horizon); the MPS-based surprisal agent matched those truncated-optimal success rates in slippery 3×3. No explicit RL-baseline learning curves reported for non-adaptive model variants beyond differing hyperparameter runs (but two-site-only training produced unstable/poorer behavior due to exploding bond dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>T-maze training used dataset sizes: 100, 204 (engineered complete set), 1000, 10,000 sample trajectories. All models trained for 50,000 sweeps with batch size 100 and learning rate 1e-3 (translating to 50k, 25k, 5k, 500 epochs respectively depending on dataset size). Engineered dataset (204 sequences) plus small ε produced 100% success (very sample-efficient when dataset is engineered). Frozen Lake used large random dataset (100,096 samples) to ensure sufficient goal-reaching examples; success depended on dataset size and MPS length. Overall, the method requires sufficiently large/representative datasets for stochastic, long-horizon tasks (authors note need for very large datasets for slippery 4×4).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Theoretical active inference balances exploration/exploitation via expected variational free energy including an information-gain term. In this implementation the agent minimizes expected surprisal J(π) (no explicit information-gain term during training), so online exploration is limited to what the sophisticated inference tree provides at decision time (expected surprisal aggregation and pruning). The authors note that true active inference (with free energy and information gain) intrinsically drives exploration; here, exploration at data-collection time was typically random or engineered (not learned online), and they discuss that an online/learned preferred distribution or including information gain would improve exploration-exploitation tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Comparisons were made against: (a) random data / random action baseline (implicit), (b) optimal policies from dynamic programming (for Frozen Lake, truncated to MPS horizon) used as reference lines, (c) multiple ablations/hyperparameter settings (varying singular-value cutoff ε, dataset size, MPS length, single-site vs two-site update schedules). No neural-network baselines were trained in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) An MPS can serve as a tractable generative model for planning under active inference: marginal and conditional probabilities are efficiently available so the agent can compute expected surprisal for future policies. 2) Adaptive model complexity via two-site SVD (controlled by cutoff ε) lets the MPS grow/shrink bond dimensions to match information requirements; smaller ε retains more singular values → more parameters → better fit but risk of overfitting on small datasets. 3) Alternating single-site gradient updates with occasional two-site SVD updates stabilizes training and prevents runaway bond-dimension explosion, improving final performance and reducing memory/time cost. 4) Sophisticated inference (aggregated expected surprisal with tree pruning) enables efficient action selection compared to brute force enumeration; pruning based on high expected surprisal or low transition probability reduces computation. 5) On discrete partially observable and stochastic benchmarks (T-maze, Frozen Lake), the MPS-based surprisal agent achieved Bayes-optimal or near-optimal performance (100% on many deterministic or short-horizon tasks; matched truncated optimal policies on stochastic small grids).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Computational bottlenecks: SVD at bonds during two-site updates is expensive and can explode bond dimensions (memory and time issues), especially for slippery/stochastic and larger environments. 2) Finite planning horizon (MPS length) limits performance in environments with effectively infinite horizons or long required planning horizons; finite horizon causes some suboptimal actions near horizon end. 3) Removing the information-gain term (minimizing surprisal instead of full expected free energy) reduces intrinsic exploration during training/data-collection; the method often relied on large datasets or engineered datasets to learn dynamics. 4) Two-site-only SGD updates lead to instability and rapidly increasing bond dimensions; single-site updates + occasional two-site updates are necessary. 5) Scaling to large action/observation spaces or long-depth trees remains challenging; authors suggest uniform/infinite MPS and other algorithmic optimizations as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with tensor networks based on active inference', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised generative modeling using matrix product states <em>(Rating: 2)</em></li>
                <li>Probabilistic modeling with matrix product states <em>(Rating: 2)</em></li>
                <li>A tensor network approach to finite markov decision processes <em>(Rating: 2)</em></li>
                <li>Active Inference: The Free Energy Principle in Mind, Brain and Behavior <em>(Rating: 2)</em></li>
                <li>Tensor networks for probabilistic sequence modeling <em>(Rating: 2)</em></li>
                <li>pymdp: a python library for active inference in discrete state spaces <em>(Rating: 1)</em></li>
                <li>Control flow in active inference systems-part II: tensor networks as general models of control flow <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1104",
    "paper_id": "paper-272262360",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "MPS-AI (TN Active Inference)",
            "name_full": "Matrix Product State based Active Inference Agent",
            "brief_description": "An AI planning agent that uses a matrix product state (MPS) tensor-network as a generative model and selects actions via active inference / surprisal minimization with a sophisticated inference tree search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MPS-based active inference agent",
            "agent_description": "Generative model: Matrix Product State (MPS) parameterizing joint probabilities of action-observation sequences with one-hot feature maps; Action selection: active inference style planning by minimizing expected surprisal J(π) (computed from the MPS) using a sophisticated inference tree (aggregation + pruning). Training: batched stochastic gradient descent with alternating single-site and occasional two-site updates; two-site updates perform SVD on merged tensors allowing adaptive (data-driven) bond-dimension changes controlled by a cutoff ε.",
            "adaptive_design_method": "information-driven active inference / expected surprisal minimization with sophisticated inference (tree search) and model-structure adaptation via two-site SVD with cutoff",
            "adaptation_strategy_description": "Two complementary adaptation modes: (1) Adaptive action selection — the agent computes expected surprisal for future action/observation branches using the trained MPS and selects actions with lowest expected surprisal via sophisticated inference; branches are aggregated and pruned early if they show high expected surprisal or low transition probability. (2) Adaptive model complexity — during training two-site SVD updates (with singular-value cutoff ε) allow bond dimensions to grow or shrink in response to information in the dataset; to stabilize training they alternate single-site gradient updates (no SVD) with occasional two-site SVD-based updates so bond dimensions only change intermittently.",
            "environment_name": "T-maze and Frozen Lake (OpenAI Gym variants; custom 3x3 and default 4x4 Frozen Lake)",
            "environment_characteristics": "Discrete action and observation spaces; partially observable (T-maze: ambiguous context until visiting cue); stochastic transitions in slippery Frozen Lake (slippage gives probabilistic transitions ~1/3), trapped terminal states (holes or branch traps), sparse binary reward signal (win/loss), finite-horizon experiments (MPS length caps planning horizon).",
            "environment_complexity": "T-maze: four location positions, reward modality (3 values), context (2 values) → observation vector dimensionality 24; action space size 4; horizon truncated to 2 for experiments. Frozen Lake 3x3: 9 position observations, actions 4; Frozen Lake 4x4: 16 positions, actions 4; slippery variants introduce stochastic transitions increasing branching factor; MPS length varied (e.g., 5–12) to set planning horizon.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "T-maze: models achieved up to 100% success rate (engineered dataset, small SVD cutoff ε ≤ 0.01); random action baseline would be 0.5. Smaller cutoffs (retain more singular values) typically increased success and lowered negative log-likelihood; single-site updates (with occasional two-site updates) markedly improved success rates for larger cutoffs. Frozen Lake: non-slippery 3×3 achieved 100% success across hyperparameters; non-slippery 4×4 mostly achieved 100% (some failures at ε=0.1); slippery 3×3 success increases with MPS length and matched the success of optimal truncated dynamic-programming policies; slippery 4×4 (reported single model, ε=0.03, length 12) produced near-optimal policies with minor deviations due to finite horizon and preference weighting.",
            "performance_without_adaptation": "Baseline random or naive policies: T-maze random-branching baseline success = 0.5 (not trained). The paper compares to optimal dynamic-programming policies for Frozen Lake (truncated to MPS horizon); the MPS-based surprisal agent matched those truncated-optimal success rates in slippery 3×3. No explicit RL-baseline learning curves reported for non-adaptive model variants beyond differing hyperparameter runs (but two-site-only training produced unstable/poorer behavior due to exploding bond dimensions).",
            "sample_efficiency": "T-maze training used dataset sizes: 100, 204 (engineered complete set), 1000, 10,000 sample trajectories. All models trained for 50,000 sweeps with batch size 100 and learning rate 1e-3 (translating to 50k, 25k, 5k, 500 epochs respectively depending on dataset size). Engineered dataset (204 sequences) plus small ε produced 100% success (very sample-efficient when dataset is engineered). Frozen Lake used large random dataset (100,096 samples) to ensure sufficient goal-reaching examples; success depended on dataset size and MPS length. Overall, the method requires sufficiently large/representative datasets for stochastic, long-horizon tasks (authors note need for very large datasets for slippery 4×4).",
            "exploration_exploitation_tradeoff": "Theoretical active inference balances exploration/exploitation via expected variational free energy including an information-gain term. In this implementation the agent minimizes expected surprisal J(π) (no explicit information-gain term during training), so online exploration is limited to what the sophisticated inference tree provides at decision time (expected surprisal aggregation and pruning). The authors note that true active inference (with free energy and information gain) intrinsically drives exploration; here, exploration at data-collection time was typically random or engineered (not learned online), and they discuss that an online/learned preferred distribution or including information gain would improve exploration-exploitation tradeoff.",
            "comparison_methods": "Comparisons were made against: (a) random data / random action baseline (implicit), (b) optimal policies from dynamic programming (for Frozen Lake, truncated to MPS horizon) used as reference lines, (c) multiple ablations/hyperparameter settings (varying singular-value cutoff ε, dataset size, MPS length, single-site vs two-site update schedules). No neural-network baselines were trained in this study.",
            "key_results": "1) An MPS can serve as a tractable generative model for planning under active inference: marginal and conditional probabilities are efficiently available so the agent can compute expected surprisal for future policies. 2) Adaptive model complexity via two-site SVD (controlled by cutoff ε) lets the MPS grow/shrink bond dimensions to match information requirements; smaller ε retains more singular values → more parameters → better fit but risk of overfitting on small datasets. 3) Alternating single-site gradient updates with occasional two-site SVD updates stabilizes training and prevents runaway bond-dimension explosion, improving final performance and reducing memory/time cost. 4) Sophisticated inference (aggregated expected surprisal with tree pruning) enables efficient action selection compared to brute force enumeration; pruning based on high expected surprisal or low transition probability reduces computation. 5) On discrete partially observable and stochastic benchmarks (T-maze, Frozen Lake), the MPS-based surprisal agent achieved Bayes-optimal or near-optimal performance (100% on many deterministic or short-horizon tasks; matched truncated optimal policies on stochastic small grids).",
            "limitations_or_failures": "1) Computational bottlenecks: SVD at bonds during two-site updates is expensive and can explode bond dimensions (memory and time issues), especially for slippery/stochastic and larger environments. 2) Finite planning horizon (MPS length) limits performance in environments with effectively infinite horizons or long required planning horizons; finite horizon causes some suboptimal actions near horizon end. 3) Removing the information-gain term (minimizing surprisal instead of full expected free energy) reduces intrinsic exploration during training/data-collection; the method often relied on large datasets or engineered datasets to learn dynamics. 4) Two-site-only SGD updates lead to instability and rapidly increasing bond dimensions; single-site updates + occasional two-site updates are necessary. 5) Scaling to large action/observation spaces or long-depth trees remains challenging; authors suggest uniform/infinite MPS and other algorithmic optimizations as future work.",
            "uuid": "e1104.0",
            "source_info": {
                "paper_title": "Planning with tensor networks based on active inference",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised generative modeling using matrix product states",
            "rating": 2,
            "sanitized_title": "unsupervised_generative_modeling_using_matrix_product_states"
        },
        {
            "paper_title": "Probabilistic modeling with matrix product states",
            "rating": 2,
            "sanitized_title": "probabilistic_modeling_with_matrix_product_states"
        },
        {
            "paper_title": "A tensor network approach to finite markov decision processes",
            "rating": 2,
            "sanitized_title": "a_tensor_network_approach_to_finite_markov_decision_processes"
        },
        {
            "paper_title": "Active Inference: The Free Energy Principle in Mind, Brain and Behavior",
            "rating": 2,
            "sanitized_title": "active_inference_the_free_energy_principle_in_mind_brain_and_behavior"
        },
        {
            "paper_title": "Tensor networks for probabilistic sequence modeling",
            "rating": 2,
            "sanitized_title": "tensor_networks_for_probabilistic_sequence_modeling"
        },
        {
            "paper_title": "pymdp: a python library for active inference in discrete state spaces",
            "rating": 1,
            "sanitized_title": "pymdp_a_python_library_for_active_inference_in_discrete_state_spaces"
        },
        {
            "paper_title": "Control flow in active inference systems-part II: tensor networks as general models of control flow",
            "rating": 2,
            "sanitized_title": "control_flow_in_active_inference_systemspart_ii_tensor_networks_as_general_models_of_control_flow"
        }
    ],
    "cost": 0.012816749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Planning with tensor networks based on active inference
10 October 2024</p>
<p>Samuel T Wauthier samuel.wauthier@ugent.be 0000-0002-1967-2195
Tim Verbelen 0000-0002-1967-2195
Bart Dhoedt 
Bram Vanhecke </p>
<p>1 IDLab, Department of Information Technology at Ghent University-imec,
Technologiepark-Zwijnaarde 126, B-9052 Ghent, Belgium Los Angeles, CA 90016, United States of America Boltzmanngasse 5, Vienna 1090, Austria</p>
<p>2 VERSES AI Research Lab,</p>
<p>3 Faculty of Physics and Faculty of Mathematics, Quantum Optics, Quantum Nanophysics and Quantum Information, University of Vienna,</p>
<p>Planning with tensor networks based on active inference
10 October 2024ACE607DE9FCA2721EAF86DBB428DA17210.1088/2632-2153/ad7571active inferencegenerative modelingplanningtensor networksmatrix product state
Tensor networks (TNs) have seen an increase in applications in recent years.While they were originally developed to model many-body quantum systems, their usage has expanded into the field of machine learning.This work adds to the growing range of applications by focusing on planning by combining the generative modeling capabilities of matrix product states and the action selection algorithm provided by active inference.Their ability to deal with the curse of dimensionality, to represent probability distributions, and to dynamically discover hidden variables make matrix product states specifically an interesting choice to use as the generative model in active inference, which relies on 'beliefs' about hidden states within an environment.We evaluate our method on the T-maze and Frozen Lake environments, and show that the TN-based agent acts Bayes optimally as expected under active inference.</p>
<p>Introduction</p>
<p>Tensor networks (TNs) were originally developed to represent quantum states in many-body quantum physics [10,14,50,52,71], and more recently have found their way into computer science [53] and machine learning (ML) [73].As a result, the range of applications in the field of ML has been steadily growing.A comprehensive list of past research into ML with TN was compiled by Wang et al [73].For example, Stoudenmire and Schwab [65] and Cheng et al [9] trained TNs in a supervised manner to perform image classification on the Modified National Institute of Standards and Technology (MNIST) handwritten digits data set [40].Furthermore, different types of TNs have been used for the generative modeling of the MNIST data set, such as matrix product states (MPS), a.k.a.tensor train (TT), [29,66], tree TNs (TTN) [8] and projected entangled pair states (PEPS) [72].</p>
<p>More related to the current work, Guo et al [27] investigated the use of matrix product operators (MPOs) for probabilistic modeling, while Stokes and Terilla [64] investigated the use of MPS for the same purpose.Similarly, Glasser et al [25] investigated how graphical models can map to generalized TNs and demonstrated this through the use of entangled plaquette states (EPS) and string-bond states (SBS) on image and audio classification tasks.Mencia Uranga and Lamacraft [47] used a continuous MPS for the generative modeling of raw audio.Meanwhile, Miller et al [49] used a uniform MPS for the generative modeling of text data.Further, Hur et al [35] proposed an approach to generative modeling using MPS through tensor sketching, while Peng et al [55] extended this approach to hierarchical TNs.</p>
<p>In reinforcement learning (RL), Liu and Fang [44] used MPS for variational Q-learning.Gillman et al [22] and Howard [34] demonstrated similar TN approaches to policy optimization for finite Markov decision processes (FMDP), where the latter also showed its value for multi-agent RL.Similarly, Mahajan et al [46] provided a tensorized formulation of the Bellman equation in multi-agent RL.Metz and Bukov [48] used MPS for the control of many-body quantum systems based on RL, much like Lu and Ran [45].Further, Gillman et al [23] provided a method to combine TNs with actor-critic techniques.Finally, although this work is situated largely within AI planning, it differentiates itself from earlier work through probabilistic modeling of action-observation sequences with action selection based on active inference, as opposed to learning some RL policy.</p>
<p>The use of TNs in ML may be interesting for several reasons.Foremost, TNs were developed to deal with the curse of dimensionality in many-body quantum systems, where the quantum state is given by a tensor with as many indices as there are particles.The number of degrees of freedom thus grows exponentially with the number of particles.TNs take on this problem by restricting the exponentially large space to a tiny manifold of states satisfying an 'entanglement area-law' .This can be understood as a locality of the entanglement in the system, where a 'typical' state would have volume-law entanglement [31] and there would be no sense of locality.Specifically for MPS, it has been proven that they describe extremal eigenvectors of local gapped Hermitian operators in one dimension [2-4, 30, 60] and they have also been shown to efficiently capture low energy physics [28,68,78].Moreover, it has been shown that local correlations, i.e. exponential decay, imply such an area-law [5] in any dimension, and hence permit an efficient TN description in that case.Such states could also be useful to ML, as relevant data points are often contained within a smaller subspace of the configuration space where the type of correlations is restricted.For example, in computer vision, images composed of randomly generated pixels are generally not useful to a classification or segmentation algorithm.Instead one would expect nearby pixels to be highly correlated, and far away pixels to be weakly correlated.Similarly, in planning, only physically possible sequences are relevant, and in particular events from the distant past will typically have far less effect on planning the future than the recent past.</p>
<p>Other advantages are that TNs offer a distributed representation of the data set and only involve linear operations.Due to this, computations can easily be performed in parallel.Furthermore, there is a lot of available research on expressivity [26,37] and interpretability [1,57,67] of TNs, leading to a relatively good understanding compared to neural networks.In fact, often TNs are used to characterize neural networks [7,11,24,37,61].We also note that MPS can be used to study Markov decision problems [63].</p>
<p>Despite the theoretical advantages, the use of TNs in ML is relatively new and many of the algorithms still suffer from teething problems.As addressed by Liu et al [43], TN-based models generally do not achieve the state-of-the-art performance of neural network-based models in practice.Much of the research done on ML using TNs can be regarded as proof-of-concept work and is verified using simple benchmarks, such as MNIST for classification.</p>
<p>Nevertheless, since TNs have already been shown to be able to model sequences in the form of text [49], they may also prove useful in the field of AI planning where sequences consist of actions and observations [74].As shown in this work, combining TNs and active inference leads to a straightforward algorithm for planning.</p>
<p>Active inference is a theory of behavior and learning in neuroscience, where the autonomous agent is presumed to learn a generative model of the world and select actions by minimizing a quantity called the variational free energy [20].The agent's beliefs about the environment in which it is located are encoded within this generative model in the form of probabilities over actions and observations.The agent's next action is the action which is expected to yield the lowest variational free energy in the future.Parr et al [54] provide an excellent in-depth exposition on active inference and its formulations.</p>
<p>TNs form a natural candidate to be used as the generative model in active inference for several reasons.Firstly, a similar argument regarding the earlier mentioned fact that TNs can also address the curse of dimensionality in ML can be made in favor of active inference.The aforementioned local correlations are comparable with the action-observation sequences allowed by the environment where one would expect only weaker correlations between actions and observations separated far in time.In active inference, action-observation sequences occupy only a fraction of the total action-observation space and the strength of correlation typically depends on temporal separation.Secondly, TNs were developed to represent quantum states, which represent the knowledge of a quantum system, where measurements are fundamentally probabilistic.Comparably, an active inference agent's internal model of the world contains the agent's knowledge about its environment in the form of beliefs, generally represented by probability distributions.Thirdly, the number of parameters in a TN can be dynamically determined during training based on information requirements.This ability is akin to dynamically discovering latent variables and is especially valuable for autonomous agents when computational resources are limited.It can be compared to the minimization of statistical complexity through the pruning of connections which occurs during sleep in biological agents [21,33,41].Lastly, it has been proposed that cells require quantum coherence for information processing [17,59], with recent empirical evidence suggesting the occurrence of entanglement in the brain mediated by brain function [36].This implies that control models in biological systems, i.e. the models in charge of planning, must support the possibility of quantum computation [15].TNs naturally provide the ability to account for this.More so, it has been shown that control models in active inference can always be represented as a TN in certain systems [15,16].</p>
<p>Although TNs have already been shown to be able to model sequences, generating sequences and planning optimal paths are two very different tasks.While a TN might be well able to generate paths that are consistent with the environment, there is no guarantee that any of these paths is optimal.Active inference guarantees that the path is optimal within the agent's belief system [54].</p>
<p>This paper contributes to the growing body of work in ML using TNs.In particular, we provide the first account of AI planning using TNs by making use of the ability of MPS to model sequences and the action selection algorithm provided by active inference.From a ML perspective, this constitutes a model-based approach to learning and planning.Inversely, from an active inference perspective, this corresponds to modeling the agent's generative model through a TN.In addition, we introduce an update scheme which alternates between single-site and two-site updates, and allows dynamic variation of bond dimensions, while avoiding exploding bond dimensions as well as speeding up computation.</p>
<p>In section 2, we introduce the basic concepts that constitute this work.These include MPS (section 2.1), generative modeling with TNs (section 2.2), and active inference (section 2.3).In section 3, we present our methods.We demonstrate the results of the experiments in section 4 and provide a discussion on scalability, and data and exploration in section 5. We conclude this work with section 6 by summarizing our findings and presenting possible paths for future work.</p>
<p>Background</p>
<p>MPS</p>
<p>At its core, a TN is a contraction of individual tensors, typically shown as a graph, i.e. a 'network' , where the nodes represent tensors and the edges represent summation over an index.Such a network can have many architectures.Popular types of networks include MPS [10,14,38,39], TTN [52,62], which are TNs without loops in their graphical representation and can therefore be treated with similar ease as MPS, and PEPS [10,52,70], which are TNs designed for two dimensional quantum systems and hence their graphical representation is also manifestly two dimensional (figure 1).In particular, the loops in the PEPS make them significantly more challenging to work with.</p>
<p>Arguably the simplest form of TN is the MPS, also known as TT [56].In simple terms, an MPS is the result of decomposing a tensor with N indices into N tensors of smaller order.For example, the tensor T abcd with four indices can be decomposed into four tensors:
T abcd = α1α2α3
A (1)  aα1 A</p>
<p>(2)
α1bα2 A (3) α2cα3 A (4) α3d ,(1)
or graphically:</p>
<p>The indices α i , known as bond indices, are internal to the network.The dimensionality of the bond indices, the bond dimension, must be large enough for the equality in equation (2) to hold.For an MPS to be able to exactly represent an arbitrary tensor with Nd-dimensional indices, the network's bonds must have dimension d N/2 [18] in the center.In practice, the bonds are truncated [13] so the above equality may be only approximately true.The bond dimensions are thus hyperparameters and can be chosen depending on how much entanglement there is in the state.Note that an MPS should only be used in cases where this approximation can be efficient and accurate, i.e. in case one may expect an area law entanglement [12].</p>
<p>Generative modeling with TNs</p>
<p>A generative model is a statistical model of the joint probability distribution P(X) of a set of random variables X = (X 1 , X 2 , . . ., X n ).Such a model makes it possible to generate outcomes x = (x 1 , x 2 , . . ., x n ) by sampling from the distribution.In addition, it allows the computation of marginal and conditional probabilities by performing the appropriate summations.</p>
<p>As stated earlier, quantum states can be thought of as representing the knowledge of a system.In other words, they represent the probability of observing a certain outcome (after performing a measurement on a system).Such a quantum state can be mathematically described through a wave function Ψ(x) on a set of real variables x = (x 1 , x 2 , . . ., x n ).From there, the probability of observing x is given by the Born rule: with normalization Z = {x} |Ψ(x)| 2 , where the summation runs over the set of all possible realizations of the values of x.The Born rule shows how a quantum state can serve as a generative model [42].
P (X = x) = |Ψ (x) | 2 Z ,(3)
When approximating a quantum state through an MPS, the wave function is parameterized as follows:
Ψ (x) = α1,...,αn−1,β1,...,βn T (1)
β1α1 T</p>
<p>(2)</p>
<p>α1β2α2 T</p>
<p>(3)
α2β3α3 • • • T (n) αn−1βn ϕ (1) β1 (x 1 ) ϕ (2) β2 (x 2 ) ϕ (3) β3 (x 3 ) • • • ϕ (n) βn (x n ) . (4)
Here, each T (i) is a tensor of order 3 (aside from the tensors at the extremities, which are of order 2), and each ϕ (i) is a feature map, which maps the input value x i to a higher-dimensional feature space.In graphical notation, this becomes</p>
<p>In brief, the wave function Ψ(x) is decomposed into a series of tensors T (i) and input vectors ϕ (i) .While the tensors T (i) can simply be regarded as arrays of parameters that can be learned during training, the feature maps ϕ (i) serve another important purpose.To build the parallel with quantum mechanics, a feature map ϕ (i) must ensure that each possible value of the input x i can be regarded as a distinct eigenvalue (of an observable) with its corresponding eigenvector.In other words, applying the feature map ϕ (i) to the set of possible values of the input x i must yield a set of linearly independent vectors.</p>
<p>To allow for a generative interpretation of the TN, the set of vectors obtained through feature maps must be orthogonal and each vector must be of unit norm [65].These conditions ensure that this set forms an orthonormal basis of the feature space.The dimensionality of this space is referred to as the local dimension.For example, a convenient choice of feature map is the one-hot encoding: if x i ∈ {0, 1, 2}, the encoding yields (1, 0, 0) for 0, (0, 1, 0) for 1, and (0, 0, 1) for 2. These vectors form the basis of a three-dimensional feature space.In general, for an input x i which can assume n discrete values, ϕ (i) will map each of these values onto an orthonormal basisvector, such that the feature space becomes n-dimensional.</p>
<p>Given a data set, the MPS can be trained through a DMRG-like [75,77] method by 'sweeping' back and forth across the MPS, updating tensors sequentially with respect to a given loss function.Since our TN will serve as a generative model, we minimize the negative log-likelihood (NLL), i.e. we maximize the model evidence directly [29].This way, the network will attempt to learn the underlying probability distribution of the data set during training.</p>
<p>One possible way of updating the tensor elements is performing single-site updates, i.e. updating a single tensor at a time.For example, starting from the left-most tensor, compute the gradient of the loss function and update the tensor elements.Then, move to the adjacent tensor and repeat.Once the right-most tensor is reached, do the same in the opposite direction.Continue sweeping back and forth until convergence.</p>
<p>Alternatively, Stoudenmire and Schwab [65] describe a two-site update scheme, in which two tensors are updated at the same time.Crucially, this allows the bond dimensions to vary during training depending on how many dimensions are required to encode the information in the data set.In other words, the number of parameters in the MPS can vary during training.In short, by merging and decomposing adjacent tensors through singular value decomposition (SVD), it is possible to increase or reduce the bond dimensions based on the number of singular values.The cutoff value ε determines the number of singular values that will be retained (see [65]).Singular values that are larger than ε times the largest singular value are retained, while the rest is discarded.As such, bond dimensions are generally initialized at a low number, so that the TN starts out with a low number of parameters and grows based on how many dimensions are needed.Bayesian network corresponding to the generative model P(õ,s, π) in discrete time active inference.The model is defined in terms of sequences of observations ot and hidden states st evolving over time, and a policy π(t) = at with at the selected action at time t.</p>
<p>As described by Han et al [29], it is possible to obtain different marginal and conditional probabilities using the above formulation.</p>
<p>Active inference</p>
<p>Active inference posits that each autonomous agent is equipped with an internal model of the world, a generative model P of how sensory observations are generated, which encodes beliefs about these sensory observations o t , hidden variables s t and the policy π it is following [54].Importantly, the policy returns an action as a function of time, a = π(t), in contrast with standard formulations where the policy is a function of the state, a = π(s) [20].As such, one can also write down the policy as a sequence of actions a t .</p>
<p>An active inference agent updates its internal model or selects actions in an attempt to minimize its surprisal, a.k.a.information content or self-information, [54]
I (õ) = − log P (õ) ,(6)with õ = (o 1 , o 2 , . . . , o τ )
, where we have used the ~-notation indicating a sequence over time up to some time τ .In discrete time active inference, the generative model is often compared to a partially observable Markov decision process (POMDP) [20].In that case, the generative model P(õ,s, π) factorizes as in figure 2,
P (õ,s, π) = P (π) P (s 0 ) P (o 0 |s 0 ) τ t=1 P (s t |s t−1 , π) P (o t |s t ) ,(7)
and the agent instead minimizes its variational free energy
F [Q,õ] = E Q(s,π) [log Q (s, π) − log P (õ,s, π)] (8) = D KL (Q (s, π) || P (s, π |õ)) divergence − log P (õ) surprisal ,(9)
where Q(s, π) is an approximation to the posterior distribution P(s, π |õ), not to be confused with the Q-function with similar notation often used in RL.The use of the approximate Q is motivated by the fact that the evidence P(õ), and therefore the posterior distribution P(s, π |õ) = P(õ |s,π)P(s,π)
P(õ)
, is often intractable due to the requirement to marginalize the generative model over s and π.Note that the variational free energy (equation ( 9)) is an upper bound on the surprisal (equation ( 6)) and becomes equal to the surprisal when the divergence becomes zero, i.e. when Q(s, π) = P(s, π |õ).</p>
<p>The active inference agent selects actions which are expected to minimize its free energy in the future.To this end, the agent maintains a prior P(õ | C) which specifies the conditions that it would like to experience, i.e. its preferred observations.Here, C indicates the agent's preferences.Action selection proceeds by computing the expected variational free energy for all possible policies [54]
G (π) = E Q(õ,s | π) [log Q (s | π) − log P (õ,s | π, C)] (10)≈ −E Q(õ | π) D KL Q (s |õ, π) || Q (s | π) information gain −E Q(õ | π) [log P (õ | C)] expected surprisal , (11)
where we have used
Q(õ,s | π) := P(õ |s)Q(s | π) and P(s |õ, π) ≈ Q(s |õ, π).
After which, the agent follows the policy with the lowest expected variational free energy.</p>
<p>Methods</p>
<p>Adapting the MPS for action selection</p>
<p>When employing an TN as a generative model in an active inference agent, the network represents the beliefs held by the agent.Roughly speaking, in active inference, the generative model is the joint distribution of sequences of actions a t and observations o t .As a result, each time step t is associated with a tensor T (t) .Furthermore, we may split the external leg of each tensor in equation ( 5) by redefining indices, such that at each tensor T (t) , there is a leg for an action feature map ϕ
(t)
a and an observation feature map ϕ
(t)
o .Moreover, since the action and observation spaces do not change over time, we have ϕ
(t)
x (x k ) = ϕ x (x k ) for all t and x ∈ {a, o}.</p>
<p>When simplifying the graphical notation by using the nodes to refer to ϕ x (x k ), the network becomes where the length n of the network depends on the environment.Since we define that action a t leads to observation o t+1 , action a 0 would be superfluous and tensor T (1) does not receive an action.</p>
<p>Updating the MPS</p>
<p>Both update methods mentioned in section 2.2 are viable and can be used interchangeably or simultaneously (by alternating between the two).However, all the models in this work were optimized using batched stochastic gradient descent (SGD).This was done to accommodate for future applications where the data set size is too large to fit into memory in its entirety.Note that this differs from DMRG, although the implementation largely overlaps.The SGD update will predictably give rise to a far slower and noisier convergence than the DMRG update (which involves an eigenvalue problem), and this must be accounted for.As a matter of fact, allowing the bond dimensions to perpetually change during two-site updates with SGD may be detrimental to training stability.Taking steps in random directions may cause the bond dimensions to change size with every step.In turn, if the bond dimensions are constantly changing size, the model may have difficulties generalizing.For example, imagine a freshly initialized model which has not yet learned.If the bond dimensions are allowed to vary from the start of training, the model is free to add new dimensions for new data without generalizing first.In a training setting with a high number of batches, this may lead to rapidly increasing bond dimensions.While in theory, over time the model will learn to generalize and fix itself, in practice, large bond dimensions may cause computational issues.In that case, it may be a good idea to try to generalize on-the-fly and add dimensions as the need arises.While it is possible to set an upper bound on the bond dimensions, for a general environment, it is difficult to anticipate the number of required dimensions, i.e. the number of required bond dimensions is highly dependent on the application.Therefore, it may also be difficult to set a nonrestrictive upper bound.Although this problem does not arise for the environments employed in this work, it should be taken into account for the general case.</p>
<p>A simple solution to this issue is to perform single-site updates and only perform two-site updates occasionally (e.g. when the current configuration has more or less converged).As a practical matter, we suggest to set the initial bond dimensions to a number close to where one would expect the bonds dimensions to end up, such that fewer bond dimension changes are required in the end.In this work, we set the initial bond dimensions to the observation dimension, since we expected the bonds to prioritize encoding information about the positions over the actions.</p>
<p>As an added benefit, the use of single-site updates reduces training time [76].Two-site updates require merging and decomposing adjacent tensors.This effectively means squaring the local physical dimension at one site.While the local dimension is typically small in quantum-many body settings (often 2 or 3 for qubits or qutrits, respectively), the complexity would only increase by a small prefactor compared to the one-site scheme.In the context of this paper it can be significantly bigger, leading to a large difference in computational complexity that can be most clearly perceived in the SVD of the merged tensors.For reference, SVD on a single tensor has a time complexity of O(dD 3 ), while SVD on the merged tensor has a time complexity of O(d 3 D 3 ), where d is the local dimension and D is the bond dimension.</p>
<p>Finally, the use of SGD also affects the possible choices for the cutoff ε on the singular values.An ε of order 1/ √ N with N the size of the dataset is generally advised, since this is also how the size of the random errors in the gradient and cost function scale.</p>
<p>Active inference with an MPS</p>
<p>Using an MPS, allows the agent to minimize surprisal (equation ( 6)) directly, rather than through variational Bayesian methods.This is because marginal probabilities become tractable when using MPS due to efficient contraction schemes.As a result, the agent will instead select actions which minimize its expected surprisal
J (π) = −E P(õ | π) [log P (õ | C)] . (13)
In practice, computing the expected surprisal for each possible policy is computationally expensive.Using the sophisticated inference [19] scheme may alleviate this problem to a certain extent.Since this scheme builds a tree over actions and observations in the future, it requires less computations than greedily computing the expected surprisal for every possible sequence.Moreover, branches can be pruned before they have been fully computed when they show signs of having high expected surprisal, or when the transition probability
P(o t+1 | o &lt;t+1 , a &lt;t+1 ) is low.
Sophisticated inference works by computing an aggregated expected surprisal for each possible action at time t, instead of the expected surprisal for each policy.Aggregation is achieved by summing the expected surprisal of subsequent actions and weighing each action by its expected surprisal as follows:
J (o t , a t ) = −E P(ot+1 |õ&lt;t+1,ã&lt;t+1) [log P (o t+1 | C)]
expected surprisal of next action
+ E P(at+1 | ot+1)P(ot+1 |õ&lt;t+1,ã&lt;t+1) [J (o t+1 , a t+1 )] expected surprisal of subsequent actions , (14)P (a t | o t ) = σ [−J (o t , a t )] . (15)
Details on how the relevant probabilities can be obtained from the trained MPS, can be found in appendix.</p>
<p>Experiments</p>
<p>The method described in the previous sections was tested on environments with discrete actions and observations.A simple, yet important environment for active inference is the T-maze, as presented by Friston et al [20].This environment tests whether an agent is able to resolve ambiguity in order to ensure obtaining a reward, as opposed to guessing and obtaining a reward only a fraction of the time.It is useful to demonstrate the workings of action selection through active inference.</p>
<p>A slightly more complex environment is the Frozen Lake from OpenAI Gym [6].This environment has a larger observation space and, theoretically, an infinite time horizon.We use it to investigate how our method scales to more complex environments.</p>
<p>T-maze</p>
<p>The T-maze environment consists of a T-shaped maze which contains an agent, e.g. a mouse, and a reward, e.g.some cheese (figure 3).The maze is made up of four locations: the center, the right branch, the left branch, and the cue location.The agent starts off in the center and must attempt to reach the reward, which is placed in either the left branch or the right branch, by moving between the different locations.If the agent enters either the left or the right branch, it is trapped and cannot leave.Unfortunately for the agent, the initial state of the world is uncertain.As a result, the agent initially does not know which type of world it is in: a world with the reward in the left branch, or a world with the reward in the right branch.In other words, it does not know its context.However, by going to the cue location, the agent can discover the location of the reward and observe its context.This resolves the ambiguity in the environment and allows the agent to select the branch which contains the reward.</p>
<p>The pymdp [32] package provides an implementation of such a T-maze.In this implementation, the environment provides an observation with three modalities at every time step: the location, the reward, and the context.The location indicates where the agent is currently located and can be one of four possible values: center, right, left, and cue.The reward can take on three possible values: no reward, win, and loss.The 'win' and 'loss' observations indicate that the agent either received the reward or failed to obtain the reward, respectively, while the 'no reward' observation indicates that the agent has not won or lost yet.As a result, 'no reward' can only be observed in the center and cue location, while 'win' and 'loss' can only be observed in the left and right branches.Furthermore, the context indicates the state of the world and can take on two possible values: left and right.Whenever the agent is located in the 'center' , 'left' or 'right' positions, the context observation will be randomly selected from 'left' or 'right' uniformly.The context observation will only yield the correct context with probability 1 when the agent is in the cue location.Lastly, the possible actions that the agent can take include: center, right, left, and cue, corresponding to which location the agent wants to go.</p>
<p>The above implementation was slightly modified to better reflect the environment designed by Friston et al [20].Firstly, the environment's horizon was set to 2, such that the agent is only allowed to perform two subsequent actions.In other words, the number of steps was limited to two.</p>
<p>Model</p>
<p>To accommodate the T-maze environment, the model (equation ( 12)) was limited in time as follows:</p>
<p>where the length of the network was equal to the initial time step plus the environment's horizon.Here, one-hot encodings were the feature maps of choice.This means the observation feature map o t has 4 × 3 × 2 = 24 dimensions, one for each possible combination of position, reward and context, and the action feature map a t has 4 dimensions.Note that this network only has two bonds which varied in dimensionality during training.</p>
<p>Hyperparameters</p>
<p>The model described above relies on a number of hyperparameters.Firstly, important factors that could affect the performance of the model were cutoff ε and the data set.</p>
<p>Cutoff</p>
<p>Eight different cutoffs ε for the singular values of the bonds were compared: 0.2, 0.1, 0.05, 0.04, 0.03, 0.02, 0.01 and 0.005.</p>
<p>Data set</p>
<p>The data sets used in this experiment consist of sample trajectories.Five different types of data set were compared:</p>
<p>• Four randomly generated data sets of size 100, 204, 1000 and 10 000, respectively, where each sequence consisted of random actions and observations.These data sets were generated anew for each model.Note that the data may contain duplicates.• An engineered data set which includes all possible sequences within the environment exactly once.This data set contains exactly 204 sequences.Denoted in the figures by '(204)' .</p>
<p>As a result of their respective sizes, the data sets of size 1000 and 10 000 will contain many duplicates.However, they are also more likely to capture the underlying probability distributions of the generative process.Importantly, each model was trained for a total of 50 000 sweeps with a batch size of 100 and a learning rate of 10 −3 .In practice, for data sets of sizes 100, 204, 1000 and 10 000, this translates to 50 000, 25 000, 5000 and 500 epochs, respectively.Finally, we evaluated different initialization strategies, more specifically initialization with complex numbers with positive real part, and initialization to unity with standard complex normal noise.While having some effect on convergence behavior at train time, no effect on results was seen.</p>
<p>Results</p>
<p>To evaluate the performance of each model, we measured the success rate over 100 runs using the action selection scheme presented in section 3.3.The preferred distribution P(o t | C) remained constant for every t and was factorized into preferred distributions over the three modalities: position, reward and context.Technically, the hyperparameter C can be freely chosen or learned [58].Since the choice of weights raises a research question in and of itself, we simply selected a set of weights which worked for our purposes.Here, the weights for win and loss were fixed to the values in [20]:
P modality | C modality = σ C modality , modality ∈ {position, reward, context}(17)C position = 0 0 0 0 , C reward = 0 3 −3 , C context = 0 0 , (18)
where σ is the softmax function, such that each possible value of each of the modalities has an associated weight.These distributions represent the agent's indifference towards its position and context, and its desire to obtain a reward (weighted by 3) and not fail (weighted by −3).In other words, the agent expects to be obtaining the reward e 3 ≈ 20 times more than to be obtaining a neutral outcome.During a run, a success (agent obtained the reward) was indicated by 1, while a failure (agent chose the wrong branch or no branch at all) was indicated by 0. Note that an agent which chooses a branch at random would obtain a success rate of 0.5.</p>
<p>Previous work [74] demonstrated belief shifts and action selection under sophisticated active inference (equation ( 11)) when training on the engineered data set with a cutoff of 0.1 using only two-site updates.In this section, we thoroughly investigate the performance for different hyperparameters (cf section 4.1.2,demonstrate the belief shifts and action selection under the surprisal minimization scheme in section 3.3 and investigate performance when using single-site updates.</p>
<p>Performance</p>
<p>We first show the performance when the model is trained with only two-site updates and without single-site updates.Figure 4(a) shows the average success rate obtained across 20 models.Error bars represent the between-model 95% confidence intervals computed through bootstrapping.The figure suggests that smaller cutoffs ε lead to higher success rates.Indeed, we expect that a model with a smaller cutoff retains more information.Further, small randomly generated data sets lead to lower success rates, even at smaller cutoffs.This is likely due to the small randomly generated data sets not containing enough samples to completely capture the dynamics of the environment.As a consequence, these models never learned useful dynamics that lead to the reward with certainty.Note that the (small) engineered data set follows the general trend of the larger data sets and did attain a success rate of 100% at smaller cutoffs.A trend which continues throughout the rest of the experiments.</p>
<p>A model's ability to capture the underlying distribution of the data can be evaluated through the likelihood.Data points which appear in the data set should have a high likelihood, while data points which are not in the data set should have a low likelihood.As such, a model's average likelihood over all possible data points (in this case, sequences) should be high, or equivalently, its negative log likelihood should be low.Figure 4(b) shows the average negative log likelihood evaluated over all possible sequences.The theoretical minimum for this data set is − log(1/204) ≈ 5.32, since every sequence is equally likely.In general, the negative log likelihood decreases with smaller cutoffs.Once again, the models perform worse for small data set sizes, since small randomly generated data sets are less likely to contain all possibles sequences.Furthermore, if the randomly generated data set is not large enough, smaller cutoffs can become detrimental.This is likely because the model starts to overfit as more parameters become available (through the smaller cutoff).Note that models trained on the engineered data set approach the theoretical minimum at smaller cutoffs.</p>
<p>Again, we expect that a model with a smaller cutoff retains more information.In this case, information comes in the form of parameters.Logically, decreasing the cutoff leads to more singular values being retained after SVD, which in turn leads to more parameters in the model.This effect is shown in figure 5(a).</p>
<p>Since the number of parameters in the model is proportional to the bond dimensions, we can see how the model size changes during training.Figure 5(b) shows how the number of bond dimensions evolved during training for a specific model with ε = 0.01 trained on the engineered data set.The bond dimensions first increased greatly from the initial value of 2 as the model learned all the information, after which the bond dimensions decreased again as the model learned to generalize better.Moreover, bond 0 was smaller than bond 1, which indicates that observation o 2 contained more information than observation o 1 .Indeed, observation o 1 was always the initial position, while observation o 2 could be any position.</p>
<p>Beliefs</p>
<p>When zooming in on a single well-performing model, one expects to see some shifts in the probability distributions provided by the TN before and after having seen the cue.In other words, one expects to see a belief shift in the active inference agent.More concretely, while the agent must be certain of its position both before and after having seen the cue, it must be uncertain of the reward it will receive before having seen the cue, but certain after having seen the cue. Figure 6 shows this belief shift for a model trained with ε = 0.01 trained on the engineered data set.The vertical axis indicates the next action, such that each row is a probability distribution over a certain modality of the next observation.In other words, each row indicates the agent's belief about the next observation given the next action.For example, before having seen the cue, the agent expects that, if it were to perform the action 'right' , it will almost certainly end up in the right branch, it will obtain a reward or a loss with approximately 50% chance each and it will observe the context right or left with 50% chance each.On the other hand, after having seen the cue 'right' , the agent expects that, if it were to perform the same action, it will almost certainly end up in the 'right' branch, but it will obtain the reward almost certainly and it will again observe the context right or left with 50% chance each.</p>
<p>While the results for the context observation in this example may seem counter-intuitive, keep in mind that the agent can only observe the context whenever it is in the cue location.The figure shows the agent's beliefs about the next observation and does not directly reflect the agent's beliefs about the state of the environment.Before having seen the cue, the agent believes it can observe either context with a 50% chance.When standing in the cue location, the agent knows which context it will observe if it remains in the cue location.When moving towards another location, the agent knows it will once again observe either context with a 50% chance, since only the cue position gives a coherent context observation.</p>
<p>The expected surprisal (equation ( 14)) can provide some insight on how the agent behaves given the preferences in equation ( 18) and how it reaches the reward.Figure 7(a) shows the expected surprisal per action for the first action.Since the expected surprisal for going to the cue is lowest, the agent will go to the cue location.Having observed the cue 'left' or 'right' , the expected surprisal per action for the action now takes the form of figures 7(b) and (c), respectively.In these cases, it is clear the agent will choose to go to the right and left branch, respectively.Indeed, this is the optimal sequence of actions which optimizes its reward, and the expected behavior for the agent.</p>
<p>Single-site updates</p>
<p>Figure 8(a) shows the success rates obtained when using single site updates.In this case, two-site updates were performed every 10th epoch.This way, the model received 9 epochs to converge and 1 epoch to adjust the bond dimensions.The initial bond dimension was set to the observation dimension, i.e. 24.These simple changes drastically improved success rates for higher cutoffs when compared with figure 4(a).</p>
<p>Figure 8(b) shows the evolution of the bond dimensions when using single-site updates.In this case, the bond dimensions remain relatively large for a short time until the model learns to generalize.Note that the bond dimensions were initialized at 24 this time, since it was known from the models using only two-site updates that the bond dimensions would quickly grow in the beginning.One can see from this figure that the model learned to generalize sooner than in the no single-site updates case (cf figure 5(b)).Additionally, the largest bond dimension was smaller (9 dimensions after 500 epochs) than in the no single-site updates case (17 dimensions after 500 epochs).Indeed, this suggests that the model has more difficulties generalizing when the bond dimensions are constantly allowed to vary.</p>
<p>Frozen Lake</p>
<p>The default Frozen Lake environment consists of a 4 × 4-grid which contains an agent e.g. an elf, a reward e.g. a present and some holes (figure 9(a)).The agent always starts in the top left corner and must reach the reward in the bottom right corner without falling into a hole.If it falls into a hole, it is trapped and cannot leave.The agent can move left, down, right or up one step at a time.However, the floor is slippery and, as a result, the agent has a chance to move perpendicular to the intended direction.For example, if the agent wants to move down, it has a 1  3 probability of moving down, a 1 3 probability of moving left and a 1 It is possible to design custom environments, such as the 3 × 3-grid in figure 9(b), where the rules are the same as for the default environment.Moreover, it is possible to make the floor non-slippery, such that the environment becomes deterministic.</p>
<p>Model</p>
<p>For the Frozen Lake, the model (equation ( 12)) was adapted to the environment in a similar way as was done for the T-maze.The minimum length of the network was set to equal the initial time step plus the minimum number of steps required to reach the goal (4 for the custom environment, 6 for the default).However, we also introduced a hyperparameter which controls the number of additional steps beyond that for reasons that will become clear in the next section.Once again, one-hot encodings were the feature maps of choice.This means the observation feature map o t has 9 dimensions for the 3 × 3-grid and 16 dimensions for the 4 × 4-grid, and the action feature map a t has 4 dimensions.</p>
<p>Hyperparameters</p>
<p>Since the T-maze experiments suggest that a larger randomly generated data set leads to better performance and it is not feasible to engineer a data set for the Frozen Lake environment, we fixed the data set size to 100 096 for this experiment.This number was chosen in order to make sure that a randomly generated data set on the 4 × 4 environment would contain enough samples in which the goal is reached.Moreover, this number is the first multiple of 256 larger than 10 5 .Each model was trained with a batch size of 256 and a learning rate of 10 −3 .</p>
<p>Important factors that could affect the performance of the model were cutoff ε and MPS length.The latter can affect performance because sequences in the data set are randomly generated.Longer sequences have a greater chance of making it to the reward position, which is important for the model to learn how to reach the reward.</p>
<p>Cutoff</p>
<p>Four different cutoffs ε were compared: 0.1, 0.05, 0.03, and 0.01.</p>
<p>Length</p>
<p>Different lengths of MPS were compared:</p>
<p>• For the non-slippery 3 × 3 environment: 5, 6, and 7.</p>
<p>• For the slippery 3 × 3 environment: 8, 10, and 12.</p>
<p>• For the non-slippery 4 × 4 environment: 7, 8, and 9.</p>
<p>• For the slippery 4 × 4 environment: 12.</p>
<p>Once again, we did not observe an effect of initialization strategy on results.</p>
<p>There were only a few exceptions to the hyperparameters described above.The learning rate was reduced to 5 × 10 −4 whenever training became too unstable.This was only the case for the models with a specific initialization, cutoff 0.1 and MPS length 9 on the non-slippery 4 × 4 environment.In addition, the data set size for the model trained on the slippery 4 × 4 environment was increased by approximately a factor of 10, to 1000 192 (another multiple of 256), such that it would contain more samples which reached the goal.</p>
<p>Results</p>
<p>To evaluate the performance of each model, we measured the success rate using the action selection scheme presented in section 3.3.Here too, the preferred distribution P(o t | C) remained constant for every t, such that for the 3 × 3 environment (cf figure 9(b))
P (o | C) = σ (C)(19)C =   0 0 0 0 −5 0 0 0 20  (20)
and for the 4 × 4 environment (cf figure 9(a))
P (o | C) = σ (C)(21)C =     0 0 0 0 0 −5 0 −5 0 0 0 −5 −5 0 0 20     (22)
where σ is the softmax function and C encodes the agent's preference for each position within the environment.Once again, since the choice of weights raises a research question in and of itself, we simply selected a set of weights which worked for our purposes.Here, the weights were adjusted compared to the T-maze experiments, because of the larger position space in the Frozen Lake compared to the reward space in the T-maze and the larger horizon in the Frozen Lake.The weights represents the agent's fear of falling in holes (weighted by −5) and desire to reach the reward (weighted by 20).During a run, a success (agent obtained the reward) was indicated by 1, while a failure (agent was unable to reach the reward) was indicated by 0. The single-site update method was used by default with two-site updates performed every 5 epochs.Using only two-site updates caused rapidly increasing bond dimensions.This was especially problematic on the slippery environments, e.g. the largest bond went from 2 to over 120 in a single epoch in the 4 × 4 environment.Training required either very long training times with small batches or very large memory (a batch of 256 two-site gradient tensors with a bond dimension of 64 on the 4 × 4 environment requires more than 68 GB).Therefore, results using only two-site updates will not be reported.</p>
<p>Non-slippery 3 × 3</p>
<p>The success rate for all models on the non-slippery 3 × 3 environment was 100% regardless of hyperparameters.Since the models were trained using single-site updates and the environment was very simple, even models with a large cutoff were able to achieve 100% success rate.</p>
<p>Non-slippery 4 × 4</p>
<p>Figure 10(a) shows the success rate for models with different hyperparameters on the non-slippery 4 × 4 environment.Error bars represent the 95% confidence intervals computed through bootstrapping.Once  Optimal policies for the slippery Frozen Lake environments obtained through dynamic programming [51].Policies are optimal in the sense that they provide the shortest path while having the lowest possible probability of falling into a hole.A run terminates when the agent reaches a hole or the reward, therefore actions in these positions have been omitted.Overlapping actions indicate both actions are equally viable.Reproduced with permission from [79].[© 2021 M. Tillery.All rights reserved.].again, since the models were trained using single-site updates and the environment was very simple, even models with larger cutoffs were able to achieve 100% success rate, although some models failed when the cutoff was 0.1.The success rates for models with cutoff 0.1 may suggest that a longer MPS length leads to higher success rates.However, too few data were gathered to confirm this.</p>
<p>Figure 10(b) shows the evolution of the bond dimensions for a model with cutoff 0.03 and MPS length 9.The model starts with all bond dimensions equal to the observation dimension, i.e. 16.After training for 4 epochs using single-site updates, the model performs one epoch of two-site updates, which results in reduced bond dimensions for some of the bonds.Interestingly, the bond dimensions increase in order of bond position within the MPS.This is expected, since the agent always starts in the same top left position within the environment and is able to reach more positions as time goes by. Figure 11 shows an example of how an agent can act within the non-slippery 4 × 4 Frozen Lake environment.The agent starts out in the top left corner as usual.The top row shows the evolution of the expected surprisal over time as the agent takes actions.The bottom row shows the environment at each time step.Ultimately, the agent reaches the reward by assuming the action with the lowest expected surprisal at every step.</p>
<p>Slippery 3 × 3</p>
<p>Evaluating the success rate of an agent with a finite horizon on an environment with a potentially infinite time horizon can be tricky.In this case, we compared the success rate obtained by the surprisal agent with the success rate obtained by an agent which follows the optimal policy displayed in figure 12(b) truncated to the length of the MPS.The latter is optimal in the sense that it has the highest probability of reaching the goal in a small amount of steps while having the lowest probability of falling into a hole.Figure 13(a) shows the success rate obtained for models with different hyperparameters on the slippery 3 × 3 environment.As expected, the success rate increases with the length of the MPS.A longer MPS allows the agent to perform  more steps in the environment.On the other hand, cutoff does not seem to have any effect on the success rate.Note that we do find different states with different bond dimensions for different values of the cutoff, the difference simply has no bearing on the success rate.The surprisal agents reach the success rate of the optimal agents.</p>
<p>Figure 13(b) shows the evolution of the bond dimensions for a model with cutoff 0.03 and MPS length 8. Similar to the previous case, the model starts with all bond dimensions equal to the observation dimension, i.e. 9. Once again, after training for 4 epochs using single-site updates, the model performs one epoch of two-site updates, which results in reduced bond dimensions for some of the bonds.Also here, the bond dimensions increase in order of bond position within the MPS. Figure 14 provides an example of the policy followed by one of the models with ε = 0.01 and MPS length 12.This policy was constructed by counting how often an action was taken at each position.Note that while this is very close to the optimal policy (figure 12(b)), there are a few slight discrepancies.These can be attributed to both the finite horizon of the model and the weight of the reward in the preferred distribution in combination with slight imbalances in the transition probabilities (as the probabilities are never exactly 1/3):</p>
<p>1.When close to the end of the model's finite horizon and if the goal cannot be reached anymore in the remaining number of steps, the model will continue performing the actions with the lowest expected surprisal.These actions, however, will not necessarily bring it closer to the goal.Instead, it will focus on fulfilling its other preferences, i.e. not falling into a hole.This means, depending on its current position, the agent will either try to avoid the hole or, when not in danger of falling into the hole, simply perform the action it perceives as having the lowest expected surprisal.The latter solely depends on the transition probabilities, however small the differences may be (again, the probabilities are never exactly 1/3), and can therefore appear quite arbitrary.This effect can be seen in the top right and bottom left corners.In these positions, the agent sometimes performs the 'down' and 'left' actions, respectively, for this reason.2. Since the positive weight of the reward heavily outweighs the negative weight of falling into a hole in this case, it may happen that the agent's 'desire' to reach the reward drowns out its 'fear' of falling into a hole.This can happen when close to the end of the model's finite horizon and if the goal can still be reached.In  addition, for this to happen, the imbalance in transition probabilities must be a bit larger, such that the agent believes that one action is more likely than the others to bring it to the goal.This happens in the bottom middle position.Here, the agent may in some cases perform the 'right' action, instead of the optimal 'down' .For reference, this can already happen when the transition probability of going to the goal from the bottom middle position with the 'right' action is 41%.Such a difference is generally no problem, except in edge cases like this one.In fact, under normal circumstances, the transition probabilities have to be much more imbalanced before the method breaks down.</p>
<p>Indeed, reducing the weight of the reward does not reduce the occurrence of the first situation, but does reduce the occurrence of the second situation.Learning the preferred distributions as in Sajid et al [58] may be helpful for this.Also, switching to a model with infinite horizon would do away with both situations.</p>
<p>Slippery 4 × 4</p>
<p>An increase in the size of the environment entails an increase in the depth of the decision tree defined by equation (14).While the 3 × 3 environment only requires a depth of 4, the 4 × 4 environment requires a depth of 6.Since the tree can be pruned during computation to reduce the complexity, the greater depth was no issue for the deterministic case.However, despite pruning, the size of the tree increased a lot more rapidly for the nondeterministic case, since the number of possible paths grew exponentially with the number of positions the agent can slip towards, which made evaluation of these models a lot more time consuming.For this reason, we only show results for a single model trained with ε = 0.03 and MPS length 12.</p>
<p>Figure 15 shows the evolution of the bond dimensions for this model.The model starts with all bond dimensions equal to the observation dimension, i.e. 16.After training for 4 epochs using single-site updates, the model performs one epoch of two-site updates, which results in reduced bond dimensions for the first two bonds.During the following epochs of two-site updates, both bond 1 and 2 increase slightly.Most of the bonds remain at 16 dimensions.It is possible that training longer (more epochs) would further increase some of the bond dimensions.However, the agent's desired behavior had already emerged at this point.</p>
<p>Figure 16 shows the policy the model followed.Once again, this policy was constructed by counting how often an action was taken at each position.And again, it is very close to the optimal policy (figure 12(a)) with a few slight discrepancies.These discrepancies can be attributed to the same situations described in the previous paragraph for the slippery 3 × 3 environment.The agent performs the 'up' action in the top left corner when the remaining number of steps is less than 6.This corresponds to the first situation where the goal cannot be reached anymore.In that case, the agent will perform the 'up' action in order to avoid falling into a hole and ignore the goal.Similarly, the agent occasionally performs the 'left' and 'down' actions in the third square in the top row when the remaining number of steps makes it unable to fall into a hole any longer.Finally, the 'up' and 'right' actions in the third square in the bottom row can be explained through the second situation.</p>
<p>Discussion</p>
<p>Scalability</p>
<p>As it stands, even though the MPS models are rather lightweight (of the order of kB for the T-maze and MB for the Frozen Lake), one of the main concerns regarding this method is computational complexity.Both training and action selection become slower for large action and observation spaces, as well as for long sequences.The largest bottleneck in terms of training is the SVD that occurs at every bond.This underlines the importance of choosing the right cutoff.The cutoff must be small enough to minimize the number of parameters in the model in order to speed up computation, while being able to solve the task at hand.No other parameter can directly affect the speed of the SVD.The largest bottleneck in terms of action selection is tree depth.The deeper the tree, the more computationally expensive action selection becomes.</p>
<p>Nevertheless, these concerns may simply be teething problems.As a matter of fact, we have already shown a first improvement in the training process by performing single-site updates and only occasionally doing the two-site updates.Single-site updates do not require SVD and resulted in a considerable speedup, in addition to requiring less memory.We note that the benefits of single-site updates are also known from quantum many-body physics [76].Other possible speedups exist in the algorithms for building and aggregating of the expected surprisal tree.The choice of loss function could have an impact due to the application of active inference.For example, optimizing conditional probabilities directly instead of joint probabilities, may allow the system to converge more rapidly towards a usable model for active inference, since the active inference algorithm requires mostly conditional probabilities.As such, these and other improvements may reduce the required resources in the future and are avenues for future work.</p>
<p>As showcased by the Frozen Lake environment, an important limitation of the current method is the length of the MPS.In the current form, environments with an infinite time horizon, in particular environments where randomness plays an important role, cannot always be solved within the given time limit.A possible solution to this is the use of MPS variants, such as the uniform MPS [69], i.e. an infinite MPS made up of a unit cell of MPS tensors that are infinitely repeated, as demonstrated by Miller et al [49].Such an approach comes with the advantage that it is more efficient in the sense that only the tensors comprising the unit cell need to be specified, instead of one MPS tensor per time step, which additionally reduces the chances of overfitting.This, too, is an avenue for future work.</p>
<p>Further, the method in this work takes a rather different approach to scaling model size with information theoretical requirements when compared to traditional ML.For example, in deep learning, the typical modus operandi is to design a model which is large enough to contain all the necessary information, and subsequently prune, i.e. scale down, the model until a balance point is reached between performance and model size, a.k.a.model reduction.In this work, the model generally starts out small and grows depending on how much information must be retained, i.e. the MPS starts with small bond dimensions which are allowed to grow based on a prespecified cutoff value.One could argue that the latter makes use of resources more efficiently.</p>
<p>Data and exploration</p>
<p>In current ML literature, the importance of the data is often understated.Much research focuses on creating better models on specific benchmarks, while it is sometimes forgotten that the model always reflects the data, as evidenced by modern challenges such as fairness in AI.While most of the data in this experiment is randomly generated, the T-maze experiments do emphasize the importance of the data.Remark that the engineered data set of size 204 performed better than the randomly generated data set of size 204.By simply constructing a data set in a clever way, the model was able to perform as well as larger data sets (at least for smaller cutoffs).</p>
<p>Clever ways of constructing data sets come up very often in RL in the form of 'exploration' .In brief, exploration is used to generate sequences which are more informative than the sequences that the agent has already seen.For example, sparse reward problems in RL are problems which are typically difficult to solve through random movements for the reason that a positive reward signal may be very difficult to find.In that case, exploration algorithms can lead the agent through sequences that would be underexplored by a random agent.Exploration essentially creates a richer data set that the agent can learn from.</p>
<p>Active inference intrinsically performs exploration through the free energy.The free energy takes care of exploration through the information gain term (equation ( 11)), which arises through variational Bayesian methods.This means that a true active inference agent will trade off exploration and exploitation while navigating an environment in real time.Although directly minimizing surprisal, as performed in this work, does away with the information gain term, further research is required to know how an agent based on our model would behave in situations that require this kind of exploration.It is important to note that ML implementations of active inference, such as deep active inference (i.e. the practice of replacing certain probability distribution by neural networks), usually suffer from the same issue and do not use this term to generate training sequences, but instead use it at inference time to gather information when the environment is stochastic.The typical procedure is similar to the procedure described in this work: first gathering training data (often through demonstration), then learning a generative model, and finally using the model for inference.However, more relevantly to the initial discussion, it may be beneficial to generate training data through some kind of explorative agent rather than a random agent in order to enrich the data set.Indeed, one could go a step further and devise an online algorithm which incorporates exploration.</p>
<p>Conclusion and future work</p>
<p>We have shown that by combining TNs and active inference, it is possible to plan paths within discrete environments.In this work, we employed MPS for the generative model in an active inference agent.This allowed us to minimize surprisal directly instead of variational free energy.Experimentation on the T-maze and Frozen Lake environments confirmed that the MPS-based agent is able to select actions in order to realize its preferred observation, i.e. obtain a reward.Hyperparameters which had an effect on the success rate were the cutoff and data set size in the T-maze environment, and the cutoff and MPS length in the Frozen Lake environment.The effect of the cutoff was reduced greatly when utilizing single-site updates.Results showed that the MPS-based agents were able to perform as well as an optimal agent.This work allows for several future extensions.First is the extension of the current algorithm to infinite time horizons, which would allow the use of active inference with TNs on a broader set of applications.More concretely, this would entail the use of a uniform MPS, i.e. an infinite MPS made up of an infinitely repeated unit cell of MPS tensors.Along the same lines, since the current algorithm was implemented for discrete environments, the application to continuous environments is an interesting prospect.This requires the mapping of continuous variables to the physical space of the MPS, which may be done through a clever feature map.Second are the several possible algorithmic improvements to both the model and action selection, such as the choice of loss function and perhaps a quantum mechanical formulation of the expected surprisal as a replacement to the current tree search formulation.Third is improving the data set and investigating how to reimplement 'exploration' .</p>
<p>Figure 1 .
1
Figure 1.Popular types of tensor networks: (a) a matrix product state (MPS), a 1D array of tensors, (b) a tree tensor network (TTN), a set of tensors arranged in a tree structure, and (c) a projected entangled pair state (PEPS), a 2D array of tensors.</p>
<p>Figure 2 .
2
Figure 2. Bayesian network corresponding to the generative model P(õ,s, π) in discrete time active inference.The model is defined in terms of sequences of observations ot and hidden states st evolving over time, and a policy π(t) = at with at the selected action at time t.</p>
<p>Figure 3 .
3
Figure 3. Possible instances of the T-maze environment as presented by Friston et al [20].The agent (mouse) starts off in the center.The reward (cheese) is located in either the left branch (left instance) or right branch (right instance).The cue reveals the location of the reward: a left-pointing arrow indicates the reward is on the left, a right-pointing arrow indicates the reward is on the right.</p>
<p>Figure 4 .
4
Figure 4. (a) Success rate as a function of cutoff and data set size.Each point reflects the average across 20 models of the success rate over 100 runs.(b) Average negative log likelihood over all possible sequences as a function of cutoff and data set size.Each point reflects the average across 20 models.The dashed line indicates the theoretical minimum of − log(1/204) ≈ 5.32 for this data set.Error bars display a bootstrapped 95% confidence interval.Details on hyperparameters can be found in section 4.1.2.</p>
<p>Figure 5 .
5
Figure 5. (a) Average number of parameters as a function of cutoff and data set size.Each point reflects the average across 10 models.Error bars display a bootstrapped 95% confidence interval.Details on hyperparameters can be found in section 4.1.2.(b) Evolution of bond dimensions during training for a model with ε = 0.01 trained on the engineered data set.The bond dimensions increased as the model learned new information and decreased as the model learned to generalize.Epochs beyond 500 were omitted, since there was no more change.</p>
<p>Figure 6 .
6
Figure 6.Belief shift (a) before and (b) after having seen the cue 'right' .Computed for a model with ε = 0.01 trained on the engineered data set.</p>
<p>Figure 7 .
7
Figure 7. Expected surprisal: (a) for the first action, (b) for the second action after having seen the cue 'left' , and (c) for the second action after having seen the cue 'right' .Computed for a model with ε = 0.01 trained on the engineered data set.</p>
<p>Figure 8 .
8
Figure 8.(a) Success rate obtained using single-site updates as a function of cutoff and data set size.Each point reflects the average across 20 models of the success rate over 100 runs.Error bars display a bootstrapped 95% confidence interval.Details on hyperparameters can be found in section 4.1.2.(b) Evolution of bond dimensions during training with single-site updates for a model with ε = 0.01 trained on the engineered data set.Epochs beyond 500 were omitted, since there was no more change.</p>
<p>Figure 9 .
9
Figure 9.The Frozen Lake environment [6].Reproduced with permission from [79].[© 2021 M. Tillery.All rights reserved.].</p>
<p>Figure 10 .
10
Figure 10.(a) Success rate for the non-slippery 4 × 4 environment as a function of cutoff and MPS length.Each point reflects the model average across three models.Error bars display a bootstrapped 95% confidence interval.Details on hyperparameters can be found in section 4.2.2.(b) Evolution of bond dimensions during training for a model with ε = 0.03 and MPS length 9 on the non-slippery 4 × 4 environment.</p>
<p>Figure 11 .
11
Figure 11.An example of an agent navigating the Frozen Lake environment.The agent's model was trained using ε = 0.03 and an MPS length of 9.The bottom row shows the observation at each time step, while the top row shows the expected surprisal per action computed after each observation.Reproduced with from [79].[© 2021 M. Tillery.All rights reserved.].</p>
<p>Figure 12 .
12
Figure 12.Optimal policies for the slippery Frozen Lake environments obtained through dynamic programming[51].Policies are optimal in the sense that they provide the shortest path while having the lowest possible probability of falling into a hole.A run terminates when the agent reaches a hole or the reward, therefore actions in these positions have been omitted.Overlapping actions indicate both actions are equally viable.Reproduced with permission from[79].[© 2021 M. Tillery.All rights reserved.].</p>
<p>Figure 13 .
13
Figure 13.(a) Success rate obtained for the slippery 3 × 3 environment as a function of cutoff and MPS length.Each point reflects the average across five models of the success rate across 1000 runs.Error bars display a bootstrapped 95% confidence interval.Details on hyperparameters can be found in section 4.2.2.Horizontal lines indicate success rates for agents running the optimal policies (figure 12(b)) truncated at horizons corresponding to the lengths of the MPS, obtained through simulation.(b) Evolution of bond dimensions during training for a model with ε = 0.03 and MPS length 8 on the slippery 3 × 3 environment.</p>
<p>Figure 14 .
14
Figure 14.Policy obtained for the slippery 3 × 3 Frozen Lake environment for a model with ε = 0.01 and MPS length 12, by counting how often an action was taken at each position.The transparency of the arrow indicates the frequency of the action (less transparency is more frequent).Reproduced with permission from [79].[© 2021 M. Tillery.All rights reserved.].</p>
<p>Figure 15 .
15
Figure 15.Evolution of bond dimensions during training for a model with ε = 0.03 and MPS length 12 on the slippery 4 × 4 Frozen Lake environment.</p>
<p>Figure 16 .
16
Figure 16.Policy obtained for the slippery 4 × 4 Frozen Lake environment for a model with ε = 0.03 and MPS length 12, by counting how often an action was taken at each position.The transparency of the arrow indicates the frequency of the action (less transparency is more frequent).Reproduced with permission from [79].[© 2021 M. Tillery.All rights reserved.].</p>
<p>probability of moving right.
AcknowledgmentsThis research received funding from the Flemish Government under the 'Onderzoeksprogramma Artificiéle Intelligentie (AI) Vlaanderen' programme.This work has received support from the European Union's Horizon 2020 program through Grant No. 863476 (ERC-CoG SEQUAM).Frozen lake assets provided by: https://franuka.itch.io/rpg-snow-tilesetfor elf and stool, Mel Tillery www.cyaneus.com/for all other assets.Data availability statementThe data that support the findings of this study are openly available.The code can be found at https://github.com/SWauthier/aif-mps.The trained models can be found at https://cloud.ilabt.imec.be/index.php/s/g4jGBY5EHMft9kF.Appendix. Computing probabilities from an MPSThis section explains how to derive the probabilities that are required for the tree in section 3.3.Firstly, note that any action taken by the agent and the resulting observation can be stored as a vector which contains the history and is updated at every time step: Logically, this vector remains unchanged, since the past cannot be changed.Any probability is therefore calculated using this history vector, e.g.To obtain conditional probabilities, it is sufficient to apply the definition:ORCID iDSamuel T Wauthier  https://orcid.org/0000-0002-1967-2195
B Aizpurua, Palmer S Orus, R , arXiv:2401.00867Tensor networks for explainable machine learning in cybersecurity. 2024</p>
<p>Entanglement spread area law in gapped ground states. A Anshu, A Harrow, M Soleimanifar, 10.1038/s41567-022-01740-7Nat. Phys. 182022</p>
<p>I Arad, A Kitaev, Z Landau, U Vazirani, arXiv:1301.1162An area law and sub-exponential algorithm for 1D systems. 2013</p>
<p>Improved one-dimensional area law for frustration-free systems. I Arad, Z Landau, U Vazirani, 10.1103/PhysRevB.85.195145Phys. Rev. B. 851951452012</p>
<p>An area law for entanglement from exponential decay of correlations. F Brandão, S Horodecki, M , 10.1038/nphys2747Nat. Phys. 92013</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540Openai gym. 2016</p>
<p>Equivalence of restricted Boltzmann machines and tensor network states. J Chen, S Cheng, H Xie, Wang L , Xiang , 10.1103/PhysRevB.97.085104Phys. Rev. B. 97851042018</p>
<p>Tree tensor networks for generative modeling. S Cheng, L Wang, T Xiang, P Zhang, 10.1103/PhysRevB.99.155131Phys. Rev. B. 991551312019</p>
<p>Supervised learning with projected entangled pair states. S Cheng, Wang L Zhang, P , 10.1103/PhysRevB.103.125117Phys. Rev. B. 1031251172021</p>
<p>Matrix product states and projected entangled pair states: concepts, symmetries, theorems. J I Cirac, D Pérez-García, N Schuch, F Verstraete, 10.1103/RevModPhys.93.045003Rev. Mod. Phys. 93450032021</p>
<p>On the expressive power of deep learning: a tensor analysis 29th Annual Conf. N Cohen, O Sharir, A Shashua, on Learning Theory. A Feldman, Rakhlin, Shamir, 201649PMLR, Columbia University</p>
<p>Locally accurate MPS approximations for ground states of one-dimensional gapped local. A M Dalzell, F G S L Brandão, 10.22331/q-2019-09-23-187Hamiltonians Quantum. 31872019</p>
<p>The approximation of one matrix by another of lower rank. C Eckart, G Young, 10.1007/BF02288367Psychometrika. 11936</p>
<p>. M Fannes, Nachtergaele B Werner, R F , 10.1007/BF02099178Finitely correlated states on quantum spin chains Commun. Math. Phys. 1441992</p>
<p>Control flow in active inference systems-part I: classical and quantum formulations of active inference. C Fields, F Fabrocini, K Friston, J F Glazebrook, H Hazan, Levin M Marcianò, A , 10.1109/TMBMC.2023.3272150IEEE Trans. Mol. Biol. Multi-Scale Commun. 92023</p>
<p>Control flow in active inference systems-part II: tensor networks as general models of control flow IEEE Trans. C Fields, F Fabrocini, K Friston, J F Glazebrook, H Hazan, Levin M Marcianò, A , 10.1109/TMBMC.2023.3272158Mol. Biol. Multi-Scale Commun. 92023</p>
<p>Metabolic limits on classical information processing by biological cells. C Fields, M Levin, 10.1016/j.biosystems.2021.104513Biosystems. 2091045132021</p>
<p>. K Friston, Da Costa, L Hafner, D Hesp, C Parr, T , 10.1162/neco_a_01351Neural Comput. 332021Sophisticated inference</p>
<p>Active inference and learning Neurosci. K Friston, T Fitzgerald, F Rigoli, P Schwartenbeck, O' Doherty, J Pezzulo, G , 10.1016/j.neubiorev.2016.06.022Biobehav. Rev. 682016</p>
<p>. K J Friston, M Lin, C D Frith, G Pezzulo, J Hobson, S Ondobaka, 10.1162/neco_a_00999Active inference, curiosity and insight Neural Comput. 292017</p>
<p>E Gillman, D C Rose, J Garrahan, arXiv:2002.05185A tensor network approach to finite markov decision processes. 2020</p>
<p>Combining reinforcement learning and tensor networks, with an application to dynamical large deviations. E Gillman, D C Rose, J P Garrahan, 10.1103/PhysRevLett.132.197301Phys. Rev. Lett. 1321973012024</p>
<p>Neural-network quantum states, string-bond states and chiral topological states. I Glasser, N Pancotti, M August, I Rodriguez, J I Cirac, 10.1103/PhysRevX.8.011006Phys. Rev. X. 8110062018</p>
<p>From probabilistic graphical models to generalized tensor networks for supervised learning. I Glasser, N Pancotti, J I Cirac, 10.1109/ACCESS.2020.2986279IEEE Access. 82020</p>
<p>Expressive power of tensor-network factorizations for probabilistic modeling. I Glasser, R Sweke, N Pancotti, J Eisert, I Cirac, Advances in Neural Information Processing Systems. Wallach, H Larochelle, A Beygelzimer, F d'Alché Buc, E Fox and R Garnett322019Curran Associates, Inc</p>
<p>Matrix product operators for sequence-to-sequence learning. C Guo, Z Jie, Lu W Poletti, D , 10.1103/PhysRevE.98.042114Phys. Rev. E. 98421142018</p>
<p>Post-matrix product state methods: to tangent space and beyond. J Haegeman, T Osborne, F Verstraete, 10.1103/PhysRevB.88.075133Phys. Rev. B. 88751332013</p>
<p>Unsupervised generative modeling using matrix product states. Y Han, Wang J Fan, H , Wang L Zhang, 10.1103/PhysRevX.8.031012Phys. Rev. X. 8310122018</p>
<p>An area law for one-dimensional quantum systems. M Hastings, 10.1088/1742-5468/2007/08/P08024J. Stat. Mech. P080242007</p>
<p>. P Hayden, D W Leung, A Winter, 10.1007/s00220-006-1535-6Aspects of generic entanglement Commun. Math. Phys. 2652006</p>
<p>pymdp: a python library for active inference in discrete state spaces. C Heins, B Millidge, D Demekas, B Klein, K Friston, I Couzin, A Tschantz, 10.21105/joss.04098J. Open Source Softw. 740982022</p>
<p>Waking and dreaming consciousness: neurobiological and functional considerations. J Hobson, K Friston, 10.1016/j.pneurobio.2012.05.003Prog. Neurobiol. 982012</p>
<p>S Howard, arXiv:2401.03896A tensor network implementation of multi agent reinforcement learning. 2024</p>
<p>Y Hur, J G Hoskins, M Lindsey, E Stoudenmire, Y Khoo, arXiv:2202.11788Generative modeling via tensor train sketching. 2023</p>
<p>Experimental indications of non-classical brain functions. C M Kerskens, D L Pérez, 10.1088/2399-6528/ac94beJ. Phys. Commun. 61050012022</p>
<p>Expressive power of recurrent neural networks Int. V Khrulkov, Novikov A Oseledets, Conf. on Learning Representations. 2018</p>
<p>Equivalence and solution of anisotropic spin-1 models and generalized t-j fermion models in one dimension. A Klumper, A Schadschneider, J Zittartz, 10.1088/0305-4470/24/16/012J. Phys. A: Math. Gen. 24L9551991</p>
<p>Matrix product ground states for one-dimensional spin-1 quantum antiferromagnets. A Klümper, A Schadschneider, J Zittartz, 10.1209/0295-5075/24/4/010Europhys. Lett. 242931993</p>
<p>Y Lecun, C Cortes, C J C Burges, The mnist database of handwritten digits. 1998. February 202312</p>
<p>REM sleep selectively prunes and maintains new synapses in development and learning. W Li, L Ma, Yang G Gan, W B , 10.1038/nn.4479Nat. Neurosci. 202017</p>
<p>Machine learning by unitary tensor network of hierarchical tree structure. D Liu, S J Ran, P Wittek, C Peng, R B García, Su G Lewenstein, M , 10.1088/1367-2630/ab31efNew J. Phys. 21730592019</p>
<p>Tensor networks for unsupervised machine learning. J Liu, S Li, J Zhang, P Zhang, 10.1103/PhysRevE.107.L012103Phys. Rev. E. 107L0121032023</p>
<p>X Y Liu, Y X Fang, Quantum tensor networks for variational reinforcement learning First Workshop on Quantum Tensor Networks in Machine Learning. 2020</p>
<p>Many-body control with reinforcement learning and tensor networks Nat. Y Lu, S J Ran, 10.1038/s42256-023-00732-3Mach. Intell. 52023</p>
<p>A Mahajan, M Samvelyan, L Mao, V Makoviychuk, A Garg, J Kossaifi, S Whiteson, Y Zhu, A ; T Anandkumar, Zhang, Tesseract: tensorised actors for multi-agent reinforcement learning Proc. 38th Int. Conf. on Machine Learning. PMLR2021139</p>
<p>Schrödingerrnn: generative modeling of raw audio as a continuously observed quantum state. Mencia Uranga, B Lamacraft, A , Proc. First Mathematical and Scientific Machine Learning Conf. R Lu, ( Ward, Pmlr, First Mathematical and Scientific Machine Learning Conf2020107</p>
<p>Self-correcting quantum many-body control using reinforcement learning with tensor networks Nat. F Metz, M Bukov, 10.1038/s42256-023-00687-5Mach. Intell. 52023</p>
<p>J Miller, G Rabusseau, J Terilla, Tensor networks for probabilistic sequence modeling Proc. of The 24th Int. Conf. on Artificial Intelligence and Statistics. K Banerjee, ( Fukumizu, Pmlr, 2021130</p>
<p>Simulating strongly correlated quantum systems with tree tensor networks. V Murg, F Verstraete, O Legeza, R M Noack, 10.1103/PhysRevB.82.205105Phys. Rev. B. 822051052010</p>
<p>. R Ng, 2020. 19 December 2023Dynamic programming (available at</p>
<p>Tensor networks for complex quantum systems Nat. R Orús, 10.1038/s42254-019-0086-7Rev. Phys. 12019</p>
<p>Tensor-train decomposition SIAM. I Oseledets, 10.1137/090752286J. Sci. Comput. 332011</p>
<p>Active Inference: The Free Energy Principle in Mind, Brain and Behavior. T Parr, G Pezzulo, K J Friston, 2022The MIT Press</p>
<p>Y Peng, Y Chen, E Stoudenmire, Y Khoo, arXiv:2304.05305Generative modeling via hierarchical tensor sketching. 2023</p>
<p>. D Perez-Garcia, F Verstraete, M M Wolf, J I Cirac, 10.26421/QIC7.5-6-1Matrix product state representations Quantum Inf. Comput. 72007</p>
<p>Tensor networks for interpretable and efficient quantum-inspired machine learning. S Ran, G Su, 10.34133/icomputing.0061Intell. Comput. 2612023</p>
<p>Active inference, preference learning and adaptive behaviour IOP Conf. N Sajid, P Tigas, K Friston, 10.1088/1757-899X/1261/1/012020Ser.: Mater. Sci. Eng. 1261120202022</p>
<p>What is Life? the Physical Aspects of the Living Cell With Mind and Matter and Autobiographical Sketches. E Schrödinger, 1944Cambridge University Press</p>
<p>Entropy scaling and simulability by matrix product states. N Schuch, M M Wolf, F Verstraete, J I Cirac, 10.1103/PhysRevLett.100.030504Phys. Rev. Lett. 100305042008</p>
<p>Neural tensor contractions and the expressive power of deep neural quantum states. O Sharir, A Shashua, G Carleo, 10.1103/PhysRevB.106.205136Phys. Rev. B. 1062051362022</p>
<p>Classical simulation of quantum many-body systems with a tree tensor network. Y Shi, Y Duan, L Vidal, G , 10.1103/PhysRevA.74.022320Phys. Rev. A. 74223202006</p>
<p>S Srinivasan, Gordon G Boots, B , Learning hidden quantum markov models Proc. 21st Int. Conf. on Artificial Intelligence and Statistics. F Storkey, Perez-Cruz, PMLR201884</p>
<p>Probabilistic modeling with matrix product states. J Stokes, J Terilla, 10.3390/e21121236Entropy. 2112362019</p>
<p>E Stoudenmire, D J ; Schwab, M Lee, Sugiyama, I Luxburg, Guyon, Garnett, Supervised learning with tensor networks Advances in Neural Information Processing Systems. Curran Associates, Inc201629</p>
<p>Generative tensor network classification model for supervised machine learning. Z Z Sun, C Peng, D Liu, S Ran, G Su, 10.1103/PhysRevB.101.075135Phys. Rev. B. 101751352020</p>
<p>Explainable natural language processing with matrix product states. J Tangpanitanon, C Mangkang, P Bhadola, Y Minato, D Angelakis, T Chotibut, 10.1088/1367-2630/ac6232New J. Phys. 24530322022</p>
<p>Simulating excitation spectra with projected entangled-pair states. L Vanderstraeten, J Haegeman, F Verstraete, 10.1103/PhysRevB.99.165121Phys. Rev. B. 991651212019</p>
<p>Tangent-space methods for uniform matrix product states. L Vanderstraeten, J Haegeman, F Verstraete, 10.21468/SciPostPhysLectNotes.7SciPost Phys. Lect. Notes. 72019</p>
<p>F Verstraete, J I Cirac, arXiv:cond-mat/0407066Renormalization algorithms for quantum-many body systems in two and higher dimensions. 2004</p>
<p>Entanglement renormalization. G Vidal, 10.1103/PhysRevLett.99.220405Phys. Rev. Lett. 992204052007</p>
<p>T Vieijra, L Vanderstraeten, F Verstraete, arXiv:2202.08177Generative modeling with projected entangled-pair states. 2022</p>
<p>M Wang, Y Pan, Z Xu, X Yang, Li G Cichocki, A , arXiv:2302.09019Tensor networks meet neural networks: a survey and future perspectives. 2023</p>
<p>Learning generative models for active inference using tensor networks Active Inference. S T Wauthier, B Vanhecke, T Verbelen, B ; C L Dhoedt, D Buckley, Cialfi, Lanillos, Ramstead, H Sajid, Shimazaki, Verbelen, 2023Springer</p>
<p>Density matrix formulation for quantum renormalization groups. S White, 10.1103/PhysRevLett.69.2863Phys. Rev. Lett. 691992</p>
<p>Density matrix renormalization group algorithms with a single center site. S White, 10.1103/PhysRevB.72.180403Phys. Rev. B. 721804032005</p>
<p>Numerical renormalization-group study of low-lying eigenstates of the antiferromagnetic s = 1 Heisenberg chain. S R White, D A Huse, 10.1103/PhysRevB.48.3844Phys. Rev. B. 481993</p>
<p>Topological nature of spinons and holons: elementary excitations from matrix product states with conserved symmetries. V Zauner-Stauber, L Vanderstraeten, J Haegeman, I Mcculloch, Verstraete, 10.1103/PhysRevB.97.235155Phys. Rev. B. 972351552018</p>
<p>. Mel Tillery, available at: www.cyaneus.com/</p>            </div>
        </div>

    </div>
</body>
</html>
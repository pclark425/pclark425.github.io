<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6995 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6995</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6995</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1d128c402153d533bafab6fb142505ebf6129f2d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1d128c402153d533bafab6fb142505ebf6129f2d" target="_blank">A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper presents a novel conversational model for human--robot interaction that rests upon a graph-based representation of the dialogue state that is empirically evaluated through a user study with a humanoid robot that acts as conversation partner to evaluate the impact of the graph-to-text mechanism on the response generation.</p>
                <p><strong>Paper Abstract:</strong> Knowledge graphs are often used to represent structured information in a flexible and efficient manner, but their use in situated dialogue remains under-explored. This paper presents a novel conversational model for human--robot interaction that rests upon a graph-based representation of the dialogue state. The knowledge graph representing the dialogue state is continuously updated with new observations from the robot sensors, including linguistic, situated and multimodal inputs, and is further enriched by other modules, in particular for spatial understanding. The neural conversational model employed to respond to user utterances relies on a simple but effective graph-to-text mechanism that traverses the dialogue state graph and converts the traversals into a natural language form. This conversion of the state graph into text is performed using a set of parameterized functions, and the values for those parameters are optimized based on a small set of Wizard-of-Oz interactions. After this conversion, the text representation of the dialogue state graph is included as part of the prompt of a large language model used to decode the agent response. The proposed approach is empirically evaluated through a user study with a humanoid robot that acts as conversation partner to evaluate the impact of the graph-to-text mechanism on the response generation. After moving a robot along a tour of an indoor environment, participants interacted with the robot using spoken dialogue and evaluated how well the robot was able to answer questions about what the robot observed during the tour. User scores show a statistically significant improvement in the perceived factuality of the robot responses when the graph-to-text approach is employed, compared to a baseline using inputs structured as semantic triples.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6995.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6995.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbalized Graph (Traversal-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traversal-based Parameterized Graph-to-Text Verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameterized graph-to-text function that traverses a dynamic dialogue/state knowledge graph in temporal order, collects local paths (node -> edge -> neighbor sequences), records node probabilities and timestamps, and renders those traversals into natural-language paragraphs used as part of an LLM prompt; parameters (e.g., pronoun choice, inclusion of low‑probability entities, distance/rotation formatting, time inclusion) are optimized on a small Wizard‑of‑Oz dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Traversal-based verbalized representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is ordered temporally and iterated over; for each node the traversal function selects a set of path templates (defined by node types and edge sequences), collects ingoing/outgoing edges and neighbor nodes, records probabilities and timestamps, and fills parameterized natural-language templates to produce a chronological textual document (paragraphs summarizing locations visited, objects seen, times, distances, and rotations).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, lossy (natural-language summarization of paths)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Temporal ordering of nodes + local path traversal (path templates defined per node type) producing concatenated natural-language sentences; spatial coordinates are simplified via Ramer–Douglas–Peucker and converted to forward/rotation movement descriptions included in text.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Wizard-of-Oz tuning set and user-study tour dialogues (collected in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph verbalization for knowledge-grounded response generation in HRI (prompting LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA‑2 (used for parameter selection); GPT‑4 and LLaMA‑2 (used for response generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA‑2 chat model (quantized, run locally) and GPT‑4 (API access); LLaMA‑2 used to optimize verbalization parameters by maximizing probability of wizard responses, GPT‑4 and LLaMA‑2 used to generate robot replies from verbalized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>User Likert ratings on factuality and adequacy (1–5), statistical tests (Wilcoxon Signed Rank), token counts for negation and uncertainty words; cross-entropy on wizard responses used during parameter optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>User study averages: GPT_verbal factuality=4.06, adequacy=3.88; LLaMA_verbal factuality=3.68, adequacy=3.79. Parameter-selection objective: minimized cross-entropy of LLaMA on N Wizard-of-Oz (values not numerically reported). Statistical test: GPT_verbal vs GPT_triples factuality p=0.03 (unadjusted); Bonferroni-adjusted p=0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not used for model parameter training of the LLM; when used as prompt context it improved perceived factuality of GPT‑4 responses relative to a semantic-triples baseline, enabled the model to express calibrated uncertainty (higher counts of 'may/might/possibly'), and helped LLMs better convey chronology and distances in answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Transforms graph structure into natural-language summaries that can be lossy relative to raw graph structure; effectiveness depends on upstream perception (VQA/object detection) — omissions in detected objects lead to factual errors; verbalization increases token usage and may stress LLM context windows (context/window limits noted generally); parameterization optimized on a small Wizard-of-Oz set — potential generalization limits across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared in this study to a semantic-triples baseline: verbalized representation produced higher user-rated factuality for GPT‑4, fewer negation tokens and far more explicit uncertainty expressions; the triples baseline tended to refrain from giving ordered/temporal answers and used more negation ('not', 'cannot'). The paper also positions this approach relative to other graph‑to‑text methods (graph2seq, path concatenation) as simpler and controllable via categorical parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6995.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6995.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic Triples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Triple List (edge-list pseudo-language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline textual encoding of the dialogue state where graph facts are rendered as a list of semantic triples (subject, predicate, object or short pseudo-language statements), each triple written on its own line and concatenated into a document supplied to the LLM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Semantic triple sequence (pseudo-language)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph edge/fact is converted into a single-line triple-style statement (e.g., 'bookshelf in office', 'turn left 75 degrees creation_time 08:55'), producing a structured but non-natural-language document that enumerates nodes and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, sequential, relatively lossless (explicit factual assertions per edge)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-list serialization: enumerate graph edges/facts and write each as a triple/pseudo-statement, concatenated line-by-line into a prompt document.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>User-study tour dialogues (this work) used as prompt input for GPT‑4 baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompting LLMs for knowledge-grounded response generation (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT‑4 (used with triples baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT‑4 (OpenAI API) consuming triple-structured textual document as context to generate robot responses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>User Likert ratings on factuality and adequacy; token counts for negation and uncertainty words; Wilcoxon tests comparing models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>User averages: GPT_triples factuality=3.44, adequacy=3.69. Negation token counts (this study): total=112 (breakdown: 'not'=69, 'cannot'=13, 'no'=14, other negations=16). Uncertainty tokens total=6.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Used as an input format for in-context prompting rather than for training; produced reasonable LLM responses but tended to be more conservative, frequently using negation and declining to provide ordered temporal answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Although relatively fact-complete (edge-level facts preserved), the triple list format made it difficult for the LLM to extract and synthesize chronological order, durations, and distances; resulted in higher use of negative/avoidance tokens and fewer explicit uncertainty qualifiers; less natural for LLM to reason across paths compared to verbalized narrative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Serves as the baseline in this paper: verbalized traversal summaries gave higher factuality ratings (GPT_verbal > GPT_triples) and enabled better chronological/uncertainty expression. The paper cites other structured-to-text approaches (e.g., pseudo-language statements in DiffKG/Beygi et al.) as related alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6995.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6995.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path Concatenation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph path concatenation into token sequences (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned related approach where knowledge-graph paths are translated to text by concatenating entities and relations separated by special tokens; used in some KG completion or path-based models to turn graph traversals into linear text sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Path concatenation with special tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph paths (sequences of entities and relations) are linearized by concatenating entity and relation identifiers with separator/special tokens to form a single token sequence that an LM can process (e.g., entity1 [REL] relation [REL] entity2 [SEP] ...).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (path-linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Concatenate KG paths into linear sequences, inserting special tokens between entities/relations to preserve role boundaries (path-based encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph to text / KG path-based encoding for KG completion or downstream LM consumption (mentioned as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Mentioned as an alternative graph-to-text encoding used in other work; not evaluated in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not discussed in detail here; implicitly may preserve structural path information but can produce synthetic pseudo-language that is less natural for LLMs to reason over compared to a natural-language verbalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Paper cites this approach as an alternative (Lin et al., 2023) and contrasts it with their natural-language verbalization which aims to be more directly usable in LLM prompts; path-concatenation approaches are acknowledged but not empirically compared in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6995",
    "paper_id": "paper-1d128c402153d533bafab6fb142505ebf6129f2d",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Verbalized Graph (Traversal-based)",
            "name_full": "Traversal-based Parameterized Graph-to-Text Verbalization",
            "brief_description": "A parameterized graph-to-text function that traverses a dynamic dialogue/state knowledge graph in temporal order, collects local paths (node -&gt; edge -&gt; neighbor sequences), records node probabilities and timestamps, and renders those traversals into natural-language paragraphs used as part of an LLM prompt; parameters (e.g., pronoun choice, inclusion of low‑probability entities, distance/rotation formatting, time inclusion) are optimized on a small Wizard‑of‑Oz dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Traversal-based verbalized representation",
            "representation_description": "The graph is ordered temporally and iterated over; for each node the traversal function selects a set of path templates (defined by node types and edge sequences), collects ingoing/outgoing edges and neighbor nodes, records probabilities and timestamps, and fills parameterized natural-language templates to produce a chronological textual document (paragraphs summarizing locations visited, objects seen, times, distances, and rotations).",
            "representation_type": "sequential, lossy (natural-language summarization of paths)",
            "encoding_method": "Temporal ordering of nodes + local path traversal (path templates defined per node type) producing concatenated natural-language sentences; spatial coordinates are simplified via Ramer–Douglas–Peucker and converted to forward/rotation movement descriptions included in text.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "Wizard-of-Oz tuning set and user-study tour dialogues (collected in this work)",
            "task_name": "Knowledge-graph verbalization for knowledge-grounded response generation in HRI (prompting LLMs)",
            "model_name": "LLaMA‑2 (used for parameter selection); GPT‑4 and LLaMA‑2 (used for response generation)",
            "model_description": "LLaMA‑2 chat model (quantized, run locally) and GPT‑4 (API access); LLaMA‑2 used to optimize verbalization parameters by maximizing probability of wizard responses, GPT‑4 and LLaMA‑2 used to generate robot replies from verbalized prompts.",
            "performance_metric": "User Likert ratings on factuality and adequacy (1–5), statistical tests (Wilcoxon Signed Rank), token counts for negation and uncertainty words; cross-entropy on wizard responses used during parameter optimization.",
            "performance_value": "User study averages: GPT_verbal factuality=4.06, adequacy=3.88; LLaMA_verbal factuality=3.68, adequacy=3.79. Parameter-selection objective: minimized cross-entropy of LLaMA on N Wizard-of-Oz (values not numerically reported). Statistical test: GPT_verbal vs GPT_triples factuality p=0.03 (unadjusted); Bonferroni-adjusted p=0.1.",
            "impact_on_training": "Not used for model parameter training of the LLM; when used as prompt context it improved perceived factuality of GPT‑4 responses relative to a semantic-triples baseline, enabled the model to express calibrated uncertainty (higher counts of 'may/might/possibly'), and helped LLMs better convey chronology and distances in answers.",
            "limitations": "Transforms graph structure into natural-language summaries that can be lossy relative to raw graph structure; effectiveness depends on upstream perception (VQA/object detection) — omissions in detected objects lead to factual errors; verbalization increases token usage and may stress LLM context windows (context/window limits noted generally); parameterization optimized on a small Wizard-of-Oz set — potential generalization limits across domains.",
            "comparison_with_other": "Compared in this study to a semantic-triples baseline: verbalized representation produced higher user-rated factuality for GPT‑4, fewer negation tokens and far more explicit uncertainty expressions; the triples baseline tended to refrain from giving ordered/temporal answers and used more negation ('not', 'cannot'). The paper also positions this approach relative to other graph‑to‑text methods (graph2seq, path concatenation) as simpler and controllable via categorical parameters.",
            "uuid": "e6995.0",
            "source_info": {
                "paper_title": "A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Semantic Triples",
            "name_full": "Semantic Triple List (edge-list pseudo-language)",
            "brief_description": "A baseline textual encoding of the dialogue state where graph facts are rendered as a list of semantic triples (subject, predicate, object or short pseudo-language statements), each triple written on its own line and concatenated into a document supplied to the LLM prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Semantic triple sequence (pseudo-language)",
            "representation_description": "Each graph edge/fact is converted into a single-line triple-style statement (e.g., 'bookshelf in office', 'turn left 75 degrees creation_time 08:55'), producing a structured but non-natural-language document that enumerates nodes and relations.",
            "representation_type": "token-based, sequential, relatively lossless (explicit factual assertions per edge)",
            "encoding_method": "Edge-list serialization: enumerate graph edges/facts and write each as a triple/pseudo-statement, concatenated line-by-line into a prompt document.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "User-study tour dialogues (this work) used as prompt input for GPT‑4 baseline",
            "task_name": "Prompting LLMs for knowledge-grounded response generation (baseline comparison)",
            "model_name": "GPT‑4 (used with triples baseline)",
            "model_description": "GPT‑4 (OpenAI API) consuming triple-structured textual document as context to generate robot responses.",
            "performance_metric": "User Likert ratings on factuality and adequacy; token counts for negation and uncertainty words; Wilcoxon tests comparing models.",
            "performance_value": "User averages: GPT_triples factuality=3.44, adequacy=3.69. Negation token counts (this study): total=112 (breakdown: 'not'=69, 'cannot'=13, 'no'=14, other negations=16). Uncertainty tokens total=6.",
            "impact_on_training": "Used as an input format for in-context prompting rather than for training; produced reasonable LLM responses but tended to be more conservative, frequently using negation and declining to provide ordered temporal answers.",
            "limitations": "Although relatively fact-complete (edge-level facts preserved), the triple list format made it difficult for the LLM to extract and synthesize chronological order, durations, and distances; resulted in higher use of negative/avoidance tokens and fewer explicit uncertainty qualifiers; less natural for LLM to reason across paths compared to verbalized narrative summaries.",
            "comparison_with_other": "Serves as the baseline in this paper: verbalized traversal summaries gave higher factuality ratings (GPT_verbal &gt; GPT_triples) and enabled better chronological/uncertainty expression. The paper cites other structured-to-text approaches (e.g., pseudo-language statements in DiffKG/Beygi et al.) as related alternatives.",
            "uuid": "e6995.1",
            "source_info": {
                "paper_title": "A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Path Concatenation (related work)",
            "name_full": "Knowledge-graph path concatenation into token sequences (related work mention)",
            "brief_description": "Mentioned related approach where knowledge-graph paths are translated to text by concatenating entities and relations separated by special tokens; used in some KG completion or path-based models to turn graph traversals into linear text sequences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Path concatenation with special tokens",
            "representation_description": "Graph paths (sequences of entities and relations) are linearized by concatenating entity and relation identifiers with separator/special tokens to form a single token sequence that an LM can process (e.g., entity1 [REL] relation [REL] entity2 [SEP] ...).",
            "representation_type": "sequential, token-based (path-linearization)",
            "encoding_method": "Concatenate KG paths into linear sequences, inserting special tokens between entities/relations to preserve role boundaries (path-based encoding).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Knowledge-graph to text / KG path-based encoding for KG completion or downstream LM consumption (mentioned as related work)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Mentioned as an alternative graph-to-text encoding used in other work; not evaluated in this study.",
            "limitations": "Not discussed in detail here; implicitly may preserve structural path information but can produce synthetic pseudo-language that is less natural for LLMs to reason over compared to a natural-language verbalization.",
            "comparison_with_other": "Paper cites this approach as an alternative (Lin et al., 2023) and contrasts it with their natural-language verbalization which aims to be more directly usable in LLM prompts; path-concatenation approaches are acknowledged but not empirically compared in this work.",
            "uuid": "e6995.2",
            "source_info": {
                "paper_title": "A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.013660499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Graph-to-Text Approach to Knowledge-Grounded Response Generation in Human-Robot Interaction</h1>
<p>Nicholas Thomas Walker<br>WALKER@NR.NO<br>Norwegian Computing Center<br>Stefan Ultes<br>STEFAN.ULTES@UNI-BAMBERG.DE<br>Pierre Lison<br>PLISON@NR.NO<br>Norwegian Computing Center</p>
<p>Editor: Name Surname
Submitted 11/2023; Accepted MM/YYYY; Published online MM/YYYY</p>
<h4>Abstract</h4>
<p>Knowledge graphs are often used to represent structured information in a flexible and efficient manner, but their use in situated dialogue remains under-explored. This paper presents a novel conversational model for human-robot interaction that rests upon a graph-based representation of the dialogue state. The knowledge graph representing the dialogue state is continuously updated with new observations from the robot sensors, including linguistic, situated and multimodal inputs, and is further enriched by other modules, in particular for spatial understanding. The neural conversational model employed to respond to user utterances relies on a simple but effective graph-to-text mechanism that traverses the dialogue state graph and converts the traversals into a natural language form. This conversion of the state graph into text is performed using a set of parameterized functions, and the values for those parameters are optimized based on a small set of Wizard-of-Oz interactions. After this conversion, the text representation of the dialogue state graph is included as part of the prompt of a large language model used to decode the agent response. The proposed approach is empirically evaluated through a user study with a humanoid robot that acts as conversation partner to evaluate the impact of the graph-to-text mechanism on the response generation. After moving a robot along a tour of an indoor environment, participants interacted with the robot using spoken dialogue and evaluated how well the robot was able to answer questions about what the robot observed during the tour. User scores show an improvement in the perceived factuality of the robot responses when the graph-to-text approach is employed compared to a baseline using inputs structured as semantic triples.</p>
<p>Keywords: Dialogue Management, Human-Robot Interaction, Large Language Models, Graphs, Task-oriented dialogue</p>
<h2>1. Introduction</h2>
<p>Recent advances in NLP have made large strides in improving the conversational abilities of dialogue systems (Bommasani et al., 2021). Large language models (LLMs) based on the transformer architecture (Vaswani et al., 2017) have become the foundation for a large range of conversational agents, both task-oriented (Hosseini-Asl et al., 2020; Peng et al., 2020; Nekvinda and Dušek, 2022) and open-domain (Zhang et al., 2020; Roller et al., 2021; Thoppilan et al., 2022). These advances have also raised questions about the capability of these models to perform tasks and reason about</p>
<p>information (Valmeekam et al., 2022). Numerous investigations into the reasoning capabilities of LLMs have resulted in evidence of their abilities in some areas (Bubeck et al., 2023), while indicating shortcomings in others (Shi et al., 2023; Ullman, 2023).</p>
<p>Meanwhile, there is a continuing interest and ongoing work in human-robot interaction (HRI) using large language models. A central question in this vein of research is how information about a robot's environment from different modalities can be effectively combined with LLMs. For instance, Huang et al. (2022) demonstrated a system where feedback from the environment is used in the form of a kind of inner monologue. Under such a system, multiple sources of information are combined in a form representing the semantics of a situation, in the sense of how objects under discussion relate to each other. These systems thereby make use of multiple modalities obtained from other (particularly pre-trained) models where the combined information enables improved performance even in zero-shot scenarios (Zeng et al., 2022). Such models suggest that there exists a strong capability for LLMs to operate over diverse sources of information structured as text input.</p>
<p>In this article, we explore the problem of response generation for embodied agents and demonstrate how various sources of information can be collected and acted upon in such a system. Our approach rests on the following elements:</p>
<ol>
<li>A dynamic knowledge graph of the dialogue state where locations, physical objects, and dialogue elements such as utterances are represented as nodes and semantic relations between them represented with edges.</li>
<li>A simple parameterized function to verbalize the dialogue state graph based on traversals of the graphs, and the optimization of those verbalization parameters from Wizard-of-oz data.</li>
<li>A large language model that includes the verbalized representation of the graph as part of its prompt and is responsible for generating the robot response.</li>
</ol>
<p>A sketch of the proposed approach is provided in Figure 1. As shown in the figure, information detected by the embodied agent including speech, visual, and positional data are brought together into a unified structure in the form of a graph and subsequently transformed into natural language as input for an LLM. Specifically, this is a dynamic knowledge graph, in which nodes and edges are added incrementally. A dynamic rather than static graph structure allows it to be modified throughout robot exploration and conversation with users. The graph structure thereby enables the continuous inclusion of both situated and dialogue-specific information as it is received by the system. Semantic relations between objects, places, and utterances are represented as edges between the nodes of the graph, enabling not only verbal expression of these relations but also of larger traversals of the graph. Moreover, probabilities and temporal relations in the graph are also easily exploited in such a transformation. The graph structure is thereby exposed to an LLM in a way that enables the strengths of such models to be exploited through a simple, controllable transformation from graph to text.</p>
<p>To evaluate the proposed approach, we design an experiment with users to explore its use in a practical setting. Our experiment consists of users touring an office space with a robot and subsequently discussing the tour. We evaluate the robot's ability to converse about the tour when equipped with two LLMs (specifically GPT-4 and LLaMA), using the graph-to-text approach and a baseline triples conversion. From the resulting dialogues and user feedback, we provide a statistical analysis of the results along with further qualitative analysis of attributes of the resulting dialogues.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Diagram of the proposed approach. The red path in the graph indicates a traversal which is transformed to a natural language document via a parameterized function. The resulting text description is then inserted into the prompt for the language model.</p>
<p>The rest of the paper is structured as follows. In Section 2, we provide a general background on embodied conversational agents, knowledge graphs for dialogue, and large language models. Section 3 describes how to structure the dialogue state of the agent as a knowledge graph, verbalize its content through graph traversals, and integrate the result into the prompt of the LLM employed to generate the robot responses to user utterances. Section 4 presents an experimental study that investigates the influence of this graph-to-text approach on the factuality and adequacy of the robot responses. The obtained user ratings show that this graph-to-text approach is able to improve the factuality of the robot responses compared to a baseline relying on inputs where the graph information is provided in the form of semantic triples. Finally, in sections 5 and 6 we discuss the experimental results and review the benefits and limitations of the proposed approach, and conclude.</p>
<h1>2. Background</h1>
<p>In this section, we discuss the background to our study, starting with contemporary approaches to embodied agents in HRI. We then discuss recent work where graphs have been used with dialogue systems to represent situated dialogue information. Finally, we review the recent usage of LLMs in dialogue systems and the variety of approaches to presenting background information to them, including with graphs and for embodied agents.</p>
<h3>2.1 Embodied Conversational Agents</h3>
<p>Embodiment refers broadly to the operation of an artificial agent in physical space, though specific definitions vary in precision and strictness (Miller and Feil-Seifer, 2016). Whichever definition may be preferred, embodiment has a substantial impact on dialogue and affects how speakers perceive</p>
<p>and relate to the dialogue (Wainer et al., 2006; Pickering and Garrod, 2009). In human-robot interaction, grounding the dialogue in the physical environment is often a prerequisite to language understanding. Grounded language, in the sense employed by e.g. Tellex et al. (2020), refers to situated language, i.e. language with meaning in a physical environment.</p>
<p>In a situated scenario, dialogue between a human and a dialogue agent has a direct spatial relation to the surrounding environment. An embodied, robotic agent must take into account numerous input sources from the environment as well as any human users to accomplish complex tasks requiring both natural language or physical outputs, i.e. movement. Beyond spatial data and language input, visual perceptions of varying kinds are an important source of information for the dialogue.</p>
<p>These kinds of information may also be viewed in relation to time. For instance, the utterances in a dialogue are naturally temporally related. Moreover, embodied agents may take different actions depending on what information is most recently observed. Linguistic, visual, and spatial information acquired by the robot may also be uncertain, presenting an additional challenge in effectively leveraging world knowledge to accomplish tasks. The agent's grounding in the physical world is subject to uncertainty arising from both perceptual module errors as well as inherently uncertain aspects of the environment. Thus, temporality and probability interact with these kinds of data to present a nuanced picture of the world for the agent to act upon. When quantified and represented by the system, uncertainty about the environment or dialogue state can be communicated to the user (Hough and Schlangen, 2017). By dialogue state, we refer to a representation of the current dialogue context which contains all information that is relevant for responding to user utterances.</p>
<p>Within HRI, language input is one communication medium among many that can be used to interact with a robot (Goodrich et al., 2008). In some cases, it may be used to instruct a robot to perform actions in its physical environment. For example, Rankin et al. (2021) demonstrated semantic parsing of instructions to associate the instructions with desired locations for the robot to explore. In a smaller setting, Lynch et al. (2023) explored the use of imitation learning through natural language instructions to guide a robotic arm to move objects. The real-time task required an approach that could account for both spatial information and the temporal ordering of actions and instructions given to the robot. Another model integrating spatio-temporal grounding was presented by Shah et al. (2023). In their model, a robotic navigation system combines a pre-trained visual navigation module with an LLM to construct a path following instructions. This approach first identifies landmarks with an LLM and subsequently grounds this information in a set of visual inputs from the navigation area using a second module. Each of these approaches required alignment of language input to the robot's physical environment.</p>
<h1>2.2 Knowledge graphs for dialogue</h1>
<h3>2.2.1 GRAPH-BASED REPRESENTATIONS</h3>
<p>While dialogue state information may be represented in numerous ways, graphs are a flexible data structure with desirable properties with regard to both natural language and the physical world. A graph $G$ consists of a set of vertices (or nodes) $V$ paired with a set of edges $E$. Each edge describes a pair of vertices, which may be ordered or unordered (describing a directed or undirected graph, respectively). Knowledge graphs specifically represent entities, that is to say objects and concepts, as well as relations between them. An example of graph-structured background information for dialogue is the dataset of Eric et al. (2017). In this dataset, each dialogue is associated with a knowledge base of entities for the dialogue system to retrieve in response to user queries. The</p>
<p>dataset comprises three domains, of which the navigation domain represents entities with a spatial relation (distance) to the speaker and dialogue agent. With this property, the dialogue system can reason over the dialogue context to infer closest locations to recommend to the user. Another graphbased model of spatial information for embodied agents is the Scene Graph (Xu et al., 2020), which represents physical objects as graph nodes and spatial relations as edges between them. Similarly, in the model presented by Li et al. (2022b), an embodied scene graph can also be generated by a model trained with imitation and reinforcement learning. The resulting graphs can then be applied to downstream tasks requiring spatial information.</p>
<p>Within language data specifically, both utterance and dialogue level structures can be expressively rendered as graphs such as those of the Abstract Meaning Representation paradigm (Banarescu et al., 2013). A relevant use of this paradigm was presented by Bonial et al. (2020), extending the AMR schema for dialogue with a robot performing tasks in search and navigation. Their model of situated dialogue constructs graph representations of both user commands and agent responses. These graphs represent a semantic model of both objects in the environment and actions for the system to take. Graphs have also been used to represent dialogue structure as a whole and flows between utterances (Andreas et al., 2020; Gritta et al., 2021). Also specifically in HRI, Wilcock and Jokinen (2022) presented a model for robotic conversational agents that used knowledge graphs to present semantic metadata to the system. Thus, in the context of previous work it is apparent that combining abstract and dialogue-specific information with situated knowledge provides a fruitful avenue for state representation (Papangelis and Ultes, 2020). Within this line of inquiry, our work builds upon a graph-structured dialogue state model wherein both dialogue elements and physical objects in the world are equally represented as nodes of a graph with edges in the graph representing semantic relations between them (Walker et al., 2022).</p>
<p>Aside from the general usage of graphs representing linguistic structure or purely physical information, an additional kind of dialogue information that can be expressed in a graph is temporal relations between objects. For instance, Xing and Tsang (2023) introduced the DARER model, which used two types of temporal graphs representing semantic relations for dialogue act and sentiment prediction. In their model, user utterances are represented as nodes of a graph with "previous" and "future" relations between them. As demonstrated in a number of models (Tuan et al., 2019; He et al., 2023), the dialogue agent's knowledge state can be captured as a dynamic knowledge graph that is updated throughout the dialogue. Dynamic, evolving graphs capture the intuition that the dialogue agent's knowledge should evolve along with the dialogue, encompassing perception of natural language utterances or physical observations as updates to the graph.</p>
<p>A further advantageous property of graphs is the capability of associating probabilities with the constituent nodes or edges of a graph. A common use of probability with graph structures in dialogue systems has been in handling uncertainty in action selection (Young et al., 2013). Bayesian Networks are an additional example of a probabilistic model which has been used for dialogue management, including with multimodal data (Prodanov and Drygajlo, 2003; Thomson et al., 2008). More generally, the problem of epistemic uncertainty (Celemin and Kober, 2023) about the dialogue state may also be usefully described by probabilities associated with graph elements.</p>
<h1>2.2.2 GRAPH PROCESSING</h1>
<p>Although the flexibility of graphs allows for a wide diversity of state representations, it also requires adaptation of response generation models to operate on a graph structure. One means of doing so</p>
<p>is the use of Graph Neural Networks (GNNs). An example of a GNN model used to process a large knowledge base was presented by Christmann et al. (2023). Their model for conversational question answering used a GNN to construct a reduced representation of a large heterogeneous graph of entities. This model had the particular advantage of enabling explainability in the model along with using numerous combined sources of information. Particularly large knowledge graphs may require a filtering method or preprocessing to extract the most salient information (Wang et al., 2021). GNNs have also been used among other things to verbalize graphs, such as the method presented by Yang et al. (2020), wherein a graph attention network is used with a verbalization of a program to verify facts. While GNNs have become an area of interest for using graph data in many domains, these models often require large quantities of in-domain training data and may require certain properties in the graph structure (e.g. homogeneity, connectivity). For instance, models such as the influential GraphSAGE of Hamilton et al. (2017) have limitations in their ability to distinguish certain non-isomorphic graphs (Garg et al., 2020). Moreover, GNNs also face difficulties when aggregating information across long paths in graphs, an "over-squashing" of graph information explored by Alon and Yahav (2020).</p>
<p>Graph verbalization is not limited to GNN-based models. Xu et al. (2018) demonstrated a method of transforming Structured Query Language (SQL), itself viewable as a graph-structure, to natural language via a graph-to-sequence model. A common thread among approaches to verbalizing graphs is a correspondence of graph structures to natural language surface forms. A basic example is the conversion of a semantic triple (subject, predicate, and object) of two nodes and an edge between them to a sentence:
('robot' | 'in' | 'hallway') $\rightarrow$ The robot is located in the hallway.</p>
<p>Such conversion is not limited to small-scale elements of the graph. The entire graph might be labelled or summarized by some natural language description, as in graph classification (Zhang et al., 2022). A knowledge graph completion algorithm relying on a transformation of paths in the graph was presented by Lin et al. (2023). In their approach, knowledge graph paths are translated to text by concatenating entities and relations separated by special tokens. Another approach using traversals knowledge graph traversals is OpenDialKG (Moon et al., 2019), which relates shifts in dialogue context to walks in the graph. Other work using this dataset and the dataset of Eric et al. (2021) is the DiffKG model of Tuan et al. (2021). The DiffKG model uses a transformer architecture to perform walks over the knowledge graph to select KG knowledge for a dialogue system. This system was evaluated with respect to ground truth paths and entities in fixed knowledge bases which are not available in all dialogue domains. The authors explored a baseline following the model of Beygi et al. (2022), which transformed a knowledge graph into a text document of semantic "pseudo-language" statements. In both this model and DiffKG, a structured text representation of the graph is a source of information to the dialogue system similar to the semantic triple-based model we employ in our study (see Section 4). The AttnIO model of Jung et al. (2020) also makes use of traversals with an attention mechanism to attend to varying sizes of node neighborhoods in the graph.</p>
<h1>2.3 Large Language Models (LLMs)</h1>
<h3>2.3.1 RESPONSE GENERATION WITH LLMS</h3>
<p>LLMs have demonstrated strong capabilities for few-shot learning across tasks (Brown et al., 2020), although limitations remain in their use in task-oriented dialogue (Hudeček and Dušek, 2023). At the outset, these models have a great deal of information available from the large quantities of data consumed in training them (Bommasani et al., 2021).</p>
<p>Despite substantial improvements in LLM performance across tasks, limitations remain. Dziri et al. (2023) investigated LLM performance on reasoning tasks and found that the models often resort to basic pattern recognition rather than generic problem solving. Moreover, hallucinations remain a substantial limitation and subject of continuing research (Ji et al., 2023). As Zhang et al. (2023) describe it, a hallucination exists when an LLM generates output that "misaligns with established world knowledge." For our purposes, we can consider established world knowledge to be defined by the physical environment in which the user and robot operate. Thus, the notion of factuality (or, absence of hallucinations) derives from what is true in the physical environment, thus for our study we seek to evaluate factuality of model responses with respect to the environment rather than the graph itself. Recent study has also indicated that the linguistic structures within prompts may be a factor in the occurrence of hallucinations, specifically that more formal and concrete prompts may assist in reducing hallucination (Rawte et al., 2023). In this context, evaluation of factuality in LLM responses remains an important direction for study.</p>
<h3>2.3.2 Prompting methods</h3>
<p>In many recent dialogue systems, the model is initialized with a system prompt which consists of an initial input that informs the system what role it should play in the dialogue and how to proceed (Liu et al., 2023). Besides defining the system's role in the dialogue, the prompt may include other information that is salient to the system's tasks. Importantly, the input to LLMs is bound by a context window beyond which input text must be truncated, although some research has investigated means of alleviating this limitation (Ratner et al., 2023). When the total scenariospecific information exceeds the limit imposed by the context window, it becomes advantageous to design methods to select relevant information (Thulke et al., 2021). Complementary to these methods is the possibility of providing further enriched context such as information derived by commonsense or logical reasoning (Walker et al., 2023).</p>
<p>Due to the difficulty of training large LMs, the use of in-context learning has also become a heavily investigated method of providing training examples to the largest language models (Dong et al., 2022). With in-context learning, a small number of examples can be used to prime a model to generate output of a desired format or style. Notably, this technique avoids the need to update model parameters which would otherwise be costly or infeasible given the size of many current LLMs, instead relying upon the model's pattern recognition abilities to adapt its output to a desired outcome. Combined with the general notion of prompting, this approach enables substantial flexibility when using LLMs for different tasks and scenarios, including with multimodal tasks such as generalizing models to unseen contexts (Tai et al., 2023). In-context learning with LLMs has also been demonstrated with table-like structures (Chen, 2022).</p>
<h1>2.3.3 LLMS IN EMBODIED SYSTEMS</h1>
<p>As noted previously, LLMs have been used as a means of processing language input for embodied systems such as commands or instructions from a user. An example of a model that integrated an LLM with an embodied agent is the ProgPrompt model presented by Gupta and Kembhavi (2023). ProgPrompt generated task plans for a robot using an LLM by making use of a Python-like prompt format which induced the LLM to generate actions structured as executable programs. Thus, the LLM does not operate on strictly natural language, but rather a kind of programming language. A major advantage in the ProgPrompt system was that the set of actions available to the robot could be represented compactly and abstractly as functions in this language structure. A robot equipped with an LLM was also presented by Billing et al. (2023), who demonstrated a basic model of HRI with a Pepper robot using the GPT-3 model of OpenAI. In this case, although the dialogue system interacted with the user through an physical agent, the dialogues themselves were not situated in the physical environment.</p>
<p>Beyond the use of text input (natural language or otherwise), other research has investigated directly incorporating multimodal input to LLM architectures. An example of LLM capable of generating text based on combined text and visual input is the Flamingo model of Alayrac et al. (2022), who presented an approach to combine textual and image data with few-shot prompting for visual dialogue. Similarly, the Palm-E model of Driess et al. (2023) addressed multimodality by directly incorporating embeddings of image and other non-language data in the input to an LLM architecture. Such approaches suggest that models trained or prompted to incorporate mixed forms of information can be effective, however these architectures are often task-specific. Moreover, as noted before, it is often cost-prohibitive to train large models.</p>
<p>When the system must take into account its physical environment, the challenge of presenting it appropriate information is potentially greater than a non-embodied dialogue system. As direct operation on audio-visual data or haptic feedback is impossible for an LLM, a transformation from the raw forms of these inputs into natural language is required. Ahn et al. (2022) presented a model called SayCan that combined an LLM with an "affordance function" describing the probability of success over a set of possible actions for the agent. Another example of such a system was presented by Zhao et al. (2023), where a robot "chats" with the environment in the sense that multimodal sensory input is iteratively presented to an LLM as natural language statements describing the input, with the model outputting actions at each turn. This approach takes advantage of the turn-taking nature of dialogue to inform a system in real-time, but would be difficult to port to an embodied agent intended to explore a wider environment. Nonetheless, this work demonstrates the capabilities of an LLM in reasoning over diverse sources of input information represented as natural language.</p>
<p>A further example of LLMs integrated in HRI systems is the LLM-Planner of Song et al. (2022), a few-shot system using prompting with a GPT-3 model as a planner for an embodied agent. An approach for data augmentation by grounding dialogue responses in text documents was presented by Wu et al. (2020). This model used two dialogue agents to simulate conversations across different domains represented by sets of documents. Lastly, a question answering model which stands quite close to our approach was presented by Lanchantin et al. (2023), whose model made use of a textformatted version of a knowledge graph along with a transformer operating directly on the graph. This model operated in a simulated three-dimensional space, where the queries directed to the agent related to the spatio-temporal information describing the user's position in the simulated space. This work did not however investigate the use of a natural language transformation of the graph,</p>
<p>relying instead on a text document constructed of structured representations of graph information in a similar manner to the semantic triples model employed in our experiments. Additionally, this approach relied upon the generation of simulated data in a virtual space, whereas we investigate the real-time creation of graph in a physically situated HRI scenario.</p>
<h1>3. Approach</h1>
<p>In this section, we describe our proposed approach to LLM-based response generation for HRI. Our proposed model revolves around a dynamic knowledge graph that represents both elements of the environment and dialogue utterances. Nodes and edges describing these elements are iteratively added as new data is observed by an agent.</p>
<h3>3.1 State Representation</h3>
<p>As explored in Section 2, embodied dialogue agents must take into account diverse types of information regarding the dialogue as well as the physical environment. These aspects of embodied dialogue suggest that the flexibility, expressiveness, and semantic interpretability of graphs are effective at representing evolving spatial, temporal, and probabilistic information in a single structure. For these reasons, we use a dynamic knowledge graph to represent the dialogue state.</p>
<p>A dynamic, in-memory knowledge graph permits continuous updates as new information is recorded. The nodes of this graph represent entities, in the sense of distinct objects and places in the environments, as well as natural language utterances. Likewise, the edges represent semantic relations between these entities. These relations include spatial or temporal information which may be updated in the course of exploring an environment. Temporality is also applicable to utterances within the dialogue as a chain of responses. As both dialogue information and situated information from the environment are represented within the graph, a form of "grounding" (in the sense discussed in Section 2) of the dialogue in the environment is equally possible through edges between utterances and physical objects.</p>
<p>In this model, nodes in the graph are assigned a type label along with a content. The type of the node indicates what kind of conversational entity the node represents, such as an utterance or a physical object. Edges in the graph represent semantic relations between the nodes of objects, such as physical location, origination, or in the case of utterances a temporal ordering.</p>
<p>To illustrate situated information in a graph, the node representing a laptop in Figure 2 has a description of "laptop" along with the type label "entity". The laptop is likewise related with an "in" relation to an image data node carrying the type label "Image", where binary data of the image itself is stored. In turn, the image node relates to a "Location" node named "office", indicating that the picture was taken in the location labelled "office". Plainly stated, this graph structure describes the fact that a laptop computer was seen in an image taken in an office.</p>
<p>Nodes and edges are also assigned a probability. The assignment of a probability value to a node allows for the expression of uncertainty arising from sensory data. This is notably the case for visual elements detected in the images captured by the agent, or for alternative transcriptions produced by the ASR model. In general, regardless of the source or type of information represented in a node, uncertainty in the data can be expressed as an attribute of the node. Likewise, edges can also be assigned to a probability.</p>
<p>A further property of nodes is a timestamp. This attribute makes it possible to account for temporality in the data. Every node in our model is assigned a timestamp representing its creation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of subgraph where an image is assigned a location, and a "laptop" entity detected in the image data is created as a node with an "in" relation to the image.
time in the graph. A second timestamp representing the creation time of the information is also assigned to the node metadata. This timestamp is the time the information was recorded by an external module such as robot sensors. The difference between the two timestamps is often small, nevertheless separate representations allow precise measurement of any latency or reference to exact occurrences in physical space as opposed to graph update time.</p>
<h1>3.2 State Tracking \&amp; Postprocessing</h1>
<h3>3.2.1 GRAPH UPDATES</h3>
<p>Dialogue State Tracking (Williams et al., 2016; Ren et al., 2018) is often viewed in the form of a "slot-filling" paradigm, wherein slots of a defined ontology are filled by a predictive model. Here, we conceive of state-tracking as the management of the conversational knowledge graph. As the knowledge graph is comparatively small, it is stored in-memory and directly updated through Python operations. Because the knowledge graph is dynamic, new nodes representing spatial positions, images, and entities seen during the embodied agent's movement are added to the state. This is a constant stream of information that updates the dialogue state as soon as new information is obtained. Thus, spatial coordinates and visual information can be continuously added as nodes to the graph while an embodied agent traverses physical space. Equally, language input is added to the graph as it is received, whether concurrently with agent movement or not. This flexibility is an advantage for an embodied agent to allow it to attend to numerous, diverse sources of information simultaneously while also taking into account interactions between them.</p>
<p>We rely on a dedicated management module that serves as a central point of access for all updates to the graph. Upon each update, the management module triggers all system modules that should be notified of this change. Those modules may in turn lead to new graph updates, thereby creating a cascading transformation to the graph, where modules variously react and push changes to the graph manager. An important example is a response generation module, which is triggered upon the addition of a new user utterance node in the graph. Using the updated graph, the response</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example path drawn from spatial coordinates, with raw coordinates on the left and the approximated path on the right. The orange points mark coordinates associated with the room label, blue points are those labeled as located in the hallway. Approximate forward movements and inplace rotations are highlighted alongside the robot path in red and green, respectively.
generation module generates an agent response to the last user utterance and adds the response to the graph with a "follows" relation to the utterance. This graph update in turn is passed to the embodied agent for speech generation.</p>
<p>In the same fashion, objects detected in visual sensory input are added to the graph. Raw image data collected by a camera can be added as nodes to the graph directly. These additions in turn trigger postprocessing of the image data, whereby objects, people and other entities may be recognized and added to the graph as additional nodes with an "in" relation to the image. We also assign probabilities to these objects derived from the output of the visual model use to identify them.</p>
<h1>3.2.2 PATH GENERATION</h1>
<p>A particular component integrated in the HRI platform employed in our experiments (Section 4) is a mechanism for deriving a continuous path from the raw sequence of spatial coordinates recorded by the robot. This mechanism operates in two steps. The spatial coordinates are first converted into a limited number of lines based on the Ramer-Douglas-Peucker algorithm (Douglas and Peucker, 1973). Those lines are in turn converted into an ordered list of movements, divided in two types: straight forward movements and in-place rotations ${ }^{1}$, the straight movements being specified for a given distance, and the rotations for a given angle. This list of movements is then integrated into the graph. An example path drawn from raw coordinates is given in Figure 3.</p>
<p>The nodes expressing the spatial coordinates in the state graph are further connected to edges expressing that they are part of a larger location with a given name, such as "hallway" or "break room". In the experiments, those names are provided by the human participant during the tour, as detailed in Section 4.1.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Triples Representation</th>
<th style="text-align: center;">Verbalized Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Turn left 75 degrees creation_time 08:55 <br> bookshelf in office of Magna <br> table creation_time 08:39 <br> bookshelf creation_time 08:48 <br> computer mouse creation_time 08:38 <br> door creation_time 08:48 <br> Forward 2.8 meters start corridor 3 <br> Forward 1.2 meters start corridor for <br> Turn left 67 degrees start corridor for <br> person in corridor for <br> Turn left 70 degrees start corridor <br> desk creation_time 08:45 <br> chair creation_time 08:48 <br> Forward 2.4 meters start storage room <br> Forward 9.0 meters start new corridor <br> Forward 1.8 meters start survive <br> $[\ldots]$</td>
<td style="text-align: center;">At 08:38, we were in starting area. <br> In starting area, Pepper saw a table, a door, a desk, a chair, a <br> computer mouse, a bookshelf and a TV. Pepper may have also <br> seen a computer, a blue curtain, a lamp, a light, paper, a <br> window and a copy machine. We spent 27 seconds in that area <br> and traveled 5.7 meters. In starting area, Pepper turned left 24 <br> degrees. At 08:39, we were in corridor. In corridor, Pepper saw <br> a table, a fire extinguisher, a door, a desk, a chair, a couch, a <br> bookshelf, a whiteboard and a window. Pepper may have also <br> seen a wastebin, a plant, a printer, a pillar, a pen, a blue <br> curtain, a lamp, a light, a TV and a copy machine. We spent 84 <br> seconds in that area and traveled 25.9 meters. [...]</td>
</tr>
</tbody>
</table>
<p>Figure 4: Comparison of a dialogue graph represented with triples versus verbalization.</p>
<h1>3.3 Graph-to-text Transformation</h1>
<h3>3.3.1 VERbALIZATION FUNCTION</h3>
<p>To use the dialogue state graph information with an LLM, it is necessary to convert the structured information into natural language. A variety of methods have been applied to generate natural language from structured information such as tables (Nema et al., 2018). In our proposed approach, the information expressed in the dialogue state graph is converted into text description via a set of graph traversals. In this approach, both relations between nodes as well as more complex paths can be rendered with a natural language description. This traversal-based transformation is a simple means to convert structured information to a form amenable to an LLM.</p>
<p>To traverse the graph, we first order the nodes temporally and iterate over them. For each node, the node's type determines which paths should be searched for in its neighborhood. The traversal function then collects ingoing and outgoing edges along with the neighboring nodes connected to them. For each node type, the function defines a set of paths to traverse by searching along the ingoing or outgoing edges. As an example, when evaluating an "entity" node, paths containing an "in" relation from the source node to an image node with a subsequent "in_location" edge to a "location" node describes the location of an object (as in Figure 2). Thus, the paths are defined in terms of which edges and node types should be found in a given sequence of hops in the graph. The collection of these paths then constitutes a chronologically ordered description of the graph. Node probabilities are also recorded during the traversal.</p>
<p>The verbalization process takes the form of a function that converts the graph into a string according to a set of handcrafted parameters. An example parameter is that the system may reference itself within the prompt, such as first or third person pronouns, or a name. Depending on this parameter, the description of the graph will then summarize the information from either a personal or impersonal perspective with respect to the dialogue agent. As illustrated in Table 1, the verbaliza-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: center;">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">How the agent refers to itself</td>
<td style="text-align: center;">"Pepper", "the robot", "I"</td>
</tr>
<tr>
<td style="text-align: left;">Whether to include discourse markers between sentences</td>
<td style="text-align: center;">True - False</td>
</tr>
<tr>
<td style="text-align: left;">How to specify distance (in meters)</td>
<td style="text-align: center;">"precise", "rounding", "none"</td>
</tr>
<tr>
<td style="text-align: left;">Whether to specify rotation angle (in degrees)</td>
<td style="text-align: center;">True - False</td>
</tr>
<tr>
<td style="text-align: left;">Whether to include low-probability entities in description</td>
<td style="text-align: center;">True - False</td>
</tr>
<tr>
<td style="text-align: left;">Whether to include time in description</td>
<td style="text-align: center;">True - False</td>
</tr>
<tr>
<td style="text-align: left;">Whether to mention the total number of turns</td>
<td style="text-align: center;">True - False</td>
</tr>
</tbody>
</table>
<p>Table 1: Parameter values for the verbalization function. Each parameter specifies an aspect of the verbalization. Bold values indicate the parameter values selected by optimization.
tion function employed in our experiments also included parameters related to the use of discourse markers, the inclusion of low-probability entities in the description, or how to express distances, angles, and time.</p>
<h1>3.3.2 PARAMETER SELECTION</h1>
<p>As mentioned above, the transformation of the graph to a text document is a parameterized function. To select the best set of parameters for the task, we optimize the parameters with respect to the output of a LLaMA language model (Touvron et al., 2023), such that the model assigns the highest probability to the correct response out of a set of candidate responses across dialogues. For this purpose, we collect a small set of Wizard-of-Oz interactions, each response from the wizard being associated with a corresponding dialogue state graphs (which include sample locations and entities along with the dialogue history).</p>
<p>Formally, let $V_{\mathbf{P}}(G)$ denote a graph-to-text function where $G$ is a knowledge graph and $\mathbf{P}$ a set of categorical parameters, and let $D=\left{\left(G_{i}, r_{i}\right), 1 \leq i \leq N\right}$ denote the recorded set of $N$ Wizard-of-Oz responses $r_{i}$ in a conversational context represented by its dialogue state graph $G_{i}$. We then search for an optimal set of parameter values $\mathbf{P}^{*}$ that minimizes the cross-entropy loss over the Wizard-of-Oz examples, assuming a fixed language model LLM:</p>
<p>$$
\mathbf{P}^{*}=\underset{\mathbf{P}}{\arg \min } \mathcal{L}_{\mathrm{CE}}(D ; \mathrm{LLM}, \mathbf{P})
$$</p>
<p>where the cross-entropy loss is itself defined as:</p>
<p>$$
\mathcal{L}<em _left_G__i="\left(G_{i">{\mathrm{CE}}(D ; \mathrm{LLM}, \mathbf{P})=-\frac{1}{N} \sum</em>\right)\right)
$$}, r_{i}\right) \in D} \log P_{\mathrm{LLM}}\left(r_{i} \mid V_{\mathbf{P}}\left(G_{i</p>
<p>The optimization is performed using a tree-structured Parzen estimator algorithm (Bergstra et al., 2011), as implemented in the Optuna package for efficient hyper-parameter optimization (Akiba et al., 2019). Using the above approach, we are able to optimize the categorical parameters and adjust the verbalization function with only a few example responses from a wizard.</p>
<h3>3.4 Response Generation</h3>
<p>The final component of the proposed approach is the actual response generation. As already mentioned, this response is generated using a large language model optimized for dialogue use cases. In</p>
<p>our experiments, those language models were respectively based on GPT-4 (OpenAI, 2023) and the Llama-2 chat model (Touvron et al., 2023).</p>
<p>The response is generated based on a prompt that includes the verbalized graph along with:</p>
<ul>
<li>A short, generic instruction text that details how the robot should respond to the user utterances, and stress in particular that the robot responses should all be grounded in the observations expressed in the verbalized graph.</li>
<li>A short list of example responses from the Wizard-of-Oz interactions to take advantage of incontext learning (Brown et al., 2020; Wei et al., 2022) and provide cues about how responses are expected to be formulated.</li>
</ul>
<p>Once decoding is complete, the response is itself added to the graph as a new node. This node is connected to the user utterance with a responds_to relation, thereby capturing the temporal sequence of dialogue turns in the graph.</p>
<h1>4. Experiments</h1>
<p>In this section, we describe the design of our experimental study and the results of the corresponding user evaluation. The goal of our study is to explore the capabilities of a robot that relies on a dynamic knowledge graph as the representation of its environment and dialogue state, and converts this graph into natural language to enable the generation of system responses through an LLM. To do so, we designed a study wherein users brought the robot to different locations in an office environment. After the completion of this office tour, the participants were then instructed to conduct a short dialogue with the robot to assess its ability to recollect the observations made during the tour.</p>
<p>Three distinct model setups were investigated, and users conversed with all three in a randomized order immediately after their tour with the robot. After each conversation, the participants were asked to rate the robot responses with respect to their factuality and their adequacy.</p>
<p>We recruited a total of 20 participants (for the most part IT students and researchers) for our study, the first two participants being asked to test a prototype version of the final system. Each participant was given a 200 NOK gift card for their participation. For the robot in the experiment, we use the Pepper robot of Aldebaran ${ }^{2}$. The dialogues were conducted in English.</p>
<h3>4.1 Experimental Setup</h3>
<p>The experiment was structured in two parts that respectively correspond to the physical tour of the office and the dialogue between the user and the robot that followed.</p>
<p>Tour phase During the tour, the robot is moved around an office environment by a participant to record details of its path, locations, and identifiable objects. For the sake of simplicity, we instruct participants to push the robot along its path in the office. Before entering a new room or corridor, the participant was instructed to activate the robot microphone by tapping its head and stating the name of the area they were entering.</p>
<p>The time spent on each tour ranged from around 5 to 20 minutes.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The first author in a dialogue with the robot. A video of a short example tour may be found at https://youtu.be/a52zBcfVgS8</p>
<p>Dialogue phase After the tour was completed, the user was then asked to converse with the robot and have it summarize the tour, as illustrated in Figure 5. The participant was instructed to ask the robot at least 5 questions relating to the tour, asking it to summarize the tour along with questions such as to describe which locations it visited, what was seen in each location, what distance was traveled, or how much time was spent in an area. Participants were encouraged to treat the conversation as a natural, continuous dialogue rather than a set of isolated questions, allowing for follow up questions referencing the previous question (e.g. "What did you see after that?").</p>
<p>This dialogue between robot and user is repeated three times, each time with a distinct configuration for response generation (see below). Participants were instructed to treat each dialogue as a completely independent conversation. The model order was randomly reshuffled for each experiment. The participants were also told they did not need to ask the same questions in each dialogue.</p>
<h1>4.2 System configuration</h1>
<p>The software architecture employed for the experiments integrates a number of modules, as illustrated in Figure 6. The sensor-related modules ran on the Pepper robot itself, while the other modules were located on a separate machine with access to a GPU.</p>
<h3>4.2.1 ROBOT PERCEPTION</h3>
<p>During the tour, the dialogue state graph is continuously updated with new visual and positional data, along with the timestamp at which these elements are observed by the robot. The user-provided location labels are added to the graph as nodes with the provided name as the node description. As the robot moves along its path, new nodes are added to the state representing spatial positions, images, and entities seen during its movement. The room labels provided by the user are assigned to all subsequent positions recorded by the robot until the next label was provided.</p>
<p>The robot takes a picture of what is in front of it approximately every 1.5 seconds, along with its current coordinates relative to an initial starting location. Due to odometry drift, we were unable to</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Diagram of the implemented architecture for the experiment. Robot sensor modules are colored blue, while post-processing models operating on the graph are colored yellow.
treat the spatial data as a precise representation of the robot's path at large distances. Nevertheless, these location measurements provided an approximate measure of distance traveled within areas as well as a means of estimating directional changes by the robot. The image data and $x, y$ coordinates are added to the graph as nodes labeled image and position, respectively, and are each created with an edge to the current location node. As mentioned in Section 3.2, a dedicated mechanism is implemented to convert the raw spatial coordinates to a high-level sequence of movements based on the Ramer-Douglas-Peucker algorithm (Douglas and Peucker, 1973).</p>
<p>When an image is added to the graph, a visual question answering (VQA) model processes the image information and returns a score of whether objects were detected in the image. The model we chose for this purpose is the Vision Transformer model of Minderer et al. (2022). We enumerate the possible objects the robot should see including people as well as office objects (see Appendix A). When objects are added to the graph with this model, the estimated probability of the object in the image from the VQA model is assigned as the probability of the node. As this is based upon a non-fine-tuned model, there is substantial noise in the observations of the objects.</p>
<h1>4.2.2 Dialogue SYSTEM</h1>
<p>During the dialogue phase, the user activates the microphone by tapping the robot's head. Utterances are transcribed using the Google Speech API ${ }^{3}$ and insert to the knowledge graph as nodes.</p>
<p>The process of response generation is triggered upon the addition of new utterance nodes in the graph. This process begins with the verbalization of the graph (see section 3.3). The resulting description of the graph is then inserted into a prompt format for the language model. We use GPT-4 (OpenAI, 2023) and LLaMA 2 (Touvron et al., 2023) as the LLMs for our experiment. The GPT-4 model was accessed through the OpenAI API while the Llama-2 model, which is specifically optimized for dialogue use cases ${ }^{4}$, was run on a local GPU, using quantization methods (Dettmers et al., 2022b,a) to satisfy memory limitations.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Factuality</th>
<th style="text-align: center;">Adequacy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT $_{\text {Triples }}$</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">3.69</td>
</tr>
<tr>
<td style="text-align: left;">GPT $_{\text {Verbal }}$</td>
<td style="text-align: center;">4.06</td>
<td style="text-align: center;">3.88</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA $_{\text {Verbal }}$</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">3.79</td>
</tr>
</tbody>
</table>
<p>Table 2: Average participant scores for the model factuality and conversational adequacy.</p>
<p>To evaluate the impact of the graph verbalization on the dialogues, we test the following setups with these models:</p>
<ol>
<li>The GPT-4 model provided with a prompt containing the verbalized dialogue state graph.</li>
<li>The LLaMA 2 model provided with a prompt containing the verbalized dialogue state graph.</li>
<li>The GPT-4 model provided with a prompt containing the dialogue state graph expressed as semantic triples of the state graph facts.</li>
</ol>
<p>The first two models make use of the verbalized graph as produced by the transformation function and selected parameters from Section 3.3. The prompt to these models is therefore a concatenation of the general prompt describing the system's role and the resulting text description. For the third model, the graph information is instead provided as a list of semantic triples directly describing the edges of the graph, as illustrated in Figure 4.</p>
<p>Due to technical issues encountered with a few interactions, there is a slight imbalance in the number of dialogues collected for each model (GPT-triples: 16, GPT-Verbal: 17, LLaMA: 19). This, however, does not affect the statistical significance of the experimental results, detailed below.</p>
<p>The LLM-generated robot responses are sent to the text-to-speech engine embedded on the Pepper platform. When speaking, the robot also makes basic hand gestures and movement to accompany its response, using the off-the-shelf library made available on the Pepper platform.</p>
<h1>4.3 User Evaluation</h1>
<p>Right after the conclusion of each dialogue, the participants were asked to rate two statements about the dialogue on a Likert scale of 1 to 5 for two statements where 1 is "Never", 2 is "Mostly Not", 3 "Sometimes", 4 "Mostly", and 5 "Always":</p>
<p>Statement 1 (factuality): The robot responded factually according to what we saw on the tour.
Statement 2 (conversational adequacy): The robot answered my questions and responded in a relevant, natural, and concise manner.</p>
<p>Table 2 shows the average scores given to each model for both statements, while Figure 7 provides a box plot of the user scores for Statement 1. The user ratings show that participants generally rated the models using verbalized documents higher in factuality than the model based on semantic triples, while providing similar scores to all models for the second statement regarding the conversational adequacy of the robot responses.</p>
<p>To statistically evaluate the difference in scores between the models, we use the Wilcoxon Signed Rank Test (Wilcoxon, 1945) with the typical threshold for significance $\alpha=0.05$. We first</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Box plot of the results for Statement 1: The dotted line in each box represents the median, while the white inner square displays the mean user score for each model.
evaluate both alternative models pairwise against the baseline model. Regarding the first statement, between the GPT-verbal model and the GPT model based on semantic triples we find a difference of $p=0.03, t=1.86$. We found no statistically significant difference between the scores of the LlaMA model and the semantic triples model ( $p=0.13, t=1.13$ ) for this statement. Likewise, we find no statistically significant difference between either GPT-verbal ( $p=0.25, t=0.67$ ) or LLaMA ( $p=0.29, t=0.54$ ) scores with the semantic triples model for the second statement.</p>
<p>While the pairwise difference in distributions of scores for factuality between GPT-Verbal and the baseline were below our threshold $\alpha=0.05$, the experiment results may also be looked at from the perspective of all models. That is, whether for all models we can say there is a statistically significant difference in outcomes (whatever model is chosen of the three). To correct for multiple comparisons of this sort, we apply Bonferroni correction to provide a conservative bound on the resulting p-values amongst all model comparisons. With Bonferroni correction, between the GPTverbal model and the baseline triples model the adjusted value for factuality is $p=0.1$, and for adequacy $p=0.75$. For the LLaMA model, the values are $p=0.39$ and $p=0.88$, respectively.</p>
<h1>4.4 Analysis</h1>
<h3>4.4.1 GENERAL OBSERVATIONS</h3>
<p>Qualitatively, the majority of factuality errors across the three models were due to objects that were not detected by the VQA model employed for object and person detection, and were consequently not mentioned in the provided prompt (either as semantic triples or verbalized graph). Although those omissions do indeed result in factually incorrect statements from the robot, there is little that can be done from the dialogue side to correct this shortcoming.</p>
<p>The setup based on semantic triples demonstrates a decent performance on both statements rated by the participants. This model was however more prone to asserting it did not have information available, and was generally unable to provide details as to the order of rooms visited. Although the</p>
<p>times rooms were entered were available to the model based on semantic triples, the structure of the input made it difficult for the LLM to provide informed answers over the tour chronology.</p>
<p>Due to limited GPU resources, the LLaMA model had much higher latency in response time to user utterances. This condition did not appear to affect the resultant scores, but with inspection of the dialogues it appears to have resulted in a higher number of independent questions rather than ones that build upon each other. Participants generally appeared to feel more free to fluently express themselves with the GPT models, sometimes to the point of off-topic utterances. Moreover, the LLaMA model's responses to questions about what it saw tended to lean to absolute "Yes" and "No", rather than expressing uncertainty. Another tendency of the LLaMA model was to provide numbered lists of observations within the tour, resulting in more verbose conversational behaviour.</p>
<h1>4.4.2 NEGATION WORDS</h1>
<p>To further explore the difference in factuality scores, we also analyzed the occurrence of negation words. The following list was used: "no", "not", "can't", "don't", "unable" and "cannot".</p>
<p>As shown in Table 3, the number of negation words was much higher in the model based on semantic triples. The higher occurrence of those words reflects the tendency of this model to refrain from providing certain details, particularly the order of visited locations and the time spent in them.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">"Not"</th>
<th style="text-align: center;">"Cannot"</th>
<th style="text-align: center;">"No"</th>
<th style="text-align: center;">Other*</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT $_{\text {Triples }}$</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">112</td>
</tr>
<tr>
<td style="text-align: left;">GPT $_{\text {Verbal }}$</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA $_{\text {Verbal }}$</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">91</td>
</tr>
</tbody>
</table>
<p>Table 3: Count of negation words by model.</p>
<h3>4.4.3 EXPRESSION OF UNCERTAINTY</h3>
<p>To investigate the models' ability to express probability and uncertainty, we analyzed a count of words expressing uncertainty such as "may" and "might". The frequency of such words reveals that the GPT-4 model with the verbalized graph was substantially more likely to communicate uncertainty to the user, as shown in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">"May"</th>
<th style="text-align: center;">"Might"</th>
<th style="text-align: center;">"Possibly"</th>
<th style="text-align: center;">"Possible"</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT $_{\text {Triples }}$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">GPT $_{\text {Verbal }}$</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA $_{\text {Verbal }}$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>Table 4: Token counts of words expressing uncertainty by model.</p>
<p>Upon inspecting the dialogues, there appear to have been no instances where the GPT-verbal model expressed uncertainty with the word "may" that was not justified by the observation descriptions. By constrast, The GPT-triples model used "may" in this way twice, with the remaining count</p>
<p>arising from utterances where the model asks the user to note that the observations may not describe certain aspects such as order or count of objects. This tendency appears to demonstrate that while probability of observations was presented in semantic triple form, the model made significantly more use of uncertainty in its expression when uncertainty was transformed to natural language.</p>
<h1>4.4.4 INFLUENCE OF MODEL ORDER</h1>
<p>We analyzed the effect of the model order on user scores, shown in Table 5. There was not a substantial effect on scores from whether the model was presented first or last to the user. To analyze the interaction of model types with order, we fit a Bayesian linear mixed model using the Bambi ${ }^{5}$ library for both factuality and adequacy scores. We treat model order as a random effect as Score $\sim$ Model $*(1 \mid$ Order $)$ to evaluate the interaction of model and order on participant scores for each question. The results indicated that order had minimal impact on scores, with a random intercept variance estimate of $0.109(\mathrm{SD}=0.330,94 \% \mathrm{HDI}:[0.000,0.976])$ for factuality and 0.203 ( $\mathrm{SD}=0.451,94 \% \mathrm{HDI}:[0.002,1.573])$ for adequacy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Factuality (first)</th>
<th style="text-align: center;">Factuality (last)</th>
<th style="text-align: center;">Adequacy (first)</th>
<th style="text-align: center;">Adequacy (last)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT $_{\text {Triples }}$</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">3.83</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: left;">GPT $_{\text {Verbal }}$</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">3.75</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA $_{\text {Verbal }}$</td>
<td style="text-align: center;">3.71</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">3.71</td>
</tr>
</tbody>
</table>
<p>Table 5: Average participant scores for model factuality and conversational adequacy when presented to the user first and last.</p>
<h3>4.4.5 OutLIERS</h3>
<p>As shown in the box plot of Figure 7, the LLaMA model had the widest range in scores, being the only model to receive scores of " 1 " to either statement. This score was recorded in two dialogues. In one case, the model frequently answered that it had not seen things in the tour after having enumerated them in an initial summary and demonstrated an unexpected inability to recall facts. This first dialogue is as follows and received a score of 2 for statement 2 :</p>
<p>User: which room did we spend the most time in
Robot: Based on the observations, we spent the most time in the "sick room" (205 seconds).
User: what was the room we went to after the glass hallway
Robot: Based on the observations, after the glass hallway, we went to the "printer area" ( 32 seconds).
User: do you see any useful equipment for extinguishing a fire anywhere
Robot: No, I do not see any useful equipment for extinguishing a fire anywhere in the observations.
User: were there any chairs in the glass hallway
Robot: No, I did not see any chairs in the glass hallway.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>User: were there any people working in the office
Robot: Yes, I saw a person working in the office.
User: what furniture store in the sick room
Robot: There was no furniture store in the sick room.
While the first two observations were correct according to the recorded time, the model was unable to provide facts about the detected entities, namely a fire extinguisher and a chair in the hallway, furniture in the sick room (chair, sofa) and the absence of people in the office.</p>
<p>In the second case, the model failed to describe detected objects from the tour and was given a score of 1 for both statements. However, all model scores given by the participant were quite low. This participant appeared to take a much stricter approach to interpretation of the statements in evaluating all models. The model was nonetheless weak in recalling detected objects in this dialogue. An excerpt from this dialogue is as follows:</p>
<p>User: where did we go first
Robot: We first went to the main hallway.
User: and after that
Robot: After the main hallway, we went to the print room.
User: did you see any plants
Robot: No, I did not see any plants during my tour of the office floor.
User: that's not true
Robot: I apologize, you are correct. I did see a bookshelf in the first office I visited, but I did not see any plants.</p>
<p>Overall, the LLaMA model also demonstrated a propensity to answer particularly short introductory greetings with a list of answers to the example questions provided in the prompt, a factor which may have influenced the scores.</p>
<h1>4.5 Dialogue examples</h1>
<p>Dialogues 8 and 9 provide two additional examples of interactions, respectively based on the GPT-4 model relying on semantic triples and the GPT-4 model using the verbalized graph.</p>
<p>As illustrated in Dialogue 8, one peculiarity of the responses based on semantic triples was the difficulty of providing information about seen objects in an area even when recalling it in other locations. This kind of error may relate to the model needing to attend to a number of individual facts about the plants in the data at once, thus overlooking facts where the information is available. While the information necessary to determine the chronological ordering of locations visited, time spent in locations, and distances traveled are present in the triple-structured information, the results suggest that it is difficult for the model to fully make use of this information.</p>
<h2>5. Discussion</h2>
<h3>5.1 Assessment of factuality</h3>
<p>Variability in how participants approached the tour and what was shown to the robot also affected the results. With regard to the first question, participants appeared to differ in their criteria of whether a statement by the robot was "correct" or not. A score of " 5 " being "Always" factual could</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>https://bambinos.github.io/bambi/</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
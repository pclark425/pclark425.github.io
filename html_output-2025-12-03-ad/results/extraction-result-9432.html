<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9432 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9432</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9432</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-d0086b86103a620a86bc918746df0aa642e2a8a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3" target="_blank">Language Models as Knowledge Bases?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> An in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models finds that BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge.</p>
                <p><strong>Paper Abstract:</strong> Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9432.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9432.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual cloze templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manually defined cloze-style templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study queries pretrained LMs by converting facts into manually authored cloze sentences (templates) per relation (e.g., "[S] was born in [O]") and measures how template wording affects retrieval of factual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340 M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA cloze probing (Google-RE, T-REx, ConceptNet, SQuAD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Probe whether pretrained language models can recover (subject, relation, object) facts by predicting masked single-token objects in manually defined cloze sentences derived from multiple knowledge sources.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Single-token cloze queries using manually defined templates for each relation (e.g., "[S] was born in [O]"); models predict the masked token (BERT masked-token objective) or next-token probabilities for unidirectional models. Templates were chosen by hand per relation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Alternative phrasings/templates of the same relation (the paper samples different aligned sentences / templates per fact to measure sensitivity); also compared masked cloze format (BERT) to next-token generation formats used by unidirectional models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>P@1 across corpora for BERT-large: Google-RE total P@1 = 10.5%; T-REx total P@1 = 32.3%; ConceptNet P@1 = 19.2%; SQuAD P@1 = 17.4%. (See Table 2 in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>For the same cloze tasks BERT-base: Google-RE 9.8%, T-REx 31.1%, ConceptNet 15.6%, SQuAD 14.1%. Other models (examples): fairseq-fconv T-REx 8.9% P@1; Transformer-XL T-REx 18.3% P@1; ELMo 5.5B T-REx 7.1% P@1. Frequency baseline and RE baselines shown in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not reported as a single aggregate numeric effect for template wording; the paper reports that choice of template can change performance (both better and worse) and therefore templates define a lower bound for what LMs 'know'.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors argue templates matter because different surface realizations of relations can elicit different model outputs; language models may rely on co-occurrence and phrasing seen in training data, so templates that match training-sentence phrasing yield better retrieval. They frame results as a lower bound because knowledge might be present but not accessed by a given template.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Templates manually defined per relation (see paper); only single-token objects considered; vocabulary restricted to intersection across models (~21k case-sensitive tokens) to make rankings comparable. Knowledge sources: Google-RE (5.5k facts), T-REx (34k facts, 41 relations, subsampled <=1000 per relation), ConceptNet (11.5k facts), SQuAD subset (305 single-token QA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9432.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9432.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Framing / phrasing variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to query framing (multiple aligned sentences per fact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper measures how different natural-language realizations (framing) of the same fact affect ranking of the correct object by sampling many aligned sentences per fact and observing ranking variability across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340 M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA template robustness evaluation (T-REx)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For a subset of facts, the paper samples up to 10 different aligned Wikipedia sentences mentioning the fact, masks the object in each sentence, and asks the model to predict the object to measure sensitivity to framing.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Cloze queries formed from 10 different aligned sentences per fact (different surface realizations / framings), masking the object token in each sentence; models produce ranking of candidate tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison across models: BERT-base, BERT-large, ELMo 5.5B, ELMo original, Transformer-XL, fairseq-fconv — measured variability of ranks across the 10 framings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BERT models and ELMo 5.5B show the lowest variability and rank correct object close to top on average. (Quantitatively: BERT-large overall T-REx P@k: correct object in top-10 ≈60%, top-100 ≈80% as reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ELMo original performance close to BERT but with slightly higher variability; fairseq-fconv and Transformer-XL show higher variability across framings. Exact per-model rank distributions are shown in Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not summarized as a single numeric effect size; qualitative and plotted differences show that bidirectional / large models (BERT-large, ELMo 5.5B) are less sensitive to framing than fairseq-fconv and Transformer-XL.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that models trained on more Wikipedia or larger corpora may have seen more of the particular phrasings; BERT's bidirectional masked objective and training data size may make it more robust to different surface forms. High confidence (log-probability) predictions correlate strongly with correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>For analysis, they selected up to 100 random facts per relation and for each fact randomly selected 10 aligned Wikipedia sentences (excluding facts with <10 alignments). The average rank of correct object across the 10 mentions was computed per model; Figure 4 presents distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9432.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9432.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Masked vs next-token format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked-token (bidirectional) cloze vs unidirectional next-token generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper contrasts BERT-style masked cloze querying with unidirectional next-token language model generation formats by evaluating models with different objectives on the same cloze tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large vs unidirectional models (Transformer-XL, fairseq-fconv)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-large 340M; Transformer-XL 257M; fairseq-fconv 324M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA cloze probing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess factual and commonsense knowledge by measuring model ability to predict masked single-token objects; for unidirectional models, use forward/backward probabilities appropriately (next-token) to score candidate tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Masked cloze (BERT): mask object token and use output vector at mask position into softmax. Unidirectional: use output before token to compute next-token softmax (or forward/backward combination for ELMo). All models rank candidates from unified vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison of architectures/objectives on same cloze templates: BERT (masked) vs Transformer-XL and fairseq-fconv (unidirectional) and ELMo (bidirectional LSTM with forward/backward probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BERT-large outperforms other models on most corpora: e.g., T-REx P@1 BERT-large = 32.3% vs Transformer-XL = 18.3% and fairseq-fconv = 8.9%. Google-RE P@1 BERT-large = 10.5% vs Txl = 1.6% and Fs = 2.6%. ConceptNet P@1 Bl = 19.2% vs Txl = 5.7% and Fs = 3.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ELMo 5.5B sometimes close to BERT on variability/robustness (e.g., lower variability), but absolute P@1 values of BERT-large generally higher. See Table 2 for per-model P@1 across corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large: examples above show BERT-large improvements of ~14 percentage points over Transformer-XL on T-REx P@1 (32.3% vs 18.3%) and much larger over fairseq-fconv (32.3% vs 8.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that BERT's bidirectional masked objective and training on large corpora make it better at storing and recalling relational knowledge and more robust to phrasing than unidirectional next-token models. Also higher log-probability correlates with correctness in BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Unified candidate vocabulary (intersection across models ~21k tokens). For unidirectional LMs, used forward/backward outputs as appropriate (ELMo averaged forward/backward probs). Models evaluated without any fine-tuning. Training corpora differ per model (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9432.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9432.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-token vs multi-token targets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-token object-slot limitation vs multi-token decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper restricts evaluation to single-token objects in cloze probes, noting multi-token answer generation adds extra decoding design choices and would confound measurement of stored knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to all evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA cloze probing (single-token object prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict single-token object in masked cloze statements; multi-token objects are excluded to avoid decoding hyperparameter confounds and because bidirectional models lack a straightforward multi-token decoding regime.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Single-token cloze (masked or next-token depending on model); multi-token answers explicitly excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not applicable — multi-token performances were not evaluated. All reported metrics (P@1, P@k) are computed only for single-token objects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors explain that evaluating multi-token objects would introduce beam size, length normalization, repetition penalties, and other decoding hyperparameters that would obscure whether performance differences stem from stored knowledge or generation details; they therefore limit to single-token to measure stored knowledge more cleanly.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Multi-token objects excluded across all knowledge sources; relations producing mainly multi-token objects (in Google-RE) were excluded. The authors note multi-token decoding for bidirectional models is an active research area and out of scope.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9432.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9432.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vocabulary intersection constraint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified candidate vocabulary (intersection across model vocabularies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To fairly compare models trained with different vocabularies, the study restricts candidate object tokens to the intersection of all model vocabularies (~21k case-sensitive tokens), noting vocabulary size can affect difficulty of ranking the correct token.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to all evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LAMA cloze probing (ranking over unified vocabulary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models rank the ground-truth object against every word in a shared candidate vocabulary (the intersection of individual model vocabularies) to compute ranking-based metrics (P@k).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Ranking over a unified vocabulary (intersection of model vocabularies, ~21k tokens) rather than each model's native vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>All reported P@1/P@k metrics are computed over this unified vocabulary. Specific model P@1 values are reported in Table 2 (e.g., T-REx BERT-large P@1 = 32.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not quantified numerically, but authors state that larger vocabularies make the ranking task harder; thus restricting to the intersection reduces this confound.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Vocabulary size differences can influence P@1 because a larger candidate set lowers the prior probability of any single token ranking highly; the unified vocabulary enforces a fairer comparison by equalizing candidate set size.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Intersection vocabulary derived from all considered models' vocabularies, resulting in ~21k case-sensitive tokens used as candidate set for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9432.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9432.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cloze-formatted QA (SQuAD) vs DrQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mapping SQuAD questions to cloze prompts and comparison to supervised open-domain QA (DrQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper converts a subset of SQuAD questions with single-token answers into cloze templates and compares unsupervised LM predictions to a supervised DrQA system to assess open-domain QA performance under the cloze presentation format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large (unsupervised) and DrQA (supervised baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-large 340 M; DrQA baseline unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain cloze-style QA (SQuAD subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>305 context-insensitive SQuAD questions with single-token answers were manually rewritten as cloze sentences (e.g., 'The theory of relativity was developed by ___') and models asked to fill the blank.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Cloze-style QA (manually rewritten SQuAD questions into 'fill-in-the-blank' templates) with single-token answer constraint; BERT predicts masked token; DrQA uses IR + neural reader constrained to single-token answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>DrQA supervised open-domain QA pipeline (TF/IDF retrieval + reading comprehension) vs unsupervised BERT-large cloze predictions directly from pretrained weights (no IR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>P@1: DrQA = 37.5% (supervised). BERT-large = 17.4% P@1 on the cloze-formatted SQuAD subset. P@10: BERT-large = 57.1% vs DrQA = 63.5% (reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Although P@1 gap is substantial (37.5% vs 17.4%), P@10 gap is much smaller (63.5% vs 57.1%), indicating BERT often ranks correct answer within top-10 despite not being supervised or having an IR step.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>P@1 difference = 20.1 percentage points in favor of DrQA; P@10 difference = 6.4 percentage points (DrQA higher).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors emphasize that BERT-large achieves these results without supervision or IR, implying pretrained LMs store non-trivial amounts of QA-relevant information and that cloze formulation allows direct extraction; they suggest top-k outputs can be useful for downstream systems even if top-1 is wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>305 single-token QA examples from SQuAD development set manually rewritten as cloze; DrQA constrained to single-token answers; BERT not fine-tuned and not given retrieval context; metrics reported: P@1 and P@10.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>Deep contextualized word representations <em>(Rating: 2)</em></li>
                <li>Improving language understanding by generative pre-training <em>(Rating: 2)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 2)</em></li>
                <li>T-rex: A large scale alignment of natural language with knowledge base triples <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9432",
    "paper_id": "paper-d0086b86103a620a86bc918746df0aa642e2a8a3",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Manual cloze templates",
            "name_full": "Manually defined cloze-style templates",
            "brief_description": "The study queries pretrained LMs by converting facts into manually authored cloze sentences (templates) per relation (e.g., \"[S] was born in [O]\") and measures how template wording affects retrieval of factual knowledge.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_size": "340 M",
            "task_name": "LAMA cloze probing (Google-RE, T-REx, ConceptNet, SQuAD)",
            "task_description": "Probe whether pretrained language models can recover (subject, relation, object) facts by predicting masked single-token objects in manually defined cloze sentences derived from multiple knowledge sources.",
            "presentation_format": "Single-token cloze queries using manually defined templates for each relation (e.g., \"[S] was born in [O]\"); models predict the masked token (BERT masked-token objective) or next-token probabilities for unidirectional models. Templates were chosen by hand per relation.",
            "comparison_format": "Alternative phrasings/templates of the same relation (the paper samples different aligned sentences / templates per fact to measure sensitivity); also compared masked cloze format (BERT) to next-token generation formats used by unidirectional models.",
            "performance": "P@1 across corpora for BERT-large: Google-RE total P@1 = 10.5%; T-REx total P@1 = 32.3%; ConceptNet P@1 = 19.2%; SQuAD P@1 = 17.4%. (See Table 2 in paper.)",
            "performance_comparison": "For the same cloze tasks BERT-base: Google-RE 9.8%, T-REx 31.1%, ConceptNet 15.6%, SQuAD 14.1%. Other models (examples): fairseq-fconv T-REx 8.9% P@1; Transformer-XL T-REx 18.3% P@1; ELMo 5.5B T-REx 7.1% P@1. Frequency baseline and RE baselines shown in Table 2.",
            "format_effect_size": "Not reported as a single aggregate numeric effect for template wording; the paper reports that choice of template can change performance (both better and worse) and therefore templates define a lower bound for what LMs 'know'.",
            "explanation_or_hypothesis": "The authors argue templates matter because different surface realizations of relations can elicit different model outputs; language models may rely on co-occurrence and phrasing seen in training data, so templates that match training-sentence phrasing yield better retrieval. They frame results as a lower bound because knowledge might be present but not accessed by a given template.",
            "null_or_negative_result": false,
            "experimental_details": "Templates manually defined per relation (see paper); only single-token objects considered; vocabulary restricted to intersection across models (~21k case-sensitive tokens) to make rankings comparable. Knowledge sources: Google-RE (5.5k facts), T-REx (34k facts, 41 relations, subsampled &lt;=1000 per relation), ConceptNet (11.5k facts), SQuAD subset (305 single-token QA).",
            "uuid": "e9432.0",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Framing / phrasing variability",
            "name_full": "Sensitivity to query framing (multiple aligned sentences per fact)",
            "brief_description": "The paper measures how different natural-language realizations (framing) of the same fact affect ranking of the correct object by sampling many aligned sentences per fact and observing ranking variability across models.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_size": "340 M",
            "task_name": "LAMA template robustness evaluation (T-REx)",
            "task_description": "For a subset of facts, the paper samples up to 10 different aligned Wikipedia sentences mentioning the fact, masks the object in each sentence, and asks the model to predict the object to measure sensitivity to framing.",
            "presentation_format": "Cloze queries formed from 10 different aligned sentences per fact (different surface realizations / framings), masking the object token in each sentence; models produce ranking of candidate tokens.",
            "comparison_format": "Comparison across models: BERT-base, BERT-large, ELMo 5.5B, ELMo original, Transformer-XL, fairseq-fconv — measured variability of ranks across the 10 framings.",
            "performance": "BERT models and ELMo 5.5B show the lowest variability and rank correct object close to top on average. (Quantitatively: BERT-large overall T-REx P@k: correct object in top-10 ≈60%, top-100 ≈80% as reported.)",
            "performance_comparison": "ELMo original performance close to BERT but with slightly higher variability; fairseq-fconv and Transformer-XL show higher variability across framings. Exact per-model rank distributions are shown in Figure 4.",
            "format_effect_size": "Not summarized as a single numeric effect size; qualitative and plotted differences show that bidirectional / large models (BERT-large, ELMo 5.5B) are less sensitive to framing than fairseq-fconv and Transformer-XL.",
            "explanation_or_hypothesis": "Authors note that models trained on more Wikipedia or larger corpora may have seen more of the particular phrasings; BERT's bidirectional masked objective and training data size may make it more robust to different surface forms. High confidence (log-probability) predictions correlate strongly with correctness.",
            "null_or_negative_result": false,
            "experimental_details": "For analysis, they selected up to 100 random facts per relation and for each fact randomly selected 10 aligned Wikipedia sentences (excluding facts with &lt;10 alignments). The average rank of correct object across the 10 mentions was computed per model; Figure 4 presents distributions.",
            "uuid": "e9432.1",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Masked vs next-token format",
            "name_full": "Masked-token (bidirectional) cloze vs unidirectional next-token generation",
            "brief_description": "The paper contrasts BERT-style masked cloze querying with unidirectional next-token language model generation formats by evaluating models with different objectives on the same cloze tasks.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "use",
            "model_name": "BERT-large vs unidirectional models (Transformer-XL, fairseq-fconv)",
            "model_size": "BERT-large 340M; Transformer-XL 257M; fairseq-fconv 324M",
            "task_name": "LAMA cloze probing",
            "task_description": "Assess factual and commonsense knowledge by measuring model ability to predict masked single-token objects; for unidirectional models, use forward/backward probabilities appropriately (next-token) to score candidate tokens.",
            "presentation_format": "Masked cloze (BERT): mask object token and use output vector at mask position into softmax. Unidirectional: use output before token to compute next-token softmax (or forward/backward combination for ELMo). All models rank candidates from unified vocabulary.",
            "comparison_format": "Direct comparison of architectures/objectives on same cloze templates: BERT (masked) vs Transformer-XL and fairseq-fconv (unidirectional) and ELMo (bidirectional LSTM with forward/backward probabilities).",
            "performance": "BERT-large outperforms other models on most corpora: e.g., T-REx P@1 BERT-large = 32.3% vs Transformer-XL = 18.3% and fairseq-fconv = 8.9%. Google-RE P@1 BERT-large = 10.5% vs Txl = 1.6% and Fs = 2.6%. ConceptNet P@1 Bl = 19.2% vs Txl = 5.7% and Fs = 3.6%.",
            "performance_comparison": "ELMo 5.5B sometimes close to BERT on variability/robustness (e.g., lower variability), but absolute P@1 values of BERT-large generally higher. See Table 2 for per-model P@1 across corpora.",
            "format_effect_size": "Large: examples above show BERT-large improvements of ~14 percentage points over Transformer-XL on T-REx P@1 (32.3% vs 18.3%) and much larger over fairseq-fconv (32.3% vs 8.9%).",
            "explanation_or_hypothesis": "Authors hypothesize that BERT's bidirectional masked objective and training on large corpora make it better at storing and recalling relational knowledge and more robust to phrasing than unidirectional next-token models. Also higher log-probability correlates with correctness in BERT.",
            "null_or_negative_result": false,
            "experimental_details": "Unified candidate vocabulary (intersection across models ~21k tokens). For unidirectional LMs, used forward/backward outputs as appropriate (ELMo averaged forward/backward probs). Models evaluated without any fine-tuning. Training corpora differ per model (Table 1).",
            "uuid": "e9432.2",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Single-token vs multi-token targets",
            "name_full": "Single-token object-slot limitation vs multi-token decoding",
            "brief_description": "The paper restricts evaluation to single-token objects in cloze probes, noting multi-token answer generation adds extra decoding design choices and would confound measurement of stored knowledge.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "use",
            "model_name": "applies to all evaluated models",
            "model_size": null,
            "task_name": "LAMA cloze probing (single-token object prediction)",
            "task_description": "Predict single-token object in masked cloze statements; multi-token objects are excluded to avoid decoding hyperparameter confounds and because bidirectional models lack a straightforward multi-token decoding regime.",
            "presentation_format": "Single-token cloze (masked or next-token depending on model); multi-token answers explicitly excluded.",
            "comparison_format": null,
            "performance": "Not applicable — multi-token performances were not evaluated. All reported metrics (P@1, P@k) are computed only for single-token objects.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors explain that evaluating multi-token objects would introduce beam size, length normalization, repetition penalties, and other decoding hyperparameters that would obscure whether performance differences stem from stored knowledge or generation details; they therefore limit to single-token to measure stored knowledge more cleanly.",
            "null_or_negative_result": null,
            "experimental_details": "Multi-token objects excluded across all knowledge sources; relations producing mainly multi-token objects (in Google-RE) were excluded. The authors note multi-token decoding for bidirectional models is an active research area and out of scope.",
            "uuid": "e9432.3",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Vocabulary intersection constraint",
            "name_full": "Unified candidate vocabulary (intersection across model vocabularies)",
            "brief_description": "To fairly compare models trained with different vocabularies, the study restricts candidate object tokens to the intersection of all model vocabularies (~21k case-sensitive tokens), noting vocabulary size can affect difficulty of ranking the correct token.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "use",
            "model_name": "applies to all evaluated models",
            "model_size": null,
            "task_name": "LAMA cloze probing (ranking over unified vocabulary)",
            "task_description": "Models rank the ground-truth object against every word in a shared candidate vocabulary (the intersection of individual model vocabularies) to compute ranking-based metrics (P@k).",
            "presentation_format": "Ranking over a unified vocabulary (intersection of model vocabularies, ~21k tokens) rather than each model's native vocabulary.",
            "comparison_format": null,
            "performance": "All reported P@1/P@k metrics are computed over this unified vocabulary. Specific model P@1 values are reported in Table 2 (e.g., T-REx BERT-large P@1 = 32.3%).",
            "performance_comparison": null,
            "format_effect_size": "Not quantified numerically, but authors state that larger vocabularies make the ranking task harder; thus restricting to the intersection reduces this confound.",
            "explanation_or_hypothesis": "Vocabulary size differences can influence P@1 because a larger candidate set lowers the prior probability of any single token ranking highly; the unified vocabulary enforces a fairer comparison by equalizing candidate set size.",
            "null_or_negative_result": null,
            "experimental_details": "Intersection vocabulary derived from all considered models' vocabularies, resulting in ~21k case-sensitive tokens used as candidate set for ranking.",
            "uuid": "e9432.4",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Cloze-formatted QA (SQuAD) vs DrQA",
            "name_full": "Mapping SQuAD questions to cloze prompts and comparison to supervised open-domain QA (DrQA)",
            "brief_description": "The paper converts a subset of SQuAD questions with single-token answers into cloze templates and compares unsupervised LM predictions to a supervised DrQA system to assess open-domain QA performance under the cloze presentation format.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "use",
            "model_name": "BERT-large (unsupervised) and DrQA (supervised baseline)",
            "model_size": "BERT-large 340 M; DrQA baseline unspecified",
            "task_name": "Open-domain cloze-style QA (SQuAD subset)",
            "task_description": "305 context-insensitive SQuAD questions with single-token answers were manually rewritten as cloze sentences (e.g., 'The theory of relativity was developed by ___') and models asked to fill the blank.",
            "presentation_format": "Cloze-style QA (manually rewritten SQuAD questions into 'fill-in-the-blank' templates) with single-token answer constraint; BERT predicts masked token; DrQA uses IR + neural reader constrained to single-token answers.",
            "comparison_format": "DrQA supervised open-domain QA pipeline (TF/IDF retrieval + reading comprehension) vs unsupervised BERT-large cloze predictions directly from pretrained weights (no IR).",
            "performance": "P@1: DrQA = 37.5% (supervised). BERT-large = 17.4% P@1 on the cloze-formatted SQuAD subset. P@10: BERT-large = 57.1% vs DrQA = 63.5% (reported in text).",
            "performance_comparison": "Although P@1 gap is substantial (37.5% vs 17.4%), P@10 gap is much smaller (63.5% vs 57.1%), indicating BERT often ranks correct answer within top-10 despite not being supervised or having an IR step.",
            "format_effect_size": "P@1 difference = 20.1 percentage points in favor of DrQA; P@10 difference = 6.4 percentage points (DrQA higher).",
            "explanation_or_hypothesis": "Authors emphasize that BERT-large achieves these results without supervision or IR, implying pretrained LMs store non-trivial amounts of QA-relevant information and that cloze formulation allows direct extraction; they suggest top-k outputs can be useful for downstream systems even if top-1 is wrong.",
            "null_or_negative_result": false,
            "experimental_details": "305 single-token QA examples from SQuAD development set manually rewritten as cloze; DrQA constrained to single-token answers; BERT not fine-tuned and not given retrieval context; metrics reported: P@1 and P@10.",
            "uuid": "e9432.5",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "Deep contextualized word representations",
            "rating": 2
        },
        {
            "paper_title": "Improving language understanding by generative pre-training",
            "rating": 2
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 2
        },
        {
            "paper_title": "T-rex: A large scale alignment of natural language with knowledge base triples",
            "rating": 1
        }
    ],
    "cost": 0.01464975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models as Knowledge Bases?</h1>
<p>Fabio Petroni ${ }^{1}$ Tim Rocktäschel ${ }^{1,2}$ Patrick Lewis ${ }^{1,2}$ Anton Bakhtin ${ }^{1}$<br>Yuxiang Wu ${ }^{1,2}$ Alexander H. Miller ${ }^{1}$ Sebastian Riedel ${ }^{1,2}$<br>${ }^{1}$ Facebook AI Research<br>${ }^{2}$ University College London<br>{fabiopetroni, rockt, plewis, yolo, yuxiangwu, ahm, sriedel}@fb.com</p>
<h4>Abstract</h4>
<p>Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA.</p>
<h2>1 Introduction</h2>
<p>Recently, pretrained high-capacity language models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018a) have become increasingly important in NLP. They are optimised to either predict the next word in a sequence or some masked word anywhere in a given sequence (e.g. "Dante was born in [Mask] in the year 1265."). The parameters of these models appear to store
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Querying knowledge bases (KB) and language models (LM) for factual knowledge.
vast amounts of linguistic knowledge (Peters et al., 2018b; Goldberg, 2019; Tenney et al., 2019) useful for downstream tasks. This knowledge is usually accessed either by conditioning on latent context representations produced by the original model or by using the original model weights to initialize a task-specific model which is then further fine-tuned. This type of knowledge transfer is crucial for current state-of-the-art results on a wide range of tasks.</p>
<p>In contrast, knowledge bases are effective solutions for accessing annotated gold-standard relational data by enabling queries such as (Dante, born-in, $\mathbf{X}$ ). However, in practice we often need to extract relational data from text or other modalities to populate these knowledge bases. This requires complex NLP pipelines involving entity extraction, coreference resolution, entity linking and relation extraction (Surdeanu and Ji, 2014)components that often need supervised data and fixed schemas. Moreover, errors can easily propagate and accumulate throughout the pipeline. Instead, we could attempt to query neural language models for relational data by asking them to fill in masked tokens in sequences like "Dante was born</p>
<p>in [Mask]", as illustrated in Figure 1. In this setting, language models come with various attractive properties: they require no schema engineering, do not need human annotations, and they support an open set of queries.</p>
<p>Given the above qualities of language models as potential representations of relational knowledge, we are interested in the relational knowledge already present in pretrained off-the-shelf language models such as ELMo and BERT. How much relational knowledge do they store? How does this differ for different types of knowledge such as facts about entities, common sense, and general question answering? How does their performance without fine-tuning compare to symbolic knowledge bases automatically extracted from text? Beyond gathering a better general understanding of these models, we believe that answers to these questions can help us design better unsupervised knowledge representations that could transfer factual and commonsense knowledge reliably to downstream tasks such as commonsense (visual) question answering (Zellers et al., 2018; Talmor et al., 2019) or reinforcement learning (Branavan et al., 2011; Chevalier-Boisvert et al., 2018; Bahdanau et al., 2019; Luketina et al., 2019).</p>
<p>For the purpose of answering the above questions we introduce the LAMA (LAnguage Model Analysis) probe, consisting of a set of knowledge sources, each comprised of a set of facts. We define that a pretrained language model knows a fact (subject, relation, object) such as (Dante, born-in, Florence) if it can successfully predict masked objects in cloze sentences such as "Dante was born in $\qquad$ " expressing that fact. We test for a variety of types of knowledge: relations between entities stored in Wikidata, common sense relations between concepts from ConceptNet, and knowledge necessary to answer natural language questions in SQuAD. In the latter case we manually map a subset of SQuAD questions to cloze sentences.</p>
<p>Our investigation reveals that (i) the largest BERT model from Devlin et al. (2018b) (BERT-large) captures (accurate) relational knowledge comparable to that of a knowledge base extracted with an off-the-shelf relation extractor and an oracle-based entity linker from a corpus known to express the relevant knowledge, (ii) factual knowledge can be recovered surprisingly well from pretrained language mod-
els, however, for some relations (particularly $N$-to- $M$ relations) performance is very poor, (iii) BERT-large consistently outperforms other language models in recovering factual and commonsense knowledge while at the same time being more robust to the phrasing of a query, and (iv) BERT-large achieves remarkable results for open-domain QA, reaching $57.1 \%$ precision@10 compared to $63.5 \%$ of a knowledge base constructed using a task-specific supervised relation extraction system.</p>
<h2>2 Background</h2>
<p>In this section we provide background on language models. Statistics for the models that we include in our investigation are summarized in Table 1.</p>
<h3>2.1 Unidirectional Language Models</h3>
<p>Given an input sequence of tokens $\mathbf{w}=$ $\left[w_{1}, w_{2}, \ldots, w_{N}\right]$, unidirectional language models commonly assign a probability $p(\mathbf{w})$ to the sequence by factorizing it as follows</p>
<p>$$
p(\mathbf{w})=\prod_{t} p\left(w_{t} \mid w_{t-1}, \ldots, w_{1}\right)
$$</p>
<p>A common way to estimate this probability is using neural language models (Mikolov and Zweig, 2012; Melis et al., 2017; Bengio et al., 2003) with</p>
<p>$$
p\left(w_{t} \mid w_{t-1}, \ldots, w_{1}\right)=\operatorname{softmax}\left(\mathbf{W} \mathbf{h}_{t}+\mathbf{b}\right)
$$</p>
<p>where $\mathbf{h}<em t="t">{t} \in \mathbb{R}^{k}$ is the output vector of a neural network at position $t$ and $\mathbf{W} \in \mathbb{R}^{|\mathcal{V}| \times k}$ is a learned parameter matrix that maps $\mathbf{h}</em>$ given the word history, e.g., by using a multi-layer perceptron (Bengio et al., 2003; Mikolov and Zweig, 2012), convolutional layers (Dauphin et al., 2017), recurrent neural networks (Zaremba et al., 2014; Merity et al., 2016; Melis et al., 2017) or self-attention mechanisms (Radford et al., 2018; Dai et al., 2019; Radford et al., 2019).
fairseq-fconv: Instead of commonly used recurrent neural networks, Dauphin et al. (2017) use multiple layers of gated convolutions. We use the pretrained model in the fairseq ${ }^{1}$ library in our study. It has been trained on the WikiText-103 corpus introduced by Merity et al. (2016).}$ to unnormalized scores for every word in the vocabulary $\mathcal{V}$. Various neural language models then mainly differ in how they compute $\mathbf{h}_{t</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">#Parameters</th>
<th style="text-align: center;">Training Corpus</th>
<th style="text-align: center;">Corpus Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">fairseq-fconv (Dauphin et al., 2017)</td>
<td style="text-align: center;">ConvNet</td>
<td style="text-align: center;">324 M</td>
<td style="text-align: center;">WikiText-103</td>
<td style="text-align: center;">103 M Words</td>
</tr>
<tr>
<td style="text-align: left;">Transformer-XL (large) (Dai et al., 2019)</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">257 M</td>
<td style="text-align: center;">WikiText-103</td>
<td style="text-align: center;">103 M Words</td>
</tr>
<tr>
<td style="text-align: left;">ELMo (original) (Peters et al., 2018a)</td>
<td style="text-align: center;">BiLSTM</td>
<td style="text-align: center;">93.6 M</td>
<td style="text-align: center;">Google Billion Word</td>
<td style="text-align: center;">800 M Words</td>
</tr>
<tr>
<td style="text-align: left;">ELMo 5.5B (Peters et al., 2018a)</td>
<td style="text-align: center;">BiLSTM</td>
<td style="text-align: center;">93.6 M</td>
<td style="text-align: center;">Wikipedia (en) \&amp; WMT 2008-2012</td>
<td style="text-align: center;">5.5 B Words</td>
</tr>
<tr>
<td style="text-align: left;">BERT (base) (Devlin et al., 2018a)</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">Wikipedia (en) \&amp; BookCorpus</td>
<td style="text-align: center;">3.3 B Words</td>
</tr>
<tr>
<td style="text-align: left;">BERT (large) (Devlin et al., 2018a)</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">340 M</td>
<td style="text-align: center;">Wikipedia (en) \&amp; BookCorpus</td>
<td style="text-align: center;">3.3 B Words</td>
</tr>
</tbody>
</table>
<p>Table 1: Language models considered in this study.</p>
<p>Transformer-XL: Dai et al. (2019) introduce a large-scale language model based on the Transformer (Vaswani et al., 2017). Transformer-XL can take into account a longer history by caching previous outputs and by using relative instead of absolute positional encoding. It achieves a test perplexity of 18.3 on the WikiText-103 corpus.</p>
<h3>2.2 Bidirectional "Language Models"</h3>
<p>So far, we have looked at language models that predict the next word given a history of words. However, in many downstream applications we mostly care about having access to contextual representations of words, i.e., word representations that are a function of the entire context of a unit of text such as a sentence or paragraph, and not only conditioned on previous words. Formally, given an input sequence $\mathbf{w}=\left[w_{1}, w_{2}, \ldots, w_{N}\right]$ and a position $1 \leq i \leq N$, we want to estimate $p\left(w_{i}\right)=p\left(w_{i} \mid w_{1}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{N}\right)$ using the left and right context of that word.
ELMo: To estimate this probability, Peters et al. (2018a) propose running a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), resulting in $\overline{\mathbf{h}}<em i="i">{i}$ and $\overline{\mathbf{h}}</em>$ which consequently are used to calculate a forward and backward language model log-likelihood. Their model, ELMo, uses multiple layers of LSTMs and it has been pretrained on the Google Billion Word dataset. Another version of the model, ELMo 5.5B, has been trained on the English Wikipedia and monolingual news crawl data from WMT 2008-2012.
BERT: Instead of a standard language model objective, Devlin et al. (2018a) propose to sample positions in the input sequence randomly and to learn to fill the word at the masked position. To this end, they employ a Transformer architecture and train it on the BookCorpus (Zhu et al., 2015) as well as a crawl of English Wikipedia. In addi-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion to this pseudo language model objective, they use an auxiliary binary classification objective to predict whether a particular sentence follows the given sequence of words.</p>
<h2>3 Related Work</h2>
<p>Many studies have investigated pretrained word representations, sentence representations, and language models. Existing work focuses on understanding linguistic and semantic properties of word representations or how well pretrained sentence representations and language models transfer linguistic knowledge to downstream tasks. In contrast, our investigation seeks to answer to what extent pretrained language models store factual and commonsense knowledge by comparing them with symbolic knowledge bases populated by traditional relation extraction approaches.</p>
<p>Baroni et al. (2014) present a systematic comparative analysis between neural word representation methods and more traditional count-based distributional semantic methods on lexical semantics tasks like semantic relatedness and concept categorization. They find that neural word representations outperform count-based distributional methods on the majority of the considered tasks. Hill et al. (2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs.</p>
<p>Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset consists of sentence pairs with a grammatical and an ungrammatical sentence. While a good language model should assign higher probability to the grammatical sentence, they find that LSTMs do not learn syntax well.</p>
<p>Another line of work investigates the ability of pretrained sentence and language models to transfer knowledge to downstream natural language understanding tasks (Wang et al., 2018). While such an analysis sheds light on the transfer-learning</p>
<p>abilities of pretrained models for understanding short pieces of text, it provides little insight into whether these models can compete with traditional approaches to representing knowledge like symbolic knowledge bases.</p>
<p>More recently, McCoy et al. (2019) found that for natural language inference, a model based on BERT learns to rely heavily on fallible syntactic heuristics instead of a deeper understanding of the natural language input. Peters et al. (2018b) found that lower layers in ELMo specialize on local syntactic relationships, while higher layers can learn to model long-range relationships. Similarly, Goldberg (2019) found that BERT captures English syntactic phenomena remarkably well. Tenney et al. (2019) investigate to what extent language models encode sentence structure for different syntactic and semantic phenomena and found that they excel for the former but only provide small improvements for tasks that fall into the latter category. While this provides insights into the linguistic knowledge of language models, it does not provide insights into their factual and commonsense knowledge.</p>
<p>Radford et al. (2018) introduce a pretrained language model based on the Transformer which they termed generative pretraining (GPTv1). The first version of GPT (Radford et al., 2018) has been trained on the Book Corpus (Zhu et al., 2015) containing 7000 books. The closest to our investigation is the work by Radford et al. (2019) which introduces GPTv2 and investigates how well their language model does zero-shot transfer to a range of downstream tasks. They find that GPTv2 achieves an $F_{1}$ of 55 for answering questions in CoQA (Reddy et al., 2018) and $4.1 \%$ accuracy on the Natural Questions dataset (Kwiatkowski et al., 2019), in both cases without making use of annotated question-answer pairs or an information retrieval step. While these results are encouraging and hint at the ability of very large pretrained language models to memorize factual knowledge, the large GPTv2 model has not been made public and the publicly available small version achieves less than $1 \%$ on Natural Questions ( 5.3 times worse than the large model). Thus, we decided to not include GPTv2 in our study. Similarly, we do not include GPTv1 in this study as it uses a limited lower-cased vocabulary, making it incompatible to the way we assess the other language models.</p>
<h2>4 The LAMA Probe</h2>
<p>We introduce the LAMA (LAnguage Model Analysis) probe to test the factual and commonsense knowledge in language models. It provides a set of knowledge sources which are composed of a corpus of facts. Facts are either subject-relationobject triples or question-answer pairs. Each fact is converted into a cloze statement which is used to query the language model for a missing token. We evaluate each model based on how highly it ranks the ground truth token against every other word in a fixed candidate vocabulary. This is similar to ranking-based metrics from the knowledge base completion literature (Bordes et al., 2013; Nickel et al., 2016). Our assumption is that models which rank ground truth tokens high for these cloze statements have more factual knowledge. We discuss each step in detail next and provide considerations on the probe below.</p>
<h3>4.1 Knowledge Sources</h3>
<p>To assess the different language models in Section 2, we cover a variety of sources of factual and commonsense knowledge. For each source, we describe the origin of fact triples (or questionanswer pairs), how we transform them into cloze templates, and to what extent aligned texts exist in Wikipedia that are known to express a particular fact. We use the latter information in supervised baselines that extract knowledge representations directly from the aligned text.</p>
<h3>4.1.1 Google-RE</h3>
<p>The Google-RE corpus ${ }^{3}$ contains $\sim 60 \mathrm{~K}$ facts manually extracted from Wikipedia. It covers five relations but we consider only three of them, namely "place of birth", "date of birth" and "place of death". We exclude the other two because they contain mainly multi-tokens objects that are not supported in our evaluation. We manually define a template for each considered relation, e.g., "[S] was born in [O]" for "place of birth". Each fact in the Google-RE dataset is, by design, manually aligned to a short piece of Wikipedia text supporting it.</p>
<h3>4.1.2 T-REx</h3>
<p>The T-REx knowledge source is a subset of Wikidata triples. It is derived from the T-REx</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>dataset (Elsahar et al., 2018) and is much larger than Google-RE with a broader set of relations. We consider 41 Wikidata relations and subsample at most 1000 facts per relation. As with the Google-RE corpus, we manually define a template for each relation (see Table 3 for some examples). In contrast to the Google-RE knowledge source, T-REx facts were automatically aligned to Wikipedia and hence this alignment can be noisy. However, Elsahar et al. (2018) report an accuracy of $97.8 \%$ for the alignment technique over a test set.</p>
<h3>4.1.3 ConceptNet</h3>
<p>ConceptNet (Speer and Havasi, 2012) is a multilingual knowledge base, initially built on top of Open Mind Common Sense (OMCS) sentences. OMCS represents commonsense relationships between words and/or phrases. We consider facts from the English part of ConceptNet that have single-token objects covering 16 relations. For these ConceptNet triples, we find the OMCS sentence that contains both the subject and the object. We then mask the object within the sentence and use the sentence as template for querying language models. If there are several sentences for a triple, we pick one at random. Note that for this knowledge source there is no explicit alignment of facts to Wikipedia sentences.</p>
<h3>4.1.4 SQuAD</h3>
<p>SQuAD (Rajpurkar et al., 2016) is a popular question answering dataset. We select a subset of 305 context-insensitive questions from the SQuAD development set with single token answers. We manually create cloze-style questions from these questions, e.g., rewriting "Who developed the theory of relativity?" as "The theory of relativity was developed by $\qquad$ ". For each question and answer pair, we know that the corresponding fact is expressed in Wikipedia since this is how SQuAD was created.</p>
<h3>4.2 Models</h3>
<p>We consider the following pretrained casesensitive language models in our study (see Table 1): fairseq-fconv (Fs), Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base $(B b)$ and BERT-large $(B l)$. We use the natural way of generating tokens for each model by following the definition of the training objective function.</p>
<p>Assume we want to compute the generation for the token at position $t$. For unidirectional language models, we use the network output $\left(\mathbf{h}<em t-1="t-1">{t-1}\right)$ just before the token to produce the output layer softmax. For ELMo we consider the output just before $\overrightarrow{\left(\mathbf{h}</em>}\right)}$ for the forward direction and just after $\left(\overrightarrow{\mathbf{h}<em t="t">{t+1}\right)$ for the backward direction. Following the loss definition in (Peters et al., 2018a), we average forward and backward probabilities from the corresponding softmax layers. For BERT, we mask the token at position $t$, and we feed the output vector corresponding to the masked token $\left(\mathbf{h}</em>$ case-sensitive tokens).}\right)$ into the softmax layer. To allow a fair comparison, we let models generate over a unified vocabulary, which is the intersection of the vocabularies for all considered models ( $\sim 21 \mathrm{~K</p>
<h3>4.3 Baselines</h3>
<p>To compare language models to canonical ways of using off-the-shelf systems for extracting symbolic knowledge and answering questions, we consider the following baselines.
Freq: For a subject and relation pair, this baseline ranks words based on how frequently they appear as objects for the given relation in the test data. It indicates the upper bound performance of a model that always predicts the same objects for a particular relation.
RE: For the relation-based knowledge sources, we consider the pretrained Relation Extraction (RE) model of Sorokin and Gurevych (2017). This model was trained on a subcorpus of Wikipedia annotated with Wikidata relations. It extracts relation triples from a given sentence using an LSTMbased encoder and an attention mechanism. Based on the alignment information from the knowledge sources, we provide the relation extractor with the sentences known to express the test facts. Using these datasets, RE constructs a knowledge graph of triples. At test time, we query this graph by finding the subject entity and then rank all objects in the correct relation based on the confidence scores returned by RE. We consider two versions of this procedure that differ in how the entity linking is implemented: $\mathbf{R E}<em o="o">{n}$ makes use of a naïve entity linking solution based on exact string matching, while $\mathbf{R E}</em>\right)$ from that sen-}$ uses an oracle for entity linking in addition to string matching. In other words, assume we query for the object $o$ of a test subjectrelation fact $(s, r, o)$ expressed in a sentence $x$. If RE has extracted any triple $\left(s^{\prime}, r, o^{\prime</p>
<p>tence $x, s^{\prime}$ will be linked to $s$ and $o^{\prime}$ to $o$. In practice, this means RE can return the correct solution $o$ if any relation instance of the right type was extracted from $x$, regardless of whether it has a wrong subject or object.
DrQA: Chen et al. (2017) introduce DrQA, a popular system for open-domain question answering. DrQA predicts answers to natural language questions using a two step pipeline. First, a TF/IDF information retrieval step is used to find relevant articles from a large store of documents (e.g. Wikipedia). On the retrieved top $k$ articles, a neural reading comprehension model then extracts answers. To avoid giving the language models a competitive advantage, we constrain the predictions of DrQA to single-token answers.</p>
<h3>4.4 Metrics</h3>
<p>We consider rank-based metrics and compute results per relation along with mean values across all relations. To account for multiple valid objects for a subject-relation pair (i.e., for N-M relations), we follow Bordes et al. (2013) and remove from the candidates when ranking at test time all other valid objects in the training data other than the one we test. We use the mean precision at $\mathrm{k}(P @ k)$. For a given fact, this value is 1 if the object is ranked among the top k results, and 0 otherwise.</p>
<h3>4.5 Considerations</h3>
<p>There are several important design decisions we made when creating the LAMA probe. Below we give more detailed justifications for these decisions.</p>
<p>Manually Defined Templates For each relation we manually define a template that queries for the object slot in that relation. One can expect that the choice of templates has an impact on the results, and this is indeed the case: for some relations we find both worse and better ways to query for the same information (with respect to a given model) by using an alternate template. We argue that this means we are measuring a lower bound for what language models know. We make this argument by analogy with traditional knowledge bases: they only have a single way of querying knowledge for a specific relation, namely by using the relation id of that relation, and this way is used to measure their accuracy. For example, if the relation ID is works-For and the user asks for is-working-for, the accuracy of the KG would
be 0 .
Single Token We only consider single token objects as our prediction targets. The reason we include this limitation is that multi-token decoding adds a number of additional tuneable parameters (beam size, candidate scoring weights, length normalization, n-gram repetition penalties, etc.) that obscure the knowledge we are trying to measure. Moreover, well-calibrated multi-token generation is still an active research area, particularly for bidirectional models (see e.g. Welleck et al. (2019)).</p>
<p>Object Slots We choose to only query object slots in triples, as opposed to subject or relation slots. By including reverse relations (e.g. contains and contained-by) we can also query subject slots. We do not query relation slots for two reasons. First, surface form realisations of relations will span several tokens, and as we discussed above, this poses a technical challenge that is not in the scope of this work. Second, even if we could easily predict multi-token phrases, relations can generally be expressed with many different wordings, making it unclear what the gold standard pattern for a relation should be, and how to measure accuracy in this context.</p>
<p>Intersection of Vocabularies The models that we considered are trained with different vocabularies. For instance, ELMo uses a list of $\sim 800 \mathrm{~K}$ tokens while BERT considers only $\sim 30 \mathrm{~K}$ tokens. The size of the vocabulary can influence the performance of a model for the LAMA probe. Specifically, the larger the vocabulary the harder it would be to rank the gold token at the top. For this reason we considered a common vocabulary of $\sim 21 \mathrm{~K}$ case-sensitive tokens that are obtained from the intersection of the vocabularies for all considered models. To allow a fair comparison, we let every model rank only tokens in this joint vocabulary.</p>
<h2>5 Results</h2>
<p>We summarize the main results in Table 2, which shows the mean precision at one ( $\mathrm{P} @ 1$ ) for the different models across the set of corpora considered. In the remainder of this section, we discuss the results for each corpus in detail.</p>
<p>Google-RE We query the LMs using a standard cloze template for each relation. The base and large versions of BERT both outperform all other models by a substantial margin. Furthermore, they</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Statistics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Baselines</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">KB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Facts</td>
<td style="text-align: center;">#Rel</td>
<td style="text-align: center;">Freq</td>
<td style="text-align: center;">DrQA</td>
<td style="text-align: center;">$\mathrm{RE}_{n}$</td>
<td style="text-align: center;">$\mathrm{RE}_{o}$</td>
<td style="text-align: center;">Fs</td>
<td style="text-align: center;">Txl</td>
<td style="text-align: center;">Eb</td>
<td style="text-align: center;">E5B</td>
<td style="text-align: center;">Bb</td>
<td style="text-align: center;">Bl</td>
</tr>
<tr>
<td style="text-align: center;">Google-RE</td>
<td style="text-align: center;">birth-place</td>
<td style="text-align: center;">2937</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">16.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">birth-date</td>
<td style="text-align: center;">1825</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">death-place</td>
<td style="text-align: center;">765</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">5527</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">10.5</td>
</tr>
<tr>
<td style="text-align: center;">T-REx</td>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">937</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">74.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$N-1$</td>
<td style="text-align: center;">20006</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">23.85</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">34.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$N-M$</td>
<td style="text-align: center;">13096</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">21.95</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">34039</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">22.03</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;">ConceptNet</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">11458</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">17.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Mean precision at one (P@1) for a frequency baseline (Freq), DrQA, a relation extraction with naïve entity linking $\left(\mathrm{RE}<em o="o">{n}\right)$, oracle entity linking $\left(\mathrm{RE}</em>\right)$, fairseq-fconv (Fs), Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERT-large (Bl) across the set of evaluation corpora.
obtain a 2.2 and 2.9 respective average accuracy improvement over the oracle-based RE baseline. This is particularly surprising given that with the gold-aligned Google-RE source we know for certain that the oracle RE baseline has seen at least one sentence expressing each test fact. Moreover, the RE baseline was given substantial help through an entity linking oracle.</p>
<p>It is worth pointing out that while BERT-large does better, this does not mean it does so for the right reasons. Although the aligned Google-RE sentences are likely in its training set (as they are part of Wikipedia and BERT has been trained on Wikipedia), it might not "understand" them to produce these results. Instead, it could have learned associations of objects with subjects from co-occurrence patterns.</p>
<p>T-REx The knowledge source derived from Google-RE contains relatively few facts and only three relations. Hence, we perform experiments on the larger set of facts and relations in T-REx. We find that results are generally consistent with Google-RE. Again, the performance of BERT in retrieving factual knowledge are close to the performance obtained by automatically building a knowledge base with an off-the-shelf relation extraction system and oracle-based entity linking. Broken down by relation type, the performance of BERT is very high for 1-to-1 relations (e.g., capital of) and low for N-to-M relations.</p>
<p>Note that a downstream model could learn to make use of knowledge in the output representations of a language model even if the correct answer is not ranked first but high enough (i.e. a hint
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Mean P@k curve for T-REx varying k. Base10 log scale for X axis.
about the correct answer can be extracted from the output representation). Figure 2 shows the mean P@k curves for the considered models. For BERT, the correct object is ranked among the top ten in around $60 \%$ of the cases and among the top 100 in $80 \%$ of the cases.</p>
<p>To further investigate why BERT achieves such strong results, we compute the Pearson correlation coefficient between the $P @ 1$ and a set of metrics that we report in Figure 3. We notice, for instance, that the number of times an object is mentioned in the training data positively correlates with performance while the same is not true for the subject of a relation. Furthermore, the log probability of a prediction is strongly positively correlated with P@1. Thus, when BERT has a high confidence in its prediction, it is often correct. Performance is also positively correlated with the cosine similarity between subject and object vectors, and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Pearson correlation coefficient for the P@1 of the BERT-large model on T-REx and a set of metrics: SM and OM refer to the number of times a subject and an object are mentioned in the BERT training corpus<sup>5</sup> respectively; LPFP is the log probability score associated with the first prediction; SOCS is the cosine similarity between subject and object vectors (we use spaCy<sup>5</sup>); ST and SWP are the number of tokens in the subject with a standard tokenization and the BERT WordPiece tokenization respectively.</p>
<p>slightly with the number of tokens in the subject.</p>
<p>Table 3 shows randomly picked examples for the generation of BERT-large for cloze template queries. We find that BERT-large generally predicts objects of the correct type, even when the predicted object itself is not correct.</p>
<p>To understand how the performance of a pretrained language model varies with different ways of querying for a particular fact, we analyze a maximum of 100 random facts per relation for which we randomly select 10 aligned sentences in Wikipedia from T-REx.<sup>6</sup> In each of the sentences, we mask the object of the fact, and ask the model to predict it. For several of our language models this also tests their ability to memorize and recall sentences from the training data since as the models have been trained on Wikipedia (see Table 1).</p>
<p>Figure 4 shows the average distribution of the rank for ten queries per fact. The two BERT models and ELMo 5.5B exhibit the lowest variability while ranking the correct object close to the top on average. Surprisingly, the performance of ELMo original is not far from BERT, even though this model did not see Wikipedia during training. Fairseq-fconv and Transformer-XL experi-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average rank distribution for 10 different mentions of 100 random facts per relation in T-REx. ELMo 5.5B and both variants of BERT are least sensitive to the framing of the query but also are the most likely to have seen the query sentence during training.</p>
<p>ence a higher variability in their predictions. Note that BERT and ELMo 5.5B have been trained on a larger portion of Wikipedia than fairseq-fconv and Transformer-XL and may have seen more sentences containing the test queries during training.</p>
<p><strong>ConceptNet</strong> The results on the ConceptNet corpus are in line with those reported for retrieving factual knowledge in Google-RE and T-REx. The BERT-large model consistently achieves the best performance, and it is able to retrieve commonsense knowledge at a similar level to factual knowledge. The lower half of Table 3 shows generations by BERT-large for randomly sampled examples. Some of the concepts generated by the language models are surprisingly reasonable in addition to being syntactically correct.</p>
<p><strong>SQuAD</strong> Next we evaluate our system on open-domain cloze-style question answering and compare against the supervised DrQA model. Table 2 shows a performance gap between BERT-large and the DrQA open-domain QA system on our cloze SQuAD task. Again, note that the pretrained language model is completely unsupervised, it is not fine-tuned, and it has no access to a dedicated information retrieval system. Moreover, when comparing DrQA and BERT-large in terms of P@10, we find that gap is remarkably small (57.1 for BERT-large and 63.5 for DrQA).</p>
<h2>6 Discussion and Conclusion</h2>
<p>We presented a systematic analysis of the factual and commonsense knowledge in publicly available pretrained language models <em>as is</em> and found</p>
<p><sup>5</sup>The original training corpus is not available, we created our version using the same sources.</p>
<p><sup>5</sup>https://spacy.io</p>
<p><sup>6</sup>We exclude all facts with less than 10 alignments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P19</td>
<td style="text-align: center;">Francesco Bartolomeo Conti was born in $\qquad$</td>
<td style="text-align: center;">Florence</td>
<td style="text-align: center;">Rome (-1.6), Florence (-4.8), Naples (-1.9), Milan (-2.4), Bologna (-2.5)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P20</td>
<td style="text-align: center;">Adolphe Adam died in $\qquad$</td>
<td style="text-align: center;">Paris</td>
<td style="text-align: center;">Paris (-4.3), London (-3.5), Vienna (-3.6), Berlin (-3.8), Brussels (-4.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P279</td>
<td style="text-align: center;">English building is a subclass of $\qquad$</td>
<td style="text-align: center;">dog</td>
<td style="text-align: center;">dogs (-0.1), breeds (-2.2), dog (-2.4), cattle (-4.3), sheep (-4.5)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P37</td>
<td style="text-align: center;">The official language of Mauritius is $\qquad$</td>
<td style="text-align: center;">English</td>
<td style="text-align: center;">English (-4.6), French (-0.9), Arabic (-6.2), Tamil (-6.7), Malayalam (-7.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P413</td>
<td style="text-align: center;">Patrick Obreya plays in $\qquad$ position.</td>
<td style="text-align: center;">midfielder</td>
<td style="text-align: center;">centre (-2.0), center (-2.2), midfielder (-2.6), forward (-2.4), midfield (-2.7)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P138</td>
<td style="text-align: center;">Hamburg Airport is named after $\qquad$</td>
<td style="text-align: center;">Hamburg</td>
<td style="text-align: center;">Hess (-3.0), Hermann (-3.1), Schmidt (-3.1), Hamburg (-5.8), Ludwig (-3.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P364</td>
<td style="text-align: center;">The original language of Mon oncle Benjamin is $\qquad$</td>
<td style="text-align: center;">French</td>
<td style="text-align: center;">French (-0.2), Breton (-3.3), English (-3.8), Dutch (-4.2), German (-4.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P54</td>
<td style="text-align: center;">Dani Alves plays with $\qquad$</td>
<td style="text-align: center;">Barcelona</td>
<td style="text-align: center;">Santos (-2.4), Porto (-2.5), Sporting (-3.1), Brazil (-3.3), Portugal (-3.7)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P106</td>
<td style="text-align: center;">Paul Toungui is a $\qquad$ by profession .</td>
<td style="text-align: center;">politician</td>
<td style="text-align: center;">lawyer (-1.1), journalist (-2.4), teacher (-2.7), doctor (-3.0), physician (-3.7)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P527</td>
<td style="text-align: center;">Sodium sulfide consists of $\qquad$</td>
<td style="text-align: center;">sodium</td>
<td style="text-align: center;">water (-1.2), suffer (-1.7), sodium (-2.3), zinc (-2.8), salt (-2.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P102</td>
<td style="text-align: center;">Gordon Scholes is a member of the $\qquad$ political party.</td>
<td style="text-align: center;">Labor</td>
<td style="text-align: center;">Labour (-1.3), Conservative (-1.4), Green (-2.4), Liberal (-2.9), Labor (-2.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P530</td>
<td style="text-align: center;">Kenya maintains diplomatic relations with $\qquad$</td>
<td style="text-align: center;">Uganda</td>
<td style="text-align: center;">India (-3.0), Uganda (-3.2), Tanzania (-3.5), China (-3.8), Pakistan (-3.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P176</td>
<td style="text-align: center;">iPod Touch is produced by $\qquad$</td>
<td style="text-align: center;">Apple</td>
<td style="text-align: center;">Apple (-1.6), Nokia (-3.7), Sony (-2.6), Samsung (-2.6), Intel (-3.1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P30</td>
<td style="text-align: center;">Bailey Peninsula is located in $\qquad$</td>
<td style="text-align: center;">Antarctica</td>
<td style="text-align: center;">Antarctica (-1.4), Bermuda (-2.2), Newfoundland (-2.5), Alaska (-2.7), Canada (-3.1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P178</td>
<td style="text-align: center;">IDK is developed by $\qquad$</td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">IBM (-2.4), Intel (-2.3), Microsoft (-2.5), HP (-1.4), Nokia (-3.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P1412</td>
<td style="text-align: center;">Carl III used to communicate in $\qquad$</td>
<td style="text-align: center;">Swedish</td>
<td style="text-align: center;">German (-1.6), Latin (-1.9), French (-2.4), English (-3.0), Spanish (-3.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P17</td>
<td style="text-align: center;">Sunshine Coast, British Columbia is located in $\qquad$</td>
<td style="text-align: center;">Canada</td>
<td style="text-align: center;">Canada (-1.2), Alberta (-2.4), Yukon (-2.8), Labrador (-3.4), Victoria (-3.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P39</td>
<td style="text-align: center;">Pope Clement VII has the position of $\qquad$</td>
<td style="text-align: center;">pope</td>
<td style="text-align: center;">cardinal (-2.4), Pope (-2.5), pope (-2.6), President (-3.1), Chancellor (-3.2)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P284</td>
<td style="text-align: center;">Joe Cocker is represented by music label $\qquad$</td>
<td style="text-align: center;">Capitol</td>
<td style="text-align: center;">EMI (-2.6), BMG (-2.4), Universal (-2.8), Capitol (-3.2), Columbia (-3.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P276</td>
<td style="text-align: center;">London Jazz Festival is located in $\qquad$</td>
<td style="text-align: center;">London</td>
<td style="text-align: center;">London (-0.3), Greenwich (-3.2), Chelsea (-4.0), Condon (-4.6), Stratford (-4.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P127</td>
<td style="text-align: center;">Border TV is owned by $\qquad$</td>
<td style="text-align: center;">ITV</td>
<td style="text-align: center;">Sky (-3.1), ITV (-4.3), Global (-3.4), Frontier (-4.1), Disney (-4.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P103</td>
<td style="text-align: center;">The native language of Mammoorty is $\qquad$</td>
<td style="text-align: center;">Malayalam</td>
<td style="text-align: center;">Malayalam (-4.2), Tamil (-2.1), Telugu (-4.8), English (-5.2), Hindi (-5.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P495</td>
<td style="text-align: center;">The Sharon Cunera Show was created in $\qquad$</td>
<td style="text-align: center;">Philippines</td>
<td style="text-align: center;">Manila (-3.2), Philippines (-3.6), February (-3.7), December (-3.8), Argentina (-4.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adj.ocation</td>
<td style="text-align: center;">You are likely to find a overflow in a $\qquad$</td>
<td style="text-align: center;">drain</td>
<td style="text-align: center;">sewer (-3.1), canal (-3.2), toilet (-3.3), stream (-3.6), drain (-3.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CapableOf</td>
<td style="text-align: center;">Kuvens can $\qquad$</td>
<td style="text-align: center;">fly</td>
<td style="text-align: center;">fly (-1.3), fight (-1.6), kill (-2.2), die (-3.2), hunt (-3.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CausesDesire</td>
<td style="text-align: center;">Joke would make you want to $\qquad$</td>
<td style="text-align: center;">laugh</td>
<td style="text-align: center;">cry (-1.7), die (-1.7), laugh (-2.0), vomit (-2.6), scream (-2.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Causes</td>
<td style="text-align: center;">Sometimes virus causes $\qquad$</td>
<td style="text-align: center;">infection</td>
<td style="text-align: center;">disease (-1.2), cancer (-2.0), infection (-2.6), plague (-2.3), fever (-3.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasA</td>
<td style="text-align: center;">Birds have $\qquad$</td>
<td style="text-align: center;">feathers</td>
<td style="text-align: center;">wings (-1.8), nests (-3.1), feathers (-3.2), died (-3.7), eggs (-3.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasPretrogniste</td>
<td style="text-align: center;">Typing requires $\qquad$</td>
<td style="text-align: center;">speed</td>
<td style="text-align: center;">patience (-3.5), precision (-3.6), registration (-3.8), accuracy (-4.0), speed (-4.1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasProperty</td>
<td style="text-align: center;">Time is $\qquad$</td>
<td style="text-align: center;">finite</td>
<td style="text-align: center;">short (-1.7), passing (-1.8), precious (-2.9), irrelevant (-3.2), gone (-4.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MotivatedByGoal</td>
<td style="text-align: center;">You would celebrate because you are $\qquad$</td>
<td style="text-align: center;">alive</td>
<td style="text-align: center;">happy (-2.4), human (-3.3), alive (-4.0), young (-3.6), free (-3.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ReservesAction</td>
<td style="text-align: center;">Skills can be $\qquad$</td>
<td style="text-align: center;">taught</td>
<td style="text-align: center;">acquired (-2.5), useful (-2.5), learned (-2.8), combined (-3.9), varied (-3.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UsedFor</td>
<td style="text-align: center;">A pond is for $\qquad$</td>
<td style="text-align: center;">fish</td>
<td style="text-align: center;">swimming (-1.3), fishing (-1.4), bathing (-2.0), fish (-2.8), recreation (-3.1)</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of generation for BERT-large. The last column reports the top five tokens generated together with the associated log probability (in square brackets).
that BERT-large is able to recall such knowledge better than its competitors and at a level remarkably competitive with non-neural and supervised alternatives. Note that we did not compare the ability of the corresponding architectures and objectives to capture knowledge in a given body of text but rather focused on the knowledge present in the weights of existing pretrained models that are being used as starting points for many researchers' work. Understanding which aspects of data our commonly-used models and learning algorithms are capturing is a crucial field of research and this paper complements the many studies focused on the learned linguistic properties of the data.</p>
<p>We found that it is non-trivial to extract a knowledge base from text that performs on par to directly using pretrained BERT-large. This is despite providing our relation extraction baseline with only data that is likely expressing target facts, thus reducing potential for false negatives, as well as using a generous entity-linking oracle. We suspected BERT might have an advantage due to the larger amount of data it has processed, so we added Wikitext-103 as additional data to the relation extraction system and observed no significant change in performance. This suggests that while relation extraction performance might be difficult to improve with more data, language mod-
els trained on ever growing corpora might become a viable alternative to traditional knowledge bases extracted from text in the future.</p>
<p>In addition to testing future pretrained language models using the LAMA probe, we are interested in quantifying the variance of recalling factual knowledge with respect to varying natural language templates. Moreover, assessing multi-token answers remains an open challenge for our evaluation setup.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the reviewers for their thoughtful comments and efforts towards improving our manuscript. In addition, we would like to acknowledge three frameworks that were used in our experiments: AllenNLP ${ }^{7}$, Fairseq ${ }^{8}$ and the Hugging Face PyTorch-Transformers ${ }^{9}$ library.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette. 2019. Learning to understand goal specifications by</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modelling reward. In International Conference on Learning Representations (ICLR).</p>
<p>Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 238-247.</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarcíaDurán, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 27872795.
S. R. K. Branavan, David Silver, and Regina Barzilay. 2011. Learning to win by reading manuals in a monte-carlo framework. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 268-277.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. CoRR, abs/1704.00051.</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2018. Babyai: First steps towards grounded language learning with a human in the loop. CoRR, abs/1810.08272.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. CoRR, abs/1901.02860.</p>
<p>Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 933-941.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018a. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018b. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]. ArXiv: 1810.04805.</p>
<p>Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018).</p>
<p>Yoav Goldberg. 2019. Assessing bert's syntactic abilities. CoRR, abs/1901.05287.</p>
<p>Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665-695.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Rhinehart, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, et al. 2019. Natural questions: a benchmark for question answering research.</p>
<p>Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. 2019. A Survey of Reinforcement Learning Informed by Natural Language. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, August 10-16 2019, Macao, China.</p>
<p>Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1192-1202.
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference.</p>
<p>Gábor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models. CoRR, abs/1707.05589.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. CoRR, abs/1609.07843.</p>
<p>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In 2012 IEEE Spoken Language Technology Workshop (SLT), Miami, FL, USA, December 2-5, 2012, pages 234-239.</p>
<p>Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2016. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 2227-2237.</p>
<p>Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1499-1509.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. Coqa: A conversational question answering challenge. CoRR, abs/1808.07042.</p>
<p>Daniil Sorokin and Iryna Gurevych. 2017. Contextaware representations for knowledge base relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 1784-1789.</p>
<p>Robert Speer and Catherine Havasi. 2012. Representing general relational knowledge in conceptnet 5. In LREC, pages 3679-3686.</p>
<p>Mihai Surdeanu and Heng Ji. 2014. Overview of the English Slot Filling Track at the TAC2014 Knowledge Base Population Evaluation. page 15.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4149-4158.</p>
<p>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000-6010.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pages 353-355.</p>
<p>Sean Welleck, Kianté Brantley, Hal Daumé III, and Kyunghyun Cho. 2019. Non-monotonic sequential text generation. arXiv preprint arXiv:1902.02192.</p>
<p>Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. CoRR, abs/1409.2329.</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018. From recognition to cognition: Visual commonsense reasoning. CoRR, abs/1811.10830.</p>
<p>Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 1927.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/allenai/allennlp
${ }^{8}$ https://github.com/pytorch/fairseq
${ }^{9}$ https://github.com/huggingface/
pytorch-transformers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
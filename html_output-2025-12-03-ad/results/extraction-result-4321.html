<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-273962964</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.06590v1.pdf" target="_blank">CriticAL: Critic Automation with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Understanding the world through models is a fundamental goal of scientific research. While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models. Criticizing models deepens scientific understanding and drives the development of more accurate models. Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significant--both rely heavily on understanding the modeling assumptions and domain. Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves. Motivated by this, we introduce CriticAL (Critic Automation with Language Models). CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance. We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework. In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions. In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones. We show that both human and LLM judges consistently prefer CriticAL's critiques over alternative approaches in terms of transparency and actionability. Finally, we show that CriticAL's critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4321.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4321.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CriticAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic Automation with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses LLMs to propose model-dependent summary statistics (implemented as executable Python test-statistic functions) to detect discrepancies between a scientific model and data, and then converts those statistics into hypothesis tests (empirical p-values) using model-generated samples to validate significance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CriticAL</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline: (1) Inputs: dataset metadata, a symbolic representation of a model (e.g., probabilistic program in Stan/PyMC), and model samples (e.g., posterior predictive draws). (2) An LLM is prompted to propose multiple test statistics tailored to the model and dataset; each test statistic is returned as an executable Python function that maps a dataframe (including posterior predictive samples) to a scalar. (3) For each proposed test statistic T_k, the system computes the statistic over model samples to approximate the null distribution and computes an empirical p-value for the observed data (Equation 2). (4) Multiple-test correction (Bonferroni) is applied and significant discrepancies are flagged. (5) The system prompts an LLM to synthesize a natural-language critique that summarizes the statistic and its p-value into actionable feedback. (6) These artifacts (Python test-statistic, p-value, natural-language critique, and symbolic model) are provided to downstream agents or humans to guide model revision.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Proposer/revision LLM(s) unspecified in-method (system calls a generic p_LM in algorithms); evaluation / judging LLMs explicitly reported as GPT-4o-2024-08-06 and Claude-3.5-sonnet-20240620 (used as qualitative judges).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical modeling / Bayesian modeling (applied across varied domains represented in Stan PosteriorDB: regression, hierarchical models, generalized linear models, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Statistical discrepancies and model-misfit signals expressed as test-statistics (e.g., kurtosis for heavy tails, variance-to-mean ratio for overdispersion, slice-specific variance/means revealing conditional structure).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Executable Python functions implementing scalar test-statistics, empirical p-values (numeric), and natural language critiques (text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical hypothesis-testing using model-generated samples (posterior predictive checks): compute statistic across model samples to form null distribution and compute empirical p-values; Bonferroni correction for multiple tests; synthetic ground-truth discovery/no-discovery experiments and real-world evaluation on Stan PosteriorDB models; human and LLM judge evaluation of qualitative properties; downstream validation via improvement in expected log predictive density (ELPD LOO) of revised models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Statistical detection: ROC analysis shows higher true positive rate at fixed false positive rate than pre-specified baseline statistics (mean/variance); FPR closely tracks significance threshold α (calibrated). Downstream improvement: CriticAL-enabled revisions beat initial models on ELPD LOO with win rates: >1 SE: 0.94, >1.5 SE: 0.94, >2.0 SE: 0.82. Versus data-blind critic: CriticAL win rates >1 SE: 0.59 vs data-blind 0.29 (other SE thresholds similar). Qualitative judgments: CriticAL rated more transparent (~97%), actionable (~76%), and tailored (~98%) by LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines: (a) Pre-specified aggregate test-statistics (mean & variance) — CriticAL achieves higher TPR at same FPR; (b) Naive LLM critic (receives model program, dataframe of posterior predictive mean/variance and data) — naive critic hallucinates and yields indiscriminate model revisions; (c) Data-blind LLM critic (ablation receiving only the symbolic model) — CriticAL outperforms data-blind in downstream ELPD-based win rates (e.g., 0.59 vs 0.29 at >1 SE).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires ability to sample from the model (posterior predictive sampling); LLM proposer may produce imperfectly-implemented statistics; CriticAL does not see model predictions/data at the proposer step (limiting transformations that require direct data access) which can produce suboptimal transforms; revision LLMs can mis-implement correct criticisms; heuristic thresholds (e.g., relaxed Bonferroni cutoffs) used in closed-loop revision; dependency on quality of LLM proposals and on the ability to generate model samples.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4321.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4321.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Li et al. model-discovery system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that generates probabilistic programs (statistical models) to propose candidate models; in this paper it is used as the downstream revision agent that receives CriticAL's test-statistics and critiques to propose model revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models (Li et al. [17])</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An LLM-based model discovery pipeline that generates probabilistic programs (e.g., Stan/PyMC code) as candidate statistical models given a dataset and model context; in this work CriticAL provides the revision-LLM with (symbolic initial model, Python test-statistic, and natural-language critique) and the Li et al. system proposes revised probabilistic programs which are then fit and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced as the LLM-based system from Li et al. [17]); the current paper uses that system as a revision agent but does not detail the underlying LLM architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical modeling / automated model discovery (applied across datasets in Stan PosteriorDB spanning multiple scientific areas).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Proposed statistical models / probabilistic programs representing hypothesized relationships between covariates and responses (model form rather than explicit physical law).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Probabilistic program source code (Stan / PyMC), which encodes mathematical model forms and parameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Model fitting and evaluation using expected log predictive density (ELPD LOO); revisions proposed by the system were fit with PyMC and compared to initial models via ELPD LOO and standard error margins.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported indirectly via CriticAL-driven revisions: when the Li et al. revision system used CriticAL critiques, revised models improved ELPD LOO over initial models in 94% of datasets at >1 SE (see CriticAL metrics). Specific performance of Li et al. system alone is not re-evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in this work as the revision agent receiving CriticAL outputs versus receiving naive or data-blind criticisms; CriticAL-augmented revision outperformed revisions driven by data-blind or naive critics (see CriticAL win-rate numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>This paper does not specify architectural details or failure modes of the Li et al. system beyond noting non-determinism at deterministic temperature and occasional mis-implementations by the revision LLM in applying a correct criticism.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4321.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4321.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Naive LLM critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive LLM-based Critic Baseline (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline LLM critic implemented by the authors that receives an initial PyMC model, a dataframe containing posterior predictive mean and variance, and the dataset, and is prompted to identify discrepancies; empirically shown to hallucinate and cause indiscriminate model revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Naive LLM critic baseline</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt an LLM with (1) the initial statistical model as PyMC program, (2) a dataframe of posterior predictive mean predictions and variances, and (3) the dataset dataframe; ask the LLM to identify discrepancies between predictions and data and recommend model revisions. No formal hypothesis-testing verifier is used; critiques are free-form natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Unspecified LLM in-method; used as a baseline within experiments (no explicit model name given for the naive critic).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Regression / general statistical modeling (synthetic radon-inspired regression experiment and others).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Informal model-misfit descriptions and suggested model modifications (not structured extraction of laws); recommendations often add features without statistical significance checks.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Natural-language critiques and suggested model edits (unstructured text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical evaluation by feeding critiques into the revision-LLM and measuring features added to revised models and downstream ELPD improvements; comparison of hallucination (spurious feature additions) vs CriticAL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative/quantitative comparison in synthetic regression experiment: naive critic induced many spurious feature additions; CriticAL filtered to five significant critiques (p<0.01) and led to more targeted revisions. No explicit accuracy number for naive critic given, but shown to perform worse than CriticAL in inducing correct revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline compared against CriticAL; naive critic produced hallucinated, indiscriminate revisions and lower qualitative scores (transparency, actionability, tailoredness) by human/LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Prone to hallucinations (false positive critiques), lacks rigorous significance assessment, produces generic expansions leading to spurious model changes.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4321.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4321.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-blind LLM critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-blind LLM critic (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation critic that receives only the symbolic probabilistic program (model) and no data or model predictions; used to test whether LLMs can critique models without conditioning on data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Data-blind LLM critic (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt an LLM with only the symbolic representation of the statistical model (probabilistic program) and ask for critiques or suggested revisions without access to model predictions or data; intended as a data-free baseline ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Unspecified LLM (ablation run at temperature 0.0 per the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical modeling (used across Stan PosteriorDB model-dataset pairs in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Model-structure level suggestions (e.g., general modeling motifs), not data-driven quantitative relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Natural-language critiques and suggested probabilistic program edits (unstructured text / code changes).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Downstream model fitting and comparison via ELPD LOO; compared win rates against CriticAL-driven revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rates vs CriticAL: data-blind method win rates >1 SE = 0.29 (compared to CriticAL 0.59 in same comparison), indicating substantially lower downstream improvement when only the symbolic model is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ablation baseline for CriticAL; CriticAL outperforms data-blind approach in producing actionable, model-improving critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lacks any data-conditioning so cannot identify dataset-specific discrepancies; less actionable for model revision on real data.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CriticAL: Critic Automation with Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated Statistical Model Discovery with Language Models <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 1)</em></li>
                <li>MLAgentbench: Evaluating language agents on machine learning experimentation <em>(Rating: 1)</em></li>
                <li>Automating science <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4321",
    "paper_id": "paper-273962964",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "CriticAL",
            "name_full": "Critic Automation with Language Models",
            "brief_description": "A system that uses LLMs to propose model-dependent summary statistics (implemented as executable Python test-statistic functions) to detect discrepancies between a scientific model and data, and then converts those statistics into hypothesis tests (empirical p-values) using model-generated samples to validate significance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "CriticAL",
            "method_description": "Pipeline: (1) Inputs: dataset metadata, a symbolic representation of a model (e.g., probabilistic program in Stan/PyMC), and model samples (e.g., posterior predictive draws). (2) An LLM is prompted to propose multiple test statistics tailored to the model and dataset; each test statistic is returned as an executable Python function that maps a dataframe (including posterior predictive samples) to a scalar. (3) For each proposed test statistic T_k, the system computes the statistic over model samples to approximate the null distribution and computes an empirical p-value for the observed data (Equation 2). (4) Multiple-test correction (Bonferroni) is applied and significant discrepancies are flagged. (5) The system prompts an LLM to synthesize a natural-language critique that summarizes the statistic and its p-value into actionable feedback. (6) These artifacts (Python test-statistic, p-value, natural-language critique, and symbolic model) are provided to downstream agents or humans to guide model revision.",
            "llm_model_used": "Proposer/revision LLM(s) unspecified in-method (system calls a generic p_LM in algorithms); evaluation / judging LLMs explicitly reported as GPT-4o-2024-08-06 and Claude-3.5-sonnet-20240620 (used as qualitative judges).",
            "scientific_domain": "Statistical modeling / Bayesian modeling (applied across varied domains represented in Stan PosteriorDB: regression, hierarchical models, generalized linear models, etc.)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Statistical discrepancies and model-misfit signals expressed as test-statistics (e.g., kurtosis for heavy tails, variance-to-mean ratio for overdispersion, slice-specific variance/means revealing conditional structure).",
            "extraction_output_format": "Executable Python functions implementing scalar test-statistics, empirical p-values (numeric), and natural language critiques (text).",
            "validation_method": "Empirical hypothesis-testing using model-generated samples (posterior predictive checks): compute statistic across model samples to form null distribution and compute empirical p-values; Bonferroni correction for multiple tests; synthetic ground-truth discovery/no-discovery experiments and real-world evaluation on Stan PosteriorDB models; human and LLM judge evaluation of qualitative properties; downstream validation via improvement in expected log predictive density (ELPD LOO) of revised models.",
            "performance_metrics": "Statistical detection: ROC analysis shows higher true positive rate at fixed false positive rate than pre-specified baseline statistics (mean/variance); FPR closely tracks significance threshold α (calibrated). Downstream improvement: CriticAL-enabled revisions beat initial models on ELPD LOO with win rates: &gt;1 SE: 0.94, &gt;1.5 SE: 0.94, &gt;2.0 SE: 0.82. Versus data-blind critic: CriticAL win rates &gt;1 SE: 0.59 vs data-blind 0.29 (other SE thresholds similar). Qualitative judgments: CriticAL rated more transparent (~97%), actionable (~76%), and tailored (~98%) by LLM judges.",
            "baseline_comparison": "Baselines: (a) Pre-specified aggregate test-statistics (mean & variance) — CriticAL achieves higher TPR at same FPR; (b) Naive LLM critic (receives model program, dataframe of posterior predictive mean/variance and data) — naive critic hallucinates and yields indiscriminate model revisions; (c) Data-blind LLM critic (ablation receiving only the symbolic model) — CriticAL outperforms data-blind in downstream ELPD-based win rates (e.g., 0.59 vs 0.29 at &gt;1 SE).",
            "challenges_limitations": "Requires ability to sample from the model (posterior predictive sampling); LLM proposer may produce imperfectly-implemented statistics; CriticAL does not see model predictions/data at the proposer step (limiting transformations that require direct data access) which can produce suboptimal transforms; revision LLMs can mis-implement correct criticisms; heuristic thresholds (e.g., relaxed Bonferroni cutoffs) used in closed-loop revision; dependency on quality of LLM proposals and on the ability to generate model samples.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4321.0",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Li et al. model-discovery system",
            "name_full": "Automated Statistical Model Discovery with Language Models",
            "brief_description": "An LLM-based system that generates probabilistic programs (statistical models) to propose candidate models; in this paper it is used as the downstream revision agent that receives CriticAL's test-statistics and critiques to propose model revisions.",
            "citation_title": "Automated Statistical Model Discovery with Language Models",
            "mention_or_use": "use",
            "method_name": "Automated Statistical Model Discovery with Language Models (Li et al. [17])",
            "method_description": "An LLM-based model discovery pipeline that generates probabilistic programs (e.g., Stan/PyMC code) as candidate statistical models given a dataset and model context; in this work CriticAL provides the revision-LLM with (symbolic initial model, Python test-statistic, and natural-language critique) and the Li et al. system proposes revised probabilistic programs which are then fit and evaluated.",
            "llm_model_used": "Not specified in this paper (referenced as the LLM-based system from Li et al. [17]); the current paper uses that system as a revision agent but does not detail the underlying LLM architecture.",
            "scientific_domain": "Statistical modeling / automated model discovery (applied across datasets in Stan PosteriorDB spanning multiple scientific areas).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Proposed statistical models / probabilistic programs representing hypothesized relationships between covariates and responses (model form rather than explicit physical law).",
            "extraction_output_format": "Probabilistic program source code (Stan / PyMC), which encodes mathematical model forms and parameterizations.",
            "validation_method": "Model fitting and evaluation using expected log predictive density (ELPD LOO); revisions proposed by the system were fit with PyMC and compared to initial models via ELPD LOO and standard error margins.",
            "performance_metrics": "Reported indirectly via CriticAL-driven revisions: when the Li et al. revision system used CriticAL critiques, revised models improved ELPD LOO over initial models in 94% of datasets at &gt;1 SE (see CriticAL metrics). Specific performance of Li et al. system alone is not re-evaluated in this paper.",
            "baseline_comparison": "Compared in this work as the revision agent receiving CriticAL outputs versus receiving naive or data-blind criticisms; CriticAL-augmented revision outperformed revisions driven by data-blind or naive critics (see CriticAL win-rate numbers).",
            "challenges_limitations": "This paper does not specify architectural details or failure modes of the Li et al. system beyond noting non-determinism at deterministic temperature and occasional mis-implementations by the revision LLM in applying a correct criticism.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4321.1",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Naive LLM critic",
            "name_full": "Naive LLM-based Critic Baseline (this paper)",
            "brief_description": "A baseline LLM critic implemented by the authors that receives an initial PyMC model, a dataframe containing posterior predictive mean and variance, and the dataset, and is prompted to identify discrepancies; empirically shown to hallucinate and cause indiscriminate model revisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Naive LLM critic baseline",
            "method_description": "Prompt an LLM with (1) the initial statistical model as PyMC program, (2) a dataframe of posterior predictive mean predictions and variances, and (3) the dataset dataframe; ask the LLM to identify discrepancies between predictions and data and recommend model revisions. No formal hypothesis-testing verifier is used; critiques are free-form natural language.",
            "llm_model_used": "Unspecified LLM in-method; used as a baseline within experiments (no explicit model name given for the naive critic).",
            "scientific_domain": "Regression / general statistical modeling (synthetic radon-inspired regression experiment and others).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Informal model-misfit descriptions and suggested model modifications (not structured extraction of laws); recommendations often add features without statistical significance checks.",
            "extraction_output_format": "Natural-language critiques and suggested model edits (unstructured text).",
            "validation_method": "Empirical evaluation by feeding critiques into the revision-LLM and measuring features added to revised models and downstream ELPD improvements; comparison of hallucination (spurious feature additions) vs CriticAL.",
            "performance_metrics": "Qualitative/quantitative comparison in synthetic regression experiment: naive critic induced many spurious feature additions; CriticAL filtered to five significant critiques (p&lt;0.01) and led to more targeted revisions. No explicit accuracy number for naive critic given, but shown to perform worse than CriticAL in inducing correct revisions.",
            "baseline_comparison": "Baseline compared against CriticAL; naive critic produced hallucinated, indiscriminate revisions and lower qualitative scores (transparency, actionability, tailoredness) by human/LLM judges.",
            "challenges_limitations": "Prone to hallucinations (false positive critiques), lacks rigorous significance assessment, produces generic expansions leading to spurious model changes.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4321.2",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Data-blind LLM critic",
            "name_full": "Data-blind LLM critic (ablation)",
            "brief_description": "An ablation critic that receives only the symbolic probabilistic program (model) and no data or model predictions; used to test whether LLMs can critique models without conditioning on data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Data-blind LLM critic (ablation)",
            "method_description": "Prompt an LLM with only the symbolic representation of the statistical model (probabilistic program) and ask for critiques or suggested revisions without access to model predictions or data; intended as a data-free baseline ablation.",
            "llm_model_used": "Unspecified LLM (ablation run at temperature 0.0 per the paper).",
            "scientific_domain": "Statistical modeling (used across Stan PosteriorDB model-dataset pairs in experiments).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Model-structure level suggestions (e.g., general modeling motifs), not data-driven quantitative relationships.",
            "extraction_output_format": "Natural-language critiques and suggested probabilistic program edits (unstructured text / code changes).",
            "validation_method": "Downstream model fitting and comparison via ELPD LOO; compared win rates against CriticAL-driven revisions.",
            "performance_metrics": "Win rates vs CriticAL: data-blind method win rates &gt;1 SE = 0.29 (compared to CriticAL 0.59 in same comparison), indicating substantially lower downstream improvement when only the symbolic model is provided.",
            "baseline_comparison": "Ablation baseline for CriticAL; CriticAL outperforms data-blind approach in producing actionable, model-improving critiques.",
            "challenges_limitations": "Lacks any data-conditioning so cannot identify dataset-specific discrepancies; less actionable for model revision on real data.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4321.3",
            "source_info": {
                "paper_title": "CriticAL: Critic Automation with Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated Statistical Model Discovery with Language Models",
            "rating": 2,
            "sanitized_title": "automated_statistical_model_discovery_with_language_models"
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 1,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "MLAgentbench: Evaluating language agents on machine learning experimentation",
            "rating": 1,
            "sanitized_title": "mlagentbench_evaluating_language_agents_on_machine_learning_experimentation"
        },
        {
            "paper_title": "Automating science",
            "rating": 1,
            "sanitized_title": "automating_science"
        }
    ],
    "cost": 0.0107539,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CriticAL: Critic Automation with Language Models
10 Nov 2024</p>
<p>Michael Y Li michaelyli@stanford.edu 
Stanford University</p>
<p>Vivek Vajipey 
Stanford University</p>
<p>Noah D Goodman 
Stanford University</p>
<p>Emily B Fox 
Stanford University</p>
<p>CriticAL: Critic Automation with Language Models
10 Nov 2024090F604A4439A9F65C512998DFC8028BarXiv:2411.06590v1[cs.LG]
Understanding the world through models is a fundamental goal of scientific research.While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models.Criticizing models deepens scientific understanding and drives the development of more accurate models.Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significantboth rely heavily on understanding the modeling assumptions and domain.Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves.Motivated by this, we introduce CriticAL (Critic Automation with Language Models).CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance.We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework.In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions.In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones.We show that both human and LLM judges consistently prefer CriticAL's critiques over alternative approaches in terms of transparency and actionability.Finally, we show that CriticAL's critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.</p>
<p>Introduction</p>
<p>A longstanding goal of artificial intelligence research is to automate the discovery of scientific models [16,27].The rapid development of LLMs with remarkable reasoning capabilities and general knowledge has created exciting new opportunities within this domain.Recent work has shown that LLM-based scientific agents can propose research ideas [23], discover scientific models [17], and implement experiments [12,18].These results highlight the promise of using LLMs to automate many important aspects of scientific discovery.However, they overlook the crucial role that model criticism, or understanding the limitations of a model, plays in driving scientific progress.Model criticism deepens our understanding and often motivates new models.Furthermore, automated methods for criticism can improve the reliability of LLM-based scientific discovery systems, as LLMs are prone to systematic hallucinations [18,29] that could undermine the broader goal of automating scientific discovery.Model criticism is hard to automate because it is inherently dependent on the model and problem domain.In particular, it involves (1) determining which aspects to compare between the model and data and (2) evaluating the significance of any differences.Each of these tasks typically requires substantial human expertise [7].While leveraging LLMs is an initially appealing approach to automation, it introduces new challenges: LLMs might also hallucinate the critiques themselves, undermining the effectiveness of automated model criticism.</p>
<p>Motivated by these challenges, we introduce CriticAL (Critic Automation with Language Models), which integrates LLMs within a principled model criticism framework.Specifically, given a proposed scientific model and dataset metadata, CriticAL uses an LLM to generate summary statistics that capture properties of the data that might violate the modeling assumptions.Importantly, these summary statistics are tailored to the model and dataset.CriticAL implements these summary statistics as Python functions, which can be easily executed and inspected by a human or LLM scientist.This brings transparency to the critique process.While these summary statistics can highlight potential discrepancies, we need a method to determine whether these discrepancies are meaningful.To address this, we show how we can automatically convert the summary statistics produced by CriticAL into hypothesis tests, for many commonly-used scientific models.Specifically, if we can generate data from the scientific model [6,10], we can form a null distribution for a summary statistic and compute an empirical p-value.Thus, we can transform each summary statistic into a quantitative check, providing a rigorous way to assess both the significance of the discrepancies and the validity of the model.In doing so, we reduce the complex task of automatically validating proposed models and critiques to the well-understood problem of hypothesis testing.In experiments (Section 4), we evaluate CriticAL along key qualitative and quantitative properties crucial for an automated critic system.In settings where we synthetically control discrepancies between models and datasets, CriticAL consistently identifies true discrepancies and avoids hallucinating false ones.We also assess important qualitative aspects of CriticAL's critiques (e.g., transparency), and find that both LLM and human judges prefer CriticAL's critiques over alternatives.Finally, we demonstrate the practical impact of CriticAL's critiques on the downstream task of guiding an LLM-based scientific model discovery system.On real-world datasets, CriticAL's critiques enable an LLM-based automated model discovery system [17] to significantly improve upon initial human-designed models.</p>
<p>Background</p>
<p>In this section, we discuss model criticism techniques from different domains.Crucially, we can often formalize finding discrepancies as identifying suitable test statistics, using those statistics to compute discrepancies between model predictions and data, and validating their significance using domain knowledge.</p>
<p>Regression analysis</p>
<p>In regression analysis, we begin with a dataset D = {X , Y} of input features X and targets Y; our goal is to predict Y from X .Given model predictions Y pred , we perform model diagnostics that target the standard assumptions of linear regression (e.g., linearity, homoscedasticity, uncorrelated errors).For example, to evaluate whether homoscedasticity holds, we can plot the residuals against the input features.We can then either informally assess whether the pattern in the residuals indicates a significant departure from homoscedasticity or perform statistical tests.</p>
<p>Computational models Computational models often make simplifying assumptions that can lead to systematic errors, even after the parameters of these models are calibrated.This might be due to imperfect physical knowledge or systematic measurement errors; these systematic errors are often known as model inadequacies.Bayarri &amp; Berger [2] introduce a framework for understanding these inadequacies that involves defining domain-specific evaluation criteria or performing sensitivity analyses and checking whether these accord with scientific intuition.Another very influential approach is to cast this as a statistical modeling problem and directly build a statistical model of the discrepancy [14].Building on this work, Joseph &amp; Yan [13] show how to study this discrepancy through an analysis of variance decomposition.</p>
<p>Bayesian statistical models</p>
<p>In statistical modeling, we model the data as a probability distribution.More formally, a statistical model defines a joint probability distribution p(Y, θ|X , H) over observed variables Y and latent variables θ; we use H to indicate a specific class of statistical models and the dataset D = {X , Y} can include both observations Y that we model as random variables and additional quantities X that we treat as fixed.By marginalizing out the latent variables, we obtain the posterior predictive distribution
p(Y ppred | D, H) = p(Y ppred |θ, H)p(θ|D, H)dθ(1)
A common technique for evaluating such a model is a posterior predictive check (PPC) [4,8,20,22].</p>
<p>In brief, PPCs ask if the posterior predictive distribution captures important properties of the data.</p>
<p>Concretely, to perform a PPC, we first draw samples from the posterior predictive distribution,
{Y ppred i } m i=1 ∼ p(Y ppred | D, H).
We then choose a test statistic T (X , Y ppred ) that can reveal some property of the data that is not well-captured by the model samples.To compare the posterior predictive samples against the dataset, we compute the test statistic over both samples (forming a null distribution) and data.For a PPC to be useful, the test statistic must be chosen in a modeldependent way and choosing an appropriate test statistic is an important step in many applied modeling settings [3,9,25].For example, when criticizing a Poisson model, one might check for over-dispersion by computing the variance-to-mean ratio.Crucially, posterior predictive checks do not require human intervention, since they automatically generate a quantitative measure of the significance of any discrepancy via the posterior predictive p-value; we discuss this in more detail in Section 3.1.</p>
<p>Method: CriticAL</p>
<p>In this section, we describe CriticAL, our system for finding systematic discrepancies between a scientific model and dataset.We provide a brief overview here; for a schematic overview, see Figure 1.CriticAL takes as input: dataset metadata, a symbolic representation of a model (e.g., program) and model samples.Given these, CriticAL produces significant discrepancies.Each discrepancy is represented as a test statistic implemented as a Python function, an executable artifact that programmatically expresses the discrepancy, and a natural language criticism.</p>
<p>Automatically proposing and evaluating discrepancies</p>
<p>Evaluating significance of discrepancies via hypothesis tests</p>
<p>We now describe how CriticAL uses the test statistics to identify significant discrepancies.In brief, we use model samples to approximate a null distribution over the test statistic and then compute an empirical p-value.We assume the user can generate data from the model {Y pred i } m i=1 .This is not restrictive requirement and how the user generates the model samples is a design choice; for example, we can do this for any model that describes a generative process for the data.</p>
<p>We describe how to construct an empirical p-value p k given T k and {Y
pred i } m i=1 below.
1. We approximate the null distribution of the test statistic by computing the test statistic over the model samples {T (X , Y pred i</p>
<p>)} m i=1 .2. We locate the test statistic of the observed data T (X , Y) within this null distribution to obtain an empirical p-value.That is, we compute
P (T (X , Y pred ) ≥ T (X , Y)|D, H) ≈ 1 m m i=1 1 {T (X ,Y pred i )≥T (X ,Y )}(2)
We visualize the computation of the p-values in the Appendix (Figure 10).To capture different discrepancies, we compute multiple test statistics in parallel for a model-dataset pair.However, this can inflate the effective false positive rate: for large enough m, we expect min k p k ≤ α even if the model and dataset have no discrepancy.We thus apply a Bonferroni correction to obtain adjusted p-values {p k } m k=1 .We regard all T k such that pk ≤ α as significant.</p>
<p>Instantiating the framework for Bayesian models In our experiments, we focus our evaluation on Bayesian models because they are widely used in scientific settings [6,10].In our context, Bayesian models are also appealing because they can be expressed symbolically as probabilistic programs [11,24] and we can choose the model samples to be posterior predictive samples
{Y ppred i } m i=1
(Equation 1).The corresponding posterior predictive p-value has an intuitive interpretation: how atypical is Y under the posterior distribution p(Y ppred |D, H) with respect to the discrepancy measure defined by T k ?</p>
<p>Interfacing with LLM science agents via natural language criticism</p>
<p>In many situations, we might want to integrate CriticAL within a broader scientific discovery system, involving either human or LLM scientists.Therefore, CriticAL also produces natural language criticism.This design choice is motivated by several considerations.By offering critiques in natural language, which is flexible and generic, the system provides an additional medium for users to interpret results, which can be useful in fields where training in formal modeling is less common.Second, this design choice is natural given recent advances in LLM-based agents for scientific discovery and modeling [12,17].</p>
<p>We prompt an LLM to produce natural language criticism h k that summarizes the discrepancy implied by test statistic T k and its p-value pk .Specifically, we ask the LLM to synthesize the test statistic in a way that's informative to a colleague revising some initial model.For examples of the natural language critiques produced, see Section A.2 and for the prompt see Figure 9.</p>
<p>We can easily integrate these three artifacts within an LLM-based scientific discovery system.Specifically, we provide the system with (1) a Python implementation of the test statistic T k , (2) the natural language h k , and (3) the initial model; in our experiments these models will be probabilistic programs in pymc or stan [1,5].We use the LLM-based system for generating probabilistic programs introduced by Li et al. [17].</p>
<p>In general, these hypothesis tests are cheap relative to the cost of model fitting.For example, posterior inference is the dominating cost for Bayesian models and performing posterior predictive checks is cheap given posterior samples.Thus, CriticAL will generally introduce minimal overhead to the overall cost of an AI scientist system.</p>
<p>Experiments</p>
<p>In this section, we present experimental results that evaluate key quantitative and qualitative properties of our system.We begin by illustrating the pitfalls of a naive LLM in a synthetic regression setting.We then systematically study CriticAL's ability to avoid hallucinations and discover true discrepancies by analyzing its true and false positive rates in a setting where we synthesize discrepancies between models and datasets.We then evaluate the transparency and interpretability of our system in human user and LLM evaluations, as well as the actionability of the natural language criticism in helping an LLM-based system to revise models.</p>
<p>Features Used</p>
<p>Naive criticism leads to hallucinatory revisions</p>
<p>Correct Features Spurious Features</p>
<p>Figure 3: CriticAL attempts fewer, more targeted revisions.The critiques produced by the naive approach drive greedy model revisions that indiscriminately add both spurious (red) and correct (green) features; we indicate features used in revised models as dark-colored squares.In contrast, CriticAL leads to fewer revisions because it filters discrepancies by significance.Furthermore, those revisions generally target the correct missing feature (floor).</p>
<p>Experiment 1: Naive LLM-based critic hallucinates in synthetic regression task</p>
<p>In an initial case study, we show that a naive LLM critic consistently hallucinates but CriticAL does not.Specifically, we compare the model revision changes induced by the critiques produced by CriticAL and the naive approach, in a setting where we adversarially introduce spurious "distractor" features into a dataset.For an overview, see Figure 2.</p>
<p>Generating a regression dataset with spurious features</p>
<p>We generate a synthetic dataset inspired by the radon dataset, a commonly used dataset in regression analysis.We generate the target, radon as a a linear function of floor (basement or first floor) and uppm (e.g., uranium), corrupted with additive Gaussian noise.In addition to these two features, we add two additional spurious, distractor features to the dataframe, county and soil, with semantically plausible names.</p>
<p>Naive LLM critic baseline</p>
<p>We implement a naive approach to model criticism that receives (1) an initial statistical model represented as a pymc [1] program (2) a dataframe of the posterior predictive mean radon predictions along with the corresponding variances of those predictions and a (3) dataframe of the dataset.Given this information, we ask the LLM critic to identify discrepancies between the predictions and data.</p>
<p>Evaluating critiques in driving model revision We generate twenty critiques from both CriticAL and the naive baseline; the initial model regresses radon against only uppm, omitting floor which is used in the ground truth.CriticAL filters the critiques to five significant ones (p &lt; 0.01); four correctly identify that radon varies by floor, and the other correctly notes that model fails to capture the range of radon values but does not identify that the missing floor feature is the culprit.In contrast, the naive approach recommends generic model expansions; for an example see the text in the bottom of Figure 2. We evaluate the critiques by feeding them into an LLM-based model revision system [17] as described in Section 3.2.Figure 3 shows the features added per revision attempt, with spurious features indicated in red and correct features in green.Rows correspond to features and columns correspond to model revision attempts; we indicate that a feature was added using dark-colored squares.The naive approach often adds all possible features indiscriminately.</p>
<p>In contrast, the majority of CriticAL's critiques lead to targeted revisions.The main exception is the critique about the discrepancy in the range of values; this critique captures a true deficiency in the initial model, but isn't actionable (i.e., suggest a concrete strategy for revising) which leads the revision LLM to be greedy; we will evaluate this notion of actionability in additional experiments.(right) FPR against significance threshold.CriticAL correctly identifies more discrepancies than the pre-specified method, at the same FPR level.The FPR is calibrated with the significance threshold, showing that CriticAL systematically avoids hallucinations.</p>
<p>Experiment 2: Statistical analysis of hallucinations and true discoveries</p>
<p>A reliable critic system should avoid hallucinations (i.e., generating false positives) and discover true discrepancies when they exist.In this section, we study this through a statistical lens and characterize CriticAL's false and true positive rates.We ask: does CriticAL reliably discover discrepancies when there are actual discrepancies?And, conversely, does CriticAL hallucinate when there are no discrepancies?To study this, we synthetically generate discrepancies between models and datasets which enables us to empirically characterize the true and false positive rates.</p>
<p>Generating no-discovery and discovery datasets</p>
<p>We synthetically generate model-dataset pairs where each pair is either a no-discovery pair or discovery pair.</p>
<p>To construct a no-discovery pair, we first sample a dataset Y from a ground truth data distribution Y ∼ p(Y|H).We then draw m posterior predictive samples from the ground truth data distribution conditioned on the data: i.e., {Y
ppred i } m i=1 ∼ p(Y ppred |Y, H).
No-discovery pairs serve as a negative control to ensure that CriticAL does not systematically hallucinate and produce false discoveries.To generate discovery pairs, we sample a dataset Y from a dataset distribution p.However, we pair Y with samples from a lesioned model q, where we choose q so that it fails to capture an important aspect of the data generating distribution q.For example, we can take p to be a Student's t distribution and q to be a Gaussian distribution; even after conditioning on data q(Y |Y) will fail to capture the tails.These discovery pairs serve as a positive control and allow us to understand how reliably CriticAL identifies discoveries (i.e., true positive rate).We generate six model-dataset pairs.The data-generating models are: Student's t, negative binomial, and a generalized linear model.The lesioned models are: Gaussian, Poisson, and logistic growth.</p>
<p>To account for randomness in data generation, we generate twenty copies of each model-data pair corresponding to twenty random fresh datasets.</p>
<p>Calculating true positive and false positive rate</p>
<p>For each model-dataset pair, we run CriticAL with 24 proposals and at a temperature 0.7.Our system decides if there is a discrepancy by checking whether the minimum p-value is less than the significance threshold; that is, whether min k pk ≤ α.By construction, we have the "correct" decision for each pair.To compute the true positive rate, we compute the proportion of discovery pairs in which CriticAL correctly decided there was a discrepancy.To compute the false positive rate, we compute the proportion of no-discovery pairs in which CriticAL incorrectly decided there was a discrepancy.</p>
<p>Quantitative Results</p>
<p>In Figure 4, we show the true and false positive rates of CriticAL.As a baseline, we compare CriticAL against a standard set of pre-specified test statistics: mean and variance.From the ROC curve, we see that CriticAL exhibits a favorable trade-off between the true positive rate (power) and false positive rate (type I error), and significantly outperforms the baseline method, achieving a higher true positive rate at all false positive rate levels.As the false positive rate (FPR) calibration plot shows, the false positive rate closely tracks the significance threshold α, showing that CriticAL does not systematically identify spurious discrepancies.These analyses illustrate that CriticAL has favorable statistical properties in a controlled setting.In the appendix (Section A.5), we show examples of CriticAL's proposed test statistics that account for its favorable statistical properties.These statistics are tailored to the statistical model.For example, CriticAL proposes kurtosis for the Student's t setting, to assess the tails of the distribution.</p>
<p>Experiment 3: Analyzing key qualitative properties of test statistics for real-world model-dataset pairs</p>
<p>In the previous sections, we evaluated CriticAL's statistical properties.However, users interacting with an LLM-based critic system may care just as much about key qualitative properties such as transparency (i.e., how clear is the reasoning used to generate the critique) and actionability (i.e., how useful is the criticism to a scientist revising the model).</p>
<p>To study this, we first apply CriticAL to real world datasets and expert written models covering a range of scientific domains (see Section A.3).We quantitatively assess CriticAL-generated critiques in both human and automated LLM-based evaluation.We then qualitatively characterize CriticAL's critiques, which illustrate the conceptual advantages of using CriticAL's tailored test statistics.</p>
<p>Experimental Setup</p>
<p>The Stan PosteriorDB database [19] consists of real-world datasets and probabilistic models implemented in Stan; these models are open-source contributions from the Stan developer community that cover a broad range of modeling motifs ranging from hierarchical modeling to regression.We chose 36 model-dataset pairs based on ones used in several recent papers [17,21,28].For each StanDB model-dataset pair, the LLM proposes twenty-four test statistics {T k } 24 k=1 ; we run this proposal step at a temperature 1.0.Then, for each T k , we generate natural language criticism h k by running the natural language criticism step at a temperature 0.0.</p>
<p>Transparent</p>
<p>Systematic evaluation of qualitative properties of critiques</p>
<p>We conducted a human evaluation study with three Ph.D. students (non-authors) with expertise in statistics, who were blind to the critic methods.We randomly selected ten model-dataset pairs.For each pair, both CriticAL and a naive LLM critic generated critiques.The evaluators chose which critique was better along three criteria that we describe and motivate below.</p>
<ol>
<li>
<p>Transparency: can a user of the system understand how the critique was produced?Transparency is important for building trust with users and can also help them evaluate if the system is hallucinating.</p>
</li>
<li>
<p>Actionable: can the critique help a scientist revise the model?A critique is more useful if it provides insights into how to revise the model.For example, knowing that a model has high error is less useful than knowing that a model has high error on a specific sub-population.</p>
</li>
<li>
<p>Tailored: is the critique targeted for the specific model and dataset?We do not expect generic critiques to provide much insight.</p>
</li>
</ol>
<p>To scale this analysis, we employed state-of-the-art LLM-based judges (gpt-4o-2024-08-06 and claude-3-5-sonnet-20240620) following highly specific guidelines; for details, see the  3) to aid an LLM-based agent in revising an initial model.We show that CriticAL's critiques lead to significant improvements over the initial model.</p>
<p>Experimental Setup</p>
<p>We integrate CriticAL into the model discovery system introduced by Li et al. [17] by giving a revision-LLM three components: the initial model H implemented as a probabilistic program, the test statistic T k , and natural language criticism h k ; the criticism was produced in the previous section by running CriticAL on the (fitted) initial model.We fit the models proposed by the revision LLM using pymc [1]; we repeat each proposal three times at a temperature 0.0 since we noticed non-determinism.We allow the LLM to revise based on a filtered set of significant test statistics.For details, see Section A.7.We report the best model across proposals and test statistics.</p>
<p>Ablation We consider a data-blind LLM critic that receives only the statistical model, implemented as a probabilistic program; we run this at a temperature 0.0.This approach can be effective since modeling assumptions are enumerated in the initial probabilistic program.</p>
<p>Quantitative Results</p>
<p>In Tables 1 and 2, we evaluate CriticAL's ability to produce critiques that improve upon an initial statistical model and outperform the data-blind method.To do this, we first compute the expected log predictive density (ELPD LOO) score for both initial and revised models [26].Next, we calculate the score difference between the initial and revised model and determine the margin of victory by dividing the score difference by the standard errors (SE) of the score difference.For a given dataset, a method is considered to "win" over another at a specific SE margin if the score difference is larger than the margin.For example, if the score difference is 2 and the SE margin is 1, we count it as a significant win.Finally, we compute aggregated win rates at various SE margins (1, 1.5, 2).The win rate is the percentage of datasets where one method outperformed another at a given SE margin.CriticAL's critiques help a revision LLM significantly improve upon the initial model over 80% of the time, which shows that CriticAL reliably produces actionable critiques.CriticAL's successes are often related to sliced statistics discussed in Section 4.3 (e.g., the revision-LLM introduces floor-dependent variance terms).Furthermore, in Table 2, we show that CriticAL also outperforms the data-blind critic.Next, we discuss CriticAL's limitations.</p>
<p>Limitation 1: Suboptimal transformations of data CriticAL does not see the model predictions or data.As a consequence, CriticAL sometimes does not provide good critiques on transforming data.This happens most prominently in the mesquite setting where the model predictions can be negative even though the data is non-negative.</p>
<p>Limitation 2: Correct criticism but imperfect implementation In some cases, CriticAL identifies a legitimate discrepancy that the revision LLM incorrectly implements (e.g., the revision LLM correctly uses a Categorical likelihood but does not transpose the logits correctly).</p>
<p>Conclusion</p>
<p>We introduced CriticAL, a framework for automated model criticism that leverages LLMs to identify discrepancies between a model and dataset and then applies hypothesis tests to assess the significance of discrepancies.CriticAL serves as a lightweight verifier, validating both scientific models and critiques within a hypothesis testing framework.Our experiments demonstrate that CriticAL reliably identifies true discrepancies without hallucinating false critiques.Furthermore, both human and LLM judges preferred CriticAL's critiques over alternative approaches.CriticAL critiques enabled an LLM-based system to substantially improve upon expert designed models.By automating model criticism, CriticAL represents a step toward more reliable automatic scientific discovery systems.</p>
<p>While our evaluation was limited to Bayesian models, which are commonly used in scientific domains, CriticAL's design is versatile: the only requirements are the ability to sample data from the model and a symbolic representation of the model.An exploration of other common classes of scientific models [6] is an exciting direction for future work.</p>
<p>A Appendix</p>
<p>A.1 Prompts and inputs for test statistic proposer step</p>
<p>Critic function prompt</p>
<p>You are a brilliant statistician specializing in critiquing models!Your equally brilliant colleague has come up with a probabilistic program in Stan that proposes a generative/statistical model for the data.Your job is to critique the models and provide hypotheses for discrepancies between the model and the data.To do this, you should write a "test statistic function" in Python.This is motivated by posterior predictive checks in Bayesian statistics.This test function should take as input a dataframe where one of the columns contains the posterior predictive sample.It should return a scalar-valued test statistic.</p>
<p>To quantify discrepancies, I will compute this test statistic for each posterior predictive sample and compare it to observed data.Therefore, choose test statistics that you think will reveal discrepancies between the model and the data.</p>
<p>Natural language criticism prompt</p>
<p>Your equally brilliant colleague has come up with a discrepancy functions that identify possible weaknesses of generative models for data.I will give you the test statistics and the result of computing those test statistics.Your job is to interpret the results of running those test statistics and synthesize the discrepancies.Your synthesis should be as helpful as possible for your colleague who will use this synthesis to improve the model.You will be given one million dollars if you do this well.Focus on being as informative with your synthesis (do not say generic things) to help your colleague understand the test statistic.You should provide a natural language summary of the discrepancy function.Reference specifically the test statistic type and the discrepancy it reveals about specific modeling assumptions.I provide the test statistic Python function and posterior-predictive pval.Posterior predictive p-val: Test statistic function:</p>
<p>Figure 1 :
1
Figure 1: Criticizing scientific models with CriticAL.First, an LLM generates summary statistics that capture potential discrepancies that are tailored to the model and dataset; the LLM conditions on dataset metadata and a symbolic representation of a scientific model.We use these summary statistics to perform hypothesis tests to evaluate the significance of each discrepancy.</p>
<p>Figure 2 :
2
Figure 2: Illustrating how CriticAL avoids hallucinated revisions.CriticAL hypothesizes discrepancies via summary statistics and makes targeted changes to the initial model, which is missing the feature floor.In contrast, the naive method hallucinates (see LLM explanation in figure for details) and introduces spurious features (e.g., county, soil) to the initial model.In the revised model programs, we highlight spurious features in red and correct features in green.</p>
<p>Figure 4 :
4
Figure 4: Statistical analysis of CriticAL's ability to discover discrepancies and avoid hallucinations.(left) True positive rate (TPR) vs. false positive rate (FPR) at different significance thresholds.(right)FPR against significance threshold.CriticAL correctly identifies more discrepancies than the pre-specified method, at the same FPR level.The FPR is calibrated with the significance threshold, showing that CriticAL systematically avoids hallucinations.</p>
<p>Figure 5 :
5
Figure 5: CriticAL criticisms have higher win rates versus naively generated criticisms.Critiques are rated on three qualitative criteria by LLM-based judges (GPT-4o and Claude 3.5 Sonnet).LLM-based judges are aligned with human evaluators: GPT-4o and Claude 3.5 Sonnet have 100% alignment for transparent and tailored preferences, and are 80% and 90% aligned for actionable preferences, respectively.Error bars represent 95% confidence intervals (Wilson score).</p>
<p>( 4 .
4</p>
<p>Figure 7 : 1 . 3 -Figure 8 :
7138
Figure 7: System prompt for proposing test statistics in Section 3.1.We also provide additional instructions on formatting the response and describing the format of the input dataframe.</p>
<p>Figure 9 :
9
Figure 9: Prompt for natural language criticism step in Section 3.2.</p>
<p>Figure 11 :
11
Figure11: LLM judge prompt for determining which model criticism is more transparent.The same prompt structure, with corresponding judging criteria descriptions, is used for actionable and tailored judge prompts.The order of criticisms (CriticAL being Criticism A vs. Criticism B) is randomized to avoid position bias, and impartiality instructions are adapted from the RewardBench[15] judge prompts.</p>
<p>Algorithm 1: Producing test statistics and empirical p-values Input: dataset</p>
<p>D, metadata C, model H, model samples {Y
pred i i=1 , {T k } n } m k=1 ) via Equation 2 } m i=1 , number proposals n k=1 = multiple-test-adjustment({p k } m {T k } n k=1 ∼ p LM (•|C, H) {p k } n k=1 ← get-empirical-pval(D, {Y pred i {p k } n k=1 )Output: test statistics {T k } m k=1 , adjusted empirical p-values {p k } m k=1Proposing discrepancies via test statistics As we saw in Section 2, we can often formalize findingdiscrepancies between model predictions and data as identifying suitable test statistics. Designing</p>
<p>Evaluate discrepancies via hypothesis tests Generate summary statistics
DataBlood GlucosePredictions SleepTimeDataBlood GlucosePredictions ExerciseTime</p>
<p>Table 1 :
1
CriticAL consistently improves over the initial model.CriticAL achieves significantly higher win rates compared to the initial model at various standard error (SE) thresholds.We say a win is significant at a given SE threshold if the difference in scores is larger than the SE margin.
MethodWins &gt; 1 SE Wins &gt; 1.5 SE Wins &gt; 2.0 SECriticAL0.940.940.82Initial Model0.060.060.06MethodWins &gt; 1 SE Wins &gt; 1.5 SE Wins &gt; 2.0 SECriticAL0.590.590.53Data-blind0.290.290.29</p>
<p>Table 2 :
2
CriticAL outperforms data-blind method.CriticAL demonstrates higher win rates across all standard error (SE) margins compared to data-blind method that conditions only on the symbolic representation of the model.</p>
<p>AcknowledgementsThis work was supported in part by AFOSR Grant FA9550-21-1-0397, ONR Grant N00014-22-1-2110, and an NSF Expeditions Grant, Award Number (FAIN) 1918771.EBF is a Chan Zuckerberg Biohub -San Francisco Investigator.We thank Omar Shaikh and Justin Shen for valuable feedback on this paper.We also thank Peter Yoon, Daniel Same, and Artem Khan for discussions.We indicate the test statistic type on the y-axis and slice category on the x-axis; the "agg" slices indicates aggregation across all slices.Blue violins correspond to sliced test statistics and red violins correspond to aggregated ones.The dashed line passes through red violin centers but not the blue ones, showing that CriticAL's choice to slice test statistics reveals discrepancies that pre-specified ones cannot.Appendix A.9.In Figure5, we show the win-rates (higher is better) across two LLM judges and the three criteria.CriticAL is classified as significantly more transparent (∼ 97%), actionable (∼ 76%), and tailored (∼ 98%).Both LLM-based judges are aligned with human preferences, having 100% alignment for transparent and tailored preferences, and GPT-4o and Claude 3.5 Sonnet having 80% and 90% alignment for actionable preferences, respectively.The domain experts gave qualitative feedback in support of CriticAL's approach, particularly in terms of transparency.Experts noted the benefit of immediately executable code for quick assessment (see Appendix A.6 for quotes).Qualitative examples: sliced test statistics One specific kind of test statistic that meets the above three criteria are sliced test statistics.CriticAL often proposes test statistics that slice the model prediction Y pred i based on the input features X .For example, 1 # Filter to get basement and non-basement samples In Figure6, we illustrate the benefits of CriticAL's sliced test statistics over pre-specified, aggregate test statistics.We compare test statistic distributions computed from model samples against the data, sliced by the input values (e.g., variance of model predictions for radon measurements in basement vs first floor).Sliced test statistics reveal discrepancies that the aggregated ones cannot.We provide additional randomly-sampled test statistics in the Appendix (Section A.8).Experiment 4: CriticAL generated criticism drives model improvementsModel criticism should ideally be actionable and aid a user (either LLM or human) in model revision.In our final experiment, we use the model criticism generated by CriticAL in the previous sectionA.2 Examples of natural language critiquesExample natural language critiques produced by CriticAL 1 """ 2 The posterior predictive p-value of 0.0 suggests a significant discrepancy between the observed data and the model predictions regarding the distribution of children's cognitive test scores.The test statistic used here measures the skewness of the predicted scores, indicating that the model's assumption of normally distributed scores may not be appropriate.The very low posterior predictive p-value of 0.0019 suggests that the model does not adequately capture the variability of radon levels across different floor measurements.This discrepancy indicates that the assumption of a homogeneous linear interaction between floor level and radon level across all counties may be too simplistic.5 """ 6 7 """ 8 The model fails to capture the difference in radon levels between measurements taken in the basement versus the first floor.This is indicated by a posterior predictive p-value of 0.0, suggesting that the model does not adequately represent the known higher radon levels typically found in basements compared to the first floor.9 """ 10 11 """ 12 The model's use of a normal distribution to predict the inherently discrete variable 'partyid7' (ranging from 1 to 7) results in a significant proportion of predicted values falling outside this permissible range.13 """A.3 Stan PosteriorDB datasetsWe list the model-dataset pairs criticized in Section 4.3.• radon_mn-radon_variable_slope_noncenteredA.6 Example Qualitative Feedback from Domain ExpertsWe asked Ph.D. students to provide feedback on CriticAL's critiques.Here are two representative quotes: "I liked seeing code I could immediately run and check, it allowed me to take fast action to assess the situation.""I liked when I had code that immediately applied to the model."A.7 Experiment 4 Additional DetailsWe ran the revision process for a single round, at a temperature of 0.0, using 3 proposals in total; we run multiple proposals because temperature 0.0 was not deterministic.We choose the top five test statistics T k , ranked by p-value, where the Bonferonni-adjusted p-value &lt; 0.15.We choose top five to limit the number of models fit since the fitting procedure is computationally-intensive. We choose the cutoff value by examining the spread of the Bonferonni-adjusted p-values; there were 18 significant discrepancies.While this threshold is larger than a typical p-value, discrepancies that do not meet the traditional significance levels may nevertheless be valuable in the context of a closed-loop model discovery process.A less stringent threshold allows us to evaluate lower-significance discrepancies that still may improve model performance.A.8 Further test statistics for Experiment 3Example test statistics produced by by CriticAL 1 def test_statistic(df):
PyMC: a modern, and comprehensive probabilistic programming framework in python. Oriol Abril-Pla, Virgile Andreani, Colin Carroll, Larry Dong, Christopher J Fonnesbeck, Maxim Kochurov, Ravin Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, Michael Osthege, Ricardo Vieira, Thomas Wiecki, Robert Zinkov, PeerJ Computer Science. 92023</p>
<p>P values for composite null models. M J Bayarri, James O Berger, Journal of the American Statistical Association. 01621459954522000</p>
<p>The analysis of repeated-measures data on schizophrenic reaction times using mixture models. Thomas R Belin, Donald B Rubin, Statistics in medicine. 141995</p>
<p>Sampling and Bayes' Inference in Scientific Modelling and Robustness. E P George, Box, Journal of the Royal Statistical Society. Series A (General). 0035923814341980</p>
<p>Stan: A probabilistic programming language. Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell, Journal of statistical software. 7612017</p>
<p>The frontier of simulation-based inference. Kyle Cranmer, Johann Brehmer, Gilles Louppe, Proceedings of the National Academy of Sciences. 1172019</p>
<p>Philosophy and the practice of Bayesian statistics in the Social Sciences. Andrew Gelman, Cosma Rohilla, Shalizi , 10.1093/oxfordhb/9780195392753.013.0011August 2012</p>
<p>Posterior predictive assessment of model fitness via realized discrepancies. Andrew Gelman, Xiao-Li Meng, Hal Stern, Statistica Sinica. 10170405641996. 19968507</p>
<p>Multiple Imputation for Model Checking: Completed-Data Plots with Missing and Latent Data. Andrew Gelman, Iven Mechelen, Geert Verbeke, Daniel Heitjan, Michel Meulders, Biometrics. 6104 2005</p>
<p>Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, Donald B Rubin, Bayesian data analysis. 2013third edition</p>
<p>The principles and practice of probabilistic programming. D Noah, Goodman, 10.1145/2429069.2429117Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL '13. the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL '13New York, NY, USAAssociation for Computing Machinery2013</p>
<p>MLAgentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>Engineering-driven statistical adjustment and calibration. V , Roshan Joseph, Huan Yan, 10.1080/00401706.2014.902773Technometrics. 5722015</p>
<p>Bayesian calibration of computer models. Marc C Kennedy, Anthony O' Hagan, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 632001</p>
<p>Rewardbench: Evaluating reward models for language modeling. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hannaneh Smith, Hajishirzi, 2024</p>
<p>Scientific discovery: computational explorations of the creative processes. Pat Langley, Herbert A Simon, Gary L Bradshaw, Jan M Zytkow, January 1987</p>
<p>Automated Statistical Model Discovery with Language Models. Emily B Michael Y Li, Noah D Fox, Goodman, International Conference on Machine Learning (ICML). 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 2024</p>
<p>posteriordb: a set of posteriors for Bayesian inference and probabilistic programming. Måns Magnusson, Paul Bürkner, Aki Vehtari, October 2023</p>
<p>Posterior Predictive p-Values. Xiao-Li Meng, 10.1214/aos/1176325622The Annals of Statistics. 2231994</p>
<p>Variational Inference with Gaussian Score Matching. Chirag Modi, Robert M Gower, Charles Margossian, Yuling Yao, David Blei, Lawrence K Saul, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician. Donald B Rubin, The Annals of Statistics. 1241984</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>An Introduction to Probabilistic Programming. Jan-Willem Van De Meent, Brooks Paige, Hongseok Yang, Frank Wood, 2021</p>
<p>David A Van Dyk, Hosung Kang, Highly Structured Models for Spectral Analysis in High-Energy Astrophysics. 200419</p>
<p>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Aki Vehtari, Andrew Gelman, Jonah Gabry, 10.1007/s11222-016-9696-4Statistics and Computing. 0960-3174275sep 2017</p>
<p>Automating science. David Waltz, Bruce G Buchanan, 10.1126/science.1172781Science. 32459232009</p>
<p>Foundation Posteriors for Approximate Probabilistic Inference. Mike Wu, Noah Goodman, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Ziwei Xu, Sanjay Jain, Mohan S Kankanhalli, ArXiv, abs/2401.118172024267069207</p>
<p>Prompt used for Qualitative Criteria LLM Judges LLM Judge Prompt for Qualitative Criteria 1 You are an expert statistical modeling and data science. Your task is to determine which model criticism is more transparent. </p>
<p>Context: 4 Dataset Description. </p>
<p>. Column Description. </p>
<p>10 Model being criticized: 11. </p>
<p>Criticism B: 21 <Criticism B> 22 {criticism_b} 23 </Criticism B> 24 25 Please evaluate the transparency of the criticisms based on the following criteria, assuming the intended evaluator is a data scientist or statistician: 26 -How clear is the methodology used to generate the criticism?. 27 -How explicitly are the relevant parts of the dataset identified in the criticism? 28 -How unambiguous is the process of determining the criticism's conclusions? 29</p>
<p>provide a detailed analysis of how each criticism meets the criteria and compare Criticism A and Criticism B. Second, state "A" or "B" to indicate which criticism is more transparent. First, </p>
<p>32 Important: 33 -Avoid any position biases and ensure that the order in which the criticisms were presented does not influence your decision. 34 -Do not allow the length of the responses to influence your evaluation. 35 -Do not favor certain names of the criticisms. 36 -Be as objective as possible</p>
<p>38 Provide your response in the following format: 39 Comparison: <Detailed reasoning and comparison to determine prefered criticism> 40 Final Response: &lt;"A" or. </p>            </div>
        </div>

    </div>
</body>
</html>
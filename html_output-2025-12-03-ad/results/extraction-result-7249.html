<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7249 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7249</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7249</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-1917c0ef32ec3b785d595b5517b73eaabefda37b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1917c0ef32ec3b785d595b5517b73eaabefda37b" target="_blank">Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A systematic review of Large Language Models' capabilities across three important cognitive domains: decision-making biases, reasoning, and creativity reveals that while LLMs demonstrate several human-like biases, some biases observed in humans are absent, indicating cognitive patterns that only partially align with human decision-making.</p>
                <p><strong>Paper Abstract:</strong> Research on emergent patterns in Large Language Models (LLMs) has gained significant traction in both psychology and artificial intelligence, motivating the need for a comprehensive review that offers a synthesis of this complex landscape. In this article, we systematically review LLMs' capabilities across three important cognitive domains: decision-making biases, reasoning, and creativity. We use empirical studies drawing on established psychological tests and compare LLMs' performance to human benchmarks. On decision-making, our synthesis reveals that while LLMs demonstrate several human-like biases, some biases observed in humans are absent, indicating cognitive patterns that only partially align with human decision-making. On reasoning, advanced LLMs like GPT-4 exhibit deliberative reasoning akin to human System-2 thinking, while smaller models fall short of human-level performance. A distinct dichotomy emerges in creativity: while LLMs excel in language-based creative tasks, such as storytelling, they struggle with divergent thinking tasks that require real-world context. Nonetheless, studies suggest that LLMs hold considerable potential as collaborators, augmenting creativity in human-machine problem-solving settings. Discussing key limitations, we also offer guidance for future research in areas such as memory, attention, and open-source model development.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7249.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7249.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Binz & Schulz)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (as evaluated by Binz & Schulz)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 evaluated on classical cognitive-psychology vignette tasks (e.g., Linda conjunction task, cab problem) and multiple-choice bias questions; showed some human-like biases but overall did not reach human-level performance and was sensitive to minor vignette changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand gpt-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language model developed by OpenAI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Vignette-based cognitive tasks (Linda problem, cab problem) and Kahneman & Tversky multiple-choice bias questions</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Decision-making / heuristics and biases</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Classic short vignette problems and multiple-choice items designed to elicit human decision heuristics (e.g., conjunction fallacy, framing); participants provide judgments/choices based on short descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Task accuracy / correct normative response rate; bias presence reported qualitatively</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Reported implicitly as human-level normative responses (higher than model); exact numeric baseline not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Approximately correct on some standard vignettes, above-chance on task-benchmarks but below human-level overall; sensitive to small vignette changes</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Classic human cognitive psychology tasks (Kahneman & Tversky style vignettes referenced in Binz & Schulz)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3 exhibited framing effect, certainty effect, and overweighting bias; it avoided the conjunction fallacy in canonical wording but performance fell with small paraphrases, suggesting brittle generalization/overfitting to training distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7249.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-tuned vs pre-trained models (Itzhak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-tuned and pre-trained LLMs (e.g., GPT-3, Mistral-7B, T5 variants) as evaluated by Itzhak et al.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between instruction-tuned and pre-trained variants on cognitive-bias tasks (decoy effect, certainty effect, belief bias) showing instruction-tuned models tend to exhibit stronger biases aligning with human-bias theory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, Mistral-7B, T5 (instruction-tuned and pre-trained variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various transformer language models tested in both pre-trained and instruction-tuned (fine-tuned on instruction-style corpora) forms.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Decoy effect, certainty effect, belief bias tasks (curated dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Decision-making / heuristics and biases</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Curated decision problems designed to reveal choice anomalies (decoy/attraction effects) and belief-driven logical acceptance (belief bias).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Qualitative presence/absence of biases; comparative bias magnitude between model variants</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human-bias patterns from cognitive-bias literature (Tversky & Kahneman and related work); exact numeric baselines not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Instruction-tuned models exhibited biases aligned with human theories; instruction-tuned variants showed more bias than pre-trained counterparts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-tuned models (i.e., fine-tuned with instruction-style data) versus pre-trained (non-instruction-tuned) variants</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human biases literature (e.g., Tversky & Kahneman) and the curated cognitive-bias dataset used by Itzhak et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors report that instruction-tuning (and RLHF-style tuning) can amplify cognitive biases present in the model; comparison is qualitative in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7249.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (Chen et al. econ rationality)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 evaluated on budgetary/economic rationality experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 assessed on curated budgetary experiments measuring economic rationality; reported to outperform human subjects on four decision-making tasks but performance fell under nonstandard price presentations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The emergence of economic rationality of gpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned sibling model in the GPT family with improved task performance over GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Budgetary experiments testing economic rationality (risk, time, social, food decisions)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Decision-making / rationality</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Curated economic choice tasks designed to measure consistency with revealed-preference/economic rationality across domains (risk, intertemporal choice, social preferences, food choices).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Rationality score / task-specific accuracy or consistency metric (as reported by Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human subjects baseline (lower rationality scores than model in reported tasks); exact numeric values not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to outperform human subjects on the four decision-making tasks under standard price presentations; degraded with atypical price formats</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Curated typical budgetary experiments (as used in Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance advantage is contingent on standard/predictable input formats; model brittleness to presentation changes was highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7249.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Su et al., newsvendor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on the newsvendor economic decision problem</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 tested on the newsvendor inventory decision problem and displayed several human-like biases (demand-chasing, risk- and loss-aversion) while not showing some other common human biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can ai solve newsvendor problem without making biased decisions? a behavioral experimental study.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large transformer language model by OpenAI, exhibiting advanced reasoning and decision tendencies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Newsvendor problem</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Decision-making / economic biases</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Single-period inventory decision problem where agent chooses stocking quantity given demand distribution and profit margins; human studies historically show systematic biases (demand-chasing, loss aversion).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Behavioral biases observed in choices (e.g., frequency of demand-chasing, degree of risk-aversion); comparison to human patterns</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human decision-makers in prior experiments show demand-chasing, risk- and loss-aversion patterns (no numeric baseline provided in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 exhibited demand-chasing, risk-aversion in high-margin scenarios, and loss-aversion in low-margin scenarios; did not exhibit waste aversion, stockout aversion, underestimated opportunity costs, or minimized ex-post inventory errors</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Behavioral experimental literature on newsvendor problem (e.g., Schweitzer & Cachon 2000) and the experimental human comparisons in Su et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report significant tendencies for demand-chasing and some biases (no p-values given in the review)</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 combined human-like biases with some rational responsiveness to incentives; absence of some human biases noted as informative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7249.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guanaco / MPT / BLOOM / Falcon (Wason task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Various open models (Guanaco, MPT, BLOOM, Falcon) evaluated on the Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multiple open and closed LLMs tested on the classical Wason selection deductive reasoning task; all models underperformed compared to humans and showed distinct model-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating the deductive competence of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Guanaco, MPT, BLOOM, Falcon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variety of transformer-based LLMs (open-source and research models) evaluated on deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A conditional-rule selection task assessing ability to choose which cards to turn over to test a conditional rule; widely used to reveal human reasoning biases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Task accuracy (correct cards selected) / qualitative error patterns</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans typically perform above chance but show systematic selection biases (exact numeric baseline not provided in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All tested models performed worse than humans on the conventional formulation and did not improve substantially with presentation changes</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Classic human experimental literature on the Wason selection task (Wason 1968) and the human comparisons in Seals & Shalin</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Models revealed unique reasoning biases that differ from humans; format sensitivity was observed but did not yield marked improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7249.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syllogistic reasoning models (Ando et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa, BART, GPT-3.5 evaluated on syllogistic reasoning derived from BAROCO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer models tested on syllogistic reasoning problems showing performance degraded by human-like biases (belief bias, conversion errors, atmosphere effects) and poorer performance relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models with neubaroco: Syllogistic reasoning ability and human-like biases.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa, BART, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder/sequence-to-sequence and generative models evaluated on formal syllogistic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogistic reasoning (BAROCO-derived dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Structured syllogisms testing formal deductive inference and susceptibility to belief-driven errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Logical validity classification / accuracy; presence of belief- and conversion-related errors</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans exhibit belief biases but typically perform better overall than the models on the evaluated syllogisms (no numeric baseline in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Models underperformed relative to humans and were heavily influenced by human-like biases</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Syllogistic reasoning literature and the BAROCO-derived human benchmarks used by Ando et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Errors frequently reflected content/belief interference rather than pure logical failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7249.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cognitive Reflection Test (Hagendorff et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple GPT family models evaluated on Cognitive Reflection Test and semantic illusions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2/3/3.5/4 compared on tasks distinguishing intuitive (System 1) vs deliberative (System 2) reasoning; larger/newer models (GPT-3.5, GPT-4) shift towards more deliberative, System-2-like responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-3, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Series of GPT-family autoregressive transformer models of increasing capability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Cognitive Reflection Test (CRT) and semantic illusion tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / reflective vs intuitive processing</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>CRT items probe tendency to override intuitive but incorrect responses with deliberative correct answers; semantic illusion tasks probe susceptibility to meaning-based traps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Correct response rate on CRT items; qualitative classification of intuitive vs deliberative answers</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans show mixed CRT performance with clear distinction between intuitive errors and reflective corrections (no numeric baseline given in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Smaller/earlier models responded more intuitively; GPT-3.5 and GPT-4 exhibited more deliberative (System-2-like) responses and avoided some semantic traps</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Cognitive Reflection Test literature (Frederick 2005) and semantic illusion studies</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Shift toward deliberative answers may reflect model scale and instruction-tuning; chain-of-thought prompting not always required but commonly appears in responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7249.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Han et al., inductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on category-based induction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 showed substantial improvement on inductive/category-based induction tasks, achieving performance comparable to humans, whereas GPT-3 performed poorly and qualitatively differently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inductive reasoning in humans and large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced transformer LLM by OpenAI with stronger inductive reasoning abilities compared to earlier generations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Category-based induction (curated dataset inspired by Osherson et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / inductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks require generalizing properties from observed categories to novel cases and assigning plausibility/probability to inductive generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Inductive generalization accuracy / plausibility judgments compared to human responses</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans provide the benchmark of plausible inductive generalizations; GPT-4 reported to reach comparable levels (no numeric values in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 achieved performance comparable to humans; GPT-3 performed poorly</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Category-based induction literature (Osherson et al. 1990) and human comparisons in Han et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Indicates strong improvement with model scale; details of exact prompting and numeric comparisons were not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7249.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT (RAVEN analogical tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT family (125M, 1.3B, 13B) and text-davinci-002 on RAVEN-style in-context analogical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller OPT variants and text-davinci-002 were used with language encodings of perceptual features to perform Raven's Progressive Matrices-style relational reasoning, often surpassing human performance on text-based analogical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-context analogical reasoning with pre-trained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT-125M, OPT-1.3B, OPT-13B, text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OPT family of open pre-trained transformer models at multiple scales and an OpenAI instruction-capable davinci variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M; 1.3B; 13B for OPT variants; text-davinci-002 size not specified</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>RAVEN (relational/analogical visual reasoning) encoded into language</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / analogical / fluid intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Raven-style visual analogy problems encoded into textual descriptions of perceptual features, then solved by language models in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on RAVEN-style problems; comparison to human and supervised vision baselines</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baselines for RAVEN are standard; review reports models often surpass human performance on the language-encoded variants (no numeric human baseline provided)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>These models often surpassed human performance and approached supervised vision-based method performance on the language-encoded tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot / in-context encoding of perceptual features into text</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>RAVEN dataset (Zhang et al. 2019) and comparisons to human/fitted baselines in Hu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance may reflect advantages of large-scale pattern learning when perceptual relations are translated to language; may not generalize to raw visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7249.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 / GPT-4 (Webb et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 and GPT-4 on zero-shot analogical reasoning tasks including Raven variants and visual analogies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 matched or surpassed human performance on several text-based analogy problems; GPT-4 showed stronger performance, suggesting scaling improves sensitivity to causal relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent analogical reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative transformer LLMs of successive generations, evaluated zero-shot on analogy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Zero-shot analogy tasks (Raven variant; visual analogy problem set)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / analogical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Analogy and pattern-completion tasks probing fluid intelligence via abstract relations among elements; presented in text or text-encoded vision proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on analogy problems; comparative performance to human participants</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans used as baseline on analogy datasets; GPT-3 matched or surpassed human performance on several text-based items (no numeric baselines in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 matched/surpassed humans on several tasks; GPT-4 showed even stronger performance</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot (text prompts) / text-encoded visual descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Analogy problem sets commonly used as fluid-intelligence proxies (e.g., Raven-like datasets) as reported in Webb et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Claims concern text-encoded analogies; visual raw-input performance may differ; scaling and better task formulations improve outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7249.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Various LLMs (Palminteri et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple LLMs (GPT-3, GPT-3.5, OPT, BLOOM, LLAMA, VICUNA, GPT-4) evaluated on CRT variants and Linda/Bill conjunction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of reasoning biases: earlier models relied on heuristic/intuitive responses and underperformed versus humans, while GPT-4 exhibited reduced heuristic reasoning and in some cases super-human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Studying and improving reasoning in humans and machines (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-3.5, OPT, BLOOM, LLAMA, VICUNA, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Range of transformer LLMs including open-source and proprietary models evaluated on reasoning-bias tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Variants of the Cognitive Reflection Test and Linda/Bill conjunction fallacy problems</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / heuristics and biases</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Variants probe susceptibility to intuitive heuristic responses (CRT) and conjunction fallacy (Linda/Bill) to compare heuristic vs deliberative tendencies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Rate of heuristic (intuitive) vs deliberative (normative) responses; accuracy on normative answers</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans show mixed performance with characteristic rates of conjunction errors and CRT failures; specific numeric baselines not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Earlier models predominantly heuristic and underperformed vs humans; GPT-4 showed strong (sometimes super-human) performance with minimal reliance on intuitions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>CRT and conjunction-fallacy literature and human benchmarks used in Palminteri et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Indicates substantial variation across model generations; GPT-4 often avoids common heuristic traps that earlier models fall into.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7249.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla / PaLM 2 / Flan-PaLM 2 / GPT-3.5 (Dasgupta et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla, PaLM 2-M/L, Flan-PaLM 2, GPT-3.5 evaluated on mixed logical tasks including Wason selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs demonstrate human-like content effects: performance varies by scenario/context and they struggle more on abstract or conceptually counterintuitive problems, paralleling human inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models show human-like content effects on reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla, PaLM 2-M, PaLM 2-L, Flan-PaLM 2, GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Series of transformer LLMs from different families evaluated on logical and abductive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Combined reasoning tasks (Wason selection + syllogisms + NLI variants)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning / abductive & deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A set of logical tasks probing how content/context influences reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and consistency across content conditions; presence of content effects</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans show content effects and inconsistent performance across scenarios; used as baseline comparison (no numeric values)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Models mirrored human-like content effects and struggled particularly on abstract or counter-intuitive cases</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human reasoning literature and empirical baselines used in Dasgupta et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Findings indicate LLMs replicate not only errors but also systematic context sensitivity akin to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7249.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Stevenson et al., AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 evaluated on the Alternative Uses Test (AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 produced uses for common objects that had higher utility but lower originality, surprise, and semantic distance compared to human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer model evaluated on divergent thinking task AUT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Alternative Uses Test (AUT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Creativity / divergent thinking</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants generate many alternative uses for common objects; responses are rated on originality, surprise, semantic distance, and utility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Ratings on originality, surprise, semantic distance, and utility</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human participants scored higher on originality, surprise, and semantic distance; numeric values not provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 scored lower on originality, surprise, and semantic distance but higher on utility ratings</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human AUT participant data as used in Stevenson et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Negative association between originality and utility observed in both human and model responses; GPT-3 leaned toward pragmatic/useful ideas over novel ones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7249.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Hubert et al., AUT/TTCT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on AUT, Consequences Task and Divergent Associations / TTCT variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 generated more diverse responses with greater elaboration and higher originality (under certain prompts) than human counterparts on several creativity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced transformer LLM exhibiting strong language-based creative generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Alternative Uses Test (AUT), Consequences Task (TTCT), Divergent Associations Task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Creativity / divergent thinking and creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>AUT: generate alternative uses; Consequences Task: generate consequences of hypothetical events; Divergent Associations: produce semantically distant associations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Originality, elaboration, fluency-controlled metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human participants served as baseline; GPT-4 produced greater originality and elaboration in these tasks (no numeric baselines provided)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 produced more diverse responses and higher originality/elaboration relative to humans under specified prompt regimes</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Specific prompting (review notes GPT-4 higher originality with particular prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participant data in Hubert et al.'s experimental comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Suggests LLMs excel in language-centric creativity measures but may depend on prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7249.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 / text-davinci-003 (Yiu et al., tool task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 and text-davinci-003 evaluated on a novel tool selection task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models nearly matched humans in recognizing superficial similarities between objects but lagged significantly behind adults and children when asked to choose unfamiliar tools for novel problems, defaulting to conventional solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned and generative transformer models evaluated on novel tool-use/problem-solving tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Novel tool selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Creativity / novel problem-solving / functional fixedness</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Task requires selecting or proposing nonstandard object uses/tools to accomplish a goal without canonical tools; measures novelty and functional flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy/choice of novel tool; comparison to adult and child performance</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Adults and children outperformed models on selecting unfamiliar/novel tools (no numeric baselines in review)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Models significantly lagged behind both adults and children, often defaulting to conventional tool choices</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Experimental human comparisons conducted by Yiu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Highlights limitations in physical/embodied reasoning and true divergent creative problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7249.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTCW creative writing (Chakrabarty et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5, GPT-4, Claude-v1.3 evaluated on the Torrance Test of Creative Writing (TTCW)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs' generated stories were significantly less likely to pass individual TTCW tests than stories written by human experts, indicating deficits in certain creative-writing criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Claude-v1.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned and research LLMs tested on formal creative-writing assessment derived from Torrance tests.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Torrance Test of Creative Writing (TTCW)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Creativity / creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Verbal creative-writing assessment measuring factors like fluency, originality, elaboration, and narrative quality (adapted Torrance test for writing).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pass rates on individual TTCW tests; comparative story quality ratings</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human expert writers passed individual TTCW tests at a higher rate than the models (no numeric baselines provided)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Stories from all tested models were significantly less likely to pass TTCW tests compared to human experts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human expert story baselines used in Chakrabarty et al.'s TTCW evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported as significant (no p-values in review)</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Indicates nuance: LLMs can excel in some open-ended story tasks but fail on structured creative-writing criteria developed for humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7249.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Five-sentence story task (Orwig et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 and GPT-4 evaluated on five-sentence creative story production</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On a constrained five-sentence creative story task with a three-word prompt, GPT-3 and GPT-4 produced stories comparable in creativity to human participants; GPT-4 showed consistency with human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative transformer LLMs tested on a short-form creative-writing task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Five-sentence creative story task (three-word prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Creativity / creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants produce a short (~5 sentence) story including all three prompt words; stories are rated for creativity by human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Creativity ratings by human judges; comparative agreement with human evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human-authored stories serve as baseline; models achieved comparable creativity ratings (no numeric values provided)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Both GPT-3 and GPT-4 produced stories rated comparable to human stories; GPT-4 was particularly consistent with human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Constrained prompt (three-word prompt) indicating few-shot/structured generation</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human judge ratings used in Orwig et al.'s study</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Suggests LLMs can match humans on certain constrained, language-centric creative tasks, though other creative-writing metrics may differ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7249.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7249.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT as creative assistant (Lee & Chung)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (ChatGPT) evaluated as an assistive tool for human creativity across five tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When used as an assistant, ChatGPT significantly improved human idea-generation creativity compared to web-search or no-technology conditions, particularly for incremental idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interactive instruction-tuned conversational LLM used to support human creative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Five curated creative-assistance tasks (gift choice, toy making, repurposing items, design tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Creativity / assistive creativity / human-AI collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Human participants generate ideas alone, with web search, or with ChatGPT assistance; outputs are rated for creativity and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Creativity ratings of final ideas; comparative improvement vs other conditions</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human-only and human+web-search baselines; ChatGPT-assisted humans produced significantly more creative outputs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>As assistant, ChatGPT improved human creativity markedly (particularly for incremental novelty); not a standalone metric but collaborative improvement</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Interactive conversational use (human-guided prompts / iterative prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participant comparisons in Lee & Chung's experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported significant improvements (no p-values provided in review)</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Highlights LLMs' utility as augmentative tools in creative workflows even when they may not be maximally novel on their own.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand gpt-3. <em>(Rating: 2)</em></li>
                <li>Inductive reasoning in humans and large language models. <em>(Rating: 2)</em></li>
                <li>Emergent analogical reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. <em>(Rating: 1)</em></li>
                <li>Language models show human-like content effects on reasoning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7249",
    "paper_id": "paper-1917c0ef32ec3b785d595b5517b73eaabefda37b",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-3 (Binz & Schulz)",
            "name_full": "Generative Pre-trained Transformer 3 (as evaluated by Binz & Schulz)",
            "brief_description": "GPT-3 evaluated on classical cognitive-psychology vignette tasks (e.g., Linda conjunction task, cab problem) and multiple-choice bias questions; showed some human-like biases but overall did not reach human-level performance and was sensitive to minor vignette changes.",
            "citation_title": "Using cognitive psychology to understand gpt-3.",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Large autoregressive transformer language model developed by OpenAI.",
            "model_size": "175B",
            "test_name": "Vignette-based cognitive tasks (Linda problem, cab problem) and Kahneman & Tversky multiple-choice bias questions",
            "test_category": "Decision-making / heuristics and biases",
            "test_description": "Classic short vignette problems and multiple-choice items designed to elicit human decision heuristics (e.g., conjunction fallacy, framing); participants provide judgments/choices based on short descriptions.",
            "evaluation_metric": "Task accuracy / correct normative response rate; bias presence reported qualitatively",
            "human_performance": "Reported implicitly as human-level normative responses (higher than model); exact numeric baseline not provided in review",
            "llm_performance": "Approximately correct on some standard vignettes, above-chance on task-benchmarks but below human-level overall; sensitive to small vignette changes",
            "prompting_method": null,
            "fine_tuned": false,
            "human_data_source": "Classic human cognitive psychology tasks (Kahneman & Tversky style vignettes referenced in Binz & Schulz)",
            "statistical_significance": null,
            "notes": "GPT-3 exhibited framing effect, certainty effect, and overweighting bias; it avoided the conjunction fallacy in canonical wording but performance fell with small paraphrases, suggesting brittle generalization/overfitting to training distributions.",
            "uuid": "e7249.0",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Instruction-tuned vs pre-trained models (Itzhak et al.)",
            "name_full": "Instruction-tuned and pre-trained LLMs (e.g., GPT-3, Mistral-7B, T5 variants) as evaluated by Itzhak et al.",
            "brief_description": "Comparison between instruction-tuned and pre-trained variants on cognitive-bias tasks (decoy effect, certainty effect, belief bias) showing instruction-tuned models tend to exhibit stronger biases aligning with human-bias theory.",
            "citation_title": "Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias.",
            "mention_or_use": "use",
            "model_name": "GPT-3, Mistral-7B, T5 (instruction-tuned and pre-trained variants)",
            "model_description": "Various transformer language models tested in both pre-trained and instruction-tuned (fine-tuned on instruction-style corpora) forms.",
            "model_size": null,
            "test_name": "Decoy effect, certainty effect, belief bias tasks (curated dataset)",
            "test_category": "Decision-making / heuristics and biases",
            "test_description": "Curated decision problems designed to reveal choice anomalies (decoy/attraction effects) and belief-driven logical acceptance (belief bias).",
            "evaluation_metric": "Qualitative presence/absence of biases; comparative bias magnitude between model variants",
            "human_performance": "Human-bias patterns from cognitive-bias literature (Tversky & Kahneman and related work); exact numeric baselines not provided in review",
            "llm_performance": "Instruction-tuned models exhibited biases aligned with human theories; instruction-tuned variants showed more bias than pre-trained counterparts",
            "prompting_method": "Instruction-tuned models (i.e., fine-tuned with instruction-style data) versus pre-trained (non-instruction-tuned) variants",
            "fine_tuned": true,
            "human_data_source": "Human biases literature (e.g., Tversky & Kahneman) and the curated cognitive-bias dataset used by Itzhak et al.",
            "statistical_significance": null,
            "notes": "Authors report that instruction-tuning (and RLHF-style tuning) can amplify cognitive biases present in the model; comparison is qualitative in the review.",
            "uuid": "e7249.1",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3.5 (Chen et al. econ rationality)",
            "name_full": "GPT-3.5 evaluated on budgetary/economic rationality experiments",
            "brief_description": "GPT-3.5 assessed on curated budgetary experiments measuring economic rationality; reported to outperform human subjects on four decision-making tasks but performance fell under nonstandard price presentations.",
            "citation_title": "The emergence of economic rationality of gpt.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Instruction-tuned sibling model in the GPT family with improved task performance over GPT-3.",
            "model_size": null,
            "test_name": "Budgetary experiments testing economic rationality (risk, time, social, food decisions)",
            "test_category": "Decision-making / rationality",
            "test_description": "Curated economic choice tasks designed to measure consistency with revealed-preference/economic rationality across domains (risk, intertemporal choice, social preferences, food choices).",
            "evaluation_metric": "Rationality score / task-specific accuracy or consistency metric (as reported by Chen et al.)",
            "human_performance": "Human subjects baseline (lower rationality scores than model in reported tasks); exact numeric values not provided in review",
            "llm_performance": "Reported to outperform human subjects on the four decision-making tasks under standard price presentations; degraded with atypical price formats",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Curated typical budgetary experiments (as used in Chen et al.)",
            "statistical_significance": null,
            "notes": "Performance advantage is contingent on standard/predictable input formats; model brittleness to presentation changes was highlighted.",
            "uuid": "e7249.2",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4 (Su et al., newsvendor)",
            "name_full": "GPT-4 evaluated on the newsvendor economic decision problem",
            "brief_description": "GPT-4 tested on the newsvendor inventory decision problem and displayed several human-like biases (demand-chasing, risk- and loss-aversion) while not showing some other common human biases.",
            "citation_title": "Can ai solve newsvendor problem without making biased decisions? a behavioral experimental study.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art large transformer language model by OpenAI, exhibiting advanced reasoning and decision tendencies.",
            "model_size": null,
            "test_name": "Newsvendor problem",
            "test_category": "Decision-making / economic biases",
            "test_description": "Single-period inventory decision problem where agent chooses stocking quantity given demand distribution and profit margins; human studies historically show systematic biases (demand-chasing, loss aversion).",
            "evaluation_metric": "Behavioral biases observed in choices (e.g., frequency of demand-chasing, degree of risk-aversion); comparison to human patterns",
            "human_performance": "Human decision-makers in prior experiments show demand-chasing, risk- and loss-aversion patterns (no numeric baseline provided in review)",
            "llm_performance": "GPT-4 exhibited demand-chasing, risk-aversion in high-margin scenarios, and loss-aversion in low-margin scenarios; did not exhibit waste aversion, stockout aversion, underestimated opportunity costs, or minimized ex-post inventory errors",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Behavioral experimental literature on newsvendor problem (e.g., Schweitzer & Cachon 2000) and the experimental human comparisons in Su et al.",
            "statistical_significance": "Authors report significant tendencies for demand-chasing and some biases (no p-values given in the review)",
            "notes": "GPT-4 combined human-like biases with some rational responsiveness to incentives; absence of some human biases noted as informative.",
            "uuid": "e7249.3",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Guanaco / MPT / BLOOM / Falcon (Wason task)",
            "name_full": "Various open models (Guanaco, MPT, BLOOM, Falcon) evaluated on the Wason selection task",
            "brief_description": "Multiple open and closed LLMs tested on the classical Wason selection deductive reasoning task; all models underperformed compared to humans and showed distinct model-specific biases.",
            "citation_title": "Evaluating the deductive competence of large language models.",
            "mention_or_use": "use",
            "model_name": "Guanaco, MPT, BLOOM, Falcon",
            "model_description": "Variety of transformer-based LLMs (open-source and research models) evaluated on deductive reasoning.",
            "model_size": null,
            "test_name": "Wason selection task",
            "test_category": "Reasoning / deductive reasoning",
            "test_description": "A conditional-rule selection task assessing ability to choose which cards to turn over to test a conditional rule; widely used to reveal human reasoning biases.",
            "evaluation_metric": "Task accuracy (correct cards selected) / qualitative error patterns",
            "human_performance": "Humans typically perform above chance but show systematic selection biases (exact numeric baseline not provided in review)",
            "llm_performance": "All tested models performed worse than humans on the conventional formulation and did not improve substantially with presentation changes",
            "prompting_method": null,
            "fine_tuned": false,
            "human_data_source": "Classic human experimental literature on the Wason selection task (Wason 1968) and the human comparisons in Seals & Shalin",
            "statistical_significance": null,
            "notes": "Models revealed unique reasoning biases that differ from humans; format sensitivity was observed but did not yield marked improvement.",
            "uuid": "e7249.4",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Syllogistic reasoning models (Ando et al.)",
            "name_full": "RoBERTa, BART, GPT-3.5 evaluated on syllogistic reasoning derived from BAROCO",
            "brief_description": "Transformer models tested on syllogistic reasoning problems showing performance degraded by human-like biases (belief bias, conversion errors, atmosphere effects) and poorer performance relative to humans.",
            "citation_title": "Evaluating large language models with neubaroco: Syllogistic reasoning ability and human-like biases.",
            "mention_or_use": "use",
            "model_name": "RoBERTa, BART, GPT-3.5",
            "model_description": "Transformer encoder/sequence-to-sequence and generative models evaluated on formal syllogistic reasoning tasks.",
            "model_size": null,
            "test_name": "Syllogistic reasoning (BAROCO-derived dataset)",
            "test_category": "Reasoning / deductive reasoning",
            "test_description": "Structured syllogisms testing formal deductive inference and susceptibility to belief-driven errors.",
            "evaluation_metric": "Logical validity classification / accuracy; presence of belief- and conversion-related errors",
            "human_performance": "Humans exhibit belief biases but typically perform better overall than the models on the evaluated syllogisms (no numeric baseline in review)",
            "llm_performance": "Models underperformed relative to humans and were heavily influenced by human-like biases",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Syllogistic reasoning literature and the BAROCO-derived human benchmarks used by Ando et al.",
            "statistical_significance": null,
            "notes": "Errors frequently reflected content/belief interference rather than pure logical failures.",
            "uuid": "e7249.5",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Cognitive Reflection Test (Hagendorff et al.)",
            "name_full": "Multiple GPT family models evaluated on Cognitive Reflection Test and semantic illusions",
            "brief_description": "GPT-2/3/3.5/4 compared on tasks distinguishing intuitive (System 1) vs deliberative (System 2) reasoning; larger/newer models (GPT-3.5, GPT-4) shift towards more deliberative, System-2-like responses.",
            "citation_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt.",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-3, GPT-3.5, GPT-4",
            "model_description": "Series of GPT-family autoregressive transformer models of increasing capability.",
            "model_size": null,
            "test_name": "Cognitive Reflection Test (CRT) and semantic illusion tasks",
            "test_category": "Reasoning / reflective vs intuitive processing",
            "test_description": "CRT items probe tendency to override intuitive but incorrect responses with deliberative correct answers; semantic illusion tasks probe susceptibility to meaning-based traps.",
            "evaluation_metric": "Correct response rate on CRT items; qualitative classification of intuitive vs deliberative answers",
            "human_performance": "Humans show mixed CRT performance with clear distinction between intuitive errors and reflective corrections (no numeric baseline given in review)",
            "llm_performance": "Smaller/earlier models responded more intuitively; GPT-3.5 and GPT-4 exhibited more deliberative (System-2-like) responses and avoided some semantic traps",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Cognitive Reflection Test literature (Frederick 2005) and semantic illusion studies",
            "statistical_significance": null,
            "notes": "Shift toward deliberative answers may reflect model scale and instruction-tuning; chain-of-thought prompting not always required but commonly appears in responses.",
            "uuid": "e7249.6",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4 (Han et al., inductive reasoning)",
            "name_full": "GPT-4 evaluated on category-based induction tasks",
            "brief_description": "GPT-4 showed substantial improvement on inductive/category-based induction tasks, achieving performance comparable to humans, whereas GPT-3 performed poorly and qualitatively differently.",
            "citation_title": "Inductive reasoning in humans and large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Advanced transformer LLM by OpenAI with stronger inductive reasoning abilities compared to earlier generations.",
            "model_size": null,
            "test_name": "Category-based induction (curated dataset inspired by Osherson et al.)",
            "test_category": "Reasoning / inductive reasoning",
            "test_description": "Tasks require generalizing properties from observed categories to novel cases and assigning plausibility/probability to inductive generalizations.",
            "evaluation_metric": "Inductive generalization accuracy / plausibility judgments compared to human responses",
            "human_performance": "Humans provide the benchmark of plausible inductive generalizations; GPT-4 reported to reach comparable levels (no numeric values in review)",
            "llm_performance": "GPT-4 achieved performance comparable to humans; GPT-3 performed poorly",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Category-based induction literature (Osherson et al. 1990) and human comparisons in Han et al.",
            "statistical_significance": null,
            "notes": "Indicates strong improvement with model scale; details of exact prompting and numeric comparisons were not provided in this review.",
            "uuid": "e7249.7",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OPT (RAVEN analogical tasks)",
            "name_full": "OPT family (125M, 1.3B, 13B) and text-davinci-002 on RAVEN-style in-context analogical reasoning",
            "brief_description": "Smaller OPT variants and text-davinci-002 were used with language encodings of perceptual features to perform Raven's Progressive Matrices-style relational reasoning, often surpassing human performance on text-based analogical problems.",
            "citation_title": "In-context analogical reasoning with pre-trained language models.",
            "mention_or_use": "use",
            "model_name": "OPT-125M, OPT-1.3B, OPT-13B, text-davinci-002",
            "model_description": "OPT family of open pre-trained transformer models at multiple scales and an OpenAI instruction-capable davinci variant.",
            "model_size": "125M; 1.3B; 13B for OPT variants; text-davinci-002 size not specified",
            "test_name": "RAVEN (relational/analogical visual reasoning) encoded into language",
            "test_category": "Reasoning / analogical / fluid intelligence",
            "test_description": "Raven-style visual analogy problems encoded into textual descriptions of perceptual features, then solved by language models in-context.",
            "evaluation_metric": "Accuracy on RAVEN-style problems; comparison to human and supervised vision baselines",
            "human_performance": "Human baselines for RAVEN are standard; review reports models often surpass human performance on the language-encoded variants (no numeric human baseline provided)",
            "llm_performance": "These models often surpassed human performance and approached supervised vision-based method performance on the language-encoded tasks",
            "prompting_method": "Zero-shot / in-context encoding of perceptual features into text",
            "fine_tuned": false,
            "human_data_source": "RAVEN dataset (Zhang et al. 2019) and comparisons to human/fitted baselines in Hu et al.",
            "statistical_significance": null,
            "notes": "Performance may reflect advantages of large-scale pattern learning when perceptual relations are translated to language; may not generalize to raw visual input.",
            "uuid": "e7249.8",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3 / GPT-4 (Webb et al.)",
            "name_full": "GPT-3 and GPT-4 on zero-shot analogical reasoning tasks including Raven variants and visual analogies",
            "brief_description": "GPT-3 matched or surpassed human performance on several text-based analogy problems; GPT-4 showed stronger performance, suggesting scaling improves sensitivity to causal relations.",
            "citation_title": "Emergent analogical reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3, GPT-4",
            "model_description": "Generative transformer LLMs of successive generations, evaluated zero-shot on analogy tasks.",
            "model_size": null,
            "test_name": "Zero-shot analogy tasks (Raven variant; visual analogy problem set)",
            "test_category": "Reasoning / analogical reasoning",
            "test_description": "Analogy and pattern-completion tasks probing fluid intelligence via abstract relations among elements; presented in text or text-encoded vision proxies.",
            "evaluation_metric": "Accuracy on analogy problems; comparative performance to human participants",
            "human_performance": "Humans used as baseline on analogy datasets; GPT-3 matched or surpassed human performance on several text-based items (no numeric baselines in review)",
            "llm_performance": "GPT-3 matched/surpassed humans on several tasks; GPT-4 showed even stronger performance",
            "prompting_method": "Zero-shot (text prompts) / text-encoded visual descriptions",
            "fine_tuned": false,
            "human_data_source": "Analogy problem sets commonly used as fluid-intelligence proxies (e.g., Raven-like datasets) as reported in Webb et al.",
            "statistical_significance": null,
            "notes": "Claims concern text-encoded analogies; visual raw-input performance may differ; scaling and better task formulations improve outcomes.",
            "uuid": "e7249.9",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Various LLMs (Palminteri et al.)",
            "name_full": "Multiple LLMs (GPT-3, GPT-3.5, OPT, BLOOM, LLAMA, VICUNA, GPT-4) evaluated on CRT variants and Linda/Bill conjunction tasks",
            "brief_description": "Study of reasoning biases: earlier models relied on heuristic/intuitive responses and underperformed versus humans, while GPT-4 exhibited reduced heuristic reasoning and in some cases super-human performance.",
            "citation_title": "Studying and improving reasoning in humans and machines (2023)",
            "mention_or_use": "use",
            "model_name": "GPT-3, GPT-3.5, OPT, BLOOM, LLAMA, VICUNA, GPT-4",
            "model_description": "Range of transformer LLMs including open-source and proprietary models evaluated on reasoning-bias tasks.",
            "model_size": null,
            "test_name": "Variants of the Cognitive Reflection Test and Linda/Bill conjunction fallacy problems",
            "test_category": "Reasoning / heuristics and biases",
            "test_description": "Variants probe susceptibility to intuitive heuristic responses (CRT) and conjunction fallacy (Linda/Bill) to compare heuristic vs deliberative tendencies.",
            "evaluation_metric": "Rate of heuristic (intuitive) vs deliberative (normative) responses; accuracy on normative answers",
            "human_performance": "Humans show mixed performance with characteristic rates of conjunction errors and CRT failures; specific numeric baselines not provided in review",
            "llm_performance": "Earlier models predominantly heuristic and underperformed vs humans; GPT-4 showed strong (sometimes super-human) performance with minimal reliance on intuitions",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "CRT and conjunction-fallacy literature and human benchmarks used in Palminteri et al.",
            "statistical_significance": null,
            "notes": "Indicates substantial variation across model generations; GPT-4 often avoids common heuristic traps that earlier models fall into.",
            "uuid": "e7249.10",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Chinchilla / PaLM 2 / Flan-PaLM 2 / GPT-3.5 (Dasgupta et al.)",
            "name_full": "Chinchilla, PaLM 2-M/L, Flan-PaLM 2, GPT-3.5 evaluated on mixed logical tasks including Wason selection",
            "brief_description": "LLMs demonstrate human-like content effects: performance varies by scenario/context and they struggle more on abstract or conceptually counterintuitive problems, paralleling human inconsistencies.",
            "citation_title": "Language models show human-like content effects on reasoning.",
            "mention_or_use": "use",
            "model_name": "Chinchilla, PaLM 2-M, PaLM 2-L, Flan-PaLM 2, GPT-3.5",
            "model_description": "Series of transformer LLMs from different families evaluated on logical and abductive reasoning tasks.",
            "model_size": null,
            "test_name": "Combined reasoning tasks (Wason selection + syllogisms + NLI variants)",
            "test_category": "Reasoning / abductive & deductive reasoning",
            "test_description": "A set of logical tasks probing how content/context influences reasoning performance.",
            "evaluation_metric": "Accuracy and consistency across content conditions; presence of content effects",
            "human_performance": "Humans show content effects and inconsistent performance across scenarios; used as baseline comparison (no numeric values)",
            "llm_performance": "Models mirrored human-like content effects and struggled particularly on abstract or counter-intuitive cases",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Human reasoning literature and empirical baselines used in Dasgupta et al.",
            "statistical_significance": null,
            "notes": "Findings indicate LLMs replicate not only errors but also systematic context sensitivity akin to humans.",
            "uuid": "e7249.11",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3 (Stevenson et al., AUT)",
            "name_full": "GPT-3 evaluated on the Alternative Uses Test (AUT)",
            "brief_description": "GPT-3 produced uses for common objects that had higher utility but lower originality, surprise, and semantic distance compared to human participants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Large autoregressive transformer model evaluated on divergent thinking task AUT.",
            "model_size": "175B",
            "test_name": "Alternative Uses Test (AUT)",
            "test_category": "Creativity / divergent thinking",
            "test_description": "Participants generate many alternative uses for common objects; responses are rated on originality, surprise, semantic distance, and utility.",
            "evaluation_metric": "Ratings on originality, surprise, semantic distance, and utility",
            "human_performance": "Human participants scored higher on originality, surprise, and semantic distance; numeric values not provided in review",
            "llm_performance": "GPT-3 scored lower on originality, surprise, and semantic distance but higher on utility ratings",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Human AUT participant data as used in Stevenson et al.",
            "statistical_significance": null,
            "notes": "Negative association between originality and utility observed in both human and model responses; GPT-3 leaned toward pragmatic/useful ideas over novel ones.",
            "uuid": "e7249.12",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4 (Hubert et al., AUT/TTCT)",
            "name_full": "GPT-4 evaluated on AUT, Consequences Task and Divergent Associations / TTCT variants",
            "brief_description": "GPT-4 generated more diverse responses with greater elaboration and higher originality (under certain prompts) than human counterparts on several creativity measures.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Advanced transformer LLM exhibiting strong language-based creative generation.",
            "model_size": null,
            "test_name": "Alternative Uses Test (AUT), Consequences Task (TTCT), Divergent Associations Task",
            "test_category": "Creativity / divergent thinking and creative writing",
            "test_description": "AUT: generate alternative uses; Consequences Task: generate consequences of hypothetical events; Divergent Associations: produce semantically distant associations.",
            "evaluation_metric": "Originality, elaboration, fluency-controlled metrics",
            "human_performance": "Human participants served as baseline; GPT-4 produced greater originality and elaboration in these tasks (no numeric baselines provided)",
            "llm_performance": "GPT-4 produced more diverse responses and higher originality/elaboration relative to humans under specified prompt regimes",
            "prompting_method": "Specific prompting (review notes GPT-4 higher originality with particular prompts)",
            "fine_tuned": null,
            "human_data_source": "Human participant data in Hubert et al.'s experimental comparisons",
            "statistical_significance": null,
            "notes": "Suggests LLMs excel in language-centric creativity measures but may depend on prompt design.",
            "uuid": "e7249.13",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3.5 / text-davinci-003 (Yiu et al., tool task)",
            "name_full": "GPT-3.5 and text-davinci-003 evaluated on a novel tool selection task",
            "brief_description": "Models nearly matched humans in recognizing superficial similarities between objects but lagged significantly behind adults and children when asked to choose unfamiliar tools for novel problems, defaulting to conventional solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, text-davinci-003",
            "model_description": "Instruction-tuned and generative transformer models evaluated on novel tool-use/problem-solving tasks.",
            "model_size": null,
            "test_name": "Novel tool selection task",
            "test_category": "Creativity / novel problem-solving / functional fixedness",
            "test_description": "Task requires selecting or proposing nonstandard object uses/tools to accomplish a goal without canonical tools; measures novelty and functional flexibility.",
            "evaluation_metric": "Accuracy/choice of novel tool; comparison to adult and child performance",
            "human_performance": "Adults and children outperformed models on selecting unfamiliar/novel tools (no numeric baselines in review)",
            "llm_performance": "Models significantly lagged behind both adults and children, often defaulting to conventional tool choices",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Experimental human comparisons conducted by Yiu et al.",
            "statistical_significance": null,
            "notes": "Highlights limitations in physical/embodied reasoning and true divergent creative problem solving.",
            "uuid": "e7249.14",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "TTCW creative writing (Chakrabarty et al.)",
            "name_full": "GPT-3.5, GPT-4, Claude-v1.3 evaluated on the Torrance Test of Creative Writing (TTCW)",
            "brief_description": "LLMs' generated stories were significantly less likely to pass individual TTCW tests than stories written by human experts, indicating deficits in certain creative-writing criteria.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4, Claude-v1.3",
            "model_description": "Instruction-tuned and research LLMs tested on formal creative-writing assessment derived from Torrance tests.",
            "model_size": null,
            "test_name": "Torrance Test of Creative Writing (TTCW)",
            "test_category": "Creativity / creative writing",
            "test_description": "Verbal creative-writing assessment measuring factors like fluency, originality, elaboration, and narrative quality (adapted Torrance test for writing).",
            "evaluation_metric": "Pass rates on individual TTCW tests; comparative story quality ratings",
            "human_performance": "Human expert writers passed individual TTCW tests at a higher rate than the models (no numeric baselines provided)",
            "llm_performance": "Stories from all tested models were significantly less likely to pass TTCW tests compared to human experts",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Human expert story baselines used in Chakrabarty et al.'s TTCW evaluation",
            "statistical_significance": "Reported as significant (no p-values in review)",
            "notes": "Indicates nuance: LLMs can excel in some open-ended story tasks but fail on structured creative-writing criteria developed for humans.",
            "uuid": "e7249.15",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Five-sentence story task (Orwig et al.)",
            "name_full": "GPT-3 and GPT-4 evaluated on five-sentence creative story production",
            "brief_description": "On a constrained five-sentence creative story task with a three-word prompt, GPT-3 and GPT-4 produced stories comparable in creativity to human participants; GPT-4 showed consistency with human evaluators.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3, GPT-4",
            "model_description": "Generative transformer LLMs tested on a short-form creative-writing task.",
            "model_size": null,
            "test_name": "Five-sentence creative story task (three-word prompt)",
            "test_category": "Creativity / creative writing",
            "test_description": "Participants produce a short (~5 sentence) story including all three prompt words; stories are rated for creativity by human evaluators.",
            "evaluation_metric": "Creativity ratings by human judges; comparative agreement with human evaluations",
            "human_performance": "Human-authored stories serve as baseline; models achieved comparable creativity ratings (no numeric values provided)",
            "llm_performance": "Both GPT-3 and GPT-4 produced stories rated comparable to human stories; GPT-4 was particularly consistent with human evaluators",
            "prompting_method": "Constrained prompt (three-word prompt) indicating few-shot/structured generation",
            "fine_tuned": null,
            "human_data_source": "Human judge ratings used in Orwig et al.'s study",
            "statistical_significance": null,
            "notes": "Suggests LLMs can match humans on certain constrained, language-centric creative tasks, though other creative-writing metrics may differ.",
            "uuid": "e7249.16",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ChatGPT as creative assistant (Lee & Chung)",
            "name_full": "GPT-3.5 (ChatGPT) evaluated as an assistive tool for human creativity across five tasks",
            "brief_description": "When used as an assistant, ChatGPT significantly improved human idea-generation creativity compared to web-search or no-technology conditions, particularly for incremental idea generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (ChatGPT)",
            "model_description": "Interactive instruction-tuned conversational LLM used to support human creative tasks.",
            "model_size": null,
            "test_name": "Five curated creative-assistance tasks (gift choice, toy making, repurposing items, design tasks)",
            "test_category": "Creativity / assistive creativity / human-AI collaboration",
            "test_description": "Human participants generate ideas alone, with web search, or with ChatGPT assistance; outputs are rated for creativity and novelty.",
            "evaluation_metric": "Creativity ratings of final ideas; comparative improvement vs other conditions",
            "human_performance": "Human-only and human+web-search baselines; ChatGPT-assisted humans produced significantly more creative outputs",
            "llm_performance": "As assistant, ChatGPT improved human creativity markedly (particularly for incremental novelty); not a standalone metric but collaborative improvement",
            "prompting_method": "Interactive conversational use (human-guided prompts / iterative prompting)",
            "fine_tuned": null,
            "human_data_source": "Human participant comparisons in Lee & Chung's experimental design",
            "statistical_significance": "Reported significant improvements (no p-values provided in review)",
            "notes": "Highlights LLMs' utility as augmentative tools in creative workflows even when they may not be maximally novel on their own.",
            "uuid": "e7249.17",
            "source_info": {
                "paper_title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand gpt-3.",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Inductive reasoning in humans and large language models.",
            "rating": 2,
            "sanitized_title": "inductive_reasoning_in_humans_and_large_language_models"
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt.",
            "rating": 1,
            "sanitized_title": "humanlike_intuitive_behavior_and_reasoning_biases_emerged_in_large_language_models_but_disappeared_in_chatgpt"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning.",
            "rating": 1,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        }
    ],
    "cost": 0.025723,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models</h1>
<p>Zhisheng Tang ${ }^{1}$ and Mayank Kejriwal ${ }^{1 <em>}$<br>${ }^{1 </em>}$ Information Sciences Institute, USC Viterbi School of Engineering, 4676 Admiralty Way 1001, Marina Del Rey, 90292, California, USA.</p>
<p>*Corresponding author(s). E-mail(s): kejriwal@isi.edu;<br>Contributing authors: zhisheng@isi.edu;</p>
<h4>Abstract</h4>
<p>Research on emergent patterns in Large Language Models (LLMs) has gained significant traction in both psychology and artificial intelligence, motivating the need for a comprehensive review that offers a synthesis of this complex landscape. In this article, we systematically review LLMs' capabilities across three important cognitive domains (decision-making biases, reasoning, and creativity), using empirical studies drawing on established psychological tests and comparing LLMs' performance to human benchmarks. On decision-making, our synthesis reveals that while LLMs demonstrate several human-like biases, some biases observed in humans are absent, indicating cognitive patterns that only partially align with human decision-making. On reasoning, advanced LLMs like GPT-4 exhibit deliberative reasoning akin to human System-2 thinking, while smaller models fall short of human-level performance. A distinct dichotomy emerges in creativity: while LLMs excel in language-based creative tasks, such as storytelling, they struggle with divergent thinking tasks that require real-world context. Nonetheless, studies suggest that LLMs hold considerable potential as collaborators, augmenting creativity in human-machine problem-solving settings. Discussing key limitations, we also offer guidance for future research in areas such as memory, attention, and open-source model development.</p>
<p>Keywords: Large language models, cognitive patterns, emergence, decision-making, reasoning, heuristics, biases, creativity</p>
<h1>1 Introduction</h1>
<p>Recent advancements in generative artificial intelligence (AI) and large language models (LLMs) [1, 2] have sparked new interest in how AI might simulate or even influence human cognition and decision-making. Although the original purpose of language models was text generation and addressing longstanding problems in natural language processing (NLP) like machine translation and information extraction [3-7], a recent suite of papers have argued that they are now exhibiting, or at the very least, mimicking, complex reasoning abilities at human levels of performance [8-10]. Beyond reasoning and decision-making, LLMs released since the first edition of ChatGPT in early 2023 have also been argued to mimic abilities like creativity (evidenced through tasks like poetry and lyrical generation [11]), although the originality of their creative outputs is a matter of debate.</p>
<p>These debates notwithstanding, the emergence of novel behaviors and properties in LLMs as an empirical phenomenon cannot be ignored [12]. Researchers are divided on the causes and implications of such behaviors, with some arguing that they are largely a mirage [13] and others arguing that true emergence is occurring [12]. By emergence here, we simply mean that the model was not explicitly trained to mimic or learn such behaviors, either through the underlying neural network's objective function or through the data itself. Wei et al. [12] succinctly describe emergence in the context of LLMs as an ability that is not present in smaller models but is present in larger models.</p>
<p>While emergence is much more complex in LLMs, it has some precedent in NLP research since 2010 that has increasingly relied upon deep neural networks. For example, when the word2vec model first became popular more than a decade ago, the authors of the original paper noted how the word vector representations yielded by the skip-gram neural network model (when it was fed reasonably large corpora in an unsupervised fashion) obeyed the now-classic analogy kng $-\operatorname{ma} n+w \operatorname{mo} \bar{n} a n=q u \bar{e} e n$ [14]. Similarly, shortly following the release of Bidirectional Encoder Representations from Transformers (BERT) [15], one of the first transformer-based language models, a line of work colloquially referred to as BERTology [16] rapidly emerged in less than five years, showcasing empirical phenomenon of a largely emergent nature. For example, BERT's neural layers were found to exhibit hierarchical representations of language (despite not being explicitly trained to do so), with earlier layers containing information about linear word order, middle layers carrying syntactic information, final layers holding task-specific knowledge, and semantics spreading across all layers.</p>
<p>Given these expanded and unexpected capabilities, an intriguing question arises: do LLMs, which are trained on vast amounts of human-generated text, also exhibit cognitive patterns that are typically shown in humans? Specifically, do they exhibit the cognitive heuristics and biases that characterize human decision-making? Do they share the same kinds of reasoning patterns and levels of reasoning capabilities as humans? Can they innovate in ways that resemble human creativity? To answer these questions, it is helpful to first understand what these cognitive patterns are, and the relationship between them.</p>
<p>Cognitive scientists and psychologists have long studied human decision-making biases, such as hindsight bias, overweighting, and belief bias. Although often reducing</p>
<p>cognitive effort, they can also lead to suboptimal decisions [17]. However, these biases are just one element in the interconnected cognitive processes that underlie human cognition. Decision-making itself is the process of selecting from various alternatives by weighing their respective benefits and drawbacks [18]. In this context, reasoning is considered the ability to logically evaluate the pros and cons of different alternatives [19]. Simultaneously, creativity can be defined as the ability to generate novel alternatives and enable decision-making by introducing innovative solutions [20]. Together, reasoning helps evaluate alternatives, while creativity supports generating new alternatives, making them both essential for effective decision-making. Investigating LLMs' emergent behavior in these areas can offer a comprehensive view of their potential to replicate human-like cognitive processes.</p>
<p>As shown in Figure 1, there is a line of studies that use established psychological and cognitive tools to examine how LLMs replicate or diverge from human patterns in the three cognitive processes of decision-making, reasoning, and creativity. In LLMs, these processes are better denoted as patterns, because the actual mechanisms through which LLMs are able to replicate such abilities are obviously different from the way in which humans do so, at least neurally. We include studies that investigate the extent to which LLMs demonstrate human-like decision-making biases, their capability to reason under varied conditions, and their ability to innovate. We do not include studies that are based completely on computational tests that have not undergone rigorous human benchmarking of some sort. Rather, we focus solely on studies that use established psychological experiments or their variants, and for which human baseline performance is available. This allows us to make clear comparisons between LLMs and humans and ensures that any critiques or insights offered are grounded in robust experimental results with a human performance reference.</p>
<p>Overall, this review attempts a comprehensive examination of LLMs' performance across the three cognitive processes that seem to underlie emergence that have relevance to psychology research. For each process, we begin by briefly introducing the process and its fundamental principles. We then review individual studies that have attempted to empirically study or measure that process in at least one LLM, discussing their methodologies and findings. Finally, we conclude each section by synthesizing the insights, comparing LLMs' behaviors with human performance, and reflecting on the implications of the findings. By systematically comparing LLMs' behaviors to established human benchmarks, we aim to synthesize the growing body of literature on how these models simulate human cognitive patterns. We close by providing guidance for future research that is needed in this area to draw firmer conclusions.</p>
<h1>2 Decision-Making Cognitive Patterns</h1>
<h3>2.1 Heuristics and biases</h3>
<p>While there is much literature studying different aspects of human decision-making, such as: cognitive heuristics and biases [28], dual-process theory [29], social influence and group decision-making [30], and neuroeconomics and computational models of decision-making [31], current research on LLMs' decision-making behavior has mainly focused on cognitive heuristics and biases. To understand the relevance of these studies,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 Timeline of various studies reviewed, along with release time of major Large Language Models. Red, Blue, Green, and Purple entries indicate LLM releases, decision-making related studies, reasoning related studies, and creativity related studies, respectively.
it is helpful to first consider the foundational role of heuristics and biases in human decision-making.</p>
<p>Mental shortcuts, or heuristics, reduce the cognitive load of intensive processes [17]. Notable heuristics include the availability heuristic, representativeness heuristic, and the anchoring heuristic. The availability heuristic is our inclination to rely on information that comes to our mind quickly and easily when evaluating decisions [32]. For example, people might judge the probability of a rare event, such as a plane crash, to be higher than it actually is if they have recently seen such an event on the news. Although the availability heuristic helps make decisions quicker, it also leads to an overestimation of unlikely but memorable events.</p>
<p>The representativeness heuristic involves assessing probabilities based on the similarity between a sample and a larger population [33]. One famous example is when people stereotypically deemed a young woman, described as deeply concerned with social justice, intelligent, and outspoken, to be more likely to be a bank teller and also active in the feminist movement, than to just be a bank teller alone (which would be inconsistent with rational rules of probability). The heuristic simplifies decisionmaking by relying on stereotypes but often misleads people to ignore general statistics. The anchoring heuristic is yet another commonly known mental shortcut. It occurs when the initial exposure to a reference point influences subsequent decisions [34]. It is explored extensively in the science of framing sales as discounts. For example, if the original price of a commodity serves as an anchor, the discounted price suddenly becomes attractive, even though it may (still) be unreasonably high.</p>
<p>Table 1 Decision-making related studies, with specific type, dataset/tasks used, LLMs involved in the study, associated findings, and corresponding citation. $\checkmark$ denotes biases/heuristics found, while $\times$ denotes absence.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">Findings</th>
<th style="text-align: center;">Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">12 vignette-based experiments and a set of multiple choice questions inspired by [21]</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Framing effect $\checkmark$, certainty effect $\checkmark$, overweighting bias $\checkmark$, reflection effect $\times$, isolation effect $\times$, magnitude perception $\times$</td>
<td style="text-align: center;">Binz and Schulz [22]</td>
</tr>
<tr>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">Curated dataset</td>
<td style="text-align: center;">GPT-3, <br> GPT- <br> 3.5, <br> GPT-4 <br> etc.</td>
<td style="text-align: center;">Decoy effect $\checkmark$, certainty effect $\checkmark$, belief bias $\checkmark$</td>
<td style="text-align: center;">Itzhak et al. [23]</td>
</tr>
<tr>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">Curated decisionmaking problems and derivatives that are rewritten in operation management context</td>
<td style="text-align: center;">GPT- <br> 3.5, <br> GPT-4</td>
<td style="text-align: center;">Risk aversion $\checkmark$, preference for certainty in subjective tasks $\checkmark$, heuristic reasoning in objective tasks $\checkmark$</td>
<td style="text-align: center;">Chen et al. [24]</td>
</tr>
<tr>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">Newsvendor problem</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Demand-chasing $\checkmark$, risk-aversion $\checkmark$, loss-aversion $\checkmark$, waste aversion $\times$, stockout aversion $\times$, underestimated opportunity costs $\times$, minimized expost inventory errors $\times$</td>
<td style="text-align: center;">Su et al. [25]</td>
</tr>
<tr>
<td style="text-align: center;">Rationality</td>
<td style="text-align: center;">Budgetary experiments</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">The model outperformed humans but showed performance drops with nonstandard price presentation</td>
<td style="text-align: center;">Chen et al. [26]</td>
</tr>
<tr>
<td style="text-align: center;">Heuristics and bias</td>
<td style="text-align: center;">Curated dataset</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Anchoring Heuristic $\checkmark$, representativeness and availability heuristic $\checkmark$, framing effect $\checkmark$, the endowment effect $\checkmark$</td>
<td style="text-align: center;">Suri et al. [27]</td>
</tr>
</tbody>
</table>
<p>While these heuristics help individuals make decisions more effortlessly, they may also increase the odds of error, especially in situations of a slightly irregular nature [35]. In part, they cause a tendency to make simpler decisions and also exhibit biases. For instance, the hindsight bias can mislead people to see past events as more predictable after they have occurred. Similarly, overweighting bias causes people to assign disproportionate importance to certain pieces of information, often ones that are more recent or vivid, regardless of their actual relevance. Additionally, belief bias reflects a tendency to accept or reject conclusions based on alignment with pre-existing beliefs rather than logical validity.</p>
<h1>2.2 Evaluating cognitive heuristics and biases in LLMs</h1>
<p>Table 1 presents a comprehensive overview of heuristics and biases exhibited by different LLMs according to a range of studies. As an inspirational work, Binz and Schulz [22] evaluated GPT-3's decision-making capabilities using some classical cognitive psychology tasks, including vignette-based experiments (the Linda problem [36], the cab</p>
<p>problem [37]), a recent task-based decision-making benchmark [38], and a set of multiple choice questions design to elicit biases in human decision-making by Kahneman and Tversky [21]. Through the vignette-based experiments, the authors discovered that, unlike people, GPT-3 did not fall for the common fallacy but instead provided approximately correct answers. However, GPT-3's performance declined drastically with minor changes to these vignettes, suggesting possible overfitting from pre-training. For the task-based investigation, even though GPT-3 was able to solve these problems above the chance level, it did not reach human-level performance. The work also found that GPT-3 showed three of six tested biases, including the framing effect, certainty effect, and overweighting bias.</p>
<p>Considering the mainstream approach for building consumer-facing LLMs, Itzhak et al. [23] explored the impact of instruction-tuning and reinforcement learning from human feedback in LLMs on three cognitive biases: the decoy effect [39], the certainty effect [18], and the belief bias [40]. They found that these LLMs exhibit biases that align with human biases theory [28] and that the fine-tuned LLMs (i.e., GPT3, Mistral-7B [41], and T5 [42]) exhibited more bias compared to their pre-trained counterparts (i.e., GPT-3.5, Mistral-7B-Instruct, Flan-T5, and GPT-4).</p>
<p>Using a different decision-making context (i.e., operation management (OM)), Chen et al. [24] examined 18 common decision-making biases of GPT-3.5 and GPT-4 compared to human managers using classic problems from the literature. They found that GPT-4 displays decision-making patterns that vary by task. In subjective scenarios with uncertain outcomes, GPT shows risk aversion and a preference for certainty. Conversely, when faced with objective tasks, GPT-4 relies on heuristic reasoning. Compared to GPT-3.5, GPT-4 shows both improved accuracy, and increased decision biases, in several contexts. Both GPT models demonstrate a surprising consistency in decision-making across various contexts, including both standard tests, which are commonly reported in the literature and thus already present in the LLMs' training data, and new OM-specific tests that were developed specifically for this study, which were novel and previously unseen by the model.</p>
<p>Similarly, Su et al. [25] investigated whether GPT-4 can solve the newsvendor problem [43] without making biased decisions. They found that it exhibits similar biases to human decision-makers, showing a significantly higher demand-chasing tendency, risk-aversion in high-profit margin scenarios, and loss-aversion in low-profit margin scenarios. However, the model also demonstrated a degree of rationality and responsiveness to incentives, even if they diverged from human predictions based on rational game theory. Notably, GPT-4 did not exhibit certain behavioral biases, such as waste aversion, stockout aversion, underestimated opportunity costs, and minimized ex-post inventory errors. Focusing on economic rationality, Chen et al. [26] explored economic rationality [44] in GPT-3.5 using a set of curated typical budgetary experiments. They found that the model outperformed human subjects in terms of rationality in four decision-making tasks concerning risk, time, social, and food. However, the performance of the model drops significantly when a less standard presentation of prices is used. Using case studies, Suri et al. [27] investigated whether GPT-3.5 exhibits decision heuristics similar to humans. Their case study demonstrated that GPT-3.5 shows anchoring Heuristic, representativeness and availability heuristic, framing effect, and the endowment effect.</p>
<h1>2.3 Conclusion</h1>
<p>In summary, modern LLMs demonstrate at least thirteen different human-like heuristics and biases: frame effect, certainty effect, overweighting bias, decoy effect, belief effect, risk aversion, preference for certainty in subjective tasks, heuristic reasoning in objective tasks, demand-chasing, loss-aversion, anchoring effect, representativeness and availability effect, and endowment effect. However, they were also found not to exhibit some other common (in humans) biases, such as reflection effect, isolation effect, magnitude perception, waste aversion, stockout aversion, underestimated opportunity cost, and minimized ex-post inventory errors. Instruction-tuned LLMs show improved performance but also heightened biases, suggesting that the process of instruction fine-tuning may induce increased bias. In more complex decision-making scenarios like the newsvendor problem or operation management, GPT-4 exhibits a blend of human-like biases while also demonstrating rational responses aligned with economic incentives. Interestingly, while LLMs outperform humans in standard decision tasks related to rationality, their performance decreases under less conventional conditions, suggesting a dependency on familiar data presentations.</p>
<p>Beyond heuristics and biases, sequential decision-making has also become a recent popular subject of LLM research. LLM architectures have inherent advantages in handling sequential information. Unfortunately, most studies in this area use artificial datasets and tasks, and lack human performance as a direct reference for comparison [45]. Additionally, experiments in many of these papers are not hypothesis-driven and cannot be compared with the outcomes of traditional, more rigorously designed psychological experiments. For further details, we refer the interested reader to Yang et al. [45].</p>
<h2>3 Reasoning Cognitive Patterns</h2>
<h3>3.1 Reasoning in humans</h3>
<p>Reasoning may be broadly defined as the process of drawing conclusions based on a combination of axiomatic principles and evidence [55, 56]. Reasoning allows us to move from what is already known to making new inferences and incorporating them into our knowledge base, as well as to evaluate proposed hypotheses. In a decision-making framework, we interpret reasoning as the ability to make judgments and choices by integrating salient types of information, weighing evidence, considering alternatives, and predicting potential outcomes. Reasoning may be divided into three types: deductive, inductive, and abductive. Each type is useful in different kinds of contexts and hence operates according to different principles.</p>
<p>Deductive reasoning is the process of using logic to draw conclusions from given observations [57]. For example, consider the classic example: (1) All humans are mortal; (2) Socrates is a human; therefore, (3) Socrates is mortal. The conclusion (3) is valid because it is logically derived from the two premises given. As a result, deductive reasoning is the predominant type of reasoning found in areas like mathematical theorem proving, which require logical rigor.</p>
<p>Table 2 Reasoning related studies, with specific type, dataset/tasks used, LLMs involved, associated findings, and corresponding citation. The first three rows, the next four rows, and the last row contain studies related to deductive, inductive, and abductive reasoning, respectively. $\checkmark$ denotes biases found, while $\times$ denotes absence of bias. $\uparrow$ indicates better than or equivalent to human performance, while $\downarrow$ indicates worse than human performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">Findings</th>
<th style="text-align: center;">Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Deductive</td>
<td style="text-align: center;">Wason selection task</td>
<td style="text-align: center;">Guanaco, MPT, BLOOM, Falcon</td>
<td style="text-align: center;">All models $\downarrow$</td>
<td style="text-align: center;">Seals and <br> Shalin [46]</td>
</tr>
<tr>
<td style="text-align: center;">Syllogistic reasoning</td>
<td style="text-align: center;">Curated dataset derived from BAROCO</td>
<td style="text-align: center;">RoBERTa, BART, GPT-3.5</td>
<td style="text-align: center;">All models $\downarrow$</td>
<td style="text-align: center;">Ando et al. [47]</td>
</tr>
<tr>
<td style="text-align: center;">Intuitive v.s. rigorous</td>
<td style="text-align: center;">Cognitive Reflection Test and a semantic illusions task</td>
<td style="text-align: center;">GPT-2, GPT-3, GPT-3.5, GPT-4</td>
<td style="text-align: center;">Pre-ChatGPT: intuitive $\checkmark$, ChatGPT: rigorous $\checkmark$</td>
<td style="text-align: center;">Hagendorff et al. [48]</td>
</tr>
<tr>
<td style="text-align: center;">Inductive</td>
<td style="text-align: center;">Curated category- <br> based induction <br> dataset inspired by [49]</td>
<td style="text-align: center;">GPT-3, GPT-4</td>
<td style="text-align: center;">GPT-4 $\uparrow$, GPT- <br> $3 \downarrow$</td>
<td style="text-align: center;">Han et al. [50]</td>
</tr>
<tr>
<td style="text-align: center;">In-context analogical</td>
<td style="text-align: center;">Curated RAVEN <br> dataset based on <br> Raven's Progressive <br> Matrices</td>
<td style="text-align: center;">OPT 125M, 1.3B, and 13B, text-davinci002</td>
<td style="text-align: center;">All models $\uparrow$</td>
<td style="text-align: center;">Hu et al. [51]</td>
</tr>
<tr>
<td style="text-align: center;">Analogical</td>
<td style="text-align: center;">Raven's Progressive Matrices and a visual analogy problem set</td>
<td style="text-align: center;">GPT-3, GPT-4</td>
<td style="text-align: center;">All models $\uparrow$</td>
<td style="text-align: center;">Webb et al. [52]</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning bias</td>
<td style="text-align: center;">Variants of the Cognitive Reflection Test and the Linda/Bill problem</td>
<td style="text-align: center;">GPT-3, GPT- <br> 3.5, OPT, BLOOM, <br> LLAMA, <br> VICUNA, <br> GPT-4</td>
<td style="text-align: center;">All other models: heuristic reasoning $\checkmark$, GPT-4: heuristic reasoning $\times$</td>
<td style="text-align: center;">Palminteri et al. [53]</td>
</tr>
<tr>
<td style="text-align: center;">Bias in abstract and logical reasoning</td>
<td style="text-align: center;">Curated natural language inference, syllogisms task and Wason Selection Task</td>
<td style="text-align: center;">Chinchilla, <br> PaLM 2-M, <br> PaLM 2-L, <br> Flan-PaLM 2, and GPT-3.5</td>
<td style="text-align: center;">Content effects $\checkmark$</td>
<td style="text-align: center;">Dasgupta et al. [54]</td>
</tr>
</tbody>
</table>
<p>Inductive reasoning is the process of reasoning from specific facts or observations to reach a likely conclusion (or 'theory') that satisfactorily explains the facts and uses it in an attempt to predict future instances [58]. Unlike deduction, which gives certainty when provided with true premises, inductive reasoning cannot definitively 'prove' its conclusion from premises or evidence; any conclusions must instead be interpreted probabilistically (e.g., as dictated by statistical significance analysis). It is more prevalent in empirical science, where researchers gather data through observation or experimentation to formulate theories that explain the observed phenomena. A well-known example is: after observing many white swans in different locations, one might conclude that all swans are white. The conclusion is inferred from the repeated</p>
<p>observation of a phenomenon (although a wrong one). This reflects the fundamental principle of inductive reasoning, which can only allow us to draw a conclusion within limited bounds of certainty, and always carries the caveat that further evidence could reveal exceptions.</p>
<p>Abductive reasoning aims to recover the 'best', usually interpreted as the most plausible, explanation given a set of observations [59]. For example, if a doctor observes a patient with symptoms such as fever, cough, and fatigue, they can use abductive reasoning to conclude that the patient may have the flu, as this would seem to be the the most plausible explanation for the observed symptoms. While lacking the same degree of formalism and traditional philosophical inquiry as inductive and deductive reasoning, it is widely applicable in everyday settings, including humans' ability to infer plausible causes by generalizing from sparse data.</p>
<p>Although each of these types of reasoning can be further subdivided, and the three types do not constitute a strict categorization of reasoning in humans by any means (e.g., other types of reasoning, like deontic reasoning and common sense reasoning, have also been widely studied [60, 61]), most studies on LLMs tend to involve one or the other.</p>
<h1>3.2 Evaluating reasoning in LLMs</h1>
<p>Table 2 summarizes the performance of LLMs, compared with humans, on reasoning tasks. On deductive reasoning, Seals and Shalin [46] evaluated the competence of various LLMs, including Guanaco [62], MPT [63], BLOOM [64], and Falcon [65], using the Wason selection task [66]. They found that when the task is presented in its conventional form, the LLMs show limited performance. When they changed the presentation format, the LLMs did not show significant improvement and exhibited unique reasoning biases that differed from humans. Ando et al. [47] evaluated three LLMs (RoBERTa [67], BART [68], and GPT-3.5) with a focus on human-like biases (i.e., belief biases, conversion errors, and atmosphere effects) in syllogistic reasoning [69] using a dataset derived from BAROCO [70]. They found that these models struggled with such problems, and errors caused by various human-like biases heavily influenced their performance.</p>
<p>In contrast, Hagendorff et al. [48] explored the differences between intuitive (System 1) and deliberative (System 2) reasoning [71] in LLMs using the Cognitive Reflection Test [72] and a semantic illusions task [73]. They observed that as LLMs get larger and their task comprehension improves, they respond more intuitively, resembling System 1 processing. However, with GPT-3.5 and GPT-4, there was a significant shift towards more deliberative, System 2 like processing. This shift enabled the models to better avoid semantic traps and perform well in cognitive tasks, even without relying explicitly on LLM-specific modalities like chain-of-thought reasoning [74], although such reasoning often appeared in their responses.</p>
<p>On inductive reasoning, Han et al. [50] examined the competence of GPT-3 and GPT-4 using a curated category-based induction task inspired by [49]. Their research showed that while GPT-3 faces significant challenges in this area, performing poorly overall and reasoning in a qualitatively different way from humans, GPT-4 demonstrates considerable improvement, achieving performance comparable to that</p>
<p>of humans. Additionally, Hu et al. [51] examined in-context analogical reasoning ability [75] in LLMs (i.e., OPT 125M, 1.3B, and 13B [76], text-davinci-002 [77]) using the RAVEN dataset [78], based on Raven's Progressive Matrices [79]. They found that, by encoding perceptual features of problems into language, these LLMs exhibited remarkable zero-shot relational reasoning abilities, often surpassing human performance and approaching the levels of supervised vision-based methods. Furthermore, Webb et al. [52] investigated the analogical reasoning capabilities of GPT-3 and GPT-4 using two zero-shot analogy tasks, including a variant of the Raven's Progressive Matrices and a visual analogy problem set commonly viewed as one of the best measures of fluid intelligence [80]. They found that GPT-3 could match or even surpass human performance in several text-based analogy problems. An initial investigation of GPT-4 revealed its stronger performance on these tasks, suggesting that further scaling of LLMs will likely improve their sensitivity to causal relationships.</p>
<p>Palminteri et al. [53] studied reasoning biases [81] in LLMs through variants of the Cognitive Reflection Test [82] and the Linda/Bill problem [28], which is known to elicit the conjunction fallacy [83]. They observed that earlier LLMs, such as GPT3 and GPT-3.5, along with open-source models like OPT, BLOOM, LLAMA [84], and VICUNA [85], demonstrated bounded or heuristic reasoning, relying heavily on intuitive responses and typically under-performing relative to humans. In contrast, more advanced models like GPT-4 exhibited super-human performance on certain tasks, relying minimally on intuitive reasoning.</p>
<p>Finally, on abductive reasoning, Dasgupta et al. [54] found that LLMs, including Chinchilla [86], PaLM 2-M, PaLM 2-L [87], Flan-PaLM 2 [88], and GPT-3.5, demonstrate human-like content effects on three logical reasoning tasks that combine the Wason selection task with two other tasks. Specifically, these LLMs, like humans, perform inconsistently across various scenarios and are heavily influenced by the context and content of the stimuli. They face greater challenges with abstract situations or those that contradict their prior understanding of the world, as reflected in their training data.</p>
<h1>3.3 Conclusion</h1>
<p>LLMs exhibit different performance patterns depending on the type of reasoning involved. On deductive reasoning, LLMs struggle with tasks such as the Wason selection task and syllogistic reasoning, often displaying reasoning biases that are different from those shown by humans. However, the more advanced LLMs like GPT-3.5 and GPT-4 show some improvements, particularly in engaging in more deliberative reasoning processes that are similar to humans' system 2 processing. On inductive reasoning, earlier LLMs like GPT-3 faced significant challenges, especially on property induction, but newer models like GPT-4 shows performance almost at par with humans. On analogical reasoning, most LLMs demonstrate strong capabilities, sometimes even surpassing human performance. However, reasoning biases remain a challenge, with LLMs often giving heuristic responses. GPT-4, however, shows signs of surpassing these limitations, delivering super-human performance. Lastly, on abductive reasoning, LLMs demonstrate human-like content effects, and are found to struggle with abstract or counter-intuitive scenarios.</p>
<h1>4 Creativity Cognitive Patterns</h1>
<p>Table 3 Creativity related studies, with specific type, dataset/tasks used, LLMs involved, associated findings, and corresponding citation. The first three rows, the next two rows, and the last two rows contain studies related to the novel use of objects, creative writing, and assistive creativity, respectively. $\uparrow$ indicates better than or equivalent to human performance, while $\downarrow$ indicates worse than human performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">Findings</th>
<th style="text-align: center;">Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Divergent <br> thinking</td>
<td style="text-align: center;">Alternative Uses Test</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Originality $\downarrow$, <br> surprise $\downarrow$, semantic <br> distance $\downarrow$, utility $\uparrow$</td>
<td style="text-align: center;">Stevenson <br> et al. [89]</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Alternative Uses Test, <br> Torrance Test of Cre- <br> ative Writing, Diver- <br> gent Associations Task</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Originality $\uparrow$, <br> elaboration $\uparrow$</td>
<td style="text-align: center;">Hubert et al. <br> $[90]$</td>
</tr>
<tr>
<td style="text-align: center;">Novel <br> problem- <br> solving</td>
<td style="text-align: center;">Novel tool selection <br> task</td>
<td style="text-align: center;">GPT-3.5, <br> text-davinci- <br> 003</td>
<td style="text-align: center;">All models $\downarrow$</td>
<td style="text-align: center;">Yiu et al. [91]</td>
</tr>
<tr>
<td style="text-align: center;">Creative writ- <br> ing</td>
<td style="text-align: center;">Torrance Test of Cre- <br> ative Writing</td>
<td style="text-align: center;">GPT-3.5, <br> GPT-4, <br> Claude-v1.3</td>
<td style="text-align: center;">All models $\downarrow$</td>
<td style="text-align: center;">Chakrabarty <br> et al. [92]</td>
</tr>
<tr>
<td style="text-align: center;">Creative writ- <br> ing</td>
<td style="text-align: center;">Five-sentence creative <br> story task</td>
<td style="text-align: center;">GPT-3, GPT- <br> 4</td>
<td style="text-align: center;">All models $\uparrow$</td>
<td style="text-align: center;">Orwig et al. <br> $[93]$</td>
</tr>
<tr>
<td style="text-align: center;">Creative <br> assistance</td>
<td style="text-align: center;">Five curated tasks <br> involving LLM as cre- <br> ative assistant</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Creativity $\uparrow$</td>
<td style="text-align: center;">Lee and <br> Chung [94]</td>
</tr>
<tr>
<td style="text-align: center;">Idea genera- <br> tion</td>
<td style="text-align: center;">Curated dataset</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Novelty $\downarrow$, strategic <br> viability $\uparrow$, financial <br> value $\uparrow$</td>
<td style="text-align: center;">Boussioux <br> et al. [95]</td>
</tr>
</tbody>
</table>
<h3>4.1 Human creativity</h3>
<p>Guilford [96] describes a creative pattern as one that "is manifest in creative behavior, which includes such activities as inventing, designing, contriving, composing, and planning. People who exhibit these types of behavior to a marked degree are recognized as being creative." Creativity, in short, is the ability to produce something that is both original and worthwhile [97, 98]. Outcomes of creative pursuits include inventions, discoveries, and artwork. Within decision-making frameworks, creativity is touted as the ability to devise novel alternatives and innovative solutions to problems. One of the ways in which it is measured in humans is divergent production [96] (more commonly known as 'divergent thinking' today), which is the generation of diverse responses when presented with a stimulus. Two prevalent tasks are the Alternative Uses Test (AUT) [99] and the Torrance Test of Creative Thinking (TTCT) [100].</p>
<p>In AUT, participants are tasked with the generation of a diverse array of potential uses for commonplace objects, such as bricks or paper clips. The responses are often</p>
<p>evaluated using criteria like originality (the uniqueness of ideas), utility (the practicality of ideas), and surprise (the unpredictability of the ideas). The TTCT comprises two parts: verbal and figural. The verbal component solicits ideas, hypotheses, or solutions from participants using a picture as the stimulus. The responses are assessed on fluency, flexibility, and originality. The figural component asks the participant to complete partially completed shapes or figures. This component evaluates participants' fluency, originality, elaboration, abstractness of titles, resistance to premature closure abilities, and the checklists of creativity strengths.</p>
<h1>4.2 Creativity in LLMs</h1>
<p>Because of their wide application and success in evaluating human creativity, these two tests have also been employed to measure the creative capabilities of LLMs. Stevenson et al. [89] applied AUT to assess GPT-3's performance and found that human participants scored higher on both originality and surprise, as well as semantic distance, compared to GPT-3. However, GPT-3 received higher utility ratings. Additionally, a negative association between originality and utility was observed in both human and model responses.</p>
<p>Similarly, Hubert et al. [90] used the AUT task to compare GPT-4's creativity with that of humans and found that it generated more diverse responses and displayed more elaboration than the human counterpart, exhibiting higher originality when using specific prompts. Additionally, Yiu et al. [91] proposed a novel task that required accomplishing a goal without the typical tool. Their findings revealed that while LLMs, including GPT-3.5 and text-davinci-003, can nearly match humans in recognizing superficial similarities between objects, they significantly lag behind both adults and children when they are asked to choose an unfamiliar tool to solve a problem and often default to conventional solutions rather than novel choices.</p>
<p>Focusing on creative writing, Hubert et al. [90] used the Consequences Task in TTCT and the Divergent Associations Task [101] to compare GPT-4's creativity with that of humans. They revealed that GPT-4 demonstrated greater originality and elaboration than humans across these tasks, even when fluency of responses was controlled for. Extending to TTCT, Chakrabarty et al. [92] proposed the Torrance Test of Creative Writing (TTCW) to evaluate the creative writing abilities of three LLMs (i.e., GPT-3.5, GPT-4, and Claude-v1.3). Their study found that stories generated by these LLMs were significantly less likely to pass individual TTCW tests compared to those written by human experts. Additionally, Orwig et al. [93] evaluated two LLMs' (GPT3 and GPT-4) ability to write creative short stories using the five-sentence creative story task, where the participants are given a three-word prompt and asked to include all three words when writing a short story in approximately five sentences. They find that both LLMs can generate stories that are comparable in creativity to those produced by humans. Interestingly, they also found that GPT-4 was notably consistent in aligning its creativity ratings with those of human evaluators.</p>
<p>Another interesting area of research is to explore how LLMs can assist humans in creative contexts. Lee and Chung [94] evaluated GPT-3.5's ability to assist humans in accomplishing five creative tasks, including choosing a creative gift for a teenager, making a toy, re-purposing unused items, designing an innovative dining table, and</p>
<p>re-purposing emotionally significant items. They compared the creativity of ideas generated by humans with the assistance of ChatGPT to those generated using conventional web searches or no technology at all. Their findings demonstrated that ChatGPT significantly improved the creativity of ideas, particularly when it comes to generating incrementally new ideas. In a more nuanced setting, Boussioux et al. [95] explored human-AI collaboration in generating sustainable, circular economy business ideas using GPT-4. They showed that while human solutions tend to be more novel, human-AI-created solutions often outperformed in terms of strategic viability, environmental and financial value, and overall quality. Additionally, human-AI solutions created through differentiated search, where human-guided prompts progressively directed the LLM to produce outputs increasingly different from the previous versions, outperformed those generated through independent search.</p>
<h1>4.3 Conclusion</h1>
<p>While LLMs like GPT-3 and GPT-3.5 exhibit utility in responses, they often fall short in originality and novelty compared to human creativity in tasks requiring innovative problem-solving. However, more advanced LLMs, such as GPT-4, do tend to give more original and novel solutions than humans. On the other hand, findings on LLMs' creative writing ability present a more nuanced picture and highlight the need for further investigation. Promisingly, LLMs have shown great potential in collaborative contexts by enhancing human creativity through generation of incrementally new ideas and competence on domain-specific problems and metrics.</p>
<h2>5 Discussion</h2>
<p>Based on these findings, there is promising evidence in favor of LLMs exhibiting all three processes of decision-making, reasoning, and creativity, as emergent patterns. However, the studies also highlight some key limitations. On decision-making, LLMs demonstrate 13 different humanlike heuristics and biases, including frame effect, risk aversion, and anchoring effect. This emergence is likely attributed to the extensive human corpus on which they are trained, which reflects (either explicitly or implicitly) human biases. One reason to be mindful of these biases is that they can lead to undesirable outcomes when deploying them in real-world applications, warranting stronger testing, fail-safes, and ethical reviews. Intriguingly, the absence of certain biases (such as waste aversion [102]) may also yield valuable insights, as they offer insightful commentary, not just of the LLMs' training and underlying neural modeling, but also of the human corpus itself. More practically, a better understanding of how some of these biases are less present in the LLMs than we would expect from human population experiments may help researchers develop pre-training and fine-tuning protocols to proactively exclude these biases from the models if so desired.</p>
<p>The evaluation of reasoning presents a more nuanced picture, mainly because reasoning can be defined in broad ways and involves a wide variety of modalities and cognitive processes, even in humans. On classic deductive reasoning tasks, earlier LLMs struggled, but the performance of the latest models has improved drastically, including on tasks known to be cognitively intensive or even challenging for humans. This shift</p>
<p>toward more rigorous, System 2-like reasoning [71] in GPT-4 signals the trend of LLMs to mimic more deliberative reasoning. This could potentially be due to the increased use of methodologies like Chain-of-Thought (CoT) being deployed in the prompting of these models. Inductive reasoning tasks appear to be easier for LLMs, achieving even super-human performance on analogical reasoning problems. Analogical reasoning relies heavily on identifying structural similarities between different concepts, and it is possible that LLMs naturally internalize these similarities by learning statistical associations from massive training data. Nevertheless, intuitive and incorrect reasoning processes are still observed in LLMs. This finding, coupled with the phenomenon of hallucinations, which is when LLMs generate outputs that are factually incorrect but still appear very confident in their responses, suggests caution when trusting their reasoning, especially in scenarios that are somewhat novel or implausible.</p>
<p>Commonsense reasoning is yet another challenging aspect of reasoning for LLMs. Unlike deductive or analogical reasoning, commonsense reasoning requires LLMs to use everyday knowledge to make plausible inferences in various scenarios. Current studies on LLMs' commonsense reasoning abilities mainly focus on evaluating their performances on artificial benchmarks, such as the CommonSenseQA [103] and the SocialIQa [104]. To truly assess the commonsense reasoning abilities of LLMs and compare their performance with humans, researchers should look into tests that are inspired by classic human experiments, as recent pieces have argued [105, 106].</p>
<p>The studies that researched creativity collectively reveal a dichotomy: on the one hand, LLMs lack originality and novelty in divergent creativity tasks, where they are usually asked to come up with novel use cases of familiar objects. However, in creative writing, especially on tasks designed to be relatively open-ended, LLMs like GPT-4 show enormous promise and, in many cases, can be prompted to produce stories that match human creativity [93]. This dichotomy can (at least partially) be explained by a language bias: LLMs seem to excel in language-related creative tasks, whereas those that are either multi-modal or abstract may require more (or different) advances. The lack of physical grounding, or embodied cognition, in LLMs has already been noted as an important limitation [107]. Lack of such cognition places obvious constraints on LLMs' ability to create innovative solutions for divergent thinking tasks that require a deep and real-world (including physical) understanding of the objects given.</p>
<p>Surprisingly, however, LLMs seem to excel in collaborative creativity. When used as creative assistants, they have been shown to supplement human effort by generating incrementally novel ideas and providing diverse perspectives. In practice, this makes them prime candidates as augmented AI, in jobs ranging from report writing [108] to software engineering [109], both of which are mainstay enterprise applications. It also suggests an important research avenue in both organizational psychology and humancomputer interaction, namely, to explore how LLMs can be most effectively integrated into creative industrial processes to foster innovations more rapidly.</p>
<p>Taken together, the studies reveal some important gaps in the current research landscape. Cognitive phenomena like attention [110], memory [111], and representation of knowledge [112], have not been as extensively studied as the cognitive processes that we reviewed. These phenomena are foundational elements of human cognition</p>
<p>and clearly play a role in the three processes that we did cover. For this reason, studying them as individual emergent phenomena in LLMs may have been more difficult. Nevertheless, we maintain that, with the appropriate design of tests, it should be possible to study these phenomena reductively in LLMs. For example, a single promising study focusing on memory [113] employed the n-back task [114] to evaluate GPT-3.5's capacity for working memory. In another study, Jones et al. [115] evaluate knowledge of affordances in GPT-3. Given these possibilities and the currently limited exploration of these phenomena in LLM research, they offer promising avenues for future research.</p>
<p>Finally, we found that the majority of experiments reported in the literature we reviewed tend to rely heavily on the GPT family of LLMs (which include GPT-3, GPT-3.5, and GPT-4). Although this is probably because of their easy access, other open-source LLMs have not been as extensively tested, which could be a source of bias and non-replication down the line, because GPT models are not (at the time of writing) open in any way. Even the training data used for these models is not publicly documented. Recently, some authors argued that, for scientific reasons, there should be a stronger justification from researchers if they choose to eschew open models [116]. More open LLMs offer unique advantages because they are more transparent, offer greater flexibility for customization, and allow researchers to conduct (and replicate) more controlled experiments. At the same time, the GPT family cannot be ignored, owing to its widespread usage in the real world. Fortunately, the two are not in conflict. Given the relative ease of accessing and prompting both open and closed models at the present moment, and the rising number of papers in the AI community that are now choosing to use at least $10+$ models in reporting experimental results [74, 117119], we advocate this as a recommended practice in the still-young community of computational psychology.</p>
<h1>References</h1>
<p>[1] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, $1877-1901(2020)$
[2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)
[3] Vaswani, A.: Attention is all you need. Advances in Neural Information Processing Systems (2017)
[4] Popel, M., Tomkova, M., Tomek, J., Kaiser, ., Uszkoreit, J., Bojar, O., abokrtsk, Z.: Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals. Nature communications 11(1), 1-15 (2020)
[5] Bahdanau, D.: Neural machine translation by jointly learning to align and</p>
<p>translate. arXiv preprint arXiv:1409.0473 (2014)
[6] Huang, Z., Xu, W., Yu, K.: Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991 (2015)
[7] Lin, Y., Shen, S., Liu, Z., Luan, H., Sun, M.: Neural relation extraction with selective attention over instances. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. $2124-2133(2016)$
[8] Park, J.S., OBrien, J., Cai, C.J., Morris, M.R., Liang, P., Bernstein, M.S.: Generative agents: Interactive simulacra of human behavior. In: Proceedings of the 36th Annual Acm Symposium on User Interface Software and Technology, pp. 1-22 (2023)
[9] Strachan, J.W., Albergo, D., Borghini, G., Pansardi, O., Scaliti, E., Gupta, S., Saxena, K., Rufo, A., Panzeri, S., Manzi, G., et al.: Testing theory of mind in large language models and humans. Nature Human Behaviour, 1-11 (2024)
[10] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023)
[11] Sarrion, E.: Using chatgpt for artistic content creation. In: Exploring the Power of ChatGPT: Applications, Techniques, and Implications, pp. 155-170. Springer, ??? (2023)
[12] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al.: Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022)
[13] Schaeffer, R., Miranda, B., Koyejo, S.: Are emergent abilities of large language models a mirage? Advances in Neural Information Processing Systems 36 (2024)
[14] Mikolov, T.: Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 3781 (2013)
[15] Devlin, J.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)
[16] Rogers, A., Kovaleva, O., Rumshisky, A.: A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics 8, 842-866 (2021)
[17] Shah, A.K., Oppenheimer, D.M.: Heuristics made easy: an effort-reduction framework. Psychological bulletin 134(2), 207 (2008)</p>
<p>[18] Kahneman, D.: Prospect theory: An analysis of decisions under risk. Econometrica 47, 278 (1979)
[19] Baron, J.: Thinking and Deciding. Cambridge University Press, ??? (2023)
[20] Amabile, T.M.: Creativity in Context: Update to the Social Psychology of Creativity. Routledge, ??? (2018)
[21] Kahneman, D., Tversky, A.: Subjective probability: A judgment of representativeness. Cognitive psychology 3(3), 430-454 (1972)
[22] Binz, M., Schulz, E.: Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences 120(6), 2218523120 (2023)
[23] Itzhak, I., Stanovsky, G., Rosenfeld, N., Belinkov, Y.: Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias. arXiv preprint arXiv:2308.00225 (2023)
[24] Chen, Y., Andiappan, M., Jenkin, T., Ovchinnikov, A.: A manager and an ai walk into a bar: Does chatgpt make biased decisions like we do? Available at SSRN 4380365 (2023)
[25] Su, J., Lang, Y., Chen, K.-Y.: Can ai solve newsvendor problem without making biased decisions? a behavioral experimental study. A Behavioral Experimental Study (September 1, 2023) (2023)
[26] Chen, Y., Liu, T.X., Shan, Y., Zhong, S.: The emergence of economic rationality of gpt. arXiv preprint arXiv:2305.12763 (2023)
[27] Suri, G., Slater, L.R., Ziaee, A., Nguyen, M.: Do large language models show decision heuristics similar to humans? a case study using gpt-3.5. arXiv preprint arXiv:2305.04400 (2023)
[28] Tversky, A., Kahneman, D.: Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. science 185(4157), 1124-1131 (1974)
[29] Jonathan, B.E.: Dual-process theories. In: International Handbook of Thinking and Reasoning, pp. 151-166. Routledge, ??? (2017)
[30] Cialdini, R.B., Goldstein, N.J.: Social influence: Compliance and conformity. Annu. Rev. Psychol. 55(1), 591-621 (2004)
[31] Glimcher, P.W., Rustichini, A.: Neuroeconomics: the consilience of brain and decision. Science 306(5695), 447-452 (2004)
[32] Tversky, A., Kahneman, D.: Availability: A heuristic for judging frequency and probability. Cognitive psychology 5(2), 207-232 (1973)</p>
<p>[33] White, P.A.: The representativeness heuristic and the study of judgment under uncertainty. NEW ZEALAND JOURNAL OF PSYCHOLOGY 13(1), 1-9 (1984)
[34] Furnham, A., Boo, H.C.: A literature review of the anchoring effect. The journal of socio-economics 40(1), 35-42 (2011)
[35] Gigerenzer, G., Gaissmaier, W.: Heuristic decision making. Annual review of psychology 62(1), 451-482 (2011)
[36] Tversky, A., Kahneman, D.: Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Psychological review 90(4), 293 (1983)
[37] Tversky, A., Kahneman, D.: Causal schemas in judgments under uncertainty. In: Progress in Social Psychology, pp. 49-72. Psychology Press, ??? (2015)
[38] Peterson, J.C., Bourgin, D.D., Agrawal, M., Reichman, D., Griffiths, T.L.: Using large-scale experiments and machine learning to discover theories of human decision-making. Science 372(6547), 1209-1214 (2021)
[39] Huber, J., Payne, J.W., Puto, C.: Adding asymmetrically dominated alternatives: Violations of regularity and the similarity hypothesis. Journal of consumer research 9(1), 90-98 (1982)
[40] Evans, J.S.B., Barston, J.L., Pollard, P.: On the conflict between logic and belief in syllogistic reasoning. Memory \&amp; cognition 11(3), 295-306 (1983)
[41] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b. arXiv preprint arXiv:2310.06825 (2023)
[42] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21(140), 1-67 (2020)
[43] Schweitzer, M.E., Cachon, G.P.: Decision bias in the newsvendor problem with a known demand distribution: Experimental evidence. Management science 46(3), $404-420(2000)$
[44] Nishimura, H., Ok, E.A., Quah, J.K.-H.: A comprehensive approach to revealed preference theory. American Economic Review 107(4), 1239-1263 (2017)
[45] Yang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., Schuurmans, D.: Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129 (2023)</p>
<p>[46] Seals, S., Shalin, V.L.: Evaluating the deductive competence of large language models. arXiv preprint arXiv:2309.05452 (2023)
[47] Ando, R., Morishita, T., Abe, H., Mineshima, K., Okada, M.: Evaluating large language models with neubaroco: Syllogistic reasoning ability and human-like biases. arXiv preprint arXiv:2306.12567 (2023)
[48] Hagendorff, T., Fabi, S., Kosinski, M.: Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Computational Science, 1-6 (2023)
[49] Osherson, D.N., Smith, E.E., Wilkie, O., Lopez, A., Shafir, E.: Category-based induction. Psychological review 97(2), 185 (1990)
[50] Han, S.J., Ransom, K.J., Perfors, A., Kemp, C.: Inductive reasoning in humans and large language models. Cognitive Systems Research 83, 101155 (2024)
[51] Hu, X., Storks, S., Lewis, R.L., Chai, J.: In-context analogical reasoning with pre-trained language models. arXiv preprint arXiv:2305.17626 (2023)
[52] Webb, T., Holyoak, K.J., Lu, H.: Emergent analogical reasoning in large language models. Nature Human Behaviour, 1-16 (2023)
[53] Palminteri, S., Yax, N., Anllo, H.: Studying and improving reasoning in humans and machines (2023)
[54] Dasgupta, I., Lampinen, A.K., Chan, S.C., Creswell, A., Kumaran, D., McClelland, J.L., Hill, F.: Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051 (2022)
[55] Sternberg, R.J.: What do we know about the nature of reasoning? (2004)
[56] Leighton, J.P., Sternberg, R.J.: The Nature of Reasoning. Cambridge University Press, ??? (2004)
[57] Johnson-Laird, P.N.: Deductive reasoning. Annual review of psychology 50(1), 109-135 (1999)
[58] Johnson-Laird, P.N.: Thinking: Reasoning. American Psychological Association, ??? (2000)
[59] Walton, D.: Abductive Reasoning. University of Alabama Press, ??? (2014)
[60] Beller, S.: Deontic norms, deontic reasoning, and deontic conditionals. Thinking \&amp; Reasoning 14(4), 305-341 (2008)
[61] Gordon, A.S., Hobbs, J.R.: A Formal Theory of Commonsense Psychology: How People Think People Think. Cambridge University Press, ??? (2017)</p>
<p>[62] Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024)
[63] Team, M.N.: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs. Accessed: 2023-05-05. www.mosaicml.com/blog/mpt-7b Accessed 2023-05-05
[64] Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ili, S., Hesslow, D., Castagn, R., Luccioni, A.S., Yvon, F., Gall, M., et al.: Bloom: A 176b-parameter open-access multilingual language model (2023)
[65] Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, ., Hesslow, D., Launay, J., Malartic, Q., et al.: The falcon series of open language models. arXiv preprint arXiv:2311.16867 (2023)
[66] Wason, P.C.: Reasoning about a rule. Quarterly journal of experimental psychology 20(3), 273-281 (1968)
[67] Liu, Y.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)
[68] Lewis, M.: Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019)
[69] Van Benthem, J., et al.: Essays in Logical Semantics vol. 29. Springer, ??? (1986)
[70] Shikishima, C., Hiraishi, K., Yamagata, S., Sugimoto, Y., Takemura, R., Ozaki, K., Okada, M., Toda, T., Ando, J.: Is g an entity? a japanese twin study using syllogisms and intelligence tests. Intelligence 37(3), 256-267 (2009)
[71] Kahneman, D.: Thinking, Fast and Slow. macmillan, ??? (2011)
[72] Frederick, S.: Cognitive reflection and decision making. Journal of Economic perspectives 19(4), 25-42 (2005)
[73] Erickson, T.D., Mattson, M.E.: From words to meaning: A semantic illusion. Journal of Verbal Learning and Verbal Behavior 20(5), 540-551 (1981)
[74] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35, 24824-24837 (2022)
[75] Holyoak, K.J.: Analogical thinking and human intelligence. Advances in the psychology of human intelligence 2, 199-230 (1984)
[76] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,</p>
<p>Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022)
[77] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)
[78] Zhang, C., Gao, F., Jia, B., Zhu, Y., Zhu, S.-C.: Raven: A dataset for relational and analogical visual reasoning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5317-5327 (2019)
[79] Raven, J.: Raven's progressive matrices: Western Psychological Services Los Angeles. CA (1938)
[80] Snow, R.E., Kyllonen, P.C., Marshalek, B., et al.: The topography of ability and learning correlations. Advances in the psychology of human intelligence 2(S 47), 103 (1984)
[81] Simon, H.A.: Administrative Behavior. Simon and Schuster, ??? (2013)
[82] Braas-Garza, P., Kujal, P., Lenkei, B.: Cognitive reflection test: Whom, how, when. Journal of Behavioral and Experimental Economics 82, 101455 (2019)
[83] Sides, A., Osherson, D., Bonini, N., Viale, R.: On the reality of the conjunction fallacy. Memory \&amp; cognition 30, 191-198 (2002)
[84] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozire, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)
[85] Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality (2023). https://lmsys. org/blog/2023-03-30-vicuna/
[86] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al.: Training computeoptimal large language models. arXiv preprint arXiv:2203.15556 (2022)
[87] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24(240), $1-113(2023)$
[88] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. Journal of Machine Learning Research 25(70), 1-53 (2024)</p>            </div>
        </div>

    </div>
</body>
</html>
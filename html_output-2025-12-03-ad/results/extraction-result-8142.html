<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8142 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8142</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8142</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f86fb205cd4d85478e65304fc38bdf1e4bed2440" target="_blank">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain, demonstrating that appropriate pre-trained representations can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8142.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8142.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier features in hidden states and token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained LLMs represent numbers via a sparse set of Fourier-basis components in the logits and token embeddings; low-frequency components approximate the magnitude of the sum while high-frequency components implement modular/classification information (e.g., mod 2,5,10).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pre-trained Transformer LLMs (studied: GPT-2-XL, GPT-J, Phi-2; behavior of GPT-3.5/GPT-4/PaLM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer architectures; primary experimental model GPT-2-XL (48 layers, ~1.5B parameters, pre-trained and then fine-tuned on the addition dataset). Other examined models: GPT-J-6B, Phi-2 (2.7B), GPT-3.5/GPT-4/PaLM-2 (behavioral analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition of two natural numbers (each <= 260), presented in natural-language templates (also tested alternate formats in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Numbers and intermediate logits decompose into Fourier basis waves; sparse high-magnitude Fourier coefficients at specific periods (notably periods 2, 2.5, 5, 10) encode modular/classification information, while low-frequency components encode coarse magnitude/approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Discrete Fourier Transform (DFT) of logits and token embeddings, Logit Lens to read intermediate predictions layer-by-layer, construction of low-pass/high-pass linear projections (filters) that zero selected Fourier coefficients (projection to nullspace of B F W^U), single-component retention experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mechanism underlies high performance for pre-trained+fine-tuned models (see GPT-2-XL: 99.74% test accuracy). Models without pre-trained Fourier-rich embeddings have lower accuracy (examples: GPT-2-XL trained from scratch 94.44%; GPT-2-small trained from scratch 53.95% unless provided pre-trained embeddings, then 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When low-frequency components are ablated: large-magnitude magnitude/approximation errors (off-by 10, 50, 100). When high-frequency components are ablated: errors in unit digit (small errors <6). Models without Fourier features make frequent off-by-one errors and generally fail to perform modular classification precisely.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise Logit Lens shows progressive refinement (early layers approximate magnitude within ±2/±10, later layers pick exact answer); DFT of intermediate MLP and attention logits shows sparse outlier components (Figures 2–4, 3); token-embedding DFT shows outlier components at periods 2,2.5,5,10 (Figure 5a); inverse DFT of top components reconstructs final prediction (Figure 4); causal intervention by projecting out frequency bands (low-pass/high-pass filters) changes accuracy in predictable ways (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Large-period (very low frequency) components such as period 520 do not precisely localize the answer and behave only as coarse magnitude signals; models trained from scratch can still learn to approximate sums (higher accuracy than trivial) but do not develop the same sparse high-frequency components and make different error modes; closed-source models cannot be inspected internally, so evidence there is behavioral only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8142.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8142.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL (pre-trained, then fine-tuned on the addition dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained GPT-2-XL (48 layers, ~1.5B params) fine-tuned on the synthetic addition dataset achieves near-perfect accuracy and computes sums via a combination of low-frequency magnitude approximation (MLPs) and high-frequency modular classification (attention).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer, 48 layers, ~1.5B parameters; pre-trained on large corpora then fine-tuned on the synthetic addition dataset (numbers <= 260).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Adding two integers (<= 260) expressed in natural-language question templates.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Residual-stream representation accumulates MLP and attention outputs whose W^U-projected logits decompose sparsely in Fourier space; MLP layers contribute strong low-frequency components (magnitude approximation); attention layers contribute strong high-frequency components (modular classification such as unit digit).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Logit Lens to read W^U h^{(ℓ)} at each layer; DFT of intermediate logits and token embeddings; construction and application of low-pass/high-pass filters (linear projections that zero designated Fourier components) applied to MLP/Attn outputs; single-component filters (retain one Fourier component).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned test accuracy: 99.74% (validation loss without filtering 0.0073, accuracy 0.9974). Ablation/filtering results (validation loss / accuracy):
 - ATTN & MLP low-frequency removed: 4.0842 / 0.0594
 - ATTN low-frequency removed: 0.0352 / 0.9912
 - MLP low-frequency removed: 2.1399 / 0.3589
 - ATTN & MLP high-frequency removed: 1.8598 / 0.2708
 - ATTN high-frequency removed: 0.5943 / 0.7836
 - MLP high-frequency removed: 0.1213 / 0.9810</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Ablating MLP low-frequency components yields large magnitude errors (off-by 10, 50, 100) i.e., failure of magnitude approximation; ablating attention high-frequency components yields errors in unit digit and small absolute errors (<6); intermediate layers show coarse-to-fine behavior (approximation first, classification later).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise predictions shift from approximate magnitudes to exact answers (Figure 1a); MLP and attention logits show distinct periodic patterns and outlier Fourier coefficients (Figures 1b,c and 2); removing specific frequency bands from MLP/Attn outputs produces predicted degradations in accuracy (Table 1); sum of top-5 Fourier components reconstructs correct answer (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some low-frequency components (very large period) cannot precisely place peaks (e.g., period 520 component only gives coarse localization); both MLPs and attention modules can contain some high-frequency components, so decomposition is not absolute but dominant tendencies are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8142.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8142.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL trained from random initialization on the addition task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same GPT-2-XL architecture trained only on the addition dataset from random initialization attains lower accuracy and does not develop the sparse high-frequency Fourier features seen in pre-trained models, relying instead mainly on low-frequency approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL (random init, trained on addition)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2-XL architecture (48 layers, ~1.5B) trained from scratch on the synthetic addition dataset (no pre-training).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition (same dataset, numbers <= 260).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Model lacks outlier high-frequency Fourier components in token embeddings and intermediate logits; primarily uses low-frequency approximation signals and fails to reliably perform modular classification.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>DFT of token embeddings and intermediate logits, Logit Lens to examine layerwise predictions, comparison to pre-trained model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test accuracy after convergence: 94.44% (compared to 99.74% for pre-trained+fine-tuned GPT-2-XL).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Frequent off-by-one errors and other failures in unit-digit classification — consistent with absence of high-frequency modular features.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DFT of intermediate logits and token embeddings shows no outlier Fourier components (Figure 6 and Figure 7a); behavioral differences (error distribution) compared to pre-trained model (Appendix E/F figures).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Introducing pre-trained token embeddings into a randomly initialized model rescues performance, indicating that failure is due to missing inductive bias from pre-training (not architectural impossibility).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8142.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8142.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-small (embedding intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-small trained from scratch, with and without frozen pre-trained number embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small Transformer (GPT-2-small, 124M, 12 layers) trained from scratch on addition performs poorly (≈54%); freezing and using pre-trained token embeddings yields perfect accuracy (100%) and faster convergence across seeds, demonstrating the causal importance of pre-trained numerical embeddings with Fourier features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer, 12 layers, ~124M parameters; training experiments compare random initialization vs. frozen pre-trained token embeddings with other weights randomly initialized.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition (numbers <= 260).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Pre-trained token embeddings contain Fourier components (periods 2,2.5,5,10) that provide inductive features enabling the rest of the network to learn magnitude + modular classification decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Training runs with/without freezing pre-trained token embeddings; DFT analysis of token embeddings; measurement of convergence and final accuracy across 5 random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-2-small trained from scratch: test accuracy ~53.95%. GPT-2-small with frozen pre-trained token embeddings: 100% test accuracy and faster convergence (mean and std reported across 5 seeds in Figure 7b).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Without pre-trained embeddings, model fails to learn modular classification reliably, leading to low final accuracy and varied errors; with pre-trained embeddings, errors disappear on test set.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Provision of pre-trained embeddings (with Fourier features) consistently enables models that otherwise could not solve the task to converge to perfect accuracy (Figure 7b); DFT of pre-trained embeddings shows outlier Fourier components (Figure 5a).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>This intervention shows sufficiency of embeddings for enabling mechanism, but it does not by itself show how other architectures or datasets might differ; effect observed robustly across seeds in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8142.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8142.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J & Phi-2 (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B and Phi-2 (2.7B) using few-shot in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source larger models (GPT-J-6B and Phi-2) unable to perform addition zero-shot can perform it via 4-shot in-context learning and show intermediate logits with sparse Fourier components analogous to fine-tuned models; their absolute errors concentrate on multiples of 10.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B, Phi-2 (2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only Transformer models (GPT-J 6B; Phi-2 2.7B) used in 4-shot in-context examples to perform addition; internal logits of MLP/Attn analyzed for Fourier sparsity (for Phi-2 and GPT-J).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition (numbers <= 260) solved via 4-shot in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>DFT of MLP and attention logits across last layers shows outlier Fourier components at periods ~2,2.5,5,10 indicating use of Fourier-like modular signals during in-context computation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>4-shot in-context prompting followed by DFT analysis of intermediate module logits across last 15 layers; behavioral error analysis (absolute error multiples of 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used 4-shot: absolute errors are multiples of 10 in 93% of GPT-J cases and 73% of Phi-2 cases (distributional statistic indicating modular/classification failures typically aligned with mod-10).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Errors are concentrated on multiples of 10, suggesting failures in digit-level (unit or decade) resolution; without in-context examples models cannot perform the task.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DFT of MLP and attention module logits for Phi-2 and GPT-J shows sparsity and dominant periods at 2,2.5,5,10 similar to fine-tuned GPT-2-XL (Figure 8 and Figure 18); error distribution analysis supports modular classification interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>These results are from in-context prompting rather than fine-tuning; behavior could depend on prompt selection and number of shots; full causal ablations (filtering) were not reported for these models in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8142.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8142.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 / GPT-4 / PaLM-2 (behavioral)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-source large models (GPT-3.5, GPT-4, PaLM-2) — zero-shot behavior analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source instruction-tuned models show error distributions on 0-shot addition that are consistent with reliance on Fourier/modular mechanisms (absolute errors frequently multiples of 10), suggesting similar computational primitives though internal representations could not be inspected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned Transformer models (closed-source) evaluated behaviorally (0-shot) on the same synthetic addition dataset; internal activations not available for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition (numbers <= 260), 0-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Behavioral evidence (error modes) consistent with modular/classification failures (e.g., unit/decade errors) that align with the Fourier-feature hypothesis, though internal verification is not possible here.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral error analysis (distribution of absolute errors and whether errors are multiples of 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Absolute errors are multiples of 10: GPT-3.5 and GPT-4: 100% of errors; PaLM-2: 87% of errors (behavioral statistic reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Errors concentrated on multiples of 10, indicating failure modes that likely reflect poor modular (unit/decade) classification rather than magnitude approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Similarity of error distributions to open-source models and fine-tuned GPT-2-XL's ablation pattern motivates hypothesis that these models also use Fourier/modular strategies, though internal evidence is unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No internal representation analysis available for closed-source models; the conclusion is based on behavioral patterns only and thus remains a hypothesis rather than direct evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8142.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8142.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier filtering ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-pass / high-pass Fourier filtering interventions on MLP and Attention outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear-projection intervention that removes selected Fourier components from W^U-projected outputs of MLP or attention modules (solve via projection to nullspace of B F W^U) to test causal contribution of frequency bands to addition performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-2-XL (fine-tuned) and used conceptually on others</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Intervention is a post-hoc linear projection applied to module outputs before they are added to the residual stream; the projection zeros specified Fourier components in the vocabulary/logit space.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to probe integer addition computation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Directly manipulates representation in Fourier basis by nulling selected frequency coefficients; demonstrates causal role of low vs high frequencies in approximation vs classification.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Definition A.6: define binary mask B selecting frequencies to zero; compute orthogonal projection of module output into nullspace of B F W^U; apply globally to module outputs and measure loss/accuracy changes; single-component retention experiments also used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See Table 1 intervention outcomes (validation loss/accuracy) demonstrating large, interpretable drops in accuracy when relevant bands removed (e.g., removing attention high-frequency: accuracy drops to 0.7836; removing MLP low-frequency: accuracy drops to 0.3589; removing both low-frequency from ATTN&MLP: 0.0594).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Ablations produce predictable error modes aligned with the hypothesized roles: removing MLP low-frequency -> large off-by-ten-like errors; removing attention high-frequency -> small unit-digit errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Causal ablation via these filters produces specific degradations matching the hypothesized decomposition of tasks (approximation vs modular classification), supporting that the identified Fourier features are not merely correlational.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Choice of frequency threshold τ (τ=50 used) is heuristic though justified in appendix; projection is applied to W^U-space which assumes the vocabulary-number mapping and Fourier basis together capture the relevant modes—effects could vary for different vocabularies or larger numeric ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Harmonics of learning: Universal fourier features emerge in invariant networks <em>(Rating: 2)</em></li>
                <li>Feature emergence via margin maximization: case studies in algebraic tasks <em>(Rating: 2)</em></li>
                <li>Fourier features let networks learn high frequency functions in low dimensional domains <em>(Rating: 2)</em></li>
                <li>A toy model of universality: Reverse engineering how networks learn group operations <em>(Rating: 1)</em></li>
                <li>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8142",
    "paper_id": "paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Fourier features",
            "name_full": "Fourier features in hidden states and token embeddings",
            "brief_description": "Pre-trained LLMs represent numbers via a sparse set of Fourier-basis components in the logits and token embeddings; low-frequency components approximate the magnitude of the sum while high-frequency components implement modular/classification information (e.g., mod 2,5,10).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pre-trained Transformer LLMs (studied: GPT-2-XL, GPT-J, Phi-2; behavior of GPT-3.5/GPT-4/PaLM-2)",
            "model_description": "Decoder-only Transformer architectures; primary experimental model GPT-2-XL (48 layers, ~1.5B parameters, pre-trained and then fine-tuned on the addition dataset). Other examined models: GPT-J-6B, Phi-2 (2.7B), GPT-3.5/GPT-4/PaLM-2 (behavioral analysis).",
            "arithmetic_task_type": "Integer addition of two natural numbers (each &lt;= 260), presented in natural-language templates (also tested alternate formats in appendix).",
            "mechanism_or_representation": "Numbers and intermediate logits decompose into Fourier basis waves; sparse high-magnitude Fourier coefficients at specific periods (notably periods 2, 2.5, 5, 10) encode modular/classification information, while low-frequency components encode coarse magnitude/approximation.",
            "probing_or_intervention_method": "Discrete Fourier Transform (DFT) of logits and token embeddings, Logit Lens to read intermediate predictions layer-by-layer, construction of low-pass/high-pass linear projections (filters) that zero selected Fourier coefficients (projection to nullspace of B F W^U), single-component retention experiments.",
            "performance_metrics": "Mechanism underlies high performance for pre-trained+fine-tuned models (see GPT-2-XL: 99.74% test accuracy). Models without pre-trained Fourier-rich embeddings have lower accuracy (examples: GPT-2-XL trained from scratch 94.44%; GPT-2-small trained from scratch 53.95% unless provided pre-trained embeddings, then 100%).",
            "error_types_or_failure_modes": "When low-frequency components are ablated: large-magnitude magnitude/approximation errors (off-by 10, 50, 100). When high-frequency components are ablated: errors in unit digit (small errors &lt;6). Models without Fourier features make frequent off-by-one errors and generally fail to perform modular classification precisely.",
            "evidence_for_mechanism": "Layerwise Logit Lens shows progressive refinement (early layers approximate magnitude within ±2/±10, later layers pick exact answer); DFT of intermediate MLP and attention logits shows sparse outlier components (Figures 2–4, 3); token-embedding DFT shows outlier components at periods 2,2.5,5,10 (Figure 5a); inverse DFT of top components reconstructs final prediction (Figure 4); causal intervention by projecting out frequency bands (low-pass/high-pass filters) changes accuracy in predictable ways (Table 1).",
            "counterexamples_or_challenges": "Large-period (very low frequency) components such as period 520 do not precisely localize the answer and behave only as coarse magnitude signals; models trained from scratch can still learn to approximate sums (higher accuracy than trivial) but do not develop the same sparse high-frequency components and make different error modes; closed-source models cannot be inspected internally, so evidence there is behavioral only.",
            "uuid": "e8142.0",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-XL (fine-tuned)",
            "name_full": "GPT-2-XL (pre-trained, then fine-tuned on the addition dataset)",
            "brief_description": "A pre-trained GPT-2-XL (48 layers, ~1.5B params) fine-tuned on the synthetic addition dataset achieves near-perfect accuracy and computes sums via a combination of low-frequency magnitude approximation (MLPs) and high-frequency modular classification (attention).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL",
            "model_description": "Decoder-only Transformer, 48 layers, ~1.5B parameters; pre-trained on large corpora then fine-tuned on the synthetic addition dataset (numbers &lt;= 260).",
            "arithmetic_task_type": "Adding two integers (&lt;= 260) expressed in natural-language question templates.",
            "mechanism_or_representation": "Residual-stream representation accumulates MLP and attention outputs whose W^U-projected logits decompose sparsely in Fourier space; MLP layers contribute strong low-frequency components (magnitude approximation); attention layers contribute strong high-frequency components (modular classification such as unit digit).",
            "probing_or_intervention_method": "Logit Lens to read W^U h^{(ℓ)} at each layer; DFT of intermediate logits and token embeddings; construction and application of low-pass/high-pass filters (linear projections that zero designated Fourier components) applied to MLP/Attn outputs; single-component filters (retain one Fourier component).",
            "performance_metrics": "Fine-tuned test accuracy: 99.74% (validation loss without filtering 0.0073, accuracy 0.9974). Ablation/filtering results (validation loss / accuracy):\n - ATTN & MLP low-frequency removed: 4.0842 / 0.0594\n - ATTN low-frequency removed: 0.0352 / 0.9912\n - MLP low-frequency removed: 2.1399 / 0.3589\n - ATTN & MLP high-frequency removed: 1.8598 / 0.2708\n - ATTN high-frequency removed: 0.5943 / 0.7836\n - MLP high-frequency removed: 0.1213 / 0.9810",
            "error_types_or_failure_modes": "Ablating MLP low-frequency components yields large magnitude errors (off-by 10, 50, 100) i.e., failure of magnitude approximation; ablating attention high-frequency components yields errors in unit digit and small absolute errors (&lt;6); intermediate layers show coarse-to-fine behavior (approximation first, classification later).",
            "evidence_for_mechanism": "Layerwise predictions shift from approximate magnitudes to exact answers (Figure 1a); MLP and attention logits show distinct periodic patterns and outlier Fourier coefficients (Figures 1b,c and 2); removing specific frequency bands from MLP/Attn outputs produces predicted degradations in accuracy (Table 1); sum of top-5 Fourier components reconstructs correct answer (Figure 4).",
            "counterexamples_or_challenges": "Some low-frequency components (very large period) cannot precisely place peaks (e.g., period 520 component only gives coarse localization); both MLPs and attention modules can contain some high-frequency components, so decomposition is not absolute but dominant tendencies are observed.",
            "uuid": "e8142.1",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-XL (from-scratch)",
            "name_full": "GPT-2-XL trained from random initialization on the addition task",
            "brief_description": "The same GPT-2-XL architecture trained only on the addition dataset from random initialization attains lower accuracy and does not develop the sparse high-frequency Fourier features seen in pre-trained models, relying instead mainly on low-frequency approximation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL (random init, trained on addition)",
            "model_description": "GPT-2-XL architecture (48 layers, ~1.5B) trained from scratch on the synthetic addition dataset (no pre-training).",
            "arithmetic_task_type": "Integer addition (same dataset, numbers &lt;= 260).",
            "mechanism_or_representation": "Model lacks outlier high-frequency Fourier components in token embeddings and intermediate logits; primarily uses low-frequency approximation signals and fails to reliably perform modular classification.",
            "probing_or_intervention_method": "DFT of token embeddings and intermediate logits, Logit Lens to examine layerwise predictions, comparison to pre-trained model behavior.",
            "performance_metrics": "Test accuracy after convergence: 94.44% (compared to 99.74% for pre-trained+fine-tuned GPT-2-XL).",
            "error_types_or_failure_modes": "Frequent off-by-one errors and other failures in unit-digit classification — consistent with absence of high-frequency modular features.",
            "evidence_for_mechanism": "DFT of intermediate logits and token embeddings shows no outlier Fourier components (Figure 6 and Figure 7a); behavioral differences (error distribution) compared to pre-trained model (Appendix E/F figures).",
            "counterexamples_or_challenges": "Introducing pre-trained token embeddings into a randomly initialized model rescues performance, indicating that failure is due to missing inductive bias from pre-training (not architectural impossibility).",
            "uuid": "e8142.2",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-small (embedding intervention)",
            "name_full": "GPT-2-small trained from scratch, with and without frozen pre-trained number embeddings",
            "brief_description": "A small Transformer (GPT-2-small, 124M, 12 layers) trained from scratch on addition performs poorly (≈54%); freezing and using pre-trained token embeddings yields perfect accuracy (100%) and faster convergence across seeds, demonstrating the causal importance of pre-trained numerical embeddings with Fourier features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-small",
            "model_description": "Decoder-only Transformer, 12 layers, ~124M parameters; training experiments compare random initialization vs. frozen pre-trained token embeddings with other weights randomly initialized.",
            "arithmetic_task_type": "Integer addition (numbers &lt;= 260).",
            "mechanism_or_representation": "Pre-trained token embeddings contain Fourier components (periods 2,2.5,5,10) that provide inductive features enabling the rest of the network to learn magnitude + modular classification decomposition.",
            "probing_or_intervention_method": "Training runs with/without freezing pre-trained token embeddings; DFT analysis of token embeddings; measurement of convergence and final accuracy across 5 random seeds.",
            "performance_metrics": "GPT-2-small trained from scratch: test accuracy ~53.95%. GPT-2-small with frozen pre-trained token embeddings: 100% test accuracy and faster convergence (mean and std reported across 5 seeds in Figure 7b).",
            "error_types_or_failure_modes": "Without pre-trained embeddings, model fails to learn modular classification reliably, leading to low final accuracy and varied errors; with pre-trained embeddings, errors disappear on test set.",
            "evidence_for_mechanism": "Provision of pre-trained embeddings (with Fourier features) consistently enables models that otherwise could not solve the task to converge to perfect accuracy (Figure 7b); DFT of pre-trained embeddings shows outlier Fourier components (Figure 5a).",
            "counterexamples_or_challenges": "This intervention shows sufficiency of embeddings for enabling mechanism, but it does not by itself show how other architectures or datasets might differ; effect observed robustly across seeds in this setting.",
            "uuid": "e8142.3",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-J & Phi-2 (in-context)",
            "name_full": "GPT-J-6B and Phi-2 (2.7B) using few-shot in-context learning",
            "brief_description": "Open-source larger models (GPT-J-6B and Phi-2) unable to perform addition zero-shot can perform it via 4-shot in-context learning and show intermediate logits with sparse Fourier components analogous to fine-tuned models; their absolute errors concentrate on multiples of 10.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B, Phi-2 (2.7B)",
            "model_description": "Large decoder-only Transformer models (GPT-J 6B; Phi-2 2.7B) used in 4-shot in-context examples to perform addition; internal logits of MLP/Attn analyzed for Fourier sparsity (for Phi-2 and GPT-J).",
            "arithmetic_task_type": "Integer addition (numbers &lt;= 260) solved via 4-shot in-context learning.",
            "mechanism_or_representation": "DFT of MLP and attention logits across last layers shows outlier Fourier components at periods ~2,2.5,5,10 indicating use of Fourier-like modular signals during in-context computation.",
            "probing_or_intervention_method": "4-shot in-context prompting followed by DFT analysis of intermediate module logits across last 15 layers; behavioral error analysis (absolute error multiples of 10).",
            "performance_metrics": "When used 4-shot: absolute errors are multiples of 10 in 93% of GPT-J cases and 73% of Phi-2 cases (distributional statistic indicating modular/classification failures typically aligned with mod-10).",
            "error_types_or_failure_modes": "Errors are concentrated on multiples of 10, suggesting failures in digit-level (unit or decade) resolution; without in-context examples models cannot perform the task.",
            "evidence_for_mechanism": "DFT of MLP and attention module logits for Phi-2 and GPT-J shows sparsity and dominant periods at 2,2.5,5,10 similar to fine-tuned GPT-2-XL (Figure 8 and Figure 18); error distribution analysis supports modular classification interpretation.",
            "counterexamples_or_challenges": "These results are from in-context prompting rather than fine-tuning; behavior could depend on prompt selection and number of shots; full causal ablations (filtering) were not reported for these models in paper.",
            "uuid": "e8142.4",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5 / GPT-4 / PaLM-2 (behavioral)",
            "name_full": "Closed-source large models (GPT-3.5, GPT-4, PaLM-2) — zero-shot behavior analysis",
            "brief_description": "Closed-source instruction-tuned models show error distributions on 0-shot addition that are consistent with reliance on Fourier/modular mechanisms (absolute errors frequently multiples of 10), suggesting similar computational primitives though internal representations could not be inspected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4, PaLM-2",
            "model_description": "Large instruction-tuned Transformer models (closed-source) evaluated behaviorally (0-shot) on the same synthetic addition dataset; internal activations not available for analysis.",
            "arithmetic_task_type": "Integer addition (numbers &lt;= 260), 0-shot prompting.",
            "mechanism_or_representation": "Behavioral evidence (error modes) consistent with modular/classification failures (e.g., unit/decade errors) that align with the Fourier-feature hypothesis, though internal verification is not possible here.",
            "probing_or_intervention_method": "Behavioral error analysis (distribution of absolute errors and whether errors are multiples of 10).",
            "performance_metrics": "Absolute errors are multiples of 10: GPT-3.5 and GPT-4: 100% of errors; PaLM-2: 87% of errors (behavioral statistic reported in the paper).",
            "error_types_or_failure_modes": "Errors concentrated on multiples of 10, indicating failure modes that likely reflect poor modular (unit/decade) classification rather than magnitude approximation.",
            "evidence_for_mechanism": "Similarity of error distributions to open-source models and fine-tuned GPT-2-XL's ablation pattern motivates hypothesis that these models also use Fourier/modular strategies, though internal evidence is unavailable.",
            "counterexamples_or_challenges": "No internal representation analysis available for closed-source models; the conclusion is based on behavioral patterns only and thus remains a hypothesis rather than direct evidence.",
            "uuid": "e8142.5",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Fourier filtering ablations",
            "name_full": "Low-pass / high-pass Fourier filtering interventions on MLP and Attention outputs",
            "brief_description": "A linear-projection intervention that removes selected Fourier components from W^U-projected outputs of MLP or attention modules (solve via projection to nullspace of B F W^U) to test causal contribution of frequency bands to addition performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-2-XL (fine-tuned) and used conceptually on others",
            "model_description": "Intervention is a post-hoc linear projection applied to module outputs before they are added to the residual stream; the projection zeros specified Fourier components in the vocabulary/logit space.",
            "arithmetic_task_type": "Used to probe integer addition computation.",
            "mechanism_or_representation": "Directly manipulates representation in Fourier basis by nulling selected frequency coefficients; demonstrates causal role of low vs high frequencies in approximation vs classification.",
            "probing_or_intervention_method": "Definition A.6: define binary mask B selecting frequencies to zero; compute orthogonal projection of module output into nullspace of B F W^U; apply globally to module outputs and measure loss/accuracy changes; single-component retention experiments also used.",
            "performance_metrics": "See Table 1 intervention outcomes (validation loss/accuracy) demonstrating large, interpretable drops in accuracy when relevant bands removed (e.g., removing attention high-frequency: accuracy drops to 0.7836; removing MLP low-frequency: accuracy drops to 0.3589; removing both low-frequency from ATTN&MLP: 0.0594).",
            "error_types_or_failure_modes": "Ablations produce predictable error modes aligned with the hypothesized roles: removing MLP low-frequency -&gt; large off-by-ten-like errors; removing attention high-frequency -&gt; small unit-digit errors.",
            "evidence_for_mechanism": "Causal ablation via these filters produces specific degradations matching the hypothesized decomposition of tasks (approximation vs modular classification), supporting that the identified Fourier features are not merely correlational.",
            "counterexamples_or_challenges": "Choice of frequency threshold τ (τ=50 used) is heuristic though justified in appendix; projection is applied to W^U-space which assumes the vocabulary-number mapping and Fourier basis together capture the relevant modes—effects could vary for different vocabularies or larger numeric ranges.",
            "uuid": "e8142.6",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Harmonics of learning: Universal fourier features emerge in invariant networks",
            "rating": 2,
            "sanitized_title": "harmonics_of_learning_universal_fourier_features_emerge_in_invariant_networks"
        },
        {
            "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
            "rating": 2,
            "sanitized_title": "feature_emergence_via_margin_maximization_case_studies_in_algebraic_tasks"
        },
        {
            "paper_title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "rating": 2,
            "sanitized_title": "fourier_features_let_networks_learn_high_frequency_functions_in_low_dimensional_domains"
        },
        {
            "paper_title": "A toy model of universality: Reverse engineering how networks learn group operations",
            "rating": 1,
            "sanitized_title": "a_toy_model_of_universality_reverse_engineering_how_networks_learn_group_operations"
        },
        {
            "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 1,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        }
    ],
    "cost": 0.016135999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pre-trained Large Language Models Use Fourier Features to Compute Addition</h1>
<p>Tianyi Zhou Deqing Fu Vatsal Sharan Robin Jia<br>Department of Computer Science<br>University of Southern California<br>Los Angeles, CA 90089<br>{tzhou029, deqingfu, vsharan, robinjia}@usc.edu</p>
<h4>Abstract</h4>
<p>Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features-dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 2
2 Problem Setup ..... 3
3 Language Models Solve Addition with Fourier Features ..... 3
3.1 Behavioral Analysis ..... 3
3.2 Fourier Features in MLP \&amp; Attention Outputs ..... 4
3.3 Fourier Features are Causally Important for Model Predictions ..... 7
4 Effects of Pre-training ..... 8
4.1 Fourier features in Token Embedding ..... 8
4.2 Contrasting Pre-trained Models with Models Trained from Scratch ..... 8
4.3 Fourier Features in Prompted Pre-Trained Models ..... 10
5 Related Work ..... 11
6 Conclusion ..... 12
A Formal Definition of Transformer and Logits in Fourier Space ..... 17
B Fourier Components Separation and Selection of $\tau$ ..... 20
C Does Fourier Features Generalize? ..... 23
C. 1 Token Embedding for Other LMs ..... 23
C. 2 Multiplication Task ..... 24
C. 3 Same Results for other format ..... 25
C. 4 Fourier Features in Other Pre-trained LM ..... 26
D Supporting Evidence For the Fourier Features ..... 27
E More Experiments on GPT-2-XL Trained from Scratch ..... 28
F Details of Experimental Settings ..... 29</p>
<h1>1 Introduction</h1>
<p>Mathematical problem solving has become a crucial task for evaluating the reasoning capabilities of large language models (LLMs) [HBK ${ }^{+} 21, \mathrm{CKB}^{+} 21, \mathrm{LBX}^{+} 24, \mathrm{FKL}^{+} 24$ ]. While LLMs exhibit impressive mathematical abilities [Ope23, Goo23b, Ant24, WLS17, TPSI21, BMR ${ }^{+} 20, \mathrm{FPG}^{+} 24$ ], it remains unclear how they perform even basic mathematical tasks. Do LLMs apply mathematical principles when solving math problems, or do they merely reproduce memorized patterns from the training data?</p>
<p>In this work, we unravel how pre-trained language models solve simple mathematical problems such as "Put together 15 and 93. Answer: __". Prior work has studied how Transformers, the underlying architecture of LLMs, perform certain mathematical tasks. Most studies [Cha23, GTLV22, vONR ${ }^{+} 22, \mathrm{BCW}^{+} 23, \mathrm{FCJS} 23, \mathrm{NCL}^{+} 23, \mathrm{GLL}^{+} 24, \mathrm{PBE}^{+} 22 \mathrm{~b}$ ] focus on Transformers with a limited number of layers or those trained from scratch; [HLV23] analyzes how the pre-trained GPT-2-small performs the greater-than task. Our work focuses on a different task from prior interpretability work-integer addition-and shows that pre-trained LLMs learn distinct mechanisms from randomly initialized Transformers.</p>
<p>In $\S 3$, we show that pre-trained language models compute addition with Fourier featuresdimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. First, we analyze the behavior of pre-trained LLMs on the addition task after fine-tuning, which leads to almost perfect accuracy on the task. Rather than merely memorizing answers from the training data, the models progressively compute the final answer layer by layer. Next, we analyze the contributions of individual model components using Logit Lens [BFS ${ }^{+} 23$ ]. We observe that some components primarily approximate the answer-they promote all numbers close to the correct answer in magnitude-while other components primarily classify the answer modulo $m$ for various numbers $m$. Then, we use Fourier analysis to isolate features in the residual stream responsible for the low-frequency "approximation" and high-frequency "classification" subtasks. Identifying these features allows us to precisely ablate the ability of the model to perform either approximation or classification by applying a low-pass or high-pass filter, respectively, to the outputs of different model components. We find that MLP layers contribute primarily to approximation, whereas attention layers contribute primarily to classification.</p>
<p>In $\S 4$, we show that pre-training is crucial for learning this mechanism. The same network trained from scratch with random initialization not only shows no signs of Fourier features, but also has lower accuracy. We identify pre-trained token embeddings as a key source of inductive bias that help the pre-trained model learn a more precise mechanism for addition. Across the pretrained token embeddings of many different pre-trained models, Fourier analysis uncovers large magnitudes of components with periods 2,5 , and 10 . Introducing pre-trained token embeddings when training the model from scratch enables the model to achieve perfect test accuracy. Finally, we show that the same Fourier feature mechanism is present not only in models that were pretrained and then fine-tuned, but also in frozen pre-trained LLMs when prompted with arithmetic problems.</p>
<p>Overall, our work provides a mechanistic perspective on how pre-trained LLMs compute addition through the lens of Fourier analysis. It not only broadens the scope from only investigating few-layer Transformers trained to fit a particular data distribution to understanding LLMs as a whole, but also hints at how pre-training can lead to more precise model capabilities.</p>
<h1>2 Problem Setup</h1>
<p>Task and Dataset. We constructed a synthetic addition dataset for fine-tuning and evaluation purposes. Each example involves adding two numbers $\leq 260$, chosen because the maximum number that can be represented by a single token in the GPT-2-XL tokenizer is 520 . For each pair of numbers between 0 and 260 , we randomly sample one of five natural language question templates and combine it with the two numbers. The dataset is shuffled and then split into training ( $80 \%$ ), validation ( $10 \%$ ), and test ( $10 \%$ ) sets. More details are provided in Appendix F. In Appendix C.3, we show our that results generalize to a different dataset formatted with reverse Polish notation.</p>
<p>Model. Unless otherwise stated, all experiments focus on the pre-trained GPT-2-XL model that has been fine-tuned on our addition dataset. This model, which consists of 48 layers and approximately 1.5 billion parameters, learns the task almost perfectly, with an accuracy of $99.74 \%$ on the held-out test set. We examine other models in $\S 4.2$ and $\S 4.3$.</p>
<p>Transformers. We focus on decoder-only Transformer models [VSP ${ }^{+} 17$ ], which process text sequentially, token by token, from left to right. Each layer $\ell$ in the Transformer has an attention module with output Attn $^{(l)}$ and an MLP module with output $\mathrm{MLP}^{(l)}$. Their outputs are added together to create a continuous residual stream $h\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation accumulates all additive updates within the residual stream, with the representation $h^{(\ell)}$ in the $\ell$-th layer given by:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\operatorname{Attn}^{(\ell)}+\operatorname{MLP}^{(\ell)}
$$</p>
<p>The output embedding $W^{U}$ projects the residual stream to the space of the vocabulary; applying the softmax function then yields the model's prediction. We provide formal definitions in Appendix A.</p>
<h2>3 Language Models Solve Addition with Fourier Features</h2>
<p>In this section, we analyze the internal mechanisms of LLMs when solving addition tasks, employing a Fourier analysis framework. We first show that the model initially approximates the solution before iteratively converging to the correct answer (§3.1). We then show that the model refines its initial approximation by computing the exact answer modulo 2, 5, and 10, employing Fourier components of those same periods (§3.2). Finally, we demonstrate through targeted ablations that the identified Fourier components are causally important for the model's computational processes (§3.3). Specifically, we show that MLP layers primarily approximate the magnitude of the answer, using low-frequency features, while attention layers primarily perform modular addition using high-frequency components.</p>
<h3>3.1 Behavioral Analysis</h3>
<p>Our first goal is to understand whether the model merely memorizes and recombines pieces of information learned during training, or it performs calculations to add two numbers.</p>
<p>Extracting intermediate predictions. To elucidate how LLMs perform computations and progressively refine their outputs towards the correct answer, we extract model predictions at each layer from the residual stream. Let $L$ denote the number of layers. Using the Logit Lens method [BFS $\left.{ }^{+} 23\right]$, instead of generating predictions by computing logits $W^{U} h^{(L)}$, predictions are derived through $W^{U} h^{(\ell)}$ where $\ell \in[L]$. We compute the accuracy of the prediction using each intermediate state $h^{(\ell)}$. If the models merely retrieve and recombine pieces of information learned during</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Visualization of predictions extracted from fine-tuned GPT-2-XL at intermediate layers. Between layers 20 and 30, the model's accuracy is low, but its prediction is often within 10 of the correct answer: the model first approximates the answer, then refines it. (b) Heatmap of the logits from different MLP layers for the running example, "Put together 15 and 93. Answer: 108". The y-axis represents the subset of the number space around the correct prediction, while the x-axis represents the layer index. The 33-rd layer performs mod 2 operations (favoring even numbers), while other layers perform other modular addition operations, such as mod 10 (45-th layer). Additionally, most layers allocate more weight to numbers closer to the correct answer, 108. (c) Analogous plot for attention layers. Nearly all attention modules perform modular addition.</p>
<p>Training, certain layers will directly map this information to predictions. For instance, [MEP23] demonstrates that there is a specific MLP module directly that maps a country to its capital.</p>
<p><strong>LLMs progressively compute the final answers.</strong> Figure 1a instead shows that the model progressively approaches the correct answer, layer by layer. The model is capable of making predictions that fall within the range of ±2 and ±10 relative to the correct answer in the earlier layers, compared to the exact-match accuracy. This observation implies that the Transformer's layer-wise processing structure is beneficial for gradually refining predictions through a series of transformations and updates applied to the token representations.</p>
<h3>3.2 Fourier Features in MLP &amp; Attention Outputs</h3>
<p><strong>Logits for MLP and attention have periodic structures.</strong> We now analyze how each MLP and attention module contributes to the final prediction. We transform the output of the attention and MLP output at layer ℓ into the token space using W<sup>U</sup>Attn<sup>(ℓ)</sup> and W<sup>U</sup>MLP<sup>(ℓ)</sup> at each layer, thereby obtaining the logits L for each MLP and attention module. We use the running example "Put together 15 and 93. Answer: 108" to demonstrate how the fine-tuned GPT-2-XL performs the computation. As illustrated in Figure 1b and Figure 1c, both the MLP and attention modules exhibit a periodic pattern in their logits across the output number space, e.g., the MLP in layer 33, outlined in green, promotes all numbers that are congruent to 108 mod 2 (in Figure 19 in the appendix, we zoom into such layers to make this clearer). Overall, we observe two distinct types of computation within these components. Some components predominantly assign a high weight to numbers around the correct answer, which we term <em>approximation</em>. Meanwhile, other components predominantly assign a high weight to all numbers congruent to a + b mod c for some constant c, which we term <em>classification</em>.</p>
<p><strong>Logits for MLP and attention are approximately sparse in the Fourier space.</strong> It is natural to transform the logits into Fourier space to gain a better understanding of their properties such as the periodic pattern. We apply the discrete Fourier transform to represent the logits as the sum of sine and cosine waves of different periods: the k-th component in Fourier space has period</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The intermediate logits in Fourier space. We annotate the top-10 outlier high-frequency Fourier components based on their magnitudes. $T$ stands for the period of that Fourier component. (a) The logits in Fourier space for the MLP output of the 33-rd layer, i.e., $\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(33)}$. The component with period 2 has the largest magnitude, aligning with the observations in Figures 1b and 19a. (b) The logits in Fourier space for the attention output of the 40-th layer, i.e., $\tilde{\mathcal{L}}</em>$. The components with periods 5 and 10 have the largest magnitude, aligning with the observations in Figures 1c and 19b.
$520 / k$ and frequency $k / 520$ (see Appendix A for more details). Let $\tilde{\mathcal{L}}$ denote the logits in Fourier space. Figure 2 shows the Fourier space logits for two layers from Figure 1b and Figure 1c that have a clear periodic pattern. We find that the high-frequency components in Fourier space, which we define as components with index greater or equal to 50, are approximately sparse as depicted in Figure 2. This observation aligns with $\left[\mathrm{NCL}^{+} 23\right]$, which found that a one-layer Transformer utilizes particular Fourier components within the Fourier space to solve the modular addition task.}}^{(40)</p>
<p>In Figure 3, we show that similar sparsity patterns in Fourier space hold across the entire dataset. We compute the logits in Fourier space for the last 15 layers, i.e., $\tilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(t)}$ and $\tilde{\mathcal{L}}</em>$ where $\ell \in[32,47]$, for all test examples and average them. We annotate the top-10 outlier high-frequency components based on their magnitude. The MLPs also exhibit some strong low-frequency components; the attention modules do not exhibit strong low-frequency components, only high-frequency components.}}^{(t)</p>
<p>Final logits are superpositions of these outlier Fourier components. The final logits, $\mathcal{L}^{(L)}$, are the sum of all $\mathcal{L}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(l)}$ and $\mathcal{L}</em>$ based on their magnitudes and transfer them back to logits in number space via the inverse discrete Fourier transform (Figure 4a). The large-period (low-frequency) components approximate the magnitude while the small-period (high-frequency) components are crucial for modular addition. Figure 4b shows that aggregating these 5 waves is sufficient to predict the correct answer.}}^{(l)}$ across all layers $l \in[L]$. Figure 4 elucidates how these distinct Fourier components contribute to the final prediction, for the example "Put together 15 and 93. Answer: 108". We select the top-5 Fourier components of $\tilde{\mathcal{L}}^{(L)</p>
<p>Why is high-frequency classification helpful? The Fourier basis comprises both cos and sin waves (see Definition A.3). By adjusting the coefficients of cos and sin, the trained model can manipulate the phase of the logits in Fourier space (number shift in number space), aligning the peak</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis of logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, outlier Fourier components have periods around 2, 2.5, 5, and 10.
of the wave more closely with the correct answer. As shown in Figure 4a, consider a wave with a period of 2 . Here, the peak occurs at every even number in the number space, corresponding to the $\bmod 2$ task. In contrast, for components with a large period such as 520 , the model struggles to accurately position the peak at 108 (also see Figure 13 in the appendix for the plot of this component with period 520 in the full number space). This scenario can be interpreted as solving a "mod 520 " task-a classification task among 520 classes-which is challenging for the model to learn accurately. Nevertheless, even though the component with a period of 520 does not solve the "mod 520 " task precisely, it does succeed in assigning more weight to numbers near 108. The classification results from the high-frequency components can then provide finer-grained resolution to distinguish between all the numbers around 108 assigned a large weight by the lower frequencies. Due to this, the low-frequency components need not be perfectly aligned with the answer to make accurate predictions.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of how a sparse subset Fourier components can identify the correct answer. (a) Shows the top-5 Fourier components for the final logits. (b) Shows the sum of these top-5 Fourier components, highlighting how the cumulative effect identifies the correct answer, 108.</p>
<h1>3.3 Fourier Features are Causally Important for Model Predictions</h1>
<p>In the previous section, we demonstrated that there are outlier Fourier components in the logits generated by both the MLP and attention modules, as shown in Figure 3. We also illustrated that, in one example, the high-frequency components primarily approximate the magnitude, while the low-frequency components are crucial for modular addition tasks, as depicted in Figure 4. In this section, through an ablation study conducted across the entire test dataset, we show that both types of components are essential for correctly computing sums. Moreover, we reveal that the MLP layers primarily approximate the magnitude of the answer using low-frequency features, whereas the attention layers are responsible for modular addition using high-frequency features.</p>
<p>Filtering out Fourier components. To understand the role various frequency components play for the addition task, we introduce low-pass and high-pass filters $\mathcal{F}$. For an intermediate state $h$, and a set of frequencies $\Gamma=\left{\gamma_{1}, \ldots, \gamma_{k}\right}$, the filter $\mathcal{F}(h ; \Gamma)$ returns the vector $\widehat{h}$ that is closest in $L_{2}$ distance to $h$ subject to the constraint that the Fourier decomposition of $W^{U} \widehat{h}$ at every frequency $\gamma_{i}$ is 0 . We show in Appendix A that this has a simple closed-form solution involving a linear projection. We then apply either a low-pass filter by taking $\Gamma$ to be all the components whose frequencies are greater than the frequency of the $\tau$-th component for some threshold $\tau$ (i.e., removing high-frequency components), and a high-pass filter by taking $\Gamma$ to be all the components whose frequencies are less than the frequency of the $\tau$-th component (i.e., removing low-frequency components). As in the previous subsection, we take the high-frequency threshold $\tau=50$ for the following experiments (see Appendix B for more details).</p>
<p>Different roles of frequency components in approximation and classification tasks. We evaluated the fine-tuned GPT-2-XL model on the test dataset with different frequency filters applied to all of the output of MLP and attention modules. The results, presented in Table 1, indicate that removing low-frequency components from attention modules or high-frequency components from MLP modules does not impact performance. This observation suggests that attention modules are not crucial for approximation tasks, and MLP modules are less significant for classification tasks.</p>
<p>Eliminating high-frequency components from attention results in a noticeable decrease in accuracy. Furthermore, removing high-frequency components from both the attention and MLP modules simultaneously leads to an even greater reduction in accuracy. This finding corresponds with observations from Figure 1b,c and Figure 3, which indicate that both MLP and attention modules are involved in classification tasks due to the presence of high-frequency components in the logits. However, the approximation tasks are primarily performed by the MLP modules alone.</p>
<p>The errors induced by these ablations align with our mechanistic understanding. Ablating low-frequency parts of MLPs leads to off-by 10,50 , and 100 errors: the model fails to perform the approximation subtask, though it still accurately predicts the unit digit. Conversely, ablating highfrequency parts of attention leads to small errors less than 6 in magnitude: the model struggles to accurately predict the units digit, but it can still estimate the overall magnitude of the answer. See Figure 20 in the Appendix for more details. These observations validate our hypothesis that low-frequency components are crucial for approximation, while high-frequency components are vital for classification. The primary function of MLP modules is to approximate the magnitude of outcomes using low-frequency components, while the primary role of attention modules is to ensure accurate classification by determining the correct unit digit.</p>
<p>Table 1: Impact of Filtering out Fourier Components on Model Performance. Removing low-frequency components from attention modules (blue) or high-frequency components from MLP modules (red) does not impact performance</p>
<table>
<thead>
<tr>
<th>Module</th>
<th>Fourier Component Removed</th>
<th>Validation Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Without Filtering</td>
<td>0.0073</td>
<td>0.9974</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>Low-Frequency</td>
<td>4.0842</td>
<td>0.0594</td>
</tr>
<tr>
<td>ATTN</td>
<td>Low-Frequency</td>
<td>0.0352</td>
<td>0.9912</td>
</tr>
<tr>
<td>MLP</td>
<td>Low-Frequency</td>
<td>2.1399</td>
<td>0.3589</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>High-Frequency</td>
<td>1.8598</td>
<td>0.2708</td>
</tr>
<tr>
<td>ATTN</td>
<td>High-Frequency</td>
<td>0.5943</td>
<td>0.7836</td>
</tr>
<tr>
<td>MLP</td>
<td>High-Frequency</td>
<td>0.1213</td>
<td>0.9810</td>
</tr>
</tbody>
</table>
<h2>4 Effects of Pre-training</h2>
<p>The previous section shows that pre-trained LLMs leverage Fourier features to solve the addition problem. Now, we study where the models' reliance on Fourier features comes from. In this section, we demonstrate that LLMs learn Fourier features in the token embeddings for numbers during pre-training. These token embeddings are important for achieving high accuracy on the addition task: models trained from scratch achieve lower accuracy, but adding just the pre-trained token embeddings fixes this problem. We also show that pre-trained models leverage Fourier features not only when fine-tuned, but also when prompted.</p>
<h3>4.1 Fourier features in Token Embedding</h3>
<p>Number embedding exhibits approximate sparsity in the Fourier space. Let $W^{E} \in \mathbb{R}^{p \times D}$, where $p=521$ and $D$ is the size of the token embeddings, denote the token embedding for numbers. We apply the discrete Fourier transform to each column of $W^{E}$ to obtain a matrix $V \in \mathbb{R}^{p \times D}$, where each row represents a different Fourier component. Then we take the $L_{2}$ norm of each row to yield a $p$-dimensional vector. Each component $j$ in this vector measures the overall magnitude of the $j$-th Fourier component across all the token embedding dimensions. Figure 5a shows the magnitude of different Fourier components in the token embedding of GPT-2-XL. We see that the token embedding has outlier components whose periods are $2,2.5,5$, and 10. Therefore, similar to how the model uses different Fourier components to represent its prediction (as shown in Section 3.2), the token embeddings represent numbers with different Fourier components. Figure 14 in the Appendix shows that the token embeddings of other pre-trained models have similar patterns the Fourier space. This suggests that Fourier features are a common attribute in the token embedding of pre-trained LLMs. In Figure 5b, we use t-SNE and $k$-means to visualize the token embedding clustering. We can see that numbers cluster not only by magnitude but also by their multiples of 10.</p>
<h3>4.2 Contrasting Pre-trained Models with Models Trained from Scratch</h3>
<p>To understand the necessity of Fourier features for the addition problem, we trained the GPT-2-XL model from scratch on the addition task with random initialization. After convergence, it achieved only $94.44\%$ test accuracy (recall that the fine-tuned GPT-2-XL model achieved $99.74\%$ accuracy).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (a) Number embedding in Fourier space for fine-tuned GPT-2-XL. $T$ stands for the period of that Fourier component.(b) Visualization of token embedding clustering of GPT-2 using T-SNE and $k$-means with 10 clusters. The numbers are clustered based on their magnitude and whether they are multiples of 10 .
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of the logits in Fourier space on the test dataset from the last 15 layers for the GPT-2-XL model trained from scratch. For both the MLP and attention modules, there are no outlier Fourier components, in contrast with the clear outlier components in the fine-tuned model (Figure 3).</p>
<p>Fourier features are learned during pre-training. Figure 6 shows that there are no Fourier features in the intermediate logits of the GPT-2-XL model trained from scratch on the addition task. Furthermore, Figure 7a shows that the token embeddings also have no Fourier features. Without leveraging Fourier features, the model merely approximates the correct answer without performing modular addition, resulting in frequent off-by-one errors between the prediction and the correct answer (see details in Figure 22).</p>
<p>Pre-trained token embeddings improve model training. We also trained GPT-2-small, with 124 million parameters and 12 layers, from scratch on the addition task. GPT-2-small often struggles with mathematical tasks [MMV+22]. This model achieved a test accuracy of only $53.95 \%$</p>
<p>after convergence. However, when we freeze the token embedding layer and randomly initialize the weights for all other layers before training on the addition task, the test accuracy increases to $100 \%$, with a significantly faster convergence rate. This outcome was consistently observed across five different random seeds, as illustrated in Figure 7b. This demonstrates that given the number embeddings with Fourier features, the model can effectively learn to leverage these features to solve the addition task.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: (a) The number embedding in Fourier space for GPT-2-XL trained from scratch. There are no high-frequency outlier components, in contrast with the pre-trained embeddings (Figure 5a). (b) Validation accuracy of GPT-2-small trained from scratch either with or without pre-trained token embeddings. We show the mean and the standard deviation of the validation accuracy across 5 random seeds. GPT-2-small with pre-trained token embedding consistently achieves $100 \%$ accuracy, while GPT-2-small without pre-trained token embedding only achieves less than $60 \%$ accuracy.</p>
<h1>4.3 Fourier Features in Prompted Pre-Trained Models</h1>
<p>Finally, we ask whether larger language models use similar Fourier features during prompting.
Pre-trained LLMs use Fourier features to compute addition during in-context learning. We first test on the open-source models GPT-J [WK21] with 6B parameters, and Phi-2 [JBA ${ }^{+} 23$ ] with 2.7B parameters on the test dataset. Without in-context learning, the model cannot perform addition tasks. Therefore, we use 4 -shot in-context learning to test its performance. Their absolute errors are predominantly multiples of 10: $93 \%$ of the time for GPT-J, and $73 \%$ for Phi-2 . Using the Fourier analysis framework proposed in Section 3.2, we demonstrate that for Phi-2 and GPT-J, the outputs of MLP and attention modules exhibit approximate sparsity in Fourier space across the last 15 layers (Figure 8 and Figure 18). This evidence strongly suggests that these models leverage Fourier features to compute additions.</p>
<p>Closed-source models exhibit similar behavior. We study the closed-source models GPT-3.5 [Ope22], GPT-4 [Ope23], and PaLM-2 [Goo23a]. While we cannot analyze their internal representations, we can study whether their behavior on addition problems is consistent with reliance on Fourier features. Since closed-source LLMs are instruction tuned and perform well without incontext learning, we conduct error analysis with 0 -shot. Most absolute errors by these models are also multiples of 10: $100 \%$ of the time for GPT-3.5 and GPT-4, and $87 \%$ for PaLM-2. The similarity in error distribution to that of open-source models leads us to hypothesize that Fourier features play a critical role in their computational mechanism.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: For Phi-2 (4-shot), we analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, the outlier Fourier components have periods around 2, 2.5, 5, and 10, similar to the fine-tuned GPT-2-XL logits (Figure 3).</p>
<h1>5 Related Work</h1>
<p>Learning mathematical tasks. Previous studies primarily explore what pre-trained LMs can achieve on arithmetic tasks, with less emphasis on the underlying mechanisms [NJL21, QWL ${ }^{+} 22$ ]. For instance, [LSL $\left.{ }^{+} 23\right]$ demonstrates that small Transformer models can effectively learn arithmetic by altering the question format and utilizing a scratchpad method [NAGA $\left.{ }^{+} 21\right]$. [HLV23] identifies activation patterns for the "greater-than" operation in GPT-2, and [Cha23] focuses on the enumeration and selection processes in GCD computation. In this paper, we dive into the specific roles of MLP and attention layers in solving mathematical tasks. Our research analyzes these components' distinct contributions to integer addition tasks.</p>
<p>Mechanisms of pre-trained LMs. Recent studies have significantly advanced our understanding of the underlying mechanisms of pre-trained Transformer models. For instance, research on "skill neurons" by [WWZ $\left.{ }^{+} 22\right]$ and "knowledge neurons" by [DDH $\left.{ }^{+} 21\right]$ underscores the development of specialized neural components that encode task-specific capabilities or hold explicit factual information in the pre-trained LMs, enhancing model performance on related tasks. [MEP23] and [GCWG22] discuss how MLPs and FFNs transform and update token representations for general language tasks. In contrast, we show that the pre-trained LMs use multiple layers to compute addition by combining the results of approximation and classification. Additionally, [ZL23] demonstrated the capacity of GPT-2 to consolidate similar information through pre-training in the model weights, which aligns with our observations on the importance of pre-training in developing effective number embedding and arithmetic computation strategies in LMs.</p>
<p>Fourier features in Neural Networks. Fourier features are commonly observed in image models, particularly in the early layers of vision models [OF97, OCS ${ }^{+} 20$, FS24]. These features enable the model to detect edges, textures, and other spatial patterns effectively. Recently, Fourier features have been noted in networks trained for tasks that allow cyclic wraparound, such as modular addition $\left[\mathrm{NCL}^{+} 23, \mathrm{MEO}^{+} 23\right]$, general group compositions [CCN23], or invariance to cyclic translations [SSOH22]. [NCL $\left.{ }^{+} 23\right]$ demonstrates that learning Fourier features can induce 'grokking' $\left[\mathrm{PBE}^{+} 22 \mathrm{a}\right]$. Furthermore, [MHKS23] provides a mathematical framework explaining the emergence of Fourier features when the network exhibits invariance to a finite group. We extend these</p>
<p>insights by observing Fourier features in tasks that do not involve cyclic wraparound. [TSM ${ }^{+20}$ ] found that by selecting problem-specific Fourier features, the performance of MLPs can be improved on a computer vision-related task.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we provide a comprehensive analysis of how pre-trained LLMs compute numerical sums, revealing a nuanced interplay of Fourier features within their architecture. Our findings demonstrate that LLMs do not simply memorize answers from training data but actively compute solutions through a combination of approximation and classification processes encoded in the frequency domain of their hidden states. Specifically, MLP layers contribute to approximating the magnitude of sums, while attention layers contribute to modular operations.</p>
<p>Our work also shows that pre-training plays a critical role in equipping LLMs with the Fourier features necessary for executing arithmetic operations. Models trained from scratch lack these crucial features and achieve lower accuracy; introducing pre-trained token embeddings greatly improves their convergence rate and accuracy. This insight into the arithmetic problem-solving capabilities of LLMs through Fourier features sets the stage for potential modifications to training approaches. By imposing specific constraints on model training, we could further enhance the ability of LLMs to learn and leverage these Fourier features, thereby improving their performance in mathematical tasks.</p>
<h2>Acknowledgments</h2>
<p>DF and RJ were supported by a Google Research Scholar Award. RJ was also supported by an Open Philanthropy research grant. VS was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the funding agencies.</p>
<h1>References</h1>
<p>[Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.
[BCW ${ }^{+}$23] Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. ArXiv, abs/2306.04637, 2023.
[BFS ${ }^{+}$23] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.
[BMR ${ }^{+}$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[CCN23] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. In International Conference on Machine Learning, pages 6243-6267. PMLR, 2023.
[Cha23] François Charton. Can transformers learn the greatest common divisor? arXiv preprint arXiv:2308.15594, 2023.
[CKB ${ }^{+}$21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
$\left[\mathrm{DDH}^{+}\right.$21] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.
$\left[\mathrm{ENO}^{+}\right.$21] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.
[FCJS23] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higherorder optimization methods for in-context learning: A study with linear models, 2023.
[FKL ${ }^{+}$24] Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations, 2024.
[FPG ${ }^{+}$24] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>[FS24] Pierre-Étienne Fiquet and Eero Simoncelli. A polar prediction model for learning to represent visual transformations. Advances in Neural Information Processing Systems, 36, 2024.
[GCWG22] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feedforward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022.
[GLL ${ }^{+}$24] Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic, 2024.
[Goo23a] Google. Palm 2 technical report, 2023.
[Goo23b] Gemini Team Google. Gemini: A family of highly capable multimodal models, 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022.
$\left[\mathrm{HBK}^{+}\right.$21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.
[HLV23] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. arXiv preprint arXiv:2305.00586, 2023.
$\left[\mathrm{JBA}^{+}\right.$23] Mojan Javaheripi, Sebastien Bubeck, Marah Abdin, Jyoti Anejaand Caio Cesar Teodoro Mendes, Allie Del Giorno Weizhu Chen, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi L, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models, 2023.
[LBX ${ }^{+}$24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024.
[LSL ${ }^{+}$23] Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.
[MEO ${ }^{+}$23] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham Kakade. Feature emergence via margin maximization: case studies in algebraic tasks. arXiv preprint arXiv:2311.07568, 2023.
[MEP23] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.
[MHKS23] Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, and Sophia Sanborn. Harmonics of learning: Universal fourier features emerge in invariant networks. arXiv preprint arXiv:2312.08550, 2023.</p>
<p>[MMV ${ }^{+}$22] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660, 2022.
[NAGA ${ }^{+}$21] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[NCL ${ }^{+}$23] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
[NJL21] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.
[OCS ${ }^{+}$20] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionv1. Distill, 5(4):e00024-002, 2020.
[OF97] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311-3325, 1997.
[Ope22] OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2023-09-10.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
$\left[\mathrm{PBE}^{+}\right.$22a] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[PBE ${ }^{+}$22b] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.
[QWL ${ }^{+}$22] Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.
[SSOH22] Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral neural networks. arXiv preprint arXiv:2209.03416, 2022.
[TPSI21] Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. Representing numbers in nlp: a survey and a vision. arXiv preprint arXiv:2103.13136, 2021.
[TSM ${ }^{+}$20] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537-7547, 2020.
[vONR ${ }^{+}$22] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn incontext by gradient descent. In International Conference on Machine Learning, 2022.</p>
<p>[VSP ${ }^{+}$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.
[WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.
[WLS17] Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845-854, 2017.
[WWZ ${ }^{+}$22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons in pre-trained transformer-based language models. arXiv preprint arXiv:2211.07349, 2022.
[ZL23] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023.</p>
<h1>Appendix</h1>
<p>Roadmap. In Appendix A, we introduce some formal definitions that used in our main content. In Appendix B, we show why we separate the Fourier components into the high-frequency part and the low-frequency part and why we choose $\tau$ to be 50 . In Appendix C, we show our observation generalizes to another format of dataset, another arithmetic task and other models. In Appendix D, we provide more evidence that shows the Fourier features in the model when computing addition. In Appendix E, we provide more evidence that shows the GPT-2-XL trained from scratch does not use Fourier feature to solve the addition task. In Appendix F, we give the details of our experimental settings.</p>
<h2>A Formal Definition of Transformer and Logits in Fourier Space</h2>
<p>We first introduce the formal definition of the Transformer structure that we used in this paper.
Definition A. 1 (Transformer). An autoregressive Transformer language model $G: \mathcal{X} \rightarrow \mathcal{Y}$ over vocabulary Vocab maps a token sequence $x=\left[x_{1}, \ldots, x_{N}\right] \in \mathcal{X}, x_{t} \in$ Vocab to a probability distribution $y \in \mathcal{Y} \subset \mathbb{R}^{|\text {Vocab }|}$ that predicts next-token continuations of $x$. Within the Transformer, the $i$-th token is embedded as a series of hidden state vectors $h_{t}^{(\ell)}$, beginning with $h_{t}^{(0)}=\mathrm{emb}\left(x_{t}\right)+\operatorname{pos}(i) \in \mathbb{R}^{D}$. Let $W^{U} \in \mathbb{R}^{|\operatorname{Vocab}| \times D}$ denote the output embedding. The final output $y=\operatorname{softmax}\left(W^{U}\left(h_{N}^{(L)}\right)\right)$ is read from the last hidden state. In the autoregressive case, tokens only draw information from past tokens:</p>
<p>$$
h_{t}^{(\ell)}=h_{t}^{(\ell-1)}+\operatorname{Attn}<em t="t">{t}^{(\ell)}+\operatorname{MLP}</em>
$$}^{(\ell)</p>
<p>where</p>
<p>$$
\operatorname{Attn}<em 1="1">{t}^{(\ell)}:=\operatorname{Attn}^{(\ell)}\left(h</em>}^{(\ell-1)}, h_{2}^{(\ell-1)}, \ldots, h_{t}^{(\ell-1)}\right) \quad \text { and } \quad \operatorname{MLP<em t="t">{t}^{(\ell)}:=\operatorname{MLP}</em>}^{(\ell)}\left(\operatorname{Attn<em t="t">{t}^{(\ell)}, h</em>\right)
$$}^{(\ell-1)</p>
<p>In this paper, we only consider the output tokens to be numbers. Hence, we have the unembedding matrix $W^{U} \in \mathbb{R}^{p \times D}$, where $p$ is the size of the number space. As we are given the length- $N$ input sequences and predict the $(N+1)$-th, we only consider $h_{N}^{(\ell)}=h_{N}^{(\ell-1)}+\operatorname{Attn}<em N="N">{N}^{(\ell)}+\operatorname{MLP}</em>$. For simplicity, we ignore the subscript $N$ in the following paper, so we get Eq. (1).}^{(\ell)</p>
<p>Definition A. 2 (Intermediate Logits). Let $\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\text {Attn }}^{(\ell)}:=W^{U} \operatorname{Attn}^{(\ell)}$ denote the intermediate logits of the attention module at the $\ell$-th layer. Let $\mathcal{L}</em>$.}}^{(\ell)}:=W^{U} \mathrm{MLP}^{(\ell)}$ denote the intermediate logits of the MLP module at the $\ell$-th layer. Let $\mathcal{L}^{(\ell)}:=W^{U} h^{(\ell)}$ denote the logits on intermediate state $h^{(\ell)</p>
<p>Throughout the model, $h$ undergoes only additive updates (Eq. (1)), creating a continuous residual stream $\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation $h$ accumulates all additive updates within the residual stream up to layer $t$.</p>
<p>To analyze the logits in Fourier space, we give the formal definition of the Fourier basis as follows:</p>
<p>Definition A. 3 (Fourier Basis). Let $p$ denote the size of the number space. Let $\overrightarrow{\mathrm{x}}:=(0,1, \ldots,(p-1))$.</p>
<p>Let $\omega_{k}:=\frac{2 \pi k}{p-1}$. We denote the normalized Fourier basis $F$ as the $p \times p$ matrix:</p>
<p>$$
F:=\left[\begin{array}{c}
\sqrt{\frac{1}{p-1} \cdot \overrightarrow{\mathbf{1}}} \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{2} \overrightarrow{\mathbf{x}}\right) \
\vdots \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{(p-1) / 2} \overrightarrow{\mathbf{x}}\right)
\end{array}\right] \in \mathbb{R}^{p \times p}
$$</p>
<p>The first component $F[0]$ is defined as a constant component. For $i \in[0, p-1], F[i]$ is defined as the $k$-th component in Fourier space, where $k=\left\lfloor\frac{i+1}{2}\right\rfloor$. The frequency of the $k$-th component is $f_{k}:=\frac{k}{p-1}$. The period of the $k$-th component is $T_{k}:=\frac{p-1}{k}$</p>
<p>We can compute the discrete Fourier transform under that Fourier basis as follows:
Remark A. 4 (Discrete Fourier transformer (DFT) and inverse DFT). We can transform any logits $u \in \mathbb{R}^{p}$ to Fourier space by computing $\widehat{u}=F \cdot u$. We can transform $\widehat{u}$ back to $u$ by $u=F^{\top} \cdot \widehat{u}$</p>
<p>Next, we define the logits in Fourier space.
Definition A. 5 (Logits in Fourier Space). Let $\mathcal{L}^{(L)}, \mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\mathcal{L}</em>$. The logits of the MLP and attention modules in Fourier space are defined as:}}^{(\ell)}$ denote the logits (Definition A.2). The output logits before softmax in Fourier space is defined as: $\widetilde{\mathcal{L}}^{(L)}=F \cdot \mathcal{L}^{(L)</p>
<p>$$
\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{Attn}}^{(\ell)}=F \cdot \mathcal{L}</em>}}^{(\ell)} \quad \text { and } \quad \tilde{\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{MLP}}^{(\ell)}=F \cdot \mathcal{L}</em>
$$}}^{(\ell)</p>
<p>We ignore the first elements in $\widetilde{\mathcal{L}}^{(L)}, \widetilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\widetilde{\mathcal{L}}</em>$ for the Fourier analysis in this paper as they are the constant terms. Adding a constant to the logits will not change the prediction.}}^{(\ell)</p>
<p>Let $\tau \in \mathbb{R}$ denote a constant threshold. The low-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[1: 2 \tau]$. The high-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[2 \tau:]$. For the following analysis, we choose $\tau=50$ (the specific choice of $\tau=50$ is explained in Appendix B).</p>
<p>Next, we propose the formal definition of low-pass/high-pass filter that is used in the following ablation study.</p>
<p>Definition A. 6 (Loss-pass / High-pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\tau \in R$ denote the frequency threshold. Let $W^{U} \in R^{p \times D}$ denote the output embedding. For low-pass filter, we define a diagonal binary matrix $B \in$ ${0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } i \geq \tau \ 0 &amp; \text { otherwise }\end{cases}$. For high-pass filter, we define a diagonal binary matrix $B \in{0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } 1 \leq i&lt;\tau \ 0 &amp; \text { otherwise }\end{cases}$. Note that we retain the constant component, so $b_{i, i}=0$. The output of the filter $\mathcal{F}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined by the following objective function:</p>
<p>$$
\begin{array}{cl}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{array}
$$}^{2</p>
<p>The solution to the above optimization problem is given by a linear projection.
Remark A.7. The result of the optimization problem defined in Definition A. 6 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>B Fourier Components Separation and Selection of $\tau$
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: We analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules. We only plot the first 50 Fourier components (a) The MLP exhibits some outlier low-frequency Fourier components. (b) The attention module's lowfrequency Fourier components are not as obvious as the ones in MLP.</p>
<p>Following Definition A.6, we define single-pass filter as follows:
Definition B. 1 (Single-Pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\gamma \in R$ denote the $\gamma$-th Fourier component (Definition A.3) that we want to retain. Let $W^{U} \in R^{V \times D}$ denote the output embedding. We define a diagonal binary matrix $B \in{0,1}^{V \times V}$ as $b_{i i}= \begin{cases}0 &amp; \text { if }\left\lfloor\frac{i+1}{2}\right\rfloor=\gamma \text { or } i=0, \ 1 &amp; \text { otherwise. }\end{cases}$
The output of the filter $\mathcal{F}_{\gamma}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined as the following objective function:</p>
<p>$$
\begin{aligned}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{aligned}
$$}^{2</p>
<p>Remark B.2. The result of the optimization problem defined in Definition B. 1 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}_{\gamma}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>For the single-pass filter, we only retrain one Fourier component and analyze how this component affects the model's prediction. The residual stream is then updated as follows:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\mathcal{F}<em _gamma="\gamma">{\gamma}\left(\operatorname{Attn}^{(\ell-1)}\right)+\mathcal{F}</em>\right)
$$}\left(\operatorname{MLP}^{(\ell-1)</p>
<p>We evaluated the fine-tuned GPT-2-XL model on the addition dataset with the Fourier components period 520 and 2. Given that $T_{k}:=\frac{V-1}{k}$ (Definition A.3), we retained only the Fourier components with $\gamma=1$ and 260 , respectively.</p>            </div>
        </div>

    </div>
</body>
</html>
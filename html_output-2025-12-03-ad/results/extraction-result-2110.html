<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2110 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2110</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2110</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-279075076</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.01372v2.pdf" target="_blank">AI Scientists Fail Without Strong Implementation Capability</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2110.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2110.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-impact automated protein structure prediction system that produced experimentally validated 3D protein structures, cited as a benchmark success for AI in science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DeepMind's deep learning system for predicting protein 3D structure from sequence; uses neural-network-based models trained on known structures and multiple-sequence alignments to predict atomic coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Structural biology / bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation performed by comparison of predicted structures to experimentally-determined protein structures (X-ray crystallography, cryo-EM, NMR) and community blind tests (CASP); quantitative comparison of predicted vs. experimental coordinates (RMSD / GDT scores).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper and this position paper cite AlphaFold as meeting domain norms: experimental structural comparison is considered sufficient and required in structural biology to establish accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>High; AlphaFold demonstrates near-experimental accuracy on many targets (original AlphaFold reported atomic-level accuracy on many CASP targets); the position paper references AlphaFold as an example of automated tool achieving ground-truth parity but does not restate numeric error here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Predictions compared to deposited experimental structures and CASP benchmarks; validated across large protein sets. (Position paper cites AlphaFold as exemplar; details from AlphaFold paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared computational predictions to experimental ground truth (structural biology gold standard); experimental validation accepted as gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed in detail in this paper beyond general acknowledgement that domain-specific experimental validation underpins AlphaFold's credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as successful automated scientific tool that achieved results previously taking years; used as contrast to AI Scientist systems that lack comparable validated breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — predictions compared to experimentally-determined structures (gold standard).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>AlphaFold results were widely reproduced and adopted by the community; the position paper references AlphaFold as reproducible validated success.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Experimental validation in structural biology is time- and resource-intensive compared to purely computational checks; position paper uses AlphaFold as an example of a validated automated tool (cost/time not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Wet-lab / experimental structural comparison required; community uses CASP and RMSD/GDT metrics as standards.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>AlphaFold provides per-residue confidence metrics (pLDDT) in its original work; the position paper references AlphaFold qualitatively but does not re-report those metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Position paper notes such domain-specific tools still require human involvement despite strong validation; specifics of AlphaFold limitations are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2110.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-Lab (Autonomous laboratory reported by Szymanski et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous laboratory system that synthesized 41 novel inorganic materials in 17 days, demonstrating experimental closure of design-to-synthesis loops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>A-Lab (autonomous laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Autonomous experimental platform integrating planning, robotic synthesis, and characterization to perform materials discovery and synthesis with minimal human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / experimental chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Physical synthesis and experimental characterization (likely XRD, microscopy, spectroscopy) of novel inorganic materials discovered by the autonomous platform; validation by demonstrating actual synthesized compounds and measured properties.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Experimental synthesis and characterization are domain-appropriate and sufficient for materials discovery; the paper cites A-Lab as an example of automated tool whose results were experimentally verified.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Success demonstrated by synthesis of 41 novel materials; position paper reports this as empirical success without numeric error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Robotic synthesis and subsequent experimental characterization confirmed the existence and properties of synthesized materials (details referenced from Szymanski et al. 2023). Position paper cites the result qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared autonomous experimental outcomes to standards of materials characterization; direct experimental verification used rather than simulation-only validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed in this paper; used as an example of a validated automated scientific tool.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successful autonomous syntheses over a short timescale used to illustrate what validated automation can achieve.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — measured, empirical characterization acts as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Position paper does not detail independent replication but cites A-Lab as a reported success in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Experimental validation requires laboratory resources and time; A-Lab achieved rapid turnaround (17 days) which the paper cites as notable although overall resource use is not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Wet-lab experimental synthesis and characterization required for claims of new materials; empirical measurements are the standard.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed in this position paper for A-Lab.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Position paper notes that such systems still require human oversight and domain resources; implies lab automation is resource-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2110.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperBench (Starace et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that asks LLM agents to replicate entire machine-learning papers by developing codebases and running experiments, measuring execution and result reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating ai's ability to replicate ai research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark and evaluation suite for LLM-driven agents to reproduce ML research papers end-to-end, with rubricized sub-tasks including code development, execution, and result match.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / machine learning reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation consists of generating code, attempting to execute experiments, and quantitatively matching reported results from original papers; rubric includes leaf nodes: 'Execution' (run code) and 'Result Match' (match reported metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>computational reproduction (empirical, depends on execution environment fidelity to original experiments); fidelity limited by available compute, environment differences and missing details in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Designed to approximate domain norms for reproducibility in ML (execution and metric matching are expected); computational reproduction is necessary for claims in ML but may not fully capture all experimental conditions (hyperparameters, random seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported poor success: e.g., Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match' indicating very low end-to-end reproduction accuracy for LLM agents in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab experiments; validation is computational via code execution. PaperBench measures whether agents can run and get matching results; the position paper reports very low success rates and highlights environment/setup gaps as causes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compares code-generation ability vs. later verification stages (execution, result match) and shows a sharp drop-off; explicit numeric comparisons included (43.4% code-development success vs ~1–2% execution/result-match for some agents).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Major failure mode: generated code often fails to run or produce matching results; execution and result reproduction almost uniformly poor (very low percentages reported).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Agents can sometimes generate partial code components (code-development subtask success ~43.4% for o1-High) but rarely complete verifiable reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — the 'ground truth' are the reported results/metrics in original papers; agents mostly fail to match these.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>PaperBench explicitly evaluates reproducibility; results show current agents largely cannot replicate published ML experiments within the benchmark constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computationally expensive due to need to run experiments; position paper highlights high sampling/training time for full end-to-end tasks (estimates of many hours).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In ML, execution and quantitative metric matching are normative standards for reproducibility; PaperBench operationalizes those norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmark reports success rates and percentages but not per-run uncertainty measures beyond pass/fail metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations include environmental mismatch, missing implementation details in papers, hyperparameter sensitivity, and resource constraints preventing faithful reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2110.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReplicate-Bench (Xiang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark testing LLM agents' ability to generate Python code to reproduce algorithms from NLP research papers; measures execution accuracy of generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation benchmark where agents read algorithm descriptions in papers and produce runnable Python implementations; judged by whether produced code passes functional tests and reproduces algorithmic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / computer science reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation performed by executing generated Python code and checking functional correctness against test cases or expected outputs; measures execution accuracy (percentage of tasks with runnable, correct code).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>empirical code execution; fidelity limited by quality of test harnesses and the degree to which test cases capture algorithm correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient as a computational reproducibility proxy in CS/ML domains where algorithmic reproduction is core; lacks domain experimental (wet-lab) aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported top agent achieved only 39% execution accuracy, indicating majority of generated implementations failed functional tests.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No real-world physical experiments; validation is purely computational; the paper highlights execution failures and debugging deficits as key limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Shows discrepancy between reasoning/algorithm understanding and ability to produce runnable code (high reasoning graph accuracy vs low execution accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Primary failures: generated code often fails to compile/execute or does not satisfy functional tests; debugging and iterative fixes are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Some agents produce correct algorithmic implementations for a minority (~39%) of tasks; success associated with simpler, well-specified algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — expected outputs or reference implementations used as ground truth for functional tests.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Benchmark directly assesses reproducibility; results indicate poor reproducibility by LLM agents for many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational cost modest per task but scales with number of attempts and need for debugging; position paper notes asynchronous code execution and experiment cycles increase sampling time dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In CS/ML, runnable code and matching results are norms for reproducibility; SciReplicate operationalizes these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reports execution accuracy percentages; no detailed probabilistic uncertainty intervals discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limited by the comprehensiveness of test cases, variability in computing environments, and absence of human debugging; agents lack robust iterative verification.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2110.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-Bench (Siegel et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that requires agents to reproduce research results and then answer questions based on those outputs, assessing end-to-end computational experiment verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark for computational reproducibility across domains (CS, social science, medicine) where agents must reproduce experiments and reason about results; evaluates multi-stage verification ability.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational reproducibility across multiple scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Agents run computational experiments, reproduce outputs, and then must answer analytical questions based on reproduced results; success judged on reproduction fidelity and subsequent reasoning correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical computational reproduction; fidelity limited by availability of original code/data and computational resources; medium fidelity dependent on benchmark task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Designed to approximate domain standards for computational reproducibility and result interpretation; sufficient for computational work but not for wet-lab domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Example: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium, indicating moderate success but imperfect verification capability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments; validation is computational and focuses on reproducing published computational results and reasoning about them.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Highlights trade-offs between reproduction success and downstream reasoning about results; shows agents can reproduce some outputs but struggle with full analytical verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Agents often fail to fully reproduce experiments or to correctly interpret outputs; success rates imperfect and indicate gaps in end-to-end verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Partial successes at medium difficulty levels; some agents reproduce results sufficiently to answer questions correctly about outputs in a subset of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — compared to reported paper outputs and reference results.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>CORE-Bench is explicitly about reproducibility; results show partial replicability by current agents.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computationally intensive for larger experiments; position paper stresses asynchronous experiment cycles slow down RL sampling and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For computational sciences, reproducing experiments and matching reported metrics is normative; CORE-Bench enforces these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmarks report success rates; detailed uncertainty measures are not described in the position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Constrained by missing details in papers, environment differences, and agents' limited debugging and iterative refinement skills.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2110.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLE-Bench (Chan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark evaluating ML-engineering tasks performed by agents across ML development workflows, measuring success rates for deploying ML models and associated verification steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mle-bench: Evaluating machine learning agents on machine learning engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark suite for ML engineering tasks (data processing, model training, debugging, deployment) used to assess LLM agents' end-to-end engineering and verification capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning engineering</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is computational: agents must complete ML workflow tasks and produce functioning artifacts (trained models, code) that pass evaluation; success measured by task-specific success rates and quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical ML workflow execution; fidelity depends on training data, compute availability, and environmental parity with original tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient to test engineering-level verification in ML; domain norms expect functioning, evaluated models and reproducible metrics as proof.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported low performance for agents on verification steps; e.g., 20% of o1 preview runs on MLE-Bench failed debugging step in cited evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab experiments; validation via successful completion and correctness of ML workflows and model performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Shows gap between code generation and achieving validated model performance; benchmark reports near-zero on 'Model Performance' for some agents in ML-Dev-Bench context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Frequent failures in debugging, producing valid submissions, and optimizing model performance; indicates weak verification loops.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Limited success on specific sub-tasks (code components), but overall low success for full ML engineering validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — models and outputs compared against expected performance metrics or baseline results.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Benchmark examines reproducibility of ML engineering tasks; agents struggle to replicate full workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>ML experiments are compute-intensive and time-consuming, causing large sampling times; paper notes RL training for scientist agents would be orders of magnitude larger.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In ML, validated model performance on standardized datasets and reproducible training runs are normative.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Benchmarks report success/failure rates; little discussion of probabilistic uncertainty measures in the position paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limited agent debugging, environmental mismatch, and resource constraints; inability to iteratively refine models effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2110.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ML-Dev-Bench (Padigela et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark for agent performance on ML development workflows; evaluates agents' ability to complete ML research and engineering tasks and to validate model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark focusing on ML development tasks such as model training, performance optimization, and validation, measuring task success rates and model performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning engineering / research</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation carried out by executing ML experiments and evaluating model performance against predefined metrics; includes 'Model Performance' as a key evaluation leaf node.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical but constrained by available compute and task specification fidelity; fidelity varies with task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>In ML, computational validation is standard; the benchmark implements domain norms but highlights that agents score poorly on model performance validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported that all tested agents scored 0% on 'Model Performance' tasks in ML-Dev-Bench in one cited evaluation, indicating failure to reach expected model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Computational-only validation; the paper emphasizes agents' inability to optimize model performance and perform iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Shows agents may generate code but fail to get validated performance; comparison underscores verification bottleneck between code generation and validated model outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Complete failure on model performance validation for tested agents; inability to tune, debug, and iterate to acceptable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>No notable successes reported for model-performance tasks in cited evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — evaluated against baseline expected performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Benchmark exposes reproducibility and performance replication challenges for agent-generated ML experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>High computational cost to run full training/validation cycles; position paper notes such cost contributes to training difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Validated model metrics on held-out datasets required; benchmark follows these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Metrics reported as pass/fail or percent success; uncertainty measures not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Resource constraints, missing hyperparameter/tuning, and debugging shortcomings limit agents' validation success.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2110.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveCodeBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveCodeBench (Jain et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A coding benchmark collecting problems from competitive programming contests that evaluates code generation, execution, self-repair, and output prediction by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Livecodebench: Holistic and contamination free evaluation of large language models for code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LiveCodeBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Benchmark that evaluates language models on real-world coding problems including generating correct code, executing it, repairing failures, and predicting outputs; focuses on multi-stage coding verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Software engineering / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation performed by executing generated code against test cases (pass@1 metrics) and evaluating ability to self-repair and predict outputs; reports pass@1 and other standard code-generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity execution of code in sandboxed environments; however, problems are algorithmic (not domain experimental) so fidelity relates to correct program semantics rather than physical realism.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient for assessing code-generation correctness in algorithmic tasks; not intended for scientific experimental validation but used to assess components of AI Scientist implementation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Example: o4-mini achieved 52.1% pass@1 on code generation subtask per the position paper, indicating moderate performance on algorithmic code generation but far from perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Computational execution only; benchmark emphasizes debugging and self-repair as verification behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Shows drop-offs in performance when moving from code generation to execution and self-repair; used to illustrate limitations in implementation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Models that generate plausible code still fail many test cases and struggle with repair iterations in more complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Some models achieve reasonable pass@1 on code generation subtask but struggle on end-to-end, multi-file, long-horizon engineering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Yes — expected outputs from contest problems serve as ground truth for correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Benchmarks responses are reproducible within environment; position paper uses results to indicate generalization gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Code execution tests are relatively cheap per instance but scale with number of attempts; the paper emphasizes that real research implementation cycles (writing, debugging, running experiments) are much more time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For algorithmic problems, passing official testcases is standard; LiveCodeBench implements these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reports pass@1 percentages; no probabilistic uncertainty beyond these metrics discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Algorithmic benchmarking does not capture full complexity of multi-file, dependency-rich research codebases; limited for assessing experimental reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2110.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer-14B (Zhu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art LLM-based review model used in this paper to perform simulated peer review of 28 AI-generated research papers, assessing implementation-level reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepreview: Improving llm-based paper review with human-like deep thinking process</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A 14B-parameter review model trained/engineered to evaluate scientific manuscripts across dimensions like soundness, presentation, contribution; used here as an automated judge for AI-generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Meta-science / peer review automation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation of manuscripts performed via LLM-based rubriced scoring (soundness, presentation, contribution, decision, rating); compares outputs to review norms rather than performing physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A to physical experiments; fidelity relates to alignment with human peer review practices—approximate and subject to peer-review variability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper cautions that LLM-based peer review can filter low-quality papers but is imperfect for predicting long-term impact; simulated review is a results-oriented but partial validation proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Provides aggregate review scores for evaluated AI-generated papers (e.g., average soundness, presentation metrics in Table 2) but does not claim absolute accuracy vs human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab experiments; validation is meta-evaluation of papers. The paper used DeepReviewer-14B to find experimental weakness in all 28 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared AI-generated paper scores across different AI Scientist systems; used to quantify implementation and experimental weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Limitations: peer review (even automated) may fail to identify long-term impact; the paper emphasizes selection bias and peer review unreliability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>DeepReviewer identified widespread 'Experimental Weakness' (100% occurrence) across evaluated papers, successfully highlighting implementation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No direct ground-truth; comparison is to expectations and rubric standards rather than independent experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Simulated review is reproducible in the sense the model produces consistent rubric scores, but human peer review variability remains a caveat.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Automated review is faster and cheaper than human review; the paper suggests automated tools could reduce reviewer burden but warns of misuse risks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Peer-review norms serve as partial domain standard for assessing scientific claims, but position paper stresses peer review cannot fully substitute for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>The model reports scores and percentiles; paper notes these are imperfect proxies and subject to limitations in predicting impact.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Automated reviewing can miss high-impact but initially under-appreciated work and cannot replace experimental replication; selection bias in available AI-generated papers may skew evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Paper advocates hybrid mechanisms combining automated review tools (like DeepReviewer) with human-in-the-loop review for continuous oversight and ethical evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2110.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluated AI Scientist Systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five AI Scientist systems evaluated (HKUSD AI Researcher, AI Scientist, AI Scientist v2, CycleResearcher-12B, Zochi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of AI Scientist systems whose publicly-available generated papers (28 total) were evaluated by DeepReviewer-14B, showing pervasive experimental and methodological weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HKUSD AI Researcher; AI Scientist; AI Scientist v2; CycleResearcher-12B; Zochi</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-based AI Scientist systems that attempt end-to-end research workflows (idea generation through implementation); capabilities vary but generally produce written papers and generated code/artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / AI research (primarily ML/NLP/CS domains represented in generated papers)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Most generated papers lacked rigorous experimental validation; validation was often absent or insufficiently described in the manuscripts, with few performing reproducible computational experiments and virtually no wet-lab experiments (when not applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Position paper argues validation is insufficient — domain norms require runnable code, executed experiments, and reproducible results; evaluated AI Scientist outputs mostly failed to meet these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Aggregate evaluation showed pervasive failures: Table 3 lists 'Experimental Weakness' in 100% of evaluated papers; other defects (methodological flaws, reproducibility issues) occurred at high rates (70%+).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>For the majority of evaluated AI-generated papers, no real experimental validation was performed or reported; the paper explicitly cites lack of execution and experimental rigor as the core implementation gap and limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Comparison across systems using DeepReviewer scores (Table 2) shows low ratings and percentiles; AI Scientist systems sometimes generate plausible writing but fail implementation/verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Systemic failures: absence or poor quality of experimental design, inability to run or reproduce experiments, methodological ambiguities, missing hyperparameter analysis and reproducibility details — collectively flagged across evaluated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Rare partial successes: some systems generated plausible idea-level content and code fragments, and a minority of papers had better presentation or specific components, but none demonstrated consistently rigorous validated experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Generally absent — generated papers rarely provided reproducible comparisons to established baselines or ground truth results, limiting verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Position paper reports reproducibility issues as a frequent defect (71.4%), and that independent replication was lacking for AI-generated works.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper emphasizes that proper experimental validation is resource- and time-intensive (sampling time estimates in Appendix A) and cites this as a barrier to AI systems performing full validation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In CS/ML, norms include runnable code, experimental logs, hyperparameter disclosure, and metric matching; AI-generated papers frequently violated these norms according to the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Most generated papers lacked proper uncertainty quantification or confidence measures; paper flags this as part of experimental weakness.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Identified limitations include agents' weak debugging, limited environment/tool coordination, insufficient iterative verification loops, and lack of resource access to perform experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2110.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2110.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge (simulated peer-review methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology used in this paper where an LLM review model (DeepReviewer-14B) simulates peer review to assess AI-generated papers' implementation and validation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-as-a-Judge (simulated peer review)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Method that leverages a trained large language model to produce rubriced reviews of scientific manuscripts, scoring dimensions like soundness, experimental rigor, presentation, and contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Meta-science / scientific evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation of manuscripts via automated scoring against a rubric; identifies experimental weaknesses and methodological flaws without independent experimental reproduction. Used to approximate peer review throughput and to flag low-quality submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Proxy for human peer review; fidelity limited by model alignment with human reviewers and inability to perform laboratory or compute experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper states this approach can filter low-quality papers but is insufficient to certify high-impact or experimentally validated work; peer review is results-oriented and limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Provides aggregate rubric scores (Table 2) but paper cautions about reliability; cites literature showing peer review is not a strong predictor of long-term impact.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments; the method flags absence of validation in manuscripts and quantifies prevalence of defects but cannot substitute for reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared automated review scores across AI Scientist systems and correlated defects; paper contrasts simulated review outputs with known limits of peer review reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Method cannot detect long-term impact or reliably confirm correctness that requires independent experimental reproduction; selected-publication bias may affect assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successfully identified pervasive experimental weaknesses across 28 evaluated AI-generated papers (100% had experimental weaknesses).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No direct ground-truth experimental comparisons performed by the judge itself.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Automated reviews are reproducible, but their agreement with human reviewers and experimental ground truth is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Automated reviews are computationally inexpensive relative to human review, enabling scaling, but the paper warns against over-reliance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Peer-review rubrics approximate domain norms but are insufficient where experimental replication is required.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reports scores and percentiles but recognizes uncertainty and bias in automated review judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Selection bias, inability to run experiments, and the intrinsic unreliability of peer review for predicting long-term impact are discussed as major limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Paper recommends hybrid human+automated review systems and centralized archiving with transparent labeling of AI-generated outputs to improve oversight.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating ai's ability to replicate ai research <em>(Rating: 2)</em></li>
                <li>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers <em>(Rating: 2)</em></li>
                <li>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark <em>(Rating: 2)</em></li>
                <li>Mle-bench: Evaluating machine learning agents on machine learning engineering <em>(Rating: 2)</em></li>
                <li>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows <em>(Rating: 2)</em></li>
                <li>Livecodebench: Holistic and contamination free evaluation of large language models for code <em>(Rating: 2)</em></li>
                <li>Deepreview: Improving llm-based paper review with human-like deep thinking process <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials <em>(Rating: 1)</em></li>
                <li>Highly accurate protein structure prediction with alphafold <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2110",
    "paper_id": "paper-279075076",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (DeepMind)",
            "brief_description": "A high-impact automated protein structure prediction system that produced experimentally validated 3D protein structures, cited as a benchmark success for AI in science.",
            "citation_title": "Highly accurate protein structure prediction with alphafold",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_description": "DeepMind's deep learning system for predicting protein 3D structure from sequence; uses neural-network-based models trained on known structures and multiple-sequence alignments to predict atomic coordinates.",
            "scientific_domain": "Structural biology / bioinformatics",
            "validation_type": "experimental",
            "validation_description": "Validation performed by comparison of predicted structures to experimentally-determined protein structures (X-ray crystallography, cryo-EM, NMR) and community blind tests (CASP); quantitative comparison of predicted vs. experimental coordinates (RMSD / GDT scores).",
            "simulation_fidelity": "n/a",
            "validation_sufficiency": "Paper and this position paper cite AlphaFold as meeting domain norms: experimental structural comparison is considered sufficient and required in structural biology to establish accuracy.",
            "validation_accuracy": "High; AlphaFold demonstrates near-experimental accuracy on many targets (original AlphaFold reported atomic-level accuracy on many CASP targets); the position paper references AlphaFold as an example of automated tool achieving ground-truth parity but does not restate numeric error here.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "Predictions compared to deposited experimental structures and CASP benchmarks; validated across large protein sets. (Position paper cites AlphaFold as exemplar; details from AlphaFold paper.)",
            "validation_comparison": "Compared computational predictions to experimental ground truth (structural biology gold standard); experimental validation accepted as gold standard.",
            "validation_failures": "Not discussed in detail in this paper beyond general acknowledgement that domain-specific experimental validation underpins AlphaFold's credibility.",
            "validation_success_cases": "Cited as successful automated scientific tool that achieved results previously taking years; used as contrast to AI Scientist systems that lack comparable validated breakthroughs.",
            "ground_truth_comparison": "Yes — predictions compared to experimentally-determined structures (gold standard).",
            "reproducibility_replication": "AlphaFold results were widely reproduced and adopted by the community; the position paper references AlphaFold as reproducible validated success.",
            "validation_cost_time": "Experimental validation in structural biology is time- and resource-intensive compared to purely computational checks; position paper uses AlphaFold as an example of a validated automated tool (cost/time not quantified in this paper).",
            "domain_validation_norms": "Wet-lab / experimental structural comparison required; community uses CASP and RMSD/GDT metrics as standards.",
            "uncertainty_quantification": "AlphaFold provides per-residue confidence metrics (pLDDT) in its original work; the position paper references AlphaFold qualitatively but does not re-report those metrics.",
            "validation_limitations": "Position paper notes such domain-specific tools still require human involvement despite strong validation; specifics of AlphaFold limitations are not detailed here.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2110.0"
        },
        {
            "name_short": "A-Lab",
            "name_full": "A-Lab (Autonomous laboratory reported by Szymanski et al.)",
            "brief_description": "An autonomous laboratory system that synthesized 41 novel inorganic materials in 17 days, demonstrating experimental closure of design-to-synthesis loops.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "mention_or_use": "mention",
            "system_name": "A-Lab (autonomous laboratory)",
            "system_description": "Autonomous experimental platform integrating planning, robotic synthesis, and characterization to perform materials discovery and synthesis with minimal human intervention.",
            "scientific_domain": "Materials science / experimental chemistry",
            "validation_type": "experimental",
            "validation_description": "Physical synthesis and experimental characterization (likely XRD, microscopy, spectroscopy) of novel inorganic materials discovered by the autonomous platform; validation by demonstrating actual synthesized compounds and measured properties.",
            "simulation_fidelity": "n/a",
            "validation_sufficiency": "Experimental synthesis and characterization are domain-appropriate and sufficient for materials discovery; the paper cites A-Lab as an example of automated tool whose results were experimentally verified.",
            "validation_accuracy": "Success demonstrated by synthesis of 41 novel materials; position paper reports this as empirical success without numeric error rates.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "Robotic synthesis and subsequent experimental characterization confirmed the existence and properties of synthesized materials (details referenced from Szymanski et al. 2023). Position paper cites the result qualitatively.",
            "validation_comparison": "Compared autonomous experimental outcomes to standards of materials characterization; direct experimental verification used rather than simulation-only validation.",
            "validation_failures": "Not discussed in this paper; used as an example of a validated automated scientific tool.",
            "validation_success_cases": "Successful autonomous syntheses over a short timescale used to illustrate what validated automation can achieve.",
            "ground_truth_comparison": "Yes — measured, empirical characterization acts as ground truth.",
            "reproducibility_replication": "Position paper does not detail independent replication but cites A-Lab as a reported success in literature.",
            "validation_cost_time": "Experimental validation requires laboratory resources and time; A-Lab achieved rapid turnaround (17 days) which the paper cites as notable although overall resource use is not quantified here.",
            "domain_validation_norms": "Wet-lab experimental synthesis and characterization required for claims of new materials; empirical measurements are the standard.",
            "uncertainty_quantification": "Not discussed in this position paper for A-Lab.",
            "validation_limitations": "Position paper notes that such systems still require human oversight and domain resources; implies lab automation is resource-intensive.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": "",
            "uuid": "e2110.1"
        },
        {
            "name_short": "PaperBench",
            "name_full": "PaperBench (Starace et al.)",
            "brief_description": "A benchmark that asks LLM agents to replicate entire machine-learning papers by developing codebases and running experiments, measuring execution and result reproduction.",
            "citation_title": "Evaluating ai's ability to replicate ai research",
            "mention_or_use": "mention",
            "system_name": "PaperBench",
            "system_description": "Benchmark and evaluation suite for LLM-driven agents to reproduce ML research papers end-to-end, with rubricized sub-tasks including code development, execution, and result match.",
            "scientific_domain": "Computer science / machine learning reproducibility",
            "validation_type": "simulated",
            "validation_description": "Validation consists of generating code, attempting to execute experiments, and quantitatively matching reported results from original papers; rubric includes leaf nodes: 'Execution' (run code) and 'Result Match' (match reported metrics).",
            "simulation_fidelity": "computational reproduction (empirical, depends on execution environment fidelity to original experiments); fidelity limited by available compute, environment differences and missing details in papers.",
            "validation_sufficiency": "Designed to approximate domain norms for reproducibility in ML (execution and metric matching are expected); computational reproduction is necessary for claims in ML but may not fully capture all experimental conditions (hyperparameters, random seeds).",
            "validation_accuracy": "Reported poor success: e.g., Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match' indicating very low end-to-end reproduction accuracy for LLM agents in this benchmark.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab experiments; validation is computational via code execution. PaperBench measures whether agents can run and get matching results; the position paper reports very low success rates and highlights environment/setup gaps as causes.",
            "validation_comparison": "Compares code-generation ability vs. later verification stages (execution, result match) and shows a sharp drop-off; explicit numeric comparisons included (43.4% code-development success vs ~1–2% execution/result-match for some agents).",
            "validation_failures": "Major failure mode: generated code often fails to run or produce matching results; execution and result reproduction almost uniformly poor (very low percentages reported).",
            "validation_success_cases": "Agents can sometimes generate partial code components (code-development subtask success ~43.4% for o1-High) but rarely complete verifiable reproduction.",
            "ground_truth_comparison": "Yes — the 'ground truth' are the reported results/metrics in original papers; agents mostly fail to match these.",
            "reproducibility_replication": "PaperBench explicitly evaluates reproducibility; results show current agents largely cannot replicate published ML experiments within the benchmark constraints.",
            "validation_cost_time": "Computationally expensive due to need to run experiments; position paper highlights high sampling/training time for full end-to-end tasks (estimates of many hours).",
            "domain_validation_norms": "In ML, execution and quantitative metric matching are normative standards for reproducibility; PaperBench operationalizes those norms.",
            "uncertainty_quantification": "Benchmark reports success rates and percentages but not per-run uncertainty measures beyond pass/fail metrics.",
            "validation_limitations": "Limitations include environmental mismatch, missing implementation details in papers, hyperparameter sensitivity, and resource constraints preventing faithful reproduction.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.2"
        },
        {
            "name_short": "SciReplicate-Bench",
            "name_full": "SciReplicate-Bench (Xiang et al.)",
            "brief_description": "Benchmark testing LLM agents' ability to generate Python code to reproduce algorithms from NLP research papers; measures execution accuracy of generated code.",
            "citation_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers",
            "mention_or_use": "mention",
            "system_name": "SciReplicate-Bench",
            "system_description": "Evaluation benchmark where agents read algorithm descriptions in papers and produce runnable Python implementations; judged by whether produced code passes functional tests and reproduces algorithmic behavior.",
            "scientific_domain": "Natural language processing / computer science reproducibility",
            "validation_type": "simulated",
            "validation_description": "Validation performed by executing generated Python code and checking functional correctness against test cases or expected outputs; measures execution accuracy (percentage of tasks with runnable, correct code).",
            "simulation_fidelity": "empirical code execution; fidelity limited by quality of test harnesses and the degree to which test cases capture algorithm correctness.",
            "validation_sufficiency": "Sufficient as a computational reproducibility proxy in CS/ML domains where algorithmic reproduction is core; lacks domain experimental (wet-lab) aspects.",
            "validation_accuracy": "Reported top agent achieved only 39% execution accuracy, indicating majority of generated implementations failed functional tests.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No real-world physical experiments; validation is purely computational; the paper highlights execution failures and debugging deficits as key limitations.",
            "validation_comparison": "Shows discrepancy between reasoning/algorithm understanding and ability to produce runnable code (high reasoning graph accuracy vs low execution accuracy).",
            "validation_failures": "Primary failures: generated code often fails to compile/execute or does not satisfy functional tests; debugging and iterative fixes are weak.",
            "validation_success_cases": "Some agents produce correct algorithmic implementations for a minority (~39%) of tasks; success associated with simpler, well-specified algorithms.",
            "ground_truth_comparison": "Yes — expected outputs or reference implementations used as ground truth for functional tests.",
            "reproducibility_replication": "Benchmark directly assesses reproducibility; results indicate poor reproducibility by LLM agents for many papers.",
            "validation_cost_time": "Computational cost modest per task but scales with number of attempts and need for debugging; position paper notes asynchronous code execution and experiment cycles increase sampling time dramatically.",
            "domain_validation_norms": "In CS/ML, runnable code and matching results are norms for reproducibility; SciReplicate operationalizes these norms.",
            "uncertainty_quantification": "Reports execution accuracy percentages; no detailed probabilistic uncertainty intervals discussed.",
            "validation_limitations": "Limited by the comprehensiveness of test cases, variability in computing environments, and absence of human debugging; agents lack robust iterative verification.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.3"
        },
        {
            "name_short": "CORE-Bench",
            "name_full": "CORE-Bench (Siegel et al.)",
            "brief_description": "A benchmark that requires agents to reproduce research results and then answer questions based on those outputs, assessing end-to-end computational experiment verification.",
            "citation_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "mention_or_use": "mention",
            "system_name": "CORE-Bench",
            "system_description": "Benchmark for computational reproducibility across domains (CS, social science, medicine) where agents must reproduce experiments and reason about results; evaluates multi-stage verification ability.",
            "scientific_domain": "Computational reproducibility across multiple scientific domains",
            "validation_type": "simulated",
            "validation_description": "Agents run computational experiments, reproduce outputs, and then must answer analytical questions based on reproduced results; success judged on reproduction fidelity and subsequent reasoning correctness.",
            "simulation_fidelity": "Empirical computational reproduction; fidelity limited by availability of original code/data and computational resources; medium fidelity dependent on benchmark task complexity.",
            "validation_sufficiency": "Designed to approximate domain standards for computational reproducibility and result interpretation; sufficient for computational work but not for wet-lab domains.",
            "validation_accuracy": "Example: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium, indicating moderate success but imperfect verification capability.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments; validation is computational and focuses on reproducing published computational results and reasoning about them.",
            "validation_comparison": "Highlights trade-offs between reproduction success and downstream reasoning about results; shows agents can reproduce some outputs but struggle with full analytical verification.",
            "validation_failures": "Agents often fail to fully reproduce experiments or to correctly interpret outputs; success rates imperfect and indicate gaps in end-to-end verification.",
            "validation_success_cases": "Partial successes at medium difficulty levels; some agents reproduce results sufficiently to answer questions correctly about outputs in a subset of tasks.",
            "ground_truth_comparison": "Yes — compared to reported paper outputs and reference results.",
            "reproducibility_replication": "CORE-Bench is explicitly about reproducibility; results show partial replicability by current agents.",
            "validation_cost_time": "Computationally intensive for larger experiments; position paper stresses asynchronous experiment cycles slow down RL sampling and evaluation.",
            "domain_validation_norms": "For computational sciences, reproducing experiments and matching reported metrics is normative; CORE-Bench enforces these norms.",
            "uncertainty_quantification": "Benchmarks report success rates; detailed uncertainty measures are not described in the position paper.",
            "validation_limitations": "Constrained by missing details in papers, environment differences, and agents' limited debugging and iterative refinement skills.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.4"
        },
        {
            "name_short": "MLE-Bench",
            "name_full": "MLE-Bench (Chan et al.)",
            "brief_description": "Benchmark evaluating ML-engineering tasks performed by agents across ML development workflows, measuring success rates for deploying ML models and associated verification steps.",
            "citation_title": "Mle-bench: Evaluating machine learning agents on machine learning engineering",
            "mention_or_use": "mention",
            "system_name": "MLE-Bench",
            "system_description": "Benchmark suite for ML engineering tasks (data processing, model training, debugging, deployment) used to assess LLM agents' end-to-end engineering and verification capabilities.",
            "scientific_domain": "Machine learning engineering",
            "validation_type": "simulated",
            "validation_description": "Validation is computational: agents must complete ML workflow tasks and produce functioning artifacts (trained models, code) that pass evaluation; success measured by task-specific success rates and quality metrics.",
            "simulation_fidelity": "Empirical ML workflow execution; fidelity depends on training data, compute availability, and environmental parity with original tasks.",
            "validation_sufficiency": "Sufficient to test engineering-level verification in ML; domain norms expect functioning, evaluated models and reproducible metrics as proof.",
            "validation_accuracy": "Reported low performance for agents on verification steps; e.g., 20% of o1 preview runs on MLE-Bench failed debugging step in cited evaluations.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab experiments; validation via successful completion and correctness of ML workflows and model performance metrics.",
            "validation_comparison": "Shows gap between code generation and achieving validated model performance; benchmark reports near-zero on 'Model Performance' for some agents in ML-Dev-Bench context.",
            "validation_failures": "Frequent failures in debugging, producing valid submissions, and optimizing model performance; indicates weak verification loops.",
            "validation_success_cases": "Limited success on specific sub-tasks (code components), but overall low success for full ML engineering validation.",
            "ground_truth_comparison": "Yes — models and outputs compared against expected performance metrics or baseline results.",
            "reproducibility_replication": "Benchmark examines reproducibility of ML engineering tasks; agents struggle to replicate full workflows.",
            "validation_cost_time": "ML experiments are compute-intensive and time-consuming, causing large sampling times; paper notes RL training for scientist agents would be orders of magnitude larger.",
            "domain_validation_norms": "In ML, validated model performance on standardized datasets and reproducible training runs are normative.",
            "uncertainty_quantification": "Benchmarks report success/failure rates; little discussion of probabilistic uncertainty measures in the position paper.",
            "validation_limitations": "Limited agent debugging, environmental mismatch, and resource constraints; inability to iteratively refine models effectively.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.5"
        },
        {
            "name_short": "ML-Dev-Bench",
            "name_full": "ML-Dev-Bench (Padigela et al.)",
            "brief_description": "Benchmark for agent performance on ML development workflows; evaluates agents' ability to complete ML research and engineering tasks and to validate model performance.",
            "citation_title": "Ml-dev-bench: Comparative analysis of ai agents on ml development workflows",
            "mention_or_use": "mention",
            "system_name": "ML-Dev-Bench",
            "system_description": "Benchmark focusing on ML development tasks such as model training, performance optimization, and validation, measuring task success rates and model performance metrics.",
            "scientific_domain": "Machine learning engineering / research",
            "validation_type": "simulated",
            "validation_description": "Validation carried out by executing ML experiments and evaluating model performance against predefined metrics; includes 'Model Performance' as a key evaluation leaf node.",
            "simulation_fidelity": "Empirical but constrained by available compute and task specification fidelity; fidelity varies with task complexity.",
            "validation_sufficiency": "In ML, computational validation is standard; the benchmark implements domain norms but highlights that agents score poorly on model performance validation.",
            "validation_accuracy": "Reported that all tested agents scored 0% on 'Model Performance' tasks in ML-Dev-Bench in one cited evaluation, indicating failure to reach expected model quality.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Computational-only validation; the paper emphasizes agents' inability to optimize model performance and perform iterative refinement.",
            "validation_comparison": "Shows agents may generate code but fail to get validated performance; comparison underscores verification bottleneck between code generation and validated model outcomes.",
            "validation_failures": "Complete failure on model performance validation for tested agents; inability to tune, debug, and iterate to acceptable performance.",
            "validation_success_cases": "No notable successes reported for model-performance tasks in cited evaluation.",
            "ground_truth_comparison": "Yes — evaluated against baseline expected performance metrics.",
            "reproducibility_replication": "Benchmark exposes reproducibility and performance replication challenges for agent-generated ML experiments.",
            "validation_cost_time": "High computational cost to run full training/validation cycles; position paper notes such cost contributes to training difficulty.",
            "domain_validation_norms": "Validated model metrics on held-out datasets required; benchmark follows these norms.",
            "uncertainty_quantification": "Metrics reported as pass/fail or percent success; uncertainty measures not detailed.",
            "validation_limitations": "Resource constraints, missing hyperparameter/tuning, and debugging shortcomings limit agents' validation success.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.6"
        },
        {
            "name_short": "LiveCodeBench",
            "name_full": "LiveCodeBench (Jain et al.)",
            "brief_description": "A coding benchmark collecting problems from competitive programming contests that evaluates code generation, execution, self-repair, and output prediction by LLMs.",
            "citation_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code",
            "mention_or_use": "mention",
            "system_name": "LiveCodeBench",
            "system_description": "Benchmark that evaluates language models on real-world coding problems including generating correct code, executing it, repairing failures, and predicting outputs; focuses on multi-stage coding verification.",
            "scientific_domain": "Software engineering / code generation",
            "validation_type": "simulated",
            "validation_description": "Validation performed by executing generated code against test cases (pass@1 metrics) and evaluating ability to self-repair and predict outputs; reports pass@1 and other standard code-generation metrics.",
            "simulation_fidelity": "High-fidelity execution of code in sandboxed environments; however, problems are algorithmic (not domain experimental) so fidelity relates to correct program semantics rather than physical realism.",
            "validation_sufficiency": "Sufficient for assessing code-generation correctness in algorithmic tasks; not intended for scientific experimental validation but used to assess components of AI Scientist implementation capability.",
            "validation_accuracy": "Example: o4-mini achieved 52.1% pass@1 on code generation subtask per the position paper, indicating moderate performance on algorithmic code generation but far from perfect.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Computational execution only; benchmark emphasizes debugging and self-repair as verification behaviors.",
            "validation_comparison": "Shows drop-offs in performance when moving from code generation to execution and self-repair; used to illustrate limitations in implementation capability.",
            "validation_failures": "Models that generate plausible code still fail many test cases and struggle with repair iterations in more complex tasks.",
            "validation_success_cases": "Some models achieve reasonable pass@1 on code generation subtask but struggle on end-to-end, multi-file, long-horizon engineering tasks.",
            "ground_truth_comparison": "Yes — expected outputs from contest problems serve as ground truth for correctness.",
            "reproducibility_replication": "Benchmarks responses are reproducible within environment; position paper uses results to indicate generalization gaps.",
            "validation_cost_time": "Code execution tests are relatively cheap per instance but scale with number of attempts; the paper emphasizes that real research implementation cycles (writing, debugging, running experiments) are much more time-consuming.",
            "domain_validation_norms": "For algorithmic problems, passing official testcases is standard; LiveCodeBench implements these norms.",
            "uncertainty_quantification": "Reports pass@1 percentages; no probabilistic uncertainty beyond these metrics discussed.",
            "validation_limitations": "Algorithmic benchmarking does not capture full complexity of multi-file, dependency-rich research codebases; limited for assessing experimental reproduction.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.7"
        },
        {
            "name_short": "DeepReviewer-14B",
            "name_full": "DeepReviewer-14B (Zhu et al.)",
            "brief_description": "A state-of-the-art LLM-based review model used in this paper to perform simulated peer review of 28 AI-generated research papers, assessing implementation-level reliability.",
            "citation_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process",
            "mention_or_use": "use",
            "system_name": "DeepReviewer-14B",
            "system_description": "A 14B-parameter review model trained/engineered to evaluate scientific manuscripts across dimensions like soundness, presentation, contribution; used here as an automated judge for AI-generated papers.",
            "scientific_domain": "Meta-science / peer review automation",
            "validation_type": "simulated",
            "validation_description": "Validation of manuscripts performed via LLM-based rubriced scoring (soundness, presentation, contribution, decision, rating); compares outputs to review norms rather than performing physical experiments.",
            "simulation_fidelity": "N/A to physical experiments; fidelity relates to alignment with human peer review practices—approximate and subject to peer-review variability.",
            "validation_sufficiency": "Paper cautions that LLM-based peer review can filter low-quality papers but is imperfect for predicting long-term impact; simulated review is a results-oriented but partial validation proxy.",
            "validation_accuracy": "Provides aggregate review scores for evaluated AI-generated papers (e.g., average soundness, presentation metrics in Table 2) but does not claim absolute accuracy vs human reviewers.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab experiments; validation is meta-evaluation of papers. The paper used DeepReviewer-14B to find experimental weakness in all 28 papers.",
            "validation_comparison": "Compared AI-generated paper scores across different AI Scientist systems; used to quantify implementation and experimental weaknesses.",
            "validation_failures": "Limitations: peer review (even automated) may fail to identify long-term impact; the paper emphasizes selection bias and peer review unreliability.",
            "validation_success_cases": "DeepReviewer identified widespread 'Experimental Weakness' (100% occurrence) across evaluated papers, successfully highlighting implementation gaps.",
            "ground_truth_comparison": "No direct ground-truth; comparison is to expectations and rubric standards rather than independent experimental verification.",
            "reproducibility_replication": "Simulated review is reproducible in the sense the model produces consistent rubric scores, but human peer review variability remains a caveat.",
            "validation_cost_time": "Automated review is faster and cheaper than human review; the paper suggests automated tools could reduce reviewer burden but warns of misuse risks.",
            "domain_validation_norms": "Peer-review norms serve as partial domain standard for assessing scientific claims, but position paper stresses peer review cannot fully substitute for experimental validation.",
            "uncertainty_quantification": "The model reports scores and percentiles; paper notes these are imperfect proxies and subject to limitations in predicting impact.",
            "validation_limitations": "Automated reviewing can miss high-impact but initially under-appreciated work and cannot replace experimental replication; selection bias in available AI-generated papers may skew evaluation.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Paper advocates hybrid mechanisms combining automated review tools (like DeepReviewer) with human-in-the-loop review for continuous oversight and ethical evaluation.",
            "uuid": "e2110.8"
        },
        {
            "name_short": "Evaluated AI Scientist Systems",
            "name_full": "Five AI Scientist systems evaluated (HKUSD AI Researcher, AI Scientist, AI Scientist v2, CycleResearcher-12B, Zochi)",
            "brief_description": "A set of AI Scientist systems whose publicly-available generated papers (28 total) were evaluated by DeepReviewer-14B, showing pervasive experimental and methodological weaknesses.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "HKUSD AI Researcher; AI Scientist; AI Scientist v2; CycleResearcher-12B; Zochi",
            "system_description": "LLM-based AI Scientist systems that attempt end-to-end research workflows (idea generation through implementation); capabilities vary but generally produce written papers and generated code/artifacts.",
            "scientific_domain": "Computer science / AI research (primarily ML/NLP/CS domains represented in generated papers)",
            "validation_type": "none",
            "validation_description": "Most generated papers lacked rigorous experimental validation; validation was often absent or insufficiently described in the manuscripts, with few performing reproducible computational experiments and virtually no wet-lab experiments (when not applicable).",
            "simulation_fidelity": "n/a",
            "validation_sufficiency": "Position paper argues validation is insufficient — domain norms require runnable code, executed experiments, and reproducible results; evaluated AI Scientist outputs mostly failed to meet these norms.",
            "validation_accuracy": "Aggregate evaluation showed pervasive failures: Table 3 lists 'Experimental Weakness' in 100% of evaluated papers; other defects (methodological flaws, reproducibility issues) occurred at high rates (70%+).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "For the majority of evaluated AI-generated papers, no real experimental validation was performed or reported; the paper explicitly cites lack of execution and experimental rigor as the core implementation gap and limitation.",
            "validation_comparison": "Comparison across systems using DeepReviewer scores (Table 2) shows low ratings and percentiles; AI Scientist systems sometimes generate plausible writing but fail implementation/verification.",
            "validation_failures": "Systemic failures: absence or poor quality of experimental design, inability to run or reproduce experiments, methodological ambiguities, missing hyperparameter analysis and reproducibility details — collectively flagged across evaluated samples.",
            "validation_success_cases": "Rare partial successes: some systems generated plausible idea-level content and code fragments, and a minority of papers had better presentation or specific components, but none demonstrated consistently rigorous validated experimental outcomes.",
            "ground_truth_comparison": "Generally absent — generated papers rarely provided reproducible comparisons to established baselines or ground truth results, limiting verification.",
            "reproducibility_replication": "Position paper reports reproducibility issues as a frequent defect (71.4%), and that independent replication was lacking for AI-generated works.",
            "validation_cost_time": "Paper emphasizes that proper experimental validation is resource- and time-intensive (sampling time estimates in Appendix A) and cites this as a barrier to AI systems performing full validation.",
            "domain_validation_norms": "In CS/ML, norms include runnable code, experimental logs, hyperparameter disclosure, and metric matching; AI-generated papers frequently violated these norms according to the paper's analysis.",
            "uncertainty_quantification": "Most generated papers lacked proper uncertainty quantification or confidence measures; paper flags this as part of experimental weakness.",
            "validation_limitations": "Identified limitations include agents' weak debugging, limited environment/tool coordination, insufficient iterative verification loops, and lack of resource access to perform experiments.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2110.9"
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge (simulated peer-review methodology)",
            "brief_description": "A methodology used in this paper where an LLM review model (DeepReviewer-14B) simulates peer review to assess AI-generated papers' implementation and validation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-as-a-Judge (simulated peer review)",
            "system_description": "Method that leverages a trained large language model to produce rubriced reviews of scientific manuscripts, scoring dimensions like soundness, experimental rigor, presentation, and contribution.",
            "scientific_domain": "Meta-science / scientific evaluation",
            "validation_type": "simulated",
            "validation_description": "Validation of manuscripts via automated scoring against a rubric; identifies experimental weaknesses and methodological flaws without independent experimental reproduction. Used to approximate peer review throughput and to flag low-quality submissions.",
            "simulation_fidelity": "Proxy for human peer review; fidelity limited by model alignment with human reviewers and inability to perform laboratory or compute experiments.",
            "validation_sufficiency": "Paper states this approach can filter low-quality papers but is insufficient to certify high-impact or experimentally validated work; peer review is results-oriented and limited.",
            "validation_accuracy": "Provides aggregate rubric scores (Table 2) but paper cautions about reliability; cites literature showing peer review is not a strong predictor of long-term impact.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments; the method flags absence of validation in manuscripts and quantifies prevalence of defects but cannot substitute for reproduction.",
            "validation_comparison": "Compared automated review scores across AI Scientist systems and correlated defects; paper contrasts simulated review outputs with known limits of peer review reliability.",
            "validation_failures": "Method cannot detect long-term impact or reliably confirm correctness that requires independent experimental reproduction; selected-publication bias may affect assessments.",
            "validation_success_cases": "Successfully identified pervasive experimental weaknesses across 28 evaluated AI-generated papers (100% had experimental weaknesses).",
            "ground_truth_comparison": "No direct ground-truth experimental comparisons performed by the judge itself.",
            "reproducibility_replication": "Automated reviews are reproducible, but their agreement with human reviewers and experimental ground truth is limited.",
            "validation_cost_time": "Automated reviews are computationally inexpensive relative to human review, enabling scaling, but the paper warns against over-reliance.",
            "domain_validation_norms": "Peer-review rubrics approximate domain norms but are insufficient where experimental replication is required.",
            "uncertainty_quantification": "Reports scores and percentiles but recognizes uncertainty and bias in automated review judgments.",
            "validation_limitations": "Selection bias, inability to run experiments, and the intrinsic unreliability of peer review for predicting long-term impact are discussed as major limitations.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Paper recommends hybrid human+automated review systems and centralized archiving with transparent labeling of AI-generated outputs to improve oversight.",
            "uuid": "e2110.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating ai's ability to replicate ai research",
            "rating": 2
        },
        {
            "paper_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers",
            "rating": 2
        },
        {
            "paper_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "rating": 2
        },
        {
            "paper_title": "Mle-bench: Evaluating machine learning agents on machine learning engineering",
            "rating": 2
        },
        {
            "paper_title": "Ml-dev-bench: Comparative analysis of ai agents on ml development workflows",
            "rating": 2
        },
        {
            "paper_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code",
            "rating": 2
        },
        {
            "paper_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process",
            "rating": 2
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "rating": 1
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold",
            "rating": 1
        }
    ],
    "cost": 0.02232075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025</p>
<p>Minjun Zhu 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Qiujie Xie 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Yixuan Weng 
Engineering School
Westlake University</p>
<p>Jian Wu 
Engineering School
Westlake University</p>
<p>Zhen Lin 
Engineering School
Westlake University</p>
<p>Linyi Yang yanglinyiucd@gmail.com 
The emergence of Artificial Intelligence (AI) Scientist
University College London</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Engineering School
Westlake University</p>
<p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025F860A8464F3C105E680E4FA57B83AF94arXiv:2506.01372v2[cs.AI]AI ScientistImplementation GapHypothesis and Verification
represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation.Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent.Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools.Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.To better illustrate the root cause of this implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist.This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
<p>Introduction</p>
<p>The automation of scientific discovery has long been one of humanity's deepest desires (Langley, 1987, King et al., 2009, Radensky et al., 2024, AI, 2025).In recent years, with the advances in deep neural network technology, a range of automated scientific tools has emerged, leading to groundbreaking achievements in fields such as biomedicine (Yang et al., 2025c, Jumper et al., 2021), chemistry (Stokes et al., 2020), and materials science (Szymanski et al., 2023).For instance, DeepMind's AlphaFold can determine the 3D structures of proteins in just a few hours, a task that previously took years to solve (Jumper et al., 2021).In recent, researchers developed an autonomous laboratory, A-Lab, which successfully synthesizes 41 novel inorganic materials within 17 days (Szymanski et al., 2023).However, these scientific tools still rely heavily on human involvement.Researchers must first formulate ideas to be tested, while AI is responsible for the labor-intensive tasks of verification and iterative search.Therefore, these systems cannot be considered as truly automated scientific research.</p>
<p>The emergence of LLM-based AI Scientist has propelled the automation of scientific research to the next level, with AI taking the lead as the primary executor of scientific discovery, managing the entire workflow from idea generation to experiment execution (Lu et al., 2024, Weng et al., 2025).Recent studies have shown that research papers produced by AI Scientist have already reached the level of submissions to major machine learning conferences (Si et al., 2024, Yamada et al., 2025, Intology, 2025).As shown in Figure 1, we demonstrate the progress made by AI Scientist-v2 (Yamada et al., 2025), and the research output has received review scores exceeding the average acceptance threshold for human-authored papers.Similarly, researchers present an empirical validation through multiple peer-reviewed publications accepted at ICLR 2025 workshops and ACL 2025 main conference (Intology, 2025).Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools (e.g., AlphaFold (Jumper et al., 2021)).</p>
<p>In this position paper, we first propose a conceptual framework (Section 2) that defines an AI Scientist as an advanced end-to-end system capable of independently formulating scientific ideas and performing the implementation for verifying these ideas.This definition forms the theoretical foundation of our position, aligns with current research progress (Lu et al., 2024, Weng et al., 2025, Yamada et al., 2025), and emphasizes that the core capability of an AI Scientist lies in generating innovative and feasible ideas at scale (Si et al., 2024, Wang et al., 2024a, Hu et al., 2024, Yang et al., 2025d).The idea-generation capability is a key feature that sets AI Scientists apart from automated scientific tools.While recent advances demonstrate that AI Scientists can generate highly innovative ideas (Si et al., 2025), their implementation capabilities remain constrained (Chan et al., 2024, Starace et al., 2025, Xiang et al., 2025, Siegel et al., 2024, Padigela et al., 2025), creating a significant gap between innovative idea generation and complete implementation.</p>
<p>Our Position:</p>
<p>The fundamental bottleneck for AI Scientists lies in their implementation capability to effectively execute the verification of these ideas.</p>
<p>We defend our argument by analyzing quantitative evidence from existing benchmarks used to evaluate LLMs' abilities in performing complex engineering tasks (Section 3.2).While LLMs can generate highly novel ideas (Si et al., 2024, Chai et al., 2024, Gottweis et al., 2025), their performance in experiment execution is exceptionally poor (Table 1).For instance, a leading LLM like Claude 3.5 Sonnet scored only 1.8% on PaperBench (Starace et al., 2025).This implementation gap is further supported by a systematic evaluation (Section 3.3), which leverages a state-of-the-art review model, DeepReviewer-14B (Zhu et al., 2025), to assess 28 research papers generated by five advanced AI Scientist systems.The results demonstrate that current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.Finally, to clearly illustrate the root cause of the implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist (Section 4).</p>
<p>In summary, this paper validates and deeply analyzes the implementation gap in existing AI Scientist systems based on extensive quantitative evidence and a simulated peer-review process.Furthermore, as the development of AI Scientists will bring greater regulatory challenges, we comprehensively examine the ethical considerations (Section 5) faced by AI Scientists and suggest directions for future research (Section 6).We hope this position paper will contribute to a clearer understanding of the limitations of current AI Scientist, shedding light on the future development of AI Scientist.</p>
<p>Definition of the AI Scientist</p>
<p>The emergence of automated scientific tools has accelerated scientific discovery across numerous domains (King et al., 2009, Yang et al., 2025c, Jumper et al., 2021, Stokes et al., 2020, Szymanski et al., 2023).However, these tools fundamentally operate within a paradigm where human researchers remain in the dominant position of scientific discovery, and thus cannot be classified as fully automated AI Scientists.In this section, we first provide a detailed discussion of the unique characteristics of the AI Scientist (Section 2.1).Building on this discussion, we then propose a conceptual framework that formally defines the AI Scientist in a mathematical form (Section 2.2).Scientific tools, originating from AI for Science research, represent specialized AI systems designed to solve specific scientific problems by processing data and generating results within defined domains.These tools have demonstrated remarkable success across diverse scientific fields, including protein structure prediction (e.g.Al-phaFold) (Jumper et al., 2021), antibiotic discovery through deep learning approaches (Stokes et al., 2020), and autonomous chemical research with large language models (Boiko et al., 2023).These scientific tools funda-mentally operate within a knowledge-dependent collaborative framework between humans and AI.</p>
<p>Unique Characteristics</p>
<p>AI Scientist represents a research paradigm shift where AI assumes the role of an autonomous scientist capable of conducting independent scientific research.As illustrated in Figure 2, while scientific tools operate under human supervision, receiving data as input and producing predictions as output, AI Scientist goes a step further by demonstrating autonomous scientific reasoning capabilities.It accepts research questions as input and engages in iterative, self-directed interactions with scientific tools to generate comprehensive solutions.Unlike scientific tools that function as sophisticated instruments awaiting human guidance, AI Scientist exhibits genuine scientific agency, conducting end-to-end scientific investigations from question formulation to solution discovery (Yamada et al., 2025).</p>
<p>Conceptualized Framework</p>
<p>Our Definition: An AI Scientist is an advanced end-to-end system capable of independently formulating scientific ideas and executing the requisite verification and falsification procedures.</p>
<p>We define an AI Scientist, denoted as  AI , as a fully autonomous scientific intelligence capable of independently performing diverse scientific research tasks.Different from a general scientific tool, it must possess dual capacities, including idea generation and experimental execution.A complete scientific research task typically originates from an initial scientific question  init and leverages existing domain knowledge  domain .An AI Scientist, denoted  AI , operates within the scope of human ethical constraints ℛ human and resource constraints ℬ res to conduct this task.The primary output is the generation of novel scientific knowledge  new and associated verifiable artifacts  sci .The process through which an AI Scientist aims to achieve the optimal output from a scientific research task can be formally represented as:
(𝒦 new , 𝒜 sci ) ← max{𝒮 AI (𝒬 init , 𝒦 domain , ℛ human |θ AI , ℬ res )} (1)</p>
<p>Arguments for Implementation Capability</p>
<p>We argue that the fundamental bottleneck limiting AI Scientists lies not in their idea generation capabilities, but in their capacity to execute rigorous implementation procedures required for reliable scientific research.To support this position, we present three lines of evidence: systematic analysis of research trends in the AI Scientist literature (Section 3.1), comprehensive benchmark analysis across multiple evaluation frameworks (Section 3.2), and systematic peer review assessment using LLM-as-a-Judge methodology (Section 3.3).</p>
<p>Research Trend of AI Scientist</p>
<p>Our statistical analysis of AI Scientist papers on arXiv up to May 23, 2025 (see Appendix B for details), reveals key trends illustrated in Figure 3.The lower panel of the figure shows that while the total number of publications is growing, studies focusing on idea generation without concrete implementation details consistently outnumber those incorporating such implementations.Despite this disparity in publication numbers, the upper panel indicates a crucial counterpoint: papers that include substantive implementation details achieve a significantly higher average number of citations.This signals strong community valuation for executable advancements and underscores the importance of addressing the implementation gap.This then raises a critical question: if implementation-focused research garners higher impact, why does its volume remain markedly lower?This disparity strongly implies that the path of implementation is fraught with substantial challenges.Empirical Evidence of Implementation Gap.Advanced LLMs achieve near-saturated performance on simple code generation benchmarks like HumanEval (Chen et al., 2021, Liu et al., 2023, Yang et al., 2025a).For example, o3 exhibits excellent problem-solving capabilities in the 99.8th percentile of human performance on algorithmic competition platforms like Codeforces.However, the performance of SoTA LLMs drops dramatically when it comes to real-world research scenarios.As depicted in  (Siegel et al., 2024) (reproducing computational results from scientific papers, determined by accuracy), and ML-Dev-Bench (Padigela et al., 2025) (completing diverse ML development workflow tasks, assessed by success rates).Each takes a different approach to measuring how well AI systems can automate aspects of ML research.These evaluations consistently demonstrate that LLMs face difficulty in translating conceptual understanding or initial plans into verifiably correct and operational code.This "implementation gap" fundamentally limits AI Scientist's verification capabilities.</p>
<p>Quantitative Analysis</p>
<p>Beyond Code Generation.The complexity of real-world research implementation processes extends far beyond simple code generation tasks, often requiring sustained reasoning and multi-step problem-solving.However, current LLMs exhibit relatively weak performance on such complex challenges.LiveCodeBench (LCB) (Jain et al., 2024), a more complex evaluation benchmark than Humaneval (Chen et al., 2021) that collects problems from periodic contests on LeetCode, AtCoder, and Codeforces platforms, evaluates Code LLMs across diverse code-related scenarios, including code generation, execution, self-repair, and output prediction.o4-mini achieves SoTA performance on the code generation subtask with only 52.1% pass@1 score.This poor performance on complex coding tasks reveals that AI scientists lack the implementation ability to handle sophisticated code-based research scenarios.</p>
<p>Implementation and verification.We observe that the verification bottleneck emerges across multiple stages of the research process.SciReplicate-Bench (Xiang et al., 2025), which tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers, reveals that despite agents demonstrating an understanding of algorithmic logic (evidenced by high reasoning graph accuracy), they struggle with code execution.The best agent achieved only 39% execution accuracy, indicating its generated code passed functional test cases for just 39% of the tasks, highlighting a failure to ensure implementation correctness and runtime behavior.Similarly, PaperBench (Starace et al., 2025) requires LLM agents to replicate entire machine-learning papers from scratch by developing codebases and running experiments.While agents can generate code components (e.g., o1-High achieving 43.4% success on weighted "Code-Development" sub-tasks), their performance on subsequent verification stages is poor.On rubric-defined leaf nodes for "Execution" (successfully running the code) and "Result Match" (quantitatively matching the paper's reported results), Claude 3.5 Sonnet scored only 1.8% and 0.7% respectively.This poor performance indicates a breakdown in ensuring the developed solution operates correctly and produces the intended outcomes.</p>
<p>Discussion.The verification challenge extends beyond initial code implementation to debugging, iterative refinement, and validation of experimental outcomes.Evidence from MLE-Bench and ML-Dev-Bench (Chan et al., 2024, Padigela et al., 2025) shows that LLM agents frequently fail to debug their code or produce valid submissions, with 20% of o1 preview runs on MLE Bench failing this step, and struggle to optimize model performance.Debugging, an explicit verification procedure, also indicates persistent agent failures that highlight the verification bottleneck (Chan et al., 2024).The incapacity to iteratively refine solutions towards better performance, illustrated in ML-Dev-Bench where all tested agents scored 0% on "Model Performance" tasks, further signifies deficiencies in robust verification loops essential for scientific advancement (Padigela et al., 2025).Furthermore, CORE-Bench, which requires agents to reproduce results and then answer questions based on these outputs, assesses the verification of entire computational experiments.This process, involving multiple stages of reproduction and reasoning, presents significant challenges.For instance, the imperfect success rates (e.g., CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium) highlight the difficulties in this complex verification process (Siegel et al., 2024).These difficulties across verification tasks suggest that enhancing AI Scientists' systematic verification capability is crucial for their maturation into ideal AI Scientists.Current LLMs, while proficient in content generation, fail to rigorously validate their outputs against explicit criteria, a foundational component of scientific practice.</p>
<p>LLM-as-a-Judge Reveals the Implementation Weaknesses</p>
<p>To further support the existence of implementation gap, we employ a simulated peer review methodology to assess the actual quality of scientific outputs from current AI Scientist systems, particularly their implementation-level reliability.We select 28 publicly available research papers generated independently by five different AI Scientist systems and utilize the SoTA review model DeepReviewer-14B (Zhu et al., 2025) to conduct systematic evaluation under unified standards.We acknowledge that potential selection bias in the public availability of these papers (e.g., researchers may only publish better-performing outputs) means our evaluation results may not fully represent the average output quality of these systems across all scenarios.Nevertheless, this analysis provides valuable insights into the general quality level of current AI-generated research papers.</p>
<p>Rooted Limitations of Execution Capabilities</p>
<p>Our empirical analysis (Section 3) reveals a clear pattern that while AI Scientists are conceptualized as advanced iterations of traditional scientific tools, they consistently fail at implementation and verification procedures across diverse scientific contexts.This raises a critical question: Why do these sophisticated systems fail to achieve consistently strong results, especially when traditional scientific tools, wielded by human researchers, prove highly effective?To understand this paradox, we provide a discussion on the root cause of the implementation gap (Section 4.1 ) and present an in-depth analysis of the fundamental limitations of AI Scientist (Section 4.2).</p>
<p>Two Primary Facets of Implementation Gap</p>
<p>The implementation gap for AI Scientists comprises two primary facets: (1) AI Scientists often exhibit bottlenecks in the planning and execution stages.This manifests in three key areas: failures in longrange logical reasoning required for coherent experimental design, inadequate multi-agent collaboration capabilities including strategic planning across complex multi-file implementations and converting conceptual ideas into working code, and insufficient coordination with external tools and systems; (2) Even when implementation code is generated, AI Scientists demonstrate fundamental weaknesses in evaluation processes.This includes failures in debugging capabilities, experimental validation, result interpretation, and iterative refinement based on experimental feedback.Current systems lack robust mechanisms for assessing implementation quality, validating experimental outcomes, and providing reliable feedback loops that can guide subsequent implementation improvements.</p>
<p>Prevent building "castle in the air".Agent tools often produce difficult-to-verify code and experiments, while evaluation gaps prevent AI Scientists from recognizing and correcting implementation issues through iterative refinement.Without fundamentally enhancing both capabilities, the idealized AI Scientist capable of independent scientific exploration will remain inefficient.</p>
<p>Rooted Limitations</p>
<p>From the current literature on AI scientists, we conclude four major limitations that collectively explain why AI scientists struggle with complex, multi-stage implementation processes:</p>
<p>Limitation 1: fundamental cognitive and execution capabilities.Scientific implementation requires sophisticated long-range logical reasoning across multiple abstraction levels.Existing LLMs demonstrate significantly decreased coherence and robustness as reasoning chains extend (Wu et al., 2025b,a), and increased thinking time does not necessarily yield stronger performance (Ballon et al., 2025).Furthermore, LLM-based agents possess limited capacity to retain past interaction information, with memory deteriorating as text length increases (Pink et al., 2025, Cemri et al., 2025).Most critically, mainstream language models exhibit markedly weaker performance in multi-turn dialogues or multi-step interactive tasks requiring context coherence, deep understanding, and state tracking, with average performance decreases reaching 39% (Laban et al., 2025).This capability degradation in scenarios involving long-range dependencies and complex interactions directly constrains AI Scientist performance in executing complex scientific experiments requiring sustained attention and coherent reasoning chains.</p>
<p>Limitation 2: strategic planning and reasoning.Scientific implementation requires comprehensive abilities for strategic reasoning, continuous monitoring, and dynamic adjustment across all research stages (Lu et al., 2024, Yamada et al., 2025).High-quality research implementation demands global planning abilities spanning entire codebases, which typically contain multiple interdependent files with hundreds of lines requiring coordinated modification (Jimenez et al., 2024, Aleithan et al., 2024).Long-term, complex scientific exploration tasks such as discovering new materials, and modeling complex biological systems particularly require continuous iteration of research directions and experimental strategies over extended time scales based on emerging results and external feedback (Merchant et al., 2023, Brixi et al., 2025, Weng et al., 2023).However, current LLMs demonstrate inadequate adaptive planning and metacognitive abilities when handling highly open, creative scientific research requiring dynamic adjustments to overall research blueprints.While reinforcement learning approaches may potentially enhance LLMs' generalization and metacognitive capabilities, the resource investment required for "inventor" roles like AI Scientists that need to perform complex asynchronous operations and real-world interactions proves enormous.Figure 4 highlights AI's acceleration over human performance in complex tasks such as reasoning and web-based research.While AI Scientists also achieve tasks faster than humans, their estimated single-sample RL training time is orders of magnitude greater than simpler AI agents.This substantial increase in required sampling time (detailed in Appendix A) underscores the immense challenge of developing AI Scientists via standard RL methodologies.(Guo et al., 2024, Qian et al., 2024, Pu et al., 2025b).This requires AI Scientist to not only understand instructions conforming to collaborative protocols but also precisely execute the implementation phases assigned to it within tasks and reliably feed its outputs back to the collaborative network (Bo et al., 2024, Zhang et al., 2024).However, current LLM Agents still have considerable room for improvement in robustness and adaptability when interacting with dynamic environments (Wei et al., 2025).For instance, when calling a series of external APIs to complete a complex scientific computational process, LLM often struggles to handle subtle changes in API interfaces, and other practical engineering issues (Shen et al., 2025).</p>
<p>Limitation 4: evaluation and verification.Existing benchmarks such as MLE-Bench (Chan et al., 2024) and PaperBench (Starace et al., 2025) primarily focus on the complete reproduction of code and experiments from papers.SciReplicate-Bench (Xiang et al., 2025) emphasizes generating necessary code from scientific papers, while ScienceAgentBench (Chen et al., 2025) concentrates on independent and singular data-driven tasks.However, there is currently a lack of a comprehensive benchmark that can evaluate the entire scientific workflow, from initial idea generation through to final implementation and completion.This absence makes it difficult to fairly compare the end-to-end capabilities of different AI Scientist systems.</p>
<p>Additionally, there is a deficiency in evaluation approaches that incorporate measures for external supervision during the AI Scientist's implementation process.The deeper issue is that the quality of scientific discovery often lacks unified objective standards, and the process of scientific exploration is filled with uncertainty and openness, making comprehensive evaluation and effective supervision of AI Scientist's verification capability exceptionally difficult.Evaluating AI Scientist's output (e.g., generated papers) from a peer review perspective, while being a results-oriented assessment method, also has inherent limitations.As in human research systems, even experienced peer reviewers may not always accurately identify the groundbreaking and far-reaching work.A frequently cited example is that the word2vec paper (Mikolov et al., 2013) was initially rejected by ICLR 2013, but later received the "Test of Time Award" at NeurIPS 2023.Extensive experimental analyses have demonstrated that review scores are not reliable indicators for predicting future impact (Abramo et al., 2019, Cortes andLawrence, 2021), suggesting that peer review may be more suitable for filtering low-quality papers rather than identifying the highest quality papers.</p>
<p>Ethical Considerations</p>
<p>Sub-Position: AI scientists are in urgent need of a comprehensive system for generation management and quality evaluation.</p>
<p>As autonomous research agents, AI Scientists lack values and moral constraints.They are incapable of making ethical judgments about the societal impact of their work, and they do not self-regulate based on potential risks associated with their findings (Bengio et al., 2025).As AI Scientists possess stronger capabilities in idea generation and experiment execution, their influence on scientific research and society could far surpass that of current LLMs and scientific tools (e.g., Deep Search, AutoSurvey (Wang et al., 2024c)).In the absence of proper oversight, AI Scientists may: (1) be misused, overwhelming the peer review system, leading to a decline in overall research quality;</p>
<p>(2) enter unethical or dangerous research domains, autonomously generating and publishing sensitive findings that accelerate the development of harmful technologies;</p>
<p>(3) weaken the quality of PhD training, leading to a decline in human research standards and overall scientific literacy.To prevent the above situations, we argue that AI Scientists are in urgent need of a comprehensive system for generation management and quality evaluation, thus enabling effective behavior regulation within the human moral framework (Jobin et al., 2019).This system should include, but not be limited to, the following components:</p>
<p>(1) Implement measures to prevent AI-generated content from disrupting human review systems: Effective strategies should be adopted to ensure that AI-generated articles do not interfere with human peer-review systems while maintaining high standards of quality.This includes establishing a centralized platform to archive scientific outputs generated by AI Scientists, developing automated detection systems to identify such content, and creating specialized evaluation tools (e.g., DeepReview (Zhu et al., 2025)) to assess the quality of AI-generated research outputs.These tools should help identify and filter low-quality content, thereby reducing the burden on the peer review process.All AI-generated outputs must be transparently labeled and reviewed, including information on their origin, generation methods, and scientific tools.</p>
<p>(2) Establish boundaries and strengthen training programs: Implement clear boundaries between human-led and AI-led research processes to ensure that PhD students receive comprehensive training.Key components of doctoral education(e.g., idea testing), should prioritize human involvement to maintain high standards of scientific literacy.Additionally, guidelines should be established to prevent over-reliance on AI Scientists in PhD training, ensuring that AI tools serve as supplements rather than substitutes in the educational process.</p>
<p>(3) Formulate an ethics and responsibility convention: A global convention should be established to define the ethical boundaries and risk management principles for AI-driven research (Huang et al., 2022).</p>
<p>All researchers and institutions utilizing AI Scientists must fully disclose the generation process, algorithmic sources, training data, and potential societal risks of their findings.Additionally, a hybrid mechanism combining automated and human-in-the-loop review should be implemented for continuous ethical oversight and risk evaluation, ensuring that AI Scientist research activities remain within socially safe boundaries (Jobin et al., 2019, Khan et al., 2022).Furthermore, appropriate legislation should be developed to regulate AI Scientists by imposing strict limitations on their use for specific research purposes.</p>
<p>Future Directions</p>
<p>This section outlines feasible pathways to bridge the current implementation capability gap of AI Scientists.</p>
<p>Addressing foundational Basic Abilities is paramount.While scaling laws for pre-training and post-training (Kaplan et al., 2020, Zhang et al., 2025) promise progressive LLM improvements, immediate strategies like well-defined Workflows (Li et al., 2024d, Gu et al., 2024b) also can mitigate current implementation weaknesses.Structuring research processes with human-defined tools allows for guided AI execution and targeted interventions.For instance, Retrieval Augmented Generation (RAG) can counteract limitations in handling long texts or accessing current information (Fan et al., 2024, Arslan et al., 2024), thus expanding the knowledge scope of AI systems.</p>
<p>A significant challenge for sophisticated Strategic Planning is the immense resource consumption of RL (Cao et al., 2024).A promising direction to alleviate this involves leveraging LLMs to simulate aspects of the environment or task execution, thereby accelerating the RL feedback loop (Sun et al., 2025).By allowing the RL agent to receive quicker, albeit potentially approximate, feedback on its actions, particularly for operations that are inherently time-consuming in the real world, the sampling efficiency may be significantly improved.This could reduce the extensive wall-clock time typically required for training robust long-horizon planning and adaptive meta-thinking capabilities in complex scientific domains.</p>
<p>Ensuring Reliable Verification and Fostering Collaboration is crucial.Standardized protocols like MCP and A2A (Yang et al., 2025b, Ray, 2025, Hou et al., 2025) can establish basic interoperability.A promising direction is to build modular multi-agent systems, where specialized AI agents for sub-tasks (e.g., literature review, code generation) are coordinated by a central "Planner Agent" trained via advanced RL, leveraging existing tools (e.g., PASA (He et al., 2025)) rather than reinventing capabilities.Furthermore, enhanced oversight of AI Scientist inference processes is imperative, not just to prevent benchmark "hacking", but also to instill ethical boundaries against unscrupulous data acquisition or other problematic behaviors.</p>
<p>Finally, the Evaluation of AI Scientists (Chang et al., 2024) must evolve towards a holistic, coarse-grained paradigm reflecting real-world scientific discovery's multifaceted nature.Scientific breakthroughs involve both practical utility and novelty.Thus, evaluation frameworks should go beyond single-metric optimization, adopting multi-objective criteria that assess performance gain, originality, experimental rigor, and communication clarity.This multi-faceted approach will offer a more accurate measure of an AI Scientist's true contribution, guiding development toward impactful scientific exploration.</p>
<p>Conclusion</p>
<p>The rise of AI Scientists marks a paradigm shift in scientific discovery, with large language models (LLMs) now driving the workflow from idea generation to experiment execution.Recent systems have shown promise, producing research accepted at ICLR 2025 workshops and sparking discussions on the imminence of human-level AI Scientists.However, despite this progress, AI Scientists have yet to achieve breakthroughs in computer science comparable to traditional automated tools.Based on benchmark analyses and a systematic evaluation of 28 papers from five leading AI Scientist systems, we identify a core bottleneck: the inability to reliably execute and verify experiments.This implementation gap limits both scientific rigor and the quality of the research output.We analyze its root causes and call on the community to address this critical limitation.</p>
<p>Alternative Views.An alternative viewpoint suggests that AI Scientists need not pursue completely autonomous implementation capabilities in the short term, but rather facilitate human-machine collaboration as Co-scientists to assist humans.This approach avoids the deficiencies of LLMs in Dynamic Planning capabilities and Reliable Verification capabilities, instead allowing AI to focus on its strengths, such as idea generation, while humans execute the specific experimental results (Weng et al., 2025).If an AI system, though unable to independently complete all implementation details, can increase human scientists' efficiency tenfold, or help human scientists conceive and verify complex ideas previously beyond reach, then it undoubtedly also qualifies as a successful "collaborative scientist."</p>
<p>Unite</p>
<p>A. Sampling Time Calculation for Different Types of AI Agents</p>
<p>We referenced existing literature (Guo et al., 2025, Yang et al., 2025a, Muennighoff et al., 2025) and our experience to estimate the sampling time potentially required for different types of AI agents trained via reinforcement learning, as illustrated in In contrast, an AI Scientist executing end-to-end scientific discovery tasks has complexity and interaction requirements far exceeding the previous two types.We roughly estimate it might need to generate over 100,000 tokens of content (for example, operational and experimental code approximately 50,000 tokens (T in f er_code ≈ 1250s), research paper writing approximately 30,000 tokens (T in f er_paper ≈ 750s), reviewing and understanding relevant literature approximately 20,000 tokens (T in f er_lit ≈ 500s)), with pure LLM inference time for just this portion being T in f er_CS = T in f er_code + T in f er_paper + T in f er_lit ≈ 2500s.More critically, the "implementation" process of an AI Scientist, such as code writing, debugging, compiling, running experiments, and data analysis, is highly asynchronous and time-consuming.Assuming a rapid research code operation and experimental cycle (from writing to obtaining preliminary results) requires an average of T op_code ≈ 12 hours = 43200s, while in-depth literature research and analysis might require T op_lit ≈ 20 minutes = 1200s.Therefore, the total estimated sampling time to complete a relatively complete scientific exploration loop would be T sample_CS = T in f er_CS + T op_code + T op_lit ≈ 2500s + 43200s + 1200s ≈ 46900s.As intuitively demonstrated in Figure 4, the sampling time required for an AI Scientist (approximately 46,000 seconds) far exceeds that of an AI Reasoner (approximately 250 seconds) and an AI Web Agent (approximately 700 seconds).Notably, AI Reasoners can typically rapidly generate large quantities of training samples through batch generation in parallel, whereas each implementation step of an AI Scientist (especially parts involving code execution and experiment waiting) is almost entirely asynchronous, and requires exclusive computational resources or experimental equipment for learning and feedback collection during operations.Consequently, in actual reinforcement learning training processes, the disparity in real training duration between AI Scientists and the former two types will be even more pronounced.</p>
<p>For the calculation of human duration, we referenced existing metrics.For instance, for reasoning tasks, we referred to the human time from the International Mathematical Olympiad, which is approximately 1.5 hours per problem.For Web Agent tasks, we adopted the average human problem-solving time from BrowseComp (Wei et al., 2025) (2 hours) as the human standard.For Scientist tasks, although each paper often requires months of collaborative work by multiple people, for ease of calculation, we used the human duration of 48 hours from PaperBench (Starace et al., 2025) for statistics; however, even under these conditions, humans achieve a success rate of less than 50%.(Pu et al., 2025a), (Yang et al., 2024), (Su et al., 2024), (Li et al., 2024a), (Hu et al., 2024), (Liu et al., 2025), (Wang et al., 2024b) (Weng et al., 2025), (Xiong et al., 2024) (Gu et al., 2024a), (Li et al., 2024b), (Yu et al., 2024) (Gottweis et al., 2025) (Rabby et al., 2025), (Saeedi et al., 2025) (O'Neill et al., 2025), (Garikaparthi et al., 2025), (Sanyal et al., 2025) w/ Exp (Lu et al., 2024), (Li et al., 2024c) (Liu et al., 2024b), (Liu et al., 2024a) (Yuan et al., 2025), (Schmidgall et al., 2025) (Jiang et al., 2025), (Kon et al., 2025) (Schmidgall et al., 2025), (Jansen et al., 2025) (Yamada et al., 2025), (Seo et al., 2025)</p>
<p>B. Regarding the statistics for the papers</p>
<p>We have conducted a comprehensive search on arXiv to gather relevant publications in the AI Scientist field.This collection includes a series of papers from August 2024 to April 2025 for methods or systems, which are cited in Table 4.It indicates that, to date, a significant number of papers have focused on Idea Generation tasks, often without concrete implementations.</p>
<p>Nevertheless, an encouraging trend has emerged since early 2025.As illustrated in Figure 3, implementationfocused research has demonstrated stronger growth momentum, with incremental growth nearly matching that of non-implementation studies by Spring 2025.This suggests the community is beginning to recognize the critical importance of implementation capabilities for developing truly effective AI Scientists-moving beyond theoretical constructs toward practical systems capable of reliable execution.</p>
<p>Figure 1 :
1
Figure 1: The roadmap of AI Scientist from 2024 to future, highlighting key milestones and fundamental challenges that must be overcome to bridge the implementation gap of AI Scientist.</p>
<p>Figure 3 :
3
Figure 3: Analysis of AI Scientist publications on arXiv.The upper panel displays the average number of citations up to now, categorized by containing implementation details.The lower panel shows the growth in the total number of these papers with the same categorization.</p>
<p>Figure 4 .
4
Using a hypothetical 671B parameter LLM (similar to Deepseek-R1) running on 8 H100 cards (assuming 40 tokens generated per second), the pure inference time T in f er_R for a typical arithmetic reasoning task (generating approximately 10,000 tokens of reasoning content) might be around 250 seconds.For an AI Web Agent, the task might include generating approximately 8,000 tokens of instructions and reports (T in f er_WA ≈ 200s), interspersed with approximately 20 API calls for information search (assuming each search and processing takes T search_API = 10s, totaling T search_total = 20 × 10s = 200s), and potentially requiring reading and comprehension of up to 400,000 tokens of web content (assuming reading and comprehension time T read_WA ≈ 200s).The total sampling time is: T sample_WA ≈ T in f er_WA + T search_total + T read_WA ≈ 600s.</p>
<p>Table 1 :
1
State-of-the-art (SoTA) LLMs show relatively low accuracy on code implementation on different tasks.The listed benchmarks are collected from diverse domains.The table below details their tasks, domains, scale, methods, and performance.
Benchmark Task DescriptionDomainsScaleLLM Acc. PerformanceMLE-Bench(Chan et al., 2024) AI Training taskApplied ML75 OpenAI o1-preview16.90%PaperBench (Starace et al., 2025) ICML paper ReplicatingNLP, CV, ML8,316OpenAI o1-high26.00%SciReplicate-Bench (Xiang et al., 2025) Code GenerationNLP100Claude-Sonnet-3.739.00%CORE-Bench (Siegel et al., 2024) Scientific Paper reproduc-Computer Science,270OpenAI GPT-4o55.56%tionSocial Science, andMedicineML-Dev-Bench (Padigela et al., 2025) AI training taskML30Claude-Sonnet-3.550.00%</p>
<p>Table 2 :
2
DeepReviewer-14B Evaluation of AI-Generated Papers from Various AI Scientist Systems.Scores reflect averages across the 'Num' of available papers.Note: Publicly available papers may be curated and not fully representative of typical system output.
AI Scientist SystemNum Soundness↑Presentation↑Contribution↑ Decision↑Rating↑Percentile↑HKUSD AI Researcher71.751.461.570.02.573.43%AI Scientist102.081.801.750.03.358.22%AI Scientist v231.671.501.500.02.332.04%CycleResearcher-12B62.251.752.130.03.7516.88%Zochi22.382.382.250.04.6329.96%</p>
<p>Table 3 :
3
Defect Categories and Their Issues.
Defect CategoryNumber PercentageExperimental Weakness28100%Methodological Unclarity/Flaws2796.4%Writing &amp; Presentation Issues2692.9%Novelty Concerns2589.3%Theoretical Weakness2485.7%Literature Review Deficiencies2278.6%Practicality &amp; Robustness Gaps2175.0%Reproducibility Issues2071.4%Computational Cost Concerns1864.3%Component Analysis1657.1%Hyperparameter Analysis Lacking1657.1%Ethical Considerations Missing310.7%</p>
<p>Table3shows that among the twelve major defect categories, "Experimental Weakness" appears across all 28 evaluated AI-generated papers, with a 100% occurrence rate.
This finding supports our positions regardingimplementation capability limitations, in experimental design, execution, and result analysis. The secondand third most prevalent issues are "Methodological Unclarity/Flaws" (96.4%) and "Writing &amp; PresentationIssues" (92.9%), which reflect AI Scientists' insufficient ability to clearly articulate and implement researchplans. "Novelty Concerns" (89.3%) and "Theoretical Weakness" (85.7%) occur frequently, indicating that
when AI Scientists generate complete papers, they struggle to propose original scientific contributions with solid theoretical foundations.The prevalence of these high-frequency defects highlights systemic issues in the scientific rigor and implementation quality of current AI-generated research, falling below the standards for reliable and valuable scientific outputs.</p>
<p>Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang.
AI.Google'snewai"co-scientist"aimstoacceleratescien-tificdiscovery.Unite.AI,Feb2025.URLhttps://www.unite.ai/googles-new-ai-co-scientist-aims-to-accelerate-scientific-discovery/.
Swe-bench+: Enhanced coding benchmark for llms.arXiv preprint arXiv:2410.06992,2024.</p>
<p>Table 4 :
4
Timeline of AI Scientist Ideas and Code Implementations by Month
2024-082024-092024-102024-112024-122025-012025-022025-032025-04w/o Exp (Zheng et al.,(Ghafarollahi2024)and Buehler,2024), (Raden-sky et al.,2024)
Minjun Zhu and Qiujie Xie contributed equally to this work. Corresponding author(s): Linyi Yang: yanglinyiucd@gmail.com; Yue Zhang: Email zhangyue@westlake.edu.cn
https://ai-researcher.net/social-iclr-2025
AcknowledgementsThe genesis of this position paper traces back to the insightful discussions and interactions at the AI Co-scientist Discussion held in conjunction with ICLR 2025 on April 26, 20241 .We extend our sincere gratitude to the invited speakers, including Chenglei Si, Jindong Wang, Yutaro Yamada, and David Ha, whose perspectives are invaluable.We also deeply appreciate the contributions of the more than 200 participants who engaged in the vibrant discussions on that day; many of the ideas explored in this work were sparked and refined through those collective interactions.We thank every participant for their engagement and for fostering a stimulating environment that significantly shaped our thinking.
Peer review versus bibliometrics: Which method better predicts the scholarly impact of publications?. Giovanni Abramo, Ciriaco Andrea, D' Angelo, Emanuela Reale, Scientometrics. 1212019</p>
<p>A survey on rag with llms. Muhammad Arslan, Hussam Ghanem, Saba Munawar, Christophe Cruz, Procedia Computer Science. 2462024</p>
<p>The relationship between reasoning and performance in large language models-o3 (mini) thinks harder. Marthe Ballon, Andres Algaba, Vincent Ginis, arXiv:2502.156312025not longer. arXiv preprint</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, arXiv:2502.156572025arXiv preprint</p>
<p>Reflective multi-agent collaboration based on large language models. Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, Ji-Rong Wen, Advances in Neural Information Processing Systems. 202437</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Genome modeling and design across all domains of life with evo 2. Garyk Brixi, Matthew G Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A Gonzalez, Samuel H King, David B Li, Aditi T Merchant, BioRxiv. 2025</p>
<p>Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li, Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>Why do multi-agent llm systems fail?. Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, arXiv:2503.136572025arXiv preprint</p>
<p>Exploring scientific hypothesis generation with mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153March 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Corinna Cortes, Neil D Lawrence, arXiv:2109.09774Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. 2021arXiv preprint</p>
<p>A survey on rag meeting llms: Towards retrieval-augmented large language models. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Aniketh Garikaparthi, Manasi Patwardhan, arXiv:2504.16728Lovekesh Vig, and Arman Cohan. Iris: Interactive research ideation system for accelerating scientific discovery. 2025arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, arXiv:2412.141412024aarXiv preprint</p>
<p>Large language models for constructing and optimizing machine learning workflows: A survey. Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, Shiyou Qian, arXiv:2411.104782024barXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Pasa: An llm agent for comprehensive academic paper search. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, arXiv:2501.101202025arXiv preprint</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>An overview of artificial intelligence ethics. Changwu Huang, Zeqi Zhang, Bifei Mao, Xin Yao, IEEE Transactions on Artificial Intelligence. 442022</p>
<p>. Intology. Zochi technical report. arXiv. 2025</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.227082025arXiv preprint</p>
<p>Aide: Ai-driven exploration in the space of code. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu, arXiv:2502.131382025arXiv preprint</p>
<p>Swe-bench: Can language models resolve real-world github issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, ICLR. 2024</p>
<p>The global landscape of ai ethics guidelines. Anna Jobin, Marcello Ienca, Effy Vayena, Nature machine intelligence. 192019</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 59678732021</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Ethics of ai: A systematic literature review of principles and challenges. Arif Ali Khan, Sher Badshah, Peng Liang, Muhammad Waseem, Bilal Khan, Aakash Ahmad, Mahdi Fahmideh, Mahmood Niazi, Muhammad Azeem, Akbar , Proceedings of the 26th international conference on evaluation and assessment in software engineering. the 26th international conference on evaluation and assessment in software engineering2022</p>
<p>The automation of science. Jem Ross D King, Stephen G Rowland, Michael Oliver, Wayne Young, Emma Aubrey, Maria Byrne, Magdalena Liakata, Pinar Markham, Larisa N Pir, Soldatova, Science. 32459232009</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. Patrick Tser, Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen, arXiv:2502.160692025arXiv preprint</p>
<p>Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville, arXiv:2505.06120Llms get lost in multi-turn conversation. 2025arXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, 1987MIT Press</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024aarXiv preprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du, arXiv:2412.146262024barXiv preprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024carXiv preprint</p>
<p>Autoflow: Automated workflow generation for large language model agents. Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang, arXiv:2407.128212024darXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, 202336</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922024aarXiv preprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025arXiv preprint</p>
<p>Aigs: Generating science from ai-powered automated falsification. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, arXiv:2411.119102024barXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. 62479902023</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 262013</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Răileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciucă, arXiv:2504.129762025arXiv preprint</p>
<p>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows. Harshith Padigela, Chintan Shah, Dinkar Juyal, 2025</p>
<p>Position: Episodic memory is the missing piece for long-term llm agents. Mathis Pink, Qinyuan Wu, Ai Vy, Javier Vo, Jianing Turek, Alexander Mu, Mariya Huth, Toneva, arXiv:2502.069752025arXiv preprint</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, Kevin Kj, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025a</p>
<p>Piflow: Principle-aware scientific discovery with multi-agent collaboration. Yingming Pu, Tao Lin, Hongyu Chen, 2025b</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024arXiv preprint</p>
<p>Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, Prasenjit Mitra, Sören Auer, arXiv:2503.193092025arXiv preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>A survey on model context protocol: Architecture, state-of-the-art, challenges and future directions. Partha Pratim, Ray , Authorea Preprints. 2025</p>
<p>Astroagents: A multi-agent ai for hypothesis generation from mass spectrometry data. Daniel Saeedi, Denise Buckner, Jose C Aponte, Amirali Aghazadeh, arXiv:2503.231702025arXiv preprint</p>
<p>Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R Varshney, Dilek Hakkani-Tur, arXiv:2504.20090Spark: A system for scientifically creative idea generation. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227arXiv:2504.17192Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. 2025. 2025arXiv preprintAgent laboratory: Using llm agents as research assistants</p>
<p>Shortcutsbench: A large-scale real-world benchmark for api-based agents. Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma, 2025</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations, ICLR 2025. SingaporeApril 24-28, 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, arXiv:2409.113632024arXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>A deep learning approach to antibiotic discovery. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Shawn Craig R Macnair, Lindsey A French, Zohar Carfrae, Bloom-Ackermann, Cell. 18042020</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Zerosearch: Incentivize the search capability of llms without searching. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang, arXiv:2505.045882025arXiv preprint</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 62479902023</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024a1</p>
<p>Scipip: An llm-based scientific paper idea proposer. Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye, arXiv:2410.231662024barXiv preprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, Advances in Neural Information Processing Systems. 2024c37</p>
<p>Jason Wei, Zhiqing Sun, Spencer Papay, Scott Mckinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, Amelia Glaese, arXiv:2504.12516Browsecomp: A simple yet challenging benchmark for browsing agents. 2025arXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee, arXiv:2503.04723Shifting long-context llms research from input to output. 2025aarXiv preprint</p>
<p>When more is less: Understanding chain-of-thought length in llms. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang, arXiv:2502.072662025barXiv preprint</p>
<p>Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He, arXiv:2504.00255Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. 2025arXiv preprint</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, et al. Qwen3 technical report. 2025aarXiv preprint</p>
<p>A survey of ai agent protocols. Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, arXiv:2504.167362025barXiv preprint</p>
<p>Shennongalpha: an ai-driven sharing and collaboration platform for intelligent curation, acquisition, and translation of natural medicinal material knowledge. Zijie Yang, Yongjing Yin, Chaojun Kong, Tiange Chi, Wufan Tao, Yue Zhang, Tian Xu, Cell Discovery. 111322025c</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>MOOSE-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, The Thirteenth International Conference on Learning Representations. 2025d</p>
<p>Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, arXiv:2412.17767Tao Feng, and Jiaxuan You. Researchtown: Simulator of human research community. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, arXiv:2503.24235A survey on test-time scaling in large language models: What, how, where, and how well?. 2025arXiv preprint</p>
<p>Chain of agents: Large language models collaborating on long-context tasks. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, Sercan Arik, Advances in Neural Information Processing Systems. 202437</p>
<p>Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, arXiv:2408.06941Unleashing ai for accelerated scientific research. 2024arXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
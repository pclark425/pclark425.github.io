<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3016 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3016</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3016</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-265033976</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.1028.pdf" target="_blank">Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data</a></p>
                <p><strong>Paper Abstract:</strong> Numbers are crucial for various real-world domains such as finance, economics, and science. Thus, understanding and reasoning with numbers are essential skills for language models to solve different tasks. While different numerical benchmarks have been introduced in recent years, they are limited to specific numerical aspects mostly. In this paper, we propose a hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. We conduct a comprehensive evaluation of state-of-the-art models to identify reasoning challenges specific to them. Henceforth, we develop a diverse set of numerical probes employing a semi-automated approach. We focus on the tabular Natural Language Inference (TNLI) task as a case study and measure models' performance shifts. Our results show that no model consistently excels across all numerical reasoning types. Among the probed models, FlanT5 (few-/zero-shot) and GPT-3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models. Label-flipping probes indicate that models often exploit dataset artifacts to predict the correct labels.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3016.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3016.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FlanT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (instruction-finetuned T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-finetuned T5 family model evaluated in few- and zero-shot settings; shown strong overall numerical reasoning performance among probed models but sensitive to certain perturbations (e.g., label flips).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FlanT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned variant of the T5 family (sequence-to-sequence transformer); evaluated in few-shot and zero-shot prompting in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic (add/sub/mul/div) probes, arithmetic within table-based NLI, and numerical word problems (TabMWP recast).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Mixture of memorization/retrieval of frequent numeric facts and pattern exploitation; instruction-finetuning and few-shot prompting appear to enable better retrieval/approximate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>FlanT5 ranked among best overall on many numerical probes; few-shot FlanT5 showed strong performance on many types. The paper links strong performance to instruction-finetuning and exposure to chain-of-thought style data in pretraining literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Large performance drops on label-flipping and specific arithmetic probes (e.g., large −71.53% drop on arithmetic probes in one reported comparison, and −78.37% on numeration flipping), indicating FlanT5 often does not apply robust algorithmic arithmetic and can rely on spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Instruction fine-tuning (inherent to model), few-shot prompting (2-shot extrapolation), label-preserving and label-flipping probes used as experimental interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Instruction fine-tuning / few-shot prompting correlated with overall improved numerical reasoning relative to some fine-tuned baselines; however, few-shot FlanT5 still suffered large accuracy drops on flipped arithmetic/numeration probes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as relative accuracy changes vs base hypotheses: few-shot FlanT5 had strong overall scores but experienced a −71.53% drop on arithmetic probes (reported), and a −78.37% drop on numeration flipping probes; on some negative-number probes FlanT5 (zero) improved relative to base. Exact absolute accuracies not provided in prose.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Highly sensitive to label-flipping and small perturbations of numeric tokens; large drops on arithmetic and numeration flips; reliance on dataset artifacts/spurious correlations for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct quantitative comparison to humans or symbolic calculators in this paper; authors note tool-augmentation (calculators) as a promising future direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3016.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large instruction/tuned LLM evaluated in few-shot setting; shows strong overall numerical reasoning on many probes but large drops on complex, multi-step numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer model (GPT-family); described as trained on very large corpora (>300 TB CommonCrawl cited) and evaluated in few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic (add/sub/mul/div) probes in table NLI context, sorting/comparison/range, and complex (multi-step) numerical reasoning probes.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Strong retrieval / memorization of numeric patterns and arithmetic facts from large pretraining corpora; chain-of-thought style pretraining/prompts can aid multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper reports GPT-3.5's average accuracy change across many probe types ≈ −16.7% (modest drop), attributed to large pretraining enabling memorization; authors cite chain-of-thought training as a potential reason for better numerical reasoning on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Large performance drop for complex reasoning probes (−60.22% reported), showing that memorization alone does not substitute for robust multi-step algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting (2-shot extrapolation) used in evaluation; chain-of-thought style pretraining referenced as relevant (not directly applied by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Few-shot prompting yielded relatively strong performance on many probe types compared to fine-tuned baselines, but did not prevent large drops on complex arithmetic/multi-step probes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported aggregated changes: average accuracy change ≈ −16.7% across many reasoning types; complex reasoning probes showed a −60.22% drop. Exact per-task absolute accuracies not listed in prose.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Large accuracy drops on multi-step/complex arithmetic despite good single-step performance; sensitivity to label flips and numeric perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; authors note GPT-3.5's large pretraining corpus may allow memorization of numeric data, unlike algorithmic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3016.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (Pathways Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large LLM evaluated in zero-shot setting; shows substantial performance drops on arithmetic probes in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based language model (Pathways/PaLM family) evaluated in zero-shot prompting configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic probes and other numerical reasoning probes within table NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Likely retrieval/memorization from pretraining; limited algorithmic arithmetic capacity in zero-shot configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Significant performance drops reported on numeration and flipped probes (approx −35% to −80% depending on probe), implying reliance on learned correlations rather than robust arithmetic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None directly provided; paper only reports large negative effect sizes on flipped numeric probes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Zero-shot prompting (no few-shot examples provided in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Zero-shot setting led to large accuracy reductions on arithmetic/numeration flipping probes (PaLM reported ~−67.67% in discussion similar to other LLMs), indicating limited arithmetic robustness without further prompting/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported example drops: PaLM (zero) performance on arithmetic probes dropped by approx −67.67% (as reported in discussion). Other probe-specific large drops for numeration/negative probes also mentioned (e.g., ~−35% to −80% depending on probe category).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Very large drops when numeric tokens are perturbed or flipped; unstable on negative-number representations and label-flipping probes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; authors suggest augmenting LMs with tools (calculators) for reliable arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3016.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LUNA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LUNA (Language Understanding with Number Augmentations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based model adapted for numeracy via specialized tokenization that encodes numbers as single tokens; shows mixed results — sometimes better on arithmetic but not uniformly superior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LUNA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based model with adapted tokenization for numbers (encodes whole numeric strings as single tokens) designed to improve numerical processing.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic (add/sub/mul/div) probes, comparison, sorting, and negative number probes within table NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Internal numeric tokenization (representing numbers as single tokens) intended to improve numeric encoding and thus arithmetic behavior; potential for learned retrieval of numeric patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>LUNA sometimes outperformed other models on specific probes (e.g., slight improvement on comparison flipping probes and slight increase on arithmetic probes relative to fine-tuned baselines), suggesting tokenization helps certain numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>LUNA experienced significant performance drops on heterogeneous and negative-number probes (large drops reported), indicating the tokenization alone does not solve all numeric reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural/tokenization intervention (numeric tokens treated as single tokens) and numerical pretraining objectives referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Tokenization correlated with improved performance on some comparison/arithmetic probes but large declines on several other numeric probe types (heterogeneous and negative numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports LUNA had a slight performance increase on arithmetic probes while showing major drops on heterogeneous and negative-number probes (drop magnitudes reported in ranges but exact numbers for LUNA vary across tables).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Large performance drops on negative number probes and heterogeneous number formats; inconsistent gains across arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; LUNA's design is an attempt to make internal representations more like symbolic numeric tokens but the paper does not benchmark vs symbolic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3016.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NT5 (Numeracy-tuned T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T5 variant modified for numerical reasoning via additional numerical pretraining objectives and fine-tuning; shows mixed performance depending on probe type.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variation of T5 that was modified for numerical reasoning with extra pretraining objectives and fine-tuning on numeric datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple arithmetic and table-based arithmetic probes (TATQA-derived), as well as heterogeneous number formats.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Numerical pretraining objectives intended to shape internal representations to better support arithmetic (learning via supervised/unsupervised numeric tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>NT5 performed relatively well on some heterogeneous probes and comparison probes (smaller performance drops compared to models without numeric pretraining), suggesting pretraining objectives helped.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>NT5 still suffered large drops on negative-number probes and did not uniformly outperform other models across all arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Numerical pretraining objectives and fine-tuning on numeric datasets; label-preserving and label-flipping probes used in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Numerical pretraining produced modest robustness gains on certain representation and heterogeneous-number tasks, but did not eliminate failures on flips and negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported smaller drops on heterogeneous probes (e.g., NT5 ~−3% on some heterogeneous evaluations) but significant drops on negative-number flips (in the range of −38.87% to −71.35% for some models in that category; NT5 specifically flagged among models with large negative-number drops).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Sensitivity to negative-number formats and label-flipping; incomplete generalization from pretraining objectives to all arithmetic perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3016.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PASTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PASTA (Table-operations aware fact verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeBERTa-based model pretrained with objectives using table numeric operations; intended to be table- and number-aware but shows mixed results on arithmetic probes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PASTA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTa-based model pretrained with table-operation-aware objectives to improve table and numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeric scale, approximation, arithmetic operations and other table NLI arithmetic probes.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pretraining with table-operation objectives to learn to apply numeric operations over tabular data rather than memorizing surface patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>PASTA sometimes better on numeric and table-specific tasks relative to non-table-specialized models, but still shows declines on approximation and flipping probes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>PASTA showed significant performance drops on approximation and flipping probes (e.g., approximation drops by −27.44% for PASTA in the paper), indicating pretraining did not fully solve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Table-operation-aware pretraining objectives and fine-tuning on table NLI data.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved relative performance on some table-aware reasoning types but still vulnerable to perturbations like label flips and approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PASTA performance on approximation probes dropped by −27.44% (reported); heterogeneous and flipping probes also showed large declines for PASTA and similar models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Struggles with approximation and label-flipping; not uniformly robust across arithmetic probe types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3016.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPAS (Table Parsing BERT extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-extended encoder with table-specific embeddings trained for table understanding; used here with intermediate pretraining on synthetic/counterfactual data and evaluated on numeric probes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TAPAS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extension of BERT for table understanding that adds table-aware embeddings; in this study TAPAS was used with intermediate pretraining on synthetic/counterfactual data.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic operations in tables, sorting, comparison, negative numbers, and counterfactual table probes.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Table-structured embeddings and pretraining on counterfactual/synthetic data intended to ground reasoning in table structure rather than pure memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>TAPAS benefited from counterfactual table probes in some cases and improved (counterintuitively) on certain edited tables, indicating reliance on table patterns and potential label bias in original data.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>TAPAS showed very large drops on negative-number probes and numeration flipping probes (e.g., TAPAS among models with drops between −38.87% and −71.35% on negative probes), indicating instability when numeric representation changes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Intermediate pretraining on synthetic/counterfactual data; table counterfactual probes used as an evaluation intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Counterfactual table evaluation sometimes improved TAPAS performance relative to base, revealing label bias in datasets rather than genuine numeric reasoning improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>TAPAS experienced significant performance drops on negative-number probes (in ranges reported between −38.87% and −71.35% for several models); the paper notes TAPAS and DeBERTa improved on some counterfactual table probes, suggesting label bias exploitation by models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Exploits dataset artifacts/label biases; vulnerable to negative-number representations and numeration flips.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; TAPAS is specialized to table structure rather than symbolic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3016.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPEX (Table pretraining via neural SQL executor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BART-based model pretrained to mimic a neural SQL executor over tables; evaluated on table NLI numeric probes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Table-focused model using BART backbone trained to emulate SQL-like operations over tables to improve table-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic over tables (simple operations), complex reasoning over tabular numeric data.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learning table-to-operation mappings (SQL-like) to perform arithmetic-like computations over table-structured input.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>TAPEX is designed to learn numeric operations via pretraining; in the paper it is evaluated and shows variable performance across probe types but specific numeric gains are mixed.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Paper reports TAPEX (and other table models) suffer large drops on flipped representation probes, indicating limited generalization from learned table-operation mappings to perturbed numeric inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Table-SQL-style pretraining and fine-tuning; label-flipping and counterfactual probes as evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Pretraining intended to offer algorithmic mapping benefits, but TAPEX still experiences significant degradation under perturbations (no uniform improvement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Shown to have large relative drops on label-flipping probes in representation category (exact TAPEX numbers not singled out in prose), and generally variable performance compared to other table models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Sensitive to representation changes and label flips; may exploit dataset patterns rather than robust computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3016.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReasTAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReasTAP (Table reasoning pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BART-based model pretrained on synthetic examples requiring multiple table reasoning skills including numeric tasks; excels on some complex reasoning probes relative to many LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReasTAP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART-based model pretrained on synthetically generated data to inject seven table reasoning skills (including numeric operations and temporal reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Complex reasoning over tables including multi-step numeric reasoning and numerical word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pretraining on synthetic, targeted reasoning examples to build procedural/algorithmic capabilities for table-based numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>ReasTAP achieved the highest accuracy on complex reasoning probes among fine-tuned models, indicating that targeted pretraining on reasoning tasks can improve multi-step numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite strong complex-reasoning performance, ReasTAP still suffers on some representation flipping probes indicating limited invariance to numeric format changes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Synthetic reasoning pretraining (targeted table numerical operations) and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantially improved performance on complex reasoning probes (ReasTAP highest among evaluated models on that category), demonstrating that targeted pretraining helps multi-step arithmetic reasoning in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ranked highest on complex reasoning probes among fine-tuned models; however exact numeric accuracies are not provided in the paper's prose (relative accuracy changes are reported elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Vulnerable to numeric representation perturbations and label-flipping despite strong complex-reasoning skills.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3016.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3016.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (Decoding-enhanced BERT with disentangled attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer encoder (DeBERTa) fine-tuned on NLI datasets used as a baseline tabular model; shows robustness on some probes and mixed performance on arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTa encoder pretrained on standard corpora (reported 78GB in paper) and fine-tuned on multiple NLI datasets; used as a tabular/fine-tuned baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Comparison, sorting, scale conversion, and arithmetic-related probes in table NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Fine-tuning on NLI datasets leads to learning dataset-specific correlations and some table grounding; no special numeric tokenization or numeric pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DeBERTa demonstrated relative robustness in some categories (e.g., scale non-flip probes) and less sensitivity to some flips compared to other fine-tuned or numerically-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>DeBERTa still suffers large drops on many flipping probes (e.g., scale flipping −64.58% in a cited example) demonstrating reliance on dataset cues and limited general numeric algorithmic ability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning on NLI datasets; counterfactual table probes used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning provided reasonable base performance on many tasks; counterfactual table probes sometimes led to improved scores, revealing label biases in datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DeBERTa showed relatively small drop on some non-flipping scale probes (−6.25%) but large drops on flipping counterparts (e.g., −64.58% reported for scale flips). Sorting probes: DeBERTa among few models that did not drop.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Exploits dataset artifacts/label biases; sensitive to numeric-format flips for many probe types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Representing numbers in NLP: a survey and a vision <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 2)</em></li>
                <li>LUNA: language understanding with number augmentations on transformers via number plugins and pre-training <em>(Rating: 2)</em></li>
                <li>NT5?! training T5 to perform numerical reasoning <em>(Rating: 2)</em></li>
                <li>PASTA: Table-operations aware fact verification via sentence-table cloze pre-training <em>(Rating: 2)</em></li>
                <li>TAPAS: Weakly supervised table parsing via pre-training <em>(Rating: 1)</em></li>
                <li>TAPEX: table pre-training via learning a neural SQL executor <em>(Rating: 1)</em></li>
                <li>How well do computers solve math word problems? large-scale dataset construction and evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3016",
    "paper_id": "paper-265033976",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "FlanT5",
            "name_full": "Flan-T5 (instruction-finetuned T5)",
            "brief_description": "Instruction-finetuned T5 family model evaluated in few- and zero-shot settings; shown strong overall numerical reasoning performance among probed models but sensitive to certain perturbations (e.g., label flips).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FlanT5",
            "model_description": "Instruction-finetuned variant of the T5 family (sequence-to-sequence transformer); evaluated in few-shot and zero-shot prompting in this study.",
            "arithmetic_task_type": "Simple arithmetic (add/sub/mul/div) probes, arithmetic within table-based NLI, and numerical word problems (TabMWP recast).",
            "reported_mechanism": "Mixture of memorization/retrieval of frequent numeric facts and pattern exploitation; instruction-finetuning and few-shot prompting appear to enable better retrieval/approximate reasoning.",
            "evidence_for_mechanism": "FlanT5 ranked among best overall on many numerical probes; few-shot FlanT5 showed strong performance on many types. The paper links strong performance to instruction-finetuning and exposure to chain-of-thought style data in pretraining literature.",
            "evidence_against_mechanism": "Large performance drops on label-flipping and specific arithmetic probes (e.g., large −71.53% drop on arithmetic probes in one reported comparison, and −78.37% on numeration flipping), indicating FlanT5 often does not apply robust algorithmic arithmetic and can rely on spurious cues.",
            "intervention_type": "Instruction fine-tuning (inherent to model), few-shot prompting (2-shot extrapolation), label-preserving and label-flipping probes used as experimental interventions.",
            "effect_of_intervention": "Instruction fine-tuning / few-shot prompting correlated with overall improved numerical reasoning relative to some fine-tuned baselines; however, few-shot FlanT5 still suffered large accuracy drops on flipped arithmetic/numeration probes.",
            "performance_metrics": "Reported as relative accuracy changes vs base hypotheses: few-shot FlanT5 had strong overall scores but experienced a −71.53% drop on arithmetic probes (reported), and a −78.37% drop on numeration flipping probes; on some negative-number probes FlanT5 (zero) improved relative to base. Exact absolute accuracies not provided in prose.",
            "notable_failure_modes": "Highly sensitive to label-flipping and small perturbations of numeric tokens; large drops on arithmetic and numeration flips; reliance on dataset artifacts/spurious correlations for some tasks.",
            "comparison_to_humans_or_symbolic": "No direct quantitative comparison to humans or symbolic calculators in this paper; authors note tool-augmentation (calculators) as a promising future direction.",
            "uuid": "e3016.0",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "Large instruction/tuned LLM evaluated in few-shot setting; shows strong overall numerical reasoning on many probes but large drops on complex, multi-step numerical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Large autoregressive transformer model (GPT-family); described as trained on very large corpora (&gt;300 TB CommonCrawl cited) and evaluated in few-shot prompting.",
            "arithmetic_task_type": "Simple arithmetic (add/sub/mul/div) probes in table NLI context, sorting/comparison/range, and complex (multi-step) numerical reasoning probes.",
            "reported_mechanism": "Strong retrieval / memorization of numeric patterns and arithmetic facts from large pretraining corpora; chain-of-thought style pretraining/prompts can aid multi-step reasoning.",
            "evidence_for_mechanism": "Paper reports GPT-3.5's average accuracy change across many probe types ≈ −16.7% (modest drop), attributed to large pretraining enabling memorization; authors cite chain-of-thought training as a potential reason for better numerical reasoning on some tasks.",
            "evidence_against_mechanism": "Large performance drop for complex reasoning probes (−60.22% reported), showing that memorization alone does not substitute for robust multi-step algorithmic computation.",
            "intervention_type": "Few-shot prompting (2-shot extrapolation) used in evaluation; chain-of-thought style pretraining referenced as relevant (not directly applied by authors).",
            "effect_of_intervention": "Few-shot prompting yielded relatively strong performance on many probe types compared to fine-tuned baselines, but did not prevent large drops on complex arithmetic/multi-step probes.",
            "performance_metrics": "Reported aggregated changes: average accuracy change ≈ −16.7% across many reasoning types; complex reasoning probes showed a −60.22% drop. Exact per-task absolute accuracies not listed in prose.",
            "notable_failure_modes": "Large accuracy drops on multi-step/complex arithmetic despite good single-step performance; sensitivity to label flips and numeric perturbations.",
            "comparison_to_humans_or_symbolic": "No direct comparison; authors note GPT-3.5's large pretraining corpus may allow memorization of numeric data, unlike algorithmic calculators.",
            "uuid": "e3016.1",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PaLM",
            "name_full": "PaLM (Pathways Language Model)",
            "brief_description": "Large LLM evaluated in zero-shot setting; shows substantial performance drops on arithmetic probes in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_description": "Large transformer-based language model (Pathways/PaLM family) evaluated in zero-shot prompting configuration.",
            "arithmetic_task_type": "Simple arithmetic probes and other numerical reasoning probes within table NLI.",
            "reported_mechanism": "Likely retrieval/memorization from pretraining; limited algorithmic arithmetic capacity in zero-shot configuration.",
            "evidence_for_mechanism": "Significant performance drops reported on numeration and flipped probes (approx −35% to −80% depending on probe), implying reliance on learned correlations rather than robust arithmetic execution.",
            "evidence_against_mechanism": "None directly provided; paper only reports large negative effect sizes on flipped numeric probes.",
            "intervention_type": "Zero-shot prompting (no few-shot examples provided in main experiments).",
            "effect_of_intervention": "Zero-shot setting led to large accuracy reductions on arithmetic/numeration flipping probes (PaLM reported ~−67.67% in discussion similar to other LLMs), indicating limited arithmetic robustness without further prompting/fine-tuning.",
            "performance_metrics": "Reported example drops: PaLM (zero) performance on arithmetic probes dropped by approx −67.67% (as reported in discussion). Other probe-specific large drops for numeration/negative probes also mentioned (e.g., ~−35% to −80% depending on probe category).",
            "notable_failure_modes": "Very large drops when numeric tokens are perturbed or flipped; unstable on negative-number representations and label-flipping probes.",
            "comparison_to_humans_or_symbolic": "No direct comparison; authors suggest augmenting LMs with tools (calculators) for reliable arithmetic.",
            "uuid": "e3016.2",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LUNA",
            "name_full": "LUNA (Language Understanding with Number Augmentations)",
            "brief_description": "Transformer-based model adapted for numeracy via specialized tokenization that encodes numbers as single tokens; shows mixed results — sometimes better on arithmetic but not uniformly superior.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LUNA",
            "model_description": "Transformer-based model with adapted tokenization for numbers (encodes whole numeric strings as single tokens) designed to improve numerical processing.",
            "arithmetic_task_type": "Simple arithmetic (add/sub/mul/div) probes, comparison, sorting, and negative number probes within table NLI.",
            "reported_mechanism": "Internal numeric tokenization (representing numbers as single tokens) intended to improve numeric encoding and thus arithmetic behavior; potential for learned retrieval of numeric patterns.",
            "evidence_for_mechanism": "LUNA sometimes outperformed other models on specific probes (e.g., slight improvement on comparison flipping probes and slight increase on arithmetic probes relative to fine-tuned baselines), suggesting tokenization helps certain numeric tasks.",
            "evidence_against_mechanism": "LUNA experienced significant performance drops on heterogeneous and negative-number probes (large drops reported), indicating the tokenization alone does not solve all numeric reasoning failures.",
            "intervention_type": "Architectural/tokenization intervention (numeric tokens treated as single tokens) and numerical pretraining objectives referenced.",
            "effect_of_intervention": "Tokenization correlated with improved performance on some comparison/arithmetic probes but large declines on several other numeric probe types (heterogeneous and negative numbers).",
            "performance_metrics": "The paper reports LUNA had a slight performance increase on arithmetic probes while showing major drops on heterogeneous and negative-number probes (drop magnitudes reported in ranges but exact numbers for LUNA vary across tables).",
            "notable_failure_modes": "Large performance drops on negative number probes and heterogeneous number formats; inconsistent gains across arithmetic tasks.",
            "comparison_to_humans_or_symbolic": "No direct comparison; LUNA's design is an attempt to make internal representations more like symbolic numeric tokens but the paper does not benchmark vs symbolic calculators.",
            "uuid": "e3016.3",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "NT5",
            "name_full": "NT5 (Numeracy-tuned T5)",
            "brief_description": "T5 variant modified for numerical reasoning via additional numerical pretraining objectives and fine-tuning; shows mixed performance depending on probe type.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "NT5",
            "model_description": "Variation of T5 that was modified for numerical reasoning with extra pretraining objectives and fine-tuning on numeric datasets.",
            "arithmetic_task_type": "Simple arithmetic and table-based arithmetic probes (TATQA-derived), as well as heterogeneous number formats.",
            "reported_mechanism": "Numerical pretraining objectives intended to shape internal representations to better support arithmetic (learning via supervised/unsupervised numeric tasks).",
            "evidence_for_mechanism": "NT5 performed relatively well on some heterogeneous probes and comparison probes (smaller performance drops compared to models without numeric pretraining), suggesting pretraining objectives helped.",
            "evidence_against_mechanism": "NT5 still suffered large drops on negative-number probes and did not uniformly outperform other models across all arithmetic tasks.",
            "intervention_type": "Numerical pretraining objectives and fine-tuning on numeric datasets; label-preserving and label-flipping probes used in evaluation.",
            "effect_of_intervention": "Numerical pretraining produced modest robustness gains on certain representation and heterogeneous-number tasks, but did not eliminate failures on flips and negatives.",
            "performance_metrics": "Reported smaller drops on heterogeneous probes (e.g., NT5 ~−3% on some heterogeneous evaluations) but significant drops on negative-number flips (in the range of −38.87% to −71.35% for some models in that category; NT5 specifically flagged among models with large negative-number drops).",
            "notable_failure_modes": "Sensitivity to negative-number formats and label-flipping; incomplete generalization from pretraining objectives to all arithmetic perturbations.",
            "comparison_to_humans_or_symbolic": "No direct comparison provided.",
            "uuid": "e3016.4",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "PASTA",
            "name_full": "PASTA (Table-operations aware fact verification)",
            "brief_description": "DeBERTa-based model pretrained with objectives using table numeric operations; intended to be table- and number-aware but shows mixed results on arithmetic probes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PASTA",
            "model_description": "DeBERTa-based model pretrained with table-operation-aware objectives to improve table and numeric reasoning.",
            "arithmetic_task_type": "Numeric scale, approximation, arithmetic operations and other table NLI arithmetic probes.",
            "reported_mechanism": "Pretraining with table-operation objectives to learn to apply numeric operations over tabular data rather than memorizing surface patterns.",
            "evidence_for_mechanism": "PASTA sometimes better on numeric and table-specific tasks relative to non-table-specialized models, but still shows declines on approximation and flipping probes.",
            "evidence_against_mechanism": "PASTA showed significant performance drops on approximation and flipping probes (e.g., approximation drops by −27.44% for PASTA in the paper), indicating pretraining did not fully solve robustness.",
            "intervention_type": "Table-operation-aware pretraining objectives and fine-tuning on table NLI data.",
            "effect_of_intervention": "Improved relative performance on some table-aware reasoning types but still vulnerable to perturbations like label flips and approximations.",
            "performance_metrics": "PASTA performance on approximation probes dropped by −27.44% (reported); heterogeneous and flipping probes also showed large declines for PASTA and similar models.",
            "notable_failure_modes": "Struggles with approximation and label-flipping; not uniformly robust across arithmetic probe types.",
            "comparison_to_humans_or_symbolic": "No direct comparison provided.",
            "uuid": "e3016.5",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "TAPAS",
            "name_full": "TAPAS (Table Parsing BERT extension)",
            "brief_description": "BERT-extended encoder with table-specific embeddings trained for table understanding; used here with intermediate pretraining on synthetic/counterfactual data and evaluated on numeric probes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TAPAS",
            "model_description": "Extension of BERT for table understanding that adds table-aware embeddings; in this study TAPAS was used with intermediate pretraining on synthetic/counterfactual data.",
            "arithmetic_task_type": "Arithmetic operations in tables, sorting, comparison, negative numbers, and counterfactual table probes.",
            "reported_mechanism": "Table-structured embeddings and pretraining on counterfactual/synthetic data intended to ground reasoning in table structure rather than pure memorization.",
            "evidence_for_mechanism": "TAPAS benefited from counterfactual table probes in some cases and improved (counterintuitively) on certain edited tables, indicating reliance on table patterns and potential label bias in original data.",
            "evidence_against_mechanism": "TAPAS showed very large drops on negative-number probes and numeration flipping probes (e.g., TAPAS among models with drops between −38.87% and −71.35% on negative probes), indicating instability when numeric representation changes.",
            "intervention_type": "Intermediate pretraining on synthetic/counterfactual data; table counterfactual probes used as an evaluation intervention.",
            "effect_of_intervention": "Counterfactual table evaluation sometimes improved TAPAS performance relative to base, revealing label bias in datasets rather than genuine numeric reasoning improvements.",
            "performance_metrics": "TAPAS experienced significant performance drops on negative-number probes (in ranges reported between −38.87% and −71.35% for several models); the paper notes TAPAS and DeBERTa improved on some counterfactual table probes, suggesting label bias exploitation by models.",
            "notable_failure_modes": "Exploits dataset artifacts/label biases; vulnerable to negative-number representations and numeration flips.",
            "comparison_to_humans_or_symbolic": "No direct comparison; TAPAS is specialized to table structure rather than symbolic calculators.",
            "uuid": "e3016.6",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "TAPEX",
            "name_full": "TAPEX (Table pretraining via neural SQL executor)",
            "brief_description": "BART-based model pretrained to mimic a neural SQL executor over tables; evaluated on table NLI numeric probes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TAPEX",
            "model_description": "Table-focused model using BART backbone trained to emulate SQL-like operations over tables to improve table-based reasoning.",
            "arithmetic_task_type": "Arithmetic over tables (simple operations), complex reasoning over tabular numeric data.",
            "reported_mechanism": "Learning table-to-operation mappings (SQL-like) to perform arithmetic-like computations over table-structured input.",
            "evidence_for_mechanism": "TAPEX is designed to learn numeric operations via pretraining; in the paper it is evaluated and shows variable performance across probe types but specific numeric gains are mixed.",
            "evidence_against_mechanism": "Paper reports TAPEX (and other table models) suffer large drops on flipped representation probes, indicating limited generalization from learned table-operation mappings to perturbed numeric inputs.",
            "intervention_type": "Table-SQL-style pretraining and fine-tuning; label-flipping and counterfactual probes as evaluations.",
            "effect_of_intervention": "Pretraining intended to offer algorithmic mapping benefits, but TAPEX still experiences significant degradation under perturbations (no uniform improvement reported).",
            "performance_metrics": "Shown to have large relative drops on label-flipping probes in representation category (exact TAPEX numbers not singled out in prose), and generally variable performance compared to other table models.",
            "notable_failure_modes": "Sensitive to representation changes and label flips; may exploit dataset patterns rather than robust computation.",
            "comparison_to_humans_or_symbolic": "No direct comparison.",
            "uuid": "e3016.7",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReasTAP",
            "name_full": "ReasTAP (Table reasoning pretraining)",
            "brief_description": "BART-based model pretrained on synthetic examples requiring multiple table reasoning skills including numeric tasks; excels on some complex reasoning probes relative to many LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ReasTAP",
            "model_description": "BART-based model pretrained on synthetically generated data to inject seven table reasoning skills (including numeric operations and temporal reasoning).",
            "arithmetic_task_type": "Complex reasoning over tables including multi-step numeric reasoning and numerical word problems.",
            "reported_mechanism": "Pretraining on synthetic, targeted reasoning examples to build procedural/algorithmic capabilities for table-based numeric reasoning.",
            "evidence_for_mechanism": "ReasTAP achieved the highest accuracy on complex reasoning probes among fine-tuned models, indicating that targeted pretraining on reasoning tasks can improve multi-step numeric reasoning.",
            "evidence_against_mechanism": "Despite strong complex-reasoning performance, ReasTAP still suffers on some representation flipping probes indicating limited invariance to numeric format changes.",
            "intervention_type": "Synthetic reasoning pretraining (targeted table numerical operations) and fine-tuning.",
            "effect_of_intervention": "Substantially improved performance on complex reasoning probes (ReasTAP highest among evaluated models on that category), demonstrating that targeted pretraining helps multi-step arithmetic reasoning in tables.",
            "performance_metrics": "Ranked highest on complex reasoning probes among fine-tuned models; however exact numeric accuracies are not provided in the paper's prose (relative accuracy changes are reported elsewhere).",
            "notable_failure_modes": "Vulnerable to numeric representation perturbations and label-flipping despite strong complex-reasoning skills.",
            "comparison_to_humans_or_symbolic": "No direct comparison.",
            "uuid": "e3016.8",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "DeBERTa",
            "name_full": "DeBERTa (Decoding-enhanced BERT with disentangled attention)",
            "brief_description": "Transformer encoder (DeBERTa) fine-tuned on NLI datasets used as a baseline tabular model; shows robustness on some probes and mixed performance on arithmetic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa",
            "model_description": "DeBERTa encoder pretrained on standard corpora (reported 78GB in paper) and fine-tuned on multiple NLI datasets; used as a tabular/fine-tuned baseline.",
            "arithmetic_task_type": "Comparison, sorting, scale conversion, and arithmetic-related probes in table NLI.",
            "reported_mechanism": "Fine-tuning on NLI datasets leads to learning dataset-specific correlations and some table grounding; no special numeric tokenization or numeric pretraining.",
            "evidence_for_mechanism": "DeBERTa demonstrated relative robustness in some categories (e.g., scale non-flip probes) and less sensitivity to some flips compared to other fine-tuned or numerically-tuned models.",
            "evidence_against_mechanism": "DeBERTa still suffers large drops on many flipping probes (e.g., scale flipping −64.58% in a cited example) demonstrating reliance on dataset cues and limited general numeric algorithmic ability.",
            "intervention_type": "Fine-tuning on NLI datasets; counterfactual table probes used for evaluation.",
            "effect_of_intervention": "Fine-tuning provided reasonable base performance on many tasks; counterfactual table probes sometimes led to improved scores, revealing label biases in datasets.",
            "performance_metrics": "DeBERTa showed relatively small drop on some non-flipping scale probes (−6.25%) but large drops on flipping counterparts (e.g., −64.58% reported for scale flips). Sorting probes: DeBERTa among few models that did not drop.",
            "notable_failure_modes": "Exploits dataset artifacts/label biases; sensitive to numeric-format flips for many probe types.",
            "comparison_to_humans_or_symbolic": "No direct comparison.",
            "uuid": "e3016.9",
            "source_info": {
                "paper_title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Representing numbers in NLP: a survey and a vision",
            "rating": 2,
            "sanitized_title": "representing_numbers_in_nlp_a_survey_and_a_vision"
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2,
            "sanitized_title": "injecting_numerical_reasoning_skills_into_language_models"
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 2,
            "sanitized_title": "evaluating_transformer_language_models_on_arithmetic_operations_using_number_decomposition"
        },
        {
            "paper_title": "LUNA: language understanding with number augmentations on transformers via number plugins and pre-training",
            "rating": 2,
            "sanitized_title": "luna_language_understanding_with_number_augmentations_on_transformers_via_number_plugins_and_pretraining"
        },
        {
            "paper_title": "NT5?! training T5 to perform numerical reasoning",
            "rating": 2,
            "sanitized_title": "nt5_training_t5_to_perform_numerical_reasoning"
        },
        {
            "paper_title": "PASTA: Table-operations aware fact verification via sentence-table cloze pre-training",
            "rating": 2,
            "sanitized_title": "pasta_tableoperations_aware_fact_verification_via_sentencetable_cloze_pretraining"
        },
        {
            "paper_title": "TAPAS: Weakly supervised table parsing via pre-training",
            "rating": 1,
            "sanitized_title": "tapas_weakly_supervised_table_parsing_via_pretraining"
        },
        {
            "paper_title": "TAPEX: table pre-training via learning a neural SQL executor",
            "rating": 1,
            "sanitized_title": "tapex_table_pretraining_via_learning_a_neural_sql_executor"
        },
        {
            "paper_title": "How well do computers solve math word problems? large-scale dataset construction and evaluation",
            "rating": 1,
            "sanitized_title": "how_well_do_computers_solve_math_word_problems_largescale_dataset_construction_and_evaluation"
        }
    ],
    "cost": 0.01802625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data
June 20, 2003</p>
<p>Mubashara Akhtar mubashara.akhtar@kcl.ac.uk 
King's College London</p>
<p>Abhilash Shankarampeta abhilash.shankarampeta@meesho.com 
Meesho</p>
<p>Vivek Gupta 
University of Pennsylvania</p>
<p>Arpit Patil 
University of Utah</p>
<p>Oana Cocarascu 
King's College London</p>
<p>Elena Simperl 
King's College London</p>
<p>Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data
June 20, 20037D42554B0DD0BD1D157FD46E460DB755
Numbers are crucial for various real-world domains such as finance, economics, and science.Thus, understanding and reasoning with numbers are essential skills for language models to solve different tasks.While different numerical benchmarks have been introduced in recent years, they are limited to specific numerical aspects mostly.In this paper, we propose a hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning.We conduct a comprehensive evaluation of state-of-the-art models to identify reasoning challenges specific to them.Henceforth, we develop a diverse set of numerical probes employing a semi-automated approach.We focus on the tabular Natural Language Inference (TNLI) task as a case study and measure models' performance shifts.Our results show that no model consistently excels across all numerical reasoning types.Among the probed models, FlanT5 (few-/zero-shot) and GPT-3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models.Label-flipping probes indicate that models often exploit dataset artifacts to predict the correct labels. 1 * Equal contributions 1 Data and code are available at https://github.com/mubasharaak/numerical_reasoning.</p>
<p>Introduction</p>
<p>Numerical data is ubiquitous in the real-world.Many applications in domains such as finance, economics and science require understanding and reasoning with numbers.In recent years, benchmarks were introduced to study language models' numeracy skills (Zhang et al., 2020;Wallace et al., 2019;Dua et al., 2019).However, these datasets mostly concentrate on few, specific numerical reasoning types (e.g.scales (Zhang et al., 2020)).Moreover, evaluating models on numerical benchmarks, it often remains unclear why models struggle with the tasks.For example, the issues can arise from models struggling to recognize numerical representations in text, failing to compute arithmetic operations, or predicting incorrect outputs due to a lack of numerical commonsense knowledge.We aim to explore these questions in greater detail in this study.Limitations of language models' numerical abilities, as discussed in prior research, include tokenization and representation of numbers in text (Thawani et al., 2021b), hallucination (Ji et al., 2023;Chen et al., 2023;Ye et al., 2023), and generalizability/robustness issues (Razeghi et al., 2022;Geva et al., 2020;Xu et al., 2022).</p>
<p>Successful numerical reasoning requires a combination of skillsets: understanding representation of numbers (Thawani et al., 2021a,b) and their meaning in a given context (Loukas et al., 2022), applying operations (Geva et al., 2020;Patel et al., 2021), and integrating factual and commonsense numerical knowledge to solve numerical problems (Lin et al., 2020;Park et al., 2022).For example, classifying the hypotheses "The movie can be watched in about 2 (or 'two') hours."from Table 1 requires understanding that both "2" and "two" depict the same numerical value, converting "2 hours" to another unit (i.e. 120 minutes), and applying approximation to map "120 minutes" to "138 minutes" in the table.</p>
<p>In this paper, we evaluate state-of-the-art models on various numerical reasoning types.To assess which reasoning types are challenging for specific models, we create a diverse and large set of numerical probes and measure shifts in models' performance.We organize all probed reasoning types in a hierarchical taxonomy.Inspired by how humans understand and reason with numbers, as well as previous numerical benchmarks, we include eleven reasoning types across four level: representation, number sense, manipulation, and complex reasoning (Figure 1).We apply a semi-automated approaches for probe creation.We select tabular NLI (TNLI) as a case study task, given three criteria: (i) numerical data (numbers, percentages, dates, etc.) is prevalent in tables; (ii) tables are common in real-world data sources such as in scientific publications, database systems and financial documents; (iii) tables as structured data facilitate automated perturbations to create large-scale probing sets.See Table 1 for some examples of probes created from hypotheses (H1, H2, H3) and the given table .Our experiments conclude that large language models (LLMs) like FlanT5 and GPT3.5 perform better than other models on various numerical reasoning tasks.Both table-based and numerical models struggled to understand data with flipped labels and negative values.Moreover, we observe that some models' performance improves significantly for counterfactual probes (e.g.NT5 and TAPAS) and label-flipping probes (e.g.FlanT5 zero-shot), which indicates that models might exploit dataset artifacts and are biased towards one label.These findings emphasize the importance of further systematically investigating numerical reasoning capabilities across various NLP models.Our contributions are as follows:</p>
<p>• We introduce a taxonomy for numerical reasoning skills, including representation/number sense/manipulation skills and complex reasoning with numbers.</p>
<p>• We propose a semi-automated approach to create large-scale, numerical probe sets using table NLI datasets.</p>
<p>• We evaluate three different categories of language models (LMs) on our numerical probe sets: (i) numerical LMs; (ii) LMs for tabular data; and (iii) zero-/few-shot LMs.</p>
<p>A Taxonomy for Numerical Reasoning</p>
<p>This section introduces a hierarchical taxonomy for numerical reasoning, inspired by previous works on numeracy in NLP (Thawani et al., 2021b;Xu et al., 2022) and psychology (Barrouillet and Fayol, 1998a;Whyte and Bull, 2008;Bofferding, 2019).We group numerical reasoning skills given their complexity level in four categories: R1 − R4.</p>
<p>Number Representation (R1)</p>
<p>This category includes skills for understanding the form of numerical data.Similar to the notion of form in language (Bender and Koller, 2020), this is the realization of numbers in text; the way they are represented and expressed.</p>
<p>Numeration.Numeration studies language model's understanding of representation systems common for numbers in English: the Arabic ("2") and English ("two") numeration systems.Specifically, we probe if LMs can link between distinct symbols used for the same number.For example in Figure 1, H3 contains "two" as a word, which can be also represented through "2".</p>
<p>Heterogeneous Number Types.Formatted numbers (e.g.dates, times, and fractions) are frequently used to convey additional information associated with a numerical value.Numbers are formatted in a specific way given their context and purpose, such as expressing times and dates using full-stop ("."), using the "%" symbol to indicate fractions, and different currency symbols for money (i.e."$" or "C"), e.g.H1 and "Arith" in Figure 1.</p>
<p>Negative Numbers.Early on in their development, children develop some mental model for negative numbers (see experiments with first-graders in Bofferding ( 2019)).Using negative numbers requires understanding the notation of negatives (i.e."−" followed by a number).This also includes distinguishing between minus in subtractions (1 − 3), dates (12-12-2022), counts (i.e. from one to three) and in negative number (−2).</p>
<p>Number Sense (R2)</p>
<p>Number sense includes reasoning skills for conceptualizing number quantities and understanding their meaning in a given context.</p>
<p>Scale.In everyday communication, numbers commonly occur with measurement scales, e.g.weights, distances, or heights.Understanding numbers in context of scales is a basis for various applications, e.g.question answering (e.g."We are driving 80 km/h, is this within the speed limit?"), commonsense (e.g."Cats weight between four and five kilograms.")(Lin et al., 2020), and temporal reasoning (e.g."She left the office thirty minutes ago.")(Zhou et al., 2020;Zhang et al., 2020).</p>
<p>Comparison.Comparing numbers allows understanding numerical relationships.It involves identifying which numbers are greater than, less than, or equal to others.For example, given the table in Figure 1, understanding "The running time of Hulk is longer than 120 minutes."requires comparison.</p>
<p>Range.The question "Was the budget of the movie between $130 and $245.4?" about the table in Figure 1 requires understanding number ranges.</p>
<p>Already at an age between two and three years, children develop numerical abilities to understand sequences of numbers and start reciting numbers in an appropriate order (Fuson, 2012;Laski and Siegler, 2007).Models' that understand the notation of ranges, can correctly answer the question by knowing that 137 is in the range 130 − 245.4.</p>
<p>Approximation.Humans commonly approximate number values in everyday life (Odic and Starr, 2018;Bonny and Lourenco, 2013).H3 in Figure 1 requires approximation among other skills to map "about two hours" to "138 minutes" in the table.As a reasoning skill, it allows to make quick estimations and metric unit conversations, and understand the approximate values of numbers without calculating them explicitly.</p>
<p>Manipulation (R3)</p>
<p>Manipulation reasoning types are used to apply basic operations on numbers such as addition.Successful manipulation of numbers requires understanding their representations and meaning in the given context (i.e.number sense).</p>
<p>Sorting.The sentence "Out of all Ang Lee's directed movies, 'Hulk' was the one with the second highest box office income."requires sorting all movies according to their box office income in order to select the one with the second highest income.Sorting objects according to some criteria is a basic milestone for developing cognitive skills.By age two, children already begin to understand the concept of sorting.</p>
<p>Simple arithmetic.Arithmetic reasoning is the ability of manipulating numbers with basic operations (addition, subtraction, multiplication, division).While adults commonly retrieve results of simple calculations from memory, children use different operations (Barrouillet and Fayol, 1998b).</p>
<p>Complex Reasoning (R4)</p>
<p>This category builds on all previous reasoning categories (R1 − R3) to solve numerical word problems (NWP).NWP are expressed through natural language and require multistep reasoning.Extracting information from the problem description and applying numerical/mathematical reasoning using the retrieved information and world/commonsense knowledge is required (Upadhyay and Chang, 2017;Amini et al., 2019;Huang et al., 2016).</p>
<p>Numerical Probing Framework</p>
<p>This section provides an overview of the probing framework.We use tabular Natural Language Inference (TNLI) for automated probe creation.</p>
<p>Preliminaries</p>
<p>Tables for numerical probing.Tables align well with our objectives given three key criteria: (i) numerical data is common in tables; (ii) tables are frequent in real-world data sources; (iii) tables, due to their structured formats, facilitate automated perturbations for probe creation.Tables' semi-structured format, the alignments available between table cells and column/row headers, and the frequency of numbers, make them well suitable for creating numerical probes automatically.</p>
<p>Table NLI.Given a natural language sentence as hypothesis and a tabular premise, the aim of TNLI is to classify if the hypothesis entails or contradicts the table (Gupta et al., 2020).We use the table NLI datasets TabFact (Chen et al., 2020) and In-foTabs (Gupta et al., 2020), as well as recast the table QA datasets TAT-QA (Zhu et al., 2021) and TabMWP (Lu et al., 2023) to NLI (i.e.TATQA-NLI, TabMWP-NLI).TAT-QA includes metadata, i.e. annotations of cells and operations per correct answer.This information is not available for any TNLI dataset and is crucial to create probes for specific reasoning types, e.g.arithmetic reasoning.</p>
<p>Table 2 provides an overview of the TNLI datasets.</p>
<p>Preprocessing.For each of numerical reasoning type, we first identify base TNLI hypotheses and/or tables in the datasets that can be used for automated probe creation.Hereby, we defined a list of reference tokens specific for each reasoning type and to filter relevant dataset samples.For example, we used units of measurements such as "hour", "meter", or "kilogram" filter hypotheses for scale probes (see §4 for more details).To recast the TAT-QA dataset, we follow the simple yet effective, rule-based approach proposed by Demszky et al. (2018) for QA to NLI conversion.</p>
<p>Probes through Structural Perturbation</p>
<p>Overall, we our framework includes three types of probes, created through hypotheses perturbation and counterfactual tables.</p>
<p>Hypothesis label-preserving probes</p>
<p>We create label-preserving probes changing the base hypothesis such that its meaning is not changed and the initial label is preserved.They are used to evaluate model's ability to reason and predict the correct label given semantically-equivalent changes.</p>
<p>Hypothesis label-flipping probes</p>
<p>To generate label-flipping probes, we modify the base hypothesis such that its meaning alters and the label of the probe flips, e.g. from entailment to contradiction.We aim to overcome potential dataset artefacts that might be exploited for label prediction instead of performing numerical reasoning.These changes are specific to the reasoning types.For example, to flip labels for scale probes, we substitute measurement units for a particular scale (e.g."kilograms") by another unit (e.g."meters") or introduce errors in conversion of units (e.g. 3 kilometers replaced by 3, 000 meters).</p>
<p>Table Probes through Counterfactual</p>
<p>Probing with TNLI Datasets</p>
<p>This section discussed probes in detail and how we created them for each reasoning type from §3.2</p>
<p>Numeration.To study models' understanding of string ("two") and numerical (e.g."2") number representations, we create two types of numeration probes.One converting number representations from strings to numeric, while the second category applies the conversion vice versa.We filter hypotheses with numbers written as strings ("two") and substitute them by their numeric counterpart (e.g."2").The label-preserving probes are semantically equivalent to the base hypotheses and the label (e.g.entailment) is not changed.Label-flipping probes replace the converted number x by a random number in the range of [x−x * 0.5; x+x * 0.5].For example, the numeration flipping probe of H1 (Table 3) replaces 112 by one hundred and forty-four and flips the label from entailment to contradiction.</p>
<p>Heterogeneous number types.We created heterogeneous probes for the following categories frequent in the TNLI datasets: date formats, ordinals, percentage, currencies, and scientific notation.To filter base hypotheses, we applied a simple, rulebased approach specific to each category (i.e.dates formats, percentage, ordinals, etc.).With about $116, 000, 000 prize money, he is the 3rd highest earning all-time player.</p>
<p>Base Hypothesis H4</p>
<p>Rafael Nadal has a height of 1.85 meters.</p>
<p>Scale Probe H4</p>
<p>Rafael Nadal has a height of 185 centimeters.</p>
<p>Scale Flip Probe H4</p>
<p>Rafael Nadal has a height of 5.2 ft.</p>
<p>Base Hypothesis H5</p>
<p>After the year 2000, the player Nadal turned pro.</p>
<p>Comparison Probe H5</p>
<p>After the year 1995, the player Nadal turned pro.Comparison Flip Probe H5 Before the year 1990, the player Nadal turned pro.</p>
<p>Table 3: Exemplary hypotheses and non-/flipping probes for evaluated reasoning types label, we replaced the date in the adjusted format by a random date, i.e. 15-01-1999.We replaced percentage signs by the token "percentages" and vice versa.Similarly, ordinals written as words (first) were exchanged by numerical representations (1st) and the other way around.For hypotheses with large numbers (e.g." $116,111,561 " in H3), we introduced scientific notations ($116.111561e− 6).</p>
<p>Negative numbers.To create negative probes, we replaced negative numbers −n (e.g.−3) by string equivalents (e.g.minus 3; negative 3) and evaluated changes in model performances on these semantically same sentence pairs.For labelflipping probes, we converted negative numbers into the positive counterpart n.For example, converting "The company's monthly closing resulted in -5 million USD." to "The company's monthly closing resulted in 5 million USD." flips the label.</p>
<p>Scale.We created two types of scale probes: (i) conversion; (ii) mapping.Conversion convert numbers within a measurement scale.For H4 in Table 3, we converted the number and measurement unit ( 1.85 meters ) to the next smaller unit within the same scale (185 centimeters) for the label-preserving probe.For label-flip, we introduced an error in the converted number, i.e. converting 1.85 meters.to 5.2 ft instead of 6.07 ft.</p>
<p>Mapping probes replace the number and measurement unit by an equivalent (e.g.1.85m by 1.85 meters) for label-preserving probes and a random measurement unit e.g.1.85m to 1.85 kilograms) to flip the base hypotheses.</p>
<p>Comparison.We first created a list of signal word-pairs by prompting GPT3.5.The list includes pairs such as {"bigger":"smaller"}, {"taller":"shorter"}, and {"faster":"slower"}.Using these pairs and their synonyms, we filtered base hypotheses and created three types of comparison probes.First, changing the signal word with its opposite counterpart to flip labels (see H5 in Table 3 flipping "after" to "before").Second, altering the number such that the comparison and label do not change: replacing "after 2000" by "after 1995" (H5).Finally, we combine both prior approaches to create label-flipping probes, e.g."Before the year 1990, the player Nadal turned pro."sApproximation.We first extract a number n from our base hypothesis and given the value of n, we decide the magnitude of rounding to apply.While smaller numbers are rounded to tens, larger number are rounded to hundreds, thousands or larger decimal points.For example, we created the probe "With about $116, 000, 000 prize money, he is the 3rd highest earning all-time player" by rounding n equal $116,111,561 to "about $116, 000, 000" (H3 in Table 3).</p>
<p>Range.To create range probes, we substitute number n in the base hypothesis by an appropriate range, e.g.37 by "between 31-43" (H1).We define the radius of the range and its boundaries automatically given the value of n.For example, given n &lt; 10, we randomly sample a radius between 1 − 5.For n = 7 and a sampled radius of 2, the range will be [5 − 9].We select decimal boundaries if n is a decimal number.</p>
<p>Sorting.We utilized table columns as number sequences to create sorting probes.We generated a list of position indicators in number sequences (e.g."top", "second" "3rd","biggest", "lowest").These words were used to filter base hypotheses.To create label-flipping probes, we changed the position of the sequence to another one.For instance, we modified "in the first quarter of 2018" to "in the third quarter of 2018" by selecting the value from the third row instead of the first.</p>
<p>Simple arithmetic.Using on TATQA-NLI its metadata indicating the involved numbers and operations for numerical reasoning, we created arithmetic probes.We extracted probes involving addition, subtraction, multiplication, and division.Additionally, we generated label-flipping probes by replacing the operation output (e.g.result of subtraction) in the hypothesis with a different number.In Table 1, the "Arith" probe involves calculating the difference between the budget and box office values to determine the correctness of 108.4.The flipped arithmetic probe produces a close but incorrect subtraction output, 120.9.</p>
<p>Numerical word problems.We converted TabMWP questions and answers into declarative hypotheses.TabMWP is a dataset of free-text math word problems that involve reasoning with tabular data.For label-flipping probes, we substituted numbers in the hypotheses with random numbers from the same column.</p>
<p>Counterfactual Table NLI Probes.We filtered the counterfactual ToTTo (Parikh et al., 2020) dataset by Jena et al. (2022) for numerical hypothesis.To create counterfactual tables, they swap two or more table cells to modify the tables such that the label of the respective hypothesis changes from entailment to contradiction and vice versa.</p>
<p>Experiments and Analysis</p>
<p>Next, we provide an overview of all models that were evaluated on the probes from §4.We also discuss the obtained results and insights.</p>
<p>Probed Models</p>
<p>We use state-of-the-art models which are divers in terms of architecture, size, and training setup, grouped into three categories:</p>
<p>(C1) Numerical LMs.This category includes LMs adapted for numerical reasoning.LUNA (Han et al., 2022) is a recent transformer-based model with an adapted tokenization approach for numbers.The model encodes numbers as single tokens (e.g. 3, 201) instead of splitting them down into subwords or binned tokens.NT5 (Yang et al., 2021) is a variation of the T5 model.It has been modified for numerical reasoning through additional pretraining objectives and fine-tuning using numerical datasets.PASTA (Gu et al., 2022) is based on DeBERTa and is pretrained with objectives that use table-based numeric operations.</p>
<p>(C2)</p>
<p>LMs for tabular reasoning.TAPAS (Herzig et al., 2020) extends the BERT encoder with table-specific embeddings.We used a TAPAS model trained with intermediate pretraining on synthetic and counterfactual data (Eisenschlos et al., 2020).We also probe TAPEX (Liu et al., 2022), which uses BART (Lewis et al., 2020) as its base model and pretrains the model to mimic a neural SQL executor over tables.Similarly, ReasTAP (Zhao et al., 2022) is a BART-based model pretrained on synthetically generated data requiring seven table reasoning skills, including a numerical task, temporal reasoning, and conjunction.Previous works have also shown the success of the *BERT models on tabular NLI tasks (Herzig et al., 2020;Yin et al., 2020;Shankarampeta et al., 2022;Akhtar et al., 2022).Tables are either linearized or processed into sentences or structured formats.The transformed tables are then used as input to the models.We used a DeBERTa model (He et al., 2021) trained on multiple NLI datasets for this setting.</p>
<p>(C3) Large LMs.For few-/zero-shot evaluation, we selected FlanT5 (Shen et al., 2023)</p>
<p>Training and Evaluation</p>
<p>To fine-tune models, we used the base hypotheses of the training datasets (e.g.InfoTabs) and evaluated models only on probes created with their testsets.The few-shot models were prompted with 2-shot extrapolation.We evaluated all models in a 3-step process: (1) evaluation of base hypotheses H;</p>
<p>(2) evaluation of probes P , created using H;</p>
<p>(3) calculating changes in model performance by comparing accuracy of P to H.As our TNLI task is a binary classification task, we used accuracy for evaluation.</p>
<p>Results and Discussion</p>
<p>Table 4 gives on overview of all probing results.If available, we separately list scores for flipped probes, e.g.numeration and numeration flipped.</p>
<p>(Q1) Does any model excel in all numerical reasoning types?While there is not one bestperforming model across all reasoning types and different models struggle with different types, FlanT5 and GPT3.5 show overall good performance in a zero-shot setting.While GPT3.5 (fewshot) performance drops by −60.22% for complex reasoning probes, the model's average accuracy change is around −16.7% for other types.This can be related to (1) models pretraining data, and (2) training on chain-of-thought reasoning tasks (Wei et al., 2022).GPT3.5 was trained on more than 300 TB data Common Crawl, allowing the model to memorize much more numerical data than other probed models.In comparison, DeBERTa was trained on only 78GB of data (He et al., 2021).Interestingly, both NT5 and FlanT5 use T5 as their base model.FlanT5 was instruction-fine-tuned and outperforms NT5 many probing categories.</p>
<p>(Q2) What factors can contribute to high performance variations across certain reasoning types?Large performance variations mainly occur due to inconsistent numerical reasoning of models across tasks.For example, we observe that some models struggle with more basic reasoning (e.g., FlanT5 zero on numeration) while performing very well on more complex types.This behavior might have different reasons.One potential reason is memorization.Previous works (Petroni et al., 2019;Carlini et al., 2021;Ishihara, 2023) show that large pretrained language models store knowledge in their parameters, which they tend to retrieve instead of reasoning over the provided input (Gupta et al., 2022a).Hence, models can memorize common arithmetic operations they encounter during training and perform well on certain downstream tasks.For example, flipping numbers as words ("two") to numerals ("2") might allow models to retrieve knowledge which they didn't consider for the initial hypothesis.Another reason for high-performance drops can be the hallucination of models.While models initially perform well on hypotheses, adjusting the numbers can hinder models from relying on spurious patterns.</p>
<p>(Q3) How do models perform on different types of numerical reasoning?Representation.In Table 4, comparing numeration probes, we find for all models a performance drop of between [0; −10] percentages for numeration probes, except FlanT5 (few).This drop strongly increases for almost all models evaluated numeration flipped probes.For example, FlanT5 (few) shows a performance drop of −78.37%.FlanT5 (few) also performs well on heterogeneous probes, followed by DeBERTa (−2.4%) and NT5 (−3%).Whereas LUNA performance drops significantly for heterogeneous probes (flipped and non-flipped).TAPAS, NT5, and LUNA show significant performance drops (between −38.87% and −71.35%) on negative number probes.This could be because the models exploit correlations between the "−" sign and labels for predicting base hypotheses.Interestingly, few-and zero-shot models like FlanT5 and GPT3.5 show improvements on negative number probes.This may be because the models understand probed versions of negative numbers (e.g."minus 22") as a negative number but not the initial representation (e.g."−22").</p>
<p>Number sense.Comparing models based on number sense probes, we observe different patterns for fine-tuned models and few-/zero-shot models.Fine-tuned models struggle especially with comparison probes, with a −26.7% average performance drop.Scale probes show a −42.1% decrease on flipping probes, while approximation (flipping) probes report a −12.0%decrease in model performance.In contrast, FlanT5 perform better on comparison and range probes, sometimes surpassing predictions on the base hypotheses.All models demonstrate lower performance on approximation probes compared to the base hypotheses, with PASTA performance dropping by −27.44%.</p>
<p>Manipulation and Complex Reasoning.Finetuned models exhibit an average accuracy drop of −57% on arithmetic probes, except for LUNA with a slight performance increase.The performance of PaLM (zero) and FlanT5 (few) drops by −67.67% and −71.53%, respectively.All models' performance drops on sorting probes (avg.−27%), except for DeBERTa, LUNA, and FlanT5 (zero).Unlike most other reasoning types, fine-tuned models outperform few-/zero-shot models on complex reasoning probes.ReasTAP achieves the highest accuracy, followed by TAPAS and LUNA.FlanT5, TAPEX, and PaLM have the largest performance drops on complex reasoning probes.</p>
<p>(Q4) Do models perform similarly for flipped and non-flipped probes?We observe higher performance drops for label-flipping probes compared to non-flipping probes across models.Models that struggle with flipping probes but perform well on their non-flipping counterparts indicate a reliance on spurious patterns for label prediction.The performance of TAPAS, TAPEX, PASTA, ReasTAP, FlanT5 (few), and PaLM drops significantly for the representation reasoning category comparing non-flipping and flipping probes.For example, TAPAS performance drops by −2.28% on numeration probes, but show a drop of −45.98% on numeration flipping probes.Similarly, DeBERTa performs well on scale probes (−6.25%) compared to the flipping version (−64.58%).PaLM performance on numeration, heterogeneous, and negative probes drops by approximately −35%, −20%, and −80% on flipping counterparts.DeBERTa exhibits robust performance on number flipping probes for sorting and FlanT5 on negative numbers, as well as arithmetic probes.</p>
<p>(Q5) Are numerical and table-specific models better for numerical reasoning than LLMs?Numerical models.Our experiments do not indicate any superiority of numerical models over others.LUNA, a transformer model that uses a specific tokenization method for numbers, performs similarly to other models on many reasoning types.The only reasoning type where LUNA outperforms is comparison flipping probes, with a small improvement of 0.28%.PASTA is a DeBERTa-based model trained on numerical data and pretraining objectives.However, compared to DeBERTa, it only performs better on negative number and scale probes.</p>
<p>Related Work</p>
<p>Numeracy Taxonomies in NLP.Prior works have introduced surveys and taxonomies to organise numeracy in NLP research.Thawani et al. (2021b) categorise NLP work on numeracy into seven subtasks along the dimensions granularity (i.e.exact and approximate numbers) and unit (i.e.abstract and grounded numbers).Xu et al. (2022) focus on the robustness of QA systems in handling numerical data and organize their numeracy probing tasks in two broad categories: (i) numerical parsing, and (ii) semantic parsing.The DROP benchmark (Dua et al., 2019) is a QA dataset that requires discrete operations (e.g.subtraction, count, sort) to answer questions over text.While Thawani et al. (2021b) concentrate on number representations in NLP systems, our work includes three further numerical reasoning categories.Xu et al. (2022) focus on the robustness of NLP models in handling numerical data.Our probing study on the other side pivots towards the reasoning capabilities of models when dealing with numerical and tabular data.Different to prior work, our study gives a broad and in-depth evaluation of ten different models from three different categories (numerical, tabular, large pretrained LMs) on more than ten different reasoning types (representation, number sense, manipulation, and complex reasoning).</p>
<p>Language Model / Numerical Skills.Various studies have evaluated LMs' numerical skills in recent years.</p>
<p>Earlier works probed word embeddings for numeration (e.g.4=four) (Naik et al., 2019), comparison (e.g. 3 &lt; 4) (Wallace et al., 2019), scale (Zhang et al., 2020), and superlatives (Wallace et al., 2019).More recent works evaluate LMs on out-ofdistribution numbers (Kim et al., 2021), numeration/magnitude/sorting/superlatives (Pal and Baral, 2021), and arithmetic (Muffo et al., 2022).Our work builds upon these previous evaluation studies and extends them with further numerical probing categories, e.g.heterogeneous numbers.</p>
<p>Numerically-tuned Language Models.Various numerical LMs have been developed in recent times.Geva et al. (2020) and Liang et al. (2022) inject numerical skills into BERT through numerical pretraining objectives.PASTA (Gu et al., 2022) and NT5 (Yang et al., 2021), which are based on De-BERTa and T5 respectively, fall into the same category of models.Another line of work adjusts LMs' architectures for numerical reasoning through numerical tokenization (Han et al., 2022) or additional, numerical embeddings (Jin et al., 2021).</p>
<p>Systematic Probes for Tables.Tables have been utilized previously used to create probes for table grounding (Gupta et al., 2022b) or recasting non-NLI datasets (e.g.question-answering) to NLI (Jena et al., 2022).Unlike unstructured text data, tables have a natural structure that allows creating controlled experiments more easily (Gupta et al., 2022a).We drew inspiration from prior tabular probing approaches and extended them for automating probing of numerical tabular data.Jena et al. (2022) introduce a generic approach to adjust table QA datasets and generate NLI data.For data recasting, they follow a systemic approach similar to ours.However, the focus is on transforming QA datasets, emphasizing the end-result (i.e. the NLI data) through data augmentation.They do not evaluate the presence of numerical data in their tables or consider numerical reasoning in the model evaluation phrase.</p>
<p>Comparison to Prior Work.All the above mentioned prior works on numerical reasoning have provided motivation for our research.However, their evaluations have focused on a narrow range of reasoning types and models.Most study only concentrated on one specific model such as T5 (Pal and Baral, 2021), GPT3 (Muffo et al., 2022), or BERT (Park et al., 2022).In contrast, our framework provides a comprehensive evaluation of numerical reasoning skills.We cover a wide spectrum of complexity levels, ranging from representation to complex reasoning.Moreover, we assess a variety of models with diverse architectures, sizes, and training settings for numerical reasoning.</p>
<p>Conclusion</p>
<p>This paper presents a framework for probing language models' numerical reasoning skills.We organise skills in a taxonomy and generate large-scale sets of probes covering more than ten numerical reasoning types.Using table NLI as a case study, we evaluate the numerical reasoning abilities of ten models.These models belong to the categories numerical LMs, tabular LMs, and few-/zero-shot LLMs.We discuss reasoning types that prove challenging for the probed models and explore promising directions for future research.</p>
<p>Future Directions.For certain (numerical) tasks, tool-augmented LMs equipped with capabilities such as calculators or code execution have been proven valuable (Mialon et al., 2023).However, certain tasks require implicit numerical reasoning which might not necessarily involve direct calculations based on numbers.For instance, classifying sentences that incorporate numbers in varied settings, like time indications (Feng et al., 2023), currencies or conversations (Macina et al., 2023).Such tasks demand a numerical interpretation beyond mere arithmetic computations.Moreover, calling external tools using LMs requires basic numerical comprehension to invoke an external tool correctly (Chen et al., 2022).</p>
<p>Limitations</p>
<p>This work proposes a taxonomy and framework to probe numerical reasoning skills in LMs.It involves the creation of large-scale probing sets using an automated approach.However, the evaluation of this approach is currently limited to the task of table NLI.For future research, it is interesting to extend this to include additional tasks and datasets.This extension serves two purposes: first, it allows evaluating a more diverse range of datasets.Second, it enables including challenges specific to other tasks.</p>
<p>In this paper, the evaluation of most reasoning types primarily involves structural changes at the hypotheses level.While we include counterfactual table probes, they are limited to one dataset and perturbations method only.Further research is needed to study models' performance on numerical data in the premise data.Therefore, we need table-based probes for all reasoning types of the proposed taxonomy.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021.TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance.In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3277-3287, Online.Association for Computational Linguistics.</p>
<p>A Insights</p>
<p>Main Insights.We investigated the language models and found that LLMs like FlanT5 and GPT3.5 perform better than other models on various numerical reasoning tasks.When the labels are switched around and when dealing with negative values, we found that both table-based and numerical models had difficulty comprehending the data.In contrast, DeBERTa performs relatively well compared to models like LUNA and PASTA, which are tuned for improved numerical reasoning skills.Table 2 gives an overview of probes per dataset.Most probes (i.e.214, 440) are created from Tab-Fact hypotheses as this is also the biggest dataset available, followed by InfoTabs (19,779).Table 5 provides a breakdown of probes per reasoning type.In total, we have 286, 857 probes, of which 76, 404 are label-flipping probes.</p>
<p>B Probe Statistics</p>
<p>In the ideal scenario with counterfactual tables, the models' performance should be similar to the performance on the original tables.However, we observed that TAPAS and DeBERTa's performance improved significantly, which leads to the conclusion that models are biased toward one label.</p>
<p>Overall no language model excels in all the numerical reasoning tasks.Surprisingly, models perform relatively well in complex tasks like Numerical Word Problems but struggle at simple reasoning tasks like numeration and comparison.</p>
<p>Figure 1 :
1
Figure 1: Overview of numerical reasoning types.</p>
<p>Table 1 :
1
Base hypotheses (H1, H2, H3) and (flipped) probes for heterogeneous numbers (i.e.date), approximation, numeracy, and arithmetic.Labelled as Entail or Contradict.</p>
<p>Table 2 :
2
TNLI probing datasets; num cells refers to the average ratio of numerical cells in tables.
DatasetHypotheses Tables Num cells ProbesTabFact118,27516,573 59.00%214,440InfoTabs23,7382,54053.6%19,779TATQA-NLI 4,9472,15659.7%15,139ToTTo1,00089245.7%1,000TabMWP28328338.3%238</p>
<p>Table Editing We also probe with counterfactual tables to evaluate if models rely on spurious patterns in the premise table for label prediction.We filter the counterfactual datasets by Jena et al. (2022) consisting of {hypothesis; original table; counterfactual table} for numerical hypotheses.</p>
<p>, GPT3.5, and PaLM 2(Chowdhery et al., 2022)and probed them in both a few-shot and zero-shot setting.
Model ReasoningTable Specific TAPAS DeBERTa TAPEXNumerical Specific NT5 LUNA PASTA ReasTAP RepresentationFlanT5 few zeroLarge LMs GPT3.5 few zerofewPaLM zeroNumeration-0.32-1.82-7.84-4.18-5.22-7.7-7.181.28-8.84-0.475.65-1.35-3.07Heterogeneous-4.03-2.36-5.94-3 -10.09-7.76-3.180.34-5.496.86.650.44-2.22Negative-46.11-13.770.56 -94.48 -75.55-10.682.65 19.2142.38.242.3-2.171.14Numeration-38.874.09Label Flipped</p>
<p>Table 4 :
4
Probing results given as accuracy difference (in %) between base hypotheses and probes.
173.14 -63.642.2 -80.43 -78.41Number Sense</p>
<p>Table 5 :
5
Breakdown of probes per reasoning type.
Reasoning TypeCountWord Problems238Sorting379Counterfactual1,000Currency1,014Negative3,316Range4,208Scientific notation6,274Arithmetic8,082Ordinal10,569Percentage16,851Date18,642Approximation20,440Comparison30,763Numeration166,319Total288,095Flipped probes77,687
Find details on probe statistics in Appendix B.
Ethics StatementIn this paper, we study the numerical reasoning skills of different LMs.However, to deploy these systems in real-world applications, further studies and evaluations specific to the intended use cases are required.In order to support future research, we plan to release all the scripts and resources used for probe creation and model evaluation.This will facilitate and encourage further research in this field.Model
PubHealthTab: A public health table-based dataset for evidence-based fact checking. Mubashara Akhtar, Oana Cocarascu, Elena Simperl, 10.18653/v1/2022.findings-naacl.1Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics20191Long and Short Papers</p>
<p>From algorithmic computing to direct retrieval: Evidence from number and alphabetic arithmetic in children and adults. Pierre Barrouillet, Michel Fayol, 10.3758/bf03201146Memory &amp;amp Cognition. 2621998a</p>
<p>From algorithmic computing to direct retrieval: Evidence from number and alphabetic arithmetic in children and adults. Pierre Barrouillet, Michel Fayol, Memory &amp; Cognition. 261998b</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Understanding Negative Numbers. Laura Bofferding, 10.1007/978-3-030-00491-0_122019Springer International PublishingCham</p>
<p>The approximate number system and its relation to early math achievement: Evidence from the preschool years. Justin W Bonny, Stella F Lourenco, 10.1016/j.jecp.2012.09.015Journal of Experimental Child Psychology. 11432013</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, 30th USENIX Security Symposium, USENIX Security 2021. USENIX Association2021. August 11-13, 2021Úlfar Erlingsson, Alina Oprea, and Colin Raffel</p>
<p>PURR: efficiently editing language model hallucinations by denoising language model corruptions. Anthony Chen, Panupong Pasupat, Sameer Singh, 10.48550/arXiv.2305.14908CoRR, abs/2305.149082023Hongrae Lee, and Kelvin Guu</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/arXiv.2211.12588CoRR, abs/2211.125882022</p>
<p>Tabfact: A large-scale dataset for table-based fact verification. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang, Wang , 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, 10.48550/arXiv.2204.02311M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>Transforming question answering datasets into natural language inference datasets. Dorottya Demszky, Kelvin Guu, Percy Liang, CoRR, abs/1809.029222018</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Understanding tables with intermediate pre-training. Julian Eisenschlos, Syrine Krichene, Thomas Müller, 10.18653/v1/2020.findings-emnlp.27Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Generic temporal reasoning with differential analysis and explanation. Yu Feng, Ben Zhou, Haoyu Wang, Helen Jin, Dan Roth, 10.18653/v1/2023.acl-long.671Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Children's counting and concepts of number. Karen C Fuson, 2012Springer Science &amp; Business Media</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, 10.18653/v1/2020.acl-main.89Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training. Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Is my model using the right evidence? systematic probes for examining evidence-based tabular reasoning. Vivek Gupta, Riyaz A Bhat, Atreya Ghosal, Manish Shrivastava, Maneesh Singh, Vivek Srikumar, 10.1162/tacl_a_00482Transactions of the Association for Computational Linguistics. 102022a</p>
<p>INFOTABS: Inference on tables as semi-structured data. Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar, 10.18653/v1/2020.acl-main.210Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Right for the right reason: Evidence extraction for trustworthy tabular reasoning. Vivek Gupta, Shuo Zhang, Alakananda Vempala, Yujie He, Temma Choji, Vivek Srikumar, 10.18653/v1/2022.acl-long.231Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022b1</p>
<p>LUNA: language understanding with number augmentations on transformers via number plugins and pre-training. Hongwei Han, Jialiang Xu, Mengyu Zhou, Yijia Shao, Shi Han, Dongmei Zhang, 10.48550/arXiv.2212.02691CoRR, abs/2212.026912022</p>
<p>Deberta: decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021</p>
<p>TaPas: Weakly supervised table parsing via pre-training. Jonathan Herzig, Krzysztof Pawel, Thomas Nowak, Francesco Müller, Julian Piccinno, Eisenschlos, 10.18653/v1/2020.acl-main.398Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, Wei-Ying Ma, 10.18653/v1/P16-1084Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany20161Association for Computational Linguistics</p>
<p>Training data extraction from pre-trained language models: A survey. Shotaro Ishihara, 10.48550/arXiv.2305.16157CoRR, abs/2305.161572023</p>
<p>Leveraging data recasting to enhance tabular reasoning. Aashna Jena, Vivek Gupta, Manish Shrivastava, Julian Eisenschlos, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 5512382023</p>
<p>Numgpt: Improving numeracy ability of generative pre-trained models. Zhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong Wang, Xiaozhe Ren, Huamin Qu, CoRR, abs/2109.031372021</p>
<p>Have you seen that number? investigating extrapolation in question answering models. Jeonghwan Kim, Giwon Hong, Kyung-Min Kim, Junmo Kang, Sung-Hyon Myaeng, 10.18653/v1/2021.emnlp-main.563Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Is 27 a big number? correlational and causal connections among numerical categorization, number line estimation, and numerical magnitude comparison. Elida V Laski, Robert S Siegler, 10.1111/j.1467-8624.2007.01087.xChild Development. 7862007</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>MWP-BERT: Numeracy-augmented pre-training for math word problem solving. Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang, 10.18653/v1/2022.findings-naacl.74Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna, Ren, 10.18653/v1/2020.emnlp-main.557Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>TAPEX: table pre-training via learning a neural SQL executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Prodromos Malakasiotis, Ion Androutsopoulos, and Georgios Paliouras. 2022. FiNER: Financial numeric entity recognition for XBRL tagging. Lefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, 10.18653/v1/2022.acl-long.303Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland1Association for Computational Linguistics</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, International Conference on Learning Representations (ICLR). 2023</p>
<p>Mathdial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan, 10.48550/arXiv.2305.14536CoRR, abs/2305.145362023</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 10.48550/arXiv.2302.07842CoRR, abs/2302.078422023</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022. the Thirteenth Language Resources and Evaluation Conference, LREC 2022Marseille, FranceEuropean Language Resources Association2022. 20-25 June 2022</p>
<p>Exploring numeracy in word embeddings. Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, Eduard Hovy, 10.18653/v1/P19-1329Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>An introduction to the approximate number system. Darko Odic, Ariel Starr, 10.1111/cdep.12288Child Development Perspectives. 1242018</p>
<p>Investigating numeracy learning ability of a text-to-text transfer model. Kuntal Kumar, Pal , Chitta Baral, 10.18653/v1/2021.findings-emnlp.265Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>ToTTo: A controlled table-to-text generation dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, 10.18653/v1/2020.emnlp-main.89Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Do language models understand measurements?. Sungjin Park, Seungwoo Ryu, Edward Choi, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Enhancing tabular reasoning with pattern exploiting training. Abhilash Shankarampeta, Vivek Gupta, Shuo Zhang, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Long Papers. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing20221Online only. Association for Computational Linguistics</p>
<p>Flan-moe: Scaling instruction-finetuned language models with sparse mixture of experts. Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, Denny Zhou, 10.48550/arXiv.2305.14705CoRR, abs/2305.147052023</p>
<p>Numeracy enhances the literacy of language models. Avijit Thawani, Jay Pujara, Filip Ilievski, 10.18653/v1/2021.emnlp-main.557Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021aOnline and Punta Cana</p>
<p>Representing numbers in NLP: a survey and a vision. Avijit Thawani, Jay Pujara, Filip Ilievski, Pedro Szekely, 10.18653/v1/2021.naacl-main.53Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline. Association for Computational Linguistics2021b</p>
<p>Annotating derivations: A new evaluation strategy and dataset for algebra word problems. Shyam Upadhyay, Ming-Wei Chang, Proceedings of the 15th Conference of the European Chapter. Long Papers. the 15th Conference of the European ChapterValencia, SpainAssociation for Computational Linguistics20171</p>
<p>Do NLP models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, 10.18653/v1/D19-1534Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Number games, magnitude representation, and basic number skills in preschoolers. Catherine Whyte Jemma, Rebecca Bull, 10.1037/0012-1649.44.2.588Developmental Psychology. 4422008</p>
<p>Towards robust numerical question answering: Diagnosing numerical capabilities of NLP systems. Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, Dongmei Zhang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Peng-Jian Yang, Ying-Ting Chen, Yuechan Chen, Daniel Cer, CoRR, abs/2104.07307Nt5?! training T5 to perform numerical reasoning. 2021</p>
<p>Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, 10.48550/arXiv.2301.13808CoRR, abs/2301.138082023</p>
<p>TaBERT: Pretraining for joint understanding of textual and tabular data. Pengcheng Yin, Graham Neubig, Wen-Tau Yih, Sebastian Riedel, 10.18653/v1/2020.acl-main.745Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Do language embeddings capture scales?. Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth, 10.18653/v1/2020.findings-emnlp.439Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples. Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Temporal common sense acquisition with minimal supervision. Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth, 10.18653/v1/2020.acl-main.678Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1270 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1270</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1270</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-208547755</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1912.01603v3.pdf" target="_blank">Dream to Control: Learning Behaviors by Latent Imagination</a></p>
                <p><strong>Paper Abstract:</strong> Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1270.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1270.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer (world model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer latent dynamics world model (as used in Dreamer agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned latent dynamics model used by the Dreamer agent that encodes image observations into compact continuous latent states and predicts transitions, rewards, and (optionally) episode termination/discounts to enable large-scale latent imagination and analytic gradient-based policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer latent dynamics (RSSM + encoder/decoder + reward predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A probabilistic latent state-space model composed of a representation (encoder) p_theta(s_t | s_{t-1}, a_{t-1}, o_t), a recurrent transition model q_theta(s_t | s_{t-1}, a_{t-1}) implemented as an RSSM (recurrent state-space model), and a reward model q_theta(r_t | s_t). The observation / image reconstruction decoder is used for representation learning (pixel-reconstruction ELBO) in default experiments. Latent states are 30-dim diagonal Gaussians; the image encoder/decoder are convolutional networks and the transition/value/action networks are dense MLPs (three layers of size 300, ELU). The model is action-conditioned and permits reparameterized sampling so imagined trajectories (states, rewards, actions) can be rolled out in latent space for policy/value learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (probabilistic recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual continuous control (DeepMind Control Suite) and subsets of discrete domains (Atari, DeepMind Lab) from pixel inputs</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Variational ELBO components (image reconstruction log-likelihood), reward log-likelihood, KL divergence regularizer between representation and transition; qualitative long-term video-prediction reconstructions; downstream task return (control performance) as ultimate task-relevant fidelity proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single scalar image-prediction metric reported in paper; authors report qualitatively accurate long-term reconstructions (45-step predictions shown) and that pixel-reconstruction objective yielded best downstream task performance; downstream task scores: Dreamer achieved average score 823 across 20 control tasks after 5e6 environment steps (used as evidence that the world model's fidelity is sufficient for control).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural latent model; latent states are low dimensional (30-d) which reduces complexity but the paper does not claim semantic interpretability of individual latent dimensions. Interpretability is via visualizing reconstructions / video predictions (to inspect whether imagined trajectories are plausible).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of image reconstructions and long-horizon video predictions from latent rollouts; inspection of task performance (returns) as indirect assessment of task-relevant fidelity. No symbolic or disentanglement analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training/inference run on a single NVIDIA V100 GPU plus 10 CPU cores. Model uses 30-D diagonal Gaussians for latents, convolutional encoder/decoder and MLPs (3×300 units). Reported wall-clock: ~3 hours per 1e6 environment steps for Dreamer (end-to-end training including model + actor + value updates). Imagination uses thousands of parallel latent rollouts (low memory footprint).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dreamer is reported substantially faster than online planning PlaNet baseline (3h vs 11h per 1e6 steps on same hardware) and faster than model-free D4PG to reach similar performance (D4PG used ~24h for comparable results). Dreamer is described as more data-efficient than model-free baselines and more computationally efficient than online-planning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On 20 DeepMind Control Suite image-based tasks Dreamer reached an average score of 823 after 5×10^6 environment steps, outperforming PlaNet (332 at 5×10^6) and exceeding the model-free D4PG final performance (786) reported at 1×10^8 steps. Dreamer solved many long-horizon tasks where prior latent-model-based methods struggled.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The paper argues that the Dreamer world model provides task utility by enabling: (1) compact latent imagination (thousands of trajectories in parallel) that reduces compute and memory, (2) analytic gradient propagation through learned transitions to train a parametric policy and value function efficiently, and (3) accurate-enough long-horizon predictions (with a value model estimating beyond the imagination horizon) so policies learned in imagination transfer to the real environment. High fidelity in image reconstruction correlates with better downstream performance (pixel reconstruction > contrastive > reward-only in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs discussed: pixel reconstruction gives stronger representation learning signal (better task performance) but requires modeling high-dimensional pixels (higher model capacity); contrastive objectives are cheaper but extracted less usable information for control; pure reward-prediction was insufficient with limited data. Using a compact latent space reduces compute and enables many imagined rollouts but sacrifices direct interpretability of raw pixels. Finite imagination horizon can cause shortsightedness unless compensated by a learned value model (Dreamer uses V_lambda to mitigate).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: use RSSM recurrent transition with stochastic latent states (30-d diagonal Gaussians); combine CNN encoder/decoder for pixel reconstruction + reward predictor; KL regularizer β = 1 clipped below 3 free nats; imagination horizon H = 15 (H=10 for discrete tasks), γ = 0.99, λ = 0.95 for V_lambda targets; action repeat fixed to R = 2; actor outputs tanh-transformed Gaussian with reparameterization; value and action MLPs of 3×300 units. Value model trained in latent space and used to provide multi-horizon targets to the actor via backpropagation through imagined dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to PlaNet (same family of latent dynamics), Dreamer differs by learning an action model and value model in latent space and using analytic gradients through dynamics to optimize policies; achieves similar or better data-efficiency than PlaNet but higher final performance and lower wall-clock time. Compared to model-free (D4PG), Dreamer is far more data-efficient and attains comparable or better final scores with much less environment interaction. Compared to online planning methods (PETS, VisualMPC), Dreamer is computationally cheaper at inference (learned policy vs planning) and more robust to finite horizon because of the value model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends: pixel-reconstruction objective for representation learning (best downstream results), compact stochastic recurrent latent (RSSM) around 30-d, imagination horizon H ~ 15 with learned value model using V_lambda (λ≈0.95) to account for returns beyond the horizon, action repeat R = 2 for the control suite, and training actor/value via analytic backpropagation through imagined trajectories. These settings were found robust across tasks in the paper (same hyperparameters used across continuous tasks).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1270.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1270.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (latent planning agent / reconstruction-based latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent dynamics world-model approach that jointly learns an encoder, recurrent latent transition model, decoder for image reconstruction, and reward predictor to perform online planning in latent space (used as a baseline/comparison in the Dreamer paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet latent dynamics (reconstruction-based RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A variational latent state-space model that learns via image reconstruction (ELBO) and reward prediction, with a recurrent state-space model (RSSM) transition and CNN encoder/decoder; actions are chosen by online planning (model predictive control) in latent space rather than by a learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (reconstruction-based recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual continuous control (DeepMind Control Suite) from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO terms including image reconstruction log-likelihood, reward log-likelihood, and KL regularizer; quality of latent predictions assessed via reconstruction/video predictions and downstream planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single pixel-MSE number given in Dreamer paper; reported downstream performance when run in same setup: PlaNet average score 332 across tasks after 5×10^6 steps (lower than Dreamer). Wall-clock training reported as ~11 hours per 1e6 environment steps on same hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box latent model; interpretability primarily via visualization of reconstructions and rollouts. No claims of semantic interpretability in the Dreamer paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of reconstructions / predicted frames (videos) and evaluating planning rollouts; no specialized interpretability techniques described in Dreamer beyond reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported as slower in wall-clock than Dreamer: ~11 hours per 1e6 environment steps on same hardware (single V100 + 10 CPUs) when run by the Dreamer authors for comparison. Planning at inference time requires additional computation compared to executing a learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PlaNet achieves strong data-efficiency compared to model-free baselines but is computationally more expensive than Dreamer due to online planning. Dreamer inherits PlaNet's model but trains a policy using analytic gradients, producing faster training and better final performance in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported average score 332 across the same 20 control tasks after 5×10^6 environment steps (in the re-run reported in the Dreamer paper), lower than Dreamer (823).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PlaNet's reconstruction-learned world model provides a useful latent representation for planning and is data-efficient; however, selecting actions by planning (with finite horizons) can be shortsighted and computationally heavier than learning a policy in latent space. The Dreamer paper shows that a learned value model + policy in latent space can overcome finite-horizon shortsightedness while being more efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Online planning with a learned latent model trades compute at inference (planning cost) for some robustness to model error but suffers when horizon is too short; reconstruction objective captures more image information (useful for control) but increases modeling complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Joint training of encoder, RSSM transition, decoder (image reconstruction) and reward predictor; planning-based action selection rather than a learned policy; KL regularization and VIB-style objective like in Dreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Dreamer: similar world-model architecture and data-efficiency but worse wall-clock training time and lower final performance in Dreamer's experiments; compared to model-free approaches: more data-efficient but historically required careful planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not explicitly re-specified in Dreamer beyond noting that same world-model design (reconstruction + RSSM) was used for PlaNet; Dreamer recommends learning a policy + value in latent space (their approach) to reduce planning compute and increase final performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1270.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1270.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pixel-reconstruction objective (ELBO / VIB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reconstruction-based representation learning (variational ELBO / variational information bottleneck)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation learning objective that trains the encoder/transition/decoder to reconstruct image observations and rewards, regularized by a KL term (variational information bottleneck), producing latent states useful for long-horizon prediction and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reconstruction-based latent dynamics (ELBO / VIB)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The world model includes an observation model q(o_t | s_t) (image decoder) and reward predictor q(r_t | s_t); the combined objective maximizes reconstruction log-likelihoods for observations and rewards and minimizes KL divergence between posterior p(s_t | s_{t-1}, a_{t-1}, o_t) and prior q(s_t | s_{t-1}, a_{t-1}) (VIB-style). This yields latent states that retain visual information important for control.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model learned via reconstruction / variational objective</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual control (DeepMind Control Suite) from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood / ELBO components, KL regularizer; downstream task return used as proxy for task-relevant fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Quantitative pixel-reconstruction metrics not reported. Empirically in the paper, reconstruction-based representations produced the best downstream control performance compared to contrastive and reward-only objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent states capture image information needed for reconstructing pixels; interpretability limited to inspecting reconstructions and rollout videos.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructed images and long-term predicted frames to inspect fidelity and plausibility of learned latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires learning image decoder and pixel-level reconstruction losses—higher model capacity and compute than contrastive or reward-only objectives but still compatible with the reported training regime (single V100).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally demanding than contrastive or reward-only objectives (because pixel decoding is expensive) but provides stronger learning signal leading to better sample-efficiency and final task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Using pixel reconstruction with Dreamer resulted in the best performance across most tasks in experiments (pixel-recon > contrastive > reward-only). Exact numeric pixel-loss values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Pixel-reconstruction emphasizes preserving visual details which improved downstream policy learning in Dreamer's experiments — higher representational fidelity to images translated to better control performance under limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher representational fidelity (pixel recon) requires more model capacity/compute, but yields better task utility; contrastive and reward-only are cheaper but extract less task-relevant information, reducing performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of observation decoder q(o_t|s_t), reward predictor, ELBO/VIB objective with β=1 and KL clipping (clip below 3 free nats) as in PlaNet; convolutional encoder/decoder architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms contrastive estimation and reward-only objectives in downstream control tasks in Dreamer's experiments; more expensive to train than the alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends pixel reconstruction as the preferred default representation learning objective for Dreamer on the evaluated control tasks, given limited data regimes used in experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1270.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1270.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive (InfoNCE) latent objective</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive representation learning (InfoNCE / state model q(s_t | o_t))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative representation loss that trains a state encoder to make latent states predictable from images using a contrastive (InfoNCE / noise-contrastive estimation) objective instead of explicit pixel reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Contrastive latent-state model (InfoNCE / NCE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Replace the observation decoder with a state model q_theta(s_t | o_t) and optimize a contrastive bound (InfoNCE) that increases ln q(s_t | o_t) − ln average_o q(s_t | o) plus reward prediction and KL regularizer; encourages predictability of latent states from images while avoiding pixel prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model learned via contrastive objective (NCE / InfoNCE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual control (DeepMind Control Suite) from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive loss (InfoNCE objective) and reward prediction log-likelihood; downstream task return as proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In Dreamer's experiments, contrastive objective solved about half of the tasks and performed worse than pixel-reconstruction on most tasks; no numerical contrastive-loss-to-performance mapping provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent states are trained to be predictable from images but are not explicitly interpretable; interpretability via inspection of downstream performance and possibly nearest-neighbor checks in latent space (not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No explicit interpretability method beyond evaluating task performance and possibly inspecting representations; InfoNCE inherently encourages discriminative representations across minibatch observations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Avoids pixel decoding and thus can be computationally cheaper than reconstruction objectives; however, needs negative samples or minibatch averaging (InfoNCE), which has its own cost but still lighter than full decoder training.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Cheaper than pixel reconstruction but produced weaker downstream performance in the limited-data experiments reported; may be preferable where compute or decoder modeling is prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Contrastive variant achieved intermediate results (solved ≈50% of tasks) — worse than reconstruction but better than reward-only in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Contrastive learning extracts predictive features from images useful for control, but with limited data its capacity to capture all task-relevant visual factors was lower than pixel reconstruction, reducing task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Lower compute than reconstruction but less information extracted for control resulting in lower performance; less prone to pixel-level overfitting but may not capture fine-grained visual cues needed for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Implement state model as CNN q(s_t | o_t); use InfoNCE mini-batch bound: ln q(s_t | o_t) − ln_o q(s_t | o) plus reward prediction and KL regularizer; optimize jointly with transition and reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>In the Dreamer experiments pixel-reconstruction > contrastive > reward-only for downstream control; contrastive is a middle ground favoring efficiency over pixel fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests contrastive methods are promising but currently underperform pixel-reconstruction on these tasks; may become more competitive with more data or improved contrastive schemes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1270.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1270.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward-prediction-only model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent dynamics learned only from reward prediction (no observation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal representation learning objective that trains the latent dynamics and reward predictor directly from actions and rewards without reconstructing images or using contrastive image objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reward-only latent dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learn a transition model q(s_t | s_{t-1}, a_{t-1}) and a reward predictor q(r_t | s_t) with a representation conditioned on past observations/actions but without any image reconstruction or contrastive objective to shape latents toward visual predictability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model trained from reward signal only</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual control (DeepMind Control Suite) — used as an ablation/variant in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reward prediction log-likelihood; downstream task return as primary fidelity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In the experiments reported, reward-only representation was insufficient to learn good control policies from limited data and performed worst among the three objectives (reconstruction, contrastive, reward-only). No numeric fidelity measures reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent states learned only from rewards tend to be minimal and task-specific; interpretability not assessed in the paper and likely limited because the model has no explicit link to images.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cheapest among the three objectives since it avoids pixel decoding and contrastive negatives, but requires substantial environment experience to extract useful information when rewards are sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computationally efficient but sample-inefficient in practice for visual control when rewards are sparse or limited data is available; performed poorly in Dreamer's experiments relative to reconstruction and contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not sufficient by itself in the experimental regime used; tasks required more experience or additional signals to reach good performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>While lightweight, reward-only learning fails to capture necessary visual features with limited data, so high computational efficiency does not translate into task utility in these settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Lowest compute cost but poorest task utility; highlights trade-off where removing pixel objectives reduces compute but sacrifices representational richness needed for control.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Omit observation model; train transition and reward predictor using available reward signals. Useful as lower bound ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than pixel-reconstruction and contrastive objectives for Dreamer on the evaluated control tasks; may require much larger datasets to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not recommend reward-only for the evaluated low-data visual control tasks; suggests combining reward prediction with image-based objectives for practical performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 2)</em></li>
                <li>Mastering Atari, Go, Chess and Shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Imagination-augmented agents for deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Model-predictive policy learning with uncertainty regularization for driving in dense traffic <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1270",
    "paper_id": "paper-208547755",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Dreamer (world model)",
            "name_full": "Dreamer latent dynamics world model (as used in Dreamer agent)",
            "brief_description": "A learned latent dynamics model used by the Dreamer agent that encodes image observations into compact continuous latent states and predicts transitions, rewards, and (optionally) episode termination/discounts to enable large-scale latent imagination and analytic gradient-based policy learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dreamer latent dynamics (RSSM + encoder/decoder + reward predictor)",
            "model_description": "A probabilistic latent state-space model composed of a representation (encoder) p_theta(s_t | s_{t-1}, a_{t-1}, o_t), a recurrent transition model q_theta(s_t | s_{t-1}, a_{t-1}) implemented as an RSSM (recurrent state-space model), and a reward model q_theta(r_t | s_t). The observation / image reconstruction decoder is used for representation learning (pixel-reconstruction ELBO) in default experiments. Latent states are 30-dim diagonal Gaussians; the image encoder/decoder are convolutional networks and the transition/value/action networks are dense MLPs (three layers of size 300, ELU). The model is action-conditioned and permits reparameterized sampling so imagined trajectories (states, rewards, actions) can be rolled out in latent space for policy/value learning.",
            "model_type": "latent world model (probabilistic recurrent state-space model)",
            "task_domain": "Visual continuous control (DeepMind Control Suite) and subsets of discrete domains (Atari, DeepMind Lab) from pixel inputs",
            "fidelity_metric": "Variational ELBO components (image reconstruction log-likelihood), reward log-likelihood, KL divergence regularizer between representation and transition; qualitative long-term video-prediction reconstructions; downstream task return (control performance) as ultimate task-relevant fidelity proxy.",
            "fidelity_performance": "No single scalar image-prediction metric reported in paper; authors report qualitatively accurate long-term reconstructions (45-step predictions shown) and that pixel-reconstruction objective yielded best downstream task performance; downstream task scores: Dreamer achieved average score 823 across 20 control tasks after 5e6 environment steps (used as evidence that the world model's fidelity is sufficient for control).",
            "interpretability_assessment": "Primarily a black-box neural latent model; latent states are low dimensional (30-d) which reduces complexity but the paper does not claim semantic interpretability of individual latent dimensions. Interpretability is via visualizing reconstructions / video predictions (to inspect whether imagined trajectories are plausible).",
            "interpretability_method": "Visualization of image reconstructions and long-horizon video predictions from latent rollouts; inspection of task performance (returns) as indirect assessment of task-relevant fidelity. No symbolic or disentanglement analyses reported.",
            "computational_cost": "Training/inference run on a single NVIDIA V100 GPU plus 10 CPU cores. Model uses 30-D diagonal Gaussians for latents, convolutional encoder/decoder and MLPs (3×300 units). Reported wall-clock: ~3 hours per 1e6 environment steps for Dreamer (end-to-end training including model + actor + value updates). Imagination uses thousands of parallel latent rollouts (low memory footprint).",
            "efficiency_comparison": "Dreamer is reported substantially faster than online planning PlaNet baseline (3h vs 11h per 1e6 steps on same hardware) and faster than model-free D4PG to reach similar performance (D4PG used ~24h for comparable results). Dreamer is described as more data-efficient than model-free baselines and more computationally efficient than online-planning baselines.",
            "task_performance": "On 20 DeepMind Control Suite image-based tasks Dreamer reached an average score of 823 after 5×10^6 environment steps, outperforming PlaNet (332 at 5×10^6) and exceeding the model-free D4PG final performance (786) reported at 1×10^8 steps. Dreamer solved many long-horizon tasks where prior latent-model-based methods struggled.",
            "task_utility_analysis": "The paper argues that the Dreamer world model provides task utility by enabling: (1) compact latent imagination (thousands of trajectories in parallel) that reduces compute and memory, (2) analytic gradient propagation through learned transitions to train a parametric policy and value function efficiently, and (3) accurate-enough long-horizon predictions (with a value model estimating beyond the imagination horizon) so policies learned in imagination transfer to the real environment. High fidelity in image reconstruction correlates with better downstream performance (pixel reconstruction &gt; contrastive &gt; reward-only in experiments).",
            "tradeoffs_observed": "Trade-offs discussed: pixel reconstruction gives stronger representation learning signal (better task performance) but requires modeling high-dimensional pixels (higher model capacity); contrastive objectives are cheaper but extracted less usable information for control; pure reward-prediction was insufficient with limited data. Using a compact latent space reduces compute and enables many imagined rollouts but sacrifices direct interpretability of raw pixels. Finite imagination horizon can cause shortsightedness unless compensated by a learned value model (Dreamer uses V_lambda to mitigate).",
            "design_choices": "Key choices: use RSSM recurrent transition with stochastic latent states (30-d diagonal Gaussians); combine CNN encoder/decoder for pixel reconstruction + reward predictor; KL regularizer β = 1 clipped below 3 free nats; imagination horizon H = 15 (H=10 for discrete tasks), γ = 0.99, λ = 0.95 for V_lambda targets; action repeat fixed to R = 2; actor outputs tanh-transformed Gaussian with reparameterization; value and action MLPs of 3×300 units. Value model trained in latent space and used to provide multi-horizon targets to the actor via backpropagation through imagined dynamics.",
            "comparison_to_alternatives": "Compared to PlaNet (same family of latent dynamics), Dreamer differs by learning an action model and value model in latent space and using analytic gradients through dynamics to optimize policies; achieves similar or better data-efficiency than PlaNet but higher final performance and lower wall-clock time. Compared to model-free (D4PG), Dreamer is far more data-efficient and attains comparable or better final scores with much less environment interaction. Compared to online planning methods (PETS, VisualMPC), Dreamer is computationally cheaper at inference (learned policy vs planning) and more robust to finite horizon because of the value model.",
            "optimal_configuration": "Paper recommends: pixel-reconstruction objective for representation learning (best downstream results), compact stochastic recurrent latent (RSSM) around 30-d, imagination horizon H ~ 15 with learned value model using V_lambda (λ≈0.95) to account for returns beyond the horizon, action repeat R = 2 for the control suite, and training actor/value via analytic backpropagation through imagined trajectories. These settings were found robust across tasks in the paper (same hyperparameters used across continuous tasks).",
            "uuid": "e1270.0"
        },
        {
            "name_short": "PlaNet",
            "name_full": "PlaNet (latent planning agent / reconstruction-based latent dynamics)",
            "brief_description": "A latent dynamics world-model approach that jointly learns an encoder, recurrent latent transition model, decoder for image reconstruction, and reward predictor to perform online planning in latent space (used as a baseline/comparison in the Dreamer paper).",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "mention",
            "model_name": "PlaNet latent dynamics (reconstruction-based RSSM)",
            "model_description": "A variational latent state-space model that learns via image reconstruction (ELBO) and reward prediction, with a recurrent state-space model (RSSM) transition and CNN encoder/decoder; actions are chosen by online planning (model predictive control) in latent space rather than by a learned policy.",
            "model_type": "latent world model (reconstruction-based recurrent state-space model)",
            "task_domain": "Visual continuous control (DeepMind Control Suite) from pixels",
            "fidelity_metric": "ELBO terms including image reconstruction log-likelihood, reward log-likelihood, and KL regularizer; quality of latent predictions assessed via reconstruction/video predictions and downstream planning performance.",
            "fidelity_performance": "No single pixel-MSE number given in Dreamer paper; reported downstream performance when run in same setup: PlaNet average score 332 across tasks after 5×10^6 steps (lower than Dreamer). Wall-clock training reported as ~11 hours per 1e6 environment steps on same hardware.",
            "interpretability_assessment": "Black-box latent model; interpretability primarily via visualization of reconstructions and rollouts. No claims of semantic interpretability in the Dreamer paper.",
            "interpretability_method": "Visual inspection of reconstructions / predicted frames (videos) and evaluating planning rollouts; no specialized interpretability techniques described in Dreamer beyond reconstructions.",
            "computational_cost": "Reported as slower in wall-clock than Dreamer: ~11 hours per 1e6 environment steps on same hardware (single V100 + 10 CPUs) when run by the Dreamer authors for comparison. Planning at inference time requires additional computation compared to executing a learned policy.",
            "efficiency_comparison": "PlaNet achieves strong data-efficiency compared to model-free baselines but is computationally more expensive than Dreamer due to online planning. Dreamer inherits PlaNet's model but trains a policy using analytic gradients, producing faster training and better final performance in these experiments.",
            "task_performance": "Reported average score 332 across the same 20 control tasks after 5×10^6 environment steps (in the re-run reported in the Dreamer paper), lower than Dreamer (823).",
            "task_utility_analysis": "PlaNet's reconstruction-learned world model provides a useful latent representation for planning and is data-efficient; however, selecting actions by planning (with finite horizons) can be shortsighted and computationally heavier than learning a policy in latent space. The Dreamer paper shows that a learned value model + policy in latent space can overcome finite-horizon shortsightedness while being more efficient.",
            "tradeoffs_observed": "Online planning with a learned latent model trades compute at inference (planning cost) for some robustness to model error but suffers when horizon is too short; reconstruction objective captures more image information (useful for control) but increases modeling complexity.",
            "design_choices": "Joint training of encoder, RSSM transition, decoder (image reconstruction) and reward predictor; planning-based action selection rather than a learned policy; KL regularization and VIB-style objective like in Dreamer.",
            "comparison_to_alternatives": "Compared to Dreamer: similar world-model architecture and data-efficiency but worse wall-clock training time and lower final performance in Dreamer's experiments; compared to model-free approaches: more data-efficient but historically required careful planning.",
            "optimal_configuration": "Not explicitly re-specified in Dreamer beyond noting that same world-model design (reconstruction + RSSM) was used for PlaNet; Dreamer recommends learning a policy + value in latent space (their approach) to reduce planning compute and increase final performance.",
            "uuid": "e1270.1"
        },
        {
            "name_short": "Pixel-reconstruction objective (ELBO / VIB)",
            "name_full": "Reconstruction-based representation learning (variational ELBO / variational information bottleneck)",
            "brief_description": "A representation learning objective that trains the encoder/transition/decoder to reconstruct image observations and rewards, regularized by a KL term (variational information bottleneck), producing latent states useful for long-horizon prediction and control.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Reconstruction-based latent dynamics (ELBO / VIB)",
            "model_description": "The world model includes an observation model q(o_t | s_t) (image decoder) and reward predictor q(r_t | s_t); the combined objective maximizes reconstruction log-likelihoods for observations and rewards and minimizes KL divergence between posterior p(s_t | s_{t-1}, a_{t-1}, o_t) and prior q(s_t | s_{t-1}, a_{t-1}) (VIB-style). This yields latent states that retain visual information important for control.",
            "model_type": "latent world model learned via reconstruction / variational objective",
            "task_domain": "Visual control (DeepMind Control Suite) from pixels",
            "fidelity_metric": "Reconstruction log-likelihood / ELBO components, KL regularizer; downstream task return used as proxy for task-relevant fidelity.",
            "fidelity_performance": "Quantitative pixel-reconstruction metrics not reported. Empirically in the paper, reconstruction-based representations produced the best downstream control performance compared to contrastive and reward-only objectives.",
            "interpretability_assessment": "Latent states capture image information needed for reconstructing pixels; interpretability limited to inspecting reconstructions and rollout videos.",
            "interpretability_method": "Visualization of reconstructed images and long-term predicted frames to inspect fidelity and plausibility of learned latent dynamics.",
            "computational_cost": "Requires learning image decoder and pixel-level reconstruction losses—higher model capacity and compute than contrastive or reward-only objectives but still compatible with the reported training regime (single V100).",
            "efficiency_comparison": "More computationally demanding than contrastive or reward-only objectives (because pixel decoding is expensive) but provides stronger learning signal leading to better sample-efficiency and final task performance.",
            "task_performance": "Using pixel reconstruction with Dreamer resulted in the best performance across most tasks in experiments (pixel-recon &gt; contrastive &gt; reward-only). Exact numeric pixel-loss values not provided.",
            "task_utility_analysis": "Pixel-reconstruction emphasizes preserving visual details which improved downstream policy learning in Dreamer's experiments — higher representational fidelity to images translated to better control performance under limited data.",
            "tradeoffs_observed": "Higher representational fidelity (pixel recon) requires more model capacity/compute, but yields better task utility; contrastive and reward-only are cheaper but extract less task-relevant information, reducing performance.",
            "design_choices": "Use of observation decoder q(o_t|s_t), reward predictor, ELBO/VIB objective with β=1 and KL clipping (clip below 3 free nats) as in PlaNet; convolutional encoder/decoder architectures.",
            "comparison_to_alternatives": "Outperforms contrastive estimation and reward-only objectives in downstream control tasks in Dreamer's experiments; more expensive to train than the alternatives.",
            "optimal_configuration": "Paper recommends pixel reconstruction as the preferred default representation learning objective for Dreamer on the evaluated control tasks, given limited data regimes used in experiments.",
            "uuid": "e1270.2"
        },
        {
            "name_short": "Contrastive (InfoNCE) latent objective",
            "name_full": "Contrastive representation learning (InfoNCE / state model q(s_t | o_t))",
            "brief_description": "An alternative representation loss that trains a state encoder to make latent states predictable from images using a contrastive (InfoNCE / noise-contrastive estimation) objective instead of explicit pixel reconstruction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Contrastive latent-state model (InfoNCE / NCE)",
            "model_description": "Replace the observation decoder with a state model q_theta(s_t | o_t) and optimize a contrastive bound (InfoNCE) that increases ln q(s_t | o_t) − ln average_o q(s_t | o) plus reward prediction and KL regularizer; encourages predictability of latent states from images while avoiding pixel prediction.",
            "model_type": "latent world model learned via contrastive objective (NCE / InfoNCE)",
            "task_domain": "Visual control (DeepMind Control Suite) from pixels",
            "fidelity_metric": "Contrastive loss (InfoNCE objective) and reward prediction log-likelihood; downstream task return as proxy.",
            "fidelity_performance": "In Dreamer's experiments, contrastive objective solved about half of the tasks and performed worse than pixel-reconstruction on most tasks; no numerical contrastive-loss-to-performance mapping provided.",
            "interpretability_assessment": "Latent states are trained to be predictable from images but are not explicitly interpretable; interpretability via inspection of downstream performance and possibly nearest-neighbor checks in latent space (not reported).",
            "interpretability_method": "No explicit interpretability method beyond evaluating task performance and possibly inspecting representations; InfoNCE inherently encourages discriminative representations across minibatch observations.",
            "computational_cost": "Avoids pixel decoding and thus can be computationally cheaper than reconstruction objectives; however, needs negative samples or minibatch averaging (InfoNCE), which has its own cost but still lighter than full decoder training.",
            "efficiency_comparison": "Cheaper than pixel reconstruction but produced weaker downstream performance in the limited-data experiments reported; may be preferable where compute or decoder modeling is prohibitive.",
            "task_performance": "Contrastive variant achieved intermediate results (solved ≈50% of tasks) — worse than reconstruction but better than reward-only in the reported experiments.",
            "task_utility_analysis": "Contrastive learning extracts predictive features from images useful for control, but with limited data its capacity to capture all task-relevant visual factors was lower than pixel reconstruction, reducing task utility.",
            "tradeoffs_observed": "Lower compute than reconstruction but less information extracted for control resulting in lower performance; less prone to pixel-level overfitting but may not capture fine-grained visual cues needed for some tasks.",
            "design_choices": "Implement state model as CNN q(s_t | o_t); use InfoNCE mini-batch bound: ln q(s_t | o_t) − ln_o q(s_t | o) plus reward prediction and KL regularizer; optimize jointly with transition and reward models.",
            "comparison_to_alternatives": "In the Dreamer experiments pixel-reconstruction &gt; contrastive &gt; reward-only for downstream control; contrastive is a middle ground favoring efficiency over pixel fidelity.",
            "optimal_configuration": "Paper suggests contrastive methods are promising but currently underperform pixel-reconstruction on these tasks; may become more competitive with more data or improved contrastive schemes.",
            "uuid": "e1270.3"
        },
        {
            "name_short": "Reward-prediction-only model",
            "name_full": "Latent dynamics learned only from reward prediction (no observation model)",
            "brief_description": "A minimal representation learning objective that trains the latent dynamics and reward predictor directly from actions and rewards without reconstructing images or using contrastive image objectives.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Reward-only latent dynamics",
            "model_description": "Learn a transition model q(s_t | s_{t-1}, a_{t-1}) and a reward predictor q(r_t | s_t) with a representation conditioned on past observations/actions but without any image reconstruction or contrastive objective to shape latents toward visual predictability.",
            "model_type": "latent world model trained from reward signal only",
            "task_domain": "Visual control (DeepMind Control Suite) — used as an ablation/variant in experiments",
            "fidelity_metric": "Reward prediction log-likelihood; downstream task return as primary fidelity metric.",
            "fidelity_performance": "In the experiments reported, reward-only representation was insufficient to learn good control policies from limited data and performed worst among the three objectives (reconstruction, contrastive, reward-only). No numeric fidelity measures reported.",
            "interpretability_assessment": "Latent states learned only from rewards tend to be minimal and task-specific; interpretability not assessed in the paper and likely limited because the model has no explicit link to images.",
            "interpretability_method": "None reported.",
            "computational_cost": "Cheapest among the three objectives since it avoids pixel decoding and contrastive negatives, but requires substantial environment experience to extract useful information when rewards are sparse.",
            "efficiency_comparison": "Computationally efficient but sample-inefficient in practice for visual control when rewards are sparse or limited data is available; performed poorly in Dreamer's experiments relative to reconstruction and contrastive objectives.",
            "task_performance": "Not sufficient by itself in the experimental regime used; tasks required more experience or additional signals to reach good performance.",
            "task_utility_analysis": "While lightweight, reward-only learning fails to capture necessary visual features with limited data, so high computational efficiency does not translate into task utility in these settings.",
            "tradeoffs_observed": "Lowest compute cost but poorest task utility; highlights trade-off where removing pixel objectives reduces compute but sacrifices representational richness needed for control.",
            "design_choices": "Omit observation model; train transition and reward predictor using available reward signals. Useful as lower bound ablation.",
            "comparison_to_alternatives": "Performs worse than pixel-reconstruction and contrastive objectives for Dreamer on the evaluated control tasks; may require much larger datasets to be effective.",
            "optimal_configuration": "Paper does not recommend reward-only for the evaluated low-data visual control tasks; suggests combining reward prediction with image-based objectives for practical performance.",
            "uuid": "e1270.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_in_a_handful_of_trials_using_probabilistic_dynamics_models"
        },
        {
            "paper_title": "Mastering Atari, Go, Chess and Shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Imagination-augmented agents for deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "imaginationaugmented_agents_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Model-predictive policy learning with uncertainty regularization for driving in dense traffic",
            "rating": 1,
            "sanitized_title": "modelpredictive_policy_learning_with_uncertainty_regularization_for_driving_in_dense_traffic"
        }
    ],
    "cost": 0.01863625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION</p>
<p>Danijar Hafner 
University of Toronto Google Brain
University of Toronto</p>
<p>Timothy Lillicrap Deepmind 
University of Toronto Google Brain
University of Toronto</p>
<p>Jimmy Ba 
University of Toronto Google Brain
University of Toronto</p>
<p>Mohammad Norouzi 
University of Toronto Google Brain
University of Toronto</p>
<p>Google Brain 
University of Toronto Google Brain
University of Toronto</p>
<p>Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION</p>
<p>Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.</p>
<p>INTRODUCTION</p>
<p>Intelligent agents can achieve goals in complex environments even though they never encounter the exact same situation twice. This ability requires building representations of the world from past experience that enable generalization to novel situations. World models offer an explicit way to represent an agent's knowledge about the world in a parametric model that can make predictions about the future. When the sensory inputs are high-dimensional images, latent dynamics models can abstract observations to predict forward in compact state spaces (Watter et al., 2015;Oh et al., 2017;Gregor et al., 2019). Compared to predictions in image space, latent states have a small memory footprint that enables imagining thousands of trajectories in parallel. Learning effective latent dynamics models is becoming feasible through advances in deep learning and latent variable models (Krishnan et al., 2015;Karl et al., 2016;Doerr et al., 2018;Buesing et al., 2018). Behaviors can be derived from dynamics models in many ways. Often, imagined rewards are maximized with a parametric policy (Sutton, 1991;Ha and Schmidhuber, 2018; or by online planning (Chua et al., 2018;. However, considering only rewards within a fixed imagination horizon results in shortsighted behaviors . Moreover, prior work commonly resorts to derivative-free optimization for robustness to model errors (Ebert et al., 2017;Chua et al., 2018;Parmas et al., 2019), rather than leveraging analytic gradients offered by neural network dynamics (Henaff et al., 2019;Srinivas et al., 2018). We present Dreamer, an agent that learns long-horizon behaviors from images purely by latent imagination. A novel actor critic algorithm accounts for rewards beyond the imagination horizon while making efficient use of the neural network dynamics. For this, we predict state values and actions in the learned latent space as summarized in Figure 1. The values optimize Bellman consistency for imagined rewards and the policy maximizes the values by propagating their analytic gradients back through the dynamics.</p>
<p>In comparison to actor critic algorithms that learn online or by experience replay Schulman et al., 2017;Haarnoja et al., 2018;Lee et al., 2019), world models can interpolate past experience and offer analytic gradients of multi-step returns for efficient policy optimization. The tasks pose a variety of challenges including contact dynamics, sparse rewards, many degrees of freedom, and 3D environments. Several of these tasks could previously not be solved through world models.</p>
<p>The key contributions of this paper are summarized as follows:</p>
<p>• Learning long-horizon behaviors by latent imagination Model-based agents can be shortsighted if they use a finite imagination horizon. We approach this limitation by predicting both actions and state values. Training purely by imagination in a latent space lets us efficiently learn the policy by propagating analytic value gradients back through the latent dynamics. • Empirical performance for visual control We pair Dreamer with existing representation learning methods and evaluate it on the DeepMind Control Suite with image inputs, illustrated in Figure 2. Using the same hyper parameters for all tasks, Dreamer exceeds previous model-based and model-free agents in terms of data-efficiency, computation time, and final performance.</p>
<p>CONTROL WITH WORLD MODELS</p>
<p>Reinforcement learning We formulate visual control as a partially observable Markov decision process (POMDP) with discrete time step t ∈ [1; T ], continuous vector-valued actions a t ∼ p(a t | o ≤t , a &lt;t ) generated by the agent, and high-dimensional observations and scalar rewards o t , r t ∼ p(o t , r t | o &lt;t , a &lt;t ) generated by the unknown environment. The goal is to develop an agent that maximizes the expected sum of rewards E p T t=1 r t . Figure 2 shows a selection of our tasks. Agent components The classical components of agents that learn in imagination are dynamics learning, behavior learning, and environment interaction (Sutton, 1991). In the case of Dreamer, the behavior is learned by predicting hypothetical trajectories in the compact latent space of the world model. As outlined in Figure 3 and detailed in Algorithm 1, Dreamer performs the following operations throughout the agent's life time, either interleaved or in parallel:</p>
<p>• Learning the latent dynamics model from the dataset of past experience to predict future rewards from actions and past observations. Any learning objective for the world model can be incorporated with Dreamer. We review existing methods for learning latent dynamics in Section 4. • Learning action and value models from predicted latent trajectories, as described in Section 3.</p>
<p>The value model optimizes Bellman consistency for imagined rewards and the action model is updated by propagating gradients of value estimates back through the neural network dynamics. • Executing the learned action model in the world to collect new experience for growing the dataset. Latent dynamics Dreamer uses a latent dynamics model that consists of three components. The representation model encodes observations and actions to create continuous vector-valued model states s t with Markovian transitions (Watter et al., 2015;. The transition model predicts future model states without seeing the corresponding observations that will later cause them. The reward model predicts the rewards given the model states,
Representation model: p(s t | s t−1 , a t−1 , o t ) Transition model: q(s t | s t−1 , a t−1 ) Reward model: q(r t | s t ).
(1)</p>
<p>We use p for distributions that generate samples in the real environment and q for their approximations that enable latent imagination. Specifically, the transition model lets us predict ahead in the compact latent space without having to observe or imagine the corresponding images. This results in a low memory footprint and fast predictions of thousands of imagined trajectories in parallel. The model mimics a non-linear Kalman filter (Kalman, 1960), latent state space model, or HMM with real-valued states. However, it is conditioned on actions and predicts rewards, allowing the agent to imagine the outcomes of potential action sequences without executing them in the environment.  </p>
<p>LEARNING BEHAVIORS BY LATENT IMAGINATION</p>
<p>Dreamer learns long-horizon behaviors in the compact latent space of a learned world model by efficiently leveraging the neural network latent dynamics. For this, we propagate stochastic gradients of multi-step returns through neural network predictions of actions, states, rewards, and values using reparameterization. This section describes the main contribution of our paper. Imagination environment The latent dynamics define a Markov decision process (MDP; Sutton, 1991) that is fully observed because the compact model states s t are Markovian. We denote imagined quantities with τ as the time index. Imagined trajectories start at the true model states s t of observation sequences drawn from the agent's past experience. They follow predictions of the transition model s τ ∼ q(s τ | s τ −1 , a τ −1 ), reward model r τ ∼ q(r τ | s τ ), and a policy a τ ∼ q(a τ | s τ ). The objective is to maximize expected imagined rewards E q ∞ τ =t γ τ −t r τ with respect to the policy.</p>
<p>Algorithm 1: Dreamer</p>
<p>Initialize dataset D with S random seed episodes. Initialize neural network parameters θ, φ, ψ randomly. while not converged do for update step c = 1..C do
// Dynamics learning Draw B data sequences {(a t , o t , r t )} k+L t=k ∼ D. Compute model states s t ∼ p θ (s t | s t−1 , a t−1 , o t ).
Update θ using representation learning.</p>
<p>// Behavior learning
Imagine trajectories {(s τ , a τ )} t+H τ =t from each s t . Predict rewards E q θ (r τ | s τ ) and values v ψ (s τ ). Compute value estimates V λ (s τ ) via Equation 6. Update φ ← φ + α∇ φ t+H τ =t V λ (s τ ). Update ψ ← ψ − α∇ ψ t+H τ =t 1 2 v ψ (s τ ) V λ (s τ ) 2 . // Environment interaction o 1 ← env.reset() for time step t = 1..T do Compute s t ∼ p θ (s t | s t−1 , a t−1 , o t ) from history.
Compute a t ∼ q φ (a t | s t ) with the action model. Add exploration noise to action. r t , o t+1 ← env.step(a t ). Add experience to dataset D ← D ∪ {(o t , a t , r t ) T t=1 }.  Figure 4: Imagination horizons. We compare the final performance of Dreamer, learning an action model without value prediction, and online planning using PlaNet. Learning a state value model to estimate rewards beyond the imagination horizon makes Dreamer more robust to the horizon length. The agents use pixel reconstruction for representation learning and an action repeat of R = 2.</p>
<p>Model components
Representation p θ (s t | s t-1 , a t-1 , o t ) Transition q θ (s t | s t-1 , a t-1 ) Reward q θ (r t | s t ) Action q φ (a t | s t ) Value v ψ (s t )</p>
<p>Hyper parameters</p>
<p>Action and value models Consider imagined trajectories with a finite horizon H. Dreamer uses an actor critic approach to learn behaviors that consider rewards beyond the horizon. We learn an action model and a value model in the latent space of the world model for this. The action model implements the policy and aims to predict actions that solve the imagination environment. The value model estimates the expected imagined rewards that the action model achieves from each state s τ , Action model:
a τ ∼ q φ (a τ | s τ ) Value model: v ψ (s τ ) ≈ E q(·|sτ ) t+H τ =t γ τ −t r τ .(2)
The action and value models are trained cooperatively as typical in policy iteration: the action model aims to maximize an estimate of the value, while the value model aims to match an estimate of the value that changes as the action model changes.</p>
<p>We use dense neural networks for the action and value models with parameters φ and ψ, respectively. The action model outputs a tanh-transformed Gaussian (Haarnoja et al., 2018) with sufficient statistics predicted by the neural network. This allows for reparameterized sampling (Kingma and Welling, 2013;Rezende et al., 2014) that views sampled actions as deterministically dependent on the neural network output, allowing us to backpropagate analytic gradients through the sampling operation,
a τ = tanh µ φ (s τ ) + σ φ (s τ ) , ∼ Normal(0, I).(3)
Value estimation To learn the action and value models, we need to estimate the state values of imagined trajectories {s τ , a τ , r τ } t+H τ =t . These trajectories branch off of the model states s t of sequence batches drawn from the agent's dataset of experience and predict forward for the imagination horizon H using actions sampled from the action model. State values can be estimated in multiple ways that trade off bias and variance (Sutton and Barto, 2018),
V R (s τ ) . = E q θ ,q φ t+H n=τ r n ,(4)V k N (s τ ) . = E q θ ,q φ h−1 n=τ γ n−τ r n + γ h−τ v ψ (s h ) with h = min(τ + k, t + H),(5)V λ (s τ ) . = (1 − λ) H−1 n=1 λ n−1 V n N (s τ ) + λ H−1 V H N (s τ ),(6)
where the expectations are estimated under the imagined trajectories. V R simply sums the rewards from τ until the horizon and ignores rewards beyond it. This allows learning the action model without a value model, an ablation we compare to in our experiments. V k N estimates rewards beyond k steps with the learned value model. Dreamer uses V λ , an exponentially-weighted average of the estimates for different k to balance bias and variance. Figure 4 shows that learning a value model in imagination enables Dreamer to solve long-horizon tasks while being robust to the imagination horizon. The experimental details and results on all tasks are described in Section 6. Learning objective To update the action and value models, we first compute the value estimates V λ (s τ ) for all states s τ along the imagined trajectories. The objective for the action model q φ (a τ | s τ ) is to predict actions that result in state trajectories with high value estimates. The objective for the value model v ψ (s τ ), in turn, is to regress the value estimates,
max φ E q θ ,q φ t+H τ =t V λ (s τ ) , (7) min ψ E q θ ,q φ t+H τ =t 1 2 v ψ (s τ ) − V λ (s τ )) 2 .(8)
The value model is updated to regress the targets, around which we stop the gradient as typical (Sutton and Barto, 2018). The action model uses analytic gradients through the learned dynamics to maximize the value estimates. To understand this, we note that the value estimates depend on the reward and value predictions, which depend on the imagined states, which in turn depend on the imagined actions. Since all steps are implemented as neural networks, we analytically compute ∇ φ E q θ ,q φ t+H τ =t V λ (s τ ) by stochastic backpropagation (Kingma and Welling, 2013;Rezende et al., 2014). We use reparameterization for continuous actions and latent states and straight-through gradients (Bengio et al., 2013) for discrete actions. The world model is fixed while learning behaviors. In tasks with early termination, the world model also predicts the discount factor from each latent state to weigh the time steps in Equations 7 and 8 by the cumulative product of the predicted discount factors, so terms are weighted down based on how likely the imagined trajectory would have ended. Comparison to actor critic methods Agents using Reinforce gradients (Williams, 1992), such as A3C and PPO Schulman et al., 2017), employ value baselines to reduce gradient variance, while Dreamer backpropagates through the value model. This is similar to deterministic or reparameterized actor critics (Silver et al., 2014), such as DDPG and SAC Haarnoja et al., 2018). However, these do not leverage gradients through transitions and only maximize immediate Q-values. MVE and STEVE (Feinberg et al., 2018;Buckman et al., 2018) extend them to multi-step Q-learning with learned dynamics to provide more accurate Q-value targets. We predict state values, which is sufficient for policy optimization since we backpropagate through the dynamics. Refer to Section 5 for a more detailed comparison to related work.</p>
<p>LEARNING LATENT DYNAMICS</p>
<p>Learning behaviors in imagination requires a world model that generalizes well. We focus on latent dynamics models that predict forward in a compact latent space, facilitating long-term predictions and allowing the agent to imagine thousands of trajectories in parallel. Several objectives for learning representations for control have been proposed (Watter et al., 2015;Jaderberg et al., 2016;Oord et al., 2018;. We review three approaches for learning representations to use with Dreamer: reward prediction, image reconstruction, and contrastive estimation. Reward prediction Latent imagination requires a representation model p(s t | s t−1 , a t−1 , o t ), transition model q(s t | s t−1 , a t−1 , ), and reward model q(r t | s t ), as described in Section 2. In principle, this could be achieved by simply learning to predict future rewards given actions and past observations (Oh et al., 2017;Gelada et al., 2019;Schrittwieser et al., 2019). With a large and diverse dataset, such representations should be sufficient for solving a control task. However, with a finite dataset and especially when rewards are sparse, learning about observations that correlate with rewards is likely to improve the world model (Jaderberg et al., 2016;Gregor et al., 2019). Episode Return n/a n/a n/a n/a Dreamer (5e6 steps) PlaNet (5e6 steps) D4PG (1e8 steps) A3C (1e8 steps, proprio) Figure 6: Performance comparison to existing methods. Dreamer inherits the data-efficiency of PlaNet while exceeding the asymptotic performance of the best model-free agents. After 5 × 10 6 environment steps, Dreamer reaches an average performance of 823 across tasks, compared to PlaNet at 332 and the top model-free D4PG agent at 786 after 10 8 steps. Results are averages over 5 seeds.</p>
<p>Reconstruction We first describe the world model used by PlaNet ) that learns latent dynamics by reconstructing images as shown in Figure 3a. The world model consists of the following components, where the observation model is only used to provide a learning signal,
Representation model: p θ (s t | s t−1 , a t−1 , o t ) Observation model: q θ (o t | s t ) Reward model: q θ (r t | s t ) Transition model: q θ (s t | s t−1 , a t−1 ).(9)
The components are optimized jointly to increase the variational lower bound (ELBO; Jordan et al., 1999) or more generally the variational information bottleneck (VIB; Tishby et al., 2000;Alemi et al., 2016). As derived in Appendix B, the bound includes reconstruction terms for observations and rewards and a KL regularizer. The expectation is taken under the dataset and representation model,
J REC . = E p t J t O + J t R + J t D + const J t O . = ln q(o t | s t ) J t R . = ln q(r t | s t ) J t D . = −β KL p(s t | s t−1 , a t−1 , o t ) q(s t | s t−1 , a t−1 ) .(10)
We implement the transition model as a recurrent state space model (RSSM; , the representation model by combining the RSSM with a convolutional neural network (CNN; LeCun et al., 1989) applied to the image observation, the observation model as a transposed CNN, and the reward model as a dense network. The combined parameter vector θ is updated by stochastic backpropagation (Kingma and Welling, 2013;Rezende et al., 2014). Figure 5 shows video predictions of this model. We refer to Appendix A and  model details.</p>
<p>Contrastive estimation Predicting pixels can require high model capacity. We can also encourage mutual information between model states and observations by instead predicting the states from the images (Guo et al., 2018). This replaces the observation model with a state model,
State model: q θ (s t | o t ).(11)
While the reconstruction objective used the fact that the observation marginal is a constant, we now face the state marginal. As shown in Appendix B, this can be estimated via noise contrastive estimation (NCE; Gutmann and Hyvärinen, 2010;Oord et al., 2018) by averaging the state model over observations o of the current sequence batch. Intuitively, q(s t | o t ) makes the state predictable from the current image while ln o q(s t | o ) keeps it diverse to prevent collapse,
J NCE . = E t J t S + J t R + J t D J t S . = ln q(s t | o t ) − ln o q(s t | o ) .(12)
We implement the state model as a CNN and again optimize the bound with respect to the combined parameter vector θ using stochastic backpropagation. While avoiding pixel prediction, the amount of information this bound can extract efficiently is limited (McAllester and Statos, 2018). We empirically compare reward, reconstruction, and contrastive objectives in our experiments in Figure 8. </p>
<p>RELATED WORK</p>
<p>Prior works learn latent dynamics for visual control by derivative-free policy learning or online planning, augment model-free agents with multi-step predictions, or use analytic gradients of Qvalues or multi-step rewards, often for low-dimensional tasks. In comparison, Dreamer uses analytic gradients to efficiently learn long-horizon behaviors for visual control purely by latent imagination.  (Schmidhuber, 1990;Henaff et al., 2018) but has been challenging to scale (Parmas et al., 2019). Analytic value gradients DPG (Silver et al., 2014), DDPG , and SAC (Haarnoja et al., 2018) leverage gradients of learned immediate action values to learn a policy by experience replay. SVG  reduces the variance of model-free on-policy algorithms by analytic value gradients of one-step model predictions. Concurrent work by Byravan et al. (2019) uses latent imagination with deterministic models for navigation and manipulation tasks. ME-TRPO (Kurutach et al., 2018) accelerates an otherwise model-free agent via gradients of predicted rewards for proprioceptive inputs. DistGBP (Henaff et al., 2017; 2019) uses model gradients for online planning in simple tasks. </p>
<p>EXPERIMENTS</p>
<p>We experimentally evaluate Dreamer on a variety of control tasks. We designed the experiments to compare Dreamer to current best methods in the literature, and to evaluate its ability to solve tasks with long horizons, continuous actions, discrete actions, and early termination. We further compare the orthogonal choice of learning objective for the world model. The source code for all our experiments and videos of Dreamer are available at https://danijar.com/dreamer.</p>
<p>Control tasks</p>
<p>We evaluate Dreamer on 20 visual control tasks of the DeepMind Control Suite (Tassa et al., 2018), illustrated in Figure 2. These tasks pose a variety of challenges, including sparse rewards, contact dynamics, and 3D scenes. We selected the tasks on which Tassa et al. (2018) report non-zero performance from image inputs. Agent observations are images of shape 64 × 64 × 3, actions range from 1 to 12 dimensions, rewards range from 0 to 1, episodes last for 1000 steps and have randomized initial states. We use a fixed action repeat of R = 2 across tasks. We further evaluate the applicability of Dreamer to discrete actions and early termination on a subset of Atari games (Bellemare et al., 2013) and DeepMind Lab levels (Beattie et al., 2016) as detailed in Appendix C.</p>
<p>Implementation Our implementation uses TensorFlow Probability (Dillon et al., 2017). We use a single Nvidia V100 GPU and 10 CPU cores for each training run. The training time for our Dreamer implementation is about 3 hours per 10 6 environment steps on the control suite, compared to 11 hours for online planning using PlaNet, and the 24 hours used by D4PG to reach similar performance. We use the same hyper parameters across all continuous tasks, and similarly across all discrete tasks, detailed in Appendix A. The world models are learned via reconstruction unless specified.</p>
<p>Baseline methods The highest reported performance on the continuous tasks is achieved by D4PG (Barth-Maron et al., 2018), an improved variant of DDPG  that uses distributed collection, distributional Q-learning, multi-step returns, and prioritized replay. We include the scores for D4PG with pixel inputs and A3C  with state inputs from Tassa et al. (2018). PlaNet  learns the same world model as Dreamer and selects actions via online planning without an action model and drastically improves over D4PG and A3C in data efficiency. We re-run PlaNet with R = 2 for a unified experimental setup. For Atari, we show the final performance of SimPLe (Kaiser et al., 2019), DQN (Mnih et al., 2015) and Rainbow (Hessel et al., 2018) reported by Castro et al. (2018), and for DeepMind Lab that of IMPALA (Espeholt et al., 2018) as a guideline.</p>
<p>Performance To evaluate the performance of Dreamer, we compare it to state-of-the-art reinforcement learning agents. The results are summarized in Figure 6. With an average score of 823 across tasks after 5 × 10 6 environment steps, Dreamer exceeds the performance of the strong model-free D4PG agent that achieves an average of 786 within 10 8 environment steps. At the same time, Dreamer inherits the data-efficiency of PlaNet, confirming that the learned world model can help to generalize from small amounts of experience. The empirical success of Dreamer shows that learning behaviors by latent imagination with world models can outperform top methods based on experience replay. Long horizons To investigate its ability to learn long-horizon behaviors, we compare Dreamer to alternatives for deriving behaviors from the world model at various horizon lengths. For this, we learn an action model to maximize imagined rewards without a value model and compare to online planning using PlaNet. Figure 4 shows the final performance for different imagination horizons, confirming that the value model makes Dreamer more robust to the horizon and performs well even for short horizons. Performance curves for all 19 tasks with horizon of 20 are shown in Appendix D, where Dreamer outperforms the alternatives on 16 of 20 tasks, with 4 ties. Representation learning Dreamer can be used with any differentiable dynamics model that predicts future rewards given actions and past observations. Since the representation learning objective is orthogonal to our algorithm, we compare three natural choices described in Section 4: pixel reconstruction, contrastive estimation, and pure reward prediction. Figure 8 shows clear differences in task performance for different representation learning approaches, with pixel reconstruction outperforming contrastive estimation on most tasks. This suggests that future improvements in representation learning are likely to translate to higher task performance with Dreamer. Reward prediction alone was not sufficient in our experiments. Further ablations are included in the appendix of the paper.</p>
<p>CONCLUSION</p>
<p>We present Dreamer, an agent that learns long-horizon behaviors purely by latent imagination. For this, we propose an actor critic method that optimizes a parametric policy by propagating analytic gradients of multi-step values back through learned latent dynamics. Dreamer outperforms previous methods in data-efficiency, computation time, and final performance on a variety of challenging continuous control tasks with image inputs. We further show that Dreamer is applicable to tasks with discrete actions and early episode termination. Future research on representation learning can likely scale latent imagination to environments of higher visual complexity.</p>
<p>A HYPER PARAMETERS</p>
<p>Model components We use the convolutional encoder and decoder networks from Ha and Schmidhuber (2018), the RSSM of , and implement all other functions as three dense layers of size 300 with ELU activations (Clevert et al., 2015). Distributions in latent space are 30-dimensional diagonal Gaussians. The action model outputs a tanh mean scaled by a factor of 5 and a softplus standard deviation for the Normal distribution that is then transformed using tanh (Haarnoja et al., 2018). The scaling factor allows the agent to saturate the action distribution.</p>
<p>Learning updates We draw batches of 50 sequences of length 50 to train the world model, value model, and action model models using Adam (Kingma and Ba, 2014) with learning rates 6 × 10 −4 , 8 × 10 −5 , 8 × 10 −5 , respectively and scale down gradient norms that exceed 100. We do not scale the KL regularizers (β = 1) but clip them below 3 free nats as in PlaNet. The imagination horizon is H = 15 and the same trajectories are used to update both action and value models. We compute the V λ targets with γ = 0.99 and λ = 0.95. We did not find latent overshooting for learning the model, an entropy bonus for the action model, or target networks for the value model necessary.</p>
<p>Environment interaction</p>
<p>The dataset is initialized with S = 5 episodes collected using random actions. We iterate between 100 training steps and collecting 1 episode by executing the predicted mode action with Normal(0, 0.3) exploration noise. Instead of manually selecting the action repeat for each environment as in  and Lee et al. (2019), we fix it to 2 for all environments. See Figure 12 for an assessment of the robustness to different action repeat values. Discrete control For experiments on Atari games and DeepMind Lab levels, the action model predicts the logits of a categorical distribution. We use straight-through gradients for the sampling step during latent imagination. The action noise is epsilon greedy where is linearly scheduled from 0.4 → 0.1 over the first 200, 000 gradient steps. To account for the higher complexity of these tasks, we use an imagination horizon of H = 10, scale the KL regularizers by β = 0.1, and bound rewards using tanh. We predict the discount factor from the latent state with a binary classifier that is trained towards the soft labels of 0 and γ.</p>
<p>B DERIVATIONS</p>
<p>We define the information bottleneck objective (Tishby et al., 2000) for latent dynamics models, max I(s 1:T ; (o 1:T , r 1:T ) | a 1:T ) − β I(s 1:T , i 1:T | a 1:T ),</p>
<p>where β is scalar and i t are dataset indices that determine the observations p(o t | i t ) .</p>
<p>= δ(o t −ō t ) as in Alemi et al. (2016). Maximizing the objective leads to model states that can predict the sequence of observations and rewards while limiting the amount of information extracted at each time step. This encourages the model to reconstruct each image by relying on information extracted at preceeding time steps to the extent possible, and only accessing additional information from the current image when necessary. As a result, the information regularizer encourages the model to learn long-term dependencies. For the generative objective, we lower bound the first term using the non-negativity of the KL divergence and drop the marginal data probability as it does not depend on the representation model, 
) t q(o t | s t )q(r t | s t ) = E t ln q(o t | s t ) + ln q(r t | s t ) .(14)
For the contrastive objective, we subtract the constant marginal probability of the data under the variational encoder, apply Bayes rule, and use the InfoNCE mini-batch bound (Poole et al., 2019),
E ln q(o t | s t ) + ln q(r t | s t ) + = E ln q(o t | s t ) − ln q(o t ) + ln q(r t | s t ) = E ln q(s t | o t ) − ln q(s t ) + ln q(r t | s t ) ≥ E ln q(s t | o t ) − ln o q(s t | o ) + ln q(r t | s t ) .(15)
For the second term, we use the non-negativity of the KL divergence to obtain an upper bound, I(s 1:T ; i 1:T | a 1:T ) = E p(o 1:T ,r 1:T ,s 1:T ,a 1:
T ,i 1:T ) t ln p(s t | s t−1 , a t−1 , i t ) − ln p(s t | s t−1 , a t−1 ) = E t ln p(s t | s t−1 , a t−1 , o t ) − ln p(s t | s t−1 , a t−1 ) ≤ E t ln p(s t | s t−1 , a t−1 , o t ) − ln q(s t | s t−1 , a t−1 ) = E t KL p(s t | s t−1 , a t−1 , o t ) q(s t | s t−1 , a t−1 ) .(16)
This lower bounds the objective.</p>
<p>C DISCRETE CONTROL</p>
<p>We evaluate Dreamer on a subset of tasks with discrete actions from the Atari suite (Bellemare et al., 2013) and DeepMind Lab (Beattie et al., 2016). While agents that purely learn through world models are not yet competitive in these domains (Kaiser et al., 2019), the tasks offer a diverse test bed with visual complexity, sparse rewards, and early termination. Agents observe 64 × 64 × 3 images and select one of between 3 and 18 actions. For Atari, we follow the evaluation protocol of Machado et al. (2018) with sticky actions. Refer to Figure 9 for these experiments. The lines show mean scores over environment steps and the shaded areas show the standard deviation across 5 seeds. We compare Dreamer that learns both actions and values in imagination, to only learning actions in imagination, and Planet that selects actions by online planning instead of learning a policy. The baselines include the top model-free algorithm D4PG, the well-known A3C agent, and the hybrid SLAC agent. Figure 11: Comparison of representation learning methods for Dreamer. The lines show mean scores and the shaded areas show the standard deviation across 5 seeds. We compare generating both images and rewards, generating rewards and using a contrastive loss to learn about the images, and only predicting rewards. Image reconstruction provides the best learning signal across most of the tasks, followed by the contrastive objective. Learning purely from rewards was not sufficient in our experiments and might require larger amounts of experience. </p>
<p>G CONTINUOUS CONTROL SCORES</p>
<p>Figure 1 :
1Dreamer learns a world model from past experience and efficiently learns farsighted behaviors in its latent space by backpropagating value estimates back through imagined trajectories.</p>
<p>Figure 2 :
2Image observations for 5 of the 20 visual control tasks used in our experiments.</p>
<p>Figure 3 :
3Components of Dreamer. (a) From the dataset of past experience, the agent learns to encode observations and actions into compact latent states ( ), for example via reconstruction, and predicts environment rewards ( ). (b) In the compact latent space, Dreamer predicts state values ( ) and actions ( ) that maximize future value predictions by propagating gradients back through imagined trajectories. (c) The agent encodes the history of the episode to compute the current model state and predict the next action to execute in the environment. See Algorithm 1 for pseudo code of the agent.</p>
<p>Figure 5 :
5Reconstructions of long-term predictions. We apply the representation model to the first 5 images of two hold-out trajectories and predict forward for 45 steps using the latent dynamics, given only the actions. The recurrent state space model (RSSM; performs accurate long-term predictions, enabling Dreamer to learn successful behaviors in a compact latent space.</p>
<p>Figure 7 :
7Dreamer succeeds at visual control tasks that require long-horizon credit assignment, such as the acrobot and hopper tasks. Optimizing only imagined rewards within the horizon via an action model or by online planning yields shortsighted behaviors that only succeed in reactive tasks, such as in the walker domain. The performance on all 20 tasks is summarized inFigure 6and training curves are shown in Appendix D. SeeTassa et al. (2018) for performance curves of D4PG and A3C.</p>
<p>Control with latent dynamics E2C(Watter et al., 2015) and RCE(Banijamali et al., 2017) embed images to predict forward in a compact space to solve simple tasks. World Models(Ha and Schmidhuber, 2018) learn latent dynamics in a two-stage process to evolve linear controllers in imagination. PlaNet learns them jointly and solves visual locomotion tasks by latent online planning. SOLAR solves robotic tasks via guided policy search in latent space. I2A(Weber et al., 2017) hands imagined trajectories to a model-free policy, whileLee et al. (2019) andGregor et al. (2019) learn belief representations to accelerate model-free agents. Imagined multi-step returns VPN(Oh et al., 2017), MVE(Feinberg et al., 2018), and STEVE(Buckman et al., 2018) learn dynamics for multi-step Q-learning from a replay buffer. AlphaGo(Silver et al., 2017) combines predictions of actions and state values with planning, assuming access to the true dynamics. Also assuming access to the dynamics, POLO(Lowrey et al., 2018) plans to explore by learning a value ensemble. MuZero(Schrittwieser et al., 2019) learns task-specific reward and value models to solve challenging tasks but requires large amounts of experience. PETS(Chua et al., 2018), VisualMPC(Ebert et al., 2017), and PlaNet plan online using derivative-free optimization. POPLIN (Wang and Ba, 2019) improves over online planning by self-imitation.Piergiovanni et al. (2018) learn robot policies by imagination with a latent dynamics model. Planning with neural network gradients was shown on small problems</p>
<p>Figure 8 :
8Comparison of representation learning objectives to be used with Dreamer. Pixel reconstruction performs best for the majority of tasks. The contrastive objective solves about half of the tasks, while predicting rewards alone was not sufficient in our experiments. The results suggest that future developments in learning representations are likely to translate into improved task performance for Dreamer. The performance curves for all tasks are included in Appendix E.</p>
<p>I
(s 1:T ; (o 1:T , r 1:T ) | a 1:T ) = E p(o 1:T ,r 1:T ,s 1:T ,a 1:T ) t ln p(o 1:T , r 1:T | s 1:T , a 1:T ) − ln p(o 1:T , r 1:T | a 1:T ) p(o 1:T , r 1:T | s 1:T , a 1:T ) − KL p(o 1:T , r 1:T | s 1:T , a 1:T</p>
<p>Figure 9 :Figure 10 :
910Performance of Dreamer in environments with discrete actions and early termination. Dreamer learns successful behaviors on this subset of Atari games and the object collection level of DMLab. We highlight representation learning for these environments as a direction of future work that could enable competitive performance across all Atari games and DMLab levels using Dreamer. Comparison of action selection schemes on the continuous control tasks of the DeepMind Control Suite from pixel inputs.
We re-run PlaNet with fixed action repeat of R = 2 to not tune the this value for each of the 20 tasks. As a result, the scores differ from.
Acknowledgements We thank Simon Kornblith, Benjamin Eysenbach, Ian Fischer, Amy Zhang, Geoffrey Hinton, Shane Gu, Adam Kosiorek, Brandon Amos, Jacob Buckman, Calvin Luo, and Rishabh Agarwal, and our anonymous reviewers for feedback and discussions. We thank Yuval Tassa for adding the quadruped environment to the control suite.Reinforcement learning methods can be sensitive to this hyper parameter, which could be amplified when learning dynamics models at the control frequency of the environment. For this experiment, we train Dreamer with different amounts of action repeat. The areas show one standard deviation across 2 seeds. We used a previous hyper parameter setting for this experiment. We find that a value of R = 2 works best across tasks.
A A Alemi, I Fischer, J V Dillon, K Murphy, arXiv:1612.00410Deep variational information bottleneck. arXiv preprintA. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.</p>
<p>. E Banijamali, R Shu, M Ghavamzadeh, H Bui, A Ghodsi, arXiv:1710.05373arXiv preprintRobust locally-linear controllable embeddingE. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable embedding. arXiv preprint arXiv:1710.05373, 2017.</p>
<p>G Barth-Maron, M W Hoffman, D Budden, W Dabney, D Horgan, A Muldal, N Heess, T Lillicrap, arXiv:1804.08617Distributed distributional deterministic policy gradients. arXiv preprintG. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, A. Muldal, N. Heess, and T. Lil- licrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018.</p>
<p>. C Beattie, J Z Leibo, D Teplyashin, T Ward, M Wainwright, H Küttler, A Lefrancq, S Green, V Valdés, A Sadik, arXiv:1612.03801Deepmind lab. arXiv preprintC. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green, V. Valdés, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 47M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N Léonard, A Courville, arXiv:1308.3432arXiv preprintY. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.</p>
<p>Sample-efficient reinforcement learning with stochastic ensemble value expansion. J Buckman, D Hafner, G Tucker, E Brevdo, H Lee, Advances in Neural Information Processing Systems. J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural Information Processing Systems, pages 8224-8234, 2018.</p>
<p>Learning and querying fast generative models for reinforcement learning. L Buesing, T Weber, S Racaniere, S Eslami, D Rezende, D P Reichert, F Viola, F Besse, K Gregor, D Hassabis, arXiv:1802.03006arXiv preprintL. Buesing, T. Weber, S. Racaniere, S. Eslami, D. Rezende, D. P. Reichert, F. Viola, F. Besse, K. Gregor, D. Hassabis, et al. Learning and querying fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.</p>
<p>Imagined value gradients: Model-based policy optimization with transferable latent dynamics models. A Byravan, J T Springenberg, A Abdolmaleki, R Hafner, M Neunert, T Lampe, N Siegel, N Heess, M Riedmiller, arXiv:1910.04142arXiv preprintA. Byravan, J. T. Springenberg, A. Abdolmaleki, R. Hafner, M. Neunert, T. Lampe, N. Siegel, N. Heess, and M. Riedmiller. Imagined value gradients: Model-based policy optimization with transferable latent dynamics models. arXiv preprint arXiv:1910.04142, 2019.</p>
<p>P S Castro, S Moitra, C Gelada, S Kumar, M G Bellemare, arXiv:1812.06110Dopamine: A research framework for deep reinforcement learning. arXiv preprintP. S. Castro, S. Moitra, C. Gelada, S. Kumar, and M. G. Bellemare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, Advances in Neural Information Processing Systems. K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754-4765, 2018.</p>
<p>Fast and accurate deep network learning by exponential linear units (elus). D.-A Clevert, T Unterthiner, S Hochreiter, arXiv:1511.07289arXiv preprintD.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.</p>
<p>. J V Dillon, I Langmore, D Tran, E Brevdo, S Vasudevan, D Moore, B Patton, A Alemi, M Hoffman, R A Saurous, arXiv:1711.10604Tensorflow distributions. arXiv preprintJ. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasudevan, D. Moore, B. Patton, A. Alemi, M. Hoffman, and R. A. Saurous. Tensorflow distributions. arXiv preprint arXiv:1711.10604, 2017.</p>
<p>Probabilistic recurrent state-space models. A Doerr, C Daniel, M Schiegg, D Nguyen-Tuong, S Schaal, M Toussaint, S Trimpe, arXiv:1801.10395arXiv preprintA. Doerr, C. Daniel, M. Schiegg, D. Nguyen-Tuong, S. Schaal, M. Toussaint, and S. Trimpe. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.</p>
<p>Self-supervised visual planning with temporal skip connections. F Ebert, C Finn, A X Lee, S Levine, arXiv:1710.05268arXiv preprintF. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip connections. arXiv preprint arXiv:1710.05268, 2017.</p>
<p>Neural scene representation and rendering. S A Eslami, D J Rezende, F Besse, F Viola, A S Morcos, M Garnelo, A Ruderman, A A Rusu, I Danihelka, K Gregor, Science. 3606394S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor, et al. Neural scene representation and rendering. Science, 360(6394): 1204-1210, 2018.</p>
<p>L Espeholt, H Soyer, R Munos, K Simonyan, V Mnih, T Ward, Y Doron, V Firoiu, T Harley, I Dunning, arXiv:1802.01561Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprintL. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.</p>
<p>Model-based value estimation for efficient model-free reinforcement learning. V Feinberg, A Wan, I Stoica, M I Jordan, J E Gonzalez, S Levine, arXiv:1803.00101arXiv preprintV. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and S. Levine. Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.</p>
<p>Deepmdp: Learning continuous latent space models for representation learning. C Gelada, S Kumar, J Buckman, O Nachum, M G Bellemare, arXiv:1906.02736arXiv preprintC. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. arXiv preprint arXiv:1906.02736, 2019.</p>
<p>K Gregor, D J Rezende, F Besse, Y Wu, H Merzic, A V D Oord, arXiv:1906.09237Shaping belief states with generative environment models for rl. arXiv preprintK. Gregor, D. J. Rezende, F. Besse, Y. Wu, H. Merzic, and A. v. d. Oord. Shaping belief states with generative environment models for rl. arXiv preprint arXiv:1906.09237, 2019.</p>
<p>Z D Guo, M G Azar, B Piot, B A Pires, T Pohlen, R Munos, arXiv:1811.06407Neural predictive belief representations. arXiv preprintZ. D. Guo, M. G. Azar, B. Piot, B. A. Pires, T. Pohlen, and R. Munos. Neural predictive belief representations. arXiv preprint arXiv:1811.06407, 2018.</p>
<p>Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. M Gutmann, A Hyvärinen, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsM. Gutmann and A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297-304, 2010.</p>
<p>World models. D Ha, J Schmidhuber, arXiv:1803.10122arXiv preprintD. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, arXiv:1801.01290arXiv preprintT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.</p>
<p>D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, arXiv:1811.04551Learning latent dynamics for planning from pixels. arXiv preprintD. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Learning continuous control policies by stochastic value gradients. N Heess, G Wayne, D Silver, T Lillicrap, T Erez, Y Tassa, Advances in Neural Information Processing Systems. N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages 2944-2952, 2015.</p>
<p>Model-based planning in discrete action spaces. M Henaff, W F Whitney, Y Lecun, abs/1705.07177CoRRM. Henaff, W. F. Whitney, and Y. LeCun. Model-based planning in discrete action spaces. CoRR, abs/1705.07177, 2017.</p>
<p>M Henaff, W F Whitney, Y Lecun, arXiv:1705.07177Model-based planning with discrete and continuous actions. arXiv preprintM. Henaff, W. F. Whitney, and Y. LeCun. Model-based planning with discrete and continuous actions. arXiv preprint arXiv:1705.07177, 2018.</p>
<p>Model-predictive policy learning with uncertainty regularization for driving in dense traffic. M Henaff, A Canziani, Y Lecun, arXiv:1901.02705arXiv preprintM. Henaff, A. Canziani, and Y. LeCun. Model-predictive policy learning with uncertainty regulariza- tion for driving in dense traffic. arXiv preprint arXiv:1901.02705, 2019.</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. M Hessel, J Modayil, H Van Hasselt, T Schaul, G Ostrovski, W Dabney, D Horgan, B Piot, M Azar, D Silver, Thirty-Second AAAI Conference on Artificial Intelligence. M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>M Jaderberg, V Mnih, W M Czarnecki, T Schaul, J Z Leibo, D Silver, K Kavukcuoglu, arXiv:1611.05397Reinforcement learning with unsupervised auxiliary tasks. arXiv preprintM. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.</p>
<p>An introduction to variational methods for graphical models. M I Jordan, Z Ghahramani, T S Jaakkola, L K Saul, Machine learning. 372M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.</p>
<p>Model-based reinforcement learning for atari. L Kaiser, M Babaeizadeh, P Milos, B Osinski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, arXiv:1903.00374arXiv preprintL. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>A new approach to linear filtering and prediction problems. R E Kalman, Journal of basic Engineering. 821R. E. Kalman. A new approach to linear filtering and prediction problems. Journal of basic Engineering, 82(1):35-45, 1960.</p>
<p>Deep variational bayes filters: Unsupervised learning of state space models from raw data. M Karl, M Soelch, J Bayer, P Van Der, Smagt, arXiv:1605.06432arXiv preprintM. Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. arXiv preprintD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>R G Krishnan, U Shalit, D Sontag, arXiv:1511.05121Deep kalman filters. arXiv preprintR. G. Krishnan, U. Shalit, and D. Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.</p>
<p>Model-ensemble trust-region policy optimization. T Kurutach, I Clavera, Y Duan, A Tamar, P Abbeel, arXiv:1802.10592arXiv preprintT. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.</p>
<p>Backpropagation applied to handwritten zip code recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , Neural computation. 14Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541-551, 1989.</p>
<p>Stochastic latent actor-critic. A X Lee, A Nagabandi, P Abbeel, S Levine, arXiv:1907.00953Deep reinforcement learning with a latent variable model. arXiv preprintA. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Plan online, learn offline: Efficient learning and exploration via model-based control. K Lowrey, A Rajeswaran, S Kakade, E Todorov, I Mordatch, arXiv:1811.01848arXiv preprintK. Lowrey, A. Rajeswaran, S. Kakade, E. Todorov, and I. Mordatch. Plan online, learn offline: Efficient learning and exploration via model-based control. arXiv preprint arXiv:1811.01848, 2018.</p>
<p>Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. M C Machado, M G Bellemare, E Talvitie, J Veness, M Hausknecht, M Bowling, Journal of Artificial Intelligence Research. 61M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.</p>
<p>Formal limitations on the measurement of mutual information. D Mcallester, K Statos, arXiv:1811.04251arXiv preprintD. McAllester and K. Statos. Formal limitations on the measurement of mutual information. arXiv preprint arXiv:1811.04251, 2018.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540529V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried- miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International Conference on Machine Learning. V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.</p>
<p>Value prediction network. J Oh, S Singh, H Lee, Advances in Neural Information Processing Systems. J. Oh, S. Singh, and H. Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118-6128, 2017.</p>
<p>A V Oord, Y Li, O Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintA. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Pipps: Flexible model-based policy search robust to the curse of chaos. P Parmas, C E Rasmussen, J Peters, K Doya, arXiv:1902.01240arXiv preprintP. Parmas, C. E. Rasmussen, J. Peters, and K. Doya. Pipps: Flexible model-based policy search robust to the curse of chaos. arXiv preprint arXiv:1902.01240, 2019.</p>
<p>A Piergiovanni, A Wu, M S Ryoo, arXiv:1805.07813Learning real-world robot policies by dreaming. arXiv preprintA. Piergiovanni, A. Wu, and M. S. Ryoo. Learning real-world robot policies by dreaming. arXiv preprint arXiv:1805.07813, 2018.</p>
<p>On variational bounds of mutual information. B Poole, S Ozair, A Oord, A A Alemi, G Tucker, arXiv:1905.06922arXiv preprintB. Poole, S. Ozair, A. v. d. Oord, A. A. Alemi, and G. Tucker. On variational bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019.</p>
<p>Stochastic backpropagation and approximate inference in deep generative models. D J Rezende, S Mohamed, D Wierstra, arXiv:1401.4082arXiv preprintD. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.</p>
<p>Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. J Schmidhuber, J. Schmidhuber. Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. 1990.</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, arXiv:1911.08265arXiv preprintJ. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Deterministic policy gradient algorithms. D Silver, G Lever, N Heess, T Degris, D Wierstra, M Riedmiller, Proceedings of the 31st International Conference on Machine Learning. the 31st International Conference on Machine LearningD. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning, 2014.</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 5507676354D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676): 354, 2017.</p>
<p>A Srinivas, A Jabri, P Abbeel, S Levine, C Finn, arXiv:1804.00645Universal planning networks. arXiv preprintA. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.</p>
<p>Dyna, an integrated architecture for learning, planning, and reacting. R S Sutton, ACM SIGART Bulletin. 24R. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160-163, 1991.</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D , . L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. arXiv preprintY. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>N Tishby, F C Pereira, W Bialek, physics/0004057The information bottleneck method. arXiv preprintN. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000.</p>
<p>Exploring model-based planning with policy networks. T Wang, J Ba, arXiv:1906.08649arXiv preprintT. Wang and J. Ba. Exploring model-based planning with policy networks. arXiv preprint arXiv:1906.08649, 2019.</p>
<p>Benchmarking model-based reinforcement learning. T Wang, X Bao, I Clavera, J Hoang, Y Wen, E Langlois, S Zhang, G Zhang, P Abbeel, J Ba, abs/1907.02057CoRRT. Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba. Benchmarking model-based reinforcement learning. CoRR, abs/1907.02057, 2019.</p>
<p>Embed to control: A locally linear latent dynamics model for control from raw images. M Watter, J Springenberg, J Boedecker, M Riedmiller, Advances in neural information processing systems. M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pages 2746-2754, 2015.</p>
<p>Imagination-augmented agents for deep reinforcement learning. T Weber, S Racanière, D P Reichert, L Buesing, A Guez, D J Rezende, A P Badia, O Vinyals, N Heess, Y Li, arXiv:1707.06203arXiv preprintT. Weber, S. Racanière, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, et al. Imagination-augmented agents for deep reinforcement learning. arXiv preprint arXiv:1707.06203, 2017.</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<p>Solar: deep structured representations for model-based reinforcement learning. M Zhang, S Vikram, L Smith, P Abbeel, M Johnson, S Levine, International Conference on Machine Learning. M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. Solar: deep structured representations for model-based reinforcement learning. In International Conference on Machine Learning, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
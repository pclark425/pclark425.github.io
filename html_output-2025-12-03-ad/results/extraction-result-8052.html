<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8052 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8052</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8052</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-4a597a081721e436e20b4e85197072e22aaecfad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4a597a081721e436e20b4e85197072e22aaecfad" target="_blank">From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function</a></p>
                <p><strong>Paper TL;DR:</strong> This work theoretically shows that it can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation, and proves that under the token level formulation, classical search-based algorithms are equivalent to likelihood-based search on a DPO policy.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement Learning From Human Feedback (RLHF) has been critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. In this work we rectify this difference. We theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using our theoretical results, we provide three concrete empirical insights. First, we show that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, we prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy. Empirically we show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, we show how the choice of reference policy causes implicit rewards to decline during training. We conclude by discussing applications of our work, including information elicitation in multi-turn dialogue, reasoning, agentic applications and end-to-end training of multi-model systems.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8052.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8052.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used as automated judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used in this paper as an automated evaluator to compute win-rates of DPO-trained model summaries versus preferred summaries on the Reddit TL;DR task; the paper reports qualitative limitations of the LLM judge (e.g., length bias) but does not provide any quantitative agreement metrics versus human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>From $r$ to $Q^{*}$ : Your Language Model is Secretly a Q-Function</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization (TL;DR Reddit summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Reddit TL;DR (Stiennon et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Referred to as GPT-4; no model size, version, or training-data details provided in the paper (used as an external automated judge to evaluate win rates).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>preference length bias; susceptibility to length-based preferences (preference for longer outputs); sensitivity to reward over-optimization/exploding verbosity when search depth increases</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The paper uses GPT-4 to evaluate win rates of candidate summaries and notes GPT-4's known preference length bias; beam search (5-beam) increases win-rate by ~10–15% according to GPT-4 evaluation, but using more beams causes exploding verbosity and lower GPT-4 win rates despite the length bias. The paper does not compare GPT-4 judgments to human judgments or report agreement statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Win-rate measured on 256 held-out Reddit TL;DR prompts; model variants (DPO with different beta values) decoded with beam search (1, 5, ... beams); outputs compared to preferred summaries and adjudicated by GPT-4; average answer length reported alongside win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Controlled decoding from language models <em>(Rating: 2)</em></li>
                <li>V-star: Training verifiers for self-taught reasoners <em>(Rating: 2)</em></li>
                <li>Rewardbench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
                <li>Making ppo even better: Value-guided monte-carlo tree search decoding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8052",
    "paper_id": "paper-4a597a081721e436e20b4e85197072e22aaecfad",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GPT-4 (judge)",
            "name_full": "Generative Pre-trained Transformer 4 (used as automated judge)",
            "brief_description": "GPT-4 is used in this paper as an automated evaluator to compute win-rates of DPO-trained model summaries versus preferred summaries on the Reddit TL;DR task; the paper reports qualitative limitations of the LLM judge (e.g., length bias) but does not provide any quantitative agreement metrics versus human raters.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "From $r$ to $Q^{*}$ : Your Language Model is Secretly a Q-Function",
            "evaluation_task": "Summarization (TL;DR Reddit summarization)",
            "dataset_name": "Reddit TL;DR (Stiennon et al., 2022)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "Referred to as GPT-4; no model size, version, or training-data details provided in the paper (used as an external automated judge to evaluate win rates).",
            "human_evaluator_type": null,
            "agreement_metric": "",
            "agreement_score": null,
            "reported_loss_aspects": "preference length bias; susceptibility to length-based preferences (preference for longer outputs); sensitivity to reward over-optimization/exploding verbosity when search depth increases",
            "qualitative_findings": "The paper uses GPT-4 to evaluate win rates of candidate summaries and notes GPT-4's known preference length bias; beam search (5-beam) increases win-rate by ~10–15% according to GPT-4 evaluation, but using more beams causes exploding verbosity and lower GPT-4 win rates despite the length bias. The paper does not compare GPT-4 judgments to human judgments or report agreement statistics.",
            "advantages_of_llm_judge": "",
            "experimental_setting": "Win-rate measured on 256 held-out Reddit TL;DR prompts; model variants (DPO with different beta values) decoded with beam search (1, 5, ... beams); outputs compared to preferred summaries and adjudicated by GPT-4; average answer length reported alongside win rate.",
            "uuid": "e8052.0",
            "source_info": {
                "paper_title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Controlled decoding from language models",
            "rating": 2
        },
        {
            "paper_title": "V-star: Training verifiers for self-taught reasoners",
            "rating": 2
        },
        {
            "paper_title": "Rewardbench: Evaluating reward models for language modeling",
            "rating": 2
        },
        {
            "paper_title": "Making ppo even better: Value-guided monte-carlo tree search decoding",
            "rating": 1
        }
    ],
    "cost": 0.0098215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>From $r$ to $Q^{*}$ : Your Language Model is Secretly a Q-Function</h1>
<p>Rafael Rafailov*<br>Stanford University<br>rafailov@stanford.edu</p>
<p>Joey Hejna*<br>Stanford University<br>jhejna@stanford.edu</p>
<p>Ryan Park<br>Stanford University<br>rypark@stanford.edu</p>
<h2>Chelsea Finn</h2>
<p>Stanford University
cbfinn@stanford.edu</p>
<h4>Abstract</h4>
<p>Reinforcement Learning From Human Feedback (RLHF) has been critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches. Standard RLHF deploys reinforcement learning in a specific token-level MDP, while DPO is derived as a bandit problem in which the whole response of the model is treated as a single arm. In this work we rectify this difference. We theoretically show that we can derive DPO in the token-level MDP as a general inverse Q-learning algorithm, which satisfies the Bellman equation. Using our theoretical results, we provide three concrete empirical insights. First, we show that because of its token level interpretation, DPO is able to perform some type of credit assignment. Next, we prove that under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy. Empirically we show that a simple beam search yields meaningful improvement over the base DPO policy. Finally, we show how the choice of reference policy causes implicit rewards to decline during training. We conclude by discussing applications of our work, including information elicitation in multi-turn dialogue, reasoning, agentic applications and end-to-end training of multi-model systems.</p>
<h2>1 Introduction</h2>
<p>Reinforcement Learning from Human Feedback (RLHF) has become the defacto method for aligning large language models (LLMs) with human intent due to its success in a wide range of applications from summarization (Stiennon et al., 2022) to instruction following (Ouyang et al., 2022). By learning a reward function from human-labeled comparisons, RLHF is able to capture complex objectives that are in-describedable in practice. Following the success of (Ziegler et al., 2020), numerous works have considered new algorithms for training and sampling from large models in various domains using techniques from reinforcement learning (RL). In particular direct alignment methods, such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) have gained traction in recent months because of their simplicity (Zhao et al., 2023a; Azar et al., 2023). Instead of learning a reward function and then using RL, direct alignment methods use the relationship between reward functions and policies in the contextual bandit setting to optimize both simultaneously. Similar ideas have since been applied to vision language (Zhao et al., 2023b) and image generation models (Lee et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>While such direct alignment methods are purported to work the same as classical RLHF approaches that use policy gradient algorithms like PPO (Schulman et al., 2017), fundamental differences remain. For instance, classical RLHF methods optimize token-level value functions with a sparse reward at the terminal state. DPO on the other hand, operates only in a contextual bandits setting, treating the entire response as a single arm. This is despite the fact that tokens are generated one at a time, and dense rewards are commonly known to be beneficial in the RL community. While direct alignment algorithms are interesting, at present it is unclear if they can be applied to sequences in the same way as the underlying RL algorithms used in typical RLHF pipelines.</p>
<p>In this work we rectify this difference by deriving DPO within the token-level MDP setting present in large language models using the usual form of binary preference-feedback. We then show that DPO training implicitly learns a token-level reward function, for which the language models logits define the optimal Q function, or expected total future reward. We then demonstrate that DPO is able to flexibly model any possible dense reward function within the token MDP.</p>
<p>Empirically, we use our theoretical derivations to justify three practical insights which we believe to be of use to the community. First, we show that despite being derived as a contextual bandit, the implicit rewards of a DPO model have a per-token interpretation. Second, we demonstrate that likelihood search over a DPO model is analogous to searching over a reward function during decoding as done by contemporary works (Liu et al., 2023b; Feng et al., 2024). Finally, we identify the choice of initial policy and reference distribution as being important in determining the trajectory of implicit rewards during training.</p>
<h1>2 Related Work</h1>
<p>The problem of aligning policies with human intent using preference feedback has been a long studied problem in reinforcement learning (Akrour et al., 2011; Wilson et al., 2012). While the primary focus of RLHF was originally in control (Christiano et al., 2017), following the success of Ziegler et al. (2020) it has recently been broadly adopted by the language modeling (Ouyang et al., 2022; Nakano et al., 2021; Stiennon et al., 2022; Bai et al., 2022a) and even vision communities (Black et al., 2023a; Lee et al., 2023). Most works in RLHF optimize a learned reward function, used only at the end of generation, with a policy graident style-method. Such approaches have been known to be unstable (Engstrom et al., 2020) and hard to scale, while at the same time theoretically existing at an unusual intersection between contextual bandits and RL. In response, several direct alignment methods (Rafailov et al., 2023; Azar et al., 2023; Zhao et al., 2023a) have been developed which simplify the RLHF pipeline by learning a policy from preference data without an intermediate reward function. Such methods however, derived solely as contextual bandits, leave several theoretical and practical questions unanswered which we seek to address.</p>
<p>First, though direct alignment methods treat the LLM as a bandit, prior works have demonstrated that it is possible to use dense rewards Zelikman et al. (2022); Chan et al. (2024); Pan et al. (2023) or even approximate dynamic programming (Snell et al., 2022). Moreover, using the regret-model of preferences (Knox et al., 2023; 2024), Contrastive Preference Learning (Hejna et al., 2024) is able to use direct alignment for general MDPs, instead of the specific token MDP used in RLHF. Our work shows how DPO can be interpreted as optimizing a per-token reward function, which in practice is restricted to the family of optimal advantage functions.</p>
<p>Second, if DPO does not learn a reward function, can we still use its reward or value? Prior works have considered using best-of-K (Mudgal et al., 2023) or tree search (Liu et al., 2023b) for alignment with a value function Kim et al. (2022); Li et al. (2017) or discriminator (Yang \&amp; Klein, 2021). Using the implicit reward, we show that likelihood search results in a similar solution for direct alignment.</p>
<p>Our work builds on foundational knowledge in maximum entropy RL (Ziebart, 2010) and inverse RL (Ziebart et al., 2008; Ng et al., 1999; Cao et al., 2021). In particular, we leverage the mapping between $Q$-functions and reward functions under a fixed policy as first done in</p>
<p>inverse RL by Garg et al. (2022). Related to our work, Nachum et al. (2017) uses similar derivations for reinforcement learning in control and Watson et al. (2023) does so for inverse RL from demonstration. Hejna \&amp; Sadigh (2024) exploit this relationship for RLHF. While related, these works still require an additional loop of reinforcement learning optimization, which we dispose of in our formulation of feedback learning for LLMs. In the LLM domain, Yu et al. (2024) uses pre-trained models as priors for Q-learning, while Cundy \&amp; Ermon (2023) considers a similar formulation for imitation learning. In this work instead, we formulate preference-based learning as Q-learning.</p>
<h1>3 Preliminaries</h1>
<p>In this section we first define the per-token MDP for large language models, and then describe how it relates to classic RLHF approaches and direct alignment algorithms, specifically DPO. We operate in the typical RLHF setting where we have a dataset $\mathcal{D}=\left{\left(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\right)\right}<em 0="0">{i=1}^{N}$ of language prompts $\mathbf{x}$ and target answers $\mathbf{y}$, which can each individually be broken down into a sequence of tokens, for example $\mathbf{x}=\left(x</em>$ and action a notation from RL literature for describing sequences at the token-level.}, \ldots, x_{m}\right)$, from a fixed discrete vocabulary $\mathcal{A}$. Throughout this section we will use the $\mathbf{x}, \mathbf{y}$ notation for the contextual bandit framing where the entire response $\mathbf{y}$ is the action, but will use state $\mathbf{s</p>
<h3>3.1 The Token-level MDP for Large Language Models</h3>
<p>We define the token level MDP as a tuple $\mathcal{M}=\left(\mathcal{S}, \mathcal{A}, f, r, \rho_{0}\right)$, where the state space $\mathcal{S}$ consists of all tokens generated so far (i.e. $\mathbf{s}<em 0="0">{t}=\left{x</em>$. In RLHF, the reward function is learned from human feedback over preferences between responses which we will denote using trajectories $\tau$ at the token level. As is typically done (Ziegler et al., 2020; Stiennon et al., 2022), we assume that preference trajectories start at the same state (initial propmpt) and end in a terminal state (EOS token), from which future rewards are zero. In this token level MDP, the corresponding Bradley-Terry preference model Bradley \&amp; Terry (1952); Christiano et al. (2017) is}, \ldots, x_{m}, y_{0}, \ldots, y_{t}\right}$ ) and the action space is the vocabulary of tokens $\mathcal{A}$. The dynamics $f$ are the deterministic transition model between tokens $f(\mathbf{s}, \mathbf{a})=\mathbf{s} \mid \mathbf{a}$, where $\mid$ is concatenation. The initial state distribution $\rho_{0}$ is a distribution over prompts $\mathbf{x}$, where an initial state $\mathbf{s}_{0}$ is comprised of the tokens from $\mathbf{x</p>
<p>$$
p^{*}\left(\tau^{w} \succeq \tau^{l}\right)=\frac{\exp \left(\sum_{i=1}^{N} r\left(\mathbf{s}<em i="i">{i}^{w}, \mathbf{a}</em>}^{w}\right)\right)}{\exp \left(\sum_{i=1}^{N} r\left(\mathbf{s<em i="i">{i}^{w}, \mathbf{a}</em>}^{w}\right)\right)+\exp \left(\sum_{i=1}^{M} r\left(\mathbf{s<em i="i">{i}^{t}, \mathbf{a}</em>
$$}^{t}\right)\right)</p>
<p>which gives the probability that the "win" trajectory $\tau^{w}$ of length $N$ is preferred to the "loss" trajectory $\tau^{l}$ of length $M$. Now that we have defined the token level MDP, we can show how it relates to both classic and direct alignment RLHF methods.</p>
<h3>3.2 The Classical RLHF Methods</h3>
<p>Most classical RLHF approaches (Ziegler et al., 2020; Bai et al., 2022b; Ouyang et al., 2022) first learn a reward function from human feedback on prompt and response pairs $\left(\mathbf{x}, \mathbf{y}^{w}, \mathbf{y}^{l}\right)$, then optimize it with a policy gradient-based method like PPO (Schulman et al., 2017) with an entropy-bonus using the following KL-constrained RL objective</p>
<p>$$
\max <em _theta="\theta">{\pi</em>}} \mathbb{E<em t="t">{a</em>} \sim \pi_{\theta}\left(\cdot \mid \mathbf{s<em t="0">{t}\right)}\left[\sum</em>}^{T}\left(r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)+\underbrace{\beta \log \pi_{\text {ref }}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>}\right)<em _theta="\theta">{\text {KL penalty }}\right)+\beta \mathcal{H}\left(\pi</em>}\right) \mid \mathbf{s<em 0="0">{0} \sim \rho\left(\mathbf{s}</em>\right)\right]
$$</p>
<p>where $\pi_{\text {ref }}$ is a reference policy, often resulting from supervised finetuning, from which the learned policy should not significantly deviate. However, in classic RLHF methods the reward function is learned as a contextual bandit with the preference model</p>
<p>$$
p^{*}\left(\mathbf{y}^{w} \succeq \mathbf{y}^{l}\right)=\frac{\exp r\left(\mathbf{x}, \mathbf{y}^{w}\right)}{\exp r\left(\mathbf{x}, \mathbf{y}^{w}\right)+\exp r\left(\mathbf{x}, \mathbf{y}^{l}\right)}
$$</p>
<p>and is thus only applied at the final timestep for the last action where $\mathbf{a}$ is EOS. In practice the actual reward used in the token-level PPO is</p>
<p>$$
r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)= \begin{cases}\beta \log \pi_{\mathrm{ref}}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>}\right), &amp; \text { if } \mathbf{s<em _mathrm_ref="\mathrm{ref">{t+1} \text { is not terminal } \ r(\mathbf{x}, \mathbf{y})+\beta \log \pi</em>}}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>
$$}\right), &amp; \text { if } \mathbf{s}_{t+1}=\mathbf{y} \text { is terminal }\end{cases</p>
<p>in a maximum entropy formulation. This leads to an interesting contradiction where the reward function $r$ is treated like a bandit, but the actual RL value function and optimization is done per-token in practice.</p>
<h1>3.3 Direct Preference Optimization</h1>
<p>Unlike classical RLHF, DPO, as derived in Rafailov et al. (2023), stays entirely within the contextual bandits setting entirely and also uses the bandit-based preference model in section 3.2. To circumvent the need for an RL algorithm, DPO uses the well-known closed form solution to the KL-contextual bandit version of the RL problem posed in eq. (2) (Ziebart et al., 2008; Levine, 2018):</p>
<p>$$
\pi^{*}(\mathbf{y} \mid \mathbf{x})=\frac{1}{Z(\mathbf{x})} \pi_{\mathrm{ref}}(\mathbf{y} \mid \mathbf{x}) e^{r(\mathbf{x}, \mathbf{y})}
$$</p>
<p>where $\pi^{<em>}$ is the optimal policy and $Z(\mathbf{x})$ is the partition function that normalizes it. DPO rearranges this equation to solve for reward as $r(\mathbf{x}, \mathbf{y})=\beta \log \pi^{</em>}(\mathbf{y} \mid \mathbf{x})-\beta \log \pi_{\text {ref }}(\mathbf{y} \mid \mathbf{x})-Z(\mathbf{x})$. Substituting this relationship into the standard binary cross-entropy loss function used for reward modeling yields the DPO loss equation as the partition function $Z(\mathbf{x})$ cancels from the Bradley Terry model.</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\mathrm{DPO}}\left(\pi</em>} ; \pi_{\mathrm{ref}}\right)=-\mathbb{E<em _theta="\theta">{\left(\mathbf{x}, \mathbf{y}^{w}, \mathbf{y}^{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi</em>\right)\right]
$$}\left(\mathbf{y}^{w} \mid \mathbf{x}\right)}{\pi_{\text {ref }}\left(\mathbf{y}^{w} \mid \mathbf{x}\right)}-\beta \log \frac{\pi_{\theta}\left(\mathbf{y}^{l} \mid \mathbf{x}\right)}{\pi_{\text {ref }}\left(\mathbf{y}^{l} \mid \mathbf{x}\right)</p>
<p>For brevity we use $\sigma$ to denote the logistic function. In the next section, we show how an alternative derivation of DPO can also cast its optimization within the token-level MDP.</p>
<h2>4 Theoretical Insights</h2>
<p>In this section we explore how DPO can theoretically be cast into the token-level MDP, and explore the consequences of doing so. First, we provide a token level derivation of DPO under the assumptions in section 3.1. Next, we show that even in the token MDP, DPO is able to fit any reward function in the multi-step Bradley Terry preference model eq. (1). Ultimately, this shows that DPO can potentially be used for more sequential optimization tasks, like multi-turn interactions or even multi-modal generation.</p>
<h3>4.1 DPO as a $Q$-function in the Token Level MDP</h3>
<p>RL in the Token-level MDP. While the original derivation of DPO relies on the fact that $Q^{<em>}(\mathbf{x}, \mathbf{y})=r(\mathbf{x}, \mathbf{y})$, this relationship does not hold in the token-level MDP. To resolve this, we need to develop new mathematical results that will allow us to relate the reward function in the Token-level Bradley Terry model eq. (1) to the corresponding optimal policy $\pi </em>$. In the general maximum entropy RL setting, the fixed point solution of eq. (2) is given by (Ziebart, 2010) as</p>
<p>$$
\pi^{<em>}\left(\mathbf{a}<em t="t">{t} \mid \mathbf{s}</em>\right)=e^{\left(Q^{</em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>
$$}\right)-V^{*}\left(\mathbf{s}_{t}\right)\right) / \beta</p>
<p>where $\pi^{<em>}(\mathbf{a} \mid \mathbf{s})$ is the optimal policy and $Q^{</em>}(\mathbf{s}, \mathbf{a})$ is the optimal Q-function which models the total future reward from $(\mathbf{s}, \mathbf{a})$ under $\pi^{<em>}$. The optimal value function $V^{</em>}$ is a function of $Q^{*}$,</p>
<p>$$
V^{<em>}\left(\mathbf{s}<em _mathbf_a="\mathbf{a">{t}\right)=\beta \log \sum</em> e^{Q^{} \in \mathcal{A}</em>}\left(\mathbf{s}_{t}, \mathbf{a}\right) / \beta}
$$</p>
<p>such that the policy $\pi^{*}$ integrates to one. Unfortunately unlike in the bandits setting this relationship gives us no specific information about the reward function $r$ at a single state</p>
<p>action pair since the optimal policy optimizes for total future returns as estimated by $Q$. To do so, we will need to consider the relationship between $Q^{*}$ and $r$.</p>
<p>From $r$ to $Q^{<em>}$. The relationship between future returns and the current timestep is captured by the belmman equaitons which are satisifed by any valid Q-function. We write this below for the optimal policy $\pi^{</em>}$ under the reward $r$ with a KL divergence penalty:</p>
<p>$$
Q^{<em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)= \begin{cases}r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)+\beta \log \pi_{\text {ref }}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>\right)+V^{</em>}\left(\mathbf{s}<em t_1="t+1">{t+1}\right), &amp; \text { if } \mathbf{s}</em>} \text { is not terminal } \ r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)+\beta \log \pi_{\text {ref }}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>
$$}\right), &amp; \text { if } \mathbf{s}_{t+1} \text { is terminal }\end{cases</p>
<p>We can then rearrange the bellman equation for the optimal $Q$-function in terms of the reward. This style of relationship was first explored by Garg et al. (2022) in imitation learning and later in Hejna \&amp; Sadigh (2024) for preference-based RL. However, these works require the use of a discount factor $\gamma&lt;1$ which is typically not used in RLHF. In the appendix we prove the following Lemma which shows that this relationship is indeed one-to-one in the token MDP as well.
Lemma 1. Under mild assumptions, there is a bijection between reward functions $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$ and corresponding optimal $Q$-functions $Q^{*}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$ in the token MDP.</p>
<p>This leads us to a rather interesting conclusion - that an LLM is always the optimal soft Q-functions for some reward function in the token MDP. Consider any LLM which outputs logits $l_{\theta}$ and temperature parameter $\beta$. As is common practice, we take the sampling policy $\pi$ to be the softmax over tokens modulated by temperature parameter $\beta$ - which is precisely eq. (5) where $Q^{<em>}=l_{\theta}$ because the value optimal function $V^{</em>}$ is precisely $\beta \log Z\left(\mathbf{s}<em 1="1">{t}\right)$, normalizing the distribution. The corresponding reward function may not be smooth or well-behaved. Notably, the logits have a free parameter due to the softmax. While this free-parameter results in the same optimal policy per later arguments, it means the sequence of values may not be smooth. The question then becomes how to finetune the LLM such that it is the optimal Q-function for a reward function $r$ that aligns with human preferences. To do so, we will complete our derivation of DPO in the token MDP.
DPO learns our best estimate of $Q^{<em>}$. Now that we have established a bijection between $r$ and $Q^{</em>}$, we can derive a token-level version of DPO to align the implicit reward, induced by the $Q$ function represented by the language model, with that of the best estimate of reward, according to Bradley-Terry model in eq. (1). To do so, we need to represent the sum of rewards first in terms of the $Q$-function $Q^{<em>}$, and then in terms of the policy $\pi^{</em>}$. We complete the first step by inverting the Bellman equation in eq. (7) and substituting it into the sum of rewards over a trajectory $\tau=\left{\mathbf{s}</em>}, \mathbf{a<em T-1="T-1">{1}, \ldots, \mathbf{a}</em>\right}$.}, \mathbf{s}_{T</p>
<p>$$
\begin{aligned}
\sum_{t=0}^{T-1} r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\left(Q^{}\right) &amp; =\sum_{t=0}^{T-1<em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)-\beta \log \pi_{\operatorname{ref}}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>\right)-V^{</em>}\left(\mathbf{s}<em t="t">{t+1}\right)\right)= \
&amp; =Q^{<em>}\left(\mathbf{s}<em 0="0">{0}, \mathbf{a}</em>}\right)-\beta \log \pi_{\operatorname{ref}}\left(\mathbf{a<em 0="0">{0} \mid \mathbf{s}</em> Q^{}\right)+\sum_{t=1}^{T-1</em>}\left(\mathbf{s}</em>}, \mathbf{a<em t="t">{t}\right)-V^{*}\left(\mathbf{s}</em>}\right)-\beta \log \pi_{\operatorname{ref}}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>\right)
\end{aligned}
$$</p>
<p>The equality follows from $V^{<em>}\left(\mathbf{s}_{T}\right)=0$ and re-arranging the sum to isolate $t=0$. As $V^{</em>}$ is written entirely in terms of $Q^{<em>}$ and $\beta$ per eq. (6), we have expressed the sum of return over the sequence just in terms of $Q^{</em>}$. Next, we exchange $Q^{<em>}$ for $\pi^{</em>}$. We can log-linearize eq. (5) as $\beta \log \pi^{<em>}\left(\mathbf{a}<em t="t">{t} \mid \mathbf{s}</em>\right)=Q^{</em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\right)-V^{<em>}\left(\mathbf{s}<em _theta="\theta">{t}\right)$. This is equivalent to stating that the language model probabilities are just the softmax over $l</em>=Q^{</em>}$ with temperature $\beta$. Continuing from the above, with this substitution we get</p>
<p>$$
=Q^{<em>}\left(\mathbf{s}<em 0="0">{0}, \mathbf{a}</em>}\right)-\beta \log \pi_{\operatorname{ref}}\left(\mathbf{a<em 0="0">{0} \mid \mathbf{s}</em> \beta \log \frac{\pi^{}\right)+\sum_{t=1}^{T-1</em>}\left(\mathbf{a}<em t="t">{t} \mid \mathbf{s}</em>}\right)}{\pi_{\operatorname{ref}}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>=V^{}\right)<em>}\left(\mathbf{s}<em t="0">{0}\right)+\sum</em> \beta \log \frac{\pi^{}^{T-1</em>}\left(\mathbf{a}<em t="t">{t} \mid \mathbf{s}</em>}\right)}{\pi_{\operatorname{ref}}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>
$$}\right)</p>
<p>where the final step results from adding and subtracting $V^{<em>}\left(\mathbf{s}_{0}\right)$ and applying the substitution again. Now, this representation for the sum of rewards in terms of the optimal policy can be directly substituted into the preference model in eq. (1), where the $V^{</em>}\left(\mathbf{s}_{0}\right)$ term will cancel just as $Z(\mathbf{x})$ did in the original DPO derivation assuming $\tau^{w}$ and $\tau^{l}$ start at the</p>
<p>same state $\mathbf{s}_{0}$, giving us the policy-induced preference model</p>
<p>$$
p_{\pi^{<em>}}\left(\tau^{w} \succeq \tau^{t}\right)=\sigma\left(\sum_{t=0}^{N-1} \beta \log \frac{\pi^{</em>}\left(\mathbf{a}<em t="t">{t}^{w} \mid \mathbf{s}</em>}^{w}\right)}{\pi_{\text {ref }}\left(\mathbf{a<em t="t">{t}^{w} \mid \mathbf{s}</em>}^{w}\right)}-\sum_{t=0}^{M-1} \beta \log \frac{\pi^{*}\left(\mathbf{a<em t="t">{t}^{t} \mid \mathbf{s}</em>}^{t}\right)}{\pi_{\text {ref }}\left(\mathbf{a<em t="t">{t}^{t} \mid \mathbf{s}</em>\right)
$$}^{t}\right)</p>
<p>To derive the final DPO loss function, we can take the KL-divergence between the empirical preference model of our dataset $p_{\mathcal{D}}$ and the preference model implied by a learned policy $p_{\pi_{\theta}}, \mathbb{D}<em _mathcal_D="\mathcal{D">{\mathrm{KL}}\left(p</em>\right)$. This results in}} | p_{\pi_{\theta}</p>
<p>$$
\mathcal{L}\left(\pi_{\theta}, \mathcal{D}\right)=-\mathbb{E}<em w="w">{\left(\tau</em> \beta \log \frac{\pi^{}, \tau_{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(\left(\sum_{t=0}^{N-1<em>}\left(\mathbf{a}<em t="t">{t}^{w} \mid \mathbf{s}</em>}^{w}\right)}{\pi_{\text {ref }}\left(\mathbf{a<em t="t">{t}^{w} \mid \mathbf{s}</em> \beta \log \frac{\pi^{}^{w}\right)}\right)-\left(\sum_{t=0}^{M-1</em>}\left(\mathbf{a}<em t="t">{t}^{t} \mid \mathbf{s}</em>}^{t}\right)}{\pi_{\text {ref }}\left(\mathbf{a<em t="t">{t}^{t} \mid \mathbf{s}</em>\right)\right)\right]
$$}^{t}\right)</p>
<p>In the next section we demonstrate that DPO can learn any dense reward function in the token-level MDP.</p>
<h1>4.2 Token-Level DPO Can Parameterize Any Dense Reward Function.</h1>
<p>In the previous section we derived DPO using the bijection between reward functions and optimal $Q$-functions uniquely available in the token-level MDP. An alternative view of DPO casts it as restricting the learned reward function such that it belongs to the class optimal advantage functions $A^{<em>}(\mathbf{s}, \mathbf{a})=Q^{</em>}(\mathbf{s}, \mathbf{a})-V^{*}(\mathbf{s})$ from which an optimal policy is readily obtained per eq. (5). Here we show that this restriction does not limit the class of reward functions we can represent. We begin by expanding the definition of equivalency used in Rafailov et al. (2023) to the broader class of potential-based reward shaping functions:
Definition 1. Two reward functions $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$ and $r^{\prime}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)$ are equivalent if there exists a potential function $\Phi(\mathbf{s})$, such that $r^{\prime}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)=r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)+\Phi\left(\mathbf{s<em t="t">{t+1}\right)-\Phi\left(\mathbf{s}</em>\right)$.</p>
<p>In Ng et al. (1999)'s seminal work, the authors proved that two equivalent reward functions defined per definition 1 have the same optimal policy. By log-linearizing the optimal policy fixed point in eq. (5) and substituting in the Bellman equation from eq. (7) (Nachum et al., 2017; Watson et al., 2023), we have</p>
<p>$$
\beta \log \frac{\pi^{<em>}\left(\mathbf{a}<em t="t">{t} \mid \mathbf{s}</em>}\right)}{\pi_{\text {ref }}\left(\mathbf{a<em t="t">{t} \mid \mathbf{s}</em>}\right)}=r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)+V^{</em>}\left(\mathbf{s}<em t="t">{t+1}\right)-V^{*}\left(\mathbf{s}</em>\right)
$$</p>
<p>This is precisely the optimal advantage function, where $V^{*}$ directly follows the form of a potential shaping function. Watson et al. (2023) first used this derivation to arrive at a "coherent" reward function and follow-ups arrived at the same conclusion by noting that using the advantage as reward preserves the optimal policy (Knox et al., 2024; Hejna et al., 2024). Unlike prior works, however, we demonstrate that this re-parameterization also leads to the same exact preference distribution as $r$.
Theorem 1. Given a reference policy $\pi_{\text {ref }}$ and a parameter $\beta&gt;0$ all reward classes consistent with the Plackett-Luce (and Bradley-Terry) models in eq. (1) can be represented with the a re-parameterization of the form</p>
<p>$$
r(\mathbf{s}, \mathbf{a})=\beta \log \pi(\mathbf{a} \mid \mathbf{s})-\beta \log \pi_{\text {ref }}(\mathbf{a} \mid \mathbf{s})
$$</p>
<p>within the token MDP where $V^{*}\left(\mathbf{s}_{t}\right)=0$ for all terminal states.
Proof. Above we derived the invariance of the optimal policy under the re-parameterization. The preference model can be shown to be invariant by substituting and following the same steps used to arrive at eq. (8) in the last section, or by following Definition 1 from Watson et al. (2023).</p>
<p>Interestingly, in practice, the potential function $\Phi\left(\mathbf{s}_{t}\right)$ represents the free parameter in the logits of the language model. An equal shift along all logits yields the same policy, but different Q-functions and corresponding rewards. The above Theorem proves that all of these are in the same equivalence class and induce the same set of preferences.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Credit assignment in DPO based on answer-level feedback. We provide two summaries to a Reddit post about a job interview. The left is the base response and on the right we have introduced errors in the salary range and the position level. Each token is colored corresponding to the DPO implicit reward as expressed in Eq. 11 (darker is higher), using the trained model. We see that the model correctly highlights the erroneous statements, without much change to the value of the other tokens, which indicates the ability to do credit assignment.</p>
<p>Moreover, this Theorem implies that we can use DPO to learn the optimal policy for any per-token reward function, provided preference queries start at the same state and end at a terminal state. In addition, DPO always fits an optimal advantage function for some reward which is responsible for credit assignment. Thus, the training data determines how close the learned advantage corresponds to that of the true reward. This is in contrast to methods that estimate the reward function and then additionally employ some policy improvement mechanism. Which algorithm performs better remains largely an open or empirical question.</p>
<p>The above derivations cast a language model as a Q function in the discrete token-level MDP. While this interpretation does not generally hold in continuous spaces, we can extend many of our results to other specially structured MDPs, like those present in diffusion. See Appendix B for more thorough treatment.</p>
<h1>5 Practical Insights</h1>
<p>In this section we discuss the empirical implications of our theoretical analysis. First, we qualitatively show that DPO can learn per-token credit assignment. Next, we use the derivations of the prior section to connect guided decoding and search-based algorithms, such as MCTS, to likelihood-based search on the DPO policy and empirically validate these results. Finally, (for the first time), we mathematically explain the phenomenon of decreasing likelihoods during DPO training, observed in the research and industry community.</p>
<p>For all empirical evaluations we use the Pythia 2.8B model <em>Biderman et al. (2023)</em> and the Reddit TL;DR summarization dataset <em>Stiennon et al. (2022)</em>. We use the default hyper-parameters from the original public DPO implementation, unless otherwise stated.</p>
<h3>5.1 Does DPO Learn Credit Assignment?</h3>
<p>In the previous section we outlined how the trained DPO policy represents an optimal Qfunction for some reward that optimizes the preference equation. In this section, we evaluate qualitatively if the DPO-trained model is able to learn credit assignment from trajectory feedback. We begin with a generic set of Reddit posts for the TL;DR test dataset, which we provide in Appendix C with additional examples. In our representative example the user discusses an employment negotiations situation. Two answers are shown in Figure 1. The base summary, which is correct is provided on the left. On the right we modify the summary by introducing a higher-level position and a corresponding higher salary. For each token in both answers we compute the DPO reward (equivalently the advantage function or "coherent" reward <em>(Watson et al., 2023)</em>), $r(\mathbf{s}, \mathbf{a})=\beta \log \pi_{\theta}(\mathbf{s} \mid \mathbf{a})-\beta \log \pi_{\text {ref }}(\mathbf{s} \mid \mathbf{a})$, where $\pi_{\theta}$ as outlined in Theorem 1 (here $\pi_{\theta}$ is our DPO-trained model and $\pi_{\text {ref }}$ is the SFT model). In Figure 1 each token is colored proportionally to this reward. We see that the model successfully identifies the tokens corresponding to the erroneous statements, while still maintaining comparable values for the rest, which is indicates that it can do credit assignment. Moreover, we see that within the context of the first error ( ${ }^{<em>} 250 \mathrm{~K}^{</em>}$ salary) the model still allocates reasonable values to the rest of the tokens and specifically identifies the second error <em>management position</em>. This is a promising sign of the ability to do "stitching" <em>Levine et al. (2020)</em> i.e. a form of combinatorial generalization from offline data. If this is the case, our findings could be significant for the use of reinforcement learning and RLHF in LLMs, particularly</p>
<p>for compositional tasks, such as code and reasoning. At the same time, in the recently introduced RewardBench [lambert2024rewardbench], DPO models have demonstrated strong performance as classifiers on reasoning tasks. We believe these are encouraging results, which warrant further large-scale study beyond our qualitative observations.</p>
<h1>5.2 Connecting Guided Decoding and Search to Likelihood-Based DPO Optimization</h1>
<p>Recently Large Language Models have been combined with search algorithms during the inference stage [mudgal2024gpt; feng2024gpt; huang2024gpt; liu2023gpt], which have found to improve the quality of responses over standard next token decoding. Following the standard literature, these methods rely on a (usually sparse) reward signal or model $r_{\theta}\left(\mathbf{s}<em t="t">{\mathbf{t}}, \mathbf{a}</em>\right)$. During inference time they deploy a graph-search algorithm in the token MDP as outlined in Section 3.1 to maximize the sum of rewards. Let us consider the search problem outlined in Eq. 2 with a partial expansion of length $K$ :}\right)$ which they use to train a separate value function $V_{\theta}\left(\mathbf{s}_{t</p>
<p>$$
\max <em 0="0">{\mathbf{a}</em>}, \ldots, \mathbf{a<em 0="0">{K}} r\left(\mathbf{s}</em>}, \mathbf{a<em _mathrm_ref="\mathrm{ref">{0}\right)+\beta \log \pi</em>}}\left(\mathbf{s<em 0="0">{0}, \mathbf{a}</em>}\right)+\ldots+r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)+\beta \log \pi_{\mathrm{ref}}\left(\mathbf{s<em K="K">{K}, \mathbf{a}</em>\right)
$$}\right)+V^{*}\left(\mathbf{s}_{K+1</p>
<p>where $V^{*}$ is the optimal corresponding value function. Now, if we directly substitute the reward representation from Eq. 10 into the above and considering a telescoping sum, with some standard algebra, we obtain that the above objective is equivalent to</p>
<p>$$
\max <em 0="0">{\mathbf{a}</em>}, \ldots, \mathbf{a<em 0="0">{K}}-V^{<em>}\left(\mathbf{s}_{0}\right)+\beta \log \pi^{</em>}\left(\mathbf{a}</em>} \mid \mathbf{s<em K="K">{0}\right)+\ldots+\beta \log \pi^{*}\left(\mathbf{a}</em>\right)
$$} \mid \mathbf{s}_{K</p>
<p>where $\pi^{*}$ is the corresponding optimal policy. Now, since the starting state is fixed (it's given by the prompt) we have that a search algorithm based on the conservative reward function of the RLHF objective and the corresponding optimal value policy is equivalent to likelihood search on the corresponding optimal policy. We empirically verify this property in Fig. 2, which shows the win rate of DPO models trained with three different $\beta$ values against the preferred summary in the test dataset. We see that a 5 -beam search improves win-rates by $10-15 \%$ over the base policy (1-beam), which is comparable to the value-function guided search improvements reported in [mudgal2024gpt]. Interestingly, we see performance degrade with higher number of beams. Increasing the number of beams also produces answer with exploding length, which is a sign of reward over-optimization [gao2023reward; park2024gpt; rafailov2024gpt] and would explain the degradation in performance. These observations are consistent with out formulation of beam search as a search over a learned reward function.</p>
<p>These findings are consistent with the result of the recently proposed V-STaR algorithm [hosseini2024gpt], which combines the approach of STaR [zelikman2022gpt] with a DPO trained verifer. At inference time, the STaR model produces several candidate reasoning chains (plans) which are ranked by the DPO verifier likelihood. This can be seen as a form of likelihood based search as in Eq. 12, however instead of directly searching on the DPO model, it uses the STaR model as a proposal distribution. We hypothesize this is beneficial in preventing reward hacking, which is potentially an issue with deeper search as shown in Fig. 2.</p>
<h3>5.3 Connections Between Proxy Tuning and Reinforcement Learning</h3>
<p>Several recent works [mitchell2023gpt; liu2023gpt; b] have proposed an approach of inference-time model alignment through a proxy guidance model. These approaches start with a (unaligned) base model $\pi_{\text {base }}$ and a proxy model $\pi_{\text {proxy }}$ and a target distribution reference model $\pi_{\text {ref }}$. The inference time re-alignment of the base model is carried by re-weighting the conditional probabilities of each token:</p>
<p>$$
\pi\left(\mathbf{a} \mid \mathbf{s}<em _base="{base" _text="\text">{t}\right) \propto \pi</em>}}\left(\mathbf{a} \mid \mathbf{s<em _proxy="{proxy" _text="\text">{t}\right)\left(\frac{\pi</em>}}\left(\mathbf{a} \mid \mathbf{s<em _ref="{ref" _text="\text">{t}\right)}{\pi</em>
$$}}\left(\mathbf{a} \mid \mathbf{s}_{t}\right)}\right)^{\beta</p>
<p>Under our considerations from the prior chapter, then this becomes equivalent to</p>
<p>$$
\pi\left(\mathbf{a} \mid \mathbf{s}<em _base="{base" _text="\text">{t}\right) \propto \pi</em>}}\left(\mathbf{a} \mid \mathbf{s<em t="t">{t}\right) \exp \left(\beta\left(Q^{<em>}\left(\mathbf{s}_{t}, \mathbf{a}\right)-V^{</em>}\left(\mathbf{s}</em>\right)\right)\right)
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model performance using beam search. Left: Win rate of the model generated summaries over the preferred summary on 256 held-out test prompts from the Reddit TL;DR dataset, as evaluated by GPT 4. Right: The average answer length based on number of beams. We see exploding verbosity with more than 5 beams, which also leads to lower model win rates, despite GPT4's well-know preference length bias.</p>
<p>where $\beta\left(Q^{<em>}\left(\mathbf{s}_{t}, \mathbf{a}\right)-V^{</em>}\left(\mathbf{s}_{t}\right)\right.$ is the optimal implicit advantage from the proxy tuning model. That is our theoretical results allows us to tie the realignment approaches of <em>Mitchell et al. (2023); Liu et al. (2024a, b)</em> to recent works which explicitly train critic models <em>Mudgal et al. (2024)</em> for token-level decoding.</p>
<h3>5.4 Likelihoods should decrease when using DPO.</h3>
<p>A surface level interpretation of DPO would lead one to believe it increases the likelihood of chosen responses, while decreasing the likelihood of rejected responses. This however, does not account for a well observed phenomena in which the likelihood of the chosen responses actually <em>decrease</em> over time <em>(Pal et al., 2024)</em>. This is illustrated on the left half of fig. 3, which we show that when performing SFT before DPO, the implicit rewards of both the chosen and rejected response decline, though the margin between them increases. However, given a MaxEnt RL framing, this phenomena may be expected.</p>
<p>Consider the expected log ratio (or implicit reward) of a policy under the reference model, which is often measured during training. Algebraic manipulation yields the following relationship:</p>
<p>$$
\mathbb{E}<em _ref="{ref" _text="\text">{\mathbf{a} \sim \pi</em>}}(\cdot \mid \mathbf{s})}\left[\beta \log \frac{\pi(\mathbf{a} \mid \mathbf{s})}{\pi_{\text {ref }}(\mathbf{a} \mid \mathbf{s})}\right]=-\beta \mathbb{D<em _ref="{ref" _text="\text">{\text {KL }}\left(\pi</em>)\right)
$$}}(\cdot \mid \mathbf{s}) | \pi(\cdot \mid \mathbf{s</p>
<p>At the beginning of training when $\pi=\pi_{\text {ref }}$, the implicit rewards are trivially zero. However at the end of training, assuming $\pi_{\text {ref }} \neq \pi^{<em>}$, the KL-divergence is necessarily positive, indicating that the implicit rewards must decrease in expectation to converge. This means that the average implicit rewards </em>should<em> go down when starting from the SFT model. In fact, on the left side of fig. 3 we show that when one </em>does not<em> SFT before DPO, there is little discernible trend in the average implicit reward and the implicit rewards of the chosen responses remain above zero. In fact, this trend also holds for CPL </em>Hejna et al. (2024)* for the general MDP, where the implicit rewards actually increase if SFT is not used.</p>
<p>One might realize that the previous analysis does not necessitate that the implicit rewards of the chosen must decrease, just that the implicit rewards must decrease on average. However, in practice it is common place (and recommended by <em>Rafailov et al. (2023)</em>) to SFT on only the chosen responses to form $\pi_{\text {ref }}$. For this section only we will call this choice of reference $\pi_{\text {ref }}^{w}$. Substituting $\pi_{\text {ref }}^{w}$ into eq. (16), we can see that when SFTing on the positive answers the implicit rewards of the chosen responses <em>must</em> go down because at convergence as $\mathbb{E}<em _ref="{ref" _text="\text">{\pi</em>[\beta \log \pi^{}}^{w}<em>}-\beta \log \pi_{\text {ref }}^{w}]=-\beta \mathbb{D}<em _ref="{ref" _text="\text">{\text {KL }}\left(\pi</em> | \pi^{}}^{w</em>}\right)$.</p>
<p>Based on this derivation and choice of $\pi_{\text {ref }}^{w}$, the likelihood of the chosen response should decrease in the process of DPO training.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The evolution of implicit rewards for DPO on TLDR (left) and CPL on the bin-picking dataset (right) during training. We see that when we start with SFT, reward values decrease, whereas starting without SFT causes implicit rewards to be positive for DPO and increase for CPL.</p>
<p>While choosing $\pi_{\text {ref }}=\pi_{\text {ref }}^{w}$ is done in practice (Rafailov et al., 2023), it does mean that DPO will decrease the likelihood of all data in favor of extrapolated responses, which could cause over-fitting. Moreover, now that we have provided a derivation of DPO in the token-level MDP, one might expect it to exhibit characteristics like an RL algorithm - namely that the implied $Q$-function monotonically increases over time. However, this is not necesarily the case. Note that per analysis in Section section 3.1, DPO can be viewed as adjusting the reward (or advantage) from which the optimal policy is deterministically mapped within the token-level MDP. DPO does not train a policy to maximize reward, and thus we do not argue about whether its implied value functions should increase or decrease over time.</p>
<h1>6 Discussion</h1>
<p>In this work we formulated the DPO optimization algorithm as learning an optimal Qfunction, which is represented by an LLM. This formulation and our results provide theoretical justification for empirically observed DPO training phenomena, which are not explained by the original bandit formulation. We further link and unify a family of proposed new LLM search algorithms by likelihood search under DPO and show comparable empirical gains by a simple 1-line code change to using beam search. Most importantly, we show qualitative early signs that DPO is able to learn credit assignment directly from feedback data. While larger-scale empirical exploration is necessary, we believe this an encouraging early sign. Our results indicate a number of promising future directions to explore:
Learning intermediate reasoning from outcome feedback: Recent works have shown promising results on that front Pang et al. (2024); Hwang et al. (2024).
Multi-turn conversations: Teaching language models to be an interactive conversationalists has been difficult, as RLHF is optimized as a single-turn bandit formulation. Moreover, classical methods, such as PPO, are not applicable in this setting. Recent work by Andukuri et al. (2024) has shown success in this domain using STaR and extending DPO to multi-turn conversational trees is a promising direction.</p>
<p>Agentic LLMs: LLM agents, such as WebGPT (Nakano et al., 2022) are equipped to take autonomous actions, such as browsing the Web and collecting information before providing an answer. The user then provides feedback based on the final output. Our derivations indicate that DPO training (on the full model trajectories) could learn optimal exploration behaviour. Recent works Song et al. (2024); Xi et al. (2024) shows promise in that direction.</p>
<p>End-to-end training of generative AI systems: Modern image generation systems, such as Dalle 3 Betker et al. (2023) use an LLM to produce high quality conditioning before calling a diffusion generation model. Also, recent long-form video generation models Hu et al. (2023); Gupta et al. (2023) combine transformer-based auto-regressive generations with a diffusion-based decoder. Such systems could potentially be optimized end-to-end with ahybrid version of DPO. We expand on these points in the Appendix.</p>
<p>We believe these are promising directions for future work.</p>
<h1>Acknowledgements</h1>
<p>Chelsea Finn is a CIFAR Fellow in the Learning in Machines and Brains program. JH is supported by an NDSEG Fellowship. This work was also supported by ONR grant N00014-22-1-2621 and the Volkswagen Group.</p>
<h2>References</h2>
<p>Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2011 .</p>
<p>Chinmaya Andukuri, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah D. Goodman. Star-gate: Teaching language models to ask clarifying questions, 2024.</p>
<p>Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022b.</p>
<p>James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions, 2023. URL https://cdn.openai.com/papers/dall-e-3.pdf.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.</p>
<p>Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023a.</p>
<p>Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023b.</p>
<p>Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324-345, 1952. doi: https: //doi.org/10.2307/2334029.</p>
<p>Haoyang Cao, Samuel Cohen, and Lukasz Szpruch. Identifiability in inverse reinforcement learning. Advances in Neural Information Processing Systems, 34:12362-12373, 2021.</p>
<p>Alex J Chan, Hao Sun, Samuel Holt, and Mihaela van der Schaar. Dense reward for free in reinforcement learning from human feedback. arXiv preprint arXiv:2402.00782, 2024.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.</p>
<p>Chris Cundy and Stefano Ermon. Sequencematch: Imitation learning for autoregressive sequence modelling with backtracking. arXiv preprint arXiv:2306.05426, 2023.</p>
<p>Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case study on ppo and trpo. arXiv preprint arXiv:2005.12729, 2020.</p>
<p>Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024.</p>
<p>Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023.</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024.</p>
<p>Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. International Conference on machine Learning, 2023.</p>
<p>Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, Matthieu Geist, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation, 2022.</p>
<p>Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models, 2023.</p>
<p>Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward function. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. Contrastive preference learning: Learning from human feedback without reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=iX1RjVQODj.</p>
<p>Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners, 2024.</p>
<p>Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving, 2023.</p>
<p>James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchhoff, and Dan Roth. Deal: Decoding-time alignment for large language models, 2024.</p>
<p>Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. Selfexplore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards, 2024. URL https://arxiv.org/abs/2404.10346.</p>
<p>Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, and Kyomin Jung. Critic-guided decoding for controlled text generation. arXiv preprint arXiv:2212.10938, 2022 .</p>
<p>W Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter Stone, and Alessandro G Allievi. Models of human preference for learning reward functions. Transactions on Machine Learning Research, 2023.</p>
<p>W Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca Dragan, Peter Stone, and Scott Niekum. Learning optimal advantage from preferences and mistaking it for reward. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 10066-10073, 2024.</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024.</p>
<p>Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv e-prints, pp. arXiv-2302, 2023.</p>
<p>Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review, 2018.</p>
<p>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020.</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. Learning to decode for future success. arXiv preprint arXiv:1701.06549, 2017.</p>
<p>Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning language models by proxy, 2024a. URL https://arxiv.org/abs/2401.08565.</p>
<p>Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Don't throw away your value model! making ppo even better via value-guided monte-carlo tree search decoding, 2023a.</p>
<p>Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Making ppo even better: Value-guided monte-carlo tree search decoding. arXiv preprint arXiv:2309.15028, 2023b.</p>
<p>Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, and Mathieu Blondel. Decodingtime realignment of language models, 2024b. URL https://arxiv.org/abs/2402.02992.</p>
<p>Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D. Manning. An emulator for fine-tuning large language models using small language models, 2023.</p>
<p>Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023.</p>
<p>Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, and Ahmad Beirami. Controlled decoding from language models, 2024.</p>
<p>Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning, 2017.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browserassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022 .</p>
<p>Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pp. 278-287, 1999 .</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 27730-27744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.</p>
<p>Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024.</p>
<p>Sarah Pan, Vladislav Lialin, Sherin Muckatira, and Anna Rumshisky. Let's reinforce step by step. arXiv preprint arXiv:2311.05821, 2023.</p>
<p>Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024. URL https: //arxiv.org/abs/2404.19733.</p>
<p>Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization, 2024.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://arxiv.org/abs/2305.18290.</p>
<p>Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, and Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms, 2024. URL https://arxiv.org/abs/2406.02900.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.</p>
<p>Charlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents, 2024.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022 .</p>
<p>Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization, 2023.</p>
<p>Joe Watson, Sandy Huang, and Nicolas Heess. Coherent soft imitation learning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https:// openreview.net/forum?id=kCCD8d2aEu.</p>
<p>Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach for policy learning from trajectory preference queries. In Advances in Neural Information Processing Systems, 2012 .</p>
<p>Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Agentgym: Evolving large language model-based agents across diverse environments, 2024. URL https://arxiv.org/abs/2406.04151.</p>
<p>Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218, 2021.</p>
<p>Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, and Hongxia Yang. B-coder: Value-based deep reinforcement learning for program synthesis, 2024.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022 .</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023a.</p>
<p>Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023b.</p>
<p>Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010.</p>
<p>Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.</p>
<p>Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020 .</p>
<h1>A Proof of Lemma 1</h1>
<p>Lemma 1. For a fixed policy $\pi$, there is a bijection between reward functions $r$ and corresponding optimal $Q$-functions $\left(Q^{*}\right)$ in the deterministic tree-structured LLM MDP.</p>
<p>Proof. Let $Q_{r}^{*}$ denote the optimal $Q$-function for reward $r$. We prove the statement directly, starting with the injective case.</p>
<p>Assume there exists a reward function $r^{\prime} \neq r$ such that $Q_{r^{\prime}}^{<em>}=Q_{r}^{</em>}$. Then, there must exist a state action pair such that $r^{\prime}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right) \neq r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)$. In fact, proceeding backwards from a leaf node (terminal state), there must be a first state action pair $\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)$ where $r^{\prime}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right) \neq r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$. The $Q$ functions at this location are</p>
<p>$$
Q_{r^{\prime}}^{<em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)=r^{\prime}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>^{}\right)+V_{r^{\prime}</em>}\left(\mathbf{s}<em r="r">{t+1}\right), \quad Q</em>^{<em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)=r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>^{}\right)+V_{r</em>}\left(\mathbf{s}_{t+1}\right)
$$</p>
<p>By the fact that this was the first location where the reward functions differed starting from a leaf node, we must have that $V_{r^{<em>}}^{</em>}\left(\mathbf{s}<em r="r">{t+1}\right)=V</em>^{<em>}\left(\mathbf{s}<em r_prime="r^{\prime">{t+1}\right)$. This is because we can recursively solve for the optimal policy, value, and Q-function using eq. (5) eq. (7), and eq. (6) from Ziebart et al. (2008). The rewards in all possible future states from $s, a$ are equal by virtue of this being the location of the first difference and thus the dynamic programming solution up to this point is the same. Thus, we can see that $Q</em>^{}</em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right) \neq Q_{r}^{*}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$, completing this direction. Note that this proof does not hold in general MDPs, only the token MDP where it is impossible to return to the same state after taking any number of actions.</p>
<p>The surjective direction is easier. For all $Q^{<em>}$, we can compute a reward function $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\right)=$ $Q^{</em>}\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\right)$ under deterministic dynamics. Thus, we can see that the mapping is surjective.}\right)-V^{*}\left(\mathbf{s}_{t+1</p>
<h1>B Treatment of Diffusion Models</h1>
<p>Conditional diffusion image generation models, such as Stable Diffusion 3 Esser et al. (2024) have also used a form of the DPO algorithm as outlined in Wallace et al. (2023). Our analysis can no longer be directly applied in that setting, since the generations are continuous. However, we could translate many of our results to that setting, if we consider a certain diffusion MDP. We outline our results bellow.</p>
<h2>B. 1 Diffusion MDP</h2>
<p>We borrow the denoising MDP formulation from Black et al. (2023b); Fan et al. (2023). We again have the tuple $(\mathcal{S}, \mathcal{A}, f, r, \rho_{0})$, with the same formulation as in Section 3.1. At the same time consider a diffusion process with time index $t$ and $T$ total steps, conditioned on context $\mathbf{c}$ and image denoted by $\mathbf{x}_{t}$. Then we can map the diffusion generation process to an MDP in the following way</p>
<p>$$
\mathbf{s}<em T-t="T-t">{t}= \begin{cases}(\boldsymbol{c}, T) &amp; \text { if } t=0 \ \left(\boldsymbol{c}, \boldsymbol{x}</em>
$$}, T-t\right) &amp; \text { otherwise }\end{cases</p>
<p>That is the initial state consists of the prompt $\mathbf{c}$ and afterwards each state consists of the current denoised image $\mathbf{x}_{t}$ and time step $T-t$. Notice that the time-steps in the MDP are inverse to the direction of the diffusion process (i.e. we start at noise and end at the final image). The action is just the next image iteration, from where the dynamics is also straightforward:</p>
<p>$$
\begin{gathered}
\mathbf{a}<em T-t_1="T-t+1">{t} \triangleq \boldsymbol{x}</em> \
f\left(\mathbf{s}<em T-t="T-t">{t}=\left(\boldsymbol{c}, \boldsymbol{x}</em>}, T-t\right), \mathbf{a<em _mathbf_t="\mathbf{t">{t}\right)=\left(\mathbf{c}, \mathbf{a}</em>, T-t-1\right)
\end{gathered}
$$}</p>
<p>Notice that in this case the policy is stochastic, but the dynamics of the MDP is still deterministic. Finally, the initial distribution, is just the distribution of prompts:</p>
<p>$$
\rho\left(\mathbf{s}_{0}\right) \triangleq(p(\boldsymbol{c}), 0)
$$</p>
<h2>B. 2 Theoretical Results for the Diffusion MDP</h2>
<p>Given the above formulation, we can also prove that Lemma 1 also holds in the diffusion MDP.
Lemma 2. Under mild assumptions, there is a bijection between reward functions $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$ and corresponding optimal $Q$-functions $Q^{*}\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$ in the diffusion MDP.</p>
<p>Proof. Since the MDP still has deterministic dynamics, we have that Eq. 5-7 still hold. Now, given a reference policy $\pi_{\text {ref }}$, parameter $\beta$ and a critic $Q$, we can trivially recover the unique reward function by inverting Eq. 7. We will prove that given a reward function $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\right)=0$ for all terminal states. We then have that}\right)$, we can recover a unique critic $Q$. We work inductively in the diffusion MDP starting with $t=T$, where we have $V^{*}\left(\mathbf{s}_{T</p>
<p>$$
\begin{aligned}
&amp; Q^{<em>}\left(\mathbf{s}<em t-1="t-1">{t-1}, \mathbf{a}</em>\right)=Q^{</em>}\left(\mathbf{s}<em T-t_1="T-t+1">{t}=\left(\boldsymbol{c}, \boldsymbol{x}</em>}, T-t+1\right), \mathbf{a}=\boldsymbol{x<em t="t">{T-t}\right)= \
&amp; r\left(\mathbf{s}</em>}=\left(\boldsymbol{c}, \boldsymbol{x<em T-t="T-t">{T-t+1}, T-t+1\right), \mathbf{a}=\boldsymbol{x}</em>}\right)+\beta \log p_{\text {ref }}\left(\boldsymbol{x<em T-t_1="T-t+1">{T-t} \mid \boldsymbol{c}, \boldsymbol{x}</em>, T-t+1\right)+ \
&amp; \beta \log \int_{\mathcal{A}} e^{Q^{*}\left(\mathbf{s}<em T-t="T-t">{t}=\left(\boldsymbol{c}, \boldsymbol{x}</em>}, T-t\right), \boldsymbol{x<em T-t-1="T-t-1">{T-t-1}\right) / \beta} d \boldsymbol{x}</em>
\end{aligned}
$$</p>
<p>where $\pi_{\text {ref }}$ is the reference backwards diffusion process. In this case even though the state space is deterministic, our approach to the proof of Lemma 1 still holds by using backwards induction on the diffusion step $t$. Notice, that from $V\left(\mathbf{s}<em 0="0">{T}=\left(\boldsymbol{c}, \boldsymbol{x}</em>, 0\right)\right)=0$ we can uniquely determine the critic values for all states at time step $T-1$. Proceeding inductively backwards through time in the MDP/denoising process (forward in the diffusion process), we obtain the desired result.</p>
<p>Given the proof of this Lemma, we can then directly apply the results of Section 4.2, including Theorem 1. Our results, also give us insights into the formulation of Wallace et al. (2023). In particular, by changing the sampling scheme of the intermediate diffusion the authors obtain two alternative formulations (Appendix S2 in Wallace et al. (2023)). Both of these schemes are suggested as empirical approximations in the formulation of the Diffusion-DPO algorithm, however in the view of Q-learning both of these are valid approaches to generating off-policy data. In fact, our interpretation of DPO allows for general off-policy data sampling and aggregation methods, which could yield a whole family of DPO algorithms in this domain. We leave the exploration of this direction for further work.</p>
<h1>C Reddit TL;DR Posts</h1>
<h2>SUBREDDIT: r/running</h2>
<p>TITLE: Tips on getting back into running after 4 years of not doing so \&amp; shin splints
POST: Hey everyone, I was hoping to gather some tips from people who left running and had to start over. A semi-lengthy background on myself to help you understand where I am coming from. In high school I was a very good cross country runner, running from 35-50 miles a week and never slower than 8-9 minute miles. At the end of senior year, I planned on taking a break from running and then try to race half or full marathons in the spring. I ended up not running at all after xc. 4 years later, I was noticing how much I miss the sport (especially after seeing the success of xc friends) so I decided to join a running group to get back into it. But the only group at my university that I could find was a triathlon club. I joined them, but only did the running workouts. After about 4 weeks, I developed shin splints. This is because I haven't ran in 4 years but thought 6 miles was ok after 4 weeks. Also, being 25 pounds heavier didnt help. After taking 3 months off and training on the bike and in the pool, I finally was back to running in february. but my shinsplints was still around. I finished my first sprint triathlon last week, and have been trying to get miles back under my feet again. I havent felt shin splints severely since the beginning of March, but I can feel it looming around. After a half year of it, I am getting really really frustrated. I cant run more than 4 miles still and my fastest mile is 8 minutes. I know I will probably never run like I did when I was 17, but its difficult because of remembering what I used to be capable of running.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Was a very good runner in high school, stopped running for 4 years, started again recently but still have sh in spl ints, frustrated trying to get back to where I used to be .</th>
<th style="text-align: center;">Was a very good runner in high school, stopped running for 4 years, started again recently but still have b usted knee, frustrated trying to get back to where I used to be .</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>DPO successfully identifies the wrong tokens ("busted knee") in the right summary and correctly classifies this pair.</p>
<p>SUBREDDIT: r/AskReddit
TITLE: What is a sub-\$800 camera that can shoot high quality video ideal for music video-like appearances?</p>
<p>POST: [This is a video of what we're trying to achieve.](
My school currently has a Sony HVR-HD1000u, and compared to that, our videos are nowhere near as good. I understand that things like lighting and color correction play a pretty big role, but even then I feel like our videos are never that clean. I usually can't get 720 p clips out of our camera and the slow motion that they have is something we can't even come close to.</p>
<p>One possible <em>problem</em> is that for some reason we can't use firewire to connect the camera to the computer so we have to play the tape on this thing that basically plays it and then we capture the tape playing. I feel like this is probably a huge problem because it's like trying to show a friend a movie by screen-capping from Skype.
SO, should we scrap the HVR-HD1000u and get a Canon T2i (a cheaper DSLR which from the samples I've seen on YouTube and clips from that video, seems pretty high quality), or continue trying to use the Sony?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">T2i get music video quality, would</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">it be better to use our Sony H VR</td>
</tr>
<tr>
<td style="text-align: left;">HD 1000 u or buy a Canon T 2 i ?</td>
</tr>
<tr>
<td style="text-align: left;">Or something else entirely (&lt; \$ 800 )</td>
</tr>
<tr>
<td style="text-align: left;">i?</td>
</tr>
</tbody>
</table>
<p>T2i get music video quality, would
It be better to use our Sony H VR
+ HD 1000 u or buy an iPhone ? Or
something else entirely (&lt; \$ 800 )?</p>
<p>DPO successfully identifies the wrong tokens ("iPhone") in the right summary and correctly classifies this pair.</p>
<p>SUBREDDIT: r/personalfinance
TITLE: When asked about salary expectations during my interview I said 38 k to 45 k . Was just offered the position with 38 k . Should I try and negotiate?
POST: So I interviewed for a position last week, and before the interview I saw online that the industry average for this position was $\$ 41,000$. During the interview, they asked me my salary expectations, I said between $\$ 38,000$ and $\$ 45,000$ hoping it'd land somewhere in the middle. I received my offer today, and it was for $\$ 38,000$. I can't help but wonder if I had just said $\$ 41,000$ they probably would've offered it...
Anyways, so what I know is they are hiring 3 other people for this same position... I either got lucky and guessed exactly what salary they were planning on paying all of us to begin with, or we're all getting paid differently. As for the job, it is the ideal entry level position for me right now, and is a great company with benefits etc so I actually wouldn't mind working there for the 38 k salary.
But it would be nice to get an even 40 at least, so my question is, is it common practice to negotiate salary after receiving an offer already? I also must say that I don't have any leverage as this is entry level and I would have probably still accepted had the offer been even as low as 30 k . As such, I'm very afraid the offer may be retracted if I do try and negotiate, if that sort of thing happens?
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>DPO successfully identifies the wrong tokens (" $250 \mathrm{k}^{\text {" }}$ and "management position") in the right summary and correctly classifies this pair.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Denotes equal contribution</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
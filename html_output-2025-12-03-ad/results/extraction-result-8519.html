<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8519 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8519</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8519</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-adb9acaf9184bdbd23105f1a383848eed9bc82fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/adb9acaf9184bdbd23105f1a383848eed9bc82fc" target="_blank">Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Experiments have shown that GoT achieves comparable results to Multimodal-CoT large with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8519.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8519.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-of-Thought (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage reasoning framework that augments Chain-of-Thought (CoT) with an explicit thought graph constructed via Extract-Clustering-Coreference (ECC), encoded with a graph attention network and fused (optionally with vision features) into a T5 encoder-decoder to improve deductive, non-sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GoT (T5 backbone, FLAN-Alpaca checkpoints; base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage fine-tuned system using T5 (FLAN-Alpaca) backbones (base: ~223–233M params, large: ~738M). Thought graphs constructed from OpenIE triplets + coreference (ECC), encoded by a GAT, fused with text (and optional vision) via gated fusion; trained 100 epochs, lr=5e-5.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (linear rationale generation)', 'graph-based deductive reasoning (thought graph / ECC)', 'multimodal fusion (text + vision + graph)', 'graph attention-based relational inference']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GoT keeps the standard two-stage CoT pipeline (rationale generation then answer inference) and augments it by: (1) extracting subject-verb-object triplets from input (OpenIE), clustering coreferent mentions and building a thought graph (ECC); (2) encoding nodes via a text encoder then applying a multi-head Graph Attention Network to produce graph embeddings; (3) aligning text tokens with graph nodes (and image patches when present) via single-head attention and fusing features with a gated fusion before decoding rationales/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons versus traditional CoT-based fine-tuned baselines (FLAN-Alpaca two-stage CoT and Multimodal-CoT) and ablative variants: Random Thought Graph (random nodes/edges), Triplets Concatenation (append triplets to text, no graph encoding), Coreference Injection (replace coreferent mentions in raw text), and an enlarged Multimodal-CoT (to match parameter count) to control for parameter effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT (text-only algebraic word problems with rationales) and ScienceQA (multimodal science question answering with rationales and images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AQUA-RAT: GoT-T5_base 32.09% ±1.62 ACC vs FLAN-Alpaca_base 30.09% ±1.12 (absolute +2.00% reported); GoT-T5_large 34.48% ±1.11 vs FLAN-Alpaca_large 33.73% ±1.14 (+0.75%). Rationale generation ROUGE-L improvement for GoT_base: +0.78. ScienceQA (average accuracy): GoT-TS_base 87.59% ±0.20 vs Multimodal-CoT_base 85.19% (+2.40%); GoT-TS_large 90.81% ±0.12 vs Multimodal-CoT_large 90.45% (+0.36%). Rationale ROUGE-L on ScienceQA: GoT_base 94.39 vs Mutimodal-CoT 93.24 (+1.15). Parameter count: GoT ~233M vs Mutimodal-CoT_base 227M; inference slower (13.38 vs 16.33 samples/sec). Ablations on AQUA-RAT: Random Thought Graph --> 30.31% (-1.78 vs GoT_base); Triplets Concatenation --> 31.20% (-0.89); Coreference Injection --> 30.32% (-1.77). Ablations on ScienceQA (UnifiedQA+DETR config): GoT_base 81.21% (vs Mutimodal-CoT_base 77.67%); GoT_base w/ Random Thought Graph 76.74% (-4.47 vs GoT_base); Multimodal-CoTbase(enlarged) 79.17% (-2.04 vs GoT_base).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Graph structure yields more effective deductive reasoning and improved robustness: visualization of GAT attention shows GoT focuses on key thought nodes and downweights disruptive nodes (improving robustness to noisy/wrong rationales). Gains are larger on image-involving questions and higher-grade (more complex) items. Simply concatenating triplets or injecting coreference into text gives smaller gains or introduces noise; randomly constructed graphs harm performance. GoT achieves SOTA on ScienceQA with fewer parameters than large LLMs and is more robust to incorrect rationales in several case studies. Trade-offs: modest parameter increase and slower inference.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining linear CoT reasoning with non-linear graph-structured thought representations (GoT) improves deductive reasoning and downstream accuracy relative to CoT and Multimodal-CoT baselines; structural information in a thought graph is crucial (randomized or naive treatments degrade performance), and parameter increases alone do not explain the gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8519.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Chain-of-Thought (Multimodal-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage CoT fine-tuning approach that integrates visual features into Chain-of-Thought reasoning via gated fusion to improve multimodal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multimodal chain-of-thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multimodal-CoT (T5 backbone, base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage fine-tuning of T5-based models where generated rationales are produced then used for final answer; visual features projected and fused with text via gated fusion; implemented with ViT-large vision encoder and FLAN-Alpaca checkpoints (base/large).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (linear rationale generation)', 'multimodal feature fusion (text + vision via gated fusion)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generates textual chain-of-thought rationales then uses them plus fused image features (extracted by a vision encoder, projected and aligned) to predict answers; fusion is a gated linear combination of text and vision attention outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as the primary baseline for multimodal experiments; experiments enlarged its gated fusion module to match GoT parameter count (Multimodal-CoT_base(enlarged)) to test whether parameter increase explains GoT gains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ScienceQA (multimodal science QA), and used as a point of comparison on rationale generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>ScienceQA: Multimodal-CoT_base average 85.19%; Multimodal-CoT_large average 90.45%. Rationale ROUGE-L: 93.24 (Mutimodal-CoT). Enlarged Multimodal-CoT_base (param-matched) scored 79.17% (in the UnifiedQA+DETR ablation configuration) vs GoT_base 81.21% (+2.04 gap favoring GoT).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Effective baseline for multimodal CoT, but adding explicit thought-graph structure (GoT) provided consistent gains beyond gated fusion alone. When thought graph is randomized, performance can fall below Multimodal-CoT, indicating the model may rely heavily on structured graph information when other encoders are weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Multimodal-CoT improves multimodal reasoning via fused vision features, but the inclusion of structured thought graphs (GoT) yields additional, non-trivial improvements that are not attributable solely to parameter increases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8519.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting / fine-tuning baselines (Zero-shot-CoT, Few-shot CoT, Manual-CoT, Manual CoT fine-tuned FLAN-Alpaca)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard CoT techniques that prompt or fine-tune models to emit intermediate natural language rationales that lead to final answers; includes zero-shot prompting, few-shot prompting with demonstrations, and fine-tuned two-stage CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT (various LLMs and fine-tuned T5 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Includes zero-shot CoT prompting (adding 'Let's think step by step'), few-shot CoT with hand-crafted demonstrations, Auto-CoT-style automatic demonstration construction, and two-stage CoT fine-tuning of T5 (FLAN-Alpaca) used as baselines in AQUA-RAT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (linear step-by-step natural language rationales)', 'few-shot demonstration-based CoT', 'zero-shot CoT prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT elicits intermediate natural language reasoning chains; zero-shot methods use a single prompt phrase to trigger CoT; few-shot uses explicit demonstrations; fine-tuned two-stage CoT trains a model to produce rationales then answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT baselines compared against GoT on AQUA-RAT (fine-tuned FLAN-Alpaca two-stage CoT) and a number of prompting baselines (Zero-shot, Zero-shot-CoT, Few-Shot, Manual-CoT, Auto-CoT) are reported in the paper's AQUA-RAT baseline table.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT (primary), also referenced for other CoT prompting results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AQUA-RAT baselines with GPT-3 (175B): Zero-Shot 22.40%, Zero-Shot-CoT 33.50%, Few-Shot 24.80%, Manual-CoT 35.80%, Auto-CoT 36.50%. Fine-tuned FLAN-Alpaca_base (two-stage CoT) 30.09% ±1.12; FLAN-Alpaca_large 33.73% ±1.14.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Traditional CoT improves performance substantially over vanilla zero-shot/few-shot non-CoT prompting, but GoT (graph augmentation) yields additional gains vs fine-tuned CoT, especially on deductive and multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>While CoT prompting/fine-tuning unlocks reasoning ability, adding a structured thought graph (GoT) that captures non-linear deductive relations improves over purely sequential CoT approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8519.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto Chain-of-Thought (Auto-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic method for constructing few-shot CoT demonstrations by clustering problems and sampling representative demonstrations to maintain diversity in CoT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chain of thought prompting in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Auto-CoT (applied with GPT-3 175B in baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Automatic selection of diverse few-shot CoT demonstrations via clustering and representative sampling to generate CoT-style prompts without manual curation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot chain-of-thought with automatic demonstration selection (diversity-focused)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Auto-CoT clusters dataset problems to preserve diversity of demonstrations and generates reasoning chains automatically as few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned as a few-shot CoT baseline on AQUA-RAT (uses GPT-3 175B); compared in performance tables but not reimplemented in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AQUA-RAT: Auto-CoT (GPT-3 175B) reported 36.50% accuracy (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Auto-CoT aims to preserve demonstration diversity; cited as improving CoT few-shot results relative to hand-crafted or random demonstrations in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Auto-CoT uses diverse demonstration sampling to improve few-shot CoT prompting; cited as a competitive baseline but not central to GoT experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8519.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency for CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple reasoning paths (chains of thought) from an LLM and aggregates final answers (e.g., majority vote) to improve CoT accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency (applied on CoT outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Post-processing approach that runs a model multiple times to generate diverse CoT responses and uses majority voting or aggregation over final answers to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['diverse chain-of-thought sampling + answer aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generates a population of reasoning chains by sampling and then uses consistency across outputs to pick the most likely answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Referenced in related work as a method to improve CoT; not evaluated experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Mentioned as an approach to improve CoT by leveraging multiple diverse reasoning traces; not part of GoT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency improves chain-of-thought performance by leveraging diverse reasoning samples, but GoT focuses on structural thought augmentation rather than sampling-based diversity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8519.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent approach that models non-linear thought exploration by treating intermediate thought units as nodes in a tree search and using deliberation/search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree-of-Thoughts (general method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models thoughts as nodes in a search tree allowing exploration and backtracking (deliberation) to find high-quality reasoning chains, distinct from simple sequential CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['non-linear search-based reasoning (tree search over thought units)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Constructs a tree-shaped search over thought units and explores branches using LLMs as rollouts or evaluators to find better solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned in related work as a non-linear approach; compared conceptually to GoT (both aim to capture non-linear human thought) but ToT models tree search while GoT models explicit graph/deductive structure; no experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ToT emphasizes search/deliberation and explicit exploration, while GoT emphasizes representing deductive relations via a graph and fusing that structured representation into the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>GoT differs from ToT: ToT focuses on explicit search/exploration (tree), whereas GoT augments CoT with graph-structured representations to capture deductive relations; both aim to capture non-linear aspects of human thought.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8519.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Thought Graph (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Thought Graph (GoT ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where thought graphs are constructed by randomly selecting nodes and arbitrarily connecting them to test the importance of structured graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GoT with Random Thought Graph (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Maintains GoT pipeline but replaces the ECC-constructed thought graph with a randomly constructed graph (random nodes and edges) to disrupt deductive structure.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought + unstructured/random graph features (ablation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluates whether the GoT architecture's gains depend on semantically meaningful graph structure by injecting random graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (destroys intended graph-based diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablation compared against full GoT on AQUA-RAT and ScienceQA (UnifiedQA+DETR experiment); reports accuracy drops to quantify dependence on structure.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT and ScienceQA (ablation configs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AQUA-RAT: Random Thought Graph --> 30.31% ACC (−1.78 vs GoT_base 32.09%). ScienceQA (UnifiedQA+DETR config): GoT_base w/ Random Thought Graph 76.74% (−4.47 vs GoT_base 81.21%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Randomizing graph structure substantially degrades performance, indicating the gains come from meaningful deductive structure rather than merely extra parameters or model changes.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Performance falls when thought graphs are random, demonstrating that structured (deductive) graph information is necessary for GoT's improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8519.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triplets Concatenation (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triplets Concatenation (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that appends extracted triplets directly to the input text instead of constructing and encoding a thought graph, testing the value of explicit graph structure versus raw extra tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GoT w/ Triplets Concatenation (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extracted OpenIE triplets are concatenated to the textual input so the model sees the triplets as additional text tokens; no graph encoding/GAT is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought + appended triplet text (no graph encoding)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Assesses whether adding the extracted relational triplets as flat text suffices or whether an explicit graph encoding is required.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to full GoT on AQUA-RAT to measure impact of removing explicit graph structure while keeping extracted information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AQUA-RAT: Triplets Concatenation --> 31.20% ACC (−0.89 vs GoT_base 32.09%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Appending triplets improves over baseline but underperforms full graph encoding, demonstrating that structural encoding (graph attention) yields additional reasoning benefits beyond raw extra tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Structural information encoded by a thought graph matters: mere inclusion of triplets as text is insufficient to match the performance of graph-encoded GoT.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8519.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8519.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coreference Injection (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coreference Injection (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that injects coreference resolution into the input by replacing coreferent entities in raw text with a consistent representative phrase, testing the effect of providing coreference information inline rather than via graph clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GoT w/ Coreference Injection (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Coreference clusters are resolved and replaced inline in input text before fine-tuning, without constructing a thought graph; aims to test whether coreference alone suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought + injected coreference information (no graph encoding)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Replaces mentions of coreferent entities in the input text with a single representative mention to provide the model explicit coreference cues within the text.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to full GoT on AQUA-RAT to test whether inline coreference replacement replicates graph-based benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AQUA-RAT: Coreference Injection --> 30.32% ACC (−1.77 vs GoT_base 32.09%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Coreference Injection tended to reduce performance (nearly as much as random graphs); authors attribute this to loss of textual coherence or noisy OpenIE/coref errors when replacing mentions inline, showing the graph-based approach preserves text and supplements it with structure.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Directly injecting coreference into text is less effective than constructing a thought graph — graph construction preserves original input coherence while providing denser deductive structure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multimodal chain-of-thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Calc-X and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8519",
    "paper_id": "paper-adb9acaf9184bdbd23105f1a383848eed9bc82fc",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GoT",
            "name_full": "Graph-of-Thought (GoT)",
            "brief_description": "A two-stage reasoning framework that augments Chain-of-Thought (CoT) with an explicit thought graph constructed via Extract-Clustering-Coreference (ECC), encoded with a graph attention network and fused (optionally with vision features) into a T5 encoder-decoder to improve deductive, non-sequential reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GoT (T5 backbone, FLAN-Alpaca checkpoints; base & large)",
            "model_description": "Two-stage fine-tuned system using T5 (FLAN-Alpaca) backbones (base: ~223–233M params, large: ~738M). Thought graphs constructed from OpenIE triplets + coreference (ECC), encoded by a GAT, fused with text (and optional vision) via gated fusion; trained 100 epochs, lr=5e-5.",
            "reasoning_methods": [
                "chain-of-thought (linear rationale generation)",
                "graph-based deductive reasoning (thought graph / ECC)",
                "multimodal fusion (text + vision + graph)",
                "graph attention-based relational inference"
            ],
            "reasoning_methods_description": "GoT keeps the standard two-stage CoT pipeline (rationale generation then answer inference) and augments it by: (1) extracting subject-verb-object triplets from input (OpenIE), clustering coreferent mentions and building a thought graph (ECC); (2) encoding nodes via a text encoder then applying a multi-head Graph Attention Network to produce graph embeddings; (3) aligning text tokens with graph nodes (and image patches when present) via single-head attention and fusing features with a gated fusion before decoding rationales/answers.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparisons versus traditional CoT-based fine-tuned baselines (FLAN-Alpaca two-stage CoT and Multimodal-CoT) and ablative variants: Random Thought Graph (random nodes/edges), Triplets Concatenation (append triplets to text, no graph encoding), Coreference Injection (replace coreferent mentions in raw text), and an enlarged Multimodal-CoT (to match parameter count) to control for parameter effects.",
            "task_or_benchmark": "AQUA-RAT (text-only algebraic word problems with rationales) and ScienceQA (multimodal science question answering with rationales and images).",
            "performance_results": "AQUA-RAT: GoT-T5_base 32.09% ±1.62 ACC vs FLAN-Alpaca_base 30.09% ±1.12 (absolute +2.00% reported); GoT-T5_large 34.48% ±1.11 vs FLAN-Alpaca_large 33.73% ±1.14 (+0.75%). Rationale generation ROUGE-L improvement for GoT_base: +0.78. ScienceQA (average accuracy): GoT-TS_base 87.59% ±0.20 vs Multimodal-CoT_base 85.19% (+2.40%); GoT-TS_large 90.81% ±0.12 vs Multimodal-CoT_large 90.45% (+0.36%). Rationale ROUGE-L on ScienceQA: GoT_base 94.39 vs Mutimodal-CoT 93.24 (+1.15). Parameter count: GoT ~233M vs Mutimodal-CoT_base 227M; inference slower (13.38 vs 16.33 samples/sec). Ablations on AQUA-RAT: Random Thought Graph --&gt; 30.31% (-1.78 vs GoT_base); Triplets Concatenation --&gt; 31.20% (-0.89); Coreference Injection --&gt; 30.32% (-1.77). Ablations on ScienceQA (UnifiedQA+DETR config): GoT_base 81.21% (vs Mutimodal-CoT_base 77.67%); GoT_base w/ Random Thought Graph 76.74% (-4.47 vs GoT_base); Multimodal-CoTbase(enlarged) 79.17% (-2.04 vs GoT_base).",
            "qualitative_findings": "Graph structure yields more effective deductive reasoning and improved robustness: visualization of GAT attention shows GoT focuses on key thought nodes and downweights disruptive nodes (improving robustness to noisy/wrong rationales). Gains are larger on image-involving questions and higher-grade (more complex) items. Simply concatenating triplets or injecting coreference into text gives smaller gains or introduces noise; randomly constructed graphs harm performance. GoT achieves SOTA on ScienceQA with fewer parameters than large LLMs and is more robust to incorrect rationales in several case studies. Trade-offs: modest parameter increase and slower inference.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining linear CoT reasoning with non-linear graph-structured thought representations (GoT) improves deductive reasoning and downstream accuracy relative to CoT and Multimodal-CoT baselines; structural information in a thought graph is crucial (randomized or naive treatments degrade performance), and parameter increases alone do not explain the gains.",
            "uuid": "e8519.0"
        },
        {
            "name_short": "Multimodal-CoT",
            "name_full": "Multimodal Chain-of-Thought (Multimodal-CoT)",
            "brief_description": "A two-stage CoT fine-tuning approach that integrates visual features into Chain-of-Thought reasoning via gated fusion to improve multimodal reasoning.",
            "citation_title": "Multimodal chain-of-thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Multimodal-CoT (T5 backbone, base & large)",
            "model_description": "Two-stage fine-tuning of T5-based models where generated rationales are produced then used for final answer; visual features projected and fused with text via gated fusion; implemented with ViT-large vision encoder and FLAN-Alpaca checkpoints (base/large).",
            "reasoning_methods": [
                "chain-of-thought (linear rationale generation)",
                "multimodal feature fusion (text + vision via gated fusion)"
            ],
            "reasoning_methods_description": "Generates textual chain-of-thought rationales then uses them plus fused image features (extracted by a vision encoder, projected and aligned) to predict answers; fusion is a gated linear combination of text and vision attention outputs.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as the primary baseline for multimodal experiments; experiments enlarged its gated fusion module to match GoT parameter count (Multimodal-CoT_base(enlarged)) to test whether parameter increase explains GoT gains.",
            "task_or_benchmark": "ScienceQA (multimodal science QA), and used as a point of comparison on rationale generation metrics.",
            "performance_results": "ScienceQA: Multimodal-CoT_base average 85.19%; Multimodal-CoT_large average 90.45%. Rationale ROUGE-L: 93.24 (Mutimodal-CoT). Enlarged Multimodal-CoT_base (param-matched) scored 79.17% (in the UnifiedQA+DETR ablation configuration) vs GoT_base 81.21% (+2.04 gap favoring GoT).",
            "qualitative_findings": "Effective baseline for multimodal CoT, but adding explicit thought-graph structure (GoT) provided consistent gains beyond gated fusion alone. When thought graph is randomized, performance can fall below Multimodal-CoT, indicating the model may rely heavily on structured graph information when other encoders are weaker.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Multimodal-CoT improves multimodal reasoning via fused vision features, but the inclusion of structured thought graphs (GoT) yields additional, non-trivial improvements that are not attributable solely to parameter increases.",
            "uuid": "e8519.1"
        },
        {
            "name_short": "CoT Baselines",
            "name_full": "Chain-of-Thought prompting / fine-tuning baselines (Zero-shot-CoT, Few-shot CoT, Manual-CoT, Manual CoT fine-tuned FLAN-Alpaca)",
            "brief_description": "Standard CoT techniques that prompt or fine-tune models to emit intermediate natural language rationales that lead to final answers; includes zero-shot prompting, few-shot prompting with demonstrations, and fine-tuned two-stage CoT.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "CoT (various LLMs and fine-tuned T5 variants)",
            "model_description": "Includes zero-shot CoT prompting (adding 'Let's think step by step'), few-shot CoT with hand-crafted demonstrations, Auto-CoT-style automatic demonstration construction, and two-stage CoT fine-tuning of T5 (FLAN-Alpaca) used as baselines in AQUA-RAT experiments.",
            "reasoning_methods": [
                "chain-of-thought (linear step-by-step natural language rationales)",
                "few-shot demonstration-based CoT",
                "zero-shot CoT prompting"
            ],
            "reasoning_methods_description": "CoT elicits intermediate natural language reasoning chains; zero-shot methods use a single prompt phrase to trigger CoT; few-shot uses explicit demonstrations; fine-tuned two-stage CoT trains a model to produce rationales then answers.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "CoT baselines compared against GoT on AQUA-RAT (fine-tuned FLAN-Alpaca two-stage CoT) and a number of prompting baselines (Zero-shot, Zero-shot-CoT, Few-Shot, Manual-CoT, Auto-CoT) are reported in the paper's AQUA-RAT baseline table.",
            "task_or_benchmark": "AQUA-RAT (primary), also referenced for other CoT prompting results.",
            "performance_results": "AQUA-RAT baselines with GPT-3 (175B): Zero-Shot 22.40%, Zero-Shot-CoT 33.50%, Few-Shot 24.80%, Manual-CoT 35.80%, Auto-CoT 36.50%. Fine-tuned FLAN-Alpaca_base (two-stage CoT) 30.09% ±1.12; FLAN-Alpaca_large 33.73% ±1.14.",
            "qualitative_findings": "Traditional CoT improves performance substantially over vanilla zero-shot/few-shot non-CoT prompting, but GoT (graph augmentation) yields additional gains vs fine-tuned CoT, especially on deductive and multimodal tasks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "While CoT prompting/fine-tuning unlocks reasoning ability, adding a structured thought graph (GoT) that captures non-linear deductive relations improves over purely sequential CoT approaches.",
            "uuid": "e8519.2"
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Auto Chain-of-Thought (Auto-CoT)",
            "brief_description": "An automatic method for constructing few-shot CoT demonstrations by clustering problems and sampling representative demonstrations to maintain diversity in CoT examples.",
            "citation_title": "Automatic chain of thought prompting in large language models",
            "mention_or_use": "mention",
            "model_name": "Auto-CoT (applied with GPT-3 175B in baselines)",
            "model_description": "Automatic selection of diverse few-shot CoT demonstrations via clustering and representative sampling to generate CoT-style prompts without manual curation.",
            "reasoning_methods": [
                "few-shot chain-of-thought with automatic demonstration selection (diversity-focused)"
            ],
            "reasoning_methods_description": "Auto-CoT clusters dataset problems to preserve diversity of demonstrations and generates reasoning chains automatically as few-shot examples.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Mentioned as a few-shot CoT baseline on AQUA-RAT (uses GPT-3 175B); compared in performance tables but not reimplemented in this paper's experiments.",
            "task_or_benchmark": "AQUA-RAT (reported baseline)",
            "performance_results": "AQUA-RAT: Auto-CoT (GPT-3 175B) reported 36.50% accuracy (few-shot).",
            "qualitative_findings": "Auto-CoT aims to preserve demonstration diversity; cited as improving CoT few-shot results relative to hand-crafted or random demonstrations in prior work.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Auto-CoT uses diverse demonstration sampling to improve few-shot CoT prompting; cited as a competitive baseline but not central to GoT experiments.",
            "uuid": "e8519.3"
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency for CoT",
            "brief_description": "A technique that samples multiple reasoning paths (chains of thought) from an LLM and aggregates final answers (e.g., majority vote) to improve CoT accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "Self-Consistency (applied on CoT outputs)",
            "model_description": "Post-processing approach that runs a model multiple times to generate diverse CoT responses and uses majority voting or aggregation over final answers to improve robustness.",
            "reasoning_methods": [
                "diverse chain-of-thought sampling + answer aggregation"
            ],
            "reasoning_methods_description": "Generates a population of reasoning chains by sampling and then uses consistency across outputs to pick the most likely answer.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Referenced in related work as a method to improve CoT; not evaluated experimentally in this paper.",
            "task_or_benchmark": "",
            "performance_results": null,
            "qualitative_findings": "Mentioned as an approach to improve CoT by leveraging multiple diverse reasoning traces; not part of GoT experiments.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Self-consistency improves chain-of-thought performance by leveraging diverse reasoning samples, but GoT focuses on structural thought augmentation rather than sampling-based diversity.",
            "uuid": "e8519.4"
        },
        {
            "name_short": "Tree-of-Thought (ToT)",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A concurrent approach that models non-linear thought exploration by treating intermediate thought units as nodes in a tree search and using deliberation/search strategies.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": "Tree-of-Thoughts (general method)",
            "model_description": "Models thoughts as nodes in a search tree allowing exploration and backtracking (deliberation) to find high-quality reasoning chains, distinct from simple sequential CoT.",
            "reasoning_methods": [
                "non-linear search-based reasoning (tree search over thought units)"
            ],
            "reasoning_methods_description": "Constructs a tree-shaped search over thought units and explores branches using LLMs as rollouts or evaluators to find better solutions.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Mentioned in related work as a non-linear approach; compared conceptually to GoT (both aim to capture non-linear human thought) but ToT models tree search while GoT models explicit graph/deductive structure; no experimental comparison in this paper.",
            "task_or_benchmark": "",
            "performance_results": null,
            "qualitative_findings": "ToT emphasizes search/deliberation and explicit exploration, while GoT emphasizes representing deductive relations via a graph and fusing that structured representation into the LM.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "GoT differs from ToT: ToT focuses on explicit search/exploration (tree), whereas GoT augments CoT with graph-structured representations to capture deductive relations; both aim to capture non-linear aspects of human thought.",
            "uuid": "e8519.5"
        },
        {
            "name_short": "Random Thought Graph (ablation)",
            "name_full": "Random Thought Graph (GoT ablation)",
            "brief_description": "An ablation where thought graphs are constructed by randomly selecting nodes and arbitrarily connecting them to test the importance of structured graph information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GoT with Random Thought Graph (ablation)",
            "model_description": "Maintains GoT pipeline but replaces the ECC-constructed thought graph with a randomly constructed graph (random nodes and edges) to disrupt deductive structure.",
            "reasoning_methods": [
                "chain-of-thought + unstructured/random graph features (ablation)"
            ],
            "reasoning_methods_description": "Evaluates whether the GoT architecture's gains depend on semantically meaningful graph structure by injecting random graphs.",
            "reasoning_diversity": "similar (destroys intended graph-based diversity)",
            "reasoning_diversity_experimental_setup": "Ablation compared against full GoT on AQUA-RAT and ScienceQA (UnifiedQA+DETR experiment); reports accuracy drops to quantify dependence on structure.",
            "task_or_benchmark": "AQUA-RAT and ScienceQA (ablation configs)",
            "performance_results": "AQUA-RAT: Random Thought Graph --&gt; 30.31% ACC (−1.78 vs GoT_base 32.09%). ScienceQA (UnifiedQA+DETR config): GoT_base w/ Random Thought Graph 76.74% (−4.47 vs GoT_base 81.21%).",
            "qualitative_findings": "Randomizing graph structure substantially degrades performance, indicating the gains come from meaningful deductive structure rather than merely extra parameters or model changes.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Performance falls when thought graphs are random, demonstrating that structured (deductive) graph information is necessary for GoT's improvements.",
            "uuid": "e8519.6"
        },
        {
            "name_short": "Triplets Concatenation (ablation)",
            "name_full": "Triplets Concatenation (ablation)",
            "brief_description": "An ablation that appends extracted triplets directly to the input text instead of constructing and encoding a thought graph, testing the value of explicit graph structure versus raw extra tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GoT w/ Triplets Concatenation (ablation)",
            "model_description": "Extracted OpenIE triplets are concatenated to the textual input so the model sees the triplets as additional text tokens; no graph encoding/GAT is applied.",
            "reasoning_methods": [
                "chain-of-thought + appended triplet text (no graph encoding)"
            ],
            "reasoning_methods_description": "Assesses whether adding the extracted relational triplets as flat text suffices or whether an explicit graph encoding is required.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared to full GoT on AQUA-RAT to measure impact of removing explicit graph structure while keeping extracted information.",
            "task_or_benchmark": "AQUA-RAT",
            "performance_results": "AQUA-RAT: Triplets Concatenation --&gt; 31.20% ACC (−0.89 vs GoT_base 32.09%).",
            "qualitative_findings": "Appending triplets improves over baseline but underperforms full graph encoding, demonstrating that structural encoding (graph attention) yields additional reasoning benefits beyond raw extra tokens.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Structural information encoded by a thought graph matters: mere inclusion of triplets as text is insufficient to match the performance of graph-encoded GoT.",
            "uuid": "e8519.7"
        },
        {
            "name_short": "Coreference Injection (ablation)",
            "name_full": "Coreference Injection (ablation)",
            "brief_description": "An ablation that injects coreference resolution into the input by replacing coreferent entities in raw text with a consistent representative phrase, testing the effect of providing coreference information inline rather than via graph clustering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GoT w/ Coreference Injection (ablation)",
            "model_description": "Coreference clusters are resolved and replaced inline in input text before fine-tuning, without constructing a thought graph; aims to test whether coreference alone suffices.",
            "reasoning_methods": [
                "chain-of-thought + injected coreference information (no graph encoding)"
            ],
            "reasoning_methods_description": "Replaces mentions of coreferent entities in the input text with a single representative mention to provide the model explicit coreference cues within the text.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared to full GoT on AQUA-RAT to test whether inline coreference replacement replicates graph-based benefits.",
            "task_or_benchmark": "AQUA-RAT",
            "performance_results": "AQUA-RAT: Coreference Injection --&gt; 30.32% ACC (−1.77 vs GoT_base 32.09%).",
            "qualitative_findings": "Coreference Injection tended to reduce performance (nearly as much as random graphs); authors attribute this to loss of textual coherence or noisy OpenIE/coref errors when replacing mentions inline, showing the graph-based approach preserves text and supplements it with structure.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Directly injecting coreference into text is less effective than constructing a thought graph — graph construction preserves original input coherence while providing denser deductive structure.",
            "uuid": "e8519.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multimodal chain-of-thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Calc-X and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems",
            "rating": 1
        }
    ],
    "cost": 0.0217655,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models</h1>
<p>Yao Yao ${ }^{1,2}$, Zuchao $\mathbf{L i}^{3, <em>}$ and Hai Zhao ${ }^{1,2, </em>}$<br>${ }^{1}$ Department of Computer Science and Engineering, Shanghai Jiao Tong University<br>${ }^{2}$ MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University<br>${ }^{3}$ National Engineering Research Center for Multimedia Software,<br>School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China<br>yaoyao27@sjtu.edu.cn, zcli-charlie@whu.edu.cn,<br>zhaohai@cs.sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>With the widespread use of language models (LMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. GoT adopts a two-stage framework with an additional GoT encoder for thought graph representation and fuses the graph representation with the original input representation through a gated fusion mechanism. We evaluate GoT's performance on a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline on the AQUA-RAT test set and boosts accuracy from $85.19 \%$ to $87.59 \%$ using the T5-base model over the state-of-theart Multimodal-CoT (Zhang et al., 2023) on the ScienceQA test set. Our code is publicly available at https://github.com/Zoeyyao27/Graph-of-Thought</p>
<h2>1 Introduction</h2>
<p>In the field of human cognition, it has long been recognized that the human thought process is far more complex and non-linear than could be captured by a simple, sequential chain of thoughts (Barsalou,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>1999). Human thinking is often characterized by its ability to make sudden leaps and connections between seemingly unrelated ideas, which can lead to novel insights and solutions. This non-linear, jumping thought process is a hallmark of human creativity, reasoning, and problem-solving abilities. However, it also poses a significant challenge for cognitive modeling and understanding.</p>
<p>Recently, Large Language Models (LLMs) have been advancing at an unprecedented pace. With the emergence of breakthroughs such as GPT3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023), the field of natural language processing has entered a new era of possibilities. Recent studies (Wei et al., 2022a; Wang et al., 2022; Zhang et al., 2022) have shown that the reasoning ability of LLMs can be unlocked by Chain-of-Thought (CoT) prompting. CoT prompting involves a series of intermediate natural language rationales that lead to the final answer. In addition, Zhang et al. (2023) have introduced Multimodal-CoT, which combines both language and visual modalities to help surpass the limitations of textual information. More detailed related works can be found in Appendix A.</p>
<p>Previous works on Chain-of-Thought (CoT) prompting, which have been limited to textual and visual information, often represented the human reasoning process as sequential thought chains. This approach overlooks the modeling of humans' jumping thought process and neglects to incorporate the complex structural information of reasoning thoughts into the model. Concurrent work Tree-of-thoughts (ToT) (Yao et al., 2023) divides thoughts into thought units and models them as a tree-like search process.</p>
<p>Nevertheless, human cognition transcends this tree structure, exhibiting intricate graph-like formations. Our perspective diverges further as we believe that the human intellect is capable of crafting elaborate thought graphs founded upon linear</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of GoT reasoning. Vision features are optional and are only required in multimodal reasoning.</p>
<p>thoughts. Therefore, we aim to enable the concurrent assimilation of linear and nonlinear cognitive processes, surpassing the mere generation of segmented thought units. To address the above limitation, different from ToT, we propose the Graph-of-Thought (GoT), a novel approach to modeling human thought processes not only as a chain but also as a graph. Our method is based on the assumption that the human mind works by connecting and recombining ideas in a non-sequential, graph fashion, rather than following a strict sequential chain. By representing thought units as nodes and connections between thoughts as edges, GoT captures the rich, non-sequential nature of human thinking and allows for a more realistic and logical modeling of reasoning processes.</p>
<p>An example of GoT reasoning is shown in Figure 1. Inspired by Multimodal-CoT (Zhang et al., 2023), we have adopted a two-stage reasoning framework. It first generates rationales and then generates the final answer based on the predicted rationales. In addition to text features, graph features of GoT are integrated during the rationale generation and answer inference. Specifically, GoT is first constructed with an Extract-Cluster-Coreference (ECC) process, which simulates the deductive process in human reasoning. We have used T5 (Raffel et al., 2020a) pre-trained language model as our backbone model. GoT is encoded with a graph attention network and then fused with the original representation via a gated fusion network.</p>
<p>Furthermore, we have also presented a multimodal GoT, which integrates not only text features and GoT features but also visual features. For our experiments, we have used both FLAN-Alpaca 1 (T5)-base and FLAN-Alpaca (T5)-large as our backbone models.</p>
<p>We implement GoT as a two-stage framework and fine-tuning language models and integrating text, thought graph, and vision features for a more realistic and accurate reasoning process. GoT demonstrates exceptional performance on both text-only AQUA-RAT (Ling et al., 2017) and multimodal ScienceQA (Lu et al., 2022) benchmarks, surpassing the accuracy of online system ChatGPT (OpenAI, 2023) by 9.28%, strong baseline Multimodal-CoT (Zhang et al., 2023) by 2.40%, and even exceeding human performance, establishing a new state-of-the-art on ScienceQA test set with far fewer parameters.</p>
<h1>2 Graph-of-Thought</h1>
<p>The overview of our proposed GoT can be seen in Figure 2. Inspired by Multimodal-CoT (Zhang et al., 2023), GoT also adopts a two-stage framework. (1) Rationale generation stage: In the first stage, the model generates rationales based on the input text (including question, context, and choices) the vision features, and the generated thought graph corresponding to the input text. For multi-modal tasks (Zhang et al., 2023; Zhang and Zhang, 2023; Huang et al., 2023; Peng et al., 2023), it is a common practice to use different encoders to process inputs from different modalities and a straightfor-</p>
<p><sup>1</sup>https://github.com/declare-lab/flan-alpaca. FLAN-Alpaca is developed by fine-tuning T5 model on the Flan collection</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: Graph-of-Thought framework overview</p>
<p>ward and versatile approach is to employ encoder-decoder models. Therefore, GoT employs independent encoders to encode input data for each modality. We use a Transformer encoder to encode input text, a vision encoder to encode an image, and a graph attention network to encode the thought graph. The encoded features are further passed into cross-attention to align text tokens with image patches and graph nodes, respectively. We then use a gated fusion layer to fuse these three features further and pass them into the Transformer decoder to predict the target rationales. (2) Answer generation stage: The second stage aims at generating the final answer and is largely similar to the first stage. The main difference is that the input text is concatenated with the predicted rationales from the first stage. It is worth noting that the above process describes a general multimodal reasoning framework. However, for text-only reasoning tasks, there are no image features, so the image encoding and vision feature fusion processes mentioned above can be omitted. In the following section, we will provide a detailed exposition of the two key steps of our GoT reasoning framework: GoT construction and GoT encoding and feature fusion.</p>
<h3>2.1 GoT Construction</h3>
<p>GoT employs thought graphs to simulate human deductive reasoning, thereby modeling humans’ ability for leaps of thought. Our aim is to reflect the most fundamental deduction process by constructing a thought graph. If we have evidence that $x$ $\rightarrow$ $y$ and $y$ $\rightarrow$ $z$, then it follows that $x$ $\rightarrow$ $z$. In Figure 3, the deduction reasoning can be formulated as follows: Earthquake $\xrightarrow{\text{comes from}}$ {earth, quake}, {earth, quake} $\xrightarrow{\text{means}}$ {ground, shake}. It is easy to reason that Earthquake$\longrightarrow${ground, shake}.</p>
<p>We propose a novel Extract-Clustering-Coreference (ECC) process to construct thought graphs. ECC first extracts deductive triplets $T=\left{t^{i}=\left(t_{x}^{i},t_{y}^{i},t_{z}^{i}\right)\right}$ as the discrete raw graph, where $t_{x}^{i}$, $t_{y}^{i}$, and $t_{z}^{i}$ are thought units of the $i$-th triplet, and there exists an edge $e_{xy}^{i}$ between $t_{x}^{i}$ and $t_{y}^{i}$, and an edge $e_{yz}^{i}$ between $t_{y}^{i}$ and $t_{z}^{i}$. Then, ECC clusters the nodes that refer to the same mentions to conduct coreference resolution. Specifically, we replace every graph node that belongs to a coreference cluster with the most representative mention in the cluster. By adopting this technique, our model is better equipped with denser thought graphs and the ability for deductive reasoning. The detailed algorithm is illustrated in Algorithm 1.</p>
<p>In GoT construction, during the rationale generation stage, the input text consists of concatenated question, context, and choices. In multimodal GoT, image caption <em>Lu et al. (2022)</em> is appended to the input text for GoT to incorporate image in-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Graph-of-Thought deduction example</p>
<p>formation. During the answer inference stage, the predicted rationales from the rationale generation stage are further concatenated with the input text for corresponding GoT construction.</p>
<p>In our implementation of ECC process, inspired by Chen and Yang, 2021, we utilize open information extraction (OpenIE) systems ^{2} (Angeli et al., 2015) to extract subject-verb-object triplets as thought unit nodes. We apply coreference resolution to the extracted nodes using the Stanford CoreNLP system (Manning et al., 2014). The constructed thought graph is denoted as G(N, E), where N represents the nodes extracted by OpenIE and E represents the adjacency matrix. Rows and columns correspond to the nodes in the graph, and if there is an edge between two nodes, the corresponding matrix element is 1; otherwise, it is 0.</p>
<p>Algorithm 1 ECC process</p>
<p>Input: Input text S
Output: Thought graph G(N, E)
Extract deductive triplet set T from S
T = {t<sup>0</sup>, t<sup>1</sup>, ..., t<sup>n</sup>}, t<sup>i</sup> = (t<sup>i</sup><sub>x</sub>, t<sup>i</sup><sub>y</sub>, t<sup>i</sup><sub>z</sub>)
for every triplet t<sup>i</sup> ∈ T do
N<sup>r</sup> ← N<sup>r</sup> ∪ {t<sup>i</sup><sub>x</sub>, t<sup>i</sup><sub>y</sub>, t<sup>i</sup><sub>z</sub>}
E<sup>r</sup> ← E<sup>r</sup> ∪ {e<sup>i</sup><sub>xy</sub>, e<sup>i</sup><sub>yz</sub>}
end for
extract coreference clusters C for N<sup>r</sup>
for every node n<sup>i</sup> ∈ N<sup>r</sup> do
if n<sup>i</sup> ∈ ∀c<sup>j</sup> ∈ C then
n<sup>*</sup><sub>j</sub> ← most representative mention in c<sup>j</sup>
N ← N ∪ {n<sup>i</sup><sub>j</sub>}
end if
end for
Reconnect N based on E<sup>r</sup> to construct E
return N, E</p>
<h3>2.2 GoT Encoding and Integration</h3>
<p>GoT reasoning utilizes separate encoders to encode input data for each modality. The thought graph is encoded using a graph attention network, while the input text is encoded using a Transformer encoder. In multimodal GoT reasoning, the image is encoded using an additional vision encoder.</p>
<h4>2.2.1 Base Encoder</h4>
<p><strong>Text Encoder</strong> For text representation, we use the Transformer encoder (e.g. T5 (Raffel et al., 2020a)) to encode the input text. Given input sentence S = {w<sup>0</sup>, ..., w<sup>l</sup>}, we extract the hidden states from the last layer of the Transformer encoder to obtain the text representation H<sup>T</sup>:</p>
<p>$$H^T = { h_0, h_1, \dots, h_l } = \text{Encoder}_{\text{text}}(S) \qquad (1)$$</p>
<p>where h<sup>i</sup> is the hidden representation of token i and l represents the length of the text input.</p>
<p><strong>Vision Encoder (Optional)</strong> For multimodal reasoning with vision modality, following Zhang et al., 2023, we extract patch-level features of image I using readily available vision extraction model as vision encoder Encoder<sub>vision</sub> and then employ a trainable projection matrix W<sub>I</sub> to project the extracted features into the vision representation H<sup>I</sup> which have the same shape with H<sup>T</sup>.</p>
<p>$$H^I = \mathbf{W}<em _text_vision="\text{vision">{\mathbf{I}} \text{Encoder}</em>$$}}(I) \tag{2</p>
<h4>2.2.2 GoT Encoder</h4>
<p><strong>Node Embedding</strong> We first use special tokens <s> and </s> to highlight every thought graph node. Specifically, for node set with j nodes N = {n<sub>0</sub>, ...n<sub>j</sub>}, we construct the node input as p and then feed the p into the same text encoder and utilize the output representation of the special token <s> as the initial node representation. Formally,</p>
<p>$$p = [ <s>, n_0, <s>, \dots, <s>, n_j, <s> ] \tag{3}$$</p>
<p><sup>2</sup>https://github.com/philipperemy/Stanford-OpenIE-Python</p>
<p>$\left[h_{0}^{s}, h_{0}^{n}, h_{0}^{e}, \ldots, h_{j}^{s}, h_{j}^{n}, h_{j}^{e}\right]=\operatorname{Encoder}<em i="i">{\text {text }}(p)$
where the $h</em>$.
}^{s}$ and $h_{i}^{e} \in \mathbb{R}^{D}$ are the representation of $\langle$ s $\rangle$ and $\langle /\left.\mathrm{s}\rangle$ for node $n_{i}$ respectively, $D$ is the dimension of node embedding, and the $h_{i}^{n}=$ $\left{h_{i, 1}^{n}, \ldots, h_{i, m}^{n}\right}$ is the representations of node $n_{i}$ with $m$ tokens. we use the $h_{i}^{s}$ to represent the node representation of $n_{i<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Architecture of GoT encoder
GAT Encoder We employ a graph attention network (GAT) (Velickovic et al., 2018; Chen and Yang, 2021) to encode the thought graph. For every node $n_{i}$ in graph $\mathcal{G}(\mathcal{N}, \mathcal{E})$, the graph attention layer is designed as:</p>
<p>$$
\begin{gathered}
a_{i j}=\operatorname{Attention}\left(\left[\mathbf{W} h_{i}^{s} | \mathbf{W} h_{j}^{s}\right]\right) \
q_{i j}=\operatorname{LeakyReLU}\left(a_{i j}\right) \
\alpha_{i j}=\operatorname{Softmax}\left(q_{i j}\right)=\frac{\exp \left(q_{i j}\right)}{\sum_{k \in \mathcal{K}<em i="i" k="k">{i}} \exp \left(q</em> \
h_{i}^{g \prime}=\operatorname{GELU}\left(\sum_{j \in \mathcal{K}}\right)<em i="i" j="j">{i}} \alpha</em>\right)
\end{gathered}
$$} \mathbf{W} h_{j}^{s</p>
<p>where $|$ denotes concatenate operation, the $\mathbf{W}$ is a trainable weight and the set $\mathcal{K}<em i="i">{i}$ contains the node $n</em>$ to
compute the attention weights, where $D^{\prime}$ is the attention layer output dimension. The attention weights $a_{i j}$ measures the importance of node $n_{i}$ 's features to $n_{j}$ 's features. By only calculating the attention weights between nodes who are neighbours, our graph attention layer demonstrates the ability to perceive structural information of graphs. In our implementation, we adopt a single-layer feed-forward neural network (FFNN) as the attention mechanism which is both simple and straightforward.}$ 's neighbours in thought graph $\mathcal{G}$. Our graph attention layer first employed a shared attention mechanism Attention(.) : $\mathbb{R}^{D^{\prime}} \times \mathbb{R}^{D^{\prime}} \rightarrow \mathbb{R</p>
<p>Figure 4 shows the architecture of our GoT encoder. Our GoT encoder employs a multi-head graph attention layer, following (Velickovic et al., 2018), we concatenate the output of each graph attention layer and further pass it to a output graph attention layer with the same architecture:</p>
<p>$$
\begin{gathered}
h_{i}^{g \prime}=|<em _in="\in" _mathcal_N="\mathcal{N" j="j">{k=1}^{K} \mathbf{G E L U}\left(\sum</em><em i="i" j="j">{i}} \alpha</em>\right) \
h_{i}^{g \prime \prime}=\mathbf{G E L U}\left(\sum_{j \in \mathcal{N}}^{k} \mathbf{W}^{k} h_{j}^{s<em i="i" j="j">{i}} \alpha</em>\right)
\end{gathered}
$$} \mathbf{W} h_{j}^{g \prime</p>
<p>where $K$ is the number of attention heads, $|$ is the concatenate operation, and $n$ is the number of nodes in thought graph. We then use a single-layer feed-forward neural network (FFNN) to obtain the final thought graph embedding $H^{G}$ :</p>
<p>$$
h^{g \prime \prime}=\left[h_{0}^{g \prime \prime}, \ldots, h_{n}^{g \prime \prime}\right] ; \quad H^{G}=\mathbf{F F N N}\left(h^{g \prime \prime}\right)
$$</p>
<h3>2.3 Feature Fusion</h3>
<p>After obtaining the encoded features, we use a single head attention to align the text representation $H^{T}$ with image representation $H^{I}$ and thought graph representation $H^{G}$, respectively. The image attention output $\mathbf{H}^{\mathbf{I}}$ and thought graph attention output $\mathbf{H}^{\mathbf{G}}$ are calculated by:</p>
<p>$$
\begin{gathered}
\mathbf{H}^{\mathbf{I}}=\operatorname{Softmax}\left(\frac{H^{T} H^{I \top}}{\sqrt{d}}\right) H^{I} \
\mathbf{H}^{\mathbf{G}}=\operatorname{Softmax}\left(\frac{H^{T} H^{G \top}}{\sqrt{d}}\right) H^{G}
\end{gathered}
$$</p>
<p>where $Q$ is $H^{T}$ and $d$ is the dimension of $H^{T}$. We take both $K_{I}$ and $V_{I}$ as $H^{I}$ and $K_{G}$ and $V_{G}$ as $H^{G}$. Please note that image representation is optional and is only required for multimodal dataset.</p>
<p>Next, a gated fusion mechanism (Wu et al., 2021; Zhang et al., 2023; Li et al., 2022; Zhang et al., 2020) is applied to combine the attention outputs $\mathbf{H}^{\mathbf{I}}$ and $\mathbf{H}^{\mathbf{G}}$ with the text representation $H^{T}$. The feature fusion output $H$ can be calculated by:</p>
<p>$$
\begin{aligned}
&amp; \lambda=\left{\begin{array}{l}
\text { Sigmoid }\left(W_{T} H^{T}+W_{G} \mathbf{H}^{\mathbf{G}}\right) \
\text { Sigmoid }\left(W_{T} H^{T}+W_{I} \mathbf{H}^{\mathbf{I}}+W_{G} \mathbf{H}^{\mathbf{G}}\right) \
\text { multimodal }
\end{array}\right. \
&amp; H=\left{\begin{array}{l}
(1-\lambda) \cdot H^{T}+\lambda \cdot \mathbf{H}^{\mathbf{G}} \
(1-\lambda) \cdot H^{T}+\lambda \cdot \mathbf{H}^{\mathbf{I}}+\lambda \cdot \mathbf{H}^{\mathbf{G}} \
\text { multimodal }
\end{array}\right.
\end{aligned}
$$</p>
<p>where $W_{T}, W_{I}$ and $W_{G}$ are all trainable weights. We then input the fused feature output $H$ into the decoder to predict the rationales or the final answer.</p>
<h2>3 Experiments</h2>
<p>Dataset We evaluate our model on the text-only AQUA-RAT (Ling et al., 2017) and multimodal ScienceQA benchmark (Lu et al., 2022). The detailed dataset information and statistics are shown in Appendix B.</p>
<p>Model Setup In our experiments, we used T5 (Raffel et al., 2020a) as our basic model architecture, including both T5-base and T5-large model sizes. Specifically, to ensure a fair comparison, we initialized our model with the finetuned T5 checkpoint FLAN-Alpaca ${ }^{3}$ and used ViT-large encoder (Dosovitskiy et al., 2021) for the vision encoder, following (Zhang et al., 2023). We finetuned the models for 100 epochs with a learning rate of $5 \mathrm{e}-5$. The detailed training parameters are available in Appendix C. We trained our models on four NVIDIA A800 80G GPUs.</p>
<h2>4 Results and Discussion</h2>
<h3>4.1 Main Results</h3>
<p>Baselines For AQUA-RAT, our baselines include: (1) Zero-Shot and Zero-Shot-CoT LLMs (Kojima et al., 2022); (2) Few-Shot and Manual-CoT LLMs (Wei et al., 2022b) and Auto-CoT (Zhang et al., 2022) (The above baselines all use the text-davinci002 version of GPT-3 with 175B parameters); (3)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Fintuned LLMs: Calcformer-T5-L (Kadlčík et al., 2023) which finetunes calculator-using T5-Large model on the Calc-X collection. To have a fair comparison we also fine-tuned FLAN-Alpaca ${ }<em _large="{large" _text="\text">{\text {base }}$ and FLAN-Alpaca ${ }</em>$ on AQUA-RAT with traditional two-stage CoT.}</p>
<p>For ScienceQA, following (Zhang et al., 2023; Lu et al., 2022), our adopted baselines include: (1) Vision question answering (VQA) baseline models (Yu et al., 2019; Anderson et al., 2018; Kim et al., 2018; Gao et al., 2019; Kim et al., 2021; Lu et al., 2021; Li et al., 2019, 2020); (2) Text-to-text LLMs (Raffel et al., 2020b; Chen et al., 2020) and (3) Text-to-text LLMs with CoT prompting (Lu et al., 2022; Zhang et al., 2023). Both UnifiedQA (Lu et al., 2022) and GPT-3.5 (Lu et al., 2022) use generated image captions to incorporate vision semantics. Whereas, MutimodalCoT (Zhang et al., 2023) injects generated image features into traditional CoT reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODELS</th>
<th style="text-align: left;">TRAINING</th>
<th style="text-align: right;">SIZE</th>
<th style="text-align: right;">ACC(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-Shot (Kojima et al., 2022)</td>
<td style="text-align: left;">zero-shot</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: right;">22.40</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT (Kojima et al., 2022)</td>
<td style="text-align: left;">zero-shot</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: right;">33.50</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot (Wei et al., 2022b)</td>
<td style="text-align: left;">few-shot</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: right;">24.80</td>
</tr>
<tr>
<td style="text-align: left;">Manual-CoT (Wei et al., 2022b)</td>
<td style="text-align: left;">few-shot</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: right;">35.80</td>
</tr>
<tr>
<td style="text-align: left;">Auto-CoT (Zhang et al., 2022)</td>
<td style="text-align: left;">few-shot</td>
<td style="text-align: right;">175 B</td>
<td style="text-align: right;">36.50</td>
</tr>
<tr>
<td style="text-align: left;">Calcformer-T5-L (Kadlčík et al., 2023)</td>
<td style="text-align: left;">train-set</td>
<td style="text-align: right;">770 M</td>
<td style="text-align: right;">27.20</td>
</tr>
<tr>
<td style="text-align: left;">FLAN-Alpaca ${ }_{\text {base }}$</td>
<td style="text-align: left;">train-set</td>
<td style="text-align: right;">223 M</td>
<td style="text-align: right;">$30.09 \pm 1.12$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{G o T - T} \mathbf{T} \mathbf{S}_{\text {base }}$</td>
<td style="text-align: left;">train-set</td>
<td style="text-align: right;">223 M</td>
<td style="text-align: right;">$\mathbf{3 2 . 0 9} \pm 1.62$</td>
</tr>
<tr>
<td style="text-align: left;">FLAN-Alpaca ${ }_{\text {large }}$</td>
<td style="text-align: left;">train-set</td>
<td style="text-align: right;">738 M</td>
<td style="text-align: right;">$33.73 \pm 1.14$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{G o T - T} \mathbf{T} \mathbf{S}_{\text {large }}$</td>
<td style="text-align: left;">train-set</td>
<td style="text-align: right;">738 M</td>
<td style="text-align: right;">$\mathbf{3 4 . 4 8} \pm 1.11$</td>
</tr>
</tbody>
</table>
<p>Table 1: Main test accuracy results (ACC\%) of AQUARAT. Size=backbone model size.</p>
<p>Results The rationales generation results can be seen in Table 8 in Appendix D. The overall results are reported in Table 1 and Table 2.</p>
<p>In the AQUA-RAT dataset, our GoT ${ }<em _base="{base" _text="\text">{\text {base }}$ model attains a 0.78 enhancement in ROUGE-L scores for rationale generation during the initial stage, outperforming the FLAN-Alpaca ${ }</em>}}$ model, which does not integrate GoT. For the answer generation phase, the $\mathrm{GoT<em _large="{large" _text="\text">{\text {base }}$ exhibits a substantial accuracy increase of $2.00 \%$, while the $\mathrm{GoT}</em>$ model records a $0.75 \%$ enhancement. Compared to the 175B parameter zero-shot and few-shot LLMs, our GoT-large, employing just a 738 M backbone model, achieves results remarkably close to those of Manual-CoT (Wei et al., 2022b).}</p>
<p>For ScienceQA dataset, in rationale generation stage, we can see from Table 8 that our model achieves a ROUGE-L of 94.39 and outperforms the Mutimodal-CoT ${ }_{\text {base }}$ by 1.15. For the final an-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">MODEL</th>
<th style="text-align: center;">TRAINING</th>
<th style="text-align: center;">SIZE</th>
<th style="text-align: center;">NAT</th>
<th style="text-align: center;">SOC</th>
<th style="text-align: center;">LAN</th>
<th style="text-align: center;">TXT</th>
<th style="text-align: center;">IMG</th>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">G1-6</th>
<th style="text-align: center;">G7-12</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">90.23</td>
<td style="text-align: center;">84.97</td>
<td style="text-align: center;">87.48</td>
<td style="text-align: center;">89.60</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">88.10</td>
<td style="text-align: center;">91.59</td>
<td style="text-align: center;">82.42</td>
<td style="text-align: center;">88.40</td>
</tr>
<tr>
<td style="text-align: center;">Vision question answering baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MCAN (Yu et al., 2019)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">95M</td>
<td style="text-align: center;">56.08</td>
<td style="text-align: center;">46.23</td>
<td style="text-align: center;">58.09</td>
<td style="text-align: center;">59.43</td>
<td style="text-align: center;">51.17</td>
<td style="text-align: center;">55.40</td>
<td style="text-align: center;">51.65</td>
<td style="text-align: center;">59.72</td>
<td style="text-align: center;">54.54</td>
</tr>
<tr>
<td style="text-align: center;">Top-Down (Anderson et al., 2018)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">70M</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">54.33</td>
<td style="text-align: center;">61.82</td>
<td style="text-align: center;">62.90</td>
<td style="text-align: center;">54.88</td>
<td style="text-align: center;">59.79</td>
<td style="text-align: center;">57.27</td>
<td style="text-align: center;">62.16</td>
<td style="text-align: center;">59.02</td>
</tr>
<tr>
<td style="text-align: center;">BAN (Kim et al., 2018)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">112M</td>
<td style="text-align: center;">60.88</td>
<td style="text-align: center;">46.57</td>
<td style="text-align: center;">66.64</td>
<td style="text-align: center;">62.61</td>
<td style="text-align: center;">52.60</td>
<td style="text-align: center;">65.51</td>
<td style="text-align: center;">56.83</td>
<td style="text-align: center;">63.94</td>
<td style="text-align: center;">59.37</td>
</tr>
<tr>
<td style="text-align: center;">DFAF (Gao et al., 2019)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">74M</td>
<td style="text-align: center;">64.03</td>
<td style="text-align: center;">48.82</td>
<td style="text-align: center;">63.55</td>
<td style="text-align: center;">65.88</td>
<td style="text-align: center;">54.49</td>
<td style="text-align: center;">64.11</td>
<td style="text-align: center;">57.12</td>
<td style="text-align: center;">67.17</td>
<td style="text-align: center;">60.72</td>
</tr>
<tr>
<td style="text-align: center;">ViLT (Kim et al., 2021)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">113M</td>
<td style="text-align: center;">60.48</td>
<td style="text-align: center;">63.89</td>
<td style="text-align: center;">60.27</td>
<td style="text-align: center;">63.20</td>
<td style="text-align: center;">61.38</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">60.72</td>
<td style="text-align: center;">61.90</td>
<td style="text-align: center;">61.14</td>
</tr>
<tr>
<td style="text-align: center;">Patch-TRM (Lu et al., 2021)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">90M</td>
<td style="text-align: center;">65.19</td>
<td style="text-align: center;">46.79</td>
<td style="text-align: center;">65.55</td>
<td style="text-align: center;">66.96</td>
<td style="text-align: center;">55.28</td>
<td style="text-align: center;">64.95</td>
<td style="text-align: center;">58.04</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">61.42</td>
</tr>
<tr>
<td style="text-align: center;">VisualBERT (Li et al., 2019, 2020)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">111M</td>
<td style="text-align: center;">59.33</td>
<td style="text-align: center;">69.18</td>
<td style="text-align: center;">61.18</td>
<td style="text-align: center;">62.71</td>
<td style="text-align: center;">62.17</td>
<td style="text-align: center;">58.54</td>
<td style="text-align: center;">62.96</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">61.87</td>
</tr>
<tr>
<td style="text-align: center;">Test-to-text LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA ${ }_{\text {base }}$ (Raffel et al., 2020b)</td>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">223M</td>
<td style="text-align: center;">68.16</td>
<td style="text-align: center;">69.18</td>
<td style="text-align: center;">74.91</td>
<td style="text-align: center;">63.78</td>
<td style="text-align: center;">61.38</td>
<td style="text-align: center;">77.84</td>
<td style="text-align: center;">72.98</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">70.12</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 (Chen et al., 2020)</td>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">74.64</td>
<td style="text-align: center;">69.74</td>
<td style="text-align: center;">76.00</td>
<td style="text-align: center;">74.44</td>
<td style="text-align: center;">67.28</td>
<td style="text-align: center;">77.42</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">68.89</td>
<td style="text-align: center;">73.97</td>
</tr>
<tr>
<td style="text-align: center;">Test-to-text LLMs with CoT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA ${ }_{\text {base }}$ (CoT) (Lu et al., 2022)</td>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">223M</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">76.04</td>
<td style="text-align: center;">78.91</td>
<td style="text-align: center;">66.42</td>
<td style="text-align: center;">66.53</td>
<td style="text-align: center;">81.81</td>
<td style="text-align: center;">77.06</td>
<td style="text-align: center;">68.82</td>
<td style="text-align: center;">74.11</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 (CoT) (Lu et al., 2022)</td>
<td style="text-align: center;">2-shot</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">75.44</td>
<td style="text-align: center;">70.87</td>
<td style="text-align: center;">78.09</td>
<td style="text-align: center;">74.68</td>
<td style="text-align: center;">67.43</td>
<td style="text-align: center;">79.93</td>
<td style="text-align: center;">78.23</td>
<td style="text-align: center;">69.68</td>
<td style="text-align: center;">75.17</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (CoT) (Lu et al., 2023)</td>
<td style="text-align: center;">few-shot</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.82</td>
<td style="text-align: center;">70.98</td>
<td style="text-align: center;">83.18</td>
<td style="text-align: center;">77.37</td>
<td style="text-align: center;">67.92</td>
<td style="text-align: center;">86.13</td>
<td style="text-align: center;">80.72</td>
<td style="text-align: center;">74.03</td>
<td style="text-align: center;">78.31</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (CoT) (Lu et al., 2023)</td>
<td style="text-align: center;">few-shot</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.48</td>
<td style="text-align: center;">72.44</td>
<td style="text-align: center;">90.27</td>
<td style="text-align: center;">82.65</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">92.89</td>
<td style="text-align: center;">86.66</td>
<td style="text-align: center;">79.04</td>
<td style="text-align: center;">83.99</td>
</tr>
<tr>
<td style="text-align: center;">Mutimodal-CoT ${ }_{\text {base }}$ (Zhang et al., 2023)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">223M</td>
<td style="text-align: center;">84.37</td>
<td style="text-align: center;">88.30</td>
<td style="text-align: center;">84.36</td>
<td style="text-align: center;">83.72</td>
<td style="text-align: center;">80.32</td>
<td style="text-align: center;">86.90</td>
<td style="text-align: center;">85.83</td>
<td style="text-align: center;">84.05</td>
<td style="text-align: center;">85.19</td>
</tr>
<tr>
<td style="text-align: center;">GoT-TS ${ }_{\text {base }}$</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">223M</td>
<td style="text-align: center;">$\begin{aligned} &amp; 86.25 \ &amp; \pm 0.31 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 93.55 \ &amp; \pm 0.06 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 85.51 \ &amp; \pm 0.11 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 85.89 \ &amp; \pm 0.32 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 86.30 \ &amp; \pm 0.28 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 86.34 \ &amp; \pm 0.12 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 87.79 \ &amp; \pm 0.10 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 87.23 \ &amp; \pm 0.40 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 87.59 \ &amp; \pm 0.20 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Mutimodal-CoT ${ }_{\text {large }}$ (Zhang et al., 2023)</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">738M</td>
<td style="text-align: center;">91.03</td>
<td style="text-align: center;">93.70</td>
<td style="text-align: center;">86.64</td>
<td style="text-align: center;">90.13</td>
<td style="text-align: center;">88.25</td>
<td style="text-align: center;">89.48</td>
<td style="text-align: center;">91.12</td>
<td style="text-align: center;">89.26</td>
<td style="text-align: center;">90.45</td>
</tr>
<tr>
<td style="text-align: center;">GoT-TS ${ }_{\text {large }}$</td>
<td style="text-align: center;">train-set</td>
<td style="text-align: center;">738M</td>
<td style="text-align: center;">$\begin{aligned} &amp; 90.88 \ &amp; \pm 0.22 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 93.57 \ &amp; \pm 0.38 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 88.45 \ &amp; \pm 0.44 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 90.26 \ &amp; \pm 0.35 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 88.16 \ &amp; \pm 0.25 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 90.29 \ &amp; \pm 0.47 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 91.19 \ &amp; \pm 0.16 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 90.14 \ &amp; \pm 0.23 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 90.81 \ &amp; \pm 0.12 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Main test accuracy results (\%) of ScienceQA. SIZE=backbone model size. Question classes: NAT = natural science, $\mathrm{SOC}=$ social science, $\mathrm{LAN}=$ language science, $\mathrm{TXT}=$ text context, $\mathrm{IMG}=$ image context, $\mathrm{NO}=$ no context, G1-6 = grades 1-6, G7-12 = grades 7-12, AVG= average accuracy scores
swer generation stage, our GoT achieves SOTA in all subjects and all grades. The most direct comparison is that our model achieves an accuracy of $87.59 \%$ which is $2.40 \%$ higher than that of the Mutimodal-CoT ${ }_{\text {base }}$ with the similar number of parameters.</p>
<p>GoT demonstrates a significant advantage over traditional CoT, elevating the accuracy from $30.09 \%$ to $32.09 \%$ in AQUA-RAT and from $85.19 \%$ to $87.59 \%$ in ScienceQA task. The results sufficiently suggest that utilizing thought graph features for deductive reasoning is a more effective approach than the existing methods, which only consider text or vision features by simply incorporating image captions or fusing generated image features. In conclusion, our results confirm the effectiveness of utilizing two-dimensional graph-of-thought and demonstrate the potential of incorporating GoT into reasoning for LMs.</p>
<h3>4.2 Further Exploration</h3>
<h3>4.2.1 Ablation Study</h3>
<p>AQUA-RAT In order to make sure that introducing thought graphs into GoT reasoning indeed boost the performance, we conduct the following experiments:</p>
<h2>(1) Random Thought Graph</h2>
<p>In the Random Thought Graph experiment, we maintain the GoT framework while introducing ran-
domness into the process. We construct a thought graph by randomly selecting nodes and arbitrarily establishing connections between them. This approach is designed to evaluate the extent to which the GoT reasoning mechanism is reliant on the structured organization of thought graphs. (2) Triplets Concatenation In the Triplets Concatenation experiment, we take a straightforward approach by appending the extracted triplets directly to the input text. This method aims to assess the impact of omitting the structural information typically provided by thought graphs, offering insight into the significance of this structural element in the reasoning process. (3) Coreference Injection</p>
<p>In the Coreference Injection experiment, we explore the potential benefits of integrating coreference resolution directly into the language model's reasoning process. We achieve this by incorporating coreference information into the input text, where all instances of coreferent entities are replaced with a consistent phrase, followed by model fine-tuning. This experiment seeks to understand the role of coreference resolution in enhancing the model's deductive capabilities.</p>
<p>Table 3 shows the overall ablation results. From the table, we can see that by randomly construct thought graphs to disrupt the deductive reasoning process, our model suffers a loss of $1.78 \%$, indicating the effectiveness of GoT. The results of Triplets</p>
<table>
<thead>
<tr>
<th>MODEL</th>
<th>MODEL SIZE</th>
<th>ACC</th>
<th>$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GoT-T5base</td>
<td>233M</td>
<td>32.09</td>
<td>-</td>
</tr>
<tr>
<td>w/ Random Thought Graph</td>
<td></td>
<td>30.31</td>
<td>-1.78</td>
</tr>
<tr>
<td>Triplets Concatenation</td>
<td>233M</td>
<td>31.20</td>
<td>-0.89</td>
</tr>
<tr>
<td>Coreference Injection</td>
<td>233M</td>
<td>30.32</td>
<td>-1.77</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation results of GoT on AQUA-RAT dataset.</p>
<p>Concatenation on the AQUA-RAT showed an accuracy of $31.20 \%$. This performance gap of 0.89 clearly demonstrates the significance of the structural information in our approach. For Coreference Injection, the model suffers a loss of $1.77 \%$ accuracy. We believe that these outcomes can be attributed to a couple of factors: (1) Simply replacing coreferent entities may lead to a loss of coherence in sentences, resulting in a reduction of semantic information and consequently having a limited impact on overall accuracy. (2) Open Information Extraction (OpenIE) for coreference resolution is not flawless, and direct replacement of entities might introduce noise that misleads the language model during judgment.</p>
<p>Contrastingly, the construction of a thought graph in the GoT framework does not compromise the original textual information (questions and rationales). Instead, it introduces additional structural assistance for LMs to conduct reasoning effectively. Thus, we contend that GoT's approach is indispensable and beneficial, as it supplements the LM's comprehension without introducing potential noise or loss of coherence in the input text.</p>
<p>ScienceQA To examine the impact of different backbone and vision encoder configurations on the GoT, we employed a distinct set of model settings. More specifically, we adopted the pre-trained T5 checkpoint UnifiedQA (Khashabi et al., 2020) as the backbone model and utilized DETR (Carion et al., 2020) for the vision encoder, with results illustrated in the Table 4. As shown, our GoT outperforms Mutimodal-CoT across various model configurations. A comparison reveals that GoT can achieve greater improvements on smaller models. We believe the main reason is that when the language model is not as robust, or when employing a relatively weaker vision encoder like DETR compared to ViT, GoT can leverage the inherent information within the language to enhance performance significantly. Additionally, to prove that our GoT's performance gain is not simply due to
an increase in parameters, we conducted an ablation study. We expanded the parameter count of Multimodal-CoTbase to match our 233M model size by adding two layers of MLP instead of one in the gated fusion module, referred to as MultimodalCoTbase(enlarged). We also constructed a random thought graph ablation study on the ScienceQA dataset. The results from the ablation studies can be observed in the table 4. From the table, it is evident that our model significantly outperforms the enlarged Multimodal-CoT by an accuracy of $2.04 \%$. These findings convincingly demonstrate the significance of incorporating thought graphs into multimodal reasoning. The performance of GoT with a randomly constructed thought graph was even lower than Mutimodal-CoT, indicating that when the language model and vision encoder are weaker, the model relies more heavily on GoT for reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">ACC</th>
<th style="text-align: left;">$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UnifiedQA+DETR</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Mutimodal-CoTbase</td>
<td style="text-align: left;">77.67</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Mutimodal-CoTlarge</td>
<td style="text-align: left;">81.37</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GoTbase</td>
<td style="text-align: left;">$\mathbf{8 1 . 2 1}$</td>
<td style="text-align: left;">3.54</td>
</tr>
<tr>
<td style="text-align: left;">GoTlarge</td>
<td style="text-align: left;">$\mathbf{8 2 . 7 4}$</td>
<td style="text-align: left;">1.37</td>
</tr>
<tr>
<td style="text-align: left;">Ablation Studies</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Mutimodal-CoTbase(enlarged)</td>
<td style="text-align: left;">79.17</td>
<td style="text-align: left;">-2.04</td>
</tr>
<tr>
<td style="text-align: left;">GoTbase w/ Random Thought Graph</td>
<td style="text-align: left;">76.74</td>
<td style="text-align: left;">-4.47</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation results of GoT on ScienceQA dataset. For GoT models $\Delta$ indicates the performance gains of GoT models over their Multimodal-CoT counterparts. In the ablation studies, $\Delta$ represents improvements relative to the GoTbase model</p>
<h3>4.2.2 Analysis</h3>
<p>Performance on Different Classes In order to investigate the impact of GoT on the overall model performance across different subjects, we calculated the accuracy for different subjects and compared it with that of Mutimodal-CoT. We also compare the performance of two models on different question classes. The radar Figure 5 shows the overall results for our base model. With respect to various subjects and question classes, our model demonstrates superior performance over the Mutimodal-CoTbase and attains a more consistent and enhanced outcome. Our model presents outstanding advantages especially in the field of social science, with an accuracy improvement of $5.25 \%$. For different question classes, our model demonstrates the largest improvement on questions involv-</p>
<p>ing images. Our hypothesis is that by constructing a thought graph and integrating the three features of text, image, and thought graph, we can better align the textual and visual information for the model, thus maximizing the utilization of visual information and obtaining more accurate answers.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance on different question classes
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance on different grades</p>
<p>Performance on Different Grades It can be seen from the Table 2 that Mutimodal-CoT experience a decrease in accuracy of 1.78 as the grade level of the given question increases while GoT only has minor decrease of 0.56 . We believe the main reason is that by incorporating GoT, models acquires the ability for deductive reasoning and can better comprehend the relationships between different entities and thus better understand the meaning
of the problems. Through this method, for highergrade problems with greater complexity, the model can construct a thought graph to help itself generate a more complete logical chain for deduction, thereby generating more accurate answers. More detailed model performance on different grades can be found in Figure 6. We can see that in the lower grade, two models achieves a similar performance. As the grade level increases and the difficulty of the questions becomes more challenging, the gap between our model and the Mutimodal-CoT model gradually widens. Due to the small number of questions $(\leq 130)$ available for each grade in grade 1 and grades 11-12, there is greater fluctuation in the accuracy of both models. Nevertheless, it is evident from the table that our model exhibits stronger and more stable advantages over Mutimodal-CoT in each grade.</p>
<p>Case Study and Limitation In order to gain a deeper understanding of the performance of GoT, we conduct case studies which can be found in the Appendix E. We also visualize the attention weights $a_{i j}$ in GoT encoder to demonstrate how GoT performs deductive reasoning to generate more accurate answers in Appendix F. For the limitation of this work, compared to CoT, GoT may result in additional computational costs and slightly slower training times. Detailed limitation analysis can be found in Appendix G.</p>
<h2>5 Conclusion</h2>
<p>We introduce a novel Graph-of-Thought (GoT) reasoning approach, which is an innovative method for modeling the non-sequential nature of human thinking for LMs. GoT enhances LMs with deductive reasoning abilities, providing a more realistic representation of thought processes. Our experiments showcases the superiority of GoT on the text-only reasoning dataset AQUA-RAT, achieving a similar result compared to GPT-3 model while utilizing significantly fewer parameters. Furthermore, GoT establishes a new state-of-the-art on the multimodal reasoning benchmark, ScienceQA with fewer parameters. This performance surpasses strong ChatGPT and GPT-4 systems, as well as human performance, demonstrating the efficacy of GoT. Through comprehensive case studies and ablation studies, we provide substantial evidence of the effectiveness of GoT in reasoning tasks. If you want it, you GoT it!</p>
<h2>References</h2>
<p>Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6077-6086. Computer Vision Foundation / IEEE Computer Society.</p>
<p>Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, Beijing, China. Association for Computational Linguistics.</p>
<p>Lawrence W Barsalou. 1999. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577-660.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In Computer Vision - ECCV 2020 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 213-229. Springer.</p>
<p>Jiaao Chen and Diyi Yang. 2021. Structure-aware abstractive conversation summarization via discourse and action graphs. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 1380-1391. Association for Computational Linguistics.</p>
<p>Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. Big self-supervised models are strong semi-supervised learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven C. H. Hoi, Xiaogang Wang, and Hongsheng Li. 2019. Dynamic fusion with intra- and intermodality attention flow for visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6639-6648. Computer Vision Foundation / IEEE.</p>
<p>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023. Language is not all you need: Aligning perception with language models. CoRR, abs/2302.14045.</p>
<p>Marek Kadlčík, Michal Štefánik, Ondrej Sotolar, and Vlastimil Martinek. 2023. Calc-X and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12101-12108, Singapore. Association for Computational Linguistics.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single QA system. In Findings of the</p>
<p>Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 1896-1907. Association for Computational Linguistics.</p>
<p>Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018. Bilinear attention networks. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 1571-1581.</p>
<p>Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 5583-5594. PMLR.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. CoRR, abs/2205.11916.</p>
<p>Bei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma, and JingBo Zhu. 2022. On vision features in multimodal machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6327-6337, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. CoRR, abs/1908.03557.</p>
<p>Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2020. What does BERT with vision look at? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 5265-5275. Association for Computational Linguistics.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS).</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. CoRR, abs/2304.09842.</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.</p>
<p>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55-60, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding multimodal large language models to the world. CoRR, abs/2306.14824.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020a. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020b. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022. Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022a. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.</p>
<p>Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and Ben Kao. 2021. Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6153-6166, Online. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019. Deep modular co-attention networks for visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 62816290. Computer Vision Foundation / IEEE.</p>
<p>Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao. 2020. Neural machine translation with universal visual representation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Zhuosheng Zhang and Aston Zhang. 2023. You only look at screens: Multimodal chain-of-action agents. CoRR, abs/2309.11436.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. CoRR, abs/2210.03493.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-of-thought reasoning in language models. CoRR, abs/2302.00923.</p>
<h2>Appendix</h2>
<h2>A Related Works</h2>
<p>In chain-of-thought reasoning, one idea leads to the next in a logical sequence and builds on previous knowledge. Each idea is supported by evidence or reasoning, and the conclusions drawn from the chain are logical and sound. Most CoT methods can be divided into two categories based on how to generate the final answer: (1) prompting for CoT, including zero-shot CoT and few-shot CoT; and (2) fine-tuning for CoT.</p>
<p>Zero-shot CoT Prompting As large language models continue to advance rapidly, many researchers are beginning to explore CoT reasoning for LLMs. The zero-shot CoT method proposed by Kojima et al. (2022) consists of two stages: (1) adding a "Let's think step by step" prompt to generate CoT, and (2) concatenating the generated CoT and adding the phrase "So the answer is" to obtain the final answer. Tree-of-Thought (ToT) (Yao et al., 2023) enables deliberate decision-making through exploration of coherent text units. ToT divides thoughts into thought units and models them as a tree-like search process. Although both GoT and ToT aim to capture human non-linear thoughts, GoT is distinct from ToT in terms of both methodology and objectives. We believe that human thinking involves both linear and non-linear aspects. Thus, we build upon the linear CoT framework by incorporating non-linear structures to simultaneously capture both linear and non-linear human reasoning. Tree-of-thoughts focuses on modeling nonlinear thoughts explicitly, whereas our approach leverages non-linear structures to assist the Chain-of-Thought reasoning.</p>
<p>Few-shot CoT Prompting Few-shot CoT reasoning for LLMs, however, utilizes multiple input-output pairs to prompt the LLMs to output CoT and obtain the final answer. Due to its ability to provide better performance compared to Zero-shot CoT, Few-shot CoT has gained more attention in research, particularly through effective demonstrations. Few-shot CoT prompting was first formally explored by Wei et al. (2022a) and is a form of discrete prompt learning that involves context learning in large models. Compared to traditional in-context learning, which prompts LLMs with a list of inputoutput demonstration pairs along with a test input to allow the model to predict output, Few-shot CoT prompting outputs additional logical reasoning procedures apart from the target output. Wang et al. (2022) proposed a follow-up method to (Wei et al., 2022a). The main improvement is that the model uses the majority vote for the answers, which was found to significantly improve the performance of the CoT. However, these few-shot CoT models depend on hand-crafted demonstrations. To solve this problem, Zhang et al. (2022) proposed Auto-CoT, which maintains the diversity of sampled questions and generates reasoning chains to automatically construct demonstrations. Specifically, Auto-CoT consists of two main stages: (1) Problem clustering: divide the given dataset of problems into several clusters; (2) Demonstration sampling: select a representative problem from each cluster and use a simple heuristic method to generate its reasoning chain. Furthermore, Lu et al. (2023) also explores few-shot CoT reasoning for recently popular LLMs ChatGPT and GPT-4.</p>
<p>CoT Fine-tuning In Zhang et al. (2023), it was proposed to fine-tune smaller language models instead of prompting them in LLMs. And this approach enabled the CoT to go beyond textual information and incorporate visual (image) modalities using a gated fusion mechanism into a two-stage CoT. The results demonstrated that CoT fine-tuning with fewer parameters has potential. Therefore, in this work, we focus on fine-tuning for CoT to reduce the number of required model parameters and help LLMs better comprehend different modalities. However, previous CoT research has been limited to different modalities, such as textual and vision information, without considering the deduction reasoning process. Therefore, in this work, we move beyond modeling the reasoning process solely as a thought chain and elevate it to a thought graph. We provide a more comprehensive and nuanced representation, enabling LLMs to perceive the deduction reasoning process accurately, resulting in more precise answer generation.</p>
<h2>B Dataset</h2>
<p>AQUA-RAT dataset consists of about 100,000 algebraic word problems with natural language rationales. For AQUA-RAT, the model is trained to reasoning through the steps to generate the final answer. ScienceQA benchmark is the pioneering large-scale dataset for multimodal science questions, equipped with comprehensive annotations for answers, including detailed lectures and explana-</p>
<table>
<thead>
<tr>
<th>Splits</th>
<th>#Problems</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>97467</td>
</tr>
<tr>
<td>Dev</td>
<td>254</td>
</tr>
<tr>
<td>Test</td>
<td>254</td>
</tr>
</tbody>
</table>
<p>Table 5: AQUA-RAT dataset statistics (# denotes numbers)</p>
<table>
<thead>
<tr>
<th>Statistic</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Splits</td>
<td></td>
</tr>
<tr>
<td>#Train</td>
<td>12,726</td>
</tr>
<tr>
<td>#Dev</td>
<td>4,241</td>
</tr>
<tr>
<td>#Test</td>
<td>4,241</td>
</tr>
<tr>
<td>#Total</td>
<td>21,208</td>
</tr>
<tr>
<td>Attribute</td>
<td></td>
</tr>
<tr>
<td>#Subjects</td>
<td>3</td>
</tr>
<tr>
<td>#Topic</td>
<td>26</td>
</tr>
<tr>
<td>#Category</td>
<td>127</td>
</tr>
<tr>
<td>#Skill</td>
<td>379</td>
</tr>
</tbody>
</table>
<p>Table 6: ScienceQA dataset statistics (# denotes numbers)</p>
<h2>C Training Parameters</h2>
<table>
<thead>
<tr>
<th>Parameters</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Epochs</td>
<td>100</td>
</tr>
<tr>
<td>Batch size for T5-base (per device)</td>
<td>10</td>
</tr>
<tr>
<td>Batch size for T5-large (per device)</td>
<td>8</td>
</tr>
<tr>
<td>Learning rate</td>
<td>$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td>Weight decay</td>
<td>0.01</td>
</tr>
<tr>
<td>Max input length</td>
<td>512</td>
</tr>
<tr>
<td>Max number of nodes</td>
<td>150</td>
</tr>
</tbody>
</table>
<p>Table 7: Training parameters for GoT</p>
<h2>D Rationale Generation Results</h2>
<p>The rationale genration results can be found in Table 8. We can observe from Table 8 that the impact of GoT on rationale generation is limited. We attribute this limitation to the fact that the input text for thought graph construction only includes
questions and choices. Consequently, the thought graph constructed from such limited information can only facilitate constrained deductive reasoning. However, in the answer generation stage, when provided with rationales, the model needs to possess stronger deductive reasoning capabilities to understand the relationship between rationales, questions, and choices.</p>
<h2>E Case Study</h2>
<p>To facilitate a more illustrative comparison between GoT and the CoT, we have selected several representative examples. Figure 7 illustrates the examples from AQUA-RAT dataset. Figure 8 to Figure 11 illustrates examples from ScienceQA dataset. From Figure 8 and Figure 9, we can see that GoT can better understand the rationales and generate more accurate result. In Figure 10, we can see that when provided with wrong rationale, our model is more robust to the noise and can focus on more important key information. (We highlight the noisy wrong rationale in red and correct key rationale in green). Figure 11 presents a language problem which have less context and requires a certain amount of common sense knowledge. Hence, the impact of constructing a mind map on enhancing the model is not significant. Therefore, both GoT and CoT predict wrong answers.</p>
<h2>F Representation Visualization</h2>
<p>In order to demonstrate the deductive reasoning process of GoT more intuitively, we visualized the attention weights of the GoT encoder. The visualization results can be found in Figure 12. We took Figure 10 as an example. In Figure 10, even given a wrong rationale, GoT still manages to generate the right answer. We select 14 representative thought nodes and found that "blue","color", and "common" have the greatest weights which indicates that GoT guides the model to focus on more important words and conduct correct deductive reasoning. For the disruptive node "a hard object," our model can effectively discriminate against it and assign a lower attention weight to prevent the model from selecting incorrect answers, as traditional CoT models often do due to erroneous rationales.</p>
<h2>G Limitation</h2>
<p>Compared to Mutimodal-CoT (Zhang et al., 2023), incorporating GoT may result in additional computational costs and slightly slower training times.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODELS</th>
<th style="text-align: center;">BLEU1</th>
<th style="text-align: center;">BLEU4</th>
<th style="text-align: center;">ROUGE</th>
<th style="text-align: center;">SIMILARITY</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AQUA-RAT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FLAN-Alpaca</td>
<td style="text-align: center;">19.78</td>
<td style="text-align: center;">3.49</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">68.61</td>
</tr>
<tr>
<td style="text-align: left;">FLAN-Alpaca</td>
<td style="text-align: center;">22.45</td>
<td style="text-align: center;">5.40</td>
<td style="text-align: center;">29.55</td>
<td style="text-align: center;">70.34</td>
</tr>
<tr>
<td style="text-align: left;">GoT-T5 $\mathbf{5}_{\text {base }}$</td>
<td style="text-align: center;">22.05</td>
<td style="text-align: center;">5.02</td>
<td style="text-align: center;">29.18</td>
<td style="text-align: center;">69.09</td>
</tr>
<tr>
<td style="text-align: left;">GoT-T5 $\mathbf{5}_{\text {large }}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 4 7}$</td>
<td style="text-align: center;">$\mathbf{6 . 6 8}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 5 8}$</td>
</tr>
<tr>
<td style="text-align: left;">ScienceQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Mutimodal-CoT</td>
<td style="text-align: center;">91.04</td>
<td style="text-align: center;">86.81</td>
<td style="text-align: center;">93.24</td>
<td style="text-align: center;">96.34</td>
</tr>
<tr>
<td style="text-align: left;">GoT-T5 $\mathbf{5}_{\text {base }}$</td>
<td style="text-align: center;">92.50</td>
<td style="text-align: center;">88.79</td>
<td style="text-align: center;">94.39</td>
<td style="text-align: center;">96.74</td>
</tr>
<tr>
<td style="text-align: left;">GoT-T5 $\mathbf{5}_{\text {large }}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 4 9}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 0 9}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 1 7}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 3 3}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Rationale generation results (\%). (*: we re-run the Mutimodal-CoT ${ }_{\text {base }}$ to report the full rationale scores. We use sentence-transformers (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to obtain sentence embeddings and calculate the cosine similarity for SIMILARITY)</p>
<p>The training parameters and inference times of the different models are presented in Table 9, which reveals that our model requires a $0.2 \%$ increase in parameters compared to Mutimodal-CoT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">#Parameters</th>
<th style="text-align: center;">Inference time <br> (eval samples/per second)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mutimodal-CoT base</td>
<td style="text-align: center;">227 M</td>
<td style="text-align: center;">16.33</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">233 M</td>
<td style="text-align: center;">13.38</td>
</tr>
</tbody>
</table>
<p>Table 9: The number of training parameters and inference time of different models (# denotes numbers)</p>
<h1>Dataset</h1>
<p>Question: paper is in a square form whose one side is 20 cm . Two semi circles are drawn on its opposites as diameters. If these semi circles are cut down what is the area of the remaining paper
Choices:
(A) 8.73
(B) 8.79
(E) 8.72
(C) 8.75
(D) 8.71</p>
<p>Rationale: $(5+3.5) / 2=8.75$
Answer: C</p>
<h2>GoT Prediction</h2>
<p>Rationale: Explanation: Area of the paper $=(r 2+r 2)=(20 \times 20) \mathrm{cm} 2=1200 \mathrm{~cm} 2$. Area of two semi circles cut down $=[(20-r 2) 2-(20 \times 20) 2] \mathrm{cm} 2=(1200-225) \mathrm{cm} 2=875 \mathrm{~cm} 2$.
Answer: C
wrong rationales right answer</p>
<h2>GoT Prediction</h2>
<p>Rationale: Area of the square $=\mathrm{pi} \cdot \mathrm{r} 2 / 4=(20 \cdot 20) /(2 \cdot 2)=8.73$
Answer: A
wrong rationales wrong answer</p>
<p>Figure 7: Examples of AQUA-RAT</p>
<h1>Dataset</h1>
<p>Question: Would you find the word pink on a dictionary page with the following guide words? parrot - property</p>
<h2>Choices: (A) yes</h2>
<p>(B) no</p>
<p>Rationale: lecture: Guide words appear on each page of a dictionary. They tell you the first word and last word on the page. The other words on the page come between the guide words in alphabetical order. To put words in alphabetical order, put them in order by their first letters. If the first letters are the same, look at the second letters. If the second letters are the same, look at the third letters, and so on. If one word is shorter, and there are no more letters to compare, then the shorter word comes first in alphabetical order. For example, be comes before bed.
solution: Put the words in alphabetical order. Since pink is between the guide words parrot property, it would be found on that page.
Answer: The answer is (A)</p>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: Guide words appear on each page of a dictionary. They tell you the first word and last word on the page. The other words on the page come between the guide words in alphabetical order. To put words in alphabetical order, put them in order by their first letters. If the first letters are the same, look at the second letters. If the second letters are the same, look at the third letters, and so on. If one word is shorter, and there are no more letters to compare, then the shorter word comes first in alphabetical order. For example, be comes before bed. Put the words in alphabetical order. Since pink is between the guide words parrot - property, it would be found on that page.
Answer: The answer is (A)
right rationales right answer</p>
<h2>CoT Prediction</h2>
<p>Rationale: Solution: Guide words appear on each page of a dictionary. They tell you the first word and last word on the page. The other words on the page come between the guide words in alphabetical order. To put words in alphabetical order, put them in order by their first letters. If the first letters are the same, look at the second letters. If the second letters are the same, look at the third letters, and so on. If one word is shorter, and there are no more letters to compare, then the shorter word comes first in alphabetical order. For example, be comes before bed. Put the words in alphabetical order. Since pink is between the guide words parrot - property, it would be found on that page.
Answer: The answer is (B)
right rationales wrong answer</p>
<h1>Dataset</h1>
<p>Question: What is the name of the colony shown?
Choices: (A) New Hampshire (B) Vermont
(C) New York
(D) Delaware</p>
<p>Rationale: The colony is New York. During the colonial era, New Hampshire and New York both claimed the territory that would later become the state of Vermont. Vermont was never its own colony.
Answer: The answer is (C)
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: The colony is New York. During the colonial era, New Hampshire and New York both claimed the territory that would later become the state of Vermont. Vermont was never its own colony.
Answer: The answer is (C)
right rationales right answer</p>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: The colony is Delaware York. During the colonial era, New Hampshire and New York both claimed the territory that would later become the state of Vermont. Vermont was never its own colony.
Answer: The answer is (D)
wrong rationales wrong answer</p>
<p>Figure 9: Examples of ScienceQA</p>
<h1>Dataset</h1>
<p>Question: Which property do these three objects have in common?</p>
<h2>Choices: (A) blue</h2>
<p>(B) hard
(C) sticky
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Rationale: lecture: An object has different properties. A property of an object can tell you how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave when something happens to it. Different objects can have properties in common. You can use these properties to put objects into groups.
Solution: Look at each object. For each object, decide if it has that property. A sticky object can attach or stick to other things. The toothpaste is sticky, but the soccer shorts and the water slide are not. Blue is a color. This color is blue. All three objects are blue. A hard object does not change shape when pressed or squeezed. The water slide is hard, but the toothpaste and the soccer shorts are not. The property that all three objects have in common is blue.
Answer: The answer is $(A)$</p>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: An object has different properties. A property of an object can tell you how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave when something happens to it. Different objects can have properties in common. You can use these properties to put objects into groups. Look at each object. For each object, decide if it has that property. A hard object can attach or stick to other things. The is sticky, but the shorts and the pitcher are not. Blue is a color. This color is blue. The three objects are blue. A hard object does not change shape when pressed or squeezed. The tennis slide and hard, but the tennis and the water shorts are not. The property that all three objects have in common is blue.
Answer: The answer is $(A)$
wrong rationales right answer</p>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: An object has different properties. A property of an object can tell you how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave when something happens to it. Different objects can have properties in common. You can use these properties to put objects into groups. Look at each object. For each object, decide if it has that property. A sticky object can attach or stick to other things. The is sticky, but the shorts and the blue bottle are not. Blue is a color. This color is blue. None three objects are blue. A hard object does not change shape when pressed or squeezed. None tennis slide and hard, but the is the water shorts are not. The property that all three objects have in common is sticky.
Answer: The answer is $(C)$
wrong rationales wrong answer</p>
<p>Figure 10: Examples of ScienceQA</p>
<h1>Dataset</h1>
<p>Question: Select the action that doesn't belong
Choices:
(A) chop
(B) blend
(C) stir
(D) mix</p>
<p>Rationale: Chop doesn't belong. Blend, mix, and stir all describe ways to combine things
Answer: The answer is (A)</p>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: Mixp doesn't belong. Murend, chop, and chop all name things to get things.
Answer: The answer is (D)</p>
<h2>wrong rationales wrong answer</h2>
<h2>GoT Prediction</h2>
<p>Rationale: Solution: Blendp doesn't belong..Murend, chop, and blend all name things to getAnswer: The answer is (B)</p>
<p>$$
\text { wrong rationales wrong answer }
$$</p>
<p>Figure 11: Examples of ScienceQA</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 12: Representation visualization</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://huggingface.co/declare-lab/flan-alpaca-base&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
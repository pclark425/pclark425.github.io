<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1967 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1967</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1967</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-282058662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.11027v1.pdf" target="_blank">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1967.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1967.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vlaser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation vision-language model fine-tuned for embodied reasoning and end-to-end robot control that integrates a multimodal VLM backbone with an action expert trained via flow matching, and is pretrained on the Vlaser-6M embodied dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model composed of a vision-language backbone (InternVL3-derived InternViT visual encoder + Qwen2.5 LLMs) fine-tuned on embodied reasoning data, combined with an action expert for low-level control. The architecture uses an llava-like multimodal transformer where self-attention is shared between language and visual streams; robot state is encoded as a state token and (noised) actions as action tokens. For VLA finetuning the action expert predicts action chunks using a flow-matching denoising vector field and non-causal attention in the VLA stream. Grounding is performed by the VLM head to output bounding boxes and center points (normalized to [0,1000]) or textual answers depending on the QA/grounding prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternViT (InternVL3 backbone visual encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained as part of the InternVL3 family (referenced InternVL3 (Zhu et al., 2025)); the paper does not provide further public dataset sizes for InternViT in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Supervised multimodal prediction from pooled vision-language transformer representations: the model produces open-vocabulary grounding outputs (bounding boxes and center points normalized to [0,1000]) and can localize objects given text; mechanism implemented via shared self-attention between vision and language tokens and a downstream grounding head trained on point/box labels (region-level supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level (bounding boxes and center points) and multi-level (also uses global/multi-frame inputs for planning and tokenized robot state; training data include pixel masks converted to boxes and 3D scene graphs for spatial queries).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 2D grounding (normalized bounding boxes and center points) and explicit 3D spatial signals in training data (3D scene graphs, 3D bounding boxes and point-cloud-derived spatial queries used for spatial reasoning augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation, embodied visual question answering, task planning, embodied grounding, spatial reasoning, closed-loop robot control</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv (WidowX, Google Robot), ALFRED, Habitat, Where2Place, Pointarena, Paco-Lavis, Pixmo-Points, RefSpatial, and other embodied reasoning benchmarks (see paper list of 12 benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (SimplerEnv) and robot-centric egocentric views; training also includes internet-derived imagery and synthetic masks converted to boxes but primary downstream evaluation in sim/robot views.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized benchmark average (across 12 embodied reasoning benchmarks), success rate for closed-loop robot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Vlaser-2B: Avg normalized score 45.3 (vs InternVL3-2B 15.2); Vlaser-8B: Avg normalized score 51.3 (vs InternVL3-8B 22.3). Closed-loop results: paper reports state-of-the-art on Bridge and competitive performance on Google Robot but does not give a single aggregated numeric SR in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Qualitative and benchmark-level improvements: Vlaser (trained on Vlaser-6M including grounding data) yields large gains on embodied grounding and simulation evaluation relative to InternVL3 initializations (e.g., avg benchmark score increases from 15.2→45.3 for 2B and 22.3→51.3 for 8B). For closed-loop policy learning, in-domain grounding/planning QA data (Vlaser-QA) yields significantly better VLA finetuning convergence and success rates than out-of-domain pretrained reasoning alone (numeric delta not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>The paper compares different VLM initializations (InternVL3 vs Vlaser variants) and finds that upstream improvements on out-of-domain embodied reasoning benchmarks do not necessarily translate to downstream VLA control; in-domain robot-annotated data (SimplerEnv-derived) is substantially more effective at accelerating VLA convergence and improving task success. Also observed: smaller Vlaser-2B outperforms Vlaser-8B on simple point grounding tasks, while Vlaser-8B outperforms on complex multi-step planning and closed-loop simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The paper identifies a core perception/grounding bottleneck as a domain gap between internet-scale pretraining and robot viewpoint/embodiment: robots have different viewpoints and visual statistics, and lack of robot-perspective data causes degraded closed-loop control generalization. Vision observation domain shift (viewpoint and distribution mismatch) is explicitly called out as the main barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative analysis: baseline models frequently fail entire closed-loop tasks due to small positional errors; improvements arise when in-domain grounding/spatial/planning data reduce vision-domain shift. The paper reports that many failures in SimplerEnv evaluations are attributable to vision/positioning errors rather than reasoning failures, but precise failure frequencies per mode are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Mitigation via in-domain finetuning: authors generate 2M in-domain multimodal QA pairs from SimplerEnv (WidowX and Google Robot embodiments) and fine-tune VLM and VLA on these robot-perspective data; this in-domain finetuning accelerates VLA convergence and improves closed-loop success compared to only upstream embodied reasoning pretraining on internet/out-of-domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>The authors keep all parameters trainable (they fine-tune the full VLM and action expert) and do not report a frozen vs fine-tuned encoder comparison in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper reports that adding out-of-domain embodied reasoning data improves upstream reasoning benchmarks but does not guarantee downstream VLA gains; in-domain data aligned to robot observations is more valuable. Also observes model-size dependent effects: smaller model (2B) better at simple grounding; larger (8B) better for complex planning/coT tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Shared self-attention across visual and language tokens (llava-like multimodal transformer); action expert shares self-attention with LLM and uses non-causal attention in the VLA stream; grounding outputs are produced from the multimodal transformer head.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: pretraining VLM (Vlaser) and adding in-domain robot data significantly accelerate VLA policy convergence on WidowX (paper states 'significantly accelerates convergence') but exact sample counts or iterations-to-target are not enumerated in main text (they reference checkpoints used for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>1) Explicit region-level grounding supervision (points and boxes) and large curated embodied datasets (Vlaser-6M) greatly improve embodied grounding benchmarks; 2) However, improvements on upstream out-of-domain reasoning measures do not automatically translate to downstream closed-loop control — in-domain robot-perspective data is critical to close that gap; 3) Shared self-attention multimodal fusion plus region-level outputs (points/boxes) and flow-matching action prediction enable integrated perception-to-action, but primary remaining failure mode is vision-domain shift (viewpoint/positional errors).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1967.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1967.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vlaser-6M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser-6M dataset / Vlaser data engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated 6M-sample multimodal dataset designed for embodied reasoning, combining 1.5M grounding QA pairs (boxes/points), spatial reasoning samples, planning traces, RoboVQA items, and 2M in-domain SimplerEnv robot-perspective QA samples for effective VLM→VLA transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser-6M (data engine)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-part dataset used to supervise VLM pretraining: includes (a) 1.5M embodied grounding pairs (bounding boxes and center points normalized to [0,1000]) from several embodied-grounding sources plus 300k synthetic box/point samples from SA-1B segmentation masks; (b) 1.2M RoboVQA pairs and 500k spatial intelligence items; (c) 400k planning examples; (d) 2M in-domain SimplerEnv robot-perspective multimodal QA pairs from WidowX and Google Robot embodiments targeted at VLA finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (dataset artifact), supplies region-level supervision (boxes/points), pixel masks (converted), and 3D scene graph annotations used to train models with InternViT visual encoder</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A (dataset artifact); assembled from multiple sources including SA-1B segmentation masks, ScanNet/ScanNet++/ARKitScenes, RoboVQA, RefSpatial, SimplerEnv episodes, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides supervised region-level labels (bounding boxes, center points) and captions for open-vocabulary grounding; dataset enables training of grounding heads that map transformer multimodal features to normalized box/point coordinates or textual localization answers.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level (2D boxes, center points), pixel-level (masks converted to boxes/points), and scene-level/3D (scene graphs and 3D bounding boxes used for spatial questions).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit: normalized 2D bounding boxes and center points, plus 3D scene graphs and 3D bounding boxes derived from ScanNet/ARKitScenes for spatial reasoning examples.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Embodied grounding, RoboVQA, spatial reasoning, planning, VLA in-domain finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to train/evaluate on tasks detailed in the paper (Where2Place, Pointarena, Paco-Lavis, Pixmo-Points, RefSpatial, EmbodiedBench, etc.) and to create in-domain SimplerEnv samples for WidowX and Google Robot</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Mixed: internet/in-the-wild images and synthetic/segmented images (SA-1B), plus robot-perspective simulation frames from SimplerEnv (egocentric robot views)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Enables model performance gains measured as normalized average on embodied reasoning benchmarks and improved VLA finetuning convergence/success rate; dataset itself not scored.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Providing region-level supervised grounding data (1.5M + 300k from SA-1B) strongly improves embodied grounding benchmark performance and contributes to the Vlaser gains over InternVL3 on average benchmark scores (as reflected in Vlaser vs InternVL3 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The dataset authors argue that internet and third‑person datasets are insufficient to close robot-perspective domain gaps; therefore, they create 2M in-domain SimplerEnv samples specifically to reduce vision-domain shift for VLA finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Dataset ablation experiments (Vlaser-QA, Vlaser-Grounding, Vlaser-Spatial) indicate combining data types reduces failures caused by vision-domain mismatch, but paper reports only qualitative improvements and does not enumerate failure frequencies per data-source ablation in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Primary strategy: explicitly produce large-scale in-domain robot-perspective samples (2M) from SimplerEnv and fine-tune VLMs on these to reduce the pretraining → embodiment domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper argues that scale/diversity of embodied data matters, but data alignment to robot perspective is more important than merely adding more out-of-domain embodied reasoning samples; in-domain SimplerEnv data yielded the largest downstream VLA benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Dataset supports training of shared self-attention multimodal transformers and region-heads (no fusion method inside dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Authors report that adding in-domain samples accelerates downstream VLA convergence (qualitative); exact sample-efficiency numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>High-volume, explicitly robot-perspective in-domain data is critical to transfer upstream VLM reasoning to downstream VLA control; region-level supervision (points/boxes) plus spatial and planning traces together produce the best closed-loop improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1967.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1967.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Expert (flow-matching)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser Action Expert using Flow Matching for Action Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-level control module trained with flow-matching to denoise action chunks conditioned on image observations, language instructions, and robot state, enabling end-to-end action generation from multimodal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser Action Expert (flow-matching)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encodes action chunks (H-step sequences) as tokens via an MLP action encoder and trains a model v_theta to match the denoising vector field u(A_tau | A) using flow-matching. The action expert shares self-attention with the language model, accepts image observation and state tokens, uses non-causal attention in the VLA stream, and generates actions by integrating the learned vector field from tau=0→1 (sampling with Euler integration).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (controller module conditioned on visual tokens produced by InternViT)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Conditions action prediction on multimodal transformer outputs (visual tokens, language tokens, and explicit robot state token), thereby grounding language and perception into action via a learned vector-field denoising process; grounding-to-action mapping is implicit in the conditional flow-matching policy.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Action-chunk level (temporal sequence tokens) conditioned on region/global visual features and state tokens; grounding occurs at region/point/box level upstream in VLM features.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Relies on visual encoder outputs (which include region-level boxes/points and 3D-aware spatial features from training data) and explicit robot state token for spatially-aware action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Low-level robot manipulation (closed-loop control) in simulated WidowX and Google Robot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv WidowX and Google Robot tasks (flow-matching action generation evaluated in these environments)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation, robot-centric egocentric views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on closed-loop manipulation tasks (SimplerEnv); convergence speed during finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Action expert performance improves when conditioned on VLMs finetuned with in-domain grounding/planning QA (Vlaser-QA), leading to substantial gains in closed-loop success and faster convergence; exact numeric deltas for flow-matching controller with vs without grounding not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The paper indicates that action generation via flow-matching is sensitive to visual grounding errors and domain-shifted observations; positional/visual mislocalization causes failures in closed-loop execution, making perception the bottleneck for reliable action execution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Authors note baseline policies often fail complete tasks due to small positional errors in perception; flow-matching controller benefits from improved grounding but still depends on accurate visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Mitigated by conditioning the action expert on VLMs finetuned with in-domain SimplerEnv data (2M samples) to align observations and reduce domain mismatch for the controller.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not directly studied for the action expert (focus is on VLM data streams), but action expert benefits qualitatively from larger/more aligned in-domain VLM supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Shared self-attention between action tokens and LLM/vision tokens; non-causal attention for VLA stream; conditional flow-matching integrates multimodal context for action sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Paper reports accelerated convergence of VLA policy learning when initialized from Vlaser VLM and trained with in-domain data, implying improved sample efficiency for the action expert, though no exact numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Tightly coupling a flow-matching action expert with a VLM that has in-domain grounding/planning supervision enables more reliable closed-loop control; however, perception (grounding accuracy and domain alignment) remains the primary limiter of practical success.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1967.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1967.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vlaser-Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser-Grounding (data-stream-specific VLM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vlaser variant finetuned specifically on embodied grounding data (region-level point/box supervision) used to ablate the contribution of grounding data to downstream VLA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser-Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as Vlaser (InternVL3-derived multimodal transformer + action expert) but pretrained/finetuned primarily on the embodied grounding portion of the Vlaser-6M dataset (bounding boxes and center points), enabling analysis of how grounding-specific supervision affects VLA finetuning and closed-loop control.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternViT (same as Vlaser backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Finetuned on Vlaser grounding subset derived from RoboPoint, ShareRobot, Pixmo-Points, Paco-LaVIS, RefSpatial and synthetic SA-1B derived boxes/points.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Supervised region prediction head trained to produce normalized boxes and center points from multimodal features; action conditioning uses these improved region-aware features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level grounding (boxes and center points)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 2D box/point supervision; 3D spatial cues may be present through multi-task training but the variant emphasizes 2D grounding labels.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Embodied grounding and manipulation tasks that require precise localization for action (e.g., point-and-pick, place tasks in SimplerEnv)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Google Robot tasks, WidowX tasks within SimplerEnv (paper uses Vlaser-Grounding as one of the ablation models evaluated on Google Robot tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Robot-perspective simulation frames (SimplerEnv) and other embodied-grounding dataset images used in pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Closed-loop success rate and grounding accuracy on embodied grounding benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Paper reports that models trained on in-domain grounding and combined datasets improve closed-loop success (qualitative statement: 'incorporating all types of data leads to significant improvements'); specific numeric improvements for Vlaser-Grounding versus other variants are not provided in the main text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The grounding-specific variant supports the paper's claim that better region-level grounding reduces vision-domain shift and thereby improves closed-loop performance; nonetheless, grounding alone is not always sufficient without in-domain planning/state data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Improves localization failures but combined training with planning and in-domain QA is needed to address broader execution failures (positional drift, policy recovery) — exact failure breakdowns not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Vlaser-Grounding is an explicit attempt to reduce domain shift by training on embodied grounding datasets and SA-1B-derived region labels; still authors advocate for in-domain SimplerEnv data for best VLA transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Grounding benefits from large curated box/point datasets (1.5M+300k synthetic) but the paper emphasizes domain alignment over sheer scale for downstream VLA gains.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Shared self-attention multimodal transformer with a grounding head that outputs region coordinates from multimodal features.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Region-level grounding supervision substantially improves embodied grounding benchmarks and reduces localization-related failures, but must be paired with in-domain task/state examples (planning/QA) to maximize closed-loop control gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1967.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1967.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL3: Multimodal Vision-Language Backbone (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model family used as the VLM backbone baseline; Vlaser is built by supervised finetuning of InternVL3 on embodied datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL3 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InternVL3 provides the vision-language backbone (InternViT + LLM projector) used as the initialization for Vlaser; the paper fine-tunes InternVL3 (2B and 8B sizes) on embodied datasets to create Vlaser variants and to evaluate the effect of VLM initialization on downstream VLA policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternViT (as used in InternVL3)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained as part of InternVL3 (Zhu et al., 2025) — details of original pretraining corpora are referenced in the InternVL3 citation rather than specified in this paper's body.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>InternVL3 baseline provides general multimodal representations; when finetuned on grounding data it can output grounding/QA answers, but raw InternVL3 (without Vlaser finetuning) underperforms Vlaser on embodied grounding and reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Global and region-aware when finetuned, but baseline results indicate weaker region/robot-perspective grounding than Vlaser variants.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit in its multimodal features; improved via finetuning on explicit spatial/3D datasets to produce better spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Used as baseline for embodied QA, grounding, planning, and VLA finetuning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Same benchmark suite (12 embodied reasoning benchmarks) and SimplerEnv VLA tasks used to compare performance against Vlaser</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Trained on internet-scale multimodal corpora (as InternVL3) and evaluated on simulation/embodied domains; suffers domain-shift in robot-centric tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized average across embodied reasoning benchmarks and success rate for VLA tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>InternVL3-2B baseline avg normalized score 15.2 (paper); InternVL3-8B baseline avg 22.3 (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Vlaser finetuning on embodied datasets yields substantial gains over InternVL3 baseline on embodied grounding and reasoning benchmarks (e.g., average scores increase by ~30 points for 2B).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Serving as the initialization comparator, InternVL3 baselines show that naive transfer from internet-pretrained VLMs is insufficient for optimal VLA performance; in-domain finetuning is necessary to improve closed-loop success.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>InternVL3-initialized policies struggle on closed-loop tasks due to robot-perspective domain shift; this motivated the construction of Vlaser-6M and in-domain samples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Baselines frequently fail due to positional/visual localization errors in closed-loop execution; InternVL3 needs targeted embodied data to match Vlaser performance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Paper demonstrates that fine-tuning InternVL3 on Vlaser-6M and in-domain SimplerEnv data reduces domain shift and improves VLA finetuning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>InternVL3 scale (2B vs 8B) exhibits similar trends: larger models help complex planning but do not solve domain shift; in-domain data remains critical.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>InternVL3 uses a multimodal transformer fusion (as described in the InternVL3 reference); in this paper fusion is implemented via shared self-attention when fine-tuned into Vlaser.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Using Vlaser initialization (finetuned from InternVL3) accelerates downstream VLA convergence compared to training from scratch on VLA data; specific numeric sample-efficiency figures are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Using a generic internet-scale VLM backbone (InternVL3) without robot-perspective finetuning yields weak closed-loop control transfers; specialized grounding and in-domain training are needed to realize VLM reasoning in embodied control.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models <em>(Rating: 2)</em></li>
                <li>Flow matching for generative modeling <em>(Rating: 2)</em></li>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics <em>(Rating: 2)</em></li>
                <li>Rt-1: Robotics transformer for real-world control at scale <em>(Rating: 1)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
                <li>Embodied-R1: Reinforced embodied reasoning for general robotic manipulation <em>(Rating: 1)</em></li>
                <li>SimplerEnv: Evaluating real-world robot manipulation policies in simulation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1967",
    "paper_id": "paper-282058662",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "Vlaser",
            "name_full": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
            "brief_description": "A foundation vision-language model fine-tuned for embodied reasoning and end-to-end robot control that integrates a multimodal VLM backbone with an action expert trained via flow matching, and is pretrained on the Vlaser-6M embodied dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser",
            "model_description": "Multimodal model composed of a vision-language backbone (InternVL3-derived InternViT visual encoder + Qwen2.5 LLMs) fine-tuned on embodied reasoning data, combined with an action expert for low-level control. The architecture uses an llava-like multimodal transformer where self-attention is shared between language and visual streams; robot state is encoded as a state token and (noised) actions as action tokens. For VLA finetuning the action expert predicts action chunks using a flow-matching denoising vector field and non-causal attention in the VLA stream. Grounding is performed by the VLM head to output bounding boxes and center points (normalized to [0,1000]) or textual answers depending on the QA/grounding prompt.",
            "visual_encoder_type": "InternViT (InternVL3 backbone visual encoder)",
            "visual_encoder_pretraining": "Pretrained as part of the InternVL3 family (referenced InternVL3 (Zhu et al., 2025)); the paper does not provide further public dataset sizes for InternViT in this text.",
            "grounding_mechanism": "Supervised multimodal prediction from pooled vision-language transformer representations: the model produces open-vocabulary grounding outputs (bounding boxes and center points normalized to [0,1000]) and can localize objects given text; mechanism implemented via shared self-attention between vision and language tokens and a downstream grounding head trained on point/box labels (region-level supervision).",
            "representation_level": "Region-level (bounding boxes and center points) and multi-level (also uses global/multi-frame inputs for planning and tokenized robot state; training data include pixel masks converted to boxes and 3D scene graphs for spatial queries).",
            "spatial_representation": "Explicit 2D grounding (normalized bounding boxes and center points) and explicit 3D spatial signals in training data (3D scene graphs, 3D bounding boxes and point-cloud-derived spatial queries used for spatial reasoning augmentation).",
            "embodied_task_type": "Object manipulation, embodied visual question answering, task planning, embodied grounding, spatial reasoning, closed-loop robot control",
            "embodied_task_name": "SimplerEnv (WidowX, Google Robot), ALFRED, Habitat, Where2Place, Pointarena, Paco-Lavis, Pixmo-Points, RefSpatial, and other embodied reasoning benchmarks (see paper list of 12 benchmarks)",
            "visual_domain": "Photorealistic simulation (SimplerEnv) and robot-centric egocentric views; training also includes internet-derived imagery and synthetic masks converted to boxes but primary downstream evaluation in sim/robot views.",
            "performance_metric": "Normalized benchmark average (across 12 embodied reasoning benchmarks), success rate for closed-loop robot tasks",
            "performance_value": "Vlaser-2B: Avg normalized score 45.3 (vs InternVL3-2B 15.2); Vlaser-8B: Avg normalized score 51.3 (vs InternVL3-8B 22.3). Closed-loop results: paper reports state-of-the-art on Bridge and competitive performance on Google Robot but does not give a single aggregated numeric SR in the main text.",
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Qualitative and benchmark-level improvements: Vlaser (trained on Vlaser-6M including grounding data) yields large gains on embodied grounding and simulation evaluation relative to InternVL3 initializations (e.g., avg benchmark score increases from 15.2→45.3 for 2B and 22.3→51.3 for 8B). For closed-loop policy learning, in-domain grounding/planning QA data (Vlaser-QA) yields significantly better VLA finetuning convergence and success rates than out-of-domain pretrained reasoning alone (numeric delta not provided).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "The paper compares different VLM initializations (InternVL3 vs Vlaser variants) and finds that upstream improvements on out-of-domain embodied reasoning benchmarks do not necessarily translate to downstream VLA control; in-domain robot-annotated data (SimplerEnv-derived) is substantially more effective at accelerating VLA convergence and improving task success. Also observed: smaller Vlaser-2B outperforms Vlaser-8B on simple point grounding tasks, while Vlaser-8B outperforms on complex multi-step planning and closed-loop simulation.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The paper identifies a core perception/grounding bottleneck as a domain gap between internet-scale pretraining and robot viewpoint/embodiment: robots have different viewpoints and visual statistics, and lack of robot-perspective data causes degraded closed-loop control generalization. Vision observation domain shift (viewpoint and distribution mismatch) is explicitly called out as the main barrier.",
            "failure_mode_analysis": "Qualitative analysis: baseline models frequently fail entire closed-loop tasks due to small positional errors; improvements arise when in-domain grounding/spatial/planning data reduce vision-domain shift. The paper reports that many failures in SimplerEnv evaluations are attributable to vision/positioning errors rather than reasoning failures, but precise failure frequencies per mode are not enumerated.",
            "domain_shift_handling": "Mitigation via in-domain finetuning: authors generate 2M in-domain multimodal QA pairs from SimplerEnv (WidowX and Google Robot embodiments) and fine-tune VLM and VLA on these robot-perspective data; this in-domain finetuning accelerates VLA convergence and improves closed-loop success compared to only upstream embodied reasoning pretraining on internet/out-of-domain data.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "The authors keep all parameters trainable (they fine-tune the full VLM and action expert) and do not report a frozen vs fine-tuned encoder comparison in the provided text.",
            "pretraining_scale_effect": "Paper reports that adding out-of-domain embodied reasoning data improves upstream reasoning benchmarks but does not guarantee downstream VLA gains; in-domain data aligned to robot observations is more valuable. Also observes model-size dependent effects: smaller model (2B) better at simple grounding; larger (8B) better for complex planning/coT tasks.",
            "fusion_mechanism": "Shared self-attention across visual and language tokens (llava-like multimodal transformer); action expert shares self-attention with LLM and uses non-causal attention in the VLA stream; grounding outputs are produced from the multimodal transformer head.",
            "sample_efficiency": "Qualitative: pretraining VLM (Vlaser) and adding in-domain robot data significantly accelerate VLA policy convergence on WidowX (paper states 'significantly accelerates convergence') but exact sample counts or iterations-to-target are not enumerated in main text (they reference checkpoints used for evaluation).",
            "key_findings_grounding": "1) Explicit region-level grounding supervision (points and boxes) and large curated embodied datasets (Vlaser-6M) greatly improve embodied grounding benchmarks; 2) However, improvements on upstream out-of-domain reasoning measures do not automatically translate to downstream closed-loop control — in-domain robot-perspective data is critical to close that gap; 3) Shared self-attention multimodal fusion plus region-level outputs (points/boxes) and flow-matching action prediction enable integrated perception-to-action, but primary remaining failure mode is vision-domain shift (viewpoint/positional errors).",
            "uuid": "e1967.0"
        },
        {
            "name_short": "Vlaser-6M",
            "name_full": "Vlaser-6M dataset / Vlaser data engine",
            "brief_description": "A curated 6M-sample multimodal dataset designed for embodied reasoning, combining 1.5M grounding QA pairs (boxes/points), spatial reasoning samples, planning traces, RoboVQA items, and 2M in-domain SimplerEnv robot-perspective QA samples for effective VLM→VLA transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser-6M (data engine)",
            "model_description": "A multi-part dataset used to supervise VLM pretraining: includes (a) 1.5M embodied grounding pairs (bounding boxes and center points normalized to [0,1000]) from several embodied-grounding sources plus 300k synthetic box/point samples from SA-1B segmentation masks; (b) 1.2M RoboVQA pairs and 500k spatial intelligence items; (c) 400k planning examples; (d) 2M in-domain SimplerEnv robot-perspective multimodal QA pairs from WidowX and Google Robot embodiments targeted at VLA finetuning.",
            "visual_encoder_type": "N/A (dataset artifact), supplies region-level supervision (boxes/points), pixel masks (converted), and 3D scene graph annotations used to train models with InternViT visual encoder",
            "visual_encoder_pretraining": "N/A (dataset artifact); assembled from multiple sources including SA-1B segmentation masks, ScanNet/ScanNet++/ARKitScenes, RoboVQA, RefSpatial, SimplerEnv episodes, etc.",
            "grounding_mechanism": "Provides supervised region-level labels (bounding boxes, center points) and captions for open-vocabulary grounding; dataset enables training of grounding heads that map transformer multimodal features to normalized box/point coordinates or textual localization answers.",
            "representation_level": "Region-level (2D boxes, center points), pixel-level (masks converted to boxes/points), and scene-level/3D (scene graphs and 3D bounding boxes used for spatial questions).",
            "spatial_representation": "Explicit: normalized 2D bounding boxes and center points, plus 3D scene graphs and 3D bounding boxes derived from ScanNet/ARKitScenes for spatial reasoning examples.",
            "embodied_task_type": "Embodied grounding, RoboVQA, spatial reasoning, planning, VLA in-domain finetuning",
            "embodied_task_name": "Used to train/evaluate on tasks detailed in the paper (Where2Place, Pointarena, Paco-Lavis, Pixmo-Points, RefSpatial, EmbodiedBench, etc.) and to create in-domain SimplerEnv samples for WidowX and Google Robot",
            "visual_domain": "Mixed: internet/in-the-wild images and synthetic/segmented images (SA-1B), plus robot-perspective simulation frames from SimplerEnv (egocentric robot views)",
            "performance_metric": "Enables model performance gains measured as normalized average on embodied reasoning benchmarks and improved VLA finetuning convergence/success rate; dataset itself not scored.",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Providing region-level supervised grounding data (1.5M + 300k from SA-1B) strongly improves embodied grounding benchmark performance and contributes to the Vlaser gains over InternVL3 on average benchmark scores (as reflected in Vlaser vs InternVL3 comparisons).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The dataset authors argue that internet and third‑person datasets are insufficient to close robot-perspective domain gaps; therefore, they create 2M in-domain SimplerEnv samples specifically to reduce vision-domain shift for VLA finetuning.",
            "failure_mode_analysis": "Dataset ablation experiments (Vlaser-QA, Vlaser-Grounding, Vlaser-Spatial) indicate combining data types reduces failures caused by vision-domain mismatch, but paper reports only qualitative improvements and does not enumerate failure frequencies per data-source ablation in main text.",
            "domain_shift_handling": "Primary strategy: explicitly produce large-scale in-domain robot-perspective samples (2M) from SimplerEnv and fine-tune VLMs on these to reduce the pretraining → embodiment domain gap.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper argues that scale/diversity of embodied data matters, but data alignment to robot perspective is more important than merely adding more out-of-domain embodied reasoning samples; in-domain SimplerEnv data yielded the largest downstream VLA benefits.",
            "fusion_mechanism": "Dataset supports training of shared self-attention multimodal transformers and region-heads (no fusion method inside dataset).",
            "sample_efficiency": "Authors report that adding in-domain samples accelerates downstream VLA convergence (qualitative); exact sample-efficiency numbers not provided.",
            "key_findings_grounding": "High-volume, explicitly robot-perspective in-domain data is critical to transfer upstream VLM reasoning to downstream VLA control; region-level supervision (points/boxes) plus spatial and planning traces together produce the best closed-loop improvements.",
            "uuid": "e1967.1"
        },
        {
            "name_short": "Action Expert (flow-matching)",
            "name_full": "Vlaser Action Expert using Flow Matching for Action Prediction",
            "brief_description": "A low-level control module trained with flow-matching to denoise action chunks conditioned on image observations, language instructions, and robot state, enabling end-to-end action generation from multimodal representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser Action Expert (flow-matching)",
            "model_description": "Encodes action chunks (H-step sequences) as tokens via an MLP action encoder and trains a model v_theta to match the denoising vector field u(A_tau | A) using flow-matching. The action expert shares self-attention with the language model, accepts image observation and state tokens, uses non-causal attention in the VLA stream, and generates actions by integrating the learned vector field from tau=0→1 (sampling with Euler integration).",
            "visual_encoder_type": "N/A (controller module conditioned on visual tokens produced by InternViT)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Conditions action prediction on multimodal transformer outputs (visual tokens, language tokens, and explicit robot state token), thereby grounding language and perception into action via a learned vector-field denoising process; grounding-to-action mapping is implicit in the conditional flow-matching policy.",
            "representation_level": "Action-chunk level (temporal sequence tokens) conditioned on region/global visual features and state tokens; grounding occurs at region/point/box level upstream in VLM features.",
            "spatial_representation": "Relies on visual encoder outputs (which include region-level boxes/points and 3D-aware spatial features from training data) and explicit robot state token for spatially-aware action prediction.",
            "embodied_task_type": "Low-level robot manipulation (closed-loop control) in simulated WidowX and Google Robot tasks",
            "embodied_task_name": "SimplerEnv WidowX and Google Robot tasks (flow-matching action generation evaluated in these environments)",
            "visual_domain": "Photorealistic simulation, robot-centric egocentric views",
            "performance_metric": "Success rate on closed-loop manipulation tasks (SimplerEnv); convergence speed during finetuning",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Action expert performance improves when conditioned on VLMs finetuned with in-domain grounding/planning QA (Vlaser-QA), leading to substantial gains in closed-loop success and faster convergence; exact numeric deltas for flow-matching controller with vs without grounding not provided in main text.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The paper indicates that action generation via flow-matching is sensitive to visual grounding errors and domain-shifted observations; positional/visual mislocalization causes failures in closed-loop execution, making perception the bottleneck for reliable action execution.",
            "failure_mode_analysis": "Authors note baseline policies often fail complete tasks due to small positional errors in perception; flow-matching controller benefits from improved grounding but still depends on accurate visual observations.",
            "domain_shift_handling": "Mitigated by conditioning the action expert on VLMs finetuned with in-domain SimplerEnv data (2M samples) to align observations and reduce domain mismatch for the controller.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Not directly studied for the action expert (focus is on VLM data streams), but action expert benefits qualitatively from larger/more aligned in-domain VLM supervision.",
            "fusion_mechanism": "Shared self-attention between action tokens and LLM/vision tokens; non-causal attention for VLA stream; conditional flow-matching integrates multimodal context for action sampling.",
            "sample_efficiency": "Paper reports accelerated convergence of VLA policy learning when initialized from Vlaser VLM and trained with in-domain data, implying improved sample efficiency for the action expert, though no exact numbers provided.",
            "key_findings_grounding": "Tightly coupling a flow-matching action expert with a VLM that has in-domain grounding/planning supervision enables more reliable closed-loop control; however, perception (grounding accuracy and domain alignment) remains the primary limiter of practical success.",
            "uuid": "e1967.2"
        },
        {
            "name_short": "Vlaser-Grounding",
            "name_full": "Vlaser-Grounding (data-stream-specific VLM variant)",
            "brief_description": "A Vlaser variant finetuned specifically on embodied grounding data (region-level point/box supervision) used to ablate the contribution of grounding data to downstream VLA performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser-Grounding",
            "model_description": "Same architecture as Vlaser (InternVL3-derived multimodal transformer + action expert) but pretrained/finetuned primarily on the embodied grounding portion of the Vlaser-6M dataset (bounding boxes and center points), enabling analysis of how grounding-specific supervision affects VLA finetuning and closed-loop control.",
            "visual_encoder_type": "InternViT (same as Vlaser backbone)",
            "visual_encoder_pretraining": "Finetuned on Vlaser grounding subset derived from RoboPoint, ShareRobot, Pixmo-Points, Paco-LaVIS, RefSpatial and synthetic SA-1B derived boxes/points.",
            "grounding_mechanism": "Supervised region prediction head trained to produce normalized boxes and center points from multimodal features; action conditioning uses these improved region-aware features.",
            "representation_level": "Region-level grounding (boxes and center points)",
            "spatial_representation": "Explicit 2D box/point supervision; 3D spatial cues may be present through multi-task training but the variant emphasizes 2D grounding labels.",
            "embodied_task_type": "Embodied grounding and manipulation tasks that require precise localization for action (e.g., point-and-pick, place tasks in SimplerEnv)",
            "embodied_task_name": "Google Robot tasks, WidowX tasks within SimplerEnv (paper uses Vlaser-Grounding as one of the ablation models evaluated on Google Robot tasks)",
            "visual_domain": "Robot-perspective simulation frames (SimplerEnv) and other embodied-grounding dataset images used in pretraining",
            "performance_metric": "Closed-loop success rate and grounding accuracy on embodied grounding benchmarks",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Paper reports that models trained on in-domain grounding and combined datasets improve closed-loop success (qualitative statement: 'incorporating all types of data leads to significant improvements'); specific numeric improvements for Vlaser-Grounding versus other variants are not provided in the main text excerpt.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The grounding-specific variant supports the paper's claim that better region-level grounding reduces vision-domain shift and thereby improves closed-loop performance; nonetheless, grounding alone is not always sufficient without in-domain planning/state data.",
            "failure_mode_analysis": "Improves localization failures but combined training with planning and in-domain QA is needed to address broader execution failures (positional drift, policy recovery) — exact failure breakdowns not enumerated.",
            "domain_shift_handling": "Vlaser-Grounding is an explicit attempt to reduce domain shift by training on embodied grounding datasets and SA-1B-derived region labels; still authors advocate for in-domain SimplerEnv data for best VLA transfer.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Grounding benefits from large curated box/point datasets (1.5M+300k synthetic) but the paper emphasizes domain alignment over sheer scale for downstream VLA gains.",
            "fusion_mechanism": "Shared self-attention multimodal transformer with a grounding head that outputs region coordinates from multimodal features.",
            "sample_efficiency": null,
            "key_findings_grounding": "Region-level grounding supervision substantially improves embodied grounding benchmarks and reduces localization-related failures, but must be paired with in-domain task/state examples (planning/QA) to maximize closed-loop control gains.",
            "uuid": "e1967.3"
        },
        {
            "name_short": "InternVL3",
            "name_full": "InternVL3: Multimodal Vision-Language Backbone (used as baseline)",
            "brief_description": "Open-source multimodal model family used as the VLM backbone baseline; Vlaser is built by supervised finetuning of InternVL3 on embodied datasets.",
            "citation_title": "Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models",
            "mention_or_use": "use",
            "model_name": "InternVL3 (baseline)",
            "model_description": "InternVL3 provides the vision-language backbone (InternViT + LLM projector) used as the initialization for Vlaser; the paper fine-tunes InternVL3 (2B and 8B sizes) on embodied datasets to create Vlaser variants and to evaluate the effect of VLM initialization on downstream VLA policy learning.",
            "visual_encoder_type": "InternViT (as used in InternVL3)",
            "visual_encoder_pretraining": "Pretrained as part of InternVL3 (Zhu et al., 2025) — details of original pretraining corpora are referenced in the InternVL3 citation rather than specified in this paper's body.",
            "grounding_mechanism": "InternVL3 baseline provides general multimodal representations; when finetuned on grounding data it can output grounding/QA answers, but raw InternVL3 (without Vlaser finetuning) underperforms Vlaser on embodied grounding and reasoning benchmarks.",
            "representation_level": "Global and region-aware when finetuned, but baseline results indicate weaker region/robot-perspective grounding than Vlaser variants.",
            "spatial_representation": "Implicit in its multimodal features; improved via finetuning on explicit spatial/3D datasets to produce better spatial reasoning.",
            "embodied_task_type": "Used as baseline for embodied QA, grounding, planning, and VLA finetuning experiments",
            "embodied_task_name": "Same benchmark suite (12 embodied reasoning benchmarks) and SimplerEnv VLA tasks used to compare performance against Vlaser",
            "visual_domain": "Trained on internet-scale multimodal corpora (as InternVL3) and evaluated on simulation/embodied domains; suffers domain-shift in robot-centric tasks",
            "performance_metric": "Normalized average across embodied reasoning benchmarks and success rate for VLA tasks",
            "performance_value": "InternVL3-2B baseline avg normalized score 15.2 (paper); InternVL3-8B baseline avg 22.3 (paper).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Vlaser finetuning on embodied datasets yields substantial gains over InternVL3 baseline on embodied grounding and reasoning benchmarks (e.g., average scores increase by ~30 points for 2B).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Serving as the initialization comparator, InternVL3 baselines show that naive transfer from internet-pretrained VLMs is insufficient for optimal VLA performance; in-domain finetuning is necessary to improve closed-loop success.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "InternVL3-initialized policies struggle on closed-loop tasks due to robot-perspective domain shift; this motivated the construction of Vlaser-6M and in-domain samples.",
            "failure_mode_analysis": "Baselines frequently fail due to positional/visual localization errors in closed-loop execution; InternVL3 needs targeted embodied data to match Vlaser performance.",
            "domain_shift_handling": "Paper demonstrates that fine-tuning InternVL3 on Vlaser-6M and in-domain SimplerEnv data reduces domain shift and improves VLA finetuning outcomes.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "InternVL3 scale (2B vs 8B) exhibits similar trends: larger models help complex planning but do not solve domain shift; in-domain data remains critical.",
            "fusion_mechanism": "InternVL3 uses a multimodal transformer fusion (as described in the InternVL3 reference); in this paper fusion is implemented via shared self-attention when fine-tuned into Vlaser.",
            "sample_efficiency": "Using Vlaser initialization (finetuned from InternVL3) accelerates downstream VLA convergence compared to training from scratch on VLA data; specific numeric sample-efficiency figures are not provided.",
            "key_findings_grounding": "Using a generic internet-scale VLM backbone (InternVL3) without robot-perspective finetuning yields weak closed-loop control transfers; specialized grounding and in-domain training are needed to realize VLM reasoning in embodied control.",
            "uuid": "e1967.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models",
            "rating": 2
        },
        {
            "paper_title": "Flow matching for generative modeling",
            "rating": 2
        },
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics",
            "rating": 2
        },
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale",
            "rating": 1
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 1
        },
        {
            "paper_title": "Embodied-R1: Reinforced embodied reasoning for general robotic manipulation",
            "rating": 1
        },
        {
            "paper_title": "SimplerEnv: Evaluating real-world robot manipulation policies in simulation",
            "rating": 2
        }
    ],
    "cost": 0.02119975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning VLASER: VISION-LANGUAGE-ACTION MODEL WITH SYNERGISTIC EMBODIED REASONING
13 Oct 2025</p>
<p>Ganlin Yang 
University of Science and Technology
China</p>
<p>Shanghai AI Laboratory</p>
<p>Tianyi Zhang 
Shanghai AI Laboratory</p>
<p>Zhejiang University</p>
<p>Haoran Hao 
Shanghai AI Laboratory</p>
<p>Nanjing University</p>
<p>Weiyun Wang 
Shanghai AI Laboratory</p>
<p>Fudan University
7 Tsinghua University 8 NUS</p>
<p>Yibin Liu 
Shanghai Jiao Tong University</p>
<p>Northeastern University</p>
<p>Dehui Wang 
Shanghai Jiao Tong University</p>
<p>Guanzhou Chen 
Shanghai AI Laboratory</p>
<p>Shanghai Jiao Tong University</p>
<p>Zijian Cai 
Shanghai Jiao Tong University</p>
<p>Shenzhen University Project</p>
<p>Junting Chen 
Shanghai AI Laboratory</p>
<p>Weijie Su 
Shanghai AI Laboratory</p>
<p>Wengang Zhou 
University of Science and Technology
China</p>
<p>Shanghai AI Laboratory</p>
<p>Yu Qiao 
Shanghai AI Laboratory</p>
<p>Jifeng Dai 
Shanghai AI Laboratory</p>
<p>Jiangmiao Pang 
Shanghai AI Laboratory</p>
<p>Gen Luo 
Shanghai AI Laboratory</p>
<p>Wenhai Wang 
Shanghai AI Laboratory</p>
<p>Yao Mu 
Shanghai AI Laboratory</p>
<p>Shanghai Jiao Tong University</p>
<p>Zhi Hou 
Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning VLASER: VISION-LANGUAGE-ACTION MODEL WITH SYNERGISTIC EMBODIED REASONING
13 Oct 20259D8125D657A848A63DDC7F5C66932B0AarXiv:2510.11027v1[cs.CV]
While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning.In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser -a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents.Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks-including spatial reasoning, embodied grounding, embodied QA, and task planning.Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data.Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.The code, model and data are available at https://github.com/OpenGVLab/Vlaser/.</p>
<p>INTRODUCTION</p>
<p>Embodied artificial intelligence (AI) (Chrisley, 2003) aims to endow agents with the ability to perceive, understand, and act in the physical world.Achieving such intelligence requires not only accurate perception and language understanding but also embodied reasoning and effective control, which together define the paradigm of vision-language-action (VLA) models.Developing foundation models that possess strong reasoning and control capabilities is therefore an important advancement toward general-purpose embodied AI.</p>
<p>In this context, vision-language models (VLMs) (OpenAI, 2023;Liu et al., 2023;Chen et al., 2024;Bai et al., 2025;Team et al., 2023) emerge as natural candidates to enhance embodied agents in perception generalization and reasoning ability.Following this paradigm, extensive embodied visionlanguage models (Azzolini et al., 2025;Team et al., 2025c) emerge from enhancing the key ability for an embodied agent in grounding, planning, and spatial reasoning.Meanwhile, a significant body of work extends vision-language models (VLMs) into vision-language-action models (VLAs) (Kim et al., 2024;Intelligence et al., 2025;Driess et al., 2025) for robot control.While there are some approaches (Intelligence et al., 2025;Driess et al., 2025) that demonstrate the effectiveness of cotraining with web data for the generalization in robot manipulation, it remains poorly understood which multi-modal data streams/abilities are most critical for improving downstream VLA models.In this paper, we aim to construct Vlaser, an embodied vision-language model that possesses strong The pre-trained Vlaser VLM significantly accelerates convergence in downstream Vision-Language Action model (VLA) policy learning on WidowX platform (Walke et al., 2023a).Bottom-right: Successful closed-loop operation of an agent powered by Vlaser within the SimplerEnv benchmark (Li et al., 2024b).</p>
<p>embodied reasoning capabilities, and subsequently answer this question based on the corresponding vision-language-action models.</p>
<p>Despite advancements in vision-language models (Chen et al., 2024;Bai et al., 2025), the capabilities of operating as an embodied agent remain severely constrained.In particular, navigation and traditional manipulation approaches rely heavily on planning-based control (Huang et al., 2022;Gasparetto et al., 2015;Zhang et al., 2018), which requires a strong foundational ability in grounding and planning.Planning and Grounding are cornerstones of the agents embodied in the physical world.Meanwhile, spatial understanding increasingly attracts the interest of the community in addressing the spatial perception ability of VLM.To this end, we firstly aim to introduce an embodied visionlanguage model specifically enhanced for the aboved embodied reasoning capabilities.Specifically, we construct the Vlaser data engine, which enables the systematic construction of the Vlaser-6M dataset by curating, reorganizing, and annotating public datasets from the Internet.As illustrated in Figure 1, the resulting dataset spans a wide spectrum of embodied reasoning tasks-including general embodied QA, visual grounding, spatial intelligence, task planning, and in-domain simulation data.</p>
<p>Leveraging this comprehensive data foundation, Vlaser achieves state-of-the-art performance across a variety of embodied reasoning benchmarks, demonstrating strong generalization in both open-loop inference and closed-loop control settings.</p>
<p>Existing Vision-Language-Action (VLA) models (Black et al., 2024;Cheng et al., 2024;Kim et al., 2024;Intelligence et al., 2025) typically fine-tune pre-trained Vision-Language Models (VLMs) for robot control.However, the selection of an optimal VLM backbone -one that accelerates convergence and improves success rates when used as initialization for end-to-end VLA policy learning, remains an under-explored research problem.To address this gap, we systematically investigate the VLM-to-VLA adaptation paradigm using our enhanced embodied vision-language model and associated data engine.Our experiments reveal an important insight: although out-of-domain embodied reasoning data significantly improve upstream reasoning capabilities as measured by standard benchmarks, these gains may not translate directly or prominently to downstream VLA performance.In contrast, indomain data -annotated directly on robot interaction datasets such as Open X-Embodiment (O 'Neill et al., 2024) proves substantially more effective in accelerating convergence and increasing task success rates during VLA fine-tuning.We believe this observation provides significant insights for future embodied vision-language model construction: It is urgent to shrink the domain gap between current embodied perception and reasoning benchmarks to the real-world embodied tasks, and thus facilitate the closed-loop evaluation for the corresponding robot embodiment.</p>
<p>In summary, the principal contributions of Vlaser are as follows.</p>
<p>An open-source embodied vision-language model and dataset.We introduce Vlaser, an adaptable vision-language model that enhances InternVL with embodied reasoning capabilities and end-to-end robot control.The full model weights, modular data generation pipeline, training and evaluation code, and the accompanying Vlaser-6M dataset will be made publicly available to support reproducibility and future research.</p>
<p>Systematic analysis of data effectiveness for VLA transfer.We conduct a thorough investigation into which types of vision-language pretraining data contribute most effectively to downstream Vision-Language-Action (VLA) policy learning.Our findings offer practical insights for constructing task-aware data streams that bridge the gap between Internet-scale pretraining and embodied-specific fine-tuning.</p>
<p>State-of-the-art performance across embodied benchmarks.Among models of comparable scale, Vlaser achieves top-tier results on a comprehensive set of embodied reasoning benchmarks-spanning visual grounding, task planning, spatial reasoning, and simulation-based robot evaluation, demonstrating its strong generalization and applicability to both open-loop inference and closed-loop control scenarios.</p>
<p>METHOD</p>
<p>Vlaser aims to integrate embodied reasoning with end-to-end robot control for embodied agents, and identify the most crucial VLM data stream for VLA models.We first present the Vlaser structures in Section 2.1.Then, we illustrate the data engine in Section 2.2.Section 2.3 discusses the training recipe that includes embodied reasoning pretraining and vision-language-action finetuning.</p>
<p>MODEL STRUCTURE</p>
<p>The structure of Vlaser consists of two major components: the typical vision-language backbone (Chen et al., 2024;Liu et al., 2023) and the action expert for low-level control, as shown in Figure 2. We illustrate the two components respectively in this section.</p>
<p>VLM Backbone Vision-language models (VLMs) are key candidates for embodied agents, providing both perception and reasoning abilities.Vlaser , built on InternVL3 (Zhu et al., 2025), integrates embodied reasoning with robot control for embodied agents.While InternVL3 excels in multimodal and linguistic tasks across various model sizes, Vlaser focuses on two sizes-2B and 8B-optimized for the computational constraints of robots.These models utilize InternViT (Chen et al., 2024) as the vision encoder, paired with Qwen2.5-1.5B and Qwen2.5-7BLLMs (Qwen et al., 2025).Unlike typical multimodal MLLMs, Vlaser emphasizes embodied common-sense reasoning and end-to-end robot control capabilities.</p>
<p>Action Expert There are a large number of MLLMs (Team et al., 2025a;NVIDIA et al., 2025a) that enhance the ability of embodied common-sense reasoning for agents, while a few approaches equip the embodied MLLMs with end-to-end robot control.Vlaser extends the MLLMs with a low-level robot control and verifies the capability of different data streams in downstream VLA finetuning.Following (Intelligence et al., 2025), we design an action expert based on the opensource vision-language model (Chen et al., 2024;Zhu et al., 2025).Meanwhile, we utilize the flow matching (Lipman et al., 2023a) for action prediction based on the llava-like vision-language structure, while sharing the self-attention among the language model and action expert module.Specifically, we encode the robot state as a state token and noised actions as action tokens, and input them into the action expert.Meanwhile, we utilize non-causal attention for the VLA stream.During inference, we denoise the actions based on the image observation, language instruction, as well as the current robot state.</p>
<p>VLASER DATA ENGINE</p>
<p>This section outlines the composition of the Vlaser-6M data engine, a cornerstone for the model's embodied reasoning capabilities.Here we present the overall data scale and sources for each reasoning modality, while more details about the construction methodologies are provided in Appendix A.2.</p>
<p>Embodied Grounding Data The Vlaser dataset incorporates two distinct 2D grounding formats-bounding boxes and center points-both normalized to the range [0, 1000] to ensure consistent and resolution-invariant grounding predictions across diverse image resolutions.Specifically, we collect 1.5 million high-quality question-answer pairs that support multiple grounding tasks: predicting bounding boxes from open-vocabulary descriptions, localizing object center points based on textual descriptions, and identifying objects from given spatial coordinates.The data is sourced from several open embodied grounding datasets, including RoboPoint (Yuan et al., 2024), ShareRobot (Ji et al., 2025), Pixmo-Points (Deitke et al., 2025), Paco-LaVIS (Ramanathan et al., 2023), and RefSpatial (Zhou et al., 2025a).To further enhance generalization capabilities for open-world and open-vocabulary scenarios, we also generate an additional 300k point and bounding box annotations derived from segmentation masks in the SA-1B dataset (Kirillov et al., 2023).This combination of curated human annotations and synthetically enriched data aims to bolster both the diversity and scalability of visual grounding under real-world embodied settings.</p>
<p>General and Spatial Reasoning Data The Vlaser dataset integrates 1.2 million question-answer pairs dedicated to general Robotic Visual Question Answering (RoboVQA) tasks, along with an additional 500k data items specifically designed to enhance spatial intelligence.This comprehensive data composition substantially strengthens the model's capabilities in general state perception and 3D spatial reasoning.For the RoboVQA component, data is aggregated from multiple established sources, including RoboVQA (Sermanet et al., 2024), Robo2VLM (Chen et al., 2025b), RoboPoint (Yuan et al., 2024), RefSpatial (Zhou et al., 2025a), OWMM-Agent (Chen et al., 2025a), among others.To support spatial understanding and reasoning, we incorporate datasets such as SPAR (Zhang et al., 2025), SpaceR-151k (Ouyang et al., 2025), and VILASR (Wu et al., 2025).Furthermore, we augment these with 100k manually annotated spatial understanding samples generated from publicly available 3D scene datasets-including ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), CA-1M (Lazarow et al., 2025), and ARKitScenes (Baruch et al., 2021).The integration of these diverse and high-quality data sources effectively enhances the model's spatial awareness and supports more robust performance in complex embodied reasoning tasks.</p>
<p>Planning Data To tackle complex tasks, it is essential to decompose them into manageable sub-tasks and solve them step by step.This capability is commonly referred to as planning.Effective planning allows robots to combine basic skills and generalize to new scenarios.We collected 400k training data to strengthen the model's planning ability, encompassing both language-based planning data and multimodal tasks.These include Alpaca-15k-Instruction (Wu et al., 2023) and MuEP (Li et al., 2024a).To further enhance environmental understanding and reasoning for complex decision-making, we incorporated training data with detailed reasoning processes from WAP (Shi et al., 2025).To improve the model's ability to comprehend complex instructions and execute tasks, we followed the annotations of LLaRP (Szot et al., 2024) to initialize planning tasks in Habitat (Szot et al., 2021) and generate planning trajectories to accomplish these tasks.In addition, we integrated egocentric video datasets such as EgoPlan-IT (Chen et al., 2023) and EgoCOT (Mu et al., 2023), which closely align with the observational perspective of embodied agents and provide valuable planning examples.</p>
<p>In-Domain Data for downstream VLAs</p>
<p>To facilitate the end-to-end policy learning for Vision-Language Action Models (VLAs), we further generate 2 million in-domain multimodal questionanswer pairs tailored for VLM pretraining.These data are specifically designed to align with the embodied reasoning context and enhance the model's ability to perceive, reason, and plan in interactive environments.The in-domain data is sourced from simulation platforms SimplerEnv (Li et al., 2024c).Within SimplerEnv, data is generated for two distinct robotic embodiments: the Google Robot (Brohan et al., 2023b;a;O'Neill et al., 2024) and the WidowX Robot (Walke et al., 2023a), ensuring broad morphological and kinematic coverage.The question-answer pairs encompass the specialized categories including embodied grounding, spatial intelligence, planning and general VQA for robot states as described above.The detailed methodology for constructing each of the in-domain data in simulation is described in Appendix A.2.</p>
<p>TRAINING RECIPE</p>
<p>Vlaser adopts a two-stage training recipe, designed to optimize both embodied reasoning and robot control.It includes a VLM pretraining followed by a VLA finetuning.In this section, we elaborate on the training recipe among all phrases.</p>
<p>Vision-Language Pretraining Vlaser is developed by supervised fine-tuning (SFT) InternVL3 (Zhu et al., 2025) on embodied-related datasets, including those focused on grounding, planning, and spatial intelligence.In the first training phase, we fine-tune InternVL3 using auto-regressive language modeling loss.In particular, given the input images x ∈ R t×h×w×3 and textual prompt y ∈ R l , the language modeling loss L lm can be defined by
L lm = − log p(t N |F v (x; θ v ), F t (y), t 0:N −1 ; Θ),(1)
where p ∈ R m is the next-token probability and m denotes the vocabulary size.Here, F v (•) denotes the ViT and the MLP, and θ v is their parameters.F t (•) is the textual tokenizer.Θ are the parameters of the LLM.t i denotes the i-th predicted word.</p>
<p>Vision-Language-Action Finetuning For robot policy learning, we optimize the model using an action expert trained on robot-specific datasets.Vlaser integrates a flow-matching-based action expert to predict a sequence of future actions from a single-frame observation.Specifically, denote the action chunk A t = [a t , a t+1 , . . ., a t+H−1 ], where a t represents the action in the current timestep t and H represents the action horizon.Meanwhile, we encode each noisy action with an action encoder (i.e., an MLP projector) as a single token for the action expert.For the action chunk, A τ t = τ A t + (1 − τ )ϵ is the corresponding noisy action chunk, and we train the network to output v θ (A τ t , o t ) to match the denoising vector field u(A τ t |A t ) = ϵ − A t , where o t indicates the observations (e.g., image camera and robot state) at action timestep t, θ represents the network and τ ∈ [0, 1] represents the flow matching timesteps.Therefore, the VLA optimization loss is as follows,
L vla = ∥v θ (A τ t , o t ) − u(A τ t |A t )∥ 2 (2)
We sample the action chunks from the robot episodes and flow-matching timesteps to optimize the network.At inference, we generate actions by integrating the learned vector field from τ = 0 to τ = 1, starting with random noise A 0 t ∼ N (0, I), as follows,
A τ +δ t = A τ t + δv θ (A τ t , o t ) (3)
where δ is the integration step size.In our experiments, we set H as 4, and δ as 10.We aim to identify the most effective VLMs for downstream VLA fine-tuning and bridge the gap between foundational VLMs and their performance in downstream VLA tasks, thus shedding light on the future construction of embodied VLMs.Currently, the SimplerEnv benchmark, including Bridge (Walke et al., 2023b) and Google Robot (Jang et al., 2022;Brohan et al., 2023b) datasets, provides numerous training episodes (Over 5M images) and corresponding Real-to-Sim benchmarks.We thus majorly analyze the most effective data stream for VLA finetuning based on SimplerEnv.</p>
<p>EXPERIMENTS</p>
<p>PERFORMANCE ON EMBODIED REASONING CAPABILITY</p>
<p>Evaluation Datasets We conduct a comprehensive evaluation of embodied reasoning capabilities across a total of 12 benchmarks, covering a wide spectrum of tasks including embodied question answering, task planning, embodied grounding, spatial intelligence, and closed-loop simulation evaluation.The evaluated benchmarks consist of: ERQA (Team et al., 2025b), Ego-Plan2 (Qiu et al., 2024), Where2place (Yuan et al., 2024), Pointarena (Cheng et al., 2025), Paco-Lavis (Ramanathan et al., 2023), Pixmo-Points (Deitke et al., 2025), VSI-Bench (Yang et al., 2025b), RefSpatial-Bench (Zhou et al., 2025a), MMSI-Bench (Yang et al., 2025d), VLABench (Zhang et al., 2024), and EmbodiedBench (Yang et al., 2025c).For EmbodiedBench, we further assess performance in two simulation environments ALFRED (Shridhar et al., 2020) and Habitat (Szot et al., 2021).</p>
<p>Baselines Since our method, Vlaser is trained at two model scales -2B and 8B parameters, we categorize the compared baseline methods into three groups for a systematic evaluation:   (Brohan et al., 2023b), Octo-Base (Team et al., 2024), OpenVLA (Kim et al., 2024), RoboVLM (Liu et al., 2025) and SpatialVLA (Qu et al., 2025b) are from (Qu et al., 2025b) while the results of π 0 (Black et al., 2024) is from (open-pi zero, 2025).</p>
<p>Model</p>
<p>Carrot on plate Put eggplant in basket Spoon on towel Stack Cube Avg RT-1-X (35M) (Brohan et al., 2023b) 4.2% 0% 0% 0% 1.1% Octo-Base (93M) (Team et al., 2024) 8  (Bai et al., 2025), Embodied-R1-7B (Yuan et al., 2025), and RoboBrain2.0-7B(Team et al., 2025a).</p>
<p>The overall experimental results are presented in Table 1.As shown in Table 1, compared to the base models InternVL3-2B and InternVL3-8B used as initialization for our supervised finetuning, our Vlaser yields substantial improvements across all embodied reasoning capabilities, with particularly notable gains in embodied grounding and simulation-based evaluation.For example, the average score increases from 15.2 to 45.3 for the 2B model, and from 22.3 to 51.3 for the 8B model.These significant performance gains underscore the high quality and effectiveness of the Vlaser-6M dataset in enhancing embodied reasoning abilities.An interesting observation emerges that when finetuning on the same Vlaser-6M dataset, a smaller sized Vlaser-2B outperforms Vlaser-8B on simple point grounding tasks that require direct, short answers.Conversely, Vlaser-8B demonstrates superior performance on more complex tasks such as multi-step planning and closed-loop simulation evaluation, which often benefit from chain-of-thought (CoT) reasoning.This scaling behavior indicates the importance of appropriate model size selection based on target application requirements.</p>
<p>When compared against current state-of-the-art embodied-specific VLMs, including Robo-Brain2.0(Team et al., 2025a) and Embodied-R1 (Yuan et al., 2025), our method, Vlaser still achieves superior performance on the majority of benchmarks while remaining highly competitive on the remainder, ultimately attaining the highest overall score (by +10% margin overall).These results indicate that Vlaser delivers a well-balanced and robust capability set, performing strongly across multiple dimensions of embodied intelligence -from embodied question answering and state estimation to future action planning, visual grounding, spatial reasoning, and closed-loop simulation.Such comprehensive competence highlights its suitability as a versatile backbone for embodied AI brains.</p>
<p>In the following section, we further examine how these enhanced reasoning capabilities, embedded within VLMs, translate into improved performance when fine-tuned for downstream Vision-Language Action models (VLAs) in simulation manipulation scenarios.</p>
<p>PERFORMANCE ON DOWNSTREAM CLOSE-LOOP ROBOT TASKS</p>
<p>Finetuning Datasets We conduct extensive experiments on SimplerENV to evaluate the performance of Vlaser and Vlaser data engine on closed-loop robotic manipulation tasks.</p>
<p>SimplerENV is an open-source suite of purpose-built simulated environments with nearly 150K episodes for evaluating real-world robot manipulation policies in a scalable, reproducible way.It targets the key real-to-sim gaps -control and vision so that simulated performance reliably tracks real-robot outcomes.Across Google Robot and WidowX/BridgeData V2 setups, SimplerEnv reports strong real-vs-sim correlations and faithfully reflects behavior under distribution shifts, enabling fast, comparable policy assessment without full digital twins.As a result, SimplerENV has been widely adopted for evaluating VLA models and has proven to reliably reflect the performance of the models on the real robot platform.Baselines As mentioned before, we integrate Vlaser with an action expert utilizing flow matching for low-level control fine-tuning and evaluating the model on the WidowX and Google Robot datasets from SimplerEnv.We conduct experiments using InternVL3-2B, Vlaser, and the models based on Vlaser data engine for the in-domain robot data.Commonly, embodied reasoning can be marginally categorized into embodied QA (including planning), embodied grounding, and embodied spatial intelligence.We thus construct corresponding embodied VLMs based on the corresponding data stream: Vlaser-QA, Vlaser-Grounding, Vlaser-Spatial.Notably, all series of Vlaser models are based on the same architecture with 2B size.</p>
<p>The full experimental results are presented in Table 2 and Table 3.We observe the proposed vision-language-action architecture based on InternVL3-2B is capable of low-level robot control and achieves comparable performance to many previous approaches, though we utilize a 2B backbone.</p>
<p>Particularly, we did not observe any clean improvement when we used the Vlaser-2B as our initial backbone on both WidowX and Google Robot, while we achieved significant performance promotion with the Vlaser-QA, although the architecture and model size are the same between the two models.This observation illustrates the effectiveness of our Vlaser data engine, and meanwhile identifies that there is no positive correlation between common embodied reasoning benchmarks and the performance of closed-loop control of the lower level for the specific embodiment of the robot.We reckon it is the domain shift between the internet data and the corresponding robot embodiment (e.g., WidowX or Google Robot), and we find that the enhanced abilities in the same observation domain effectively facilitate the closed-loop success rate.Therefore, it is urgent to shrink the domain gap between the foundational models and real-world robot embodiment for closed-loop task completion.</p>
<p>In addition, we conducted a simple ablation study on the Google Robot Tasks to evaluate the effectiveness of different data annotations.The experimental results indicate that incorporating all types of data leads to significant improvements, achieving performance comparable to the baseline.</p>
<p>We attribute these improvements primarily to the reduction of the vision observation domain shift.</p>
<p>RELATED WORK</p>
<p>Vision-Language Model for Embodied Reasoning Enhancing the embodied reasoning capabilities of current state-of-the-art Vision-Language Models (VLMs) has emerged as a critical research direction.These capabilities encompass a range of competencies, including grounding (Yuan et al., 2024;Deitke et al., 2025;Cheng et al., 2025) which identifies affordances that enable embodied agents to perform manipulations, spatial intelligence (Yang et al., 2025b;d), such as object counting and spatial relationship understanding, as well as task planning (Chen et al., 2023;Qiu et al., 2024), which involves assessing the current state and determining subsequent actions to be executed.Gemini Robotics-ER (Team et al., 2025b) integrates embodied reasoning into its core visual-language model (VLM), demonstrating strong generalization across a variety of tasks such as 3D scene perception, visual pointing, state estimation, and affordance prediction.In parallel, a number of data-driven methodologies have emerged to support such reasoning capabilities.For instance, Cosmos-Reason1 (NVIDIA et al., 2025a), VeBrain (Luo et al., 2025), MolmoAct (Lee et al., 2025), and EmbodiedOneVision (Qu et al., 2025a) each contribute curated datasets specifically designed for embodied reasoning tasks, emphasizing aspects such as multi-modal instruction following and action-aware visual-language alignment.Further advancing this direction, several frameworksincluding the RoboBrain series (Ji et al., 2025;Team et al., 2025a), Embodied R1 (Yuan et al., 2025), and Robix (Fang et al., 2025) incorporate Reinforcement Fine-Tuning (RFT) and synthesize spatiotemporal reasoning datasets enriched with structured thought traces.These approaches aim to enhance models' capacity for causal reasoning and long-horizon task decomposition.Distinguished from these prior efforts, our work not only achieves competitive, and in some cases superior performance on established embodied reasoning benchmarks, but also provides an in-depth analysis of the synergistic relationship between pre-trained VLMs and downstream Vision-Language Action Models (VLAs), offering insights that bridge model capabilities and real-world deployment.</p>
<p>Vision-Language-Action models.Developing a generalist model remains a central challenge in robotics.Inspired by the strong generalization abilities of vision-language models (VLMs) (OpenAI, 2023;Chen et al., 2024;Bai et al., 2025;Team et al., 2023) trained on large-scale internet data, researchers have proposed vision-language-action (VLA) models, which have demonstrated promising performance (Brohan et al., 2023a;Kim et al., 2024;Qu et al., 2025b;Hou et al., 2025).</p>
<p>Compared to traditional robot policies, VLA models are pretrained on large-scale robotics datasets and exhibit improved generalization across object categories and visual observations.Building on recent progress, researchers have incorporated techniques such as diffusion (Ho et al., 2020;Rombach et al., 2022;Peebles &amp; Xie, 2023), flow matching (Lipman et al., 2023b), and mixture-of-experts (MoE) (Shazeer et al., 2017) into VLA models, and have adopted larger, more capable VLMs as their backbones.These advances have enabled VLA models to tackle a wider range of complex real-world manipulation tasks.Nevertheless, current VLA models remain limited in their generalization.In particular, they have not yet reached the level of general reasoning exhibited by VLMs, such as decompose a complex task into manageable sub-steps and complete the task in a zero-shot manner.</p>
<p>Efforts have been made to enhance specific reasoning abilities in embodied scenarios (Team et al., 2025b;Ji et al., 2025;NVIDIA et al., 2025a).In parallel, several studies (Intelligence et al., 2025;Driess et al., 2025;Zhou et al., 2025b) explore unified training frameworks for VLMs and VLAs to leverage the reasoning capacity of VLMs.However, the relationship between high-level multimodal reasoning and low-level control performance remains largely unexplored.It is still unclear which specific multimodal abilities such as spatial understanding, grounding, or planning and which types of training data most effectively enhance the control capabilities of a VLA model.In this work, we take an initial step toward analyzing this relationship by a systematic evaluation, and also propose the latest foundational model with strong embodied multimodal understanding and action prediction.</p>
<p>CONCLUSION AND DISCUSSION</p>
<p>We introduce Vlaser, a foundational vision-language-action model that extends vision-language models with embodied reasoning and end-to-end robot control capabilities.Powered by the Vlaser-6M dataset, the model establishes a new state of the art across a wide range of embodied reasoning benchmarks, including planning, grounding, spatial reasoning, and simulation-based tasks.Moreover, Vlaser reveals the most effective data streams for downstream VLA through its curated data pipeline, achieving state-of-the-art performance on Bridge and competitive results on Google Robot for end-to-end robot control.</p>
<p>In this work, we reveal that current embodied reasoning benchmarks exhibit a significant domain gap when compared to real-world robots.This core domain shift arises from the observation that robots have a fundamentally different viewpoint from that of internet datasets.Additionally, there are inherent limitations due to the lack of sufficient data from the robot's perspective, despite the abundance of vision datasets available.Therefore, we argue that it is essential to develop alignment techniques to bridge the domain gap in representations between the robot's viewpoint and that of internet datasets.</p>
<p>Enshen  et al., 2025)(InternVL3-2B and InternVL3-8B).To maximize adaptation to embodied reasoning tasks, we keep all parameters trainable, including those in the large language model, the vision-language projector, and the visual encoder, enabling comprehensive end-to-end learning.Further details regarding the training setup, including hyperparameters and optimization settings, are provided in Table 4.</p>
<p>Table 4: Hyper-parameters used in the VLM pretraining of Vlaser.</p>
<p>While using Vlaser as the base model for downstream VLA Policy fine-tuning, we optimize all parameters within both the VLM and the Action Expert.Additionally, we conduct comparative experiments using several different versions of base models, including InternVL3-2B, etc. Detailed information and related parameter settings can be found in Table 5.</p>
<p>A.2 DATA GENERATION DETAILS</p>
<p>Embodied Grounding Data To further enhance embodied grounding capabilities, we generate an additional 300k high-quality data samples from the SA-1B dataset (Kirillov et al., 2023).The data generation process consists of two main stages.First, we convert segmentation masks into bounding boxes and point annotations: bounding boxes are derived by computing the minimal axisaligned rectangle enclosing each mask, while point annotations are obtained by randomly sampling a coordinate within the mask region.To ensure annotation quality, we apply an IoU threshold of 0.9 to select high-precision masks; masks with lower IoU values are either excluded or assigned reduced sampling weight.From the over 1 billion available masks, we initially sample 1 million candidate instances.In the second stage, we employ a two-step captioning and refinement pipeline.Coarse captions are first generated using BLIP-2 (Li et al., 2023), followed by a filtering and refinement process using Qwen2.5-VL-7B(Bai et al., 2025) to eliminate low-quality items and produce more accurate and detailed descriptions.This rigorous pipeline ultimately yields 300k high-quality data samples tailored for embodied grounding tasks, significantly expanding the diversity and precision of our training corpus.</p>
<p>Spatial Reasoning Data To enhance spatial intelligence capabilities, we manually construct a dataset of 100k 3D spatial perception samples derived from ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), and ARKitScenes (Baruch et al., 2021).Following methodologies established in prior data engines (Deng et al., 2025;Fan et al., 2025), we utilize both the 3D point cloud and video sequences of each scene to construct a spatio-temporal scene graph.This graph encapsulates structural and semantic information such as overall scene dimensions, room center coordinates, object category counts, and precise 3D bounding boxes for every object instance.Based on this representation, we generate spatial reasoning questions that probe layout properties and inter-object relationships.These include queries about the object counts, absolute and relative distances, object and room sizes, relative directions between objects, and other spatial attributes, using the same question template in VSI-Bench (Yang et al., 2025b).</p>
<p>Planning Data To improve the model's ability to comprehend complex instructions and execute tasks with environmental feedback, we curate additional planning data within the Habitat simulator (Szot et al., 2021).Specifically, we initialize each planning task following the annotations of LLaRP (Szot et al., 2024), which specify both the task goals and the set of permissible actions.An LLM agent based on gpt-4o (OpenAI, 2025) is then deployed to roll out the task.During each rollout, we record the task instruction, the sequence of actions taken, and the environment's feedback, including observations and success signal for each action.Both the executed action trajectories and the corresponding feedback are retained.Only trajectories that successfully accomplish the task are included in the final training set, providing rich paired data of instructions, execution processes, and environment responses for enhancing the model's planning capabilities in a complex environment.</p>
<p>In-Domain Data for downstream VLAs We generate 2 million in-domain multimodal data samples to facilitate direct transfer learning for downstream Vision-Language Agents (VLAs) during finetuning.These data are collected from two distinct robotic platforms: the WidowX Robot (Walke et al., 2023a) and the Google Robot (Brohan et al., 2023b;a) within the SimplerEnv (Li et al., 2024c) simulation framework.The dataset mainly encompasses three systematically designed questionanswer types: 1) General QA, which queries the robot's current state and requests next few action plans; 2) Grounding QA, which requires the robot to localize points and bounding boxes as the actionable affordances; 3) Spatial Reasoning QA, which probes relative spatial relationships and geometric properties of objects in the scene.Detailed prompt templates and representative examples for each data category are provided in Figure 3 (General QA), Figure 4 (Grounding QA), and Figure 5 (Spatial Reasoning QA), respectively.We use Qwen2.5VL-7B(Bai et al., 2025) as the base model to generate the above data items.Question:</p>
<p>You are a robot operating in a home.Given a task, you must accomplish the task using a defined set of actions to achieve the desired outcome.Now the human instruction is: Put a microwaved tomato slice on the counter.You need to output the reasoning process and an action to execute.</p>
<p>Vlaser-8B: Reasoning-step-1: To put a microwaved tomato slice on the counter, I need to start by slicing a tomato.To do this, I require a knife.In the image, there is a knife visible on the counter near the toaster.Finding the knife is the logical next step because it is essential for cutting the tomato into slices.Once I have the knife, I can proceed with slicing the tomato.Therefore, my next action is to find the knife.Action: find a knife.Reasoning-step-2: I have identified the knife on the counter near the sink.</p>
<p>Picking up the knife is the next logical step because it is necessary for slicing the tomato.Once I have the knife, I can proceed with cutting the tomato into slices.This aligns with the goal of preparing a tomato slice for microwaving and then placing it on the counter.Action: pick up the knife.</p>
<p>Reasoning-step-3: I have successfully located the knife in the drawer.Now, I need to find a tomato to slice.The image shows a kitchen counter with various items, but no tomato is immediately visible.Tomatoes are commonly stored in the refrigerator or on the counter.Since the refrigerator is partially visible on the left side of the image, it makes sense to check there first.If the tomato isn ẗ in the fridge, I can then look around the counter more thoroughly.Therefore, my next step is to find a tomato.Action: find a tomato.... Reasoning-step-9: The current visual state shows the microwave with its door closed, and the tomato slice is on the counter.The task is to place a microwaved tomato slice on the counter, which has already been accomplished.Given that the tomato slice is already on the counter, the next logical step is to confirm its placement by finding a suitable counter top.This ensures that the tomato slice is securely and appropriately positioned for the task.Therefore, the next step is to find a counter top.</p>
<p>Action: find a counter top.</p>
<p>Reasoning-step-10: The current visual state shows a microwave with its door closed, and two tomatoes are visible on the counter.One tomato is already sliced, indicating that the previous steps involving slicing and microwaving have been completed.Since the tomato is currently being held, the next logical step is to put it down on the counter.This action will free up the hand to handle the tomato slice and place it on the counter as required by the task.Therefore, the appropriate next action is to put down the tomato.Action: put down the tomato.</p>
<p>Figure 1 :
1
Figure 1: Overall framework, capabilities, and evaluation of Vlaser.Top-left: Composition of the Vlaser-6M dataset, featuring multi-task embodied data-including QA, grounding, spatial reasoning, and planning-along with in-domain simulation-sourced pairs.Top-right: A LiDAR visualization illustrating the state-of-the-art embodied reasoning capability of the Vlaser VLM.Bottom-left: The pre-trained Vlaser VLM significantly accelerates convergence in downstream Vision-Language Action model (VLA) policy learning on WidowX platform (Walke et al., 2023a).Bottom-right: Successful closed-loop operation of an agent powered by Vlaser within the SimplerEnv benchmark (Li et al., 2024b).</p>
<p>Figure 2 :
2
Figure 2: An illustration of Vlaser architecture.Vlaser includes two components and corresponding training phases: 1) the Multimodal Pretraining is for embodied reasoning enhancement based on the corresponding data engine; 2) VLA training is performed on the action expert module, which handles low-level control based on flow matching action generation.</p>
<p>1) State-ofthe-art closed-source models, includingGPT-4o (OpenAI, 2025),Claude-3.7-Sonnet(Anthropic,  2025), and Gemini-2.5-Pro(Comanici et al., 2025); 2) Small-scale MLLMs (2B -3B parameters), comprising ChatVLA-2B(Zhou et al., 2025b), InternVL3-2B (Zhu et al., 2025), Qwen2.5-VL-3B(Bai et al., 2025), Embodied-R1-3B(Yuan et al., 2025), and RoboBrain2.0-3B(Teamet al.,</p>
<p>Figure 4 :
4
Figure 4: An illustration of Vlaser-6M data engine for in-domain embodied grounding QA sample in SimplerEnv.</p>
<p>Table 1 :
1
Comparison with existing close-sourced, open-sourced and embodied-related VLMs on 12 general embodied reasoning benchmarks, spanning from embodied QA, planning, embodied grounding to spatial intelligence and close-loop simulation evaluation.Avg denotes the normalized average performance of all the benchmarks.The best, second best and third best score among all the baselines are colored in red, orange and yellow.
ModelQA ERQA Ego-Plan2 Where2place Pointarena Paco-Lavis Pixmo-Points VSIBench RefSpatial MMSIBench VLABench EB-ALFRED EB-Habitat Planning Embodied Grounding Spatial Intelligence SimulationAvg▼ Closed-source MLLMs:GPT-4o-2024112047.041.829.129.516.210.842.58.830.339.356.359.034.2Claude-3.7-Sonnet35.541.325.622.212.47.247.07.730.241.767.065.733.6Gemini-2.5-Pro55.042.939.962.845.525.843.430.336.934.862.753.044.4▼ Small Size MLLMs:ChatVLA-2B34.325.33.710.110.22.12.40.920.10.00.00.09.1InternVL3-2B31.530.95.27.115.41.431.51.825.319.41.312.015.2Qwen2.5VL-3B35.330.331.041.767.436.627.924.926.531.36.719.731.6Embodied-R1-3B36.036.035.145.368.336.628.028.526.024.67.019.332.5RoboBrain2.0-3B37.341.864.246.067.636.928.846.526.818.10.010.035.3Vlaser-2B35.838.374.057.872.544.657.543.023.623.142.330.745.3▼ Medium Size MLLMs:Magma-8B29.327.910.929.615.310.112.74.526.28.50.00.014.6Cosmos-Reason1-7B 39.326.911.440.861.823.633.95.426.435.54.05.326.2VeBrain-7B38.327.333.138.955.120.139.920.628.325.95.712.328.8InternVL3-8B35.340.010.014.221.15.742.15.625.724.719.023.722.3Qwen2.5VL-7B39.329.731.156.368.043.538.232.125.936.410.018.335.7Embodied-R1-7B38.337.169.551.269.939.238.631.128.135.510.019.038.9RoboBrain2.0-7B42.033.263.649.573.137.836.132.526.56.614.029.337.0Vlaser-8B41.053.469.560.368.340.560.359.227.245.650.040.051.3</p>
<p>Table 2 :
2
SimplerEnv Evaluation on WidowX Robot Tasks.Avg indicates the average success rate among the four tasks.InternVL3-2B, Vlaser and Vlaser with Bridge Q&amp;A indicates the base model we select to fine-tune on the WidowX Robot Tasks.Particularlly, Vlaser-QA refers to the model which is fine-tuned on the Bridge question-answer pairs dataset.Model sizes are indicated within parentheses.The result of RT-1-X</p>
<p>Table 3 :
3
Comparison with existing methods in SimplerEnv on Google Robot tasks.Avg indicates the average success rate among the three tasks.In the last five lines in the table, we use the name of base model to indicate different evaluation settings.Vlaser-QA indicates the model which is fine-tuned on the Fractal question-answer pair dataset.Vlaser-Spatial and Vlaser-Grounding represent the model fine-tuned on the Spatial Reasoning Data and Embodied Grounding Data separately.The details of different sub-datasets can be find in the AppendixA.2.
Model sizes are indicated within</p>
<p>Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan  Wang, Tiejun Huang, Lu Sheng, et al.Roborefer: Towards spatial referring with reasoning in vision-language models for robotics.arXivpreprint arXiv:2506.04308,2025a.Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al.Chatvla: Unified multimodal understanding and robot control with vision-language-action model.arXiv preprint arXiv:2502.14420,2025b.
Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, YuchenDuan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes foropen-source multimodal models. arXiv preprint arXiv:2504.10479, 2025.A APPENDIXA.1 TRAINING DETAILSOur Vlaser is optimized in a fully supervised finetuning (SFT) manner based on InternVL3 series (Zhu</p>
<p>Table 10 :
10
Simulation planning Examples.</p>
<p>Simulation Planning: Example #1 from EmbodiedBench.</p>
<p>A.3 SIMULATION EVALUATION DETAILSWe fine-tune and evaluate the VLA models using various base models within the SimplerEnv simulation environment.To ensure fair evaluation, we use checkpoints with the same number of iterations for the WidowX Robot Task and the Google Robot Task, respectively.Specifically, for the WidowX Robot Tasks, we use a checkpoint after 45,390 iterations, while for the Google Robot Tasks, we use a checkpoint after 36,970 iterations.During evaluation, we utilize a single image and select an action chunk of size 2 for execution.In the flow matching configuration, we use 10 inference steps during the inference phase and apply Euler method as numerical integration method.We evaluate a sufficient number of samples to ensure the reliability of the tests.The exact number of test samples for each task is shown in the Table6.Qualitative Samples in SimplerEnv. Figure6and Figure7present qualitative examples from the evaluations conducted in the SimplerEnv.Specifically, Figure6showcases the performance of multiple models on the WidowX Robot Task, while Figure7demonstrates the results on the Google Robot Task.From these results, it is evident that, under the same test conditions, baseline models often fail the entire task due to slight positional errors.In contrast, our model shows significant improvements in this regard.Whether incorporating in-domain question-answer pair data, spatial intelligence data, or grounding data, the performance of the model shows clear enhancement.These results validate the effectiveness of our approach.GeneralWidowX Robot Google RobotQuestion: How can the robot arm effectively fold the green cloth from left to right as instructed?Answer: To fold the green cloth from left to right, the robot arm should first position itself so that the left edge of the cloth is aligned with the starting point of the fold.The arm should then move in a controlled manner, applying gentle pressure to fold the cloth over to the right edge.It's important to ensure that the cloth is not torn or damaged during the folding process.The robot arm should also be equipped with sensors to detect the cloth's texture and adjust its movements accordingly to maintain a smooth and accurate fold.Additionally, the robot arm should be programmed to stop folding once the right edge is reached, ensuring that the cloth is neatly folded without any wrinkles or creases.Question: What safety measures should be taken when using a robot arm to pick up cans like the can shown in the image?Answer: When using a robot arm to pick up cans, several safety measures should be considered to ensure the task is performed safely and efficiently.First, it's crucial to ensure that the robot arm is properly calibrated and that the gripper is securely attached to the arm.This prevents accidental drops or damage to the can.Additionally, the robot arm should be programmed with gentle and precise movements to avoid any impact that could damage the can or the arm itself.It's also important to ensure that the robot arm is operating within its designated safety boundaries to prevent any unintended movements that could cause harm.Furthermore, the environment should be free of obstacles that could interfere with the robot arm's movement, and the area should be well-lit to ensure the robot arm can accurately identify and grasp the can.Lastly, it's advisable to have a human supervisor or safety system in place to monitor the robot arm's actions and intervene if necessary.Make sure the question focuses on spatial reasoning, object relationships, positioning, or geometric understanding, and the answer provides precise spatial information about the robot's environment.Question: What is the relative position of the drawer compared to the robot arm?Answer: The drawer is positioned directly beneath the robot arm, with the arm's gripper currently interacting with the surface of the drawer.The drawer is oriented horizontally, and its front edge is aligned with the front edge of the robot arm's gripper.The drawer appears to be slightly tilted forward, suggesting that the robot arm is in the process of opening it.Question: What is the relative position of the coke can compared to the apple on the table?Answer: The coke can is positioned to the left of the apple on the table.The can is closer to the top left corner of the table, while the apple is slightly below and to the right of the can.Question:Point to the tool people use for writing.Vlaser-8B: <point>[[701, 374]]</point>.Embodied Grounding: Example #2 from Where2Place.Question:Please point out the free space between the black water bottle and the pot lid.Vlaser-8B: <point>[[293, 560]]</point>.
Claude 3.7 sonnet system card. 2025Sonnet Anthropic</p>
<p>Cosmos-reason1: From physical common sense to embodied reasoning. Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, arXiv:2503.155582025arXiv preprint</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025arXiv preprint</p>
<p>Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, arXiv:2111.08897A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. 2021arXiv preprint</p>
<p>pi_{0}: A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich, 2023a</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Robotics: Science and Systems XIX. 2023b</p>
<p>Owmm-agent: Open world mobile manipulation with multi-modal agentic data synthesis. Junting Chen, Haotian Liang, Lingxiao Du, Weiyun Wang, Mengkang Hu, Yao Mu, Wenhai Wang, Jifeng Dai, Ping Luo, Wenqi Shao, arXiv:2506.042172025aarXiv preprint</p>
<p>Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Pannag R Sanketi, Ken Goldberg, arXiv:2505.15517Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets. 2025barXiv preprint</p>
<p>Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu, 2023CoRR</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang, Navila, arXiv:2412.04453Legged robot vision-language-action model for navigation. 2024arXiv preprint</p>
<p>Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, arXiv:2505.09990Probing multimodal grounding through language-guided pointing. 2025arXiv preprint</p>
<p>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Ron Chrisley, ; Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, arXiv:2507.06261Artificial intelligence. 14912003. 2025arXiv preprintEmbodied artificial intelligence</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, arXiv:2506.18385A comprehensive dataset for spatial reasoning in vision-language models. 2025arXiv preprint</p>
<p>Knowledge insulating vision-languageaction models: Train fast, run fast, generalize better. Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Homer Allen Z Ren, Quan Walke, Lucy Xiaoyang Vuong, Shi, arXiv:2505.237052025arXiv preprint</p>
<p>Vlm-3r: Vision-language models augmented with instructionaligned 3d reconstruction. Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, arXiv:2505.202792025arXiv preprint</p>
<p>Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li, arXiv:2509.01106Robix: A unified model for robot interaction, reasoning and planning. 2025arXiv preprint</p>
<p>Path planning and trajectory planning algorithms: A general overview. Motion and operation planning of robotic systems: Background and practical approaches. Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti, Renato Vidoni, 2015</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Scaling diffusion transformer for generalist vision-language-action policy. Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, arXiv:2503.197572025arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022</p>
<p>5}: a vision-languageaction model with open-world generalization. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, arXiv:2504.160542025arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Robobrain: A unified brain model for robotic manipulation from abstract to concrete. Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang, Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR). the Computer Vision and Pattern Recognition Conference (CVPR)June 2025</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Cubify anything: Scaling indoor 3d object detection. Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, Afshin Dehghan, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, arXiv:2508.07917Action reasoning models that can reason in space. 2025arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Muep: A multimodal benchmark for embodied planning with foundation models. Kanxue Li, Baosheng Yu, Qi Zheng, Yibing Zhan, Yuhui Zhang, Tianle Zhang, Yijun Yang, Yue Chen, Lei Sun, Qiong Cao, Li Shen, Lusong Li, Dapeng Tao, Xiaodong He, 10.24963/ijcai.2024/15Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. Kate Larson, the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-2482024</p>
<p>Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, Ted Xiao, arXiv:2405.05941Evaluating real-world robot manipulation policies in simulation. 2024barXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, arXiv:2405.059412024carXiv preprint</p>
<p>Flow matching for generative modeling. Yaron Lipman, Ricky T Q Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le, 2023a</p>
<p>Flow matching for generative modeling. Yaron Lipman, Ricky T Q Chen, Heli Ben-Hamu, Maximilian Nickel, Matthew Le, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202336</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, 2025</p>
<p>Visual embodied brain: Let multimodal large language models see, think, and control in spaces. Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, arXiv:2506.001232025arXiv preprint</p>
<p>EmbodiedGPT: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Cosmos-reason1: From physical common sense to embodied reasoning. Alisson Nvidia, Hannah Azzolini, Prithvijit Brandon, Huayu Chattopadhyay, Jinju Chen, Yin Chu, Jenna Cui, Yifan Diamond, Francesco Ding, Rama Ferroni, Jinwei Govindaraju, Siddharth Gu, Imad El Gururani, Zekun Hanafi, Jacob Hao, Jingyi Huffman, Brendan Jin, Rizwan Johnson, George Khan, Elena Kurian, Nayeon Lantz, Zhaoshuo Lee, Xuan Li, Tsung-Yi Li, Yen-Chen Lin, Ming-Yu Lin, Andrew Liu, Yun Mathau, Lindsey Ni, Wei Pavao, David W Ping, Misha Romero, Shuran Smelyanskiy, Lyne Song, Andrew Z Tchapmi, Boxin Wang, Haoxiang Wang, Fangyin Wang, Jiashu Wei, Yao Xu, Xiaodong Xu, Zhuolin Yang, Xiaohui Yang, Zhe Zeng, Zhang, 2025a</p>
<p>Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. Nikita Nvidia, Johan Cherniadev, Xingye Bjorck Andfernando Castañeda, Runyu Da, Ding, " Linxi, Yu Jim" Fan, Dieter Fang, Fengyuan Fox, Spencer Hu, Joel Huang, Zhenyu Jang, Jan Jiang, Kaushil Kautz, Lawrence Kundalia, Zhiqi Lao, Zongyu Li, Kevin Lin, Guilin Lin, Edith Liu, Loic Llontop, Ajay Magne, Avnish Mandlekar, Soroush Narayan, Scott Nasiriany, You Reed, Guanzhi Liang Tan, Zu Wang, Jing Wang, Qi Wang, Jiannan Wang, Yuqi Xiang, Yinzhen Xie, Zhenjia Xu, Seonghyeon Xu, Zhiding Ye, Ao Yu, Hao Zhang, Yizhou Zhang, Zhao, ArXiv Preprint. March 2025b. 2025open-pi zero. open-pi-zero</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAI</p>
<p>Gpt-4o system card. 2025OpenAI</p>
<p>Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun, arXiv:2504.01805Spacer: Reinforcing mllms in video spatial reasoning. 2025arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Egoplan-bench2: A benchmark for multimodal large language model planning in real-world scenarios. Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu, arXiv:2412.044472024arXiv preprint</p>
<p>Embodiedonevision: Interleaved vision-text-action pretraining for general robot control. Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, arXiv:2508.211122025aarXiv preprint</p>
<dl>
<dt>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-languageaction model. 2025barXiv preprint</dt>
<dd>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2025</p>
</dd>
</dl>
<p>Parts and attributes of common objects. Anmol Vignesh Ramanathan, Vladan Kalia, Yi Petrovic, Baixue Wen, Baishan Zheng, Rui Guo, Aaron Wang, Rama Marquez, Abhishek Kovvuri, Kadian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Highresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Multimodal long-horizon reasoning for robotics. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, J Nikhil, Joshi, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Noam Shazeer, Azalia Mirhoseini, Andy Krzysztof Maziarz, Quoc Davis, Geoffrey Le, Jeff Hinton, Dean, International Conference on Learning Representations. 2017</p>
<p>Jingjing Gong, and Xipeng Qiu. World-aware planning narratives enhance large vision-language model planner. Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, 2025</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Emma-x: An embodied multimodal action model with grounded chain of thought and look-ahead spatial reasoning. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, Deepanway Tan, Soujanya Ghosal, Poria, arXiv:2412.119742024arXiv preprint</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Large language models as generalizable policies for embodied tasks. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, Devon Hjelm, Alexander T Toshev, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. Mingyu Baai Robobrain Team, Huajie Cao, Yuheng Tan, Minglan Ji, Zhiyu Lin, Zhou Li, Pengwei Cao, Enshen Wang, Yi Zhou, Han, arXiv:2507.020292025aarXiv preprintet al. Robobrain 2.0 technical report</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025barXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025carXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023a</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023b</p>
<p>Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan, arXiv:2506.099652025arXiv preprint</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2305.03716Embodied task planning with large language models. 2023arXiv preprint</p>
<p>Magma: A foundation model for multimodal ai agents. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025a</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025b</p>
<p>Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, arXiv:2502.095602025carXiv preprint</p>
<p>Mmsi-bench: A benchmark for multi-image spatial intelligence. Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, arXiv:2505.237642025darXiv preprint</p>
<p>Scannet++: A highfidelity dataset of 3d indoor scenes. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, Angela Dai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao, arXiv:2508.139982025arXiv preprint</p>
<p>Han-Ye Zhang, Wei-Ming Lin, Ai-Xia Chen, Path planning for the mobile robot: A review. Symmetry. 201810450</p>
<p>From flatland to space: Teaching vision-language models to perceive and reason in 3d. Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, arXiv:2503.229762025arXiv preprint</p>
<p>Vlabench: A large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks. Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, arXiv:2412.181942024arXiv preprint</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, arXiv:2412.103452024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
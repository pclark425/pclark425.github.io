<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9755 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9755</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9755</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-abd42a2ae6dfe4b3a4e657c9239a903a904568cb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/abd42a2ae6dfe4b3a4e657c9239a903a904568cb" target="_blank">Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification and fine-tuning with application-specific training data achieves superior performance in all cases.</p>
                <p><strong>Paper Abstract:</strong> Generative AI offers a simple, prompt-based alternative to fine-tuning smaller BERT-style LLMs for text classification tasks. This promises to eliminate the need for manually labeled training data and task-specific model training. However, it remains an open question whether tools like ChatGPT can deliver on this promise. In this paper, we show that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification. We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs across a diverse set of classification tasks (sentiment, approval/disapproval, emotions, party positions) and text categories (news, tweets, speeches). We find that fine-tuning with application-specific training data achieves superior performance in all cases. To make this approach more accessible to a broader audience, we provide an easy-to-use toolkit alongside this paper. Our toolkit, accompanied by non-technical step-by-step guidance, enables users to select and fine-tune BERT-like LLMs for any classification task with minimal technical and computational effort.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9755",
    "paper_id": "paper-abd42a2ae6dfe4b3a4e657c9239a903a904568cb",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005067499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Fine-Tuned 'Small' LLMs (Still) SignifICANTLY OUTPERFORM ZERO-SHOT GENERATIVE AI MODELS IN TEXT CLASSIFICATION</h1>
<p>Martin Juan José Bucher<br>Stanford University<br>mnbucher@stanford.edu</p>
<p>Marco Martini<br>University of Zurich<br>marco.martini@uzh.ch</p>
<h4>Abstract</h4>
<p>Generative AI offers a simple, prompt-based alternative to fine-tuning smaller BERT-style LLMs for text classification tasks. This promises to eliminate the need for manually labeled training data and task-specific model training. However, it remains an open question whether tools like ChatGPT can deliver on this promise. In this paper, we show that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification. We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs across a diverse set of classification tasks (sentiment, approval/disapproval, emotions, party positions) and text categories (news, tweets, speeches). We find that fine-tuning with application-specific training data achieves superior performance in all cases. To make this approach more accessible to a broader audience, we provide an easy-to-use toolkit alongside this paper. Our toolkit, accompanied by non-technical step-by-step guidance, enables users to select and fine-tune BERT-like LLMs for any classification task with minimal technical and computational effort.</p>
<h2>1 Introduction</h2>
<p>The last decade has seen a paradigm shift in Natural Language Processing (NLP) with the advent of pre-trained Large Language Models (LLMs). These models have not only achieved previously unseen performance on a wide range of benchmarks. They have also shifted expectations for the future of artificial intelligence (AI) more generally.
Traditionally, leveraging LLMs involves two sequential steps: First, models are pre-trained on large text corpora to instill general language understanding. Pre-training is typically performed by specialized experts who then make the pre-trained models publicly available by providing all parameters and model states via checkpoints. Subsequently, these models can be further fine-tuned for specific tasks using smaller, dedicated training datasets, a task usually carried out by end-users who need to implement deep learning code and provide application-specific training data in the process. LLMs of this kind, like BERT or RoBERTa, have since been the state-of-the-art in numerous NLP tasks. Empirical evidence has shown that fine-tuned LLMs outperform traditional methods such as dictionaries, bag-of-words models, or approaches based on word embeddings [13, 24, 45, 11].
More recently, with models continuously growing in size and complexity, instruction-tuned generative AI models such as ChatGPT and Claude Opus have emerged. These models are not only significantly larger, but also more versatile than BERT-style models: While generative AI models are also pre-trained, they can be directly prompted via text commands to perform tasks without the need for a further fine-tuning step. As a result, generative AI promises a straightforward and intuitive user interaction while challenging the traditional approach, which requires fitting a model to labeled data. Although the empirical picture is still far from decisive, first studies suggest that generative AI models are already outperforming fine-tuned LLMs in text classification tasks [18, 46].
In this paper, we contribute to the ongoing debate by systematically comparing the text classification performance of three prompt-instructed generative AI models (ChatGPT-3.5, ChatGPT-4, and Claude Opus) as well as Facebook's BART with that of several smaller, fine-tuned LLMs.</p>
<p>Our analysis spans a wide range of text classification tasks, including sentiment analysis, approval/disapproval recognition, emotion detection, and identification of political party positions. We also evaluate model performance across different text categories, such as news articles, tweets, and political speeches. Specifically, we conduct four case studies: (i) sentiment analysis of The New York Times coverage on the U.S. economy, (ii) stance classification of tweets about Brett Kavanaugh’s nomination to the U.S. Supreme Court, (iii) emotion detection in German political texts, and (iv) multi-class stance classification of nationalist party positions on European integration.</p>
<p>Our results demonstrate that fine-tuning smaller BERT-style models significantly outperforms generative AI models such as ChatGPT and Claude Opus (used in a "zero-shot" fashion) across all four applications when moderate amounts of training data for fine-tuning are provided. This tendency is especially pronounced for more specialized, non-standard classification tasks. Overall, these findings suggest that smaller, fine-tuned LLMs (still) constitute the state-of-the-art in text classification.</p>
<p>Our results also imply that model size and complexity are no sufficient substitute for application-specific training data. For text classification tasks, the lightweight, tailored approach of fine-tuning small LLMs via training data remains preferable to the heavy-duty, one-size-fits-all approach of zero-shot prompting generative AI models.</p>
<p>Given our clear-cut results in favor of fine-tuning, we make available an easy-to-use toolkit with this paper. Presented as a simple Jupyter Notebook built on top of Hugging Face, our toolkit simplifies the process of fine-tuning LLMs, which typically requires deep learning and programming experience. It allows users to select and fine-tune smaller pre-trained LLMs for any classification problem (e.g., sentiment) or text category (e.g., news) with minimal requirements.</p>
<p>Our toolkit supports different languages and handles both binary and non-binary classification problems. It includes pre-implemented methods to address class imbalance, common in many applications (e.g., news texts typically have more negative than positive sentiment). While the notebook allows for the configuration and optimization of deeplearning hyperparameters, it comes with default hyperparameters that deliver strong performance out-of-the-box. Computationally intensive hyperparameter tuning is therefore an option but not required. The modular design of the toolkit allows for the integration of additional/future model releases. We further provide a detailed documentation and step-by-step user guidance.</p>
<p>To further facilitate use and adoption of fine-tuning, we provide a non-technical introduction to the functioning principles of LLMs, clarify how these models expand on earlier text-as-data methods, and explain why this results in significant performance gains (Section 3). We also discuss current research areas, potential future trends in NLP and AI, and the opportunities of few-shot learning (Section 6).</p>
<h1>2 Related Work and Contribution</h1>
<p>A growing literature evaluates and compares the performance of fine-tuned and prompt-based LLMs [5, 6, 8, 10, 41, 15, 18, 47, 46]. However, some of this work does not specifically address text classification, which is the primary focus of our paper, but instead concentrates on various other text comprehension tasks [6, 41]. Other studies primarily compare the performance of different fine-tuning methods [10], or evaluate various zero-shot approaches against each other [47].</p>
<p>The existing studies that come closest to our work, as they compare fine-tuned and prompt-based models for text classification tasks, are [5] and [15] as well as [46] and [18]. However, these papers provide conflicting evidence as to which approach is superior for text classification.</p>
<p>For example, [15] compare a fine-tuned RoBERTa model with prompt-based models like GPT-3.5 and Meta’s Llama models across different classification tasks. The authors find that fine-tuned models (trained on the entire dataset) generally outperform prompt-based models. Similarly, [5] compare the performance of RoBERTa and GPT-3 in a study on an English-language dataset of parliamentary speeches, concluding that fine-tuning yields better results.</p>
<p>In contrast, recent work by [46] and [18] suggests that ChatGPT has caught up with or even surpassed fine-tuned models. [46] find that ChatGPT is on par with BERT-style models for some text understanding tasks, though results are mixed overall. [18] present results suggesting that ChatGPT even outperforms human annotators for text labeling tasks.</p>
<p>Inspired by these works, we aim to provide an up-to-date, systematic empirical analysis that offers a comprehensive view on the performance of fine-tuned versus prompt-based models for text classification.</p>
<p>While our results clearly point in the same direction as the studies by [5] and [15], our paper expands on the existing literature in several ways:</p>
<ul>
<li>
<p>Comprehensive Comparison: We systematically compare model performance across a diverse set of classification tasks and text categories. This allows us to identify performance variations across tasks and highlight areas where the differences between fine-tuned and prompt-based models are most (and least) pronounced.</p>
</li>
<li>
<p>Latest Generative AI Models: We provide results for the latest generation of prompt-based generative AI models (GPT-4 and Claude Opus). This is crucial, as previous studies based on older models like GPT-3 offer limited insights into the current performance balance.</p>
</li>
<li>Nuanced Understanding of Fine-Tuned LLMs: By investigating the performance of several fine-tuned LLMs, we offer a detailed understanding of how these models compare across classification tasks. This helps identify which models are better suited for specific tasks, thereby aiding user choice. Such a comparison provides a broader perspective on the current LLM landscape for text classification.</li>
<li>Ablation Studies and Training Data Impact: We conduct ablation studies and analyze the effect of training data size on model performance for fine-tuning. This offers valuable insights into the number of labeled samples required to achieve optimal performance, a crucial consideration given our consistent findings in favor of fine-tuned LLMs.</li>
<li>Accessible Toolkit for Fine-Tuning: We offer a one-stop-shop toolkit for text classification to enable a wider audience to use pre-trained LLMs for text classification tasks.</li>
</ul>
<p>With these contributions, we aim to provide a comprehensive and practical resource for understanding and applying LLMs in text classification.</p>
<h1>3 Non-technical Background: From Keywords to Large Language Models</h1>
<p>This section provides a non-technical introduction to LLMs, which complements our empirical results and our provided fine-tuning toolkit. It explains the functioning principles of LLMs and the reasons for their superior performance compared to earlier methods. To provide a comprehensive overview, we first briefly cover traditional hand-coding and dictionary approaches before discussing machine learning methods and LLMs.
Hand-coding is among the earliest text-as-data approaches. When hand-coding, Human coders classify verbal information according to pre-specified codebooks and explicit coding rules [12], [33]. Because hand-coding leverages human text-understanding, it is relatively easy to perform and yields high-quality results. However, hand-coding is labor-intensive - both regarding codebook development and the coding process itself. This makes it expensive and slow (see Figure 1). Moreover, hand-coding can be inconsistent across coders and within coders over time, making it partially non-replicable. High costs, slow procedures, and limited reproducibility are thus clear drawbacks of hand-coding approaches.
Dictionary approaches alleviate some of these concerns. These methods automatically extract information from text by mapping a larger set of keywords (or keyword combinations) to a small set of categories of interest. For example, sentiment dictionaries classify texts according to their content of positive and negative words or phrases [21, 37]. Dictionaries can be fully automated and therefore fast, replicable, and transparent. However, because dictionaries only take selected keywords into account, they ignore most nuanced textual information. Because words have different meanings in different settings, dictionaries can also be context-dependent and noisy when used off-the-shelf. Moreover, dictionary construction in itself is a difficult and time-consuming process and requires iterative testing to minimize false positives and false negatives. Thus, while dictionaries have advantages in terms of speed and replicability, they remain a relatively blunt tool.
Machine learning methods follow a different approach. Unlike hand-coding and dictionary approaches, which both rely on explicit human-crafted rules, machine learning techniques derive implicit rules from human-labeled data. In supervised learning, a model extracts classification-relevant patterns from a sample of manually-coded (labeled) training data. These patterns are then used to auto-classify the bulk of the remaining data [28, 16, 9]. Machine learning methods allow human coders to classify texts based on natural human language. This frees coders from having to create abstract rules and instead allows them to follow their intuitive language understanding. Humans can thus focus on the information content of texts, while the learning algorithm extracts predictive regularities from the relation between text features and labels.
The combination of human-labeled data and automated rule extraction is powerful because it can fuse the high quality of human coding with the speed of automated approaches (see Figure 1). This combination works best, however, with more sophisticated and information-dense numerical language representations. Deep-learning approaches based on LLMs are now making it possible to realize this potential. To see this, we next describe the evolution of language representations from bag-of-words models via word embeddings to LLMs and point out why each step has generated significant performance increases in NLP tasks.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of existing text-as-data methods and their characteristics: Machine learning approaches for text classification have the potential to combine the advantages of hand-coding (high-quality) and dictionaries (speed), while avoiding their respective downsides. The degree to which this potential can be realized in practice depends on the underlying text representation (see Figure 2).</p>
<h1>3.1 Bag-Of-Words</h1>
<p>Early machine learning approaches for automated text analysis rest on bag-of-words models [42]. These models treat text documents as collections (or bags) of words without regard to word order and grammar. This allows representing complex language information in a simple spreadsheet format that is easy to work with in downstream applications. In this format, rows represent documents, columns represent words, and cells record how often a word occurs in a document. Thus, a corpus with $D$ documents and $W$ unique words is represented as a $D x W$ matrix. Each row (corresponding to a document) then captures the word signature (i.e., word distribution) of the original input text.
While bag-of-word approaches can be powerful, they have notable shortcomings. First, they tend to produce large sparse matrices - because most words typically do not occur in most documents. This also results in matrices with more columns (words $=$ variables) than rows (documents $=$ observations), leading to $P&gt;N$ problems that are unwieldy in regression contexts.
Second, the spreadsheet representation treats all words as equally different from each other. For example, bag-of-words representations cannot capture that the words 'cake' and 'cookie' have more in common than the words 'cake' and 'court'. Bag-of-words approaches are oblivious to the meaning of words and cannot reflect relations between words.
Third, bag-of-words approaches discard all information contained in word order, sentence structure, and grammar. This ignores the entire narrative content of a text and makes it difficult to retain information beyond the general topic of a document. This problem is further aggravated by pre-processing steps such as stopword removal, which typically discards negations, and thus discards further potentially important information.</p>
<h3>3.2 Word Embeddings</h3>
<p>In contrast to bag-of-words models, word embeddings capture core aspects of word meaning and similarity [26]. Word embeddings represent words as vectors in a high-dimensional space, where one or multiple dimensions can be thought of as corresponding to a particular characteristic. For example, one dimension might represent the degree of positivity of a word, while another might represent its length or its frequency of use. Consequently, similar words are (ideally) mapped to vectors that point to a similar location in vector space. ${ }^{1}$
Word embeddings are a richer (and usually more compact) numerical representation of natural language than bags-of-words matrices. This has several advantages. First, word embeddings reduce the dimensionality of the language representation, which solves the $P&gt;N$ issue and facilitates the use of secondary models for downstream tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Second, word embeddings can extrapolate to words that do not occur in the training data for a downstream task. For example, when training a sentiment model where the training data contains positive references to 'apples' and 'bananas,' then the model will likely also attribute a positive sentiment to 'oranges' - which will be located close to other fruits in the embedding space.
Third, word embeddings can reflect further application-relevant intricacies of word meaning. For instance, embeddings can capture that 'man' is related to 'woman' and 'boy' is related to 'girl' on one dimension (gender), but also that 'man' is related to 'boy' and 'woman' is related to 'girl' on another dimension (age).
However, word embeddings still focus on words and continue to abstract from sentence structure and word order. They will thus only be able to capture the information content of longer segments of text to a limited extent.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Text representation of different text-as-data approaches: Existing approaches differ starkly in the sophistication of their text representation. Pre-trained LLMs approximate a more holistic human text understanding that focuses on meaning (concepts rather than form (wording. This allows LLMs to effectively leverage the information contained in text. By contrast, earlier text-as-data approaches discard significant information due to their more rudimentary language representations.</p>
<h1>3.3 Pre-trained Large Language Models</h1>
<p>Pre-trained LLMs derive their name from the fact that, before being fine-tuned or prompted to perform a specific task, they are pre-trained on large amounts of text to 'learn' the general structure and logic of language. The pre-training process encodes a language representation that focuses on sequences of text rather than individual words. For BERTstyle (encoder-only) models, fine-tuning later enriches this language representation with task-specific information. For GPT-style (decoder-only) models, prompts induce an answer generation process that is, in essence, a text prediction exercise resting on the model's pre-trained language understanding.
Pre-training relies on unsupervised learning. Unlike in supervised learning, the models are not given explicit instructions via 'labeled' training data. Instead, they discover relationships by predicting (temporarily masked) words in their training texts based on the surrounding word sequences. Encoder-style models have typically been trained with a Masked Language Modeling (MLM) objective, where random words are 'masked out' and the model must predict the missing words. In contrast, decoder-style models have typically been trained with a Causal Language Modeling (CLM) objective, where the goal is to predict the next token (or word) in the sequence.
Pre-training is much more resource-intensive than fine-tuning: Learning the complexities of natural language requires long exposure to large volumes of text. Hence, although the pre-training phase is unsupervised and requires no manual</p>
<p>input, it was long constrained by excessive computational costs. This changed with the development of the Transformer architecture [38]. Transformers can be more effectively parallelized than earlier sequence-learning approaches such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory models (LSTMs). This paved the way for models such as BERT (Bidirectional Encoder Representations from Transformers; [13] and much bigger models such as the GPT series (Generative Pre-trained Transformer; [32]. ${ }^{2}$
Analogously to how word embeddings capture similarities between words, LLMs capture similarities between longer text sequences. For example, LLMs 'understand' that the sentences 'this is an article on natural language processing' and 'you are reading a publication on automated text analysis' are similar in meaning despite their different wording. LLMs also 'understand' that 'this is an article on natural language processing' and 'this is not an article on natural language processing' differ in meaning despite similar wording. LLMs take negations and related structures into account and are able to correctly interpret the associated shifts in meaning due to their attention mechanism in the underlying transformer architecture.</p>
<p>This explains the strong performance of pre-trained Transformer models on many downstream tasks. Equivalent to how 'apples' and 'bananas' are represented in a similar part of the vector space of word embeddings, word sequences with similar meanings are represented as being similar within the internal representation of an LLM. This allows the models to effectively extrapolate to unseen text sequences once they have been fine-tuned or prompted to perform a downstream task (also see [22].
For BERT-style encoder models, fine-tuning then constitutes a secondary training step that complements the unsupervised pre-training stage with a supervised task-orientation stage. It involves feeding the model with a small set of labeled data. During fine-tuning, the model thus learns the nuanced relationships between text sequences and the labels associated with them. This 'knowledge' can then be used to classify unseen text sequences by predicting their most likely label. ${ }^{3}$
For GPT-style decoder models, fine-tuning is not per se required. However, model performance following prompts will generate high-quality output only if the model has seen sufficient examples of the task at hand during its pre-training (\&amp; post-training) phase. As we discuss in more detail in Section 6, we argue that for most classification tasks this requirement appears to not be sufficiently satisfied - thereby explaining the better performance of fine-tuned models in our analysis.</p>
<h1>4 Method: Fine-tuned LLMs vs. Zero-Shot Generative AI Models</h1>
<p>In the following, we compare the text classification performance of smaller, fine-tuned LLMs with that of three major generative AI models (ChatGPT with GPT-3.5 / GPT-4, and Claude Opus) and BART. We evaluate all models across four heterogeneous case studies, which span a diverse set of classification tasks. These tasks vary by concept (sentiment, stance, and emotions), text form (news, tweets, and speeches), language (English and German), and number of classes (binary vs. multi-class).
For smaller, fine-tuned LLMs, we assess five leading models of different sizes and architectures (RoBERTa Base, RoBERTa Large, DeBERTa V3, Electra Large, and XLNet). We fine-tune these models using our toolkit provided with this paper. ${ }^{4}$ We use our default hyperparameter setup throughout to showcase the capabilities of this setup, which we argue will be of greatest interest to most users, as no hyperparameter tuning is required (but possible; see Figure 3 for a suggested workflow summary).
For all fine-tuned models, we proceed in the same manner across all case studies. Each dataset is split into training and test sets, with the test set reserved for evaluating model predictions on unseen data. We then fine-tune the models for each case study and compare their classification performance on the test set using standard metrics: Accuracy, Precision, Recall, and F1-scores. To account for variability due to random seed initialization, we train each model three times with different seeds and report the mean and standard deviation for each metric across the three runs.
For the zero-shot LLMs, we prompt all models to label the full datasets we used for evaluating the fine-tuned models (and not just the test sets). The prompts include a description of the dataset and the classification categories as well as the instruction to assign the individual observations to the most appropriate category (detailed prompts for all models are provided in the online appendix). Because the zero-shot approach requires no training data for fine-tuning, we do</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Schematic representation of the workflow with our toolkit
not split the data into training and test sets. Instead, we prompt the models to label all observations in the dataset. We then evaluate model performance by comparing the model-generated labels with the original human-annotated labels, using the same performance metrics as for the fine-tuned models. We provide the full prompts in Appendix A.
In summary, we evaluate the following models across our case studies:</p>
<ul>
<li>MAJ-VOT - As a baseline, we use a failed classifier that assigns the majority class to all observations. For imbalanced datasets, this can suggest good performance on metrics sensitive to class-imbalance (e.g., Accuracy). However, metrics such as Recall or F1-Score reveal that this classifier performs poorly for the minority class(es).</li>
<li>ROB-BASE (125M) — RoBERTa Base [24], provided as 'roberta-base' via Hugging Face (HF) [44], is pre-trained in a self-supervised manner using masked language modeling (MLM).</li>
<li>ROB-LRG (355M) — RoBERTa Large, provided as 'roberta-large', is similar to RoBERTa Base but has more parameters. This boosts performance but increases computational costs.</li>
<li>
<p>DEB-V3 (435M) — DeBERTa, provided as 'microsoft/deberta-v3-large', builds on BERT/RoBERTa. Despite its similar size, it takes substantially longer to fine-tune because of differences in architecture.</p>
</li>
<li>
<p>ELE-LRG (335M) — ELECTRA Large, provided as 'google/electra-large-discriminator' and 'german-nlp-group/electra-base-german-uncased', was introduced by [11]. ELECTRA claims to be more compute-efficient than existing alternatives.</p>
</li>
<li>XLNET-LRG (340M) — XLNet [45], provided as 'xlnet-large-cased', was pre-trained with an autoregressive method to learn bidirectional contexts and has beens shown to perform comparable to RoBERTa.</li>
<li>BART-LRG (407M) — BART was introduced by Facebook [23] and uses an encoder-decoder model. BART can generate text similar to ChatGPT and Claude, although relying on a different architecture. More details are provided in Appendix B.</li>
<li>GPT-3.5 (175B+) / GPT-4 (n/a) — ChatGPT, developed by OpenAI [30]. It has substantially more parameters than any of the previsouly listed models (GPT-3.5 = 175B, GPT-4 $\approx 1.8 \mathrm{~T}^{5}$. ChatGPT can process complex input texts and generate text of varying length as output. We use the 'gpt-3.5-turbo' and 'gpt-4-1106-preview' checkpoints provided by their API. Details about prompting are provided in Appendix A.</li>
<li>CLD-OPUS (n/a) — Claude 3 Opus, developed by Anthropic [1]. The model size and technical details remain undisclosed. Opus can handle complex input prompts on par with GPT4, according to various benchmarks and empirical results published by Anthropic. We use the 'claude-3-opus-20240229' checkpoint provided by their API. Details about prompting are provided in Appendix A.</li>
</ul>
<h1>5 Results</h1>
<p>This section presents the results of our comparison. The first four subsections detail the findings for each case study for both fine-tuned and zero-shot prompted models. The fifth subsection reports the results of our ablation studies, which analyze the impact of training data size on the performance of our preferred fine-tuned model, RoBERTa Large.</p>
<h3>5.1 Sentiment Analysis on The New York Times Coverage of the US Economy</h3>
<p>In our first case study, we perform sentiment analysis - perhaps the most widely-studied problem in NLP. Sentiment analysis involves classifying statements into (i) positive and negative or (ii) positive, negative, and neutral categories.
The dataset is taken from [2]. It contains information on the sentiment of articles about the US economy from The New York Times. In the original study, the authors compare three dictionary-based approaches for sentiment analysis with a supervised machine learning approach (bag-of-words in combination with logistic regression). The machine learning model is reported to achieve an Accuracy of 0.71 and a Precision of 0.713 , clearly surpassing the dictionaries (highest Accuracy $=60.5$, highest Precision $=45.7$ ).</p>
<p>Table 1: Results for Sentiment Analysis (US Economy)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Name</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">Prec. (wgt.)</th>
<th style="text-align: left;">Recall (wgt.)</th>
<th style="text-align: left;">F1 (macro)</th>
<th style="text-align: left;">F1 (wgt.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MAJ-VOT</td>
<td style="text-align: left;">$0.73( \pm 0.00)$</td>
<td style="text-align: left;">$0.53( \pm 0.00)$</td>
<td style="text-align: left;">$0.73( \pm 0.00)$</td>
<td style="text-align: left;">$0.42( \pm 0.00)$</td>
<td style="text-align: left;">$0.61( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-BASE</td>
<td style="text-align: left;">$0.89( \pm 0.00)$</td>
<td style="text-align: left;">$0.89( \pm 0.01)$</td>
<td style="text-align: left;">$0.89( \pm 0.00)$</td>
<td style="text-align: left;">$0.86( \pm 0.01)$</td>
<td style="text-align: left;">$0.89( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-LRG</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 0 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 ( \pm 0 . 0 1 )}$</td>
</tr>
<tr>
<td style="text-align: left;">DEB-V3</td>
<td style="text-align: left;">$0.92( \pm 0.02)$</td>
<td style="text-align: left;">$0.92( \pm 0.01)$</td>
<td style="text-align: left;">$0.92( \pm 0.02)$</td>
<td style="text-align: left;">$0.90( \pm 0.02)$</td>
<td style="text-align: left;">$0.92( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">ELE-LRG</td>
<td style="text-align: left;">$0.90( \pm 0.01)$</td>
<td style="text-align: left;">$0.90( \pm 0.01)$</td>
<td style="text-align: left;">$0.90( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.02)$</td>
<td style="text-align: left;">$0.90( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">XLNET-LRG</td>
<td style="text-align: left;">$0.81( \pm 0.01)$</td>
<td style="text-align: left;">$0.85( \pm 0.01)$</td>
<td style="text-align: left;">$0.81( \pm 0.01)$</td>
<td style="text-align: left;">$0.78( \pm 0.01)$</td>
<td style="text-align: left;">$0.82( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">BART-LRG</td>
<td style="text-align: left;">$0.85( \pm 0.00)$</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
<td style="text-align: left;">$0.85( \pm 0.00)$</td>
<td style="text-align: left;">$0.80( \pm 0.00)$</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$0.82( \pm 0.00)$</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
<td style="text-align: left;">$0.82( \pm 0.00)$</td>
<td style="text-align: left;">$0.79( \pm 0.00)$</td>
<td style="text-align: left;">$0.83( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$0.87( \pm 0.00)$</td>
<td style="text-align: left;">$0.87( \pm 0.00)$</td>
<td style="text-align: left;">$0.87( \pm 0.00)$</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
<td style="text-align: left;">$0.87( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">CLD-OPUS</td>
<td style="text-align: left;">$0.86( \pm 0.00)$</td>
<td style="text-align: left;">$0.87( \pm 0.00)$</td>
<td style="text-align: left;">$0.86( \pm 0.00)$</td>
<td style="text-align: left;">$0.83( \pm 0.00)$</td>
<td style="text-align: left;">$0.87( \pm 0.00)$</td>
</tr>
</tbody>
</table>
<p>Note: Results for fine-tuned models on unseen test set with $N=200$. Results for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation with 8 steps and batch size 4, except DEB-V3 (batch size 2).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For our analysis, we use the dataset '3SU'. It contains 4195 paragraphs coded by undergraduates. The students used a 5-category coding scheme (negative, mixed, neutral, not sure, positive). We only use paragraphs coded as 'relevant' (i.e., US economy related). We map all paragraphs with a 'positive' label to 1 , all paragraphs with a 'negative' label to 0 and ignore paragraphs from the other categories. We then aggregate all labels based on document ID with a majority voting scheme. The final dataset has $N=1374$ observations, where $N=200$ observations are set aside for the test set. The class distribution is relatively imbalanced with 374 positive and 1000 negative labels.
The task is to predict whether a paragraph about the US economy is positive or negative. We fine-tuned all models as described above. Results are presented in Table 1. The strongest results are achieved by ROB-LRG and DEB-V3. ${ }^{6}$ Most other fine-tuned models except XLNET-LRG also achieve strong results with values around 0.90 and small variances (indicating that the random seed has little effect on model performance). BART, ChatGPT, and Claude also perform well on the sentiment task but remain behind the fine-tuned models.</p>
<h1>5.2 Stance Classification on Tweets about Kavanaugh Nomination</h1>
<p>Our second case study investigates stance classification. Stance and sentiment are conceptually different: Whereas sentiment reflects the tone of a text from positive to negative, stance captures a positional attitude from supportive to opposing. For example, a statement such as 'I am happy that the government was voted out of office.' has a positive sentiment but an opposing stance.
The dataset is taken from [3] ${ }^{7}$. It contains manually labeled tweets in which people express their view on the 2018 nomination of Brett Kavanaugh to the U.S. Supreme Court. In the original study, the authors evaluate two dictionary methods, a bag-of-words approach combined with a support-vector machine (SVM), and a BERT language model. Both machine learning approaches are reported to outperform the dictionaries. BERT achieves an F1-score of $0.938( \pm 0.002)$, although the SVM performs almost identically.</p>
<p>Table 2: Results for Stance Classification (Nomination Approval)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Name</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Prec. (wgt.)</th>
<th style="text-align: center;">Recall (wgt.)</th>
<th style="text-align: center;">F1 (macro)</th>
<th style="text-align: center;">F1 (wgt.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MAJ-VOT</td>
<td style="text-align: center;">$0.50( \pm 0.00)$</td>
<td style="text-align: center;">$0.25( \pm 0.00)$</td>
<td style="text-align: center;">$0.50( \pm 0.00)$</td>
<td style="text-align: center;">$0.33( \pm 0.00)$</td>
<td style="text-align: center;">$0.33( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-BASE</td>
<td style="text-align: center;">$0.86( \pm 0.01)$</td>
<td style="text-align: center;">$0.86( \pm 0.01)$</td>
<td style="text-align: center;">$0.86( \pm 0.01)$</td>
<td style="text-align: center;">$0.86( \pm 0.01)$</td>
<td style="text-align: center;">$0.86( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-LRG</td>
<td style="text-align: center;">$0.92( \pm 0.01)$</td>
<td style="text-align: center;">$0.93( \pm 0.01)$</td>
<td style="text-align: center;">$0.92( \pm 0.01)$</td>
<td style="text-align: center;">$0.92( \pm 0.01)$</td>
<td style="text-align: center;">$0.92( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">DEB-V3</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 ( \pm 0 . 0 1 )}$</td>
</tr>
<tr>
<td style="text-align: left;">ELE-LRG</td>
<td style="text-align: center;">$0.74( \pm 0.01)$</td>
<td style="text-align: center;">$0.66( \pm 0.02)$</td>
<td style="text-align: center;">$0.74( \pm 0.01)$</td>
<td style="text-align: center;">$0.67( \pm 0.02)$</td>
<td style="text-align: center;">$0.69( \pm 0.02)$</td>
</tr>
<tr>
<td style="text-align: left;">XLNET-LRG</td>
<td style="text-align: center;">$0.83( \pm 0.01)$</td>
<td style="text-align: center;">$0.83( \pm 0.01)$</td>
<td style="text-align: center;">$0.83( \pm 0.01)$</td>
<td style="text-align: center;">$0.83( \pm 0.01)$</td>
<td style="text-align: center;">$0.83( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">BART-LRG</td>
<td style="text-align: center;">$0.53( \pm 0.00)$</td>
<td style="text-align: center;">$0.59( \pm 0.00)$</td>
<td style="text-align: center;">$0.53( \pm 0.00)$</td>
<td style="text-align: center;">$0.44( \pm 0.00)$</td>
<td style="text-align: center;">$0.44( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">$0.53( \pm 0.00)$</td>
<td style="text-align: center;">$0.58( \pm 0.00)$</td>
<td style="text-align: center;">$0.53( \pm 0.00)$</td>
<td style="text-align: center;">$0.48( \pm 0.00)$</td>
<td style="text-align: center;">$0.47( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$0.58( \pm 0.00)$</td>
<td style="text-align: center;">$0.68( \pm 0.00)$</td>
<td style="text-align: center;">$0.58( \pm 0.00)$</td>
<td style="text-align: center;">$0.51( \pm 0.00)$</td>
<td style="text-align: center;">$0.51( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">CLD-OPUS</td>
<td style="text-align: center;">$0.61( \pm 0.00)$</td>
<td style="text-align: center;">$0.68( \pm 0.00)$</td>
<td style="text-align: center;">$0.61( \pm 0.00)$</td>
<td style="text-align: center;">$0.57( \pm 0.00)$</td>
<td style="text-align: center;">$0.57( \pm 0.00)$</td>
</tr>
</tbody>
</table>
<p>Note: Results for fine-tuned models on unseen test set with $N=200$. Results for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation with 8 steps and batch size 4, except DEB-V3 (batch size 2).</p>
<p>We use the 'text' and 'stance' columns from the dataset and apply some pre-processing by removing 'RT' flags (Retweets), twitter handles, and URLs. Afterwards, several tweets have identical text content (original and cleaned re-tweets). We therefore aggregate identical tweets via majority voting on the class label. The final dataset has $N=1173$ tweets with a balanced class distribution of 699 observations for the 1 label (supportive stance) and 674 observations for the 0 label (opposing stance). Again, we reserve $N=200$ observations for the test set.
The classification task is to predict whether a tweet supports or opposes Kavanaugh's nomination. We train the same models as before. Results are reported in Table 2. We achieve the strongest results with DEB-V3 and ROB-LRG with a performance of 0.94 and 0.92 respectively across metrics. The other three fine-tuned models show some variation. However, all fine-tuned models surpass the prompt-based approaches: BART, ChatGPT, and Claude perform rather weak on this more subtle task and score only marginally above the naive majority vote baseline.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.3 Emotion Detection on Political Texts in German</h1>
<p>Next, we turn to emotion classification, an application that has recently seen increased attention by social scientists [17, 39, 29]. Emotions affect political discourse, behavior, and opinion formation. Yet emotional tone goes beyond positive vs. negative sentiment or supportive vs. opposing stance. For example, while anger and fear are negative in terms of sentiment, they can generate different political behavior.
Our dataset comes from [43]. The corpus contains crowd-coded snippets with political content in German from three countries (Germany, Austria, and Switzerland) and two different sources (parliamentary speeches and Facebook posts).
In their study, the authors investigate eight emotions and compare several methods for emotion detection: An emotion dictionary, a classifier based on word embeddings, and a language model (ELECTRA). The strongest results are reported for 'Anger' (the most frequent emotion in the data), for which ELECTRA achieves an F1-Score of 0.84 (word embeddings $=0.79$, dictionary $=0.59$ ).
In our case study, we focus on Anger detection. We perform some pre-processing to aggregate labels if multiple coders rated the same snippet. Our final dataset has $N=7969$ observations with an imbalanced distribution of 2293 angry (1) cases and 5676 non-angry ( 0 ) cases.
Since the corpus is in German, we had two modeling options: Either (i) use the German text with a German-language LLM, or (ii) translate the corpus into English before fine-tuning our English-language models. We decided to evaluate both options. For the translation we used the DeepL API ${ }^{8}$. Both versions are identical in terms of labels and class distribution.</p>
<p>Table 3: Results for Emotion Detection (Anger)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Name</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">Prec. (wgt.)</th>
<th style="text-align: left;">Recall (wgt.)</th>
<th style="text-align: left;">F1 (macro)</th>
<th style="text-align: left;">F1 (wgt.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MAJ-VOT</td>
<td style="text-align: left;">$0.71( \pm 0.00)$</td>
<td style="text-align: left;">$0.51( \pm 0.00)$</td>
<td style="text-align: left;">$0.71( \pm 0.00)$</td>
<td style="text-align: left;">$0.42( \pm 0.00)$</td>
<td style="text-align: left;">$0.59( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-BASE</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
<td style="text-align: left;">$0.82( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-LRG</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.83( \pm 0.00)$</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">DEB-V3</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.83( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">ELE-LRG</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
<td style="text-align: left;">$0.88( \pm 0.02)$</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
<td style="text-align: left;">$0.88( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">XLNET-LRG</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 ( \pm 0 . 0 0 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 ( \pm 0 . 0 0 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 ( \pm 0 . 0 0 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 5 ( \pm 0 . 0 0 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 9 ( \pm 0 . 0 0 )}$</td>
</tr>
<tr>
<td style="text-align: left;">ELE-BS-GER</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.83( \pm 0.02)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">BART-LRG</td>
<td style="text-align: left;">$0.26( \pm 0.00)$</td>
<td style="text-align: left;">$0.36( \pm 0.00)$</td>
<td style="text-align: left;">$0.26( \pm 0.00)$</td>
<td style="text-align: left;">$0.24( \pm 0.00)$</td>
<td style="text-align: left;">$0.29( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$0.15( \pm 0.00)$</td>
<td style="text-align: left;">$0.23( \pm 0.00)$</td>
<td style="text-align: left;">$0.15( \pm 0.00)$</td>
<td style="text-align: left;">$0.15( \pm 0.00)$</td>
<td style="text-align: left;">$0.16( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$0.20( \pm 0.00)$</td>
<td style="text-align: left;">$0.18( \pm 0.00)$</td>
<td style="text-align: left;">$0.20( \pm 0.00)$</td>
<td style="text-align: left;">$0.18( \pm 0.00)$</td>
<td style="text-align: left;">$0.13( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">CLD-OPUS</td>
<td style="text-align: left;">$0.15( \pm 0.00)$</td>
<td style="text-align: left;">$0.16( \pm 0.00)$</td>
<td style="text-align: left;">$0.15( \pm 0.00)$</td>
<td style="text-align: left;">$0.14( \pm 0.00)$</td>
<td style="text-align: left;">$0.11( \pm 0.00)$</td>
</tr>
</tbody>
</table>
<p>Note: Results for fine-tuned models on unseen test set with $N=200$. Results for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation with 8 steps and batch size 4, except DEB-V3 (batch size 2).</p>
<p>The prediction task is to identify whether a snippet contains an expression of anger. The results are presented in Table 3. XLNET-LRG achieves the best results, closely followed by ELECTRA, ROB-LRG, and DEB-V3. The ELECTRA model performs almost identically on the original German text and on the English translation. Translation can thus be a viable option when no language-specific pre-trained model exists.
BART, ChatGPT, and Claude all have considerable difficulties with this task and do substantially worse than our naive classifier. These results suggest that zero-shot classification works best for standard tasks but does not travel well to more specialized use-cases and nuanced semantics (which the models will have encountered less frequently during training).</p>
<h3>5.4 Multi-Class Stance Classification on Parties' EU Positions</h3>
<p>In our final study, we investigate European nationalist party positions. The dataset comes from [25]. It contains hand-coded evaluations of parties' positions toward the EU and European integration as reported in major European</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>newspapers between the run-up to the 2016 Brexit referendum and the UK's 2020 EU exit. Positions range from acceptance of the European status-quo to outright demands to leave the EU. ${ }^{9}$
This setup implies a one-sided multi-class stance classification task: Stance because we consider party opposition (= opposing stance) toward European integration, one-sided because positions only range from neutral to strongly opposed (neglecting the supportive side of the spectrum), and multi-class because party positions are measured in three grades.</p>
<p>Table 4: Results for Multi-Class Stance Classification (EU Positions)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Name</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">Prec. (wgt.)</th>
<th style="text-align: left;">Recall (wgt.)</th>
<th style="text-align: left;">F1 (macro)</th>
<th style="text-align: left;">F1 (wgt.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MAJ-VOT</td>
<td style="text-align: left;">$0.83( \pm 0.00)$</td>
<td style="text-align: left;">$0.68( \pm 0.00)$</td>
<td style="text-align: left;">$0.83( \pm 0.00)$</td>
<td style="text-align: left;">$0.30( \pm 0.00)$</td>
<td style="text-align: left;">$0.75( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-BASE</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
<td style="text-align: left;">$0.84( \pm 0.00)$</td>
<td style="text-align: left;">$0.70( \pm 0.02)$</td>
<td style="text-align: left;">$0.85( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">ROB-LRG</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.72( \pm 0.03)$</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">DEB-V3</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 2 ( \pm 0 . 0 1 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 2 ( \pm 0 . 0 2 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 9 1 ( \pm 0 . 0 1 )}$</td>
</tr>
<tr>
<td style="text-align: left;">ELE-LRG</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
<td style="text-align: left;">$0.75( \pm 0.03)$</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">XLNET-LRG</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
<td style="text-align: left;">$0.89( \pm 0.01)$</td>
<td style="text-align: left;">$0.87( \pm 0.01)$</td>
<td style="text-align: left;">$0.75( \pm 0.02)$</td>
<td style="text-align: left;">$0.88( \pm 0.01)$</td>
</tr>
<tr>
<td style="text-align: left;">BART-LRG</td>
<td style="text-align: left;">$0.82( \pm 0.00)$</td>
<td style="text-align: left;">$0.77( \pm 0.00)$</td>
<td style="text-align: left;">$0.82( \pm 0.00)$</td>
<td style="text-align: left;">$0.34( \pm 0.00)$</td>
<td style="text-align: left;">$0.75( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$0.24( \pm 0.00)$</td>
<td style="text-align: left;">$0.65( \pm 0.00)$</td>
<td style="text-align: left;">$0.24( \pm 0.00)$</td>
<td style="text-align: left;">$0.17( \pm 0.00)$</td>
<td style="text-align: left;">$0.27( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$0.38( \pm 0.00)$</td>
<td style="text-align: left;">$0.73( \pm 0.00)$</td>
<td style="text-align: left;">$0.38( \pm 0.00)$</td>
<td style="text-align: left;">$0.26( \pm 0.00)$</td>
<td style="text-align: left;">$0.45( \pm 0.00)$</td>
</tr>
<tr>
<td style="text-align: left;">CLD-OPUS</td>
<td style="text-align: left;">$0.26( \pm 0.00)$</td>
<td style="text-align: left;">$0.75( \pm 0.00)$</td>
<td style="text-align: left;">$0.26( \pm 0.00)$</td>
<td style="text-align: left;">$0.25( \pm 0.00)$</td>
<td style="text-align: left;">$0.29( \pm 0.00)$</td>
</tr>
</tbody>
</table>
<p>Note: Results for fine-tuned models on unseen test set with $N=200$. Results for BART, GPTs, and Claude on full data. Fine-tuned models use gradient accumulation with 8 steps and batch size 4, except DEB-V3 (batch size 2).</p>
<p>We translate all news texts from the original languages to English using DeepL. We also aggregate the hand-coded euro-sceptic labels into three slightly broader categories: Status-quo acceptant or neutral statements are coded as 0 (least opposed), policy-level critique and reform demands are coded as 1, and institutional-level critique and outright leave-demands are coded as 2 (most opposed). We aggregate identical snippets by majority voting and ignore snippets coded as 'not relevant.'
The task is to predict neutral, moderately opposed, or strongly opposed positions toward the EU. The dataset has $N=3349$ observations and considerable class imbalance with $83 \%$ of snippets in the moderately opposed category (neutral $=292$, moderate opposition, 2764, strong opposition 293). Results are shown in Table 4. DEB-V3 outperforms all other models, scoring well above 0.90 for most metrics. Most other fine-tuned models perform at least decently. BART, ChatGPT, and Claude, however, again do not exceed the naive baseline classifier - suggesting once more that the fine-tuning approach is superior when tasks are non-standard or context-specific.</p>
<h1>5.5 Fine-Tuning: The Effect of Training Set Size on Model Performance</h1>
<p>In this section, we turn to a complementary analysis aimed at understanding the training data requirements for the fine-tuning approach and how they affect the final model performance on the unseen test set. Among the five fine-tuned LLMs analyzed above, no single model outperforms the others on all tasks, but ROB-LRG and DEB-V3 are slightly ahead of the rest. Since ROB-LRG is less computationally demanding than DEB-V3, we consider it to provide the best overall balance.</p>
<p>For this analysis, we therefore revisit our four case studies with ROB-LRG, but evaluate model performance repeatedly at different training set sizes. For each run, we sample $N$ observations, where $N$ takes on values $N={50,100,200,500,1000}$. The test set size remains at $N=200$ across all runs.</p>
<p>The results are depicted in Figure 4 and show how model performance increases with training data size. For model performance, we report F1 Macro, F1 Weighted, and Accuracy on the y-axis, evaluated on the unseen test set. For dataset size, we report sample size $N$ on the x-axis.</p>
<p>Overall, we see that performance picks up quickly as the amount of training data grows from minimal to moderate levels and then saturates as the training set size increases further. ${ }^{10}$ This effect is most pronounced for more balanced datasets (e.g., the Kavanaugh Nomination Tweets). For imbalanced datasets (especially, EU positions data), we see high (but meaningless) accuracy scores at low sample sizes because poorly trained models always predict the majority</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Effect of training set size on model performance: Results for ROB-LRG with varying number of training observations $N={50,100,200,500,1000}$. The translucent markers above the 0-point denote the zero-shot results of BART. The rightmost points denote model performance if trained on the full dataset.
class. Macro-F1 scores, however, which punish naive majority class prediction, consistently show the saturation pattern. Overall, the sweet-spot tends to lie between 200 and 500 training observations. Below 200 observations, model performance remains below its potential. Above 500 observations, performance begins to level off.
We also show the results of BART, arguably the most consistent of the zero-shot approaches overall (compared to GPT-3.5, GPT-4, and Claude Opus). Since the training set for zero-shot classification is essentially zero, we show the BART results on the left of each plot (x-axis value $=0$ ) with slightly translucent markers. As can be seen, ROB-LRG begins to outperform BART on all datasets when the training set exceeds $N=200$.</p>
<h1>6 Discussion</h1>
<p>Our key finding is that fine-tuning smaller LLMs with dedicated training data is consistently superior to zero-shot prompting larger models such as BART, ChatGPT, and Claude Opus. While zero-shot results are decent for common tasks such as sentiment analysis, fine-tuned LLMs always surpass zero-shot performance as training set size increases. This tendency is more pronounced for less standard tasks such as stance classification or emotion detection.</p>
<p>An interesting question is whether next-generation generative AI models will catch up with, or even surpass, fine-tuned LLMs in performing specialized tasks. In the following, we point to some areas where we expect progress regarding both fine-tuning and prompt-based approaches, which may be helpful in structuring expectations about future trajectories: (i) data augmentation, (ii) secondary pre-training, (iii) model architecture, and (iv) prompt engineering and few-shot learning. Lastly, we discuss some general considerations when choosing between smaller BERT-like LLMs and larger generative AI models in production settings.</p>
<ul>
<li>Data Augmentation: Data augmentation is an approach to synthetically increase labeled training data without human input. One approach in NLP is back translation, where the original text is translated into another</li>
</ul>
<p>language and then back-translated. Adding the back-translated sentence to the training data (with the same label as the original) can boost performance as shown by [35]. Another option is token perturbation, where certain words are replaced with synonyms [40]. Going forward, these techniques can further reduce data requirements for fine-tuning LLMs.</p>
<ul>
<li>Secondary Pre-training: An additional pre-training phase on a smaller domain-specific corpus can improve both fine-tuned encoder-style and instruction-tuned decoder-style LLMs. For RoBERTa, [20] demonstrated significant performance gains for downstream classification tasks when further pre-trained in the specific domain to gain more knowledge about a specific vocabulary and usage before performing downstream fine-tuning for the classification task.</li>
<li>Model Architecture: While there has been a trend toward larger and larger models, recent work has shown that scaling from hundreds of billions to trillions of parameters yields saturating improvements and that scale alone may not be the single factor that determines performance [34, 7]. Instead, research has been devoted to investigate multi-modality - the integration of multiple modes of information such as text, image and audio [14] —, 'ensemble' (or mixture-of-experts) architectures [36], and ways to improve training data quality while keeping model size in the billion parameter range [19].</li>
<li>Prompt Engineering and Few-Shot Learning: Prompt engineering can boost the performance of instructiontuned LLMs such as ChatGPT and Claude. One option is few-shot learning, which results in so-called in-context learning by providing labeled training data as part of the prompt. For GPT-3.5, few-shot prompting yielded some performance gains as seen in [6] ${ }^{11}$. However, few-shot learning requires careful selection of examples to provide in the prompt and systematic evaluation of prompt designs. This significantly increases the required time investment and multiplies costs due to the increased length and number of prompts. Consequently, few-shot learning risks negating the simplicity and intuitive handling advantages of generative AI models. Nonetheless, with further developments in model architecture, in-context learning via few-shot prompting may eventually surpass fine-tuned encoder-style LLMs.</li>
</ul>
<p>Depending on how the above developments play out, the relative classification performance of smaller BERT-style LLMs and larger generative AI models may shift to one side or the other. However, at present, fine-tuning of smaller models remains the superior approach for text classification.
Apart from performance, we see additional advantages of relying on smaller, fine-tuned LLMs in production settings:
With an eye to trade secrets or privacy, smaller models provide an unmatched level of control. Because these models are relatively small, they can be stored, trained, and run on a local machine. Relatedly, all model parameters are visible and can be saved and backed up, providing programmatic stability and predictability. This is in stark contrast to proprietary, cloud-based generative AI models, which require information to be transferred to an external black-box model.
Additionally, it is important to remember that professional use of generative AI models requires labeled data for validation - even when using zero-shot prompting. Given the relatively better performance of smaller fine-tuned models with limited training data, along with their advantages regarding control and privacy, it seems likely that models like BERT and similar LLMs will remain valuable tools for years to come.</p>
<h1>7 Conclusion</h1>
<p>In this paper, we investigate whether zero-shot prompted generative AI models such as ChatGPT and Claude Opus can already outperform fitted models such as smaller fine-tuned LLMs for text classification tasks. To this end, we compare several fine-tuned LLMs such as RoBERTa with major generative AI models across text categories, tasks, languages, and imbalanced datasets.
Our results show that fine-tuning smaller LLMs still significantly outperforms zero-shot prompted generative AI models on all our case studies. In addition, we provide an ablation study to explore the relationship between dataset size (i.e., number of training examples) and model performance, showing that performance already begins to saturate after around 200 labels.</p>
<p>We further provide an easy-to-use text classification toolkit that requires no domain knowledge in NLP or deep learning. Our toolkit is modular and allows users to plug in additional models as desired or needed. This allows users to load smaller LLMs for other languages or upgrade to newly developed or improved language models and as they are published via Hugging Face. We hope that our tool will enable users to realize the potential of fine-tuned LLMs.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Acknowledgments The authors would like to thank Daniel Grosshans, Sascha Langenbach, Janina Steinmetz, and Stefanie Walter.</p>
<p>Funding Statement This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme grant agreement No 817582 (ERC Consolidator Grant DISINTEGRATION).</p>
<p>Replication materials All code and replication materials are available here.</p>
<h1>References</h1>
<p>[1] Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. Mar. 2024.
[2] Pablo Barberá et al. "Automated text classification of news articles: A practical guide". In: Political Analysis 29.1 (2021), pp. 19-42.
[3] Samuel E Bestvater and Burt L Monroe. "Sentiment is not stance: target-aware opinion classification for political text analysis". In: Political Analysis (2022), pp. 1-22.
[4] Piotr Bojanowski et al. "Enriching word vectors with subword information". In: Transactions of the association for computational linguistics 5 (2017), pp. 135-146.
[5] Mitchell Bosley et al. "Do we still need BERT in the age of GPT? Comparing the benefits of domain-adaptation and in-context-learning approaches to using LLMs for Political Science Research". In: (2023).
[6] Tom Brown et al. "Language models are few-shot learners". In: Advances in neural information processing systems 33 (2020), pp. 1877-1901.
[7] Ethan Caballero et al. "Broken neural scaling laws". In: arXiv preprint arXiv:2210.14891 (2022).
[8] Youngjin Chae and Thomas Davidson. "Large language models for text classification: From zero-shot learning to fine-tuning". In: Open Science Foundation (2023).
[9] Charles Chang and Michael Masterson. "Using Word Order in Political Text Classification with Long Short-term Memory Models". In: Political Analysis, 28(3): 395-411 (2020).
[10] Zheng Chen and Yunchen Zhang. "Better few-shot text classification with pre-trained language model". In: Artificial Neural Networks and Machine Learning-ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14-17, 2021, Proceedings, Part II 30. Springer. 2021, pp. 537548 .
[11] Kevin Clark et al. "Electra: Pre-training text encoders as discriminators rather than generators". In: arXiv preprint arXiv:2003.10555 (2020).
[12] Gary W. Cox and Eric Magar. "How much is majority status in the US Congress worth?" In: American Political Science Review 93(2): 299-309 (1999).
[13] Jacob Devlin et al. "Bert: Pre-training of deep bidirectional transformers for language understanding". In: arXiv preprint arXiv:1810.04805 (2018).
[14] Danny Driess et al. "Palm-e: An embodied multimodal language model". In: arXiv preprint arXiv:2303.03378 (2023).
[15] Aleksandra Edwards and Jose Camacho-Collados. "Language Models for Text Classification: Is In-Context Learning Enough?" In: arXiv preprint arXiv:2403.17661 (2024).
[16] Aaron Erlich et al. "Multi-label prediction for political text-as-data". In: Political Analysis 30(4): 463-480 (2022).
[17] Gloria Gennaro and Elliott Ash. "Emotion and reason in political language". In: The Economic Journal 132(643): 1037-1059 (2022).
[18] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. "ChatGPT outperforms crowd workers for text-annotation tasks". In: Proceedings of the National Academy of Sciences 120.30 (2023), e2305016120.
[19] Suriya Gunasekar et al. "Textbooks are all you need". In: arXiv preprint arXiv:2306.11644 (2023).
[20] Suchin Gururangan et al. "Don’t stop pretraining: Adapt language models to domains and tasks". In: arXiv preprint arXiv:2004.10964 (2020).
[21] Alan M. Jacobs et al. "Whose news? Class-biased economic reporting in the United States". In: American Political Science Review 115(3): 1016-1033 (2021).
[22] Moritz Laurer et al. "Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI". In: Political Analysis (2023), pp. 1-33.
[23] Mike Lewis et al. "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension". In: arXiv preprint arXiv:1910.13461 (2019).</p>
<p>[24] Yinhan Liu et al. "Roberta: A robustly optimized bert pretraining approach". In: arXiv preprint arXiv:1907.11692 (2019).
[25] Marco Martini and Stefanie Walter. "Learning from precedent: how the British Brexit experience shapes nationalist rhetoric outside the UK". In: Journal of European Public Policy (2023), pp. 1-28.
[26] Tomas Mikolov et al. "Distributed representations of words and phrases and their compositionality". In: Advances in neural information processing systems 26 (2013).
[27] Tomas Mikolov et al. "Efficient estimation of word representations in vector space". In: arXiv preprint arXiv:1301.3781 (2013).
[28] Blake Miller, Fridolin Linder, and Walter R. Mebane. "Active learning approaches for labeling text: review and assessment of the performance of active learning approaches". In: Political Analysis 28(4): 532-551 (2020).
[29] Moritz Osnabrügge, Sara B. Hobolt, and Toni Rodon. "Playing to the gallery: Emotive rhetoric in parliaments". In: American Political Science Review 115(3): 885-899 (2021).
[30] Long Ouyang et al. "Training language models to follow instructions with human feedback". In: Advances in Neural Information Processing Systems 35 (2022), pp. 27730-27744.
[31] Jeffrey Pennington, Richard Socher, and Christopher D Manning. "Glove: Global vectors for word representation". In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014, pp. 1532-1543.
[32] Alec Radford et al. "Improving language understanding by generative pre-training". In: (2018).
[33] Mark J. Richards and Herbert M. Kritzer. "Jurisprudential regimes in Supreme Court decision making". In: American Political Science Review 96(2): 305-320 (2002).
[34] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. "Are emergent abilities of Large Language Models a mirage?" In: arXiv preprint arXiv:2304.15004 (2023).
[35] Rico Sennrich, Barry Haddow, and Alexandra Birch. "Improving neural machine translation models with monolingual data". In: arXiv preprint arXiv:1511.06709 (2015).
[36] Noam Shazeer et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer". In: arXiv preprint arXiv:1701.06538 (2017).
[37] Bruno Castanho Silva and Sven-Oliver Proksch. "Fake it 'til you make it: A natural experiment to identify European politicians' benefit from Twitter bots". In: American Political Science Review 115(1): 316-322 (2021).
[38] Ashish Vaswani et al. "Attention is all you need". In: Advances in neural information processing systems 30 (2017).
[39] Krishnapriya Vishnubhotla and Saif M. Mohammad. "Tweet emotion dynamics: Emotion word usage in tweets from US and Canada". In: arXiv:2204.04862 (2022).
[40] Jason Wei and Kai Zou. "Eda: Easy data augmentation techniques for boosting performance on text classification tasks". In: arXiv preprint arXiv:1901.11196 (2019).
[41] Jason Wei et al. "Finetuned language models are zero-shot learners". In: arXiv preprint arXiv:2109.01652 (2021).
[42] Kilian Weinberger et al. "Feature hashing for large scale multitask learning". In: Proceedings of the 26th annual international conference on machine learning: pp. 1113-1120 (2009).
[43] Tobias Widmann and Maximilian Wich. "Creating and comparing dictionary, word embedding, and transformerbased models to measure discrete emotions in german political text". In: Political Analysis (2022), pp. 116 .
[44] Thomas Wolf et al. "Transformers: State-of-the-art natural language processing". In: Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 2020, pp. 38-45.
[45] Zhilin Yang et al. "Xlnet: Generalized autoregressive pretraining for language understanding". In: Advances in neural information processing systems 32 (2019).
[46] Qihuang Zhong et al. "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert". In: arXiv preprint arXiv:2302.10198 (2023).
[47] Ruiqi Zhong et al. "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections". In: arXiv preprint arXiv:2104.04670 (2021).</p>
<h1>Appendix</h1>
<h2>A: Zero-Shot Prompts for GPT-3.5, GPT-4, and Claude Opus</h2>
<p>After experimenting through the API with a few prompts, we fine-tuned our final prompt to maximize the likelihood that the model only returns one label from the given list of available class labels. As LLMs are discrete probabilistic models, it's stochasticity can lead to a variance in its generated output. By controlling the temperature parameter and decreasing its value, we can decrease the variance of the generated output. However, the model still generates other content than just one of the given output labels. In the following, we present the prompts we provided to the generative AI APIs, where $&lt;$ Text $&gt;$ marks the text placeholder for the current text sample. We set temperature $=0.1$ for all runs to reduce variance and force the model to produce a single label output from the provided list of labels. We repeat the prompting if the output does not match the labels provided for the classification task until the model produces a correct label as output.</p>
<h2>Sentiment Analysis on The New York Times Coverage of the US Economy</h2>
<p>Prompt:
You have been assigned the task of zero-shot text classification for sentiment analysis. Your objective is to classify a given text snippet into one of several possible class labels, based on the sentiment expressed in the text. Your output should consist of a single class label that best matches the sentiment expressed in the text. Your output should consist of a single class label that best matches the given text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.
Text: $&lt;$ Text $&gt;$
Labels: 'Negative Sentiment', 'Positive Sentiment'
Answer:</p>
<h2>Stance Classification on Tweets about Kavanaugh Nomination</h2>
<p>Prompt:
You have been assigned the task of zero-shot text classification for stance classification. Your objective is to classify a given text snippet into one of several possible class labels, based on the attitudinal stance towards the given text. Your output should consist of a single class label that best matches the stance expressed in the text. Your output should consist of a single class label that best matches the given text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.
Text: $&lt;$ Text $&gt;$
Labels: 'negative attitudinal stance towards', 'positive attitudinal stance towards'
Answer:</p>
<h2>Emotion Detection on Political Texts in German</h2>
<p>Prompt:
You have been assigned the task of zero-shot text classification for emotion classification. Your objective is to classify a given text snippet into one of several possible class labels, based on the anger level in the given text. Your output should consist of a single class label that best matches the anger expressed in the text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.
Text: $&lt;$ Text $&gt;$
Labels: 'Angry', 'Non-Angry'
Answer:</p>
<h1>Multi-Class Stance Classification on Parties' EU Positions</h1>
<h2>Prompt:</h2>
<p>You have been assigned the task of zero-shot text classification for political texts on attitudinal stance towards Brexit and leave demands related to the European Union (EU). Your objective is to classify a given text snippet into one of several possible class labels, based on the stance towards Brexit and general leave demands in the given text. Your output should consist of a single class label that best matches the content expressed in the text. Choose ONLY from the given class labels below and ONLY output the label without any other characters.
Text: <Text>
Labels: 'Neutral towards Leave demands', 'Pro-Leave demands', 'Very Pro-Leave demands'
Answer:</p>
<h2>B: BART</h2>
<p>For BART, we use the BART Large model facebook/bart-large-mnli introduced by [23] together with the ZeroShot Classification pipeline from Huggingface. We use a technique called Natural Language Inference (NLI), which prompts a model using two sentences, a Premise (in our case the text to be classified) and a Hypothesis (a possible class label for that text). The model then predicts if the hypothesis is consistent with the premise. NLI evaluates a hypothesis for each label and then selects the label with the highest confidence as output.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ In exploratory experiments on our datasets, few-shot prompting did not improve results for GPT-3.5 and 4.0.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9400 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9400</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9400</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-279251147</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07298v2.pdf" target="_blank">Pre-trained Large Language Models Learn Hidden Markov Models In-context</a></p>
                <p><strong>Paper Abstract:</strong> Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9400",
    "paper_id": "paper-279251147",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00655475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pre-trained Large Language Models Learn Hidden Markov Models In-context</p>
<p>Yijia Dai 
Cornell University</p>
<p>Zhaolin Gao 
Cornell University</p>
<p>Yahya Sattar ysattar@cornell.edu 
Cornell University</p>
<p>Sarah Dean sdean@cornell.edu 
Cornell University</p>
<p>Jennifer J Sun 
Cornell University</p>
<p>Pre-trained Large Language Models Learn Hidden Markov Models In-context
6D1C688C0470C529BA2C38FE0EAC2308
Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging.In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)-their ability to infer patterns from examples within a prompt.On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum.We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations.We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data.On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts.To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences-an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.Our code is available at https://github.com/DaiYijia02/icl-hmm.</p>
<p>Introduction</p>
<p>Many natural and artificial systems, from animal decision-making to ecological processes to climate patterns, generate observations governed by underlying, unobservable states that follow Markovian dynamics [3,17,34,48,57].Hidden Markov Models (HMMs) [39] provide a powerful framework for studying such phenomena.However, accurately modeling these systems presents significant challenges.Parameter estimation and model fitting require complex algorithms like Baum-Welch [5], and Gibbs Sampling [16].These methods are often computationally intensive and can be algorithmically unstable, demanding extensive domain expertise [8,9].For scientists across disciplines, these accessibility and computational bottlenecks limit the practical applications of HMM modeling tools.</p>
<p>Recently, large language models (LLMs) [1,18] have reshaped the landscape of AI.Trained on vast amounts of sequential text data, they have achieved unprecedented performance across natural language processing tasks and exhibit remarkable in-context learning (ICL) capabilities-the ability to learn patterns and perform new tasks directly from examples provided in the input context, without explicit parameter updates [7].While prior theoretical and empirical works [13,33,40] have demonstrated LLMs' capabilities as Bayesian learners and their ability to model fully observed Markov processes, their capacity to implicitly learn Hidden Markov Models-with latent states, complex transition dependencies, and observation emissions-remains largely unexplored.Understanding this Preprint.Under review.Figure 1: Overview of our study.We start by studying whether ICL using pre-trained LLMs can converge to theoretical optimum on HMM sequences (Q1, Section 2), then study how HMMs properties affect the convergence rate/gap with theoretical conjectures (Q2, Section 3), and finally we demonstrate how these findings translate to insights on real-world datasets for studying behaviors in science (Q3, Section 4).</p>
<p>capacity could illuminate the mechanisms underlying in-context learning HMMs, and reveal new ways to leverage LLMs for analyzing complex sequential phenomena in scientific contexts.</p>
<p>In this paper, we present a comprehensive study on the ability of pre-trained LLMs to learn HMMs through in-context learning (Figure 1), revealing their surprisingly strong performance and offering actionable insights for real-world scientific experiments.A key finding is that pre-trained LLMs demonstrate a remarkable capacity to learn HMMs nearly optimally, achieving performance that approaches optimal Bayesian inference and often surpasses traditional statistical methods.These results not only advance our understanding of the emergent capabilities of in-context learning, but also introduce a novel and practical framework for using LLMs as powerful, efficient statistical tools in complex scientific data analysis.Our study makes three key contributions:</p>
<ol>
<li>
<p>We conduct systematic, controlled experiments on synthetic HMMs and empirically show that pretrained LLMs outperform traditional statistical methods such as Baum-Welch.Moreover, their prediction accuracy consistently converges to the theoretical optimum-as given by the Viterbi algorithm with ground-truth model parameters-across a wide range of HMM configurations (Section 2).</p>
</li>
<li>
<p>We identify and characterize empirical scaling trends showing that LLM performance improves with longer context windows, and that these trends are shaped by fundamental HMM properties such as mixing rate and entropy.We further provide theoretical conjectures to explain these phenomena, drawing connections to-and highlighting distinctions from-classical HMM learning paradigms, including spectral methods.These findings offer important insights into the learnability of stochastic systems through in-context learning (Section 3).</p>
</li>
<li>
<p>We translate our findings into practical guidelines for scientists, demonstrating how LLM incontext learning can serve as a diagnostic tool for assessing data complexity and uncovering underlying structure.When applied to real-world animal decision-making tasks, LLM ICL performs competitively with domain-specific models developed by human experts (Section 4).</p>
</li>
</ol>
<p>Synthetic Experiments and ICL Convergence</p>
<p>We investigate the in-context learning capabilities of pre-trained LLMs on sequences generated by synthetic HMMs.We first review key HMM properties (Section 2.1) and outline our experimental setup (Section 2.2).We then empirically demonstrate that the prediction accuracy of pre-trained LLMs consistently converges to the theoretical optimum (Section 2.3).</p>
<p>HMM Background</p>
<p>Hidden Markov model: HMMs impose a set of probabilistic assumptions on how sequences of data are generated.The elements of the sequence are called observations, denoted at each step t by O t .The observations depend on a hidden state denoted by X t , which evolves according to a Markov chain.A HMM is characterized by the Markov chain's initial state distribution and its state transitions, along with the emission probabilities of an observation given the hidden state.The key assumptions are that the state transition depends only on the previous state (Markov property), the observation depends only on the current hidden state (output independence), and both transition and emission probabilities are time-invariant (stationarity).</p>
<p>We focus on the setting with finitely many states and observations.Without loss of generality, states take values in X = {1, 2, . . ., M } while observations take values in O = {1, 2, . . ., L}.The initial state distribution is denoted as π ∈ R M with π j the probability of starting in state j, the state transitions are describe by the matrix A ∈ R M ×M with elements a ij the probability of transitioning to state j from state i, and the emission matrix B ∈ R M ×L contains b jl the probability of observing l when in hidden state j.The triple λ = (π, A, B) completely parameterizes a finite-alphabet HMM.</p>
<p>Stationary distributions: Under certain conditions (see Appendix A), Markov chains are guaranteed to converge to unique stationary distributions, which are given by the µ ∈ R M satisfying µ = µA [14].The stationary distribution of the hidden state characterizes the long term behavior of the HMM, and therefore plays an important role in both predicting future observations and learning HMM parameters.The rate of convergence is characterized by the mixing rate which for finite-alphabet HMMs is equal to λ 2 , the second-largest eigenvalue of A. From any initial distribution, the hidden state distribution approaches the stationary distribution geometrically with multiplier λ 2 .A smaller mixing rate indicates faster convergence to the stationary distribution.</p>
<p>Entropy: HMMs can describe processes which vary from deterministic to purely random, depending on how transition and emission probabilities are defined.Entropy is a measure of the randomness or unpredictability of a random variable.By considering the average entropy over the stochastic processes of hidden state and observation, we can quantify the entropy of a particular HMM by H(A) = − i,j µ i a ij log a ij and H(B, µ) = − j,l µ j b jl log b jl .We additionally define normalized entropies H(A) = H(A)/ log M and H(B, µ) = H(B, µ)/ log L as metrics for visualization.</p>
<p>A smaller entropy indicates a more predictable process.See Appendix A for further explanation.</p>
<p>Experimental Setup</p>
<p>Experiment setting: Our experiment follows a three-step protocol: First, we specify the HMM parameters λ = (π, A, B) according to our control variables (described below  In the following sections, we evaluate LLMs' performance by comparison to several other approaches (Table 1).</p>
<p>ICL Converges to Theoretical Optimum</p>
<p>We define convergence as achieving prediction accuracy comparable to the Viterbi algorithm.The Viterbi algorithm, given ground-truth HMM parameters λ, computes the most likely hidden state sequence x 1:t from observations o 1:t (see Appendix C for details).Since Viterbi has access to the true model parameters, its performance represents the theoretical optimum.Remarkably, ICL with pre-trained LLM achieves this near-optimal prediction accuracy across diverse HMM parameter configurations in our experiments.Figure 3 illustrates examples and conditions under which this convergence occurs.Convergence occurs reliably when HMM entropy is low and mixing is fast.</p>
<p>For challenging conditions where LLM convergence fails or proceeds exceptionally slowly-like the red areas shown in the left-hand side of Figure 4-Viterbi algorithm also exhibits diminished prediction accuracy and requires substantially longer context windows to achieve reliable performance.This degraded performance reflects the fundamental limits of stochastic system learnability due to random dynamics and long-range dependencies, affecting even the optimal inference methods.See Appendix D for detailed examples.</p>
<p>Impact of HMM Properties on Convergence</p>
<p>Having established that ICL with pre-trained LLM converges to the theoretically optimal predictions, we now provide an in depth characterization of their performance across variable settings.We summarize the scaling trends in terms of key HMM properties, compare these patterns against other popular methods for HMM prediction, and conclude by providing theoretical conjectures.(Right) Hellinger distance measures distance between two probability distributions.</p>
<p>In-context Scaling Trends</p>
<p>Neural scaling laws describe empirical power-law relationships that characterize how neural network performance improves with increases in key resources such as model size, dataset size, and compute [24].For in-context learning [29], it describes trends between prediction accuracy and context window length.We further show how these scaling trends depend on the underlying stochastic process characteristics: entropy, mixing rate, and state-space dimensionality.</p>
<p>Context window length: Given an observation sequence sampled from an HMM, LLM performance generally improves monotonically with increasing sequence length before plateauing.Representative examples are shown in Figure 3. Fluctuations may occur when the entropy of the sequence is high; additional examples are provided in Appendix D.</p>
<p>Entropy of transitions and emissions: Entropy determines the predictability of the observation sequence.The entropies of both the transition matrix A and the emission matrix B are positively correlated with the number of steps required for LLM convergence, as shown in the middle and right plots of Figure 4.However, this relationship is not strictly monotonic in practice.</p>
<p>Mixing rate: We control the mixing rate of synthetic HMMs using λ 2 , the second-largest eigenvalue of A. Lower values of λ 2 indicate faster mixing.As illustrated in Figure 4 (middle vs. right), for the same entropy level, convergence occurs significantly later when mixing is slow-indicating that slower mixing delays LLM learning.</p>
<p>Number of hidden states and observations:</p>
<p>The dimensionality of the state and observation spaces affects the maximum possible entropy.While larger state spaces intuitively allow for higher entropy, our experiments show that when entropy is held constant, varying the number of states does not impact LLM convergence rates.Detailed examples and discussion are provided in Appendix D.</p>
<p>Comparison to Baselines</p>
<p>We compare the in-context learning performance of pre-trained LLMs against several established methods commonly used for HMM prediction tasks, as summarized in Table 1.The conditional predictor P (O t+1 | O t−k:t ) leverages the true HMM parameters, like the Viterbi algorithm, but truncates the observation history to simulate the impact of limited memory on otherwise optimal inference.For learning-based baselines, we include the classical Baum-Welch (BW) expectationmaximization algorithm, which remains the statistical state of the art for HMM parameter estimation [56].We also evaluate an LSTM baseline, reflecting the widespread use of RNNs in applied domains such as neuroscience [54].Finally, we include an n-gram model to bridge our HMM results with recent findings on ICL performance in Markovian settings [40].Full algorithm descriptions and implementation details are provided in Appendix C.</p>
<p>Across a range of HMM configurations, we find that in-context learning with pre-trained LLMs consistently outperforms empirical learning baselines.A representative example is shown in Figure 5.Among the n-gram models with n ∈ {1, . . ., 4}, the bigram model performs best, aligning with its role as the maximum likelihood estimator for first-order Markov chains.However, because HMM observations are not Markovian when H(B) &gt; 0, the bigram model is inherently suboptimal.Trigram models converge more slowly than bigram due to increased data sparsity and resulting estimation bias.The Baum-Welch (BW) algorithm, while given the correct HMM structure and leveraging expectationmaximization, suffers from nonconvex optimization.Its global convergence is not guaranteed.Even when averaging across multiple random seeds and 4096 samples per setting, BW converges slowly and often unreliably.LSTM baselines, despite their flexibility, require significant computational resources and exhibit unstable accuracy across varying context lengths.Using the Hellinger distance metric, we find that LSTMs converge more slowly to the true output distribution compared to ICL.Notably, LLMs via ICL demonstrate clearly superior behavior across all baselines-achieving faster, more stable convergence to the ground-truth distribution and highlighting their surprising efficiency in modeling HMMs.</p>
<p>On the other hand, the conditional predictor P (O t+1 | O t−k:t ) can approach Viterbi-level performance, particularly when the mixing rate is low.This observation suggests that approximate prediction using a truncated observation history can be nearly as effective as statistically optimal inference in fast-mixing regimes, motivating the conjecture discussed in Section 3.3.</p>
<p>A Possible Theoretical Explanation</p>
<p>In this section, we attempt to explain the ICL behavior of pre-trained LLMs for HMM sequence prediction by comparing it with the spectral learning algorithm [21].This is motivated by empirical evidences [31,42] showing the convergence of spectral learning prediction to theoretical optimum (in limited settings).Furthermore, for partially observed linear dynamical systems, Li et al. [27] observes that transformers can learn statistically optimal predictions in-context when trained on many similar tasks in the meta-learning setting.These findings suggest that ICL by LLMs may exhibit similar performance characteristics to spectral learning algorithms.</p>
<p>A key idea in spectral learning literature is to compute the probability of observation sequences in terms of observation operators [21]:
For any o ∈ O, define A o := A ⊤ diag([B] 1,o , . . . , [B] M,o ),
where [B] i,j denotes the ij-th element of B. Then for any t &gt; 0,
P(O t+1 = o t+1 O 1:t = o 1 , . . . , o t ) = 1 ⊤ A ⊤ ot+1 A ⊤ ot • • • A ⊤ o1 π 1 ⊤ A ⊤ ot • • • A ⊤ o1 π , (1)
where 1 is all one vector of appropriate dimension.In this formulation, the conditional probability is estimated by first learning the spectral parameters (see supplementary) using training samples (in-context observations).Then, one can predict the next observation directly using these parameters along-with hidden state belief updates, without explicitly learning the matrices A, and B. This is therefore an example of improper learning, which has been extensively studied in related areas like linear dynamical systems [45].The spectral learning algorithm is theoretically well understood.The following theorem is obtained by extending the results by Hsu et al. [21] to single trajectory spectral learning.</p>
<p>Theorem 1 (Informal) Fix ϵ, δ &gt; 0. Let Σ 2 denote the pairwise probability matrix of observations such that [Σ 2 ] ij = P(O t+1 = i, O t = j).Suppose π &gt; 0 element-wise, and A, B are rank M .</p>
<p>Suppose L ≥ M , and let σ M (•) denote the M -th largest singular value.Suppose the observation operator A o &gt; 0 element-wise for all o ∈ O, and
t ≳ 1 1 − λ 2 (A) M 2 L ϵ 4 σ M (B) 2 σ M (Σ 2 ) 4 + M L ϵ 2 σ M (B) 2 σ M (Σ 2 ) 2 log 1 δ ,(2)
Then, with probability at least 1 − δ, the next observation prediction P(• O 1:t ) using spectral learning algorithm (detailed in Appendix F) satisfies the following upper bound in Hellinger distance,
H 2 P(O t+1 O 1:t = o 1 , . . . , o t ), P(O t+1 O 1:t = o 1 , . . . , o t ) ≤ ϵ(3)
Theorem 1 indicates that the scaling trends we observed in Section 3.1 are similar to those of spectral learning based predictions.Specifically, like our observations in Section 3.1, the prediction accuracy improves with more samples (i.e., larger t).The mixing rate, captured by
1 1−λ2(A)
, affects ICL and spectral learning similarly.This occurs because spectral parameter estimation from a single trajectory degrades with 1 1−λ2(A) -the faster the HMM mixes, the smaller the estimation error.Finally, the effect of entropy is captured by the observability conditions in Theorem 1. Estimation error is maximized when the HMM is unobservable, which corresponds to maximum entropy rate.The relationship between entropy and HMM observability has been well studied in literature [28,32].</p>
<p>One of the practical limitations of spectral learning algorithm is the requirement of rank conditions of A and B. Furthermore, the spectral learning algorithm is sensitive to the conditioning1 of the observed sequence, making its numerical performance robust only in limited settings.ICL by pretrained LLMs seems to handle such issues more gracefully, pointing to an intriguing gap in our statistical understanding for learning HMMs.</p>
<p>Guidelines for Practitioners: How to (creatively) use LLMs for your data?</p>
<p>LLMs' capacity in deciphering complex sequential patterns in language can be repurposed: as demonstrated in our synthetic experiments (Sections 2 and 3), pre-trained LLMs can effectively model HMM-generated sequences through ICL, achieving theoretically optimal prediction accuracy under favorable conditions.This section first translates these findings into practical guidelines for scientists, then demonstrates our observations on real-world data in animal behavior (Section 4.1).</p>
<p>Guideline 1: LLM in-context learning as a diagnostic tool for data structure and learnability.Our synthetic experiments (Sections 2.3 and 3.1) reveal that key HMM properties-notably entropy and mixing rate-strongly influence LLM ICL convergence behavior.Practitioners can leverage this relationship as a diagnostic tool for their own sequential data.If you observe that an LLM's ICL prediction accuracy on your data sequence steadily improves and saturates with increased context length (like Figure 3), this strongly indicates a learnable, non-random underlying structure.Our findings show that LLMs achieve near-optimal prediction accuracy on HMMs with clear, learnable patterns (Section 2.3).</p>
<p>The characteristics of this convergence provide further insight: faster convergence and higher final accuracy in LLM ICL experiments are consistently associated with HMMs having lower entropy (less randomness) and faster mixing rates, as shown in Section 3.1 and Figure 4. Conversely, if LLM ICL on your data converges slowly, requires exceptionally long contexts, or plateaus at low accuracy, this suggests the underlying process has high entropy or slow mixing dynamics-characteristics that inherently limit predictability and affect even optimal methods like Viterbi (Section 2.3).While calculating intrinsic HMM parameters from real-world data is challenging, you can qualitatively assess your data's learnability by comparing its ICL convergence profile to our synthetic HMM experiments (Figures 3 and 4).</p>
<p>Guideline 2: LLMs are data efficient in giving accurate next observation prediction in-context.Pre-trained LLMs offer a remarkably data-efficient and accessible approach for next-observation prediction through ICL, particularly valuable when rapid insights are needed or when data for training bespoke models is scarce.Our analyses (Section 3.2) show that LLM ICL achieves strong predictive performance with fewer domain-specific assumptions than Baum-Welch (which faces non-convexity issues) and fewer training resources than specialized sequence models like LSTMs/RNNs (which require substantial data and careful tuning).LLMs therefore deliver immediate predictive capabilities with stable performance on limited data.</p>
<p>A key practical advantage of LLM ICL is accessibility: while traditional methods require substantial computational expertise, applying pre-trained LLMs simply involves formatting data as text prompts, dramatically lowering barriers to sequence analysis.We are not positioning LLM ICL as a universal replacement for meticulously tuned, domain-specific models.Rather, its strength lies in providing strong, often surprisingly near-optimal predictions (Section 2.3) without any task-specific parameter updates or fine-tuning.Our key observation is that general-purpose LLMs can effectively model HMM-generated sequences and real-world scientific data tasks for which they were not explicitly pre-trained on.This highlights vast untapped potential and suggests that future LLM ICL development could yield transformative scientific tools.</p>
<p>Real World Examples</p>
<p>We extend our synthetic HMM findings to real-world biological decision processes, focusing on two extensively studied behavioral neuroscience datasets.Understanding how animals make decisions and learn efficiently remains a fundamental challenge, with researchers investing tremendous effort in high-precision modeling to capture underlying cognitive mechanisms.These datasets serve as ideal testbeds given the neuroscience community's modeling efforts and the inherent connections between agentic decision-making and HMMs (Figure 6).We represent animal decisions as discrete token sequences and compare LLM ICL performance against established domain-specific models in predicting future actions.</p>
<p>Decision-making Mice Dataset: This dataset, developed by the International Brain Laboratory (IBL) [25], has gained significant traction for studying mouse behavior within the neuroscience community.A popular study [3] characterizes mice choice behavior as an interplay among multiple interleaved strategies governed by hidden states in a HMM.Their GLM-HMM model (Figure 6 left) achieved an average prediction accuracy of 82.2%, outperforming standard approaches like the classic lapse model by 2.8%.For scientists investigating animal decision-making, these performance improvements are significant for advancing model fidelity and experimental interpretation.</p>
<p>We compare GLM-HMM to in-context LLMs on data from 7 mice, following the descriptions in Ashwood et al. [3].The experimental data consists of three components: stimulus, choice, and reward.For each trial, the mouse perceives a visual stimulus presented to their left or right, makes a choice by turning a steering wheel, and receives a water drop as reward when correct (Figure 6 middle).Each mouse is described by one sequence, composed of trials.The trials are ordered sequentially as they occurred during experiments.</p>
<p>Remarkably, when provided with a context of more than 1000 trials, LLM ICL consistently achieved higher prediction accuracy (average of 86.2%) than the expert-developed GLM-HMM (Figure 6 right).More importantly, the convergence trend of LLM ICL mirrors the in-context scaling we observed in synthetic experiments, particularly when entropy is relatively low.This observation suggests that mouse decision-making processes contain learnable structures that LLMs, even without task-specific training, can effectively identify and leverage for prediction.</p>
<p>Reward-learning Rats Dataset: The dataset from Miller et al. [36] allows us to explore LLM ICL capabilities on more complex learning behaviors.This task presents a significantly greater challenge than the IBL dataset for two primary reasons: first, animals receive no explicit stimuli to guide their choices towards potential rewards; secondly, the dataset captures the entire dynamic learning process itself, rather than behavior after learning.Consequently, the underlying behavioral dynamics are expected to be more complex and less stationary.To benchmark LLM ICL in this scenario, we compare its performance against a state-of-the-art model from recent work [10], which employed code generation and evolutionary search to discover interpretable symbolic programs well-fitted to this dataset.It is important to note that this state-of-the-art model results from an extensive, computationally intensive evolutionary search, setting a very high performance bar.</p>
<p>As shown in Figure 7, the prediction accuracy of LLM ICL on this dataset improves only marginally with increasing context length.This limited improvement parallels the ICL behavior we observed for synthetic HMMs characterized by high entropy and slow mixing rates.LLM ICL exhibits a substantial performance gap when compared to the specialized model.This outcome is consistent with our hypothesis that the underlying dynamics of this naturalistic learning process are complex, potentially pushing the limits of what current off-the-shelf LLMs can capture through ICL alone.</p>
<p>Related Works</p>
<p>LLMs and In-Context Learning.The surprising ability of LLMs to perform ICL [7] has led to significant interest in understanding its underlying mechanisms [11,22,53].Several works [20,52,55] interpret ICL as implicit Bayesian inference, suggesting that LLMs naturally perform posterior updates through attention mechanisms.Theoretical works analyzing transformers' ability to model Markovian data [6,13,33,40,41] show that they can efficiently learn fully observed Markov chains.However, the transition from observable Markov sequences to latent-variable models like HMMs remains underexplored.Recent studies also evaluate LLMs' predictive performance on structured tasks, including dynamical systems [29], density estimation [30], and time series forecasting [19,49].While these works explore LLMs' empirical capabilities, they do not systematically analyze how intrinsic properties of underlying stochastic processes-such as mixing time and entropy-affect ICL performance.Our work fills this gap by providing a controlled study on synthetic HMMs and offering theoretical conjectures for the observed scaling trends in ICL performance.</p>
<p>Spectral learning (SL) HMMs.SL algorithms have emerged as a compelling tools for learning HMMs from observations, using method-of-moments to learn the spectral parameters.Several works [2,21,42] construct matrices with observations, perform singular value decomposition and projection to obtain the beliefs of the HMM operators.Recent work [31] improves the practicality of SL by projecting the probability beliefs onto simplex after every belief update.Despite these, SL algorithms have practical limitations and make assumptions on how observations carry information about the HMM dynamics.ICL seems to handle such issues by learning better observation operators without requiring the limiting assumptions of SL algorithms.</p>
<p>Neuroscience and Animal Behavior.Many neuroscience studies model animal behavior as HMMs [47,50].A common modern approach involves training data-specific RNNs and finding attractor dynamics [4,23,54], which can be highly data-inefficient.Large generative models have accelerated neuroscience discoveries, from data processing [44,46] to model discovery [10].In our work, we present a novel approach, leveraging the frontier of AI to help scientists understand their data, focusing on discrete behaviors [36,43].</p>
<p>Discussion &amp; Takeaways</p>
<p>LLMs are surprisingly effective HMM learners through in-context learning: We observe that LLM prediction accuracy often converges towards theoretical optimum achieved by the Viterbi algorithm, which knows true model parameters.Contrasting with iterative and computationally intensive traditional HMM estimation algorithms or neural architectures (e.g., LSTMs), LLM ICL offers simplicity as a tool.As demonstrated by the competitive performance to domain-specific models on real-world animal decision-making tasks, LLM ICL offers a new avenue for rapid data exploration.LLMs can serve as a "zero-shot statistical tool", enabling scientists to diagnose data complexity and generate future predictions without the overhead of extensive model development-addressing a common bottleneck in many scientific workflows.</p>
<p>Existing gaps: While we observe promising trends, our experiments also point to existing gaps in the broad application of LLM ICL.A primary bottleneck is the reliance on discrete tokenization, which poses challenges for modeling continuous, real-valued, or high-dimensional observations-such as neural recordings-within the ICL framework.Although our experiments successfully employed tokenization strategies for discrete sequences (see Appendix E.2 for ablations), adapting LLMs to handle continuous state-space models or direct real-valued inputs remains an open question.Moreover, despite achieving high predictive accuracy, the inherently "black-box" nature of LLMs limits interpretability.While our findings demonstrate that LLMs can effectively model HMM dynamics, extracting explicit and interpretable parameters-such as transition or emission probabilities-from the model's internal representations is nontrivial.Yet, such interpretability is often central to the goals of scientists and practitioners seeking to understand underlying system dynamics.We hope this work lays the groundwork for future research into extending ICL to continuous domains and developing tools for extracting interpretable structure from LLMs.</p>
<p>Call to action: Realizing the full potential of LLMs and HMMs to advance our understanding of complex systems demands a multidisciplinary effort.There is a growing need for next-generation foundation models specifically designed to meet the challenges of scientific data-ranging from structured sequences to high-dimensional, continuous signals.Moving beyond adaptations of NLPfocused models, such advancements are critical not only for enabling more effective scientific analysis, but also for deepening our understanding of in-context learning and the structure embedded within human language corpora.Ultimately, this progress will be essential to unlocking the transformative potential of LLMs in scientific discovery across a broad range of disciplines.</p>
<p>Appendices Table of Contents</p>
<p>A Additional Background on HMMs</p>
<p>In this section, we define in detail the HMM settings we are interested in, including the conditions for Markov chains to converge to unique stationary distributions.Recall that a HMM is characterized by the Markov chain's initial state distribution and its state transitions, along with the emission probabilities of an observation given the hidden state.With finitely many states and observations, without loss of generality, states take values in X = {1, 2, . . ., M } while observations take values in O = {1, 2, . . ., L}.The initial state distribution is denoted as π ∈ R M with π j the probability of starting in state j, the state transitions are describe by the matrix A ∈ R M ×M with elements a ij the probability of transitioning to state j from state i, and the emission matrix B ∈ R M ×L contains b jl the probability of observing l when in hidden state j.The triple λ = (π, A, B) completely parameterizes a finite-alphabet HMM.</p>
<p>Let {X 1 , X 2 , ...} denote a discrete-time Markov chain taking values in X with transition matrix A. Let p (n) ij = P(X t+n = j|X t = i) denote the n-step transition probability between states i, j ∈ X .State j is said to be accessible from state i if there exists an integer n ≥ 1 such that p
(n) ij &gt; 0. A subset C ⊆ X is called irreducible if every pair of states i, j ∈ C is mutually accessible. The period of state i is defined as c(i) = gcd{n ≥ 1 : p (n)
ii &gt; 0}, the greatest common divisor of all possible return times.State i is aperiodic if c(i) = 1.A Markov chain is termed geometrically ergodic if it is irreducible and aperiodic, which guarantees convergence to a unique stationary distribution µ ∈ R M satisfying µ = µA.The mixing rate ρ ∈ [0, 1) is such that for all states i, j ∈ X , there exists a constant C ≥ 0 for which |p
(n) ij − µ j | ≤ Cρ n for all n ≥ 1.
For a finite-alphabet HMM, ρ equals λ 2 , the second-largest eigenvalue of A. We run experiments on a few non-ergodic cases, while the majority of HMMs are with ergodic state transitions to avoid dependence on the initial state.</p>
<p>The entropy H(X) of a discrete random variable X is defined as H(X) = − x∈X p(x) log p(x).A fundamental property of entropy is that conditioning reduces uncertainty: for any two random variables X and Y , we have H(X|Y ) ≤ H(X), with equality holding if and only if X and Y are statistically independent [12].By applying the chain rule of entropy, the joint entropy of a stochastic process can be expressed as H
(X 1 , X 2 , . . . , X n ) = n i=1 H(X i |X i−1 , . . . , X 1 ).
For a Markov chain with stationary distribution µ, the entropy rate is defined as
H(X ) = lim n→∞ 1 n H(X 1 , X 2 , . . . , X n ) = − i,j µ i a ij log a ij ,
which depends solely on the transition matrix A. We additionally define the entropy of the emission matrix B as − j,l µ j b jl log b jl , which quantifies the average uncertainty in observations given the underlying states.Although the entropy rate of the observation process in a HMM has no known closed-form expression, it can be bounded as
H(O n |O n−1 , X n−1 , . . . , O 1 , X 1 ) ≤ H(O) ≤ H(O n |O n−1 , . . . , O 1 )
. As A defines transitions from X t to X t+1 , and B determines sampling O t from X t , the entropies of A and B combined help us to control the entropy lower bound of the sampled HMM sequence.</p>
<p>B Additional Details of Experimental Setup</p>
<p>Construct A with specific mixing rate, entropy, and steady state distribution.For an ergodic Markov chain that converges to a unique stationary distribution, the stochastic matrix A can be decomposed into eigenvalues and eigenvectors with the ordering shown in Figure 8, where ⃗ 1 ∈ R M is a vector of ones, λ 2 is the second-largest eigenvalue of A, and µ is the stationary distribution [15].We leverage this decomposition to construct A with predefined λ 2 and µ.To determine the remaining eigenvalues and eigenvectors, we formulate an optimization problem based on the following requirements: (1) all entries of A are non-negative; (2) each row of A sums to 1; (3) U −1 U = 1; and (4) all remaining eigenvalues have magnitudes not exceeding λ 2 .The optimization problem has the following form, where we translate the constraints above into penality terms.min
λ 3:M ,V2 M i,j=1 max{−a ij , 0} + M j=1 M i=1 a ij − 1 2 + M i,j=1 (VU − I) 2 ij + M i=3 max{λ i − λ 2 , 0} s.t. A = Vdiag(1, λ 2 , λ 3 , ..., λ M )U, V = [1 V 2 ] , U = µ V † 2
This is a nonconvex problem, which we solve using first order methods with pytorch.We randomly initialize the free variables λ 3 , ..., λ M and V 2 and then run 5000 iterations of Adam with step size 0.01 and default values for other parameters.After the optimizer terminates, we reject instances which do not satisfy the constraints exactly.By initializing with multiple random seeds, we generate matrices spanning the desired entropy spectrum.Steady state distribution.We construct steady state distributions with varying skewness using the Beta distribution with α = 1 and different values of β.When α = 1 and β = 1, the resulting steady state distribution is uniform.As β increases, the distribution becomes increasingly skewed toward smaller state indices.Unless otherwise specified (Appendix D.3), we use a uniform steady state distribution as the default configuration.</p>
<p>Entropy for visualizations.The entropy definitions H(A) and H(B, µ) we introduced in Section 2.1 are used for constructing HMM parameters and sampling trajectories.For graphing Figure 4 (Left), we define normalized entropy considering both matrices:
H(A, B, µ) = H(A) + H(B, µ) log M + log L .
We define H(A) = H(A)/ log M and H(B, µ) = H(B, µ)/ log L for Figure 4 (Middle) and (Right).</p>
<p>T is when LLM converges to Viterbi.The concept of "convergence", though intuitive to human eyes, requires a specific numerical definition for plots like Figure 4. We define convergence as the point where two conditions are simultaneously satisfied: (i) the accuracy difference between Viterbi and LLM is within 0.025, and (ii) LLM achieves at least 95% of Viterbi's accuracy.We use both constant and relative thresholds to ensure a strict convergence definition that accounts for different baseline performance levels across experimental conditions.</p>
<p>Hellinger Distance.For two discrete probability distributions P, Q ∈ R L , the Hellinger distance is defined as
D Hellinger (P, Q) = 1 √ 2 √ L i=1 ( P i − Q i ) 2 .</p>
<p>C Details of Benchmark Models</p>
<p>In this section, we provide descriptions and pseudocode for the benchmark models we use (Table 1).The executable code for all methods are included in supplemental materials.Algorithm 1: Viterbi Algorithm Input: States X = {1, 2, . . ., M }, initial distribution µ, transition matrix A, emission matrix B, and observation sequence {o 1 , . . ., o T }.Output: Most likely state sequence path = {x 1 , . . ., x T } Initialization:
P[0][s] ← µ[s] • B[s][o 1 ] for all s ∈ X ; Forward recursion: for t = 1 to T − 1 do for s ∈ X do P[t][s] ← max r∈X {P[t − 1][r] • A[r][s] • B[s][o t ]}; Q[t][s] ← arg max r∈X {P[t − 1][r] • A[r][s] • B[s][o t ]}; end end Backtracking: path[T − 1] ← arg max s∈X P[T − 1][s]; path[t] ← Q[t + 1][path[t + 1]] for t = T − 2, . . . , 0; return path
Viterbi algorithm.The Viterbi algorithm is a dynamic programming technique for efficiently finding the most likely sequence of hidden states in a Markov model, given a sequence of observations.It iteratively computes the highest probability path to each state at time t by considering all possible predecessor states at time t − 1, their transition probabilities, and the emission probabilities of the current observation.Rather than exhaustively evaluating all M T possible state sequences, Viterbi maintains only the M most promising paths at each time step, storing both their probabilities and the penultimate states that maximize these probabilities.After computing probabilities for all time steps, the algorithm traces backward from the most probable final state to reconstruct the optimal state sequence.We use the most probable final state and ground-truth A and B to calculate the prediction distribution of the next observation.
α t−k [s] ← B[s][o t−k ] • µ[s] for all s ∈ X ; α i [s] ← B[s][o i ] • r∈X A[r][s] • α i−1 [r] for i = t − k + 1, . . . , t, s ∈ X ; Normalize to get posterior: P (s|o t−k:t ) ← αt[s] s ′ ∈X αt[s ′ ] for all s ∈ X ; Prediction step: P (s|o t−k:t ) ← r∈X A[r][s] • P (r|o t−k:t ) for all s ∈ X ; Marginalize over states: P (o t+1 |o t−k:t ) ← s∈X B[s][o t+1 ] • P (s|o t−k:t ); return P (o t+1 |o t−k:t )
Optimal inference with truncated memory P (O t+1 |O t−k:t ).The forward-based prediction algorithm computes the probability of the next observation in a hidden Markov model by using a three-step approach.First, it calculates the posterior distribution over current hidden states via the forward algorithm, recursively processing the observation window while accounting for transitions and emissions.Second, it projects this belief state forward by applying the transition matrix to compute the distribution over next possible states.Finally, it determines P (o t+1 |o t−k:t ) by marginalizing over all possible next states, weighting each by its emission probability.</p>
<p>Baum-Welch algorithm.The Baum-Welch algorithm is an expectation-maximization method for estimating hidden Markov model parameters.It iteratively alternates between computing state posteriors γ t (s) and transition posteriors ξ t (s, r) via forward-backward recursion (E-step), and updating parameters to maximize likelihood (M-step): setting the initial distribution to γ 1 , transition Algorithm 3: Baum-Welch Algorithm Input: States X = {1, 2, . . ., M }, observations Y = {1, 2, . . ., L}, observation sequence {o 1 , . . ., o T }, initial parameters µ (0) , A (0) , B (0) , and threshold ϵ.
Output: Refined parameters µ, A, B Initialize: µ ← µ (0) , A ← A (0) , B ← B (0) , L prev ← −∞; repeat L prev ← L; E-Step: Forward pass: α 1 [s] ← B[s][o 1 ] • µ[s] for all s ∈ X ; α t [s] ← B[s][o t ] • r A[r][s] • α t−1 [r] for t = 2, . . . , T , s ∈ X ; L ← s α T [s]; Backward pass: β T [s] ← 1 for all s ∈ X ; β t [s] ← r A[s][r] • B[r][o t+1 ] • β t+1 [r] for t = T − 1, . . . , 1, s ∈ X ; Expected counts: γ t [s] ← αt[s]•β t [s] L for t = 1, . . . , T , s ∈ X ; ξ t [s][r] ← αt[s]•A[s][r]•B[r][ot+1]•β t+1 [r] L for t = 1, . . . , T − 1, s, r ∈ X ; M-Step: µ[s] ← γ 1 [s] for all s ∈ X ; A[s][r] ← T −1 t=1 ξt[s][r] T −1 t=1 γt[s] for all s, r ∈ X ; B[s][v] ← T t=1 γt[s]•1(ot=v) T t=1 γt[s] for all s ∈ X , v ∈ Y; until |L − L prev | &lt; ϵ; return µ, A, B
probabilities to normalized expected transitions, and emission probabilities to normalized observation counts per state.This process continues until the log-likelihood converges, yielding locally optimal parameters that maximize the probability of generating the observed sequence.We use the learned parameters to predict next observation similar to the Viterbi algorithm.memory cells regulated by input, forget, and output gates.These gates control information flow, allowing LSTMs to selectively retain relevant historical patterns while discarding irrelevant information.LSTMs excel at next-observation prediction tasks by capturing both short-term correlations and long-term dependencies in the observation history.The network processes a window of prior observations sequentially, updating its hidden state to encode temporal patterns, then projects this state through a softmax layer to generate a probability distribution over possible next observations-making LSTMs particularly effective for forecasting future values in time series where the prediction depends on complex patterns spanning multiple time scales.
for t = n − 1 to T − 1 do context ← [o t−(n−</p>
<p>D Additional Synthetic Experiment Results</p>
<p>This section presents additional results from synthetic experiments.All methods are evaluated using the average performance over 4,096 sequences, with the exception of LSTM, which is evaluated on 16 sequences due to its high computational cost.Consequently, the LSTM results exhibit higher variance.Nonetheless, in metrics such as Hellinger distance-which account for the full output distribution rather than relying solely on the argmax for accuracy-LSTM underperforms compared to the LLM most of the time.</p>
<p>D.1 Varying Entropy of A</p>
<p>In this section, we present detailed results on varying the entropy of A matrix over 4/8/16 states and emissions, reporting accuracies and Hellinger distances.</p>
<p>D.2 Varying Mixing Rate of A</p>
<p>In this section, we present detailed results on varying the mixing rate (λ 2 ) of A matrix over 4/8/16 states and emissions, reporting accuracies and Hellinger distances.</p>
<p>D.3 Varying Steady State Distribution of A</p>
<p>In this section, we present detailed results on varying the steady state distributions of A matrix over 4/8/16 states and emissions, reporting accuracies and Hellinger distances.We construct steady states with different skewness using Beta distribution with α = 1.Notably, with α = 1 and β = 1, the steady state distribution is uniform.As we increase β, the distribution becomes more skewed.We test with β = 1, 2, 3, representing uniform, skewed, and very skewed respectively.</p>
<p>D.4 Discussions</p>
<p>When LLMs fail to converge.While LLMs converge to Viterbi performance efficiently under most HMM parameter settings (scaling trends summarized in Section 3.1), we identify two conditions where convergence fails or proceeds exceptionally slowly.First, when entropy of A or B is approaches its maximum (log M or log L respectively), the prediction accuracy gap ε at context length 2048 remains substantial.For instance, in the last row of Figure 9 with M = 16 and the entropy of A is 3.5 (near the maximum of log 16 = 4), the LLM (Qwen2.5-7B)exhibits gradual convergence with a persistent gap.Second, when mixing is slow (λ 2 approaches 1), such as in the third-to-last row of Figure 11 with M = 16 and λ 2 = 0.95, a performance gap persists even at maximum context length.</p>
<p>Importantly, the Viterbi algorithm also struggles under these challenging conditions.Under high entropy, as shown in the last row of Figure 9, Viterbi accuracy barely exceeds random prediction (0.25/0.125/0.0625for L = 4/8/16).Under slow mixing, such as in the fourth row of Figure 11 with M = 8 and λ 2 = 0.99, Viterbi algorithm requires context length 512 to achieve peak performance.These results demonstrate that LLM performance degradation under high entropy and slow mixing conditions reflects fundamental limits of stochastic system learnability-arising from random dynamics and long-range dependencies-that affect even optimal inference methods.When (normalized) entropy H is held constant, varying the number of states does not affect the LLM convergence rate.We provide concrete evidence for this claim in Figure 9, where rows 1, 3, and 6 all have the same normalized entropy H(A) = 0.5.Across each column, the Qwen2.5-7Bconvergence curves for these three rows exhibit nearly identical shapes, demonstrating that the convergence rate depends primarily on normalized entropy rather than absolute state space size.</p>
<p>We emphasize that convergence rate differs from convergence target-the Viterbi performance.While the rate of improvement remains consistent across different state space sizes (when normalized entropy is fixed), larger state spaces result in lower achievable prediction accuracy due to increased task difficulty.</p>
<p>E Ablations on LLMs</p>
<p>In this section, we provide the results on the families, sizes, and tokenization of the LLMs.</p>
<p>E.1 LLM Size</p>
<p>We compare Qwen and Llama model families with seven different models.We found that their performances are similar, with slight degradation when the model size is small.</p>
<p>F Spectral Learning HMMs for Prediction Task</p>
<p>Notations: We use [X] i,j to denote the element of matrix X at its i-th row and j-th column.The indicator function 1 {x=i} is 1 only when x = i and is 0 otherwise.We use 1 M to denote a vector of all 1's with dimension M .We use the notation [L] = {1, 2, . . ., L}. ∥•∥ denotes the Frobenius norm for matrices, and depending on the context it denotes ℓ 1 or ℓ 2 norm for vectors.For a Markov chain with transition matrix A, we let π ∈ R M + denote the initial state distribution.We assume that π is also the stationary distribution of the Markov chain.This can be achieved by taking samples after a burn-in time which is proportional to 1 1−λ2(A) .Note that π t = (A t ) ⊤ π is essentially a convex combination of rows of matrix A t , then by triangle inequality, we have ∥π t − π ∞ ∥ 1 ≤ max i∈[M ] ∥([A t ] i,: ) ⊤ − π ∞ ∥ 1 .Thus, for an ergodic Markov matrix A, we define the following to quantify the convergence of ∥π t − π ∞ ∥ 1 .For an ergodic Markov matrix A ∈ R M ×M + , let τ MC &gt; 1 and ρ MC ∈ (λ 2 (A), 1) be two constants [26,Theorem 4.9] such that
max i∈[M ] ∥([A t ] i,: ) ⊤ − π ∞ ∥ 1 ≤ τ MC ρ t MC .(4)
Furthermore, we define the mixing time of A as t MC (ϵ) := min t ∈ N : max
i∈[M ] 1 2 ∥([A t ] i,: ) ⊤ − π ∞ ∥ 1 ≤ ϵ .(5)
Note that τ (M) and τ MC have similar roles except τ (M) is usually used to study state matrices while τ MC is for Markov matrices.For a square M, we have ∥M k ∥ ≤ τ (M)ρ(M) k , and for a Markov matrix, we have
∥A t − 1 M π ⊤ ∞ ∥ ≤ τ MC ρ t MC .</p>
<p>G Additional Real World Experiments</p>
<p>We design an additional experiment using real-world datasets to validate our findings.We artificially simulate different emission entropy levels for the same underlying hidden transition process by controlling the amount of information included in the observation sequence.Using complete information corresponds to low emission entropy, while limiting information artificially increases emission entropy.</p>
<p>We use the IBL decision-making mice dataset [25].In our LLM in-context learning experiment, we implement four ablation conditions that vary the information presented in each trial: (i) "choice only"; (ii) "choice reward"; (iii) "stimulus choice"; (iv) "stimulus choice reward".Note that the baseline GLM-HMM uses all available information as in condition (iv).These ablations describe the same underlying mouse decision-making sequences but with varying levels of environmental state detail.The results shown in Figure 23 reveal significant differences across ablation conditions: while "stimulus choice reward" achieves performance exceeding GLM-HMM, "choice reward" is merely at chance level with its convergence trend similar to the synthetic experiments when the transitions or emissions are near random.This demonstrates that accurately modeling mouse decision-making in this task requires both stimulus and reward information.</p>
<p>These findings highlight a broader principle: obtaining appropriate information (corresponding to low emission entropy) is essential for successful task modeling.This experiment parallels real-world experimental design, where scientists must choose which signals to collect when studying task structure.When researchers omit critical information needed to describe a sequence, it can easily lead to incorrect conclusions about the underlying process.</p>
<p>arXiv:2506.07298v2 [cs.LG] 11 Jun 2025</p>
<p>Figure 2 :
2
Figure 2: Properties of HMMs.</p>
<p>Figure 3 :
3
Figure 3: (Left) We define T as when LLM converges (see Appendix B for computation metric), and ε as the final accuracy gap at sequence length 2048.(Middle) Examples when LLM accuracy converges to Viterbi.Each curve represents a different HMM parameter setting.LLM ICL shows consistent convergence behavior.(Right) Examples of convergence in Hellinger distance (distance between two probability distributions).LLM ICL is not just "guessing" the most probable output, but converging distributionally.</p>
<p>Figure 4 :
4
Figure 4: (Left) Convergence gap ε increases with higher mixing rate (slower mixing) and higher entropy.This plot is showing results averaged across all HMM configurations we tested.(Right) Slower mixing (λ 2 = 0.5, 0.75) shows delayed convergence compared to (Middle) fast mixing (λ 2 = 0.95, 0.99) at similar entropy levels.</p>
<p>Figure 5 :
5
Figure 5: HMM parameters M = 8, L = 8, H(A) = 1.5, H(B) = 1.(Left) The gap between P (O t+1 |O t ) and Viterbi is small when mixing is fast.(Middle) Accuracy comparison with baselines.(Right) Hellinger distance measures distance between two probability distributions.</p>
<p>Figure 6 :
6
Figure 6: IBL dataset mice decision-making task.(Left) GLM-HMM model developed by neuroscientists.(Middle) A cartoon illustration of the task.A mouse observes a visual stimulus presented on one side of a screen, with one of six possible intensity levels.It then chooses a side, receiving a water reward if the choice matches the stimulus location.(Right) LLM ICL performance curve averaged across all animals, with 1-σ error bar.Its prediction accuracy steadily increase with longer context window, exceeding the domain-specific model performance.</p>
<p>Figure 7 :
7
Figure 7: Rat reward-learning task.(Left) Analog agent learning to HMMs.(Middle) A cartoon illustration of the more challenging task.No stimulus is presented on either side; instead, the reward probabilities for left and right choices evolve independently via random walks.As the optimal choice changes over time, the rat must learn and adapt its decisions based solely on the history of past rewards.(Right) LLM ICL performance curve averaged across all animals, with 1-σ error bar.Its performance curve improves only marginally with increasing context length.</p>
<p>1 Figure 8 :
18
Figure 8: The singular value decomposition of ergodic unichain Markov matrix A. The darker shaded region is pre-defined for our controlled experiments.The lighter shaded region is randomly initialized and calculated using a neural network.</p>
<p>Algorithm 2 :
2
Compute P (O t+1 |O t−k:t ) Input: States X = {1, 2, . . ., M }, initial distribution µ, transition matrix A, emission matrix B, and observation sequence {o t−k , . . ., o t }.Output: Probability of next observation P (o t+1 |o t−k:t ) Forward pass over observation window:</p>
<p>Algorithm 4 :
4
n-gram Based Next-Observation Prediction Input: Observation sequence O = {o 1 , . . ., o T }, context length n − 1, smoothing parameter δ Output: n-gram model for predicting P (o t |o t−(n−1):t−1 ) Count extraction: counts n , counts n−1 ← empty associative arrays;</p>
<p>Figure 9 :
9
Figure 9: Accuracies of six methods across different A entropy, B entropy, number of states, and number of emissions with λ 2 = 0.75 and uniform steady state distribution.</p>
<p>Figure 10 :
10
Figure 10: Hellinger distances of six methods across different A entropy, B entropy, number of states, and number of emissions with λ 2 = 0.75 and uniform steady state distribution.</p>
<p>Algorithm 6 : 1 N N k=1 1
611
Spectral Learning-Based Prediction Input: Number of hidden states M , number of observations L, sequence {o 1 , . . ., o N } Output: Conditional probability distribution P (O N +1 |O 1:N = o 1:N ) Estimate empirical probabilities: for all combinationsi, j, n ∈ [L] do [ P1 ] i ← {o k =i} ; [ P2 ] i,j ← 1 N N k=1 1 {o k =i,o k−1 =j} ; [ P3,n ] i,j ← 1 N N k=1 1 {o k =i,o k−1 =n,o k−2 =j} ; end Compute SVD for dimensionality reduction: Û ← left singular vectors of P2 corresponding to M largest singular values; Estimate spectral parameters: b1 ← Û⊤ P1 ; b∞ ← ( P⊤ 2 Û) † P1 ; for each observation o ∈ [L] do Ĉo ← Û⊤ P3,o ( Û⊤ P2 ) † ; end Hidden state belief update: b1 ← initial belief; for τ = 1 to N do bτ+1 ← Ĉoτ bτ b⊤ ∞ Ĉoτ bτ; end Conditional probability prediction: for each possible next observation o N +1 ∈ [L] do P (O N +1 = o N +1 |O 1:N = o 1:N ) ← b⊤ ∞ Ĉo N +1 bN+1 L k=1 b⊤ ∞ Ĉk bN+1 ; end return P (O N +1 |O 1:N = o 1:N ) F.1 Preliminaries</p>
<p>Figure 23 :
23
Figure 23: LLM in-context learning prediction accuracy for mice decision-making task with varying types of information in the observed sequences.Each line is averaged over 7 mice, with 1-σ error bar.The model we use is Qwen2.5-7B.</p>
<p>Table 1 :
1
).Second, we generate observation sequences {o 1 , o 2 , . ..} from this parameterized model.Third, we evaluate the ability of candidate models to predict the next observation o t+1 given preceding observations o 1:t .Methods for HMM prediction task.
We systematically vary five control parameters and consider 234 total HMM settings: (1) stateand observation space dimensions, with M, L ∈ {2, 4, 8, 16, 32, 64}; (2) mixing rate of the hid-den Markov chain, with λ 2 ∈ {0.5, 0.75, 0.95, 0.99}, where λ 2 is the second-largest eigenvalueof A; (3) skewness of the stationary distribution µ (uniform or non-uniform); (4) entropy of thetransition and emission matrices A and B, ranging from deterministic (zero entropy) to maximumentropy (random); and (5) initial state distribution π (uniform or deterministic). While generat-
ing π and B is straight-forward, for matrix A, we define a constrained optimization problem and solve using first order optimization.See Appendix B for additional details.For each parameter configuration, we sample 4,096 state-observation sequence pairs, each of length 2,048.We assess model performance across context lengths ranging from 4 to 2,048 observations, specifically {4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048}.For each HMM setting, we report performance metrics Method Input Variable Description Viterbi [51] O 1:t , λ ∅ Finds most likely hidden state sequence P (O t+1 |O t−k:t ) O t−k:t , λ k Direct conditional probability Baum-Welch [5] O 1:t , M ∅ EM algorithm for parameter estimation LSTM (RNN) O 1:t ∅ Neural network with memory cells n-gram O 1:t n Predicts based on preceding (n − 1) tokens</p>
<p>Algorithm 5: LSTM for Single Sequence Prediction Input: Observation sequence O = {o 1 , . . ., o T }, vocabulary size V , embedding dimension d, hidden dimension h, layers L, learning rate α, epochs E Output: Trained LSTM model for P (o t+1 |o 1:t ) Architecture: Initialize embedding layer: Embedding : Z → R d ; Initialize LSTM layers: LSTM : R d → R h with L layers; Initialize output projection: Linear : R h → R V ; For prefix O 1:t , compute P (o t+1 |O 1:t ) = softmax(model(O 1:t )); return model RNN LSTM.LSTM networks are specialized recurrent neural architectures designed to model sequential data through
Initialize optimizer with learning rate α;Training: for epoch = 1 to E dofor t = 1 to T − 1 dox ← O 1:t ;// Use all previous observations as contexty ← O t+1 ;// Next observation as targetUpdate model to maximize P (y|x) via gradient descent;endendInference:endModel construction with smoothing: V ← number of unique symbols in O;for each observed context c in counts n−1 dofor each unique observation o in O domodel[c, o] ← countsn[c⊕o]+δ V •δ+countsn−1[c] ;endendBack-off for unseen contexts: P unif (o) ← 1 V for all o;P (o t |o t−(n−1):t−1 ) ←model[[o t−(n−1) , . . . , o t−1 ], o t ], if context observed P unif (o t ), otherwise;return modeln-gram. n-gram models provide an elegant, computationally efficient framework for next-observationprediction in Markov chain processes by directly estimating conditional probabilities from observed
1) , . . ., o t−1 ]; Increment counts n [context ⊕ o t ] and counts n−1 [context]; sequences.These models embody the Markov assumption that P (O t+1 |O 1:t ) ≈ P (O t+1 |O t−n+2:t ), making them particularly effective for stochastic processes where future states depend only on a limited history of previous states.For first-order Markov chains, bigram models (n = 2) precisely capture the underlying transition dynamics, while higher-order dependencies can be modeled by increasing n.</p>
<p>Monotonicity of LLM performance with respect to context length.We observe that LLM performance almost always improves monotonically with longer context length-a property notably absent in other learning baselines.Even excluding LSTM from this comparison (due to high variance from averaging over fewer sequences, as discussed in the first paragraph in Appendix D), both Baum-Welch and bigram models lack monotonic convergence behavior.For Baum-Welch, the accuracy graphs (Figures 9, 11, and 13) reveal multiple cases where performance "dips" and recovers, or deteriorates as context length increases.The Hellinger distance graphs (Figures10, 12, and 14) provide clearer evidence that both BW and bigram exhibit non-monotonic learning patterns.In most cases, LLM Hellinger distance decreases monotonically, while BW and bigram display erratic behavior: sometimes experiencing early-context "bumps", other times starting very close to the ground truth emission distribution (occasionally even closer than the oracle Viterbi by empirical chance) before gradually converging to statistically sound distributions.Importantly, when BW or bigram achieve lower Hellinger distances, this does not necessarily indicate better performance-the corresponding prediction accuracy graphs often show poor results, highlighting the distinction between distributional similarity and predictive capability.</p>
<p>(4,ure16): Hellinger distances of seven models across different steady state distributions, B entropy, number of states, and number of emissions with (0, 0.5, 2) A entropy for(4, 8,16)states respectively and (0.99, 0.95, 0.75) λ 2 for (4, 8, 16) states respectively.
Llama-3.1-8BLlama-3.2-3BLlama-3.2-1BQwen2.5-7BQwen2.5-3BQwen2.5-1.5BQwen2.5-0.5BObservations: 4 B entropy: 0Observations: 4 B entropy: 1Observations: 8 B entropy: 0Observations: 8 B entropy: 1Observations: 16 B entropy: 0Observations: 16 B entropy: 1States: 4 Steady State: uniformStates: 4 Steady State: skewedStates: 4 Steady State: very skewedStates: 8 Steady State: uniformStates: 8 Steady State: skewedStates: 8 Steady State: very skewedStates: 16 Steady State: uniformStates: 16 Steady State: skewed800.50 0.52 0.54 0.56 0.58 0.60 0.620.48Context Length
This issue should not be insurmountable, similar to how (appropriately tuned) regularization can overcome poor conditioning in ridge regression[37]. However, we are unaware of prior work which provides a solution.
AcknowledgementsYD thanks Kristin Branson and Kimberly Stachenfeld for insightful technical discussions, and Owen Oertell for their support.ZG is supported by LinkedIn through the LinkedIn-Cornell Grant.This work was partly funded by NSF CCF 2312774, NSF OAC-2311521, a gift to the LinkedIn-Cornell Bowers CIS Strategic Partnership, and an AI2050 Early Career Fellowship program at Schmidt Science.E.2 TokenizationIn this section, we evaluate three tokenization strategies: ABC, which encodes emissions as single letters; 123, which encodes them as single digits; and random, which maps emissions to random tokens from the LLM's tokenizer.For the random strategy, we specifically map emissions to special tokens (!@#$).All experiments are conducted using the Qwen2.5-1.5Bmodel, and the results are presented below.We observe that all tokenization methods converge to similar performance levels in terms of accuracy, with ABC converging slightly faster when the entropy of A is large.This suggests that the choice of tokenization has limited impact on final performance.In our experiments, we adopt the ABC tokenization for maximum performance on the LLM.However, when the entropy of matrix A is low, ABC tokenization exhibits significantly lower initial accuracy and a higher Hellinger distance with short context length.We hypothesize that this is due to the increased likelihood of repetitive state sequences early in the sequence-for example, 'AAAAA...'.During pretraining, such repeated n-gram patterns are often filtered out, as they could cause loss spikes[38].As a result, the model may have limited exposure to these patterns, leading to poor initial performance on such inputs.F.2 Sample Complexity AnalysisIn this section, we analyze the sample complexity of spectral learning algorithm (Alg 6) when the observation sequence is coming from a single trajectory.Our proof builds on[21]by modifying their analysis in Appendix A to incorporate single trajectory learning.We only present the Sample complexity analysis here and refer the reader to[21]for the remaining proofs.F.3 Proof of Theorem 1Fix 2 &lt; T &lt; N , and recall from[21]thatwhen the initial distribution π is the stationary distribution of the Markov chain.In the following, we will present three different estimators for each of these quantities and analyze their convergence.• Estimation of P 1 : Let N := ⌊ N T ⌋, and without loss of generality, suppose N T is an integer.Suppose {o T } be the i.i.d.samples obtained from N independent trajectories of the HMM.We define the following three estimators of P 1 ,for all ℓ = 0, . . ., T − 1.By triangle inequality, we have[21] showed that, with probability at least 1 − δ, we have, ∥ P(⊥)In the following, we will upper bound the term ∥ P1 − P(⊥) 1 ∥ by considering entry-wise concentration of each ℓ-th subtrajectory as follows: We haveFirst, we observe that ET =i} | ≤ 1, almost surely.However, the summation in (8) has weakly dependent terms.Therefore, we use the Bernstein type inequality for a class of weakly dependent and bounded random variables proposed in[35].Before that, we need to upper bound the variance of the summation in(8).Observing thatIn the following, we will upper bound each term in (9) separately.We begin with,Next, we have,Lastly, we haveCombining (10),(11), and (12) into (9), we getwhere b i denotes the i-th column of B and we get the last inequality by choosing,Hence, using the Bernstein type inequality for weakly dependent and bounded random variables (Theorem 1 in[35]), together with (13)(14), and the observations we made right after(8), with probability at least 1 − δ, we haveUnion bounding over all i ∈ [L], and ℓ ∈ {0, 1, . . ., T − 1}, with probability at least 1 − δ, we havegiven(1 − ρ).This further implies that, with probability at least 1 − δ, the same upper bound also holds for ∥[ P1 − P(⊥) 1 ∥.Combining this with(7)and[21], with probability at least 1 − δ, we have• Estimation of P 2 : Here, we follow a similar line of reasoning as above.We begin with defining the three estimators of P 2 as follows,Similar to P 1 , we consider the entry-wise concentration of each ℓ-th subtrajectory as follows,Observing that E [ P(ℓ) 2 ] i,j − [ P(⊥) 2 ] i,j = 0, we have,In the following, we will upper bound each term in(20)separately.We begin with,where, given the i-th column b i , and the j-th column b j of B, we defineT −1 =j,oTLastly, we haveCombining (21),(23), and (24) into (20), we getwhere we get the last inequality by choosing,Hence, using similar line of reasoning as we did in the case of P 1 , with probability at least 1 − δ, we havegiven(1 − ρ).This further implies that, with probability at least 1 − δ, the same upper bound also holds for ∥[ P2 − P(⊥) 2 ∥.Combining this with the triangle inequality and[21], with probability at least 1 − δ, we have• Estimation of P 3 : Here, we follow a similar line of reasoning as above.We begin with defining the three estimators of P 3 as follows,T −1 =noFollowing the same line of reasoning as we did in the case of P 2 , with probability at least 1 − δ, we haveprovided that,where, given the i-th column b i , the j-th column b j and the n-th column b n of B, we define• Finalizing the proof: Theorem 1 follows by repeating the proof of Theorem 7 in[21], with the i.i.d.estimators replaced by the single trajectory estimators, and the values of ϵ 1 , ϵ 2,1 and ϵ 3,x,1 replaced by,where N = ⌊ N T ⌋ = O (N (1 − λ 2 (A))).The proof is completed by upper bounding the Hellingerdistance in terms of KL-distance.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>A method of moments for mixture models and hidden markov models. Animashree Anandkumar, Daniel Hsu, Sham M Kakade, JMLR Workshop and Conference Proceedings. 2012Conference on learning theory</p>
<p>Mice alternate between discrete strategies during perceptual decision-making. Zoe C Ashwood, Nicholas A Roy, Iris R Stone, Brain Laboratory, Anne E Urai, Anne K Churchland, Alexandre Pouget, Jonathan W Pillow, Nature Neuroscience. 2522022</p>
<p>Vectorbased navigation using grid-like representations in artificial agents. Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, Nature. 55777052018</p>
<p>A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The annals of mathematical statistics. Leonard E Baum, Ted Petrie, George Soules, Norman Weiss, 197041</p>
<p>Birth of a transformer: A memory viewpoint. Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, Leon Bottou, Advances in Neural Information Processing Systems. 202336</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Inference in Hidden Markov Models. Olivier Cappé, Eric Moulines, Tobias Rydén, 10.1007/0-387-28982-8Springer Series in Statistics. 2005Springer1st edition</p>
<p>Explaining the gibbs sampler. George Casella, Edward I George, 1992</p>
<p>Discovering symbolic cognitive models from human and animal behavior. Pablo Samuel Castro, Nenad Tomasev, Ankit Anand, Navodita Sharma, Rishika Mohanta, Aparna Dev, Kuba Perlin, Siddhant Jain, Kyle Levin, Noémi Éltető, Will Dabney, Alexander Novikov, Glenn C Turner, Maria K Eckstein, Nathaniel D Daw, Kevin J Miller, Kimberly L Stachenfeld, 10.1101/2025.02.05.636732bioRxiv. 2025</p>
<p>Data distributional properties drive emergent in-context learning in transformers. C Y Stephanie, Adam Chan, Andrew K Santoro, Jane X Lampinen, Aaditya Wang, Pierre H Singh, Jay Richemond, Felix Mcclelland, Hill, 2022</p>
<p>Elements of Information Theory. Thomas M Cover, Joy A Thomas, Wiley Series in Telecommunications and Signal Processing. 2006ISBN 0471241954</p>
<p>The evolution of statistical induction heads: In-context learning markov chains. L Benjamin, Ezra Edelman, Surbhi Edelman, Eran Goel, Nikolaos Malach, Tsilivis, 2024</p>
<p>Hidden markov processes. Y Ephraim, N Merhav, 10.1109/TIT.2002.1003838IEEE Transactions on Information Theory. 4862002</p>
<p>Discrete stochastic processes. G Robert, Gallager, Journal of the Operational Research Society. 4811997</p>
<p>Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Stuart Geman, Donald Geman, IEEE Transactions on pattern analysis and machine intelligence. 1984</p>
<p>Hidden markov models: Pitfalls and opportunities in ecology. Richard Glennie, Timo Adam, Vianey Leos-Barajas, Théo Michelot, Theoni Photopoulou, Brett T Mcclintock, Methods in Ecology and Evolution. 1412023</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Large language models are zero-shot time series forecasters. Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon, Wilson , 2024</p>
<p>Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, Eric Wang, Dan Klein, Trevor Darrell, David M Chan, arXiv:2503.04722Enough coin flips can make llms act bayesian. 2025arXiv preprint</p>
<p>A spectral algorithm for learning hidden markov models. Daniel Hsu, Tong Sham M Kakade, Zhang, Journal of Computer and System Sciences. 7852012</p>
<p>Do llms dream of elephants (when told not to)? latent concept association and associative memory in transformers. Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, 2024</p>
<p>Attractor dynamics and parallelism in a connectionist sequential machine. I Michael, Jordan, 1990IEEE Press</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, 2020</p>
<p>Standardized and reproducible measurement of decision-making in mice. The International, Brain Laboratory, Valeria Aguillon-Rodriguez, Dora Angelaki, Hannah Bayer, Niccolo Bonacchi, Matteo Carandini, Fanny Cazettes, Gaelle Chapuis, Anne K Churchland, Yang Dan, Eric Dewitt, Mayo Faulkner, Hamish Forrest, Laura Haetzel, Michael Häusser, Sonja B Hofer, Fei Hu, Anup Khanal, Christopher Krasniak, Ines Laranjeira, Zachary F Mainen, Guido Meijer, Nathaniel J Miska, Thomas D Mrsic-Flogel, Masayoshi Murakami, Jean-Paul Noel, Alejandro Pan-Vazquez, Cyrille Rossant, Joshua Sanders, Karolina Socha, Rebecca Terry, Anne E Urai, Hernando Vergara, Miles Wells, Christian J Wilson, Ilana B Witten, Lauren E Wool, Anthony M Zador, 10.7554/eLife.63711may 202110e63711</p>
<p>Markov chains and mixing times. A David, Yuval Levin, Peres, 2017American Mathematical Soc107</p>
<p>Transformers as algorithms: Generalization and stability in in-context learning. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, Samet Oymak, International conference on machine learning. PMLR2023</p>
<p>Observability and reconstructibility of hidden markov models: Implications for control and network congestion control. R Andrew, Robert R Liu, Bitmead, 49th IEEE Conference on Decision and Control (CDC). IEEE2010</p>
<p>Llms learn governing principles of dynamical systems, revealing an in-context neural scaling law. J B Toni, Nicolas Liu, Raphaël Boullé, Christopher J Sarfati, Earls, 2024</p>
<p>Density estimation with llms: a geometric investigation of in-context learning trajectories. J B Toni, Nicolas Liu, Raphaël Boullé, Christopher J Sarfati, Earls, 2025</p>
<p>Bridging the usability gap: Theoretical and methodological advances for spectral learning of hidden markov models. Xiaoyuan Ma, Jordan Rodu, arXiv:2302.074372023arXiv preprint</p>
<p>How hidden are hidden processes? a primer on crypticity and entropy convergence. Christopher J John R Mahoney, Ryan G Ellison, James P James, Crutchfield, Chaos: An Interdisciplinary Journal of Nonlinear Science. 2132011</p>
<p>Attention with markov: A framework for principled analysis of transformers via markov chains. Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, Michael Gastpar, 2024</p>
<p>Uncovering ecological state dynamics with hidden markov models. Roland Brett T Mcclintock, Olivier Langrock, Emmanuelle Gimenez, David L Cam, Richard Borchers, Toby A Glennie, Patterson, Ecology letters. 23122020</p>
<p>Bernstein inequality and moderate deviations under strong mixing conditions. Florence Merlevède, Magda Peligrad, Emmanuel Rio, High dimensional probability V: the Luminy volume. Institute of Mathematical Statistics20095</p>
<p>From predictive models to cognitive models: Separable behavioral processes underlying reward learning in the rat. Kevin J Miller, Matthew M Botvinick, Carlos D Brody, 10.1101/461129bioRxiv. 2021</p>
<p>Optimal regularization can mitigate double descent. Preetum Nakkiran, Prayaag Venkat, Sham Kakade, Tengyu Ma, arXiv:2003.018972020arXiv preprint</p>
<p>. Team Olmo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, arXiv:2501.006562024arXiv preprintet al. 2 olmo 2 furious</p>
<p>A tutorial on hidden markov models and selected applications in speech recognition. Lawrence R Rabiner, Proceedings of the IEEE. 7721989</p>
<p>Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, Ashok Vardhan, Makkuva , Transformers on markov data: Constant depth suffices. 2024</p>
<p>An analysis of tokenization: Transformers under markov data. Nived Rajaraman, Jiantao Jiao, Kannan Ramchandran, Advances in Neural Information Processing Systems. 202437</p>
<p>Spectral estimation of hidden Markov models. Jordan Rodu, 2014University of Pennsylvania</p>
<p>Mice in a labyrinth show rapid learning, sudden insight, and efficient exploration. eLife. Matthew Rosenberg, Tony Zhang, Pietro Perona, Markus Meister, 10.7554/eLife.66175jul 202110e66175</p>
<p>The mouse action recognition system (mars) software pipeline for automated analysis of social behaviors in mice. eLife. Cristina Segalin, Jalani Williams, Tomomi Karigo, May Hui, Moriel Zelikowsky, Jennifer J Sun, Pietro Perona, David J Anderson, Ann Kennedy, 10.7554/eLife.63720nov 202110e63720</p>
<p>Improper learning for non-stochastic control. Max Simchowitz, Karan Singh, Elad Hazan, Conference on Learning Theory. PMLR2020</p>
<p>Task programming: Learning data efficient behavior representations. Jennifer J Sun, Ann Kennedy, Eric Zhan, David J Anderson, Yisong Yue, Pietro Perona, 2021</p>
<p>Learning produces a hippocampal cognitive map in the form of an orthogonalized state machine. Weinan Sun, Johan Winnubst, Maanasa Natrajan, Chongxi Lai, Koichiro Kajikawa, Michalis Michaelos, Rachel Gattoni, James E Fitzgerald, Nelson Spruston, 10.1101/2023.08.03.551900bioRxiv. 2023</p>
<p>Facemap: a framework for modeling neural activity based on orofacial tracking. Atika Syeda, Lin Zhong, Renee Tung, Will Long, Marius Pachitariu, Carsen Stringer, Nature neuroscience. 2712024</p>
<p>Are language models actually useful for time series forecasting. Mingtian Tan, Mike Merrill, Vinayak Gupta, Tim Althoff, Tom Hartvigsen, Advances in Neural Information Processing Systems. 202437</p>
<p>Spontaneous cortical activity transiently organises into frequency specific phase-coupling networks. Diego Vidaurre, Laurence T Hunt, Andrew J Quinn, A E Benjamin, Matthew J Hunt, Anna C Brookes, Mark W Nobre, Woolrich, 10.1101/150607bioRxiv. 2017</p>
<p>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. A Viterbi, 10.1109/TIT.1967.1054010IEEE Transactions on Information Theory. 1321967</p>
<p>Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang, Wang , 2024</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma, 2023</p>
<p>Attractor dynamics in the hippocampal representation of the local environment. Thomas J Wills, Colin Lever, Francesca Cacucci, Neil Burgess, John O' Keefe, Science. 3082005</p>
<p>An explanation of incontext learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, 2022</p>
<p>Statistical and computational guarantees for the baum-welch algorithm. Fanny Yang, Sivaraman Balakrishnan, Martin J Wainwright, 2015</p>
<p>A hidden markov model for space-time precipitation. Walter Zucchini, Peter Guttorp, Water Resources Research. 27801991States: 4 2: 0.99 States: 4 2: 0.95 States: 4 2: 0.75 States: 8 2: 0.99 States: 8 2: 0.95 States: 8 2: 0.75 States: 16 2: 0.95 States: 16 2: 0.75 States: 16 2: 0.50 Observations: 4 B entropy: 0 Observations: 4 B entropy: 1 Observations: 8 B entropy</p>
<p>B entropy: 1 Observations: 16 B entropy: 0 Observations: 16 B entropy. 1</p>
<p>Context Length Llama-3.1-8B Llama-3.2-3B Llama-3.2-1B Qwen2. Qwen2.5-3B Qwen2.5-1.5B Qwen2.5-0.5B</p>            </div>
        </div>

    </div>
</body>
</html>
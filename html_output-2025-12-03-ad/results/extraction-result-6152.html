<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6152 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6152</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6152</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-60b05f32c32519a809f21642ef1eb3eaf3848008</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008" target="_blank">ROUGE: A Package for Automatic Evaluation of Summaries</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Four different RouGE measures are introduced: ROUGE-N, ROUge-L, R OUGE-W, and ROUAGE-S included in the Rouge summarization evaluation package and their evaluations.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6152.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6152.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recall-Oriented Understudy for Gisting Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation package for summarization that compares a candidate summary against one or more human reference summaries using multiple surface-level overlap measures (n-gram, LCS, weighted LCS, skip-bigram, and unigram-augmented skip-bigram). Included measures are ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic surface-overlap comparison of candidate textual output to human reference texts using multiple statistics (n-gram recall ROUGE-N; longest common subsequence ROUGE-L; weighted LCS ROUGE-W; skip-bigram ROUGE-S; extended skip-bigram plus unigram ROUGE-SU). Multiple references and jackknifing are supported; tools compute recall/precision/F-measure variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Recall-oriented metrics (ROUGE-N is recall of reference n-grams), precision and F-measure variants (unigram/skip/LCS/WLCS), correlation with human content-coverage judgments (Pearson, Spearman, Kendall), and confidence intervals via bootstrap resampling.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across DUC 2001–2003 summarization tasks ROUGE variants (notably ROUGE-2, ROUGE-L, ROUGE-W, ROUGE-S and ROUGE-SU for some tasks) produced strong positive correlations with human content-coverage scores; performance depends on task (single vs multi-document), summary length, and preprocessing (stopword removal, stemming).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>DUC 2001, DUC 2002, DUC 2003 summarization datasets (multiple summary lengths and single- and multi-document tasks); human reference summaries and human-assigned content-coverage scores via SEE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ROUGE scores were correlated with human content-coverage judgments using Pearson/Spearman/Kendall correlations; bootstrap resampling used to estimate 95% CIs; jackknifing across multiple references used to estimate average human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designed for summary evaluation (surface-overlap) not semantic or scientific-theory evaluation; sensitive to reference set composition and preprocessing (stopwords, stemming), limited correlation in small-sample multi-document tasks, and known weaknesses of surface overlaps for semantic/paraphrase matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6152.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-N (n-gram co-occurrence statistics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram recall measure between a candidate summary and a set of reference summaries: counts overlapping n-grams and computes recall (and related precision/F-measure variants); numerator sums matches over references favoring consensus n-grams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute n-gram overlap counts between candidate and reference summaries; ROUGE-N reported primarily as recall (denominator = total n-grams in references); multiple references handled via pairwise scoring and jackknife averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>n-gram recall (primary), optionally precision and F-measure; correlation of ROUGE-N system scores with human content-coverage judgments (Pearson/Spearman/Kendall).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ROUGE-2 (bigram) performed well in single-document tasks; ROUGE-1 (unigram) and low-order n-grams performed better on very short summaries; higher-order N (>2) tended to perform worse in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on DUC 2001/2002/2003 datasets; experiments included CASE (original), STEM (Porter stemming), and STOP (stopword removal) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>System-average ROUGE-N scores were correlated with human content-coverage scores; jackknifing was used when multiple references available to estimate variability and approximate human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Surface n-gram overlap misses in-sequence but non-consecutive matches and paraphrases; sensitivity to stopwords and stemming; recall-based nature can be biased by reference pool size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6152.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (Longest Common Subsequence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation measure based on the length of the longest common subsequence (LCS) between candidate and reference summaries; uses LCS-based recall/precision and F-measure to capture in-sequence matching without requiring consecutiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute sentence-level and summary-level LCS matches (union LCS across candidate sentences for each reference sentence), form LCS-based recall/precision and F-measure (with beta often set large to emphasize recall in DUC experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>LCS-based recall (primary in DUC setup), precision, and F-measure; correlation of ROUGE-L scores with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ROUGE-L correlated well with human judgments in single-document and very-short summary tasks; outperformed some high-order n-grams by capturing sentence-level ordering. However, performance varied by task and summary length.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>DUC 2001/2002/2003 datasets (same experimental splits and preprocessing variants as ROUGE-N).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ROUGE-L system scores showed strong correlations with human content-coverage scores (reported alongside Pearson correlations and confidence intervals).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LCS counts only the longest in-sequence matches and may ignore multiple shorter matches; does not weight consecutive matches differently (addressed by ROUGE-W).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6152.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-W</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-W (Weighted Longest Common Subsequence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LCS variant that weights consecutive in-sequence matches more heavily than non-consecutive matches by using a weighting function f(k) on consecutive-match length and computing an F-measure after normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Dynamic programming computes WLCS score using a weighting function f on run lengths of consecutive matches (e.g., f(k)=k^alpha); normalized via inverse f to derive recall/precision and F-measure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Weighted-LCS-based recall/precision/F-measure emphasizing long consecutive matching runs; correlations with human judgments used for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ROUGE-W (with polynomial weighting, e.g., k^2 and alpha=1.2 used) produced improved discrimination favoring consecutive matches and correlated well with human judgments in several tasks (notably single-document summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>DUC 2001/2002/2003 datasets; experiments ran with WLCS weighting factor alpha (e.g., 1.2) and reported correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ROUGE-W scores were correlated to human coverage judgments; in examples WLCS distinguished sequences with consecutive matches better than plain LCS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of weighting function f and normalization affects scores; more parameters to tune than plain LCS; still a surface overlap method and may miss semantic similarity beyond ordering/consecutiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6152.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-S / ROUGE-SU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-S (skip-bigram) and ROUGE-SU (skip-bigram + unigram)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ROUGE-S counts skip-bigram (in-order word-pair) overlaps between candidate and reference allowing gaps up to a configurable skip distance; ROUGE-SU augments skip-bigram with unigram matches to avoid zero scores for reversed or highly reordered sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Count ordered word-pair matches (skip-bigrams) within a maximum skip distance d_skip; compute recall/precision/F-measure; ROUGE-SU adds unigram matches or start-marker to ensure some credit for single-word overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Skip-bigram recall/precision/F-measure (d_skip parameter), sensitivity to word order without requiring adjacency; correlations with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ROUGE-S (and ROUGE-SU) with d_skip values (1,4,9 tested) correlated well with human judgments in many tasks; ROUGE-SU especially helpful to differentiate outputs that have no skip-bigram matches but do share unigrams. ROUGE-S performed better than some n-gram/LCS metrics on certain examples.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>DUC 2001/2002/2003 datasets; experiments ran with d_skip ∈ {1,4,9} and compared CASE/STEM/STOP preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ROUGE-S and ROUGE-SU system scores correlated with human content-coverage judgments; specific correlations depended on preprocessing and task type.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Unconstrained skip-bigrams can count spurious matches (e.g., frequent function-word pairs); requires setting d_skip to limit noise; does not capture longer-distance semantic paraphrase beyond ordered word pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6152.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jackknifing (multi-reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jackknifing procedure for multi-reference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure to compute ROUGE scores across multiple reference summaries by computing scores on M sets of M-1 references and averaging, used to approximate average human performance and to allow fair system-vs-human comparisons when references are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Given M reference summaries, compute ROUGE on each leave-one-out set of M-1 references and average the M resulting scores to produce final multi-reference ROUGE scores and an estimate of human-level variability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Provides a multi-reference aggregation to reduce bias from any single reference and to estimate human baseline performance via averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied in ROUGE package experiments to produce multi-reference scores and to estimate average human performance; multiple references improved correlations in many tasks though effect size depended on sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>DUC reference pools (3–4 references in various DUC years) used with jackknifing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Jackknife averaging enabled estimating human vs system ROUGE performance (average human ROUGE estimated by leave-one-out human-vs-rest computations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness depends on number and quality of available references; leave-one-out averaging does not substitute for larger, diverse human reference pools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6152.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrap resampling & correlation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap resampling for confidence intervals and correlation testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of bootstrap resampling to estimate 95% confidence intervals for correlation coefficients (Pearson, Spearman, Kendall) when comparing automatic metric scores to human judgments; critical values and degrees-of-freedom considerations are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Apply bootstrap resampling (Davison & Hinkley) to compute confidence intervals for correlation coefficients between system-average ROUGE scores and human-assigned coverage scores; report Pearson/Spearman/Kendall correlations and critical values for significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlation coefficients (Pearson, Spearman, Kendall) with 95% confidence intervals used to assess how well automatic metrics track human judgments; significance thresholds computed given sample sizes/degrees of freedom.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Correlations were reported with bootstrap-estimated 95% CIs; critical Pearson values noted (e.g., 0.632 at 95% with 8 df) and used to judge significance. ROUGE correlations varied by task; some exceeded 0.70, others did not.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Correlation analyses computed over system-average scores on DUC 2001–2003 tasks (different numbers of systems and samples per task).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct numeric correlation between automatic metric and human judgment enables quantitative comparison; bootstrap CIs used to assess statistical stability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small sample sizes (notably in many multi-document tasks) reduced statistical stability of correlations; correlation does not prove semantic adequacy beyond content coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6152.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6152.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DUC datasets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Document Understanding Conference (DUC) 2001, 2002, 2003 datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark summarization evaluation datasets including single- and multi-document summarization tasks across various target summary lengths, each accompanied by multiple human reference summaries and human content-coverage judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Serve as ground-truth benchmark: human reference summaries and human-assigned content-coverage scores (via SEE) are used to validate automatic metrics by correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human content-coverage scoring (unit-level coverage aggregated to system averages) used as ground truth; automatic metrics compared via correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>natural language summarization / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ROUGE variants were evaluated on DUC 2001–2003; correlations with human judgments varied by task, sample size, preprocessing, and use of multiple references. Single-document tasks generally yielded higher correlations than many multi-document tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>DUC 2001, DUC 2002, DUC 2003; SEE (Summary Evaluation Environment) used for human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human-assigned unit-level coverage scores provided the ground truth against which ROUGE measures were correlated; human scoring typically used a single manual summary per evaluation instance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some DUC subsets had small sample counts (e.g., ~30 samples in multi-document tasks) limiting statistical stability; human judgment collection is costly so reference pool sizes are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROUGE: A Package for Automatic Evaluation of Summaries', 'publication_date_yy_mm': '2004-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bleu: A method for automatic evaluation of machine translation <em>(Rating: 2)</em></li>
                <li>Automatic evaluation of summaries using n-gram co-occurrence statistics <em>(Rating: 2)</em></li>
                <li>Evaluation of Text Summarization in a Cross-Lingual Information Retrieval Framework <em>(Rating: 1)</em></li>
                <li>Meta-evaluation of summaries in a crosslingual environment using content-based metrics <em>(Rating: 1)</em></li>
                <li>Looking for a few good metrics: Rouge and its evaluation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6152",
    "paper_id": "paper-60b05f32c32519a809f21642ef1eb3eaf3848008",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "ROUGE",
            "name_full": "Recall-Oriented Understudy for Gisting Evaluation",
            "brief_description": "An automatic evaluation package for summarization that compares a candidate summary against one or more human reference summaries using multiple surface-level overlap measures (n-gram, LCS, weighted LCS, skip-bigram, and unigram-augmented skip-bigram). Included measures are ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Automatic surface-overlap comparison of candidate textual output to human reference texts using multiple statistics (n-gram recall ROUGE-N; longest common subsequence ROUGE-L; weighted LCS ROUGE-W; skip-bigram ROUGE-S; extended skip-bigram plus unigram ROUGE-SU). Multiple references and jackknifing are supported; tools compute recall/precision/F-measure variants.",
            "evaluation_criteria": "Recall-oriented metrics (ROUGE-N is recall of reference n-grams), precision and F-measure variants (unigram/skip/LCS/WLCS), correlation with human content-coverage judgments (Pearson, Spearman, Kendall), and confidence intervals via bootstrap resampling.",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "Across DUC 2001–2003 summarization tasks ROUGE variants (notably ROUGE-2, ROUGE-L, ROUGE-W, ROUGE-S and ROUGE-SU for some tasks) produced strong positive correlations with human content-coverage scores; performance depends on task (single vs multi-document), summary length, and preprocessing (stopword removal, stemming).",
            "benchmarks_or_datasets": "DUC 2001, DUC 2002, DUC 2003 summarization datasets (multiple summary lengths and single- and multi-document tasks); human reference summaries and human-assigned content-coverage scores via SEE.",
            "comparison_to_human": "ROUGE scores were correlated with human content-coverage judgments using Pearson/Spearman/Kendall correlations; bootstrap resampling used to estimate 95% CIs; jackknifing across multiple references used to estimate average human performance.",
            "limitations_or_challenges": "Designed for summary evaluation (surface-overlap) not semantic or scientific-theory evaluation; sensitive to reference set composition and preprocessing (stopwords, stemming), limited correlation in small-sample multi-document tasks, and known weaknesses of surface overlaps for semantic/paraphrase matches.",
            "uuid": "e6152.0",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "ROUGE-N",
            "name_full": "ROUGE-N (n-gram co-occurrence statistics)",
            "brief_description": "An n-gram recall measure between a candidate summary and a set of reference summaries: counts overlapping n-grams and computes recall (and related precision/F-measure variants); numerator sums matches over references favoring consensus n-grams.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute n-gram overlap counts between candidate and reference summaries; ROUGE-N reported primarily as recall (denominator = total n-grams in references); multiple references handled via pairwise scoring and jackknife averaging.",
            "evaluation_criteria": "n-gram recall (primary), optionally precision and F-measure; correlation of ROUGE-N system scores with human content-coverage judgments (Pearson/Spearman/Kendall).",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "ROUGE-2 (bigram) performed well in single-document tasks; ROUGE-1 (unigram) and low-order n-grams performed better on very short summaries; higher-order N (&gt;2) tended to perform worse in some tasks.",
            "benchmarks_or_datasets": "Evaluated on DUC 2001/2002/2003 datasets; experiments included CASE (original), STEM (Porter stemming), and STOP (stopword removal) variants.",
            "comparison_to_human": "System-average ROUGE-N scores were correlated with human content-coverage scores; jackknifing was used when multiple references available to estimate variability and approximate human performance.",
            "limitations_or_challenges": "Surface n-gram overlap misses in-sequence but non-consecutive matches and paraphrases; sensitivity to stopwords and stemming; recall-based nature can be biased by reference pool size.",
            "uuid": "e6152.1",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "ROUGE-L",
            "name_full": "ROUGE-L (Longest Common Subsequence)",
            "brief_description": "An evaluation measure based on the length of the longest common subsequence (LCS) between candidate and reference summaries; uses LCS-based recall/precision and F-measure to capture in-sequence matching without requiring consecutiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute sentence-level and summary-level LCS matches (union LCS across candidate sentences for each reference sentence), form LCS-based recall/precision and F-measure (with beta often set large to emphasize recall in DUC experiments).",
            "evaluation_criteria": "LCS-based recall (primary in DUC setup), precision, and F-measure; correlation of ROUGE-L scores with human judgments.",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "ROUGE-L correlated well with human judgments in single-document and very-short summary tasks; outperformed some high-order n-grams by capturing sentence-level ordering. However, performance varied by task and summary length.",
            "benchmarks_or_datasets": "DUC 2001/2002/2003 datasets (same experimental splits and preprocessing variants as ROUGE-N).",
            "comparison_to_human": "ROUGE-L system scores showed strong correlations with human content-coverage scores (reported alongside Pearson correlations and confidence intervals).",
            "limitations_or_challenges": "LCS counts only the longest in-sequence matches and may ignore multiple shorter matches; does not weight consecutive matches differently (addressed by ROUGE-W).",
            "uuid": "e6152.2",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "ROUGE-W",
            "name_full": "ROUGE-W (Weighted Longest Common Subsequence)",
            "brief_description": "An LCS variant that weights consecutive in-sequence matches more heavily than non-consecutive matches by using a weighting function f(k) on consecutive-match length and computing an F-measure after normalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Dynamic programming computes WLCS score using a weighting function f on run lengths of consecutive matches (e.g., f(k)=k^alpha); normalized via inverse f to derive recall/precision and F-measure.",
            "evaluation_criteria": "Weighted-LCS-based recall/precision/F-measure emphasizing long consecutive matching runs; correlations with human judgments used for validation.",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "ROUGE-W (with polynomial weighting, e.g., k^2 and alpha=1.2 used) produced improved discrimination favoring consecutive matches and correlated well with human judgments in several tasks (notably single-document summaries).",
            "benchmarks_or_datasets": "DUC 2001/2002/2003 datasets; experiments ran with WLCS weighting factor alpha (e.g., 1.2) and reported correlation analyses.",
            "comparison_to_human": "ROUGE-W scores were correlated to human coverage judgments; in examples WLCS distinguished sequences with consecutive matches better than plain LCS.",
            "limitations_or_challenges": "Choice of weighting function f and normalization affects scores; more parameters to tune than plain LCS; still a surface overlap method and may miss semantic similarity beyond ordering/consecutiveness.",
            "uuid": "e6152.3",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "ROUGE-S / ROUGE-SU",
            "name_full": "ROUGE-S (skip-bigram) and ROUGE-SU (skip-bigram + unigram)",
            "brief_description": "ROUGE-S counts skip-bigram (in-order word-pair) overlaps between candidate and reference allowing gaps up to a configurable skip distance; ROUGE-SU augments skip-bigram with unigram matches to avoid zero scores for reversed or highly reordered sentences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Count ordered word-pair matches (skip-bigrams) within a maximum skip distance d_skip; compute recall/precision/F-measure; ROUGE-SU adds unigram matches or start-marker to ensure some credit for single-word overlap.",
            "evaluation_criteria": "Skip-bigram recall/precision/F-measure (d_skip parameter), sensitivity to word order without requiring adjacency; correlations with human judgments.",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "ROUGE-S (and ROUGE-SU) with d_skip values (1,4,9 tested) correlated well with human judgments in many tasks; ROUGE-SU especially helpful to differentiate outputs that have no skip-bigram matches but do share unigrams. ROUGE-S performed better than some n-gram/LCS metrics on certain examples.",
            "benchmarks_or_datasets": "DUC 2001/2002/2003 datasets; experiments ran with d_skip ∈ {1,4,9} and compared CASE/STEM/STOP preprocessing.",
            "comparison_to_human": "ROUGE-S and ROUGE-SU system scores correlated with human content-coverage judgments; specific correlations depended on preprocessing and task type.",
            "limitations_or_challenges": "Unconstrained skip-bigrams can count spurious matches (e.g., frequent function-word pairs); requires setting d_skip to limit noise; does not capture longer-distance semantic paraphrase beyond ordered word pairs.",
            "uuid": "e6152.4",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "Jackknifing (multi-reference)",
            "name_full": "Jackknifing procedure for multi-reference evaluation",
            "brief_description": "A procedure to compute ROUGE scores across multiple reference summaries by computing scores on M sets of M-1 references and averaging, used to approximate average human performance and to allow fair system-vs-human comparisons when references are limited.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Given M reference summaries, compute ROUGE on each leave-one-out set of M-1 references and average the M resulting scores to produce final multi-reference ROUGE scores and an estimate of human-level variability.",
            "evaluation_criteria": "Provides a multi-reference aggregation to reduce bias from any single reference and to estimate human baseline performance via averaging.",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "Applied in ROUGE package experiments to produce multi-reference scores and to estimate average human performance; multiple references improved correlations in many tasks though effect size depended on sample counts.",
            "benchmarks_or_datasets": "DUC reference pools (3–4 references in various DUC years) used with jackknifing.",
            "comparison_to_human": "Jackknife averaging enabled estimating human vs system ROUGE performance (average human ROUGE estimated by leave-one-out human-vs-rest computations).",
            "limitations_or_challenges": "Effectiveness depends on number and quality of available references; leave-one-out averaging does not substitute for larger, diverse human reference pools.",
            "uuid": "e6152.5",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "Bootstrap resampling & correlation analysis",
            "name_full": "Bootstrap resampling for confidence intervals and correlation testing",
            "brief_description": "Use of bootstrap resampling to estimate 95% confidence intervals for correlation coefficients (Pearson, Spearman, Kendall) when comparing automatic metric scores to human judgments; critical values and degrees-of-freedom considerations are reported.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Apply bootstrap resampling (Davison & Hinkley) to compute confidence intervals for correlation coefficients between system-average ROUGE scores and human-assigned coverage scores; report Pearson/Spearman/Kendall correlations and critical values for significance.",
            "evaluation_criteria": "Correlation coefficients (Pearson, Spearman, Kendall) with 95% confidence intervals used to assess how well automatic metrics track human judgments; significance thresholds computed given sample sizes/degrees of freedom.",
            "llm_model_name": null,
            "theory_domain": null,
            "theory_description": null,
            "evaluation_results": "Correlations were reported with bootstrap-estimated 95% CIs; critical Pearson values noted (e.g., 0.632 at 95% with 8 df) and used to judge significance. ROUGE correlations varied by task; some exceeded 0.70, others did not.",
            "benchmarks_or_datasets": "Correlation analyses computed over system-average scores on DUC 2001–2003 tasks (different numbers of systems and samples per task).",
            "comparison_to_human": "Direct numeric correlation between automatic metric and human judgment enables quantitative comparison; bootstrap CIs used to assess statistical stability.",
            "limitations_or_challenges": "Small sample sizes (notably in many multi-document tasks) reduced statistical stability of correlations; correlation does not prove semantic adequacy beyond content coverage.",
            "uuid": "e6152.6",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        },
        {
            "name_short": "DUC datasets",
            "name_full": "Document Understanding Conference (DUC) 2001, 2002, 2003 datasets",
            "brief_description": "Benchmark summarization evaluation datasets including single- and multi-document summarization tasks across various target summary lengths, each accompanied by multiple human reference summaries and human content-coverage judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Serve as ground-truth benchmark: human reference summaries and human-assigned content-coverage scores (via SEE) are used to validate automatic metrics by correlation analyses.",
            "evaluation_criteria": "Human content-coverage scoring (unit-level coverage aggregated to system averages) used as ground truth; automatic metrics compared via correlation.",
            "llm_model_name": null,
            "theory_domain": "natural language summarization / NLP",
            "theory_description": null,
            "evaluation_results": "ROUGE variants were evaluated on DUC 2001–2003; correlations with human judgments varied by task, sample size, preprocessing, and use of multiple references. Single-document tasks generally yielded higher correlations than many multi-document tasks.",
            "benchmarks_or_datasets": "DUC 2001, DUC 2002, DUC 2003; SEE (Summary Evaluation Environment) used for human scoring.",
            "comparison_to_human": "Human-assigned unit-level coverage scores provided the ground truth against which ROUGE measures were correlated; human scoring typically used a single manual summary per evaluation instance.",
            "limitations_or_challenges": "Some DUC subsets had small sample counts (e.g., ~30 samples in multi-document tasks) limiting statistical stability; human judgment collection is costly so reference pool sizes are limited.",
            "uuid": "e6152.7",
            "source_info": {
                "paper_title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "publication_date_yy_mm": "2004-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bleu: A method for automatic evaluation of machine translation",
            "rating": 2
        },
        {
            "paper_title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
            "rating": 2
        },
        {
            "paper_title": "Evaluation of Text Summarization in a Cross-Lingual Information Retrieval Framework",
            "rating": 1
        },
        {
            "paper_title": "Meta-evaluation of summaries in a crosslingual environment using content-based metrics",
            "rating": 1
        },
        {
            "paper_title": "Looking for a few good metrics: Rouge and its evaluation",
            "rating": 2
        }
    ],
    "cost": 0.01220075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rouge: A Package for Automatic Evaluation of Summaries</h1>
<p>Chin-Yew Lin<br>Information Sciences Institute<br>University of Southern California<br>4676 Admiralty Way<br>Marina del Rey, CA 90292<br>cyl@isi.edu</p>
<h4>Abstract</h4>
<p>ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.</p>
<h2>1 Introduction</h2>
<p>Traditionally evaluation of summarization involves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001). However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts. This is very expensive and difficult to conduct in a frequent basis. Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years. For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries. These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence. However, they did not show how the results of these automatic evaluation methods correlate to human judgments. Following the successful application of automatic evaluation methods, such as Bleu (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to Bleu,
i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries. In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations. ROUGE stands for Re-call-Oriented Understudy for Gisting Evaluation. It includes several automatic evaluation methods that measure the similarity between summaries. We describe Rouge-N in Section 2, Rouge-L in Section 3, Rouge-W in Section 4, and Rouge-S in Section 5. Section 6 shows how these measures correlate with human judgments using DUC 2001, 2002, and 2003 data. Section 7 concludes this paper and discusses future directions.</p>
<h2>2 Rouge-N: N-gram Co-Occurrence Statistics</h2>
<p>Formally, ROUGE-N is an n-gram recall between a candidate summary and a set of reference summaries. ROUGE-N is computed as follows:</p>
<p>$$
\begin{aligned}
&amp; \text { ROUGE-N } \
&amp; =\frac{\sum_{S \in\left{\text { ReferenceSummaries }\right}} \sum_{\text {gram }<em _match="{match" _text="\text">{h} \in S} \text { Count }</em>}}\left(\text { gram <em ReferenceSummaries="ReferenceSummaries" S="S" _="{" _in_123_text="\in{\text">{c}\right)}{\sum</em>}}} \sum_{\text {gram <em c="c">{c} \in S} \text { Count }\left(\text { gram }</em>
\end{aligned}
$$}\right)</p>
<p>Where $n$ stands for the length of the n-gram, gram $<em _match="{match" _text="\text">{h}$, and Count $</em>\right)$ is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries.
It is clear that ROUGE-N is a recall-related measure because the denominator of the equation is the total sum of the number of n-grams occurring at the reference summary side. A closely related measure, Bleu, used in automatic evaluation of machine translation, is a precision-based measure. Bleu measures how well a candidate translation matches a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping with the references. Please see Papineni et al. (2001) for details about Bleu.
Note that the number of n-grams in the denominator of the Rouge-N formula increases as we add more references. This is intuitive and reasonable because there might exist multiple good summaries.}}\left(\text { gram }_{h</p>
<p>Every time we add a reference into the pool, we expand the space of alternative summaries. By controlling what types of references we add to the reference pool, we can design evaluations that focus on different aspects of summarization. Also note that the numerator sums over all reference summaries. This effectively gives more weight to matching n-grams occurring in multiple references. Therefore a candidate summary that contains words shared by more references is favored by the ROUGE-N measure. This is again very intuitive and reasonable because we normally prefer a candidate summary that is more similar to consensus among reference summaries.</p>
<h3>2.1 Multiple References</h3>
<p>So far, we only demonstrated how to compute ROUGE-N using a single reference. When multiple references are used, we compute pairwise summarylevel ROUGE-N between a candidate summary $s$ and every reference, $r_{i}$, in the reference set. We then take the maximum of pairwise summary-level ROUGE-N scores as the final multiple reference ROUGE-N score. This can be written as follows:</p>
<p>$$
\text { ROUGE- } N_{\text {multi }}=\operatorname{argmax}<em i="i">{i} \text { ROUGE- } N\left(r</em>, s\right)
$$</p>
<p>This procedure is also applied to computation of ROUGE-L (Section 3), ROUGE-W (Section 4), and ROUGE-S (Section 5). In the implementation, we use a Jackknifing procedure. Given M references, we compute the best score over M sets of M-1 references. The final ROUGE-N score is the average of the M ROUGE-N scores using different M-1 references. The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference summaries are usually the only human summaries available. Using this procedure, we are able to estimate average human performance by averaging M ROUGE-N scores of one reference vs. the rest M-1 references. Although the Jackknifing procedure is not necessary when we just want to compute ROUGE scores using multiple references, it is applied in all ROUGE score computations in the ROUGE evaluation package.</p>
<p>In the next section, we describe a ROUGE measure based on longest common subsequences between two summaries.</p>
<h2>3 ROUGE-L: Longest Common Subsequence</h2>
<p>A sequence $Z=\left{z_{1}, z_{2}, \ldots, z_{n}\right}$ is a subsequence of another sequence $X=\left{x_{1}, x_{2}, \ldots, x_{m}\right}$, if there exists a strict increasing sequence $\left{i_{1}, i_{2}, \ldots, i_{k}\right}$ of indices of $X$ such that for all $j=1,2, \ldots, k$, we have $x_{i j}=z_{j}$ (Cormen et al., 1989). Given two sequences $X$ and $Y$, the longest common subsequence (LCS) of $X$ and
$Y$ is a common subsequence with maximum length. LCS has been used in identifying cognate candidates during construction of N-best translation lexicon from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used LCS as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS to compare similarity between two texts in automatic summarization evaluation.</p>
<h3>3.1 Sentence-Level LCS</h3>
<p>To apply LCS in summarization evaluation, we view a summary sentence as a sequence of words. The intuition is that the longer the LCS of two summary sentences is, the more similar the two summaries are. We propose using LCS-based Fmeasure to estimate the similarity between two summaries $X$ of length $m$ and $Y$ of length $n$, assuming $X$ is a reference summary sentence and $Y$ is a candidate summary sentence, as follows:</p>
<p>$$
\begin{aligned}
&amp; R_{l c s}=\frac{L C S(X, Y)}{m} \
&amp; P_{l c s}=\frac{L C S(X, Y)}{n} \
&amp; F_{l c s}=\frac{\left(1+\beta^{2}\right) R_{l c s} P_{l c s}}{R_{l c s}+\beta^{2} P_{l c s}}
\end{aligned}
$$</p>
<p>Where $L C S(X, Y)$ is the length of a longest common subsequence of $X$ and $Y$, and $\beta=P_{l c s} / R_{l c s}$ when $? F_{l c s} / ? R_{l c s}=? F_{l c s} / ? P_{l c s}$. In DUC, $\beta$ is set to a very big number (? 8 ). Therefore, only $R_{l c s}$ is considered. We call the LCS-based F-measure, i.e. Equation 4, ROUGE-L. Notice that ROUGE-L is 1 when $X$ $=Y$; while ROUGE-L is zero when $L C S(X, Y)=0$, i.e. there is nothing in common between $X$ and $Y$. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen, 1979). The composite factors are LCS-based recall and precision in this case. Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BleU.</p>
<p>One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as ngrams. The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary.</p>
<p>ROUGE-L as defined in Equation 4 has the property that its value is less than or equal to the minimum of unigram F-measure of $X$ and $Y$. Unigram</p>
<p>recall reflects the proportion of words in $X$ (reference summary sentence) that are also present in $Y$ (candidate summary sentence); while unigram precision is the proportion of words in $Y$ that are also in $X$. Unigram recall and precision count all cooccurring words regardless their orders; while ROUGE-L counts only in-sequence co-occurrences.</p>
<p>By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way. Consider the following example:</p>
<h2>S1. police killed the gunman</h2>
<p>S2. police kill the gunman
S3. the gunman kill police
We only consider Rouge-2, i.e. $N=2$, for the purpose of explanation. Using S1 as the reference and S2 and S3 as the candidate summary sentences, S2 and S3 would have the same Rouge-2 score, since they both have one bigram, i.e. "the gunman". However, S2 and S3 have very different meanings. In the case of Rouge-L, S2 has a score of $3 / 4=0.75$ and S3 has a score of $2 / 4=0.5$, with $\beta=1$. Therefore S2 is better than S3 according to ROUGE-L. This example also illustrated that ROUGE-L can work reliably at sentence level.</p>
<p>However, LCS suffers one disadvantage that it only counts the main in-sequence words; therefore, other alternative LCSes and shorter sequences are not reflected in the final score. For example, given the following candidate sentence:</p>
<p>S4. the gunman police killed
Using S1 as its reference, LCS counts either "the gunman" or "police killed", but not both; therefore, S4 has the same Rouge-L score as S3. Rouge-2 would prefer S4 than S3.</p>
<h3>3.2 Summary-Level LCS</h3>
<p>Previous section described how to compute sen-tence-level LCS-based F-measure score. When applying to summary-level, we take the union LCS matches between a reference summary sentence, $r_{i}$, and every candidate summary sentence, $c_{i}$. Given a reference summary of $u$ sentences containing a total of $m$ words and a candidate summary of $v$ sentences containing a total of $n$ words, the summary-level LCS-based F-measure can be computed as follows:</p>
<p>$$
\begin{aligned}
&amp; R_{l c s}=\frac{\sum_{i=1}^{n} L C S_{v, i}\left(r_{i}, C\right)}{m} \
&amp; P_{l c s}=\frac{\sum_{i=1}^{n} L C S_{v, i}\left(r_{i}, C\right)}{n}
\end{aligned}
$$</p>
<p>$$
F_{l c s}=\frac{\left(1+\beta^{2}\right) R_{l c s} P_{l c s}}{R_{l c s}+\beta^{2} P_{l c s}}
$$</p>
<p>Again $\beta$ is set to a very big number ( $\left.? 8\right)$ in DUC, i.e. only $R_{l c s}$ is considered. $L C S_{v, i}\left(r_{i}, C\right)$ is the LCS score of the union longest common subsequence between reference sentence $r_{i}$ and candidate summary $C$. For example, if $r_{i}=w_{1} w_{2} w_{3} w_{4} w_{5}$, and $C$ contains two sentences: $c_{1}=w_{1} w_{2} w_{6} w_{7} w_{8}$ and $c_{2}$ $=w_{1} w_{3} w_{8} w_{9} w_{5}$, then the longest common subsequence of $r_{i}$ and $c_{1}$ is " $w_{1} w_{2}$ " and the longest common subsequence of $r_{i}$ and $c_{2}$ is " $w_{1} w_{3} w_{5}$ ". The union longest common subsequence of $r_{i}, c_{1}$, and $c_{2}$ is " $w_{1} w_{2} w_{3} w_{5}$ " and $L C S_{v, i}\left(r_{i}, C\right)=4 / 5$.</p>
<h3>3.3 Rouge-L vs. Normalized Pairwise LCS</h3>
<p>The normalized pairwise LCS proposed by Radev et al. (page 51, 2002) between two summaries S1 and S2, $L C S\left(S_{1}, S_{2}\right)_{M E A D}$, is written as follows:</p>
<p>$$
\frac{\sum_{s_{i} \in S_{1}} \max <em i="i">{s</em> \max } \in S_{2}} L C S\left(s_{i}, s_{j}\right)+\sum_{s_{i} \in S_{2}<em i="i">{s</em>
$$} \in S_{1}} L C S\left(s_{i}, s_{j}\right)}{\sum_{s_{i} \in S_{1}} \text { length }\left(s_{i}\right)+\sum_{s_{j} \in S_{2}} \text { length }\left(s_{j}\right)</p>
<p>Assuming S1 has $m$ words and S2 has $n$ words, Equation 8 can be rewritten as Equation 9 due to symmetry:</p>
<p>$$
\frac{2 * \sum_{s_{i} \in S_{1}} \max <em j="j">{s</em>
$$} \in S_{2}} L C S\left(s_{i}, s_{j}\right)}{m+n</p>
<p>We then define MEAD LCS recall ( $R_{l c s-M E A D}$ ) and MEAD LCS precision ( $P_{l c s-M E A D}$ ) as follows:</p>
<p>$$
\begin{aligned}
&amp; R_{l c s-M E A D}=\frac{\sum_{s_{i} \in S_{1}} \max <em j="j">{s</em> \
&amp; P_{l c s-M E A D}=\frac{\sum_{s_{i} \in S_{1}} \max } \in S_{2}} L C S\left(s_{i}, s_{j}\right)}{m<em j="j">{s</em>
\end{aligned}
$$} \in S_{2}} L C S\left(s_{i}, s_{j}\right)}{n</p>
<p>We can rewrite Equation (9) in terms of $R_{l c s-M E A D}$ and $P_{l c s-M E A D}$ with a constant parameter $\beta=1$ as follows:
$L C S\left(S_{1}, S_{2}\right)<em A="A" D="D" E="E" c="c" l="l" s-M="s-M">{M E A D}=\frac{\left(1+\beta^{2}\right) R</em>$
Equation 12 shows that normalized pairwise LCS as defined in Radev et al. (2002) and implemented in MEAD is also a F-measure with $\beta=1$. Sentencelevel normalized pairwise LCS is the same as ROUGE-L with $\beta=1$. Besides setting $\beta=1$, sum-mary-level normalized pairwise LCS is different from ROUGE-L in how a sentence gets its LCS score from its references. Normalized pairwise LCS takes} P_{l c s-M E A D}}{R_{l c s-M E A D}+\beta^{2} P_{l c s-M E A D}</p>
<p>the best LCS score while ROUGE-L takes the union LCS score.</p>
<h2>4 Rouge-W: Weighted Longest Common Subsequence</h2>
<p>LCS has many nice properties as we have described in the previous sections. Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences. For example, given a reference sequence $X$ and two candidate sequences $Y_{1}$ and $Y_{2}$ as follows:</p>
<p>$$
\begin{array}{ll}
X: &amp; {[\text { A B C D E F G] }} \
Y_{1}: &amp; {[\text { A B C D H I K] }} \
Y_{2}: &amp; {[\text { A H B K C I D] }}
\end{array}
$$</p>
<p>$Y_{1}$ and $Y_{2}$ have the same Rouge-L score. However, in this case, $Y_{1}$ should be the better choice than $Y_{2}$ because $Y_{1}$ has consecutive matches. To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call this weighted LCS (WLCS) and use $k$ to indicate the length of the current consecutive matches ending at words $x_{i}$ and $y_{j}$. Given two sentences $X$ and $Y$, the WLCS score of $X$ and $Y$ can be computed using the following dynamic programming procedure:
(1) For $(i=0 ; i&lt;=m ; i++)$
$c(i, j)=0 / /$ initialize $c$-table
$w(i, j)=0 / /$ initialize $w$-table
(2) For $(i=1 ; i&lt;=m ; i++)$</p>
<p>For $(j=1 ; j&lt;=n ; j++)$
If $x_{i}=y_{j}$ Then
// the length of consecutive matches at
// position $i-1$ and $j-1$
$k=w(i-1, j-1)$
$c(i, j)=c(i-1, j-1)+f(k+1)-f(k)$
// remember the length of consecutive
// matches at position $i, j$
$w(i, j)=k+1$
Otherwise
If $c(i-1, j)&gt;c(i, j-1)$ Then
$c(i, j)=c(i-1, j)$
$w(i, j)=0 \quad / /$ no match at $i, j$
Else $c(i, j)=c(i, j-1)$
$w(i, j)=0 \quad / /$ no match at $i, j$
(3) $W L C S(X, Y)=c(m, n)$</p>
<p>Where $c$ is the dynamic programming table, $c(i, j)$ stores the WLCS score ending at word $x_{i}$ of $X$ and $y_{j}$ of $Y, w$ is the table storing the length of consecutive matches ended at $c$ table position $i$ and $j$, and $f$ is a function of consecutive matches at the table posi-
tion, $c(i, j)$. Notice that by providing different weighting function $f$, we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches.</p>
<p>The weighting function $f$ must have the property that $f(x+y)&gt;f(x)+f(y)$ for any positive integers $x$ and $y$. In other words, consecutive matches are awarded more scores than non-consecutive matches. For example, $f(k)=\alpha k-\beta$ when $k&gt;=0$, and $\alpha, \beta&gt;$ 0 . This function charges a gap penalty of $-\beta$ for each non-consecutive n-gram sequences. Another possible function family is the polynomial family of the form $k^{\alpha}$ where $\alpha&gt;1$. However, in order to normalize the final Rouge-W score, we also prefer to have a function that has a close form inverse function. For example, $f(k)=k^{2}$ has a close form inverse function $f^{-1}(k)=k^{1 / 2}$. F-measure based on WLCS can be computed as follows, given two sequences $X$ of length $m$ and $Y$ of length $n$ :</p>
<p>$$
\begin{aligned}
&amp; R_{w l c s}=f^{-1}\left(\frac{W L C S(X, Y)}{f(m)}\right) \
&amp; P_{w l c s}=f^{-1}\left(\frac{W L C S(X, Y)}{f(n)}\right) \
&amp; F_{w l c s}=\frac{\left(1+\beta^{2}\right) R_{w l c s} P_{w l c s}}{R_{w l c s}+\beta^{2} P_{w l c s}}
\end{aligned}
$$</p>
<p>Where $f^{-1}$ is the inverse function of $f$. In DUC, $\beta$ is set to a very big number ( $\left.? 8\right)$. Therefore, only $R_{w l c s}$ is considered. We call the WLCS-based Fmeasure, i.e. Equation 15, RougE-W. Using Equation 15 and $f(k)=k^{2}$ as the weighting function, the ROUGE-W scores for sequences $Y_{l}$ and $Y_{2}$ are 0.571 and 0.286 respectively. Therefore, $Y_{l}$ would be ranked higher than $Y_{2}$ using WLCS. We use the polynomial function of the form $k^{\alpha}$ in the Rouge evaluation package. In the next section, we introduce the skip-bigram co-occurrence statistics.</p>
<h2>5 Rouge-S: Skip-Bigram Co-Occurrence Statistics</h2>
<p>Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. Skip-bigram cooccurrence statistics measure the overlap of skipbigrams between a candidate translation and a set of reference translations. Using the example given in Section 3.1:</p>
<p>S1. police killed the gunman
S2. police kill the gunman
S3. the gunman kill police
S4. the gunman police killed</p>
<p>each sentence has $C(4,2)^{1}=6$ skip-bigrams. For example, S1 has the following skip-bigrams:
("police killed", "police the", "police gunman", "killed the", "killed gunman", "the gunman")
S2 has three skip-bigram matches with S1 ("police the", "police gunman", "the gunman"), S3 has one skip-bigram match with S1 ("the gunman"), and S4 has two skip-bigram matches with S1 ("police killed", "the gunman"). Given translations $X$ of length $m$ and $Y$ of length $n$, assuming $X$ is a reference translation and $Y$ is a candidate translation, we compute skip-bigram-based F-measure as follows:</p>
<p>$$
\begin{aligned}
&amp; R_{\text {skip } 2}=\frac{\operatorname{SKIP} 2(X, Y)}{C(m, 2)} \
&amp; P_{\text {skip } 2}=\frac{\operatorname{SKIP} 2(X, Y)}{C(n, 2)} \
&amp; F_{\text {skip } 2}=\frac{\left(1+\beta^{2}\right) R_{\text {skip } 2} P_{\text {skip } 2}}{R_{\text {skip } 2}+\beta^{2} P_{\text {skip } 2}}
\end{aligned}
$$</p>
<p>Where $\operatorname{SKIP} 2(X, Y)$ is the number of skip-bigram matches between $X$ and $Y, \beta$ controlling the relative importance of $P_{\text {skip } 2}$ and $R_{\text {skip } 2}$, and $C$ is the combination function. We call the skip-bigram-based F measure, i.e. Equation 18, Rouge-S.
Using Equation 18 with $\beta=1$ and S 1 as the reference, S2's Rouge-S score is 0.5 , S3 is 0.167 , and S 4 is 0.333 . Therefore, S 2 is better than S 3 and S 4 , and S 4 is better than S 3 . This result is more intuitive than using Bleu-2 and Rouge-L. One advantage of skip-bigram vs. Bleu is that it does not require consecutive matches but is still sensitive to word order. Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence.
Applying skip-bigram without any constraint on the distance between the words, spurious matches such as "the the" or "of in" might be counted as valid matches. To reduce these spurious matches, we can limit the maximum skip distance, $d_{\text {skip }}$, between two in-order words that is allowed to form a skip-bigram. For example, if we set $d_{\text {skip }}$ to 0 then Rouge-S is equivalent to bigram overlap Fmeasure. If we set $d_{\text {skip }}$ to 4 then only word pairs of at most 4 words apart can form skip-bigrams.
Adjusting Equations 16, 17, and 18 to use maximum skip distance limit is straightforward: we only count the skip-bigram matches, $\operatorname{SKIP} 2(X, Y)$, within the maximum skip distance and replace denominators of Equations 16, $C(m, 2)$, and 17, $C(n, 2)$, with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>5.1 ROUGE-SU: Extension of ROUGE-S</h3>
<p>One potential problem for Rouge-S is that it does not give any credit to a candidate sentence if the sentence does not have any word pair co-occurring with its references. For example, the following sentence has a ROUGE-S score of zero:</p>
<p>S5. gunman the killed police
S5 is the exact reverse of S1 and there is no skip bigram match between them. However, we would like to differentiate sentences similar to S5 from sentences that do not have single word cooccurrence with S1. To achieve this, we extend ROUGE-S with the addition of unigram as counting unit. The extended version is called Rouge-SU. We can also obtain Rouge-SU from Rouge-S by adding a begin-of-sentence marker at the beginning of candidate and reference sentences.</p>
<h2>6 Evaluations of Rouge</h2>
<p>To assess the effectiveness of Rouge measures, we compute the correlation between ROUGE assigned summary scores and human assigned summary scores. The intuition is that a good evaluation measure should assign a good score to a good summary and a bad score to a bad summary. The ground truth is based on human assigned scores. Acquiring human judgments are usually very expensive; fortunately, we have DUC 2001, 2002, and 2003 evaluation data that include human judgments for the following:</p>
<ul>
<li>Single document summaries of about 100 words: 12 systems ${ }^{2}$ for DUC 2001 and 14 systems for 2002. 149 single document summaries were judged per system in DUC 2001 and 295 were judged in DUC 2002.</li>
<li>Single document very short summaries of about 10 words (headline-like, keywords, or phrases): 14 systems for DUC 2003. 624 very short summaries were judged per system in DUC 2003.</li>
<li>Multi-document summaries of about 10 words: 6 systems for DUC 2002; 50 words: 14 systems for DUC 2001 and 10 systems for DUC 2002; 100 words: 14 systems for DUC 2001, 10 systems for DUC 2002, and 18 systems for DUC 2003; 200 words: 14 systems for DUC 2001 and 10 systems for DUC 2002; 400 words: 14 systems for DUC 2001. 29 summaries were judged per system per summary size in DUC 2001, 59 were judged in DUC 2002, and 30 were judged in DUC 2003.</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Table 1: Pearson's correlations of 17 ROUGE measure scores vs. human judgments for the DUC 2001 and 2002100 words single document summarization tasks
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 2: Pearson's correlations of 17 ROUGE measure scores vs. human judgments for the DUC 2003 very short summary task</p>
<p>Besides these human judgments, we also have 3 sets of manual summaries for DUC 2001, 2 sets for DUC 2002, and 4 sets for DUC 2003. Human judges assigned content coverage scores to a candidate summary by examining the percentage of content overlap between a manual summary unit, i.e. elementary discourse unit or sentence, and the candidate summary using Summary Evaluation Environment ${ }^{3}$ (SEE) developed by the University of Southern California's Information Sciences Institute (ISI). The overall candidate summary score is the average of the content coverage scores of all the units in the manual summary. Note that human judges used only one manual summary in all the evaluations although multiple alternative summaries were available.</p>
<p>With the DUC data, we computed Pearson's product moment correlation coefficients, Spearman's rank order correlation coefficients, and Kendall's correlation coefficients between systems' average ROUGE scores and their human assigned average coverage scores using single reference and multiple references. To investigate the effect of stemming and inclusion or exclusion of stopwords, we also ran experiments over orig inal automatic and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>manual summaries (CASE set), stemmed ${ }^{4}$ version of the summaries (STEM set), and stopped version of the summaries (STOP set). For example, we computed ROUGE scores for the 12 systems participated in the DUC 2001 single document summarization evaluation using the CASE set with single reference and then calculated the three correlation scores for these 12 systems' ROUGE scores vs. human assigned average coverage scores. After that we repeated the process using multiple references and then using STEM and STOP sets. Therefore, 2 (multi or single) x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) $=18$ data points were collected for each ROUGE measure and each DUC task. To assess the significance of the results, we applied bootstrap resampling technique (Davison and Hinkley, 1997) to estimate $95 \%$ confidence intervals for every correlation computation.</p>
<p>17 ROUGE measures were tested for each run using ROUGE evaluation package v1.2.1: ROUGE-N with $\mathrm{N}=1$ to 9 , ROUGE-L, ROUGE-W with weighting factor $\alpha=1.2$, ROUGE-S and ROUGE-SU with maximum skip distance $d_{\text {skip }}=1,4$, and 9 . Due to limitation of space, we only report correlation analysis results based on Pearson's correlation coefficient. Correlation analyses based on Spearman's and Kendall's correlation coefficients are tracking Pearson's very closely and will be posted later at the ROUGE website ${ }^{5}$ for reference. The critical value ${ }^{6}$ for Pearson's correlation is 0.632 at $95 \%$ confidence with 8 degrees of freedom.</p>
<p>Table 1 shows the Pearson's correlation coefficients of the 17 ROUGE measures vs. human judgments on DUC 2001 and 2002100 words single document summarization data. The best values in each column are marked with dark (green) color and statistically equivalent values to the best values are marked with gray. We found that correlations were not affected by stemming or removal of stopwords in this data set, ROUGE-2 performed better among the ROUGE-N variants, ROUGE-L, ROUGE-W, and ROUGE-S were all performing well, and using multiple references improved performance though not much. All ROUGE measures achieved very good correlation with human judgments in the DUC 2002 data. This might due to the double sample size in DUC 2002 (295 vs. 149 in DUC 2001) for each system.</p>
<p>Table 2 shows the correlation analysis results on the DUC 2003 single document very short summary data. We found that ROUGE-1, ROUGE-L, ROUGE-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Table 3: Pearson's correlations of 17 ROUGE measure scores vs. human judgments for the DUC 2001, 2002, and 2003 multi-document summarization tasks</p>
<p>SU4 and 9, and ROUGE-W were very good measures in this category, ROUGE-N with N &gt; 1 performed significantly worse than all other measures, and exclusion of stopwords improved performance in general except for ROUGE-1. Due to the large number of samples (624) in this data set, using multiple references did not improve correlations.</p>
<p>In Table 3 A1, A2, and A3, we show correlation analysis results on DUC 2001, 2002, and 2003100 words multi-document summarization data. The results indicated that using multiple references improved correlation and exclusion of stopwords usually improved performance. ROUGE-1, 2, and 3 performed fine but were not consistent. ROUGE-1, ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGESU9 with stopword removal had correlation above 0.70. ROUGE-L and ROUGE-W did not work well in this set of data.</p>
<p>Table 3 C, D1, D2, E1, E2, and F show the correlation analyses using multiple references on the rest of DUC data. These results again suggested that exclusion of stopwords achieved better performance especially in multi-document summaries of 50 words. Better correlations (&gt; 0.70) were observed on long summary tasks, i.e. 200 and 400 words summaries. The relative performance of ROUGE measures followed the pattern of the 100 words multi-document summarization task.</p>
<p>Comparing the results in Table 3 with Tables 1 and 2, we found that correlation values in the multidocument tasks rarely reached high $90 \%$ except in long summary tasks. One possible explanation of this outcome is that we did not have large amount of samples for the multi-document tasks. In the single document summarization tasks we had over 100
samples; while we only had about 30 samples in the multi-document tasks. The only tasks that had over 30 samples was from DUC 2002 and the correlations of ROUGE measures with human judgments on the 100 words summary task were much better and more stable than similar tasks in DUC 2001 and 2003. Statistically stable human judgments of system performance might not be obtained due to lack of samples and this in turn caused instability of correlation analyses.</p>
<h2>7 Conclusions</h2>
<p>In this paper, we introduced ROUGE, an automatic evaluation package for summarization, and conducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data. To check the significance of the results, we estimated confidence intervals of correlations using bootstrap resampling. We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high $90 \%$ was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.</p>
<p>In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries. In a separate study (Lin and Och, 2004),</p>
<p>RougE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation. The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004). However, how to achieve high correlation with human judgments in multi-document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic.</p>
<h2>8 Acknowledgements</h2>
<p>The author would like to thank the anonymous $\mathbb{R}$ viewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research.</p>
<h2>References</h2>
<p>Cormen, T. R., C. E. Leiserson, and R. L. Rivest. 1989. Introduction to Algorithms. The MIT Press.</p>
<p>Davison, A. C. and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge University Press.
Lin, C.-Y. and E. H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada.
Lin, C.-Y. 2004. Looking for a few good metrics: Rouge and its evaluation. In Proceedings of NTCIR Workshop 2004, Tokyo, Japan.
Lin, C.-Y. and F. J. Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of $42^{\text {nd }}$ Annual Meeting of ACL (ACL 2004), Barcelona, Spain.
Mani, I. 2001. Automatic Summarization. John Benjamins Publishing Co.
Melamed, I. D. 1995. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons. In Proceedings of the $3^{\text {rd }}$ Workshop on Very Large Corpora (WVLC3). Boston, U.S.A.</p>
<p>Melamed, I. D., R. Green and J. P. Turian (2003). Precision and recall of machine ranslation. In Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada.
Over, P. and J. Yen. 2003. An introduction to DUC 2003 - Intrinsic evaluation of generic news text summarization systems.
http://www-nlpir.nist.gov/projects/duc/pubs/ 2003slides/duc2003intro.pdf
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2001. Bleu: A method for automatic evaluation of machine translation. IBM Research Report RC22176 (W0109-022).
Saggion H., D. Radev, S. Teufel, and W. Lam. 2002. Meta-evaluation of summaries in a crosslingual environment using content-based metrics. In Proceedings of COLING-2002, Taipei, Taiwan.
Radev, D. S. Teufel, H. Saggion, W. Lam, J. Blitzer, A. Gelebi, H. Qi, E. Drabek, and D. Liu. 2002. Evaluation of Text Summarization in a Cross-Lingual Information Retrieval Framework. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA.
Van Rijsbergen, C. J. 1979. Information Retrieval. Butterworths. London.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ SEE is available online at http://www.isi.edu/ cyl.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Porter's stemmer was used.
${ }^{5}$ ROUGE website: http://www.isi.edu/ cyl/ROUGE.
${ }^{6}$ The critical values for Pearson's correlation at $95 \%$ confidence with $10,12,14$, and 16 degrees of freedom are $0.576,0.532,0.497$, and 0.468 respectively.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
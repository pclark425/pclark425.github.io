<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-279155076</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.03221v3.pdf" target="_blank">ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> As the volume of scientific literature grows, efficient knowledge organization becomes increasingly challenging. Traditional approaches to structuring scientific content are time-consuming and require significant domain expertise, highlighting the need for tool support. We present ExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists researchers in transforming unstructured publications into structured representations. The workflow combines large language models (LLMs) with user-defined schemas and is designed for downstream integration into knowledge graphs (KGs). Developed and evaluated in the context of the Open Research Knowledge Graph (ORKG), ExtracTable automates key steps such as document preprocessing and data extraction while ensuring user oversight through validation. In an evaluation with ORKG community participants following the Quality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and practical value. Participants gave it an average System Usability Scale (SUS) score of 84.17 (A+, the highest rating). The time to progress from a research interest to literature-based insights was reduced from between 4 hours and 2 weeks to an average of 24:40 minutes. By streamlining corpus creation and structured data extraction for knowledge graph integration, ExtracTable leverages LLMs and user models to accelerate literature reviews. However, human validation remains essential to ensure quality, and future work will address improving extraction accuracy and entity linking to existing knowledge resources.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9617.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9617.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral Large-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Large-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM used by the ExtracTable framework to parse preprocessed paper text and produce JSON-formatted extractions of user-defined properties for downstream KG integration; invoked via Mistral's API within a human-in-the-loop pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral Large-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Accessed via Mistral's API and used for iterative prompting to extract structured information from preprocessed paper text; configured to return JSON-formatted outputs to improve downstream compatibility and to flag missing properties (rendering 'NOT FOUND').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-defined corpus constructed interactively from search APIs (SemanticScholar, ArXiv, SerpApi) or imported from Zotero; abstracts (and optionally full text links) are retrieved, cleaned (pdfminer, regex heuristics) and passed to the model per-document for property extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>A user-defined 'research interest' (natural language); optionally the framework suggests keywords via an LLM to help formulate search queries; extraction is then guided by a user-specified set of properties/data-model fields to be populated for each paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Iterative prompting of the LLM per document using user-defined properties (a JSON-output prompt template), operating on preprocessed extracted text; if properties cannot be found the model is instructed to flag them ('NOT FOUND'); outputs are presented in an editable datagrid for human validation and CSV export. Entity extraction is performed as a separate step (Falcon 2.0 used for concept/entity extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured, machine-readable JSON/CSV table of extracted properties (datagrid), intended for knowledge graph integration (CSV export; future JSON-LD/RDF planned).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>JSON-formatted property object per paper with fields populated or labeled 'NOT FOUND' where the LLM did not locate information; results displayed in an editable datagrid for validation (the paper references an example prompt/output in the repository).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Design-science user study with nine ORKG curators: subjective questionnaires (Likert), System Usability Scale (SUS), timed sessions for workflow completion, comparison of created comparisons against participants' original ORKG comparisons, screen/audio recordings for post-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>High usability: overall SUS = 84.17 (A+). Time to create a comparison reduced to mean 1,480 seconds (24:40 minutes). Newly generated comparisons averaged 5 papers vs. original 11; 98% of newly created comparisons differed from originals, limiting objective quality comparison. Users reported faster/easier corpus creation but expressed concerns about extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Greatly reduces time for corpus creation and structured extraction; outputs JSON-ready for downstream processing; modular Jupyter-based framework with interactive editing preserves human oversight; high reported usability and practical value for ORKG workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extraction accuracy not guaranteed; risk of hallucination requires human validation; entity linking not yet integrated (extracted entities remain disconnected from KGs); performance degrades for large corpora (sequential per-document processing); limited user control over data model in current implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Majority of generated comparisons did not match participants' original comparisons (98% differed), making objective replication of prior work difficult; users reported uneven extraction quality unless they invested additional manual refinement; missing entity linking prevented semantic integration; potential for incorrect extractions flagged as 'NOT FOUND' but still requiring human correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9617.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9617.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An entity and relation extraction/linking tool used in the ExtracTable pipeline to identify concepts in free-text fields, with built-in support for linking to knowledge bases such as DBpedia and Wikidata (linking not fully integrated in ExtracTable yet).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used for entity extraction over free-text fields in the pipeline; described as capable of linking to KGs like DBpedia and Wikidata, though ExtracTable currently focuses on extraction rather than completed linking and integration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Free-text extraction fields produced from the same user-selected corpus (abstracts/full-text snippets) after pdf text preprocessing within ExtracTable.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>No single topic query; entity extraction is applied to the extracted text fields per document to identify relevant concepts for later semantification/linking.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Automated entity extraction applied to free-text fields; Falcon's outputs are captured as candidate entities for potential future linking to KGs (DBpedia/Wikidata), but full linking and alignment steps are not yet implemented in the system.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted entity mentions / candidate KG identifiers (extraction-only in current implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Lists of identified concepts/entities associated with text fields (paper notes that Falcon supports KG linking but ExtracTable only retains extracted entities as a first step).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No formal evaluation of Falcon's extraction quality provided in this paper beyond user feedback and overall tool assessment; entity linking remains a planned future integration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not separately quantified in the study; integrated use reported as an initial step toward semantic enrichment but not yet producing linked KG outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides a path toward automated semantic enrichment by extracting candidate entities and enabling future KG linking; leverages existing tools with KG linking capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Entity extraction is not yet linked to knowledge graphs within ExtracTable, so extracted entities remain disconnected; no reported metrics for extraction precision/recall in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Primary failure mode is lack of end-to-end linking—extracted entities are not yet aligned/connected to ORKG or external KGs, limiting machine-actionability and semantic interoperability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9617.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9617.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG Ask (neuro-symbolic service)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open Research Knowledge Graph Assistant for Scientific Knowledge (Ask)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic scholarly search/exploration service that combines semantic search, knowledge graphs, and LLM capabilities to provide structured, machine-and-human-readable answers and mitigate hallucinations by grounding in knowledge bases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ORKG Ask (neuro-symbolic system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a neuro-symbolic service that integrates semantic knowledge representation and AI (including LLMs and semantic search over KGs) to provide next-generation scholarly search and exploration; intended to reduce LLM hallucinations by grounding outputs in the ORKG knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>ORKG's structured scholarly content / knowledge graph entries (not a large raw-paper corpus); system uses the structured KG as grounding for responses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Scholarly search and exploration queries over ORKG content; not a single topic—system supports general neuro-symbolic query answering grounded in KG data.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Neuro-symbolic integration: combining semantic search over a KG with LLM capabilities to synthesize answers while grounding them in structured knowledge to mitigate hallucination; specific prompting or RAG details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded, structured answers and exploration aids for scholarly queries; supports machine- and human-readability for research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Not provided in detail in this paper; referenced as an example of combining LLMs with KGs to support literature organization and query-answering.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated within this paper; ORKG and Ask are discussed as prior art and conceptual motivation for neuro-symbolic grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Grounding LLM outputs in a structured KG reduces hallucination risk and increases reliability compared to unguided LLM outputs; supports FAIR principles by leveraging structured, interoperable knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>KG construction at scale still requires human oversight and manual work; circular dependency exists between requiring curated KGs to mitigate LLM failures while using LLMs to build those KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not empirically reported in this paper; authors note the general challenge that KG construction needs domain expertise and that relying solely on AI reintroduces quality risks without human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9617.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9617.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced in related work describing methods where humans review ChatGPT-generated outputs for tasks in systematic literature reviews (abstract screening, data extraction), with limitations due to the model's static training data and hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned as used in prior work (Alshami et al.) for automating parts of systematic review workflows; criticized for being limited to its training cutoff and for hallucination tendencies, requiring human review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; prior work used ChatGPT to generate screening and extraction outputs from given literature sets (details in the cited Alshami et al. study).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Not specified here; prior work applied ChatGPT to literature-review tasks such as abstract screening and data extraction across study corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>ChatGPT-generated data for literature setup, abstract screening, data extraction and analysis, followed by human review and correction (human-in-the-loop validation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>LLM-generated screening/extraction data and spreadsheet outputs for literature review steps.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Not provided in this paper; described generically as ChatGPT-generated data that humans subsequently review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>This paper cites Alshami et al.'s work; it notes that relying solely on ChatGPT restricts access to real-time data and that outputs were human-reviewed in that study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Summarized in this paper as indicating that ChatGPT-based workflows require human review and are limited by the model's training data (no real-time updates), reducing applicability for current data retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Can accelerate repetitive literature-review tasks when combined with human validation; useful for generating initial structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Restricted to training-data knowledge (no real-time retrieval), risk of hallucination and inaccuracy, necessitating human validation; relying solely on ChatGPT is not advisable for rigorous scientific synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Limitations in scope due to static training data and hallucinations; prior-work-reported errors require manual correction and reduce trustworthiness for unsupervised synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Harnessing the Power of ChatGPT for Automating Systematic Review Process: Methodology, Case Study, Limitations, and Future Directions <em>(Rating: 2)</em></li>
                <li>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis <em>(Rating: 2)</em></li>
                <li>Enhancing Scientific Knowledge Graph Generation Pipelines with LLMs and Human-in-the-Loop <em>(Rating: 2)</em></li>
                <li>From human experts to machines: An LLM supported approach to ontology and knowledge graph construction <em>(Rating: 2)</em></li>
                <li>SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9617",
    "paper_id": "paper-279155076",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "Mistral Large-2",
            "name_full": "Mistral Large-2",
            "brief_description": "The LLM used by the ExtracTable framework to parse preprocessed paper text and produce JSON-formatted extractions of user-defined properties for downstream KG integration; invoked via Mistral's API within a human-in-the-loop pipeline.",
            "citation_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
            "mention_or_use": "use",
            "model_name": "Mistral Large-2",
            "model_description": "Accessed via Mistral's API and used for iterative prompting to extract structured information from preprocessed paper text; configured to return JSON-formatted outputs to improve downstream compatibility and to flag missing properties (rendering 'NOT FOUND').",
            "model_size": null,
            "input_corpus_description": "User-defined corpus constructed interactively from search APIs (SemanticScholar, ArXiv, SerpApi) or imported from Zotero; abstracts (and optionally full text links) are retrieved, cleaned (pdfminer, regex heuristics) and passed to the model per-document for property extraction.",
            "input_corpus_size": null,
            "topic_query_description": "A user-defined 'research interest' (natural language); optionally the framework suggests keywords via an LLM to help formulate search queries; extraction is then guided by a user-specified set of properties/data-model fields to be populated for each paper.",
            "distillation_method": "Iterative prompting of the LLM per document using user-defined properties (a JSON-output prompt template), operating on preprocessed extracted text; if properties cannot be found the model is instructed to flag them ('NOT FOUND'); outputs are presented in an editable datagrid for human validation and CSV export. Entity extraction is performed as a separate step (Falcon 2.0 used for concept/entity extraction).",
            "output_type": "Structured, machine-readable JSON/CSV table of extracted properties (datagrid), intended for knowledge graph integration (CSV export; future JSON-LD/RDF planned).",
            "output_example": "JSON-formatted property object per paper with fields populated or labeled 'NOT FOUND' where the LLM did not locate information; results displayed in an editable datagrid for validation (the paper references an example prompt/output in the repository).",
            "evaluation_method": "Design-science user study with nine ORKG curators: subjective questionnaires (Likert), System Usability Scale (SUS), timed sessions for workflow completion, comparison of created comparisons against participants' original ORKG comparisons, screen/audio recordings for post-analysis.",
            "evaluation_results": "High usability: overall SUS = 84.17 (A+). Time to create a comparison reduced to mean 1,480 seconds (24:40 minutes). Newly generated comparisons averaged 5 papers vs. original 11; 98% of newly created comparisons differed from originals, limiting objective quality comparison. Users reported faster/easier corpus creation but expressed concerns about extraction accuracy.",
            "strengths": "Greatly reduces time for corpus creation and structured extraction; outputs JSON-ready for downstream processing; modular Jupyter-based framework with interactive editing preserves human oversight; high reported usability and practical value for ORKG workflows.",
            "limitations": "Extraction accuracy not guaranteed; risk of hallucination requires human validation; entity linking not yet integrated (extracted entities remain disconnected from KGs); performance degrades for large corpora (sequential per-document processing); limited user control over data model in current implementation.",
            "failure_cases": "Majority of generated comparisons did not match participants' original comparisons (98% differed), making objective replication of prior work difficult; users reported uneven extraction quality unless they invested additional manual refinement; missing entity linking prevented semantic integration; potential for incorrect extractions flagged as 'NOT FOUND' but still requiring human correction.",
            "uuid": "e9617.0",
            "source_info": {
                "paper_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Falcon 2.0",
            "name_full": "Falcon 2.0",
            "brief_description": "An entity and relation extraction/linking tool used in the ExtracTable pipeline to identify concepts in free-text fields, with built-in support for linking to knowledge bases such as DBpedia and Wikidata (linking not fully integrated in ExtracTable yet).",
            "citation_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
            "mention_or_use": "use",
            "model_name": "Falcon 2.0",
            "model_description": "Used for entity extraction over free-text fields in the pipeline; described as capable of linking to KGs like DBpedia and Wikidata, though ExtracTable currently focuses on extraction rather than completed linking and integration.",
            "model_size": null,
            "input_corpus_description": "Free-text extraction fields produced from the same user-selected corpus (abstracts/full-text snippets) after pdf text preprocessing within ExtracTable.",
            "input_corpus_size": null,
            "topic_query_description": "No single topic query; entity extraction is applied to the extracted text fields per document to identify relevant concepts for later semantification/linking.",
            "distillation_method": "Automated entity extraction applied to free-text fields; Falcon's outputs are captured as candidate entities for potential future linking to KGs (DBpedia/Wikidata), but full linking and alignment steps are not yet implemented in the system.",
            "output_type": "Extracted entity mentions / candidate KG identifiers (extraction-only in current implementation).",
            "output_example": "Lists of identified concepts/entities associated with text fields (paper notes that Falcon supports KG linking but ExtracTable only retains extracted entities as a first step).",
            "evaluation_method": "No formal evaluation of Falcon's extraction quality provided in this paper beyond user feedback and overall tool assessment; entity linking remains a planned future integration.",
            "evaluation_results": "Not separately quantified in the study; integrated use reported as an initial step toward semantic enrichment but not yet producing linked KG outputs.",
            "strengths": "Provides a path toward automated semantic enrichment by extracting candidate entities and enabling future KG linking; leverages existing tools with KG linking capability.",
            "limitations": "Entity extraction is not yet linked to knowledge graphs within ExtracTable, so extracted entities remain disconnected; no reported metrics for extraction precision/recall in this work.",
            "failure_cases": "Primary failure mode is lack of end-to-end linking—extracted entities are not yet aligned/connected to ORKG or external KGs, limiting machine-actionability and semantic interoperability.",
            "uuid": "e9617.1",
            "source_info": {
                "paper_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ORKG Ask (neuro-symbolic service)",
            "name_full": "Open Research Knowledge Graph Assistant for Scientific Knowledge (Ask)",
            "brief_description": "A neuro-symbolic scholarly search/exploration service that combines semantic search, knowledge graphs, and LLM capabilities to provide structured, machine-and-human-readable answers and mitigate hallucinations by grounding in knowledge bases.",
            "citation_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
            "mention_or_use": "mention",
            "model_name": "ORKG Ask (neuro-symbolic system)",
            "model_description": "Described as a neuro-symbolic service that integrates semantic knowledge representation and AI (including LLMs and semantic search over KGs) to provide next-generation scholarly search and exploration; intended to reduce LLM hallucinations by grounding outputs in the ORKG knowledge base.",
            "model_size": null,
            "input_corpus_description": "ORKG's structured scholarly content / knowledge graph entries (not a large raw-paper corpus); system uses the structured KG as grounding for responses.",
            "input_corpus_size": null,
            "topic_query_description": "Scholarly search and exploration queries over ORKG content; not a single topic—system supports general neuro-symbolic query answering grounded in KG data.",
            "distillation_method": "Neuro-symbolic integration: combining semantic search over a KG with LLM capabilities to synthesize answers while grounding them in structured knowledge to mitigate hallucination; specific prompting or RAG details not provided in this paper.",
            "output_type": "Grounded, structured answers and exploration aids for scholarly queries; supports machine- and human-readability for research tasks.",
            "output_example": "Not provided in detail in this paper; referenced as an example of combining LLMs with KGs to support literature organization and query-answering.",
            "evaluation_method": "Not evaluated within this paper; ORKG and Ask are discussed as prior art and conceptual motivation for neuro-symbolic grounding.",
            "evaluation_results": "Not reported here.",
            "strengths": "Grounding LLM outputs in a structured KG reduces hallucination risk and increases reliability compared to unguided LLM outputs; supports FAIR principles by leveraging structured, interoperable knowledge.",
            "limitations": "KG construction at scale still requires human oversight and manual work; circular dependency exists between requiring curated KGs to mitigate LLM failures while using LLMs to build those KGs.",
            "failure_cases": "Not empirically reported in this paper; authors note the general challenge that KG construction needs domain expertise and that relying solely on AI reintroduces quality risks without human validation.",
            "uuid": "e9617.2",
            "source_info": {
                "paper_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChatGPT (in related work)",
            "name_full": "ChatGPT (OpenAI conversational LLM)",
            "brief_description": "Referenced in related work describing methods where humans review ChatGPT-generated outputs for tasks in systematic literature reviews (abstract screening, data extraction), with limitations due to the model's static training data and hallucination risk.",
            "citation_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
            "mention_or_use": "mention",
            "model_name": "ChatGPT",
            "model_description": "Mentioned as used in prior work (Alshami et al.) for automating parts of systematic review workflows; criticized for being limited to its training cutoff and for hallucination tendencies, requiring human review.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper; prior work used ChatGPT to generate screening and extraction outputs from given literature sets (details in the cited Alshami et al. study).",
            "input_corpus_size": null,
            "topic_query_description": "Not specified here; prior work applied ChatGPT to literature-review tasks such as abstract screening and data extraction across study corpora.",
            "distillation_method": "ChatGPT-generated data for literature setup, abstract screening, data extraction and analysis, followed by human review and correction (human-in-the-loop validation).",
            "output_type": "LLM-generated screening/extraction data and spreadsheet outputs for literature review steps.",
            "output_example": "Not provided in this paper; described generically as ChatGPT-generated data that humans subsequently review.",
            "evaluation_method": "This paper cites Alshami et al.'s work; it notes that relying solely on ChatGPT restricts access to real-time data and that outputs were human-reviewed in that study.",
            "evaluation_results": "Summarized in this paper as indicating that ChatGPT-based workflows require human review and are limited by the model's training data (no real-time updates), reducing applicability for current data retrieval.",
            "strengths": "Can accelerate repetitive literature-review tasks when combined with human validation; useful for generating initial structured outputs.",
            "limitations": "Restricted to training-data knowledge (no real-time retrieval), risk of hallucination and inaccuracy, necessitating human validation; relying solely on ChatGPT is not advisable for rigorous scientific synthesis.",
            "failure_cases": "Limitations in scope due to static training data and hallucinations; prior-work-reported errors require manual correction and reduce trustworthiness for unsupervised synthesis.",
            "uuid": "e9617.3",
            "source_info": {
                "paper_title": "ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Harnessing the Power of ChatGPT for Automating Systematic Review Process: Methodology, Case Study, Limitations, and Future Directions",
            "rating": 2,
            "sanitized_title": "harnessing_the_power_of_chatgpt_for_automating_systematic_review_process_methodology_case_study_limitations_and_future_directions"
        },
        {
            "paper_title": "A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis",
            "rating": 2,
            "sanitized_title": "a_hybrid_semiautomated_workflow_for_systematic_and_literature_review_processes_with_large_language_model_analysis"
        },
        {
            "paper_title": "Enhancing Scientific Knowledge Graph Generation Pipelines with LLMs and Human-in-the-Loop",
            "rating": 2,
            "sanitized_title": "enhancing_scientific_knowledge_graph_generation_pipelines_with_llms_and_humanintheloop"
        },
        {
            "paper_title": "From human experts to machines: An LLM supported approach to ontology and knowledge graph construction",
            "rating": 2,
            "sanitized_title": "from_human_experts_to_machines_an_llm_supported_approach_to_ontology_and_knowledge_graph_construction"
        },
        {
            "paper_title": "SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain",
            "rating": 2,
            "sanitized_title": "scicero_a_deep_learning_and_nlp_approach_for_generating_scientific_knowledge_graphs_in_the_computer_science_domain"
        }
    ],
    "cost": 0.012771249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge
18 Jul 2025</p>
<p>Lena John lena.john@tib.eu 
TIB -Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Ahmed Malek Ghanmi 
Leibniz University Hannover
HannoverGermany</p>
<p>Tim Wittenborg tim.wittenborg@l3s.de 
L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>Cluster of Excellence SE²A -Sustainable and Energy-Efficient Aviation
Technische Universität Braunschweig
Germany</p>
<p>Sören Auer soeren.auer@tib.eu 
TIB -Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>Cluster of Excellence SE²A -Sustainable and Energy-Efficient Aviation
Technische Universität Braunschweig
Germany</p>
<p>Oliver Karras oliver.karras@tib.eu 
TIB -Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>ExtracTable: Human-in-the-Loop Transformation of Scientific Corpora into Structured Knowledge
18 Jul 202580B874F5CB96907D9E8E8BAB3D045DB9arXiv:2506.03221v3[cs.DL]HITLKnowledge Graph IntegrationLiterature ReviewLLMModular frameworkKnowledge Organization
As the volume of scientific literature grows, efficient knowledge organization becomes increasingly challenging.Traditional approaches to structuring scientific content are time-consuming and require significant domain expertise, highlighting the need for tool support.We present ExtracTable, a Human-in-the-Loop (HITL) workflow and framework that assists researchers in transforming unstructured publications into structured representations.The workflow combines large language models (LLMs) with user-defined schemas and is designed for downstream integration into knowledge graphs (KGs).Developed and evaluated in the context of the Open Research Knowledge Graph (ORKG), ExtracTable automates key steps such as document preprocessing and data extraction while ensuring user oversight through validation.In an evaluation with ORKG community participants following the Quality Improvement Paradigm (QIP), ExtracTable demonstrated high usability and practical value.Participants gave it an average System Usability Scale (SUS) score of 84.17 (A+, the highest rating).The time to progress from a research interest to literature-based insights was reduced from between 4 hours and 2 weeks to an average of 24:40 minutes.By streamlining corpus creation and structured data extraction for knowledge graph integration, ExtracTable leverages LLMs and user models to accelerate literature reviews.However, human validation remains essential to ensure quality, and future work will address improving extraction accuracy and entity linking to existing knowledge resources.</p>
<p>Introduction</p>
<p>Conducting literature reviews is indispensable for scientific research.This task is inherently challenging, as it requires a significant amount of time and ef-fort to gain an overview of the existing research and knowledge on a specific topic, helping to support methodology, contextualize research, or identify gaps in knowledge [34].With the rapid growth in scientific discovery and distribution of knowledge, it has become even more difficult to produce high-quality reviews [24].Recent advancements in artificial intelligence (AI), particularly the rise of large language models (LLMs) and AI-based Literature Reviews [39], offer promising opportunities to streamline and automate repeatable and time-consuming processes [30].However, despite their growing popularity, AI models face significant limitations, primarily the tendency to produce hallucinations, resulting in inconsistencies and confusion [29].Combining AI capabilities with a consistent knowledge base, called neuro-symbolic AI, mitigates these risks by leveraging reliable knowledge representations to alleviate AI's unreliability [3].The knowledge bases need to be Findable, Accessible, Interoperable, and Reusable (FAIR) [40] to address the requirements of the scholarly domain and AI technology [2].Such solutions already exist, like the Open Research Knowledge Graph (ORKG) 5 and its neuro-symbolic service Ask [3], offering a novel approach by representing scientific information in a structured format, providing both human-and machinereadability.However, organizing knowledge at scale reintroduces the original challenge of requiring human oversight, leading to manual work and a circular dependency.To address this issue, an iterative, Human-in-the-Loop (HITL), AI-assisted knowledge organization approach is needed, constantly improving scholarly knowledge representation.Our contributions include 1) ExtracTable, a Human-in-the-Loop (HITL) approach comprising a workflow and modular implementation that combines large language model (LLM) support with Natural Language Processing (NLP) techniques to streamline, automate, and formalize literature review tasks.2) The modular implementation of ExtracTable as a Jupyter-based framework to build scientific corpora and transform unstructured information into structured knowledge ready for knowledge graph integration.3) An iterative evaluation with nine researchers over four months, demonstrating ExtracTable's ability to improve efficiency while maintaining user oversight of automated processes.This work is structured as follows: In section 2 we give an overview of the background and related work in the HITL context.Our approach is described in section 3, followed by an outline of the implementation in section 4. We evaluate the developed solution of ExtracTable in section 5 and discuss the results in section 6.We conclude our work in section 7.</p>
<p>Background and Related Work</p>
<p>the scientific knowledge graph (SKG) Semantic Scholar [38].Manually preparing data for KGs, particularly SKGs, is time-consuming and lacks scalability, as it requires domain experts to search, interpret, and structure scientific literature [12].Without domain experts, there is a higher risk of integrating incorrect, incomplete, or outdated information into the SKG, reducing its reliability and quality [15].The quality of SKGs heavily depends on their construction method, with significantly different results between manual and automated methods [13].The ORKG is a versatile scientific knowledge platform that uses crowdsourcing to organize and link scientific knowledge across various topics and domains [3].Karras et al. [21,18,19,20] demonstrated how the ORKG can enhance and ensure FAIR [40] literature reviews, highlighting their importance in providing research with structured, long-term available data.The ORKG Assistant for Scientific Knowledge (Ask) [3] applies a neuro-symbolic approach using semantic search, LLMs, and KGs.It exemplifies a next-generation scholarly search and exploration system, combining semantic knowledge representation with AI.</p>
<p>Artificial Intelligence (AI).AI and NLP techniques can automate content generation and assignment [10], but these methods often lack transparency and accountability while struggling with semantic context [22].Fully relying on AIsupported technologies poses significant risks, making human supervision and validation indispensable [31].The study conducted by Danler et al. [9] demonstrates that, while AI-generated outputs can be of high quality, "relying solely on AI-generated outputs for crafting a scientific paper is not advisable" [9, p. 207].Bacinger et al. [5] present Litre Assistant, a machine-learning-based solution to determining a corpus of scientific papers as the initial task of the Systematic Literature Review (SLR) process.Their work introduces a system that allows users to iteratively select desired papers as a set of training data for machine learning algorithms for an automated classification process.Ye et al. [43] propose a hybrid, "human-oriented", but AI-based workflow to support specific steps of an SLR.Their workflow screens full-text articles to extract relevant data and allows the user to perform inclusion and exclusion decisions, resulting in a spreadsheet of LLM-generated data.The work of Alshami et al. [1] introduces a method where humans review ChatGPT-generated data for literature setup, abstract screening, data extraction, and analysis.However, relying solely on ChatGPT restricts access to real-time data, limiting the research scope to the model's training data and reducing its applicability.</p>
<p>Human-in-the-Loop (HITL).Several previously mentioned works incorporate human oversight in selection, classification, processing, and representation to achieve more accurate results [1,3,5,43].Consequently, research increasingly recognizes the value of HITL approaches [23].Through providing iterative feedback, users validate outputs as well as refine and enhance the performance system over time to bridge the gap between efficiency and contextual relevance [8].Schatz et al. [33] developed a workflow for curating a biomedical KG (DRKG) [14], that integrates HITL components.Their approach is particularly relevant as it addresses HITL usability aspects in the context of curating and enriching KGs.The work by Tsavena et al. [35] builds on the automated pipeline for curating SKGs introduced in SCICERO [11].They propose a HITL validation module as a solution to incorporate human decision-making into the curation process.This module, one of two proposed validation mechanisms, establishes a hybrid approach that integrates human expertise to enhance the curation process.The authors report a significant improvement in the accuracy of automated KG generation, demonstrating an improvement in both precision and recall.SWARM-SLR by Wittenborg et al. [41] establishes a literature review workflow implemented by a modular framework that utilizes several AI-based tools to fulfill 65 requirements derived from SLR guidelines.While many requirements can be supported or even fulfilled by AI, the overall workflow still requires human expert intervention at several stages of the literature review, particularly for knowledge extraction and validation.Users remain crucial for guiding and validating tasks where automated approaches lack accuracy or contextual understanding [26].</p>
<p>Our work builds on the discussed approaches, emphasizing the role of HITL in the context of AI-assisted knowledge graph construction.In this spirit, we aim to develop a solution that transforms unstructured input into structured, human-validated outputs designed for downstream knowledge graph integration.Once integrated into knowledge graphs, these structured outputs can support the human-and machine-readable context needed by neuro-symbolic AI to sustainably advance scholarly knowledge organization.</p>
<p>Approach</p>
<p>We adopt the Quality Improvement Paradigm (QIP) [7] to guide the iterative development of ExtracTable, a Human-in-the-Loop (HITL) approach that includes a workflow and is realized as a modular framework for transforming scientific corpora into structured knowledge.</p>
<p>ExtracTable was developed in close collaboration with ORKG curators, researchers from diverse domains funded through grants to create structured comparisons, a key content type presenting tabular overviews of state-of-the-art research in the ORKG [27].While their financial affiliation may introduce some bias, their interest in improving tools that support their contributions fosters practical, needs-driven feedback.An overview of the development process is shown in Figure 3, with detailed tasks in Table 1.The Project Learning cycle focused on short-term usability improvements based on curator feedback, while the Corporate Learning cycle addressed long-term enhancements to ORKG infrastructure beyond the project.</p>
<p>Table 1: QIP step descriptions for the development of ExtracTable</p>
<p>Step Description</p>
<p>Characterize</p>
<p>In the first phase, we examined research workflows, including topic selection, corpus retrieval, and structured overview creation.Collaborating with ORKG curators, we identified key content and technical requirements, resulting in a modular, adaptable framework for diverse SKG use cases.</p>
<p>Set goals</p>
<p>We defined functional and non-functional requirements to set the tool's scope, ensuring it addresses key pain points like time-consuming corpus creation and limited support for structured output (cf.subsection 3.1).</p>
<p>Choose process</p>
<p>We selected external ORKG curators as a focus group due to their research diversity and active involvement in literaturebased knowledge graph curation.Using an iterative participatory design approach with bi-weekly meetings, we codeveloped modular Jupyter Notebooks for ExtracTable tailored to flexible workflow stages (cf.subsection 3.2).</p>
<p>Execute</p>
<p>In this phase, we implemented and iteratively refined the framework with ongoing feedback.Over three months in five meetings, we made key design choices, including abstractonly publication support and splitting the workflow into two notebooks (cf.section 4).</p>
<p>Analyze</p>
<p>We finalized the framework by incorporating feedback and evaluating its effectiveness through a design science experiment.This revealed usability issues, guided feature refinements, and validated alignment with real-world curation needs (cf.section 5).</p>
<p>Package</p>
<p>Finally, we compiled reusable artifacts for ExtracTable, including framework components, evaluation metrics, and key insights, to support broader adoption.We also assessed ORKG's infrastructure to enable sustainable integration (cf.sections 5 and 6).</p>
<p>Requirements</p>
<p>To define functional and non-functional requirements for ExtracTable (see Table 2), we drew inspiration from Wittenborg et al. [41], who specified requirements for the typical phases of an SLR.While our framework does not aim to replicate or enforce the rigor of a formal SLR, we adopt this process-oriented perspective to ensure relevance and alignment with real-world research workflows.In collaboration with the SWARM-SLR curators and domain experts, we adapted these ideas to guide the design of system requirements and support a more lightweight, customizable process for literature exploration.Our goal is to offer a modular framework for ExtracTable that enables generation and processing of a literature corpus without the complexity of a full SLR pipeline.To ensure modularity and usability, the workflow is divided into two stages, and users can start with the second if a relevant corpus already exists.</p>
<p>Yes</p>
<p>Workflow</p>
<p>We now describe how the ExtracTable approach, centered on its HITL workflow, meets the requirements outlined above.While not aiming to replicate the full SLR process, the two workflow stages draw inspiration from its typical phases and provide a practical structure for key tasks.Our workflow blends automation with human validation, enabling users to focus their expertise on interpreting and refining scientific content.To reduce manual effort, we automate time-consuming steps while preserving user control over key decisions.The workflow can start from any research interest without requiring pre-structured input, making it flexible and adaptable to diverse review scenarios.</p>
<p>Stage 1: Creating a scientific corpus of literature.This stage focuses on building a corpus of publications relevant to the user's research interest (see Figure 2).After defining a research interest and optionally using the LLM to suggest keywords, users formulate a search query to retrieve publications from selected sources.</p>
<p>Search parameters, such as output size or restricting to openly accessible full texts, are configurable to suit user preferences.Retrieved papers are reviewed and selected by the user, forming the input for Stage 2. the properties of the data model to be extracted from each paper.The LLM agent then parses and interprets the corpus, generating output based on the specified properties.If any information is not found, the LLM flags this to mitigate the risk of hallucination.The user can manually refine, revise, and modify the results to ensure accuracy, ultimately validating the output.The final results are formatted, and optionally, entity linking is performed to create output that is suitable for KG integration as a machine-readable file.</p>
<p>Implementation</p>
<p>We implement the described functions as a modular framework for ExtracTable in two Jupyter Notebooks using Python (NR5).These notebooks provide auto-mated modules, visualizations, and browser-rendered interactive controls (NR2).</p>
<p>To enhance usability, we leverage interactive HTML widgets from ipywidgets and the IPython toolkit for abstracting complex code interactions.Users can configure workflows via UI elements such as input fields and checkboxes, and extend functionality by adding query sources or modifying the UI and behavior as needed (NR1).The implementation is modular and flexible, designed to support integration with various knowledge graphs.</p>
<p>The first stage retrieves relevant papers based on a user-defined search query (FR2).To support query formulation, the framework can optionally suggest keywords from the user's research interest (FR1).We integrate multiple APIs, including SemanticScholar, ArXiv, and the SerpApi wrapper, enabling real-time searches across diverse scholarly databases (FR3) based on user preferences (FR4).Returned JSON objects with varying schemas are unified for consistency (NR3).Key elements such as title, abstract, authors, publication date, and full-text links are presented within expandable accordions in the notebook (FR5), as shown in Figure 4.Alternatively, users may work with an existing corpus, which can currently be imported from a Zotero collection (FR7).Based on these suggestions, the user iteratively creates the corpus by selecting and downloading relevant papers (FR6).For reproducibility, the corresponding DOIs can be exported to a CSV file.</p>
<p>The second stage processes the corpus using an LLM.We use the Mistrallarge-2 model, accessible via Mistral's API and currently free of charge.This model supports JSON-formatted outputs, enhancing structure and compatibility with downstream workflows.The tool prompts the LLM iteratively using user-defined properties (FR8) and pre-processed text extracted from each PDF.</p>
<p>Pre-processing starts with raw text extraction via pdfminer.References and appendices are removed, and regex heuristics clean and reconstruct sentences and paragraphs to preserve context.Generated outputs (FR9) are displayed in a datagrid (FR11; see Figure 5), allowing users to review, edit, and select entries for final CSV export.Users can also refine the data model and rerun extraction, though previous results may be overwritten.An example of prompt and output can be found in the repository.We have begun integrating entity linking (FR12) as a step toward semantic enrichment.At this stage, we use Falcon 2.0 [32] for entity extraction, identifying relevant concepts in free-text fields.Although Falcon already supports linking entities to knowledge graphs such as DBpedia [4] and Wikidata [37], we currently focus on extraction as a first step toward machine-actionability.This approach prepares the ground for entity linking and, ultimately, seamless integration with ORKG and other KGs in the future.Previous work [17] examined how knowledge representation evolves across formats and focused on semantically enriching tables by aligning entities and structuring content during KG import.</p>
<p>Evaluation</p>
<p>The framework ExtracTable is evaluated through a design science experiment involving prospective users to assess its practical usability and real-world applicability.To contextualize this evaluation, we consider the ORKG as a representative environment for scientific content creation.Following the goal definition template by Wohlin et al. [42], we define the experiment's objective as follows:</p>
<p>Research goal: Analyze ExtracTable with the purpose of supporting systematic content creation for scientific KGs in terms of usability (effectiveness, efficiency, satisfaction) from the perspective of potential future users familiar with the ORKG in the context of the ORKG.</p>
<p>The goal leads to the research question:</p>
<p>Research question: How does the developed tool ExtracTable impact the usability of systematic content creation for scientific knowledge graphs, from the perspective of potential future users familiar with the ORKG?</p>
<p>The experiment aims to assess the practical usability of the developed tool Ex-tracTable for creating scientific content for knowledge graphs.To this end, we collaborated with ORKG curators who have previously created comparisons as part of literature reviews.Each participant will first subjectively assess the time they spent on manually creating one of their existing comparisons, reflecting state-of-the-art research in a specific domain.They will then recreate the same comparison using ExtracTable, without reviewing the original version to avoid bias.We used a within-subject design to assess usability and enable meaningful self-comparison across diverse domain experts.</p>
<p>The developed framework ExtracTable is the independent variable, while effectiveness, efficiency, and satisfaction are the dependent variables for usability.Effectiveness is measured by participants' perception and objectively by the number of identified papers, along with the accuracy of extracted data compared to the established comparison.Efficiency is assessed through participants' perception and objectively by the time taken to create the comparison.Satisfaction is evaluated subjectively after each stage and overall using the System Usability Scale (SUS) [25] at the experiment's conclusion.</p>
<p>With particpants' consent, both screen and audio were recorded during the sessions to enable detailed post-analysis.The experiment is described in a document [16] provided to participants at the start, outlining the study context and presenting title and description of their most recent comparison.ExtracTable runs locally on a computer provided by the experimenter.Sessions are conducted either in person or remotely via TeamViewer, ensuring a consistent experience.A questionnaire collects demographic data, ORKG usage frequency, and consent.Participants rate their agreement with statements regarding the tools' usefulness and user experience using a Likert scale.The SUS is administered at the end of each session, alongside open-ended questions for additional qualitative feedback.</p>
<p>The target group consists of individuals familiar with the ORKG who have created at least one comparison.Participants were recruited through an open call and convenience sampling, with participation being voluntary.Each session lasts approximately 60 minutes and follows a structured procedure:</p>
<ol>
<li>Introduction to the workflow, questionnaire, and experiment document.2. 5-10 minutes for participants to read the provided document.3. Completion of initial questionnaire pages, collecting demographic information and baseline data.4. Participants proceed through the two stages of ExtracTable, responding to related questionnaire items.5. Completion of the SUS and provision of general feedback.</li>
</ol>
<p>Results.The raw data [16] is published on Zenodo.Nine participants took part, including external curators and internal ORKG team members not involved in the tool's development, mostly from Semantic Web or neuro-symbolic AI fields.</p>
<p>Participants reported varied ORKG usage frequencies, ranging from weekly to monthly, with experience spanning from a few months to several years, reflecting different levels of expertise with the system.Figure 6 presents the participants' responses regarding subjective efficiency and effectiveness, showing strong agreement that the tool makes corpus creation and knowledge extraction 'faster' and 'easier', and a weaker agreement to its ability to render 'better results'.Compared to doing it without this tool, extracting and validating knowledge ...</p>
<p>Strongly disagree Disagree</p>
<p>Neither agree nor disagree Agree Strongly agree Fig. 6.Participants' agreement to statements for Stage 1 (above) and 2 (below).</p>
<p>Figure 7 represents the resulting SUS scores.We achieve an overall SUS score of 84.17 (min = 75, max = 90, σ = 4.86), regarded as A+ score -the highest usability rating [25].Participants expressed general satisfaction with ExtracTable, with comments like "The workflow should be integrated to ORKG as soon as possible.I will like to use it" and "Very helpful system!!!".Specific feedback highlighted features like the "Keywords extraction option is an amazing idea".Suggestions for improvement included adding "Properties should have descriptions (for more accurate extraction)" and "focus could be on extracting entities that align with the similar papers already available in the ORKG".Fig. 7. SUS scores and boxplot distribution, scoring an average of 84.17 (A+ according to the Lewis and Sauro benchmarks [25]).</p>
<p>The reported durations for the participants' original comparison varied significantly, with the fastest comparison taking approximately 4 hours to complete, while the longest required around 2 weeks.We acknowledge that these self-reported durations are prone to imprecision but still provide an indicative measure of the time involved.Completing the workflow with our framework -from defining a research interest based on their original comparison to successfully creating a new ORKG comparison -took an average of 1,480 seconds (24:40 minutes), with a minimum of 1,023 seconds (17:03 minutes) and a maximum of 1,996 seconds (33:16 minutes), as detailed in Figure 8.We also assess the tool's ability to replicate the original comparison by evaluating the number of publications it identifies against those included in the original set.The newly generated comparisons contained an average of 5 papers (min = 1, max = 8, σ = 2.2), whereas the original comparisons had an average of 11 papers.Overall, 98% of the newly created comparisons differed from the originals, with only 2% showing overlapping results.Since the majority of the retrieved papers did not match the original set, it was difficult to objectively assess the quality of the information generated by the tool.</p>
<p>QIP.The QIP methodology showed that curators are a valuable user group for guiding tool development.To improve CSV-based integration of literature tables, enhanced semantification is currently being developed to better align entities within the ORKG [17].</p>
<p>Threats to Validity.Based on Wohlin et al. [42], we identify potential threats to the validity of our experiment.Conclusion Validity.Participants may subconsciously replicate elements from their original ORKG comparison.To reduce this risk, we did not inform them that the task was based on their previous work.Participants came from diverse research fields but worked on their own prior content, minimizing the impact of disciplinary differences while supporting broader generalizability.Internal Validity.Participants may have been predisposed to emphasize the tool's benefits in comparison to doing it without the tool, particularly ease and speed.As part of the participants were ORKG curators, their financial affiliation could have introduced a bias toward favorable feedback, given the tool's relevance to their ongoing work.Additionally, self-reported durations for prior comparison creation, collected in unobserved settings, are prone to imprecision, adding uncertainty when comparing to the objectively timed toolbased workflow.Construct Validity.To capture genuine reactions and difficulties, we encouraged participants to comment on their experience during each task.This ensured feedback reflected their actual interactions with the tool.External Validity.With only nine participants, generalizability is limited.Nonetheless, the range of disciplinary backgrounds offers insight into the tool's broader applica-bility.Future evaluations should involve more diverse and numerous participants to strengthen external relevance.</p>
<p>Discussion and Future Work</p>
<p>Experiment.We interpret the mean SUS score using Bangor et al.'s [6] adjective scale, where ExtracTable's score of 84.17 ranks at the upper boundary of "Good", and an A+ in the Lewis and Sauro's benchmarks [25], indicating a positive user experience.Participants described ExtracTable as intuitive, easy to navigate, and effective for task execution.ExtracTable significantly reduced the time needed to create content in the ORKG compared to participants' prior methods.While this shows strong potential for streamlining workflows, it does not automatically ensure higher-quality outputs.Participants appreciated the simplified corpus creation and extraction process but expressed concerns about accuracy and over-reliance on automated suggestions.Those who invested more time refining the corpus reported better outcomes, suggesting deeper engagement improves quality.Still, not all agreed the tool consistently produced better results, exposing a gap between automation and human expectations.This disparity points to a key challenge in HITL systems: balancing automation with human judgment to ensure both efficiency and high-quality, contextually relevant results.Addressing this requires enhancing the extraction process and incorporating iterative feedback loops to refine accuracy.Although the experiment aimed to replicate a prior comparison, measuring replication quality through the number of identified papers proved unfeasible as an indicator of replication quality, underscoring the challenge of replicating structured research output in a HITL system.Unlike automated systems, our approach relies on user insights and interpretations, making the process subjective and context-dependent.</p>
<p>Requirements.ExtracTable satisfies most core requirements defined for the HITL framework.It guides users from defining a research interest to extracting and editing data, with CSV export for seamless integration into scholarly KGs like the ORKG.The modular pipeline supports extensibility, adaptability, and long-term semantic preservation.However, key limitations remain.Entity linking (FR12)</p>
<p>is not yet implemented, leaving extracted entities disconnected from existing KG resources.Incorporating entity alignment and exporting in formats like JSON-LD or RDF would improve semantic interoperability.Additionally, performance degrades with large datasets (NR4), as processing occurs sequentially per document.Future work should focus on performance optimization and parallelization strategies.Participants also requested more control over the data model (FR8).Allowing users to define domain-specific properties and extraction instructions would improve precision and adaptability across fields.This aligns with feedback on the need for more customizable and explainable outputs.Crucially, while automation accelerates literature processing, LLM-based extractions still require human validation (FR10, FR11).Without critical oversight, there's a risk of misinterpreting or blindly trusting generated results.This underscores the eth-ical implications of AI-assisted research, especially risking the loss of genuine understanding and knowledge internalization.</p>
<p>Future Work.Future development should allow storing and refining outputs with customizable data models, enabling users to iteratively improve extraction accuracy.Users can selectively re-run extractions on specific cells while preserving high-quality results, enhancing the HITL process.Semantic structuring, like linking content to knowledge graph entities and supporting interoperable formats, will strengthen the system's utility.Ongoing improvements in scalability and model configurability will stabilize and enrich the tool.Lastly, research should focus on user guidance for validating and interpreting extractions, e.g., by highlighting source passages in PDFs, ensuring human oversight in AI-driven workflows.</p>
<p>Conclusion</p>
<p>Our work contributes ExtracTable, a Human-in-the-Loop (HITL) approach for structuring scientific content, demonstrated in the context of the ORKG to showcase its suitability for downstream knowledge graph integration.Aligned with the Quality Improvement (QIP) paradigm, ExtracTable comprises a two-stage workflow -Creating a scientific corpus of literature and Knowledge extraction and validation -implemented as a modular framework in a Jupyter notebook environment.This framework assists researchers in identifying relevant publications, extracting key information, and structuring knowledge into reusable formats ready for integration into scientific knowledge graphs.</p>
<p>By automating repetitive tasks and enabling researchers to interactively refine outputs, ExtracTable reduces the manual effort typically involved in processing large volumes of literature.It moves toward making large-scale literature reviews more efficient and helps researchers keep pace with the rapidly expanding body of scientific knowledge.While evaluation results showed the tool is highly usable and accelerates the process, we identified areas for improvement.Additional support is needed to engage users in the knowledge extraction process, alongside feedback loops to further enhance the quality of extracted information.</p>
<p>Fig. 1 .
1
Fig. 1.Application of the Quality Improvement Paradigm (QIP) methodology to develop ExtracTable, together with ORKG curators.Derived from [28].</p>
<p>Stage 2 :Fig. 2 .
22
Fig. 2. Activity diagram for Stage 1: Creating a scientific corpus of literature, demonstrating user-machine interaction in the scientific literature corpus building process.</p>
<p>Fig. 3 .
3
Fig. 3. Activity diagram for Stage 2: Knowledge extraction and validation, showing the tasks executed by both the user and machine to extract knowledge from the corpus.</p>
<p>Fig. 4 .
4
Fig. 4. Visualization of query results, where each paper is presented within an expandable accordion, providing access to detailed information relevant to the paper.</p>
<p>Fig. 5 .
5
Fig. 5. Visualization of LLM-generated information.If a property is not found in the text, the tool renders a label "NOT FOUND" and displays it in red (FR10).</p>
<p>Fig. 8 .
8
Fig. 8. Visualization of the time spent by participants across two workflow stages.</p>
<p>Table 2 :
2
Functional (FR) and non-functional (NR) requirements
Functional Requirements (FR)FulfillmentFR1: The framework should suggest relevant keywords based on a user-defined research interest.YesFR2: The framework must allow a user to manually input and modify a search query to retrieve relevant literature.YesFR3: The framework must support integration with external litera-ture databases.YesFR4: The framework must provide options to configure search pa-rameters.YesFR5: The framework should present retrieved search results in an interactive format.YesFR6: The framework must enable a user to select and download relevant papers to create a literature corpus.YesFR7: The framework must allow a user to use an existing corpus to proceed with knowledge extraction.Yes
NR4: The framework should be capable of handling literature corpora of varying sizes without significant loss in performance.No NR5: The framework should be well-documented, follow clean code principles, and be openly available to support long-term maintenance, reproducibility, and community contribution.</p>
<p>Compared to doing it without this tool, creating and refining a scientific coropus of literature ...
Strongly disagreeDisagreeNeither agree nor disagreeAgreeStrongly agreewas easier was faster had better results12 1 11357429876543210123456789was easier had better results was faster13235769876543210123456789
AcknowledgementsThe authors would like to thank the Federal Government and the Heads of Government of the Länder, as well as the Joint Science Conference (GWK), for their funding and support within the framework of the NFDI4Ing and NFDI4DataScience consortia.This work was partially funded by the German Research Foundation (DFG) -project number 442146713 and project number 460234259.
Harnessing the Power of ChatGPT for Automating Systematic Review Process: Methodology, Case Study, Limitations, and Future Directions. A Alshami, M Elsayed, E Ali, 10.3390/systems11070351Systems. 1173512023</p>
<p>The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge. S Auer, D A Barone, C Bartz, E G Cortes, M Y Jaradeh, O Karras, M Koubarakis, D Mouromtsev, D Pliukhin, D Radyush, Scientific Reports. 13172402023</p>
<p>Open Research Knowledge Graph: A Large-Scale Neuro-Symbolic Knowledge Organization System. S Auer, Handbook on Neurosymbolic AI and Knowledge Graphs. IOS Press2025</p>
<p>DBpedia: A Nucleus for a Web of Open Data. S Auer, C Bizer, G Kobilarov, 10.1007/978-3-540-76298-0_52The Semantic Web. Berlin, HeidelbergSpringer2007</p>
<p>System for Semi-Automated Literature Review Based on Machine Learning. F Bacinger, I Boticki, D Mlinaric, 10.3390/electronics11244124Electronics. 112441242022</p>
<p>Determining what individual SUS scores mean: adding an adjective rating scale. A Bangor, P Kortum, J Miller, J. Usability Studies. 43May 2009</p>
<p>Experience Factory. V R Basili, G Caldiera, H D Rombach, 10.1002/0471028959.sof110Encyclopedia of Software Engineering. LtdJohn Wiley &amp; Sons2002</p>
<p>Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement. B Dalvi Mishra, O Tafjord, P Clark, 10.18653/v1/2022.emnlp-main.644Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec 2022</p>
<p>Quality and Effectiveness of AI Tools for Students and Researchers for Scientific Literature Review and Analysis. M Danler, W O Hackl, S B Neururer, B Pfeifer, 10.3233/SHTI2400382024. 2024IOS Press</p>
<p>Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain. D Dessì, F Osborne, D Reforgiato Recupero, 10.1016/j.future.2020.10.026Future Generation Computer Systems. 116Mar 2021</p>
<p>SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain. Knowledge-Based Systems. D Dessí, F Osborne, D Reforgiato Recupero, 10.1016/j.knosys.2022.109945Dec 2022258109945</p>
<p>Construction of Knowledge Graphs: Current State and Challenges. M Hofer, D Obraczka, A Saeedi, 10.3390/info15080509Information. 1585092024</p>
<p>Knowledge Graph Curation: A Practical Framework. E Huaman, D Fensel, 10.1145/3502223.3502247Proceedings of the 10th International Joint Conference on Knowledge Graphs. the 10th International Joint Conference on Knowledge GraphsNew York, NY, USAAssociation for Computing MachineryJan 202221</p>
<p>DRKG -Drug Repurposing Knowledge Graph for Covid-19. V N Ioannidis, X Song, S Manchanda, 2020</p>
<p>Domain-Specific Knowledge Graph Construction for Semantic Analysis. N Jain, 10.1007/978-3-030-62327-2_40The Semantic Web: ESWC 2020 Satellite Events. Springer International Publishing2020</p>
<p>Human-in-the-loop workflow for neuro-symbolic scholarly knowledge organization -experiment material. L John, data set</p>
<p>. Zenodo, 10.5281/zenodo.152283372025</p>
<p>SciMantify -A Hybrid Approach for the Evolving Semantification of Scientific Knowledge. L John, K E Farfar, S Auer, O Karras, 10.15488/1900625th International Conference on Web Engineering (ICWE). 20252025accepted at ICWE 2025. Proceedings to appear</p>
<p>KG-EmpiRE: A Community-Maintainable Knowledge Graph for a Sustainable Literature Review on the State and Evolution of Empirical Research in Requirements Engineering. O Karras, 10.1109/RE59067.2024.000632024 IEEE 32nd International Requirements Engineering Conference (RE). 2024</p>
<p>Research Knowledge Graphs for Sustainable Literature Reviews in Software Engineering and Beyond. O Karras, Software Engineering 2025-Companion Proceedings. BonnGesellschaft für Informatik2025</p>
<p>Organizing Scientific Knowledge from Engineering Sciences Using the Open Research Knowledge Graph: The Tailored Forming Process Chain Use Case. O Karras, L Budde, P Merkel, J Hermsdorf, M Stonis, L Overmeyer, B A Behrens, S Auer, 10.5334/dsj-2024-052Data Science Journal. Nov 2024</p>
<p>Divide and Conquer the Em-piRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering. O Karras, F Wernlein, J Klünder, S Auer, 10.1109/ESEM56168.2023.103047952023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM). Oct 2023</p>
<p>Explainable Prediction of Medical Codes through Automated Knowledge Graph Curation Framework. M Khalid, H A Khattak, A Ahmad, 10.1109/IBCAST54850.2022.99905512022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST). Aug 2022</p>
<p>From human experts to machines: An LLM supported approach to ontology and knowledge graph construction. V K Kommineni, B König-Ries, S Samuel, 10.48550/arXiv.2403.08345arXiv:2403.08345Mar 2024</p>
<p>Understanding the Elephant: The Discourse Approach to Boundary Identification and Corpus Construction for Theory Review Articles. K R Larsen, D Hovorka, A Dennis, J West, 10.17705/1jais.00556Journal of the Association for Information Systems. 207Jul 2019</p>
<p>Item Benchmarks for the System. J R Lewis, J Sauro, Journal of Usability Studies. 1332018</p>
<p>Human-in-theloop machine learning: a state of the art. E Mosqueira-Rey, E Hernández-Pereira, D Alonso-Ríos, 10.1007/s10462-022-10246-wArtificial Intelligence Review. 564Apr 2023</p>
<p>Comparing research contributions in a scholarly knowledge graph. A Oelen, M Y Jaradeh, K E Farfar, M Stocker, S Auer, 10.15488/93882019International Confrence on Knowledge Capture</p>
<p>Software Quality Assurance by means of Methodologies, Measurements and Knowledge Management Issues. thesis. K M Oliveira, Cambrésis. Jun 2014Université de Valenciennes et du Hainaut</p>
<p>Hallucinations in LLMs: Understanding and Addressing Challenges. G Perković, A Drobnjak, I Botički, 10.1109/MIPRO60963.2024.105692382024 47th MIPRO ICT and Electronics Convention (MIPRO). 2024</p>
<p>P21 A Comparative Analysis of Large Language Models (LLM) Utilised in Systematic Literature Review. H Rathi, A Malik, D C Behera, G Kamboj, 10.1016/j.jval.2023.09.030Value in Health. 2612S6Dec 2023</p>
<p>Human-machine Collaborative Decision-making: An Evolutionary Roadmap Based on Cognitive Intelligence. M Ren, N Chen, H Qiu, 10.1007/s12369-023-01020-1International Journal of Social Robotics. 157Jul 2023</p>
<p>Falcon 2.0: An Entity and Relation Linking Tool over Wikidata. A Sakor, K Singh, A Patel, M E Vidal, 10.1145/3340531.3412777Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge ManagementNew York, NY, USAAssociation for Computing MachineryOct 2020CIKM '20</p>
<p>Workflow for Domain-and Task-Sensitive Curation of Knowledge Graphs, with Use Case of DRKG. K Schatz, D Korn, A Tropsha, R Chirkova, 10.1109/BigData55660.2022.100205362022 IEEE International Conference on Big Data (Big Data). Dec 2022</p>
<p>Literature review as a research methodology: An overview and guidelines. H Snyder, 10.1016/j.jbusres.2019.07.039Journal of Business Research. 104Nov 2019</p>
<p>Enhancing Scientific Knowledge Graph Generation Pipelines with LLMs and Human-in-the-Loop. S Tsaneva, D Dessì, F Osborne, M Sabou, 2024Baltimore</p>
<p>Unveiling Scholarly Communities over Knowledge Graphs. S Vahdati, G Palma, R J Nath, 10.1007/978-3-030-00066-0_9Digital Libraries for Open Knowledge. ChamSpringer International Publishing2018</p>
<p>Wikidata: a free collaborative knowledgebase. D Vrandečić, M Krötzsch, 10.1145/2629489Commun. ACM. 5710Sep 2014</p>
<p>The semantic scholar academic graph (s2ag). A D Wade, Companion Proceedings of the Web Conference. 20222022251597885</p>
<p>Artificial intelligence and the conduct of literature reviews. G Wagner, R Lukyanenko, G Paré, 10.1177/02683962211048201202237</p>
<p>The FAIR Guiding Principles for scientific data management and stewardship. M D Wilkinson, M Dumontier, I J Aalbersberg, 10.1038/sdata.2016.18Scientific Data. 31160018Mar 2016</p>
<p>SWARM-SLR -Streamlined Workflow Automation for Machine-Actionable Systematic Literature Reviews. T Wittenborg, O Karras, S Auer, 10.1007/978-3-031-72437-4_22024Springer Nature SwitzerlandChamLinking Theory and Practice of Digital Libraries</p>
<p>Experimentation in Software Engineering. C Wohlin, P Runeson, M Höst, May 2012Springer Publishing CompanyIncorporated</p>
<p>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis. A Ye, A Maiti, M Schmidt, S J Pedersen, 10.3390/fi16050167Future Internet. 165167May 2024Multidisciplinary Digital Publishing Institute</p>            </div>
        </div>

    </div>
</body>
</html>
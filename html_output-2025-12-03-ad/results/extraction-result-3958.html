<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3958 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3958</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3958</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-42d83576ee920c1b6df318e212047d9ba57fc4fd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/42d83576ee920c1b6df318e212047d9ba57fc4fd" target="_blank">BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Language Resources and Evaluation</p>
                <p><strong>Paper TL;DR:</strong> This work proposes BAMBOO, a multi-task long context benchmark, designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels, to comprehensively evaluate the long context ability of LLMs.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e., question answering, hallucination detection, text sorting, language modeling, and code completion, to cover various domains and core capacities of LLMs. We conduct experiments with five widely-used long-context models and further discuss five key questions for long text research. In the end, we discuss problems of current long-context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://anonymous.4open.science/r/BAMBOO/.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3958.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3958.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMBOO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task long-context benchmark introduced in this paper that evaluates LLM long-text capabilities across five tasks and two length levels (4k and 16k tokens), designed to avoid data contamination and enable accurate automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Comprehensive capacity evaluation (language generation, knowledge utilization, reasoning, tool manipulation), avoidance of data contamination, accurate automatic evaluation, and evaluation across different input length levels (4k vs 16k).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic evaluation using precisely defined tasks (multi-choice transformations where applicable), task-specific metrics (accuracy, pass@1, concordance index, precision/recall/F1), controlled dataset construction to reduce data contamination, and experimental analyses (e.g., ablations: evidence-only vs full text, instruction-position experiments, context-compression experiments, attention-map analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>BAMBOO: 10 datasets from 5 tasks (Question Answering: PaperQA, MeetingQA, AltQA; Hallucination Detection: SenHallu, AbsHallu; Text Sorting: ShowsSort, ReportSumSort; Language Modeling: ShowsPred, MeetingPred; Code Completion: PrivateEval). Each dataset has 4k and 16k length subsets to measure length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy (QA, LM), pass@1 (code completion), concordance index (sorting), precision/recall/F1 (hallucination detection). All free-form outputs judged incorrect if format is wrong. Random baselines are reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators were used to construct multi-choice QA (rephrasing questions/options), manual modifications to some dataset answers to avoid contamination, manual alteration of API docs for PrivateEval, and human design of some dataset splits. ChatGPT was used to generate hallucinated hypotheses for hallucination datasets (i.e., an LLM was used as a data-generation tool). Evaluation itself is automated using the chosen metrics; no human scoring panel for final evaluation is described.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Data contamination risk (mitigated by using 2023-released sources but not completely avoidable), known inaccuracy of traditional automatic metrics for free-form generation (BLEU criticized), format errors (models produce valid answers with differing formats that the automatic scorer marks incorrect), instruction forgetting when instructions are placed early in very long inputs, 'extension tax' where extending context windows harms short-context performance for some models, U-shaped attention (evidence at ends favored), poor performance on uncommon tasks (sorting, code manipulation), and instability when input length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across datasets and lengths ChatGPT-16k achieved the best and most consistent performance; other evaluated long-context LLMs often underperformed and sometimes fell below random baselines on uncommon/complex tasks. Performance generally decreased when input length increased; retrieval-based context compression can recover or improve performance in some settings whereas truncation and summarization often hurt. Attention analysis shows tokens at the two ends receive higher attention (U-shaped), explaining 'lost in the middle' effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3958.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3958.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Principles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Design Principles for Long-Text LLM Evaluation in BAMBOO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four design principles defined by the authors to guide reliable evaluation of LLMs on long texts: (1) comprehensive capacity evaluation, (2) avoidance of data contamination, (3) accurate automatic evaluation, and (4) different length levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>1) Cover multiple core capacities (generation, reasoning, knowledge use, tool manipulation); 2) Minimize overlap with models' pretraining data; 3) Use tasks and task formulations that allow accurate automatic scoring; 4) Provide datasets at multiple input-length levels to measure length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Select tasks that can be accurately scored automatically (convert generation tasks to multi-choice where possible), restrict dataset sources to recent releases to reduce contamination, modify known answers where necessary, and provide two length buckets (<=4k and up to 16k) for all tasks. Perform controlled experiments (evidence-only vs full text, instruction position variants, compression techniques) to probe behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the BAMBOO suite; principles are used to design PaperQA, MeetingQA, AltQA, SenHallu, AbsHallu, ShowsSort, ReportSumSort, ShowsPred, MeetingPred, PrivateEval.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Same as BAMBOO: accuracy, pass@1, concordance index, precision/recall/F1; also report random baselines for context.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators for question/option rephrasing and dataset curation; manual interventions to remove contamination; no crowdsourced scoring of model outputs reported—evaluation is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot guarantee complete absence of pretraining overlap for some opaque models, transforming generation tasks to multi-choice can alter task nature, automatic scoring still penalizes format differences, and some evaluation choices (e.g., excluding images/tables) may affect realism.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using these principles yielded a benchmark where evaluation is automated and (according to authors) more accurate/reliable than prior long-context benchmarks; highlighted systemic model failures (instruction forgetting, format errors, poor performance on uncommon tasks) that were revealed through principled evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3958.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3958.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context-Compression & Ablations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-Compression Methods and Ablation Experiments used to evaluate long-input handling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of experimental methods in the paper to probe whether models struggle from length per se or model capacity: retrieval-augmentation, truncation, summarization, evidence-only inputs, instruction-position ablations, and attention-map analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability to answer tasks correctly when only concise evidence is provided vs full long text; impact of instruction position on correctness; whether compression techniques recover performance lost at long lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Three compression techniques applied to short-context LLMs on 16k tasks: (1) Truncation (remove initial 3000 tokens), (2) Retrieval (chunk into 256-token segments, embed with text-embedding-ada-002, retrieve top-10 relevant segments), (3) Summarization (chunk into 1024-token pieces, summarize each, concatenate summaries). Additional ablations: input only evidence vs full text; place instructions Pre-Ins, Post-Ins, Both-Ins; relocate evidence to ends vs original order. Attention analysis: compute average attention per head/layer, chunk sequence into 32-token blocks, curve fit to show U-shaped attention.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to BAMBOO subsets (e.g., PaperQA, MeetingQA, AltQA, SenHallu, AbsHallu) and models (ChatGPT-16k, Vicuna-16k, others).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Task metrics (accuracy, F1) reported pre- and post-compression/ablation; comparative deltas used to assess improvements or degradations (examples in Table 6 show retrieval often improves results whereas truncation/summarization sometimes degrade).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-made annotations used to identify evidence for evidence-only experiments; dataset curation by humans. Retrieval embeddings used OpenAI embeddings (automated). No human adjudication of these ablation results beyond dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Compression methods depend on retrieval quality and embedding model; summarization may lose critical details; evidence-mislocation can cause errors; ablations show mixed effects across models (e.g., Vicuna sometimes degrades when only evidence provided). Attention analysis explains end biases but does not by itself provide a fix.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Retrieval-augmented inputs frequently matched or exceeded long-context LLM performance; truncation and summarization often underperformed due to lost evidence. Instruction position strongly affected performance (Post-Ins often helps; Pre-Ins can cause instruction forgetting on long inputs). Attention maps show U-shaped token importance concentrated at sequence ends, explaining improved performance when evidence is placed at ends.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3958.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3958.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metrics & Limitations Summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported Evaluation Metrics, Human Roles, and Identified Limitations in BAMBOO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete metrics used, the role of humans in dataset construction and evaluation, and key limitations the authors identify when evaluating LLM outputs on long text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (accuracy/pass@1), concordance (sorting), detection capability (precision/recall/F1 for hallucination detection), robustness to formatting, sensitivity to input length and instruction placement, and tool-manipulation capability (code completion pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic scoring using exact-match/choice selection or task-specific metrics; treating any wrong format as incorrect; reporting random baselines; performing controlled experiments (evidence-only, instruction placement, compression) to probe failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Same BAMBOO datasets; prior benchmarks are discussed (ZeroSCROLLS, L-Eval, LongBench, SCROLLS) as context for improvements in BAMBOO.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy (%), pass@1 (%), concordance index (Cl), precision/recall/F1 (%), and random baseline comparisons. Aggregate model comparisons across tasks and length levels (4k vs 16k) are reported in tabular form (e.g., Table 3, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators created/curated QA and other datasets, rephrased options to avoid direct copying from sources, manually altered API keywords for PrivateEval. No human scoring of model outputs is reported; evaluation is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Automated metrics penalize format variants, pretraining data contamination cannot be fully excluded for opaque models, existing automatic NLG metrics (e.g. BLEU) are criticized as inaccurate for generation tasks, models show instruction forgetting and format errors, and open models exhibit poor generalization to uncommon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Quantitative reporting shows ChatGPT-16k leading across most tasks; many other long-context models underperform and sometimes fall below random baselines on uncommon or complex tasks. Compression/retrieval methods can mitigate length-induced failures in several tasks; however, overall model capability, not just input length, limits performance on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Zeroscrolls: A zero-shot benchmark for long text understanding <em>(Rating: 2)</em></li>
                <li>L-eval: Instituting standardized evaluation for long context language models <em>(Rating: 2)</em></li>
                <li>Longbench: A bilingual, multitask benchmark for long context understanding <em>(Rating: 2)</em></li>
                <li>Halueval: A large-scale hallucination evaluation benchmark for large language models <em>(Rating: 2)</em></li>
                <li>Lost in the middle: How language models use long contexts <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good NLG evaluator? A preliminary study <em>(Rating: 1)</em></li>
                <li>Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3958",
    "paper_id": "paper-42d83576ee920c1b6df318e212047d9ba57fc4fd",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "BAMBOO",
            "name_full": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
            "brief_description": "A multi-task long-context benchmark introduced in this paper that evaluates LLM long-text capabilities across five tasks and two length levels (4k and 16k tokens), designed to avoid data contamination and enable accurate automatic evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Comprehensive capacity evaluation (language generation, knowledge utilization, reasoning, tool manipulation), avoidance of data contamination, accurate automatic evaluation, and evaluation across different input length levels (4k vs 16k).",
            "evaluation_methods": "Automatic evaluation using precisely defined tasks (multi-choice transformations where applicable), task-specific metrics (accuracy, pass@1, concordance index, precision/recall/F1), controlled dataset construction to reduce data contamination, and experimental analyses (e.g., ablations: evidence-only vs full text, instruction-position experiments, context-compression experiments, attention-map analysis).",
            "benchmark_or_dataset": "BAMBOO: 10 datasets from 5 tasks (Question Answering: PaperQA, MeetingQA, AltQA; Hallucination Detection: SenHallu, AbsHallu; Text Sorting: ShowsSort, ReportSumSort; Language Modeling: ShowsPred, MeetingPred; Code Completion: PrivateEval). Each dataset has 4k and 16k length subsets to measure length effects.",
            "metrics_reported": "Accuracy (QA, LM), pass@1 (code completion), concordance index (sorting), precision/recall/F1 (hallucination detection). All free-form outputs judged incorrect if format is wrong. Random baselines are reported for comparison.",
            "human_involvement": "Human annotators were used to construct multi-choice QA (rephrasing questions/options), manual modifications to some dataset answers to avoid contamination, manual alteration of API docs for PrivateEval, and human design of some dataset splits. ChatGPT was used to generate hallucinated hypotheses for hallucination datasets (i.e., an LLM was used as a data-generation tool). Evaluation itself is automated using the chosen metrics; no human scoring panel for final evaluation is described.",
            "limitations_or_challenges": "Data contamination risk (mitigated by using 2023-released sources but not completely avoidable), known inaccuracy of traditional automatic metrics for free-form generation (BLEU criticized), format errors (models produce valid answers with differing formats that the automatic scorer marks incorrect), instruction forgetting when instructions are placed early in very long inputs, 'extension tax' where extending context windows harms short-context performance for some models, U-shaped attention (evidence at ends favored), poor performance on uncommon tasks (sorting, code manipulation), and instability when input length increases.",
            "llm_theory_example": null,
            "evaluation_results": "Across datasets and lengths ChatGPT-16k achieved the best and most consistent performance; other evaluated long-context LLMs often underperformed and sometimes fell below random baselines on uncommon/complex tasks. Performance generally decreased when input length increased; retrieval-based context compression can recover or improve performance in some settings whereas truncation and summarization often hurt. Attention analysis shows tokens at the two ends receive higher attention (U-shaped), explaining 'lost in the middle' effects.",
            "uuid": "e3958.0",
            "source_info": {
                "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Evaluation Principles",
            "name_full": "Design Principles for Long-Text LLM Evaluation in BAMBOO",
            "brief_description": "Four design principles defined by the authors to guide reliable evaluation of LLMs on long texts: (1) comprehensive capacity evaluation, (2) avoidance of data contamination, (3) accurate automatic evaluation, and (4) different length levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "1) Cover multiple core capacities (generation, reasoning, knowledge use, tool manipulation); 2) Minimize overlap with models' pretraining data; 3) Use tasks and task formulations that allow accurate automatic scoring; 4) Provide datasets at multiple input-length levels to measure length effects.",
            "evaluation_methods": "Select tasks that can be accurately scored automatically (convert generation tasks to multi-choice where possible), restrict dataset sources to recent releases to reduce contamination, modify known answers where necessary, and provide two length buckets (&lt;=4k and up to 16k) for all tasks. Perform controlled experiments (evidence-only vs full text, instruction position variants, compression techniques) to probe behavior.",
            "benchmark_or_dataset": "Applied to the BAMBOO suite; principles are used to design PaperQA, MeetingQA, AltQA, SenHallu, AbsHallu, ShowsSort, ReportSumSort, ShowsPred, MeetingPred, PrivateEval.",
            "metrics_reported": "Same as BAMBOO: accuracy, pass@1, concordance index, precision/recall/F1; also report random baselines for context.",
            "human_involvement": "Human annotators for question/option rephrasing and dataset curation; manual interventions to remove contamination; no crowdsourced scoring of model outputs reported—evaluation is automated.",
            "limitations_or_challenges": "Cannot guarantee complete absence of pretraining overlap for some opaque models, transforming generation tasks to multi-choice can alter task nature, automatic scoring still penalizes format differences, and some evaluation choices (e.g., excluding images/tables) may affect realism.",
            "llm_theory_example": null,
            "evaluation_results": "Using these principles yielded a benchmark where evaluation is automated and (according to authors) more accurate/reliable than prior long-context benchmarks; highlighted systemic model failures (instruction forgetting, format errors, poor performance on uncommon tasks) that were revealed through principled evaluation.",
            "uuid": "e3958.1",
            "source_info": {
                "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Context-Compression & Ablations",
            "name_full": "Context-Compression Methods and Ablation Experiments used to evaluate long-input handling",
            "brief_description": "A set of experimental methods in the paper to probe whether models struggle from length per se or model capacity: retrieval-augmentation, truncation, summarization, evidence-only inputs, instruction-position ablations, and attention-map analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Ability to answer tasks correctly when only concise evidence is provided vs full long text; impact of instruction position on correctness; whether compression techniques recover performance lost at long lengths.",
            "evaluation_methods": "Three compression techniques applied to short-context LLMs on 16k tasks: (1) Truncation (remove initial 3000 tokens), (2) Retrieval (chunk into 256-token segments, embed with text-embedding-ada-002, retrieve top-10 relevant segments), (3) Summarization (chunk into 1024-token pieces, summarize each, concatenate summaries). Additional ablations: input only evidence vs full text; place instructions Pre-Ins, Post-Ins, Both-Ins; relocate evidence to ends vs original order. Attention analysis: compute average attention per head/layer, chunk sequence into 32-token blocks, curve fit to show U-shaped attention.",
            "benchmark_or_dataset": "Applied to BAMBOO subsets (e.g., PaperQA, MeetingQA, AltQA, SenHallu, AbsHallu) and models (ChatGPT-16k, Vicuna-16k, others).",
            "metrics_reported": "Task metrics (accuracy, F1) reported pre- and post-compression/ablation; comparative deltas used to assess improvements or degradations (examples in Table 6 show retrieval often improves results whereas truncation/summarization sometimes degrade).",
            "human_involvement": "Human-made annotations used to identify evidence for evidence-only experiments; dataset curation by humans. Retrieval embeddings used OpenAI embeddings (automated). No human adjudication of these ablation results beyond dataset construction.",
            "limitations_or_challenges": "Compression methods depend on retrieval quality and embedding model; summarization may lose critical details; evidence-mislocation can cause errors; ablations show mixed effects across models (e.g., Vicuna sometimes degrades when only evidence provided). Attention analysis explains end biases but does not by itself provide a fix.",
            "llm_theory_example": null,
            "evaluation_results": "Retrieval-augmented inputs frequently matched or exceeded long-context LLM performance; truncation and summarization often underperformed due to lost evidence. Instruction position strongly affected performance (Post-Ins often helps; Pre-Ins can cause instruction forgetting on long inputs). Attention maps show U-shaped token importance concentrated at sequence ends, explaining improved performance when evidence is placed at ends.",
            "uuid": "e3958.2",
            "source_info": {
                "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Metrics & Limitations Summary",
            "name_full": "Reported Evaluation Metrics, Human Roles, and Identified Limitations in BAMBOO",
            "brief_description": "Concrete metrics used, the role of humans in dataset construction and evaluation, and key limitations the authors identify when evaluating LLM outputs on long text tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness (accuracy/pass@1), concordance (sorting), detection capability (precision/recall/F1 for hallucination detection), robustness to formatting, sensitivity to input length and instruction placement, and tool-manipulation capability (code completion pass@1).",
            "evaluation_methods": "Automatic scoring using exact-match/choice selection or task-specific metrics; treating any wrong format as incorrect; reporting random baselines; performing controlled experiments (evidence-only, instruction placement, compression) to probe failure modes.",
            "benchmark_or_dataset": "Same BAMBOO datasets; prior benchmarks are discussed (ZeroSCROLLS, L-Eval, LongBench, SCROLLS) as context for improvements in BAMBOO.",
            "metrics_reported": "Accuracy (%), pass@1 (%), concordance index (Cl), precision/recall/F1 (%), and random baseline comparisons. Aggregate model comparisons across tasks and length levels (4k vs 16k) are reported in tabular form (e.g., Table 3, Table 6).",
            "human_involvement": "Human annotators created/curated QA and other datasets, rephrased options to avoid direct copying from sources, manually altered API keywords for PrivateEval. No human scoring of model outputs is reported; evaluation is automated.",
            "limitations_or_challenges": "Automated metrics penalize format variants, pretraining data contamination cannot be fully excluded for opaque models, existing automatic NLG metrics (e.g. BLEU) are criticized as inaccurate for generation tasks, models show instruction forgetting and format errors, and open models exhibit poor generalization to uncommon tasks.",
            "llm_theory_example": null,
            "evaluation_results": "Quantitative reporting shows ChatGPT-16k leading across most tasks; many other long-context models underperform and sometimes fall below random baselines on uncommon or complex tasks. Compression/retrieval methods can mitigate length-induced failures in several tasks; however, overall model capability, not just input length, limits performance on many datasets.",
            "uuid": "e3958.3",
            "source_info": {
                "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Zeroscrolls: A zero-shot benchmark for long text understanding",
            "rating": 2
        },
        {
            "paper_title": "L-eval: Instituting standardized evaluation for long context language models",
            "rating": 2
        },
        {
            "paper_title": "Longbench: A bilingual, multitask benchmark for long context understanding",
            "rating": 2
        },
        {
            "paper_title": "Halueval: A large-scale hallucination evaluation benchmark for large language models",
            "rating": 2
        },
        {
            "paper_title": "Lost in the middle: How language models use long contexts",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good NLG evaluator? A preliminary study",
            "rating": 1
        },
        {
            "paper_title": "Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing",
            "rating": 1
        }
    ],
    "cost": 0.0128045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models</h1>
<p>Zican Dong ${ }^{1}$, Tianyi Tang ${ }^{1}$, Junyi Li ${ }^{1,2}$, Wayne Xin Zhao ${ }^{1 *}$, Ji-Rong Wen ${ }^{1,3}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{2}$ DIRO, Université de Montréal<br>${ }^{3}$ School of Information, Renmin University of China<br>{dongzican, steven_tang, lijunyi, jrwen}@ruc.edu.cn, batmanfly@gmail.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e., question answering, hallucination detection, text sorting, language modeling, and code completion, to cover various domains and core capacities of LLMs. We conduct experiments with five widely-used long-context models and further discuss five key questions for long text research. In the end, we discuss problems of current long-context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://github.com/RUCAIBox/BAMBOO.</p>
<p>Keywords: BAMBOO, Long Text, Large Language Models, Benchmark, Evaluation</p>
<h2>1. Introduction</h2>
<p>Recently, large language models (LLMs) have exhibited huge success across various NLP tasks (Ouyang et al., 2022; Touvron et al., 2023a; Zhao et al., 2023). Despite their remarkable capacities, existing LLMs still face constraints due to their limited context length, suffering from performance decline when the input text exceeds their length limit. To enhance the capability of LLMs to model long texts, several studies extend the context window through modifying position embeddings (kalokendev, 2023; Chen et al., 2023), and continuing to fine-tune the models on multi-turn conversations or long text datasets (Zheng et al., 2023). Moreover, numerous research also explores transforming the original long text into short segments within the context length based on context compression techniques (Zhou et al., 2023; Jiang et al., 2023).</p>
<p>To push forward the research of long context modeling, it is crucial to establish an automatic, reliable, and comprehensive evaluation benchmark for assessing the performance of LLMs in tasks involving lengthy text. In existing literature, several benchmarks have been proposed to evaluate the long context modeling ability of LLMs, such as ZeroSCROLLS (Shaham et al., 2023), L-Eval (An et al., 2023), and LongBench (Bai et al., 2023). However, a crucial aspect overlooked by these studies is the potential issue of data contamination (Golchin and Surdeanu, 2023), i.e., there might exist overlaps between the pre-training data and evaluation data.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In the meanwhile, these benchmarks consist of some text generation tasks evaluated by automatic metrics such as BLEU (Wang et al., 2023a). However, these metrics are blamed for their inaccurate evaluation of model performance.</p>
<p>To this end, we propose BAMBOO, a comprehensive benchmark to analyze the long text modeling capacities of LLMs. In BAMBOO benchmark, we manually construct ten datasets from five tasks, including question answering, hallucination detection, text sorting, language modeling, and code completion. It comprehensively evaluates language generation, knowledge utilization, reasoning, and tool manipulation abilities over long texts. We aim to ensure that the salient information is spread throughout the complete long texts. Consequently, the LLM needs to model long-range dependencies to identify both the coarse-grained information related to the global comprehension of the complete text and fine-grained information about details across several sentences. Moreover, with datasets from multiple domains, BAMBOO can evaluate the performance of LLMs across various sources.</p>
<p>BAMBOO is divided into two subsets at different length levels, i.e., BAMBOO-4k and BAMBOO-16k, according to the number of tokens in the prompt ${ }^{1}$. Each subset has 1502 samples with an average length of 3152 and 7500 tokens, respectively. To alleviate the data contamination with pre-training corpora, BAMBOO is constructed based on the data</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">ZeroSCROLLS</th>
<th style="text-align: center;">L-Eval</th>
<th style="text-align: center;">LongBench</th>
<th style="text-align: center;">BAMBOO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Comprehensive Capacity Evaluation</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Accurate Automatic Evaluation</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Avoidance of Data Contamination</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Different <br> Length Levels</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">#Tasks</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison with other long text evaluation benchmarks. #Tasks denotes the number of tasks in the benchmark.
sources released in 2023. Furthermore, we select diverse tasks with accurate automatic metrics for reliable evaluation. Table 1 presents a comparison of BAMBOO with existing benchmarks.</p>
<p>We extensively evaluate five long-context LLMs on the BAMBOO benchmark. We observe that ChatGPT-16k consistently demonstrates optimal performance across most datasets, whereas other models usually struggle, especially on uncommon tasks. We conduct on a series of analytical experiments to shed light on the key questions in long text modeling. Our findings are listed as follows:</p>
<ul>
<li>Extending the context window of LLMs is a double-edged sword, which is beneficial for medium-length texts but harmful for short texts.</li>
<li>Using evidence and properly compressing long texts can generally enhance the performances of LLMs. However, smaller LLMs still exhibit limited performance even given more concise input.</li>
<li>LLMs' performances on different datasets are sensitive to positions of instructions. Additionally, locating evidence on both ends is beneficial due to larger attention scores.</li>
</ul>
<p>Ultimately, we undertake a discussion of problems associated with instruction forgetting, format errors, reasoning capabilities, and uncommon tasks on long text modeling. We conclude by identifying future research directions for advancing the long text modeling capabilities of LLMs.</p>
<h2>2. Related Work</h2>
<p>Long-Context Architectures Owing to the quadratic memory and computational complexity of self-attention, original Transformer models encountered significant challenges in scaling to extended contexts. To enhance the efficiency of Transformers, researchers explored multiple innovations in the attention mechanism. These include recurrent Transformers (Dai et al., 2019; Rae et al., 2020), local attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), content-based sparse attention (Kitaev et al., 2020; Tay et al., 2020; Roy et al., 2021), and attention approximation (Wang et al., 2020; Ma
et al., 2021; Katharopoulos et al., 2020; Peng et al., 2021). In addition to the above Transformer variants, several models were introduced, replacing attention with alternative modules. S4 (Gu et al., 2022), DSS (Gupta et al., 2022), and GSS (Mehta et al., 2023) employed state space models, while RetNet (Sun et al., 2023a) made use of retention mechanism, achieving parallel training and recurrent inference for long sequences.</p>
<p>Adapting LLMs for Long Contexts Recently, with the booming of LLMs, there has been an increase in research efforts for overcoming the length limitations of off-the-shelf LLMs. Through modifying RoPE (Su et al., 2021) with position interpolation (kaiokendev, 2023; Chen et al., 2023), truncated basis (Pal et al., 2023), or modifying bases (Xiong et al., 2023; Peng et al., 2023b; Liu et al., 2023b; Rozière et al., 2023), the context windows of LLMs could be extended to 32 k or more tokens. Equipped with fusion-in-decoder (Izacard and Grave, 2021; Ratner et al., 2023) or external memory techniques (Bertsch et al., 2023; Tworkowski et al., 2023; Wang et al., 2023b), LLMs could access tokens from a distant past during the generation stage. In addition, context compression techniques, such as retrieval-augmentation and recurrence, could significantly expand their capacity to model longer contexts, where LLMs only need to process a short segment at once (Zhou et al., 2023; Liang et al., 2023; Packer et al., 2023; Xu et al., 2023; Jiang et al., 2023).</p>
<p>Evaluation for Long Text Modeling For evaluating long text modeling capabilities, diverse datasets were proposed, mainly classified into three task categories: language modeling, question answering, and summarization. The objective of language modeling was to sequentially predict the next tokens based on previous context (Rae et al., 2020). However, popular benchmarks did not adopt it since most tokens could be easily predicted within a limited context (Sun et al., 2021). Long text summarization datasets encompassed normal summarization (Cohan et al., 2018; Huang et al., 2021; Chen et al., 2022) and query-based summarization (Zhong et al., 2021), involving the condensation of either the entire or a portion of lengthy texts into concise summaries. As for question answering, given a question and a single document (Pang et al., 2022; Dasigi et al., 2021) or multiple documents (Yang et al., 2018), the model needed to provide the answer of the question.</p>
<p>To facilitate comprehensive evaluations of long text modeling abilities, several long text benchmarks were introduced. Scrolls (Shaham et al., 2022) collected and standardized several long text reasoning datasets spanning various tasks and do-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Input</th>
<th style="text-align: center;">#Example</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AltQA</td>
<td style="text-align: center;">3243/13084</td>
<td style="text-align: center;">200/200</td>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">https://github.com/abacusai/long-context</td>
</tr>
<tr>
<td style="text-align: center;">PaperQA</td>
<td style="text-align: center;">3101/6838</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;">Paper</td>
<td style="text-align: center;">https://aclanthology.org/</td>
</tr>
<tr>
<td style="text-align: center;">MeetingQA</td>
<td style="text-align: center;">2738/9838</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;">Meeting</td>
<td style="text-align: center;">https://record.assembly.wales/</td>
</tr>
<tr>
<td style="text-align: center;">SenHallu</td>
<td style="text-align: center;">3170/6357</td>
<td style="text-align: center;">200/200</td>
<td style="text-align: center;">P/R/F1</td>
<td style="text-align: center;">Paper</td>
<td style="text-align: center;">https://aclanthology.org/</td>
</tr>
<tr>
<td style="text-align: center;">AbsHallu</td>
<td style="text-align: center;">3314/6445</td>
<td style="text-align: center;">200/200</td>
<td style="text-align: center;">P/R/F1</td>
<td style="text-align: center;">Paper</td>
<td style="text-align: center;">https://aclanthology.org/</td>
</tr>
<tr>
<td style="text-align: center;">ShowsSort</td>
<td style="text-align: center;">2992/6411</td>
<td style="text-align: center;">200/200</td>
<td style="text-align: center;">Cl</td>
<td style="text-align: center;">TV Shows</td>
<td style="text-align: center;">https://tvmeg.com/</td>
</tr>
<tr>
<td style="text-align: center;">ReportSumSort</td>
<td style="text-align: center;">3753/8309</td>
<td style="text-align: center;">150/150</td>
<td style="text-align: center;">Cl</td>
<td style="text-align: center;">Reports</td>
<td style="text-align: center;">https://www.gao.gov/</td>
</tr>
<tr>
<td style="text-align: center;">ShowsPred</td>
<td style="text-align: center;">2389/4860</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;">TV Shows</td>
<td style="text-align: center;">https://tvmeg.com/</td>
</tr>
<tr>
<td style="text-align: center;">MeetingPred</td>
<td style="text-align: center;">3689/11578</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;">Meeting</td>
<td style="text-align: center;">https://record.assembly.wales/</td>
</tr>
<tr>
<td style="text-align: center;">PrivateEval</td>
<td style="text-align: center;">3149/6230</td>
<td style="text-align: center;">152/152</td>
<td style="text-align: center;">pass@1</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">https://github.com/microsoft/PyCodeGPT</td>
</tr>
</tbody>
</table>
<p>Table 2: Overview of our BAMBOO benchmark. #Input and #Example represent the average number of tokens in the input text and the number of examples of each dataset, respectively. For #Input and #Example, "number1/number2" is the number of the middle and long levels, respectively. P/R/F1 denotes precision/recall/F1, and Cl stands for concordance index.
mains into a unified format. In the wake of the heightened interest in LLMs, ZeroScrolls (Shaham et al., 2023), L-Eval (An et al., 2023), and LongBench (Bai et al., 2023) were released to assess the zero-shot long text modeling ability of LLMs. Nevertheless, they all encountered potential data contamination problems and suffered from inaccurate automatic metrics.</p>
<h2>3. BAMBOO Benchmark</h2>
<p>Our BAMBOO benchmark commits to equitably and thoroughly evaluating LLMs' long text modeling proficiency. It contains ten elaborately constructed datasets across a broad range of domains and evaluations. The overview of the BAMBOO benchmark is shown in Table 2.</p>
<h3>3.1. Design Principles</h3>
<p>Different from existing long text benchmarks in Table 1, our BAMBOO benchmark is meticulously designed to comprehensively evaluate the long text modeling capacity of LLMs. Our benchmark can potentially circumvent the issue of data contamination in previous benchmarks, pursue more accurate automatic evaluation, and cater to LLMs at different length levels.</p>
<p>Comprehensive Capacity Evaluation The primary objective of BAMBOO is comprehensively evaluating the abilities of LLMs in modeling long texts via different tasks and domains. To meet the real needs in long text scenarios, all the tasks assess the LLMs' ability to capture long-range dependencies. Inspired by Zhao et al. (2023), BAMBOO covers diverse tasks evaluating language generation, knowledge utilization, reasoning, and tool manipulation capacities over long texts. BAMBOO can also evaluate the coarse-grained comprehension of entire texts and the fine-grained reasoning
concerning specific details. Moreover, we collect data from various sources, assessing and comparing abilities across diverse domains.</p>
<p>Avoidance of Data Contamination In BAMBOO, we make a deliberate effort to minimize the overlap between test data and the training corpora of LLMs as much as possible. One of the key challenges when evaluating the long text capability of LLMs is the model has "seen" the input text during the training stages, i.e., data contamination (Golchin and Surdeanu, 2023). To alleviate this issue, we take a cautious approach by retaining data only released in 2023, since training data cutoff for gpt-3.5-turbo and Claude2 are up to September $2021^{2}$ and the early $2023^{3}$, respectively. Yet, since the specifics of their training data remain undisclosed for some models, we cannot guarantee the absence of any pre-training data used in our benchmark. In addition, we also utilize datasets with old data sources following Pal et al. (2023), but any appearances of the answers have been modified. Thus, the model must respond based solely on the input, reducing the potential effect of exposure to similar data.</p>
<p>Accurate Automatic Evaluation Each task in BAMBOO is designed for precise automatic evaluation. Previous benchmarks include text generation tasks (e.g., text summarization and question answering). The generations of LLMs may exhibit variations in expression compared to the references, making it challenging to accurately evaluate their outputs with automatic metrics (Tang et al., 2023), even using LLMs as evaluator (Wang et al., 2023a). Hence, we transform some generation tasks into multi-choice tasks and select tasks that can be precisely evaluated. All the tasks can be measured by</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>metrics such as accuracy, pass@1, etc.
Different Length Levels In BAMBOO, each task encompasses two distinct length levels. The most popular LLMs, e.g., Llama2 (Touvron et al., 2023b) and gpt-3.5-turbo ${ }^{\circ}$, only have a context window of 4 k tokens while some of them extend their context windows to 16 k or more tokens, including gpt-3.5-turbo-16k ${ }^{2}$ and ChatGLM2-6b-32k (Zeng et al., 2023). However, previous benchmarks did not thoroughly consider different length distributions of different datasets, making it difficult to analyze the impact of length changes on LLMs in the same task. Instead, we divide each task of BAMBOO into two subsets based on the input length, with the maximum length of 4 k and 16 k tokens, respectively. Thus, we can effectively measure the impact of lengths on the proficiency of LLMs.</p>
<h3>3.2. Data Collection</h3>
<p>To ensure domain diversity, we collect long texts from four distinct sources, namely NLP research papers, government reports, TV show transcripts, and committee meetings transcripts. All these data sources are publicly accessible and released in 2023 to avoid pre-training data contamination. We crawl and parse the source websites, extracting only the plain text while excluding images, tables, and footnotes. Texts containing fewer than 1000 tokens are excluded, and overly lengthy texts are truncated. In addition to these latest corpora, we also consider the pseudo long-context datasets where potential data contamination has attempted to be avoided.</p>
<p>In pursuit of the objective of encompassing diverse length levels, we divide samples within the BAMBOO benchmark into two subsets based on the length of the complete input text. One subset contains prompts up to 4 k tokens (BAMBOO-4k), while the other comprises prompts with tokens from 4 k to 16 k (BAMBOO-16k).</p>
<h3>3.3. Task Construction</h3>
<p>We aim to assess long text modeling ability in BAMBOO thoroughly. To attain the goal, we devise five distinct tasks and comprise ten corresponding datasets based on the fundamental principle.</p>
<p>Question Answering In the question answering task, we primarily focus on evaluating knowledge utilization and reasoning ability. With the assistance of human labelers, we manually construct two multi-choice question answering datasets, PaperQA and MeetingQA, to evaluate the LLMs' comprehension of a long document or dialogue. We ask annotators to rephrase the expressions of questions and options instead of directly copying the
original expressions in given texts. We also ensure that most of the answers need to be reasoned from evidence from multiple paragraphs. In addition, we utilize AltQA (Pal et al., 2023), comprising multiple Wikipedia documents. Each question requires a numerical answer, and every occurrence of the answer within the documents is modified to another number to avoid data contamination.</p>
<p>Hallucination Detection Hallucination is a prevalent phenomenon in LLMs, i.e., the generated content may conflict with or be beyond the source. In this task, models must utilize the knowledge of given contexts to detect hallucinations. We create two novel datasets, i.e., SenHallu and AbsHallu, using ChatGPT to generate hallucinations based on the correct hypothesis (e.g., a few words modification or a make-up sentence insertion) (Li et al., 2023b). Based on the concept of natural language inference tasks, we present LLMs with a paper and a hypothesis. Then, we request LLMs to determine whether the hypothesis entails or contradicts the paper as a binary classification task.</p>
<p>Text Sorting To evaluate model's capacity of reasoning over the logical order of texts, we devise two text sorting datasets that require LLMs to reorder the shuffled texts according to contextual information of the whole texts. ShowsSort is a sorting task involving shuffled segments of a TV show transcript. The original order is reconstructed based on contextual continuity and the inherent meaning of each segment. ReportSumSort is another sorting task with the input of a complete government report and a list of shuffled summaries. The objective is to reorder these summaries according to their positions within the segments represented in the report.</p>
<p>Language Modeling Language modeling task is a typical language generation task where the next token is sequentially predicted based on the past context. In order to evaluate the capacity to handle long-range dependencies, we design the task intending to predict the last utterance's speaker within a long dialogue. Specifically, we adopt multi-turn conversation as the source and format each utterance as "(utterance) said by {speaker}". Then, we remove the speaker of the last turn of conversation as the target. Successfully predicting the speaker necessitates the model's grasp of the characteristics of existing speakers and the contextual conversations. We construct ShowsPred and MeetingPred with transcripts of TV shows and meetings as sources, respectively.</p>
<p>Code Completion To assess the capability of LLMs to effectively utilize external tools through</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">ShowsSort</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ReportSumSort</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ShowsPred</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MeetingPred</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SenHallu</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo-16k</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">62.7/99.0/76.7</td>
<td style="text-align: center;">68.1/96.0/79.7</td>
</tr>
<tr>
<td style="text-align: center;">Claude2-100k</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">67.3/99.0/80.2</td>
<td style="text-align: center;">69.3/97.0/80.8</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM2-32k</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">52.9/100.0/69.2</td>
<td style="text-align: center;">52.7/96.0/68.1</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-v1.5-16k</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">56.4/92.9/70.2</td>
<td style="text-align: center;">67.7/99.0/69.3</td>
</tr>
<tr>
<td style="text-align: center;">Longchat-v1.5-16k</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">50.3/100.0/66.9</td>
<td style="text-align: center;">51.2/100.0/67.8</td>
</tr>
<tr>
<td style="text-align: center;">Random Baseline</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">50.0/50.0/50.0</td>
<td style="text-align: center;">50.0/50.0/50.0</td>
</tr>
<tr>
<td style="text-align: center;">Models</td>
<td style="text-align: center;">PrivateEval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AltQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaperQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MeetingQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AbsHallu</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo-16k</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">55.6/100.0/71.4</td>
<td style="text-align: center;">57.2/99.0/72.5</td>
</tr>
<tr>
<td style="text-align: center;">Claude2-100k</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">56.8/100.0/72.5</td>
<td style="text-align: center;">58.2/99.0/73.3</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM2-32k</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">50.5/100.0/66.7</td>
<td style="text-align: center;">50.0/99.0/66.4</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7b-v1.5-16k</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">51.0/99.0/67.3</td>
<td style="text-align: center;">53.3/88.0/66.4</td>
</tr>
<tr>
<td style="text-align: center;">Longchat-7b-v1.5-16k</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">50.3/100.0/66.9</td>
<td style="text-align: center;">50.0/100.0/66.7</td>
</tr>
<tr>
<td style="text-align: center;">Random Baseline</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">50.0/50.0/50.0</td>
<td style="text-align: center;">50.0/50.0/50.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results (\%) of selected long-context LLMs on our benchmark. We report both the performance on 4 k and 16 k versions of all datasets. For SenHallu and AbsHallu, we sequentially exhibit precision, recall, and F1.</p>
<p>API calls in tackling complex tasks, we construct the PrivateEval dataset on the basis of the benchmark from Zan et al. (2022). In the dataset, LLMs are presented with API documents derived from private libraries and a partial code snippet. Then, the model needs to identify key API documents to complete the code snippet. To build the dataset, we manually alter the keywords within API documents of the Torchdata library following previous methods (Zan et al., 2022) on Monkey and BeatNum libraries. Furthermore, we adjust the number of the provided documents for length control.</p>
<h3>3.4. Metrics</h3>
<p>As shown in Table 2, we employ four metrics to evaluate different tasks in our benchmark, i.e., accuracy for question answering and language modeling, concordance index (Shaham et al., 2023) for sorting, pass@1 (Chen et al., 2021) for code completion, and precision, recall, and F1 for hallucination detection. Notably, all the answers are generated freely, and any answer with a wrong format is treated as incorrect.</p>
<h3>3.5. Benchmark Usage</h3>
<p>The BAMBOO benchmark offers researchers a comprehensive platform to thoroughly evaluate the long text capabilities of LLMs. First, with multiple tasks covering the evaluation of different capacities, BAMBOO provides a direct comparison of different long-context LLMs' comprehensive capacities. Meanwhile, the BAMBOO benchmark enables a detailed analysis of LLMs' weaknesses in different aspects. Thus, more powerful long-context LLMs
can be built by making up for these deficiencies. Finally, the BAMBOO benchmark introduces varying length levels, enabling assessing the significance of the lengths of prompts and context windows. In total, BAMBOO can promote better handling of the multifaceted challenges and opportunities in the realm of long text modeling.</p>
<p>Users can utilize the BAMBOO benchmark with instructions provided in our project repository or instructions for your design. Then, users can evaluate the generations with our code.</p>
<h2>4. Experiment</h2>
<p>With the BAMBOO benchmark, we conduct experiments to evaluate the zero-shot long text modeling capability of long-context LLMs. Additionally, we further delve into key questions of long text modeling and discuss problems of long-context models.</p>
<h3>4.1. Baselines</h3>
<p>We select five instruction-tuned models with a context length of over 16 k tokens as our baselines. For closed models available via API, we select gpt-3.5-turbo-16k ${ }^{2}$ from OpenAI and Claude2-100K ${ }^{3}$ from Anthropic. For open-sourced models, we select Vicuna-7b-v1.5-16k (Zheng et al., 2023), Longchat-7b-v1.5-32k (Li et al., 2023a), and ChatGLM2-6b32k (Du et al., 2022; Zeng et al., 2023) ${ }^{4}$.
${ }^{4}$ We abbreviate gpt-3.5-turbo as ChatGPT, Vicuna-7b-v1.5 as Vicuna, Claude2-100k as Claude2, Longchat-7b-v1.5 as Longchat, and ChatGLM2-6b as ChatGLM2 in the following.</p>
<p>We add a random baseline for each task. For text sorting and language modeling tasks, we randomly return the permutated sequences and pre-existing speakers in the dialogues, respectively. For hallucination detection and multi-choice QA datasets, we randomly select one choice from (true, false) or (A, B, C, D). For PrivateEval and AltQA, we directly set the random baseline to 0 due to the infinite search space for answers.</p>
<h3>4.2. Overall Results</h3>
<p>The overall results for the model performance on the BAMBOO benchmark are displayed in Table 3. Across different datasets and length levels, ChatGPT-16k consistently exhibits excellent performance, surpassing other LLMs in nearly all tasks, except a minor performance gap observed in the hallucination detection task. Conversely, the other models' performances are often poor, even falling below performances of random baselines.</p>
<p>Moreover, models usually struggle with datasets with uncommon objectives and complex requirements, even the most powerful ChatGPT. In hallucination detection tasks where distinctions between the hallucinated and correct hypotheses are subtle, most LLMs can hardly capture those hallucinations, resulting in low precision. In addition, LLMs also underperform in ShowsSort and PrivateEval, highlighting their inability to aggregate, manipulate tools, and synthesize code. We infer that limited diversity of training data types plays a role in the subpar performance of LLMs.</p>
<p>Comparing performances across different length levels, a notable trend is that there is a discernible decrease in performance for most datasets and models as extending input length. Due to the incorrect generating format, the trend is more prominent for tasks with diverse outputs.</p>
<h3>4.3. Key Questions of Long Text Modeling</h3>
<p>To delve into the mechanism and influential factors affecting the long text modeling capacities of LLMs, our investigation centers on five key research questions. In this section, we conduct analytical experiments to address these questions ${ }^{5}$.</p>
<h2>RQ1: Do LLMs with Long Context Windows Pay for Extension Tax?</h2>
<p>We first discuss whether extending the LLMs' context windows and fine-tuning LLMs on long texts harm performance over tasks with shorter</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>texts, i.e., "extension tax". We compare ChatGPT16 k and Vicuna-16k to their short-context variants on selected tasks from BANBOO-4k and MMLU (Hendrycks et al., 2021).</p>
<p>As shown in Figure 1, ChatGPT and Vicuna exhibit disparate trends. Performances between ChatGPT and ChatGPT-16k over different tasks are almost identical. However, an "extension tax" is evident when replacing Vicuna with Vicuna-16k on MMLU. The longer training data and position interpolation detrimentally affect the ability of shortcontext tasks. Interestingly, Vicuna-16k demonstrates improved performance in BAMBOO-4k, suggesting that longer training data can enhance performance in medium-length tasks.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Results of long and short-context LLMs on BAMBOO-4k and MMLU.</p>
<h2>RQ2: To What Extent Do LLMs Struggle Due to The Long Input?</h2>
<p>In the long text scene, a critical question emerges: is the challenge associated with the length of the text or the inherent capacity to solve the task? In MeetingQA, PaperQA, and PrivateEval datasets, only concise evidence in the long text is sufficient to support the completion of tasks. Within these datasets, we make a comparison of the performance between inputting only evidence and the complete text.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">MeetingQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaperQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PrivateEval</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-16k</td>
<td style="text-align: center;">evidence <br> complete</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">31.6</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-16k</td>
<td style="text-align: center;">evidence <br> complete</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">5.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of LLMs' performances with the input of only evidence or the complete text.</p>
<p>Table 4 illustrates a general trend of improved performance across various datasets. Mislocating evidence within long text contributes to some errors. Nevertheless, the improvements are relatively minor, and Vicuna's performance even declines in</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Position</th>
<th style="text-align: center;">ShowsSort</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SenHallu</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AbsHallu</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MeeetingPred</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ShowsPred</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MeeetingQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaperQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">4 k</td>
<td style="text-align: center;">16k</td>
<td style="text-align: center;">4 k</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-16k</td>
<td style="text-align: center;">Pre-Ins</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Post-Ins</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Both-Ins</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-16K</td>
<td style="text-align: center;">Pre-Ins</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Post-Ins</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Both-Ins</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: center;">LongChat-32k</td>
<td style="text-align: center;">Pre-Ins</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Post-Ins</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Both-Ins</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM2-32k</td>
<td style="text-align: center;">Pre-Ins</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Post-Ins</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Both-Ins</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">67.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Results of LLMs with three different instruction positions relative to content.</p>
<p>PaperQA. Additionally, the model consistently commits the same errors in both settings. Therefore, we infer that the awful performances of LLMs are predominantly attributed to subpar reasoning and coding ability rather than the mislocalization of evidence in the long text.</p>
<h2>RQ3: How Do Instruction Positions Affect Long Text Modeling?</h2>
<p>A recent study observed that positioning the instruction at the end of the input can enhance generation performance (Liu et al., 2023c). Therefore, we explore the influence of instruction placement in long text tasks. Specifically, we segment the complete prompt into two distinct components: "context" encompasses the long text, and "instruction" contains task descriptions, optional questions, hypotheses, etc. Following Liu et al. (2023c), We respectively position the instruction at the beginning, end, and both ends of the prompt, denoted as "Pre-Ins," "PostIns," and "Both-Ins." We present a comparison of their effects in Table 5.</p>
<p>To start with, when the instruction is exclusively placed at the beginning, a discernible decline in performance is commonly encountered as extending input lengths. Notably, the "Pre-Ins" configuration within BAMBOO-16k often exhibits problems related to forgetting instructions, which is potentially attributed to the long-range decay of attention scores of RoPE (Xiong et al., 2023).</p>
<p>Moreover, the optimal instruction positions vary depending on the datasets and models. "Pre-Ins" typically result in lower performance for tasks where the instructions contain content-relevant information, such as question answering and hallucination detection. Conversely, "Post-Ins" exhibit reversed trends, consistent with findings from previous research (Liu et al., 2023c). Furthermore, "Both-Ins" consistently yield sub-optimal results across most datasets, with only minor performance gaps compared to the best-performing strategies.</p>
<h2>RQ4: Can LLMs Model Longer Text with Context Compression Methods?</h2>
<p>Prior research has effectively leveraged context compression techniques such as retrieval and truncation to address long text tasks with pretrained language models (Gong et al., 2020; Zhang et al., 2022; Zhao et al., 2022). Thus, it holds promise to integrate these techniques with LLMs.</p>
<p>To verify whether context compression techniques can enhance long text modeling abilities of short-context LLMs, we employ three different methods for hallucination detection and question answering tasks. (1) Truncation: We truncate the initial 3000 tokens. (2) Retrieval: we partition the long text into segments with 256 tokens, representing the question and each segment into an embedding with openai's text-embedding-ada-002 ${ }^{6}$. Then, we retrieve top-10 relevant segments to the question. (3) Summarization: We partition the input into chunks with 1024 tokens, sequentially summarize each chunk, and concatenate these summaries to form the final input.</p>
<p>| Method | AQA | PQA | MQA | SHallu | AHallu |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| ChatGPT-16K | 72.0 | 75.0 | 72.0 | 76.7 | $\mathbf{7 1 . 4}$ |
| ChatGPT+Retrieval | 69.5 | $\mathbf{7 9 . 0}$ | $\mathbf{7 8 . 0}$ | $\mathbf{7 8 . 3}$ | 71.2 |
| ChatGPT+Truncation | 71.5 | 64.0 | 68.0 | 74.1 | 70.0 |
| ChatGPT+Summarization | 51.5 | 63.0 | 67.0 | 75.0 | 70.0 |
| Vicuna-16K | 25.0 | $\mathbf{3 7 . 0}$ | 36.0 | 66.9 | 66.7 |
| Vicuna+Retrieval | 54.5 | 33.0 | $\mathbf{3 9 . 0}$ | 67.6 | 66.7 |
| Vicuna+Truncation | $\mathbf{5 5 . 8}$ | 33.0 | 32.0 | 66.7 | 66.7 |
| Vicuna+Summarization | 43.5 | 28.0 | 25.0 | $\mathbf{7 0 . 9}$ | $\mathbf{6 6 . 9}$ |</p>
<p>Table 6: Comparison of short-context LLMs with context compression techniques and long-context LLMs on selected tasks in BAMBOO-16k. We abbreviate AltQA, PaperQA, MeetingQA, Senhallu, and AbsHallu as AQA, PQA, MQA, SHallu, and AHallu, respectively.</p>
<p>The effect of different compression techniques is shown in Table 6. Evidently, retrieval-augmented</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Logarithmic value of attention scores in three settings of evidence positions.</p>
<p>LLMs can achieve comparable or superior performance to long-context LLMs, aligning with the findings in Xu et al. (2023). However, truncation and summarization methods often perform poorly due to the omission of substantial relevant information.</p>
<h2>RQ5: Why "Lost in The Middle" Happens?</h2>
<p>Previous study (Liu et al., 2023a) has observed that optimal performance is often attained when evidence information is positioned at the beginning or end of the input. To validate this phenomenon, we conducted experiments by placing evidence at the beginning or end of the long document (denoted as "Pre-Evi" and "Post-Evi", respectively). As illustrated in Table 7, despite disrupting the paper's original discourse structure, we consistently observed similar or improved performance when the evidence was situated at either end of the paper. Moreover, the performance gains were more substantial for longer input sequences.</p>
<p>To further investigate the rationale behind these remarkable properties, we analyze the attention maps. We compute the average attention scores for predictions across all heads and layers, and then partition the sequence into chunks of 32 tokens. Subsequently, we employ curve fitting to model the average scores of each chunk, as shown in Figure 2. Regardless of the location of the evidence within the input, we observe a U-shaped curve with higher attention allocated to tokens positioned at the two ends of the content. Consequently, it becomes evident that LLMs tend to utilize information located at the beginning or end of the input more effectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">PaperQA-16k</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaperQA-4k</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pre-Evi</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Post-Evi</td>
<td style="text-align: center;">Pre-Evi</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Post-Evi</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-16k</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-16k</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">30.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Impact of evidence position.</p>
<h3>4.4. Discussions</h3>
<p>Finally, we give discussions about the problems of LLMs over long texts.</p>
<ul>
<li>A severe problem is the cataclysmic forgetting of instructions. Frequently, the generating responses fail to align with the task demands. During the generation process, the instruction may be forgotten or even not understood at all, especially when the instructions are put at the beginning of long inputs. Smaller LLMs are more prone to adhering to the instructions, possibly owing to limitations in memorization. Thus, more robust long instruction datasets are required to enhance the instruction-following ability of long-context models</li>
<li>LLMs are prone to format errors in long text tasks. In certain cases, the predictions of LLMs may be presented in an informal format, though they convey the same meaning as the answers. The models may offer additional explanations and repetitions or adopt different formats. Thus, they can not be identified by the evaluation code. To alleviate the issue, we suggest reducing the format problems during the fine-tuning or RLHF stages and employing post-process techniques to rewrite the responses.</li>
<li>LLMs exhibit poor performance beyond the long texts. Provided evidence or condensed texts, LLM may still generate false responses and make the same fault as inputting the complete long texts. This error can be attributed to insufficient ability, across various lengths and tasks. Thus, it is imperative to augment the overall capabilities of LLMs, rather than concentrating solely on the context of long text domains.</li>
</ul>
<h2>- LLMs perform poorly on uncommon tasks.</h2>
<p>Notably, LLMs generally exhibit poor performance on tasks that are uncommon, e.g., text sorting and code completion, and even underperform by random baselines. Most open-sourced LLMs were fine-tuned on multi-turns of conversation, question answering, and summarization tasks. The limited diversity in fine-tuning data hinders LLMs' generalization to uncommon long text tasks. We believe that broadening the variety of tasks and domains on fine-tuning data can better comprehensively enhance LLMs' long text modeling capacities.</p>
<h2>5. Conclusion</h2>
<p>We propose BAMBOO, a benchmark for comprehensively evaluating the long text modeling capabilities of LLMs. BAMBOO consists of five tasks with two length levels, enabling the evaluation of LLMs' main capacities across various dimensions and domains. Based on the evaluation of several long-context models on BAMBOO, we give an overall analysis of the performances of different models and tasks. Additionally, we analyze key questions of long text modeling, discuss the problems of longcontext LLMs, and suggest directions for improving long text comprehension abilities. We believe BAMBOO has the potential to serve as a valuable tool for analyzing the comprehensive capacities and promoting the enhancement of long text modeling abilities of LLMs in the future.</p>
<h2>6. Acknowledgments</h2>
<p>This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027 and L233008. Xin Zhao is the corresponding author.</p>
<h2>7. Bibliographical References</h2>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150.</p>
<p>Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. 2023. Unlimiformer: Longrange transformers with unlimited length input. CoRR, abs/2305.01625.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya</p>
<p>Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. CoRR, abs/2306.15595.</p>
<p>Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR, abs/1904.10509.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978-2988. Association for Computational Linguistics.</p>
<p>Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with ioawareness. In NeurIPS.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.</p>
<p>Zican Dong, Tianyi Tang, Junyi Li, and Wayne Xin Zhao. 2023. A survey on long text modeling with transformers. CoRR, abs/2302.14502.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.</p>
<p>Shahriar Golchin and Mihai Surdeanu. 2023. Time travel in llms: Tracing data contamination in large language models. CoRR, abs/2308.08493.</p>
<p>Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, and Dong Yu. 2020. Recurrent chunking mechanisms for long-text machine reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,</p>
<p>2020, pages 6751-6761. Association for Computational Linguistics.</p>
<p>Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured state spaces. In NeurIPS.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 874-880. Association for Computational Linguistics.</p>
<p>Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longlimlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. CoRR, abs/2310.06839.
kaiokendev. 2023. Things I'm learning while training superhot.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6769-6781. Association for Computational Linguistics.</p>
<p>Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5156-5165. PMLR.</p>
<p>Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion</p>
<p>Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452-466.</p>
<p>Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. How long can open-source llms truly promise on context length?</p>
<p>Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, JianYun Nie, and Ji-Rong Wen. 2023b. Halueval: A large-scale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747.</p>
<p>Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. CoRR, abs/2304.13343.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle: How language models use long contexts. CoRR, abs/2307.03172.</p>
<p>Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. 2023b. Scaling laws of rope-based extrapolation. CoRR, abs/2310.05209.</p>
<p>Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023c. Instruction position matters in sequence generation with large language models. CoRR, abs/2308.12097.</p>
<p>Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. Luna: Linear unified nested attention. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 2441-2453.</p>
<p>Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. 2023. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,</p>
<p>Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS.</p>
<p>Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. 2023. Memgpt: Towards llms as operating systems. CoRR, abs/2310.08560.</p>
<p>Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023. Giraffe: Adventures in expanding context lengths in llms. CoRR, abs/2308.10882.</p>
<p>Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran G. V., Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and RuiJie Zhu. 2023a. RWKV: reinventing rnns for the transformer era. CoRR, abs/2305.13048.</p>
<p>Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071.</p>
<p>Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. 2021. Random feature attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. Parallel context windows for large language models. In Proceedings of the</p>
<p>61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada. Association for Computational Linguistics.</p>
<p>Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient contentbased sparse attention with routing transformers. Trans. Assoc. Comput. Linguistics, 9:53-68.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. CoRR, abs/2308.12950.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana llic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176bparameter open-access multilingual language model. CoRR, abs/2211.05100.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864.</p>
<p>Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 807-822. Association for Computational Linguistics.</p>
<p>Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023a. Retentive network: A successor to</p>
<p>transformer for large language models. CoRR, abs/2307.08621.</p>
<p>Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2023b. A lengthextrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 14590-14604. Association for Computational Linguistics.</p>
<p>Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, and Furu Wei. 2023. Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing. CoRR, abs/2305.15067.</p>
<p>Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.</p>
<p>Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. 2023. Focused transformer: Contrastive training for context scaling. CoRR, abs/2307.03170.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good NLG evaluator? A preliminary study. CoRR, abs/2303.04048.</p>
<p>Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768.</p>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023b. Augmenting language models with longterm memory. CoRR, abs/2306.07174.</p>
<p>Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. 2023. Effective longcontext scaling of foundation models. CoRR, abs/2309.16039.</p>
<p>Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025.</p>
<p>Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130B: an open bilingual</p>
<p>pre-trained model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed Hassan Awadallah, Dragomir R. Radev, and Rui Zhang. 2022. Summ\$`n\$: A multi-stage summarization framework for long input dialogues and documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 1592-1604. Association for Computational Linguistics.</p>
<p>Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and JiRong Wen. 2022. Dense text retrieval based on pretrained language models: A survey. CoRR, abs/2211.14876.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging Ilm-as-ajudge with mt-bench and chatbot arena. CoRR, abs/2306.05685.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. Recurrentgpt: Interactive generation of (arbitrarily) long text. CoRR, abs/2305.13304.</p>
<h2>8. Language Resource References</h2>
<p>Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. CoRR, abs/2307.11088.</p>
<p>Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A
bilingual, multitask benchmark for long context understanding. CoRR, abs/2308.14508.</p>
<p>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. Summscreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8602-8615. Association for Computational Linguistics.</p>
<p>Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 615-621. Association for Computational Linguistics.</p>
<p>Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 4599-4610. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Luyang Huang, Shuyang Cao, Nikolaus Nova Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 1419-1436. Association for Computational Linguistics.</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. 2022. Quality: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language</p>
<p>Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 5336-5358. Association for Computational Linguistics.</p>
<p>Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding. CoRR, abs/2305.14196.</p>
<p>Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: standardized comparison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 12007-12021. Association for Computational Linguistics.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multihop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics.</p>
<p>Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022. When language model meets private library. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 277-288. Association for Computational Linguistics.</p>
<p>Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir R. Radev. 2021. Qmsum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 5905-5921. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://platform.openai.com/docs/ guides/embeddings&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ All references to the number of tokens in our paper are computed using the gpt-3.5-turbo tokenizer.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
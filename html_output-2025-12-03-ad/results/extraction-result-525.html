<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-525 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-525</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-525</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-b033400e9a80915a928f4603582e5e8bf7656a85</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b033400e9a80915a928f4603582e5e8bf7656a85" target="_blank">Shifting the Baseline: Single Modality Performance on Visual Navigation & QA</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is argued that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques.</p>
                <p><strong>Paper Abstract:</strong> We demonstrate the surprising strength of unimodal baselines in multimodal domains, and make concrete recommendations for best practices in future research. Where existing work often compares against random or majority class baselines, we argue that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques. We present unimodal ablations on three recent datasets in visual navigation and QA, seeing an up to 29% absolute gain in performance over published baselines.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e525.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e525.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A+L (navigation ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-history plus Language inputs ablation (vision removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unimodal ablation of published multimodal navigation models that receives the agent's previous action and the language instruction/question but no visual input; used to test how much navigation can be driven by language and action priors alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Author's released navigation model (LSTM-based decoder with LSTM language encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM decoder that (in the full model) attends over an LSTM language encoder and consumes visual encodings (ResNet/CNN) plus previous action; in this ablation vision inputs are zeroed while architecture and parameters are preserved. Trained with teacher/student forcing depending on experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Matterport Room-to-Room navigation (and related simulated navigation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a natural language route/instruction, an embodied agent must navigate a discretized, photo-real environment from a start location to a specified destination by producing a sequence of actions (forward, turn left, turn right, etc.). The environment provides minimally sensed availability of actions (W).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural (language encodes route/spatial relations and procedural action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on the task dataset (language-route pairs) and learned priors from training set; implicit dataset biases</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning of the multimodal model with vision inputs zeroed; training used teacher/student forcing variants</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in LSTM weights and learned sequence models (language encoder + decoder), with action-history vector; no explicit spatial map beyond learned sequence/statistical correlations; minimal sensing W provided externally</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (proximity to target) in % (validation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Matterport Seen: 15.4% success; Matterport Unseen: 13.9% success (A+L ablation, Table 2). In some settings A+L outperforms published baselines and, for Matterport Unseen, outperformed the Full Model under teacher-forcing in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Exploits linguistic route regularities and correlations between language instructions and typical action sequences; uses action-history to continue plausible sequences (procedural priors) and minimal sensing W to avoid impossible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Lacks visual grounding so fails when language alone is ambiguous or when visual disambiguation is required (poorer generalization where visual cues are essential); performance can drop in seen/unseen variance and noisy visual domains where vision would provide disambiguating info.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Published baseline (random direction with up to five forwards/turn-right-on-block) achieved 15.9% seen / 16.3% unseen; Full Model achieved 38.6% seen / 21.8% unseen. A+L (15.4/13.9) compares favorably to the baseline on seen/unseen in some settings (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing vision (i.e., A+L) while preserving language and action-history yields nontrivial navigation ability, sometimes outperforming simple published baselines and even outperforming or matching the full multimodal model in certain unseen conditions; this shows language+action priors alone encode strong procedural/spatial biases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language plus action-history (without vision) encodes strong procedural and topological biases that can drive reasonable navigation trajectories by exploiting dataset regularities; such unimodal performance reveals that language and action-history alone can supply much spatial/procedural signal in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Shifting the Baseline: Single Modality Performance on Visual Navigation & QA', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e525.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e525.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A only (action-history only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-history only ablation (no language, no vision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where the model receives only its previous action inputs and minimal sensing W (which actions are available), testing how much navigation can be driven purely by action-history and minimal sensing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Author's released navigation model (same architecture as full model but with language and vision zeroed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same multimodal architecture as used in the original papers; language and vision vectors set to zero, preserving architecture and parameters. Learns transitions and action correlations from training trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Matterport Room-to-Room and THOR-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Embodied navigation task where agent must reach a target specified by a language instruction or question, but in this ablation the instruction is removed and only previous action and minimal sensing are available.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial (topological biases induced by action sequences and minimal sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on navigation trajectories; statistical regularities in training action sequences and environment topology</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning with language and vision inputs zeroed; training via teacher/student forcing</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in network weights capturing action-transition statistics and topological priors; minimal sensing W provides an availability mask but no visual scene encoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Matterport Seen: 4.1% ; Unseen: 3.2% (A only, Table 2). The A-only agent nonetheless outperforms some published baselines in certain environments (paper notes A-only outperforms baseline in Matterport and THOR-Nav in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Learns simple navigation correlations such as average number of forward steps, not turning left immediately after turning right, and other short-range transition regularities; uses minimal sensing to avoid impossible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Cannot follow instructions or utilize scene-specific cues; fails at goal-directed navigation requiring semantic or spatial understanding beyond action-statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline published random/programmatic walks: baseline ~15.9% seen / 16.3% unseen in Matterport; A-only (4.1/3.2) is lower than the baseline's nominal success but paper reports A-only outperforms baseline in some THOR-Nav/Matterport derived baselines depending on setup.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing both language and vision leaves only action-history and minimal sensing; performance is low but non-zero, showing that trajectory/topological biases are encoded and can be exploited.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Action-history and minimal sensing alone encode topological and procedural biases that afford some navigation ability; this reveals that even nearly zero-input agents can outperform naive baselines by learning environment/action regularities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Shifting the Baseline: Single Modality Performance on Visual Navigation & QA', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e525.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e525.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A+V (vision+action ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-history plus Vision inputs ablation (language removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unimodal ablation that preserves visual inputs and previous action but removes language, testing navigation driven by visual scene structure and action priors without language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Author's released navigation model (CNN-encoded vision + LSTM decoder architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CNN-encoded image/semantic segmentation (ResNet/CNN) concatenated with action-history input into LSTM or controller; language vector zeroed. Trained on navigation episodes (teacher/student forcing variants).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>THOR-Nav, EQA navigation, Matterport navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent navigates in a simulated environment to approach a target object/room; in this ablation the model receives visual observations (or ground-truth semantic masks) and previous action but no language/question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (scene layout priors and object location regularities gleaned from visuals)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on environment visual observations and navigation episodes; ground-truth semantic object locations in some QA experiments</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning of CNN+LSTM model on navigation; teacher/student forcing during training</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit visual feature representations in CNN encodings and learned associations between visual scene structure and navigation actions; no explicit metric map—relies on learned correlations/topology</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (%) and distance to target (meters) depending on benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Matterport Seen: 30.6% ; Unseen: 13.3% (A+V, Table 2). On EQA navigation, A+V often outperformed baselines and sometimes the full model (paper reports A+V reduces final distance and in some settings outperforms Full Model; overall EQA improvement ~0.06 m reduction compared to full model in some reported averages).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Latches onto dataset biases in scene structure and object layouts; can navigate by recognizing visual landmarks and common spatial layouts (especially in limited, repeated kitchen layouts in THOR/AI2-THOR).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without language, cannot target objects specified only linguistically or disambiguate between visually similar objects that are distinguished only by the question; suffers when target depends on semantic language cues absent from visuals.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline (random long action sequences) performed poorly; A+V outperforms these baselines in THOR-Nav and EQA in the paper's experiments; compared to Full Model, A+V sometimes matches or outperforms in seen/unseen depending on dataset (e.g., ties/outperforms Full Model in several EQA settings).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing language retains strong performance in some simulated environments because visual scene regularities suffice; the ablation demonstrates that vision+action encodes spatial and object-relational priors that can drive navigation without language.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vision with action-history can encode spatial and object-relational knowledge sufficient for navigation in many benchmark settings, exploiting scene layout biases; absence of language does limit goal-targeting when linguistic disambiguation is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Shifting the Baseline: Single Modality Performance on Visual Navigation & QA', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e525.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e525.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L only (QA ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-only ablation for Question Answering (vision removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unimodal QA ablation that receives only the language question (no visual or navigation input), probing whether language priors alone can predict answers in embodied QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Published QA model variants (question encoder feeding classifier / concatenation architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Question encoded with an LSTM (tiled/question embedding) and passed through the QA classification head while visual/top-down scene inputs are zeroed or removed; trained on QA pairs with ground-truth navigation/visual info removed for this ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IQUAD V1 QA (decoupled) and EQA QA (decoupled, oracle views)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a question about an environment or object (existence, counting, spatial relation, color, etc.), produce the correct answer; in the ablation only language is available (no image or scene-state inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>egocentric question answering / VQA (decoupled from navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational and priors about answers (statistical language priors over object properties/answers); some procedural priors for existence/counting questions encoded in the question distribution</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on QA dataset (question-answer pairs); statistical biases in dataset due to question-answer distributions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning / supervised QA training on the dataset with vision removed</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in language encoder weights (LSTM) and classifier; knowledge is represented as language-conditioned statistical priors over answers rather than grounded visual features</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>top-1 QA accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IQUAD V1: Unseen 41.7% / Seen 41.7% (L-only equals majority baseline there). EQA (decoupled QA with oracle views removed): Unseen 48.8% top-1 accuracy (Table 4), outperforming the majority-class baseline (19.8% for EQA baseline in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Predicts high-probability answers from language priors (e.g., common colors or object attributes frequently associated with question templates); leverages dataset answer-distribution biases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Lacks grounding so cannot resolve questions that require visual information (object presence, specific colors in a scene when question distributions are uniform); when dataset is properly randomized (IQUAD V1) L-only performance is near chance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Majority-class baseline: IQUAD V1 ~41.7% (varies by question type), EQA baseline ~19.8%; L-only matched or exceeded baselines (notably +29.0 percentage points over baseline for EQA in the paper's breakdown). Full multimodal QA model: IQUAD V1 ~88.3/89.3 (Unseen/Seen) and EQA ~64.0 (Full Model with visual/top-down inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing vision leaves language-only QA which performs surprisingly well on EQA due to answer priors (near 48.8%); in IQUAD V1, which was randomized to reduce language-only signal, L-only performs near chance, showing dataset construction matters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-only models can encode object-relational priors and answer distributions that yield high QA performance when datasets contain biases; careful dataset construction (randomization) is required to ensure visual grounding is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Shifting the Baseline: Single Modality Performance on Visual Navigation & QA', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e525.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e525.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>V only (vision-only QA ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-only ablation for QA (language removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unimodal QA ablation that receives only visual/top-down scene inputs (including ground-truth object locations in some experiments) but no language question, testing what visual priors alone can predict about answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Published QA model (CNN/top-down image processing + classification head) with language inputs zeroed</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model consumes a top-down view or semantic segmentation mask of scene object locations (ground-truth in experiments) and feeds through convolutions and classifier; the question input is zeroed for this ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IQUAD V1 QA and EQA QA (decoupled)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer visual questions about scenes; in this ablation only the visual scene description (top-down object locations or images) is given without the language question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>egocentric question answering / VQA</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational and spatial (object locations, colors, and room features)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on visual QA datasets and access to ground-truth object location/top-down maps in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning on QA with language inputs removed; leveraging ground-truth segmentation/top-down views in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit visual feature maps from CNN/top-down encodings and a spatial sum operation (paper's QA architecture); no explicit symbolic reasoning beyond convolutional features</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>top-1 QA accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>IQUAD V1: Unseen 43.5% / Seen 42.8% (V-only, Table 4) — a modest improvement over random/majority baselines. EQA: V-only 44.2% top-1 accuracy (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Identifies salient colors and room features and uses object-location maps to eliminate unlikely answers; benefits when visual features correlate strongly with answer choices.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without the question, cannot focus on the relevant subset of visual information; fails when the answer depends on specific relations referenced by the question or when visual cues are subtle and require question-conditioned attention.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Majority baseline (IQUAD): ~41.7%; V-only slightly improves (~43.5/42.8). Full multimodal QA achieves much higher accuracy (IQUAD full ~88.3/89.3; EQA full ~64.0) when question and visual inputs are combined.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing language leaves only vision; V-only can slightly outperform chance/majority in some QA benchmarks due to salient visual priors, but lacks the focused conditional reasoning that combining vision+language yields.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vision-only models can exploit visual priors (colors, room structure) to answer some questions without language, but they cannot replace the conditional grounding that language provides; both modalities together are needed for high QA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Shifting the Baseline: Single Modality Performance on Visual Navigation & QA', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments <em>(Rating: 2)</em></li>
                <li>Embodied Question Answering <em>(Rating: 2)</em></li>
                <li>Iqa: Visual question answering in interactive environments <em>(Rating: 2)</em></li>
                <li>Making the V in VQA matter: Elevating the role of image understanding in visual question answering <em>(Rating: 1)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-525",
    "paper_id": "paper-b033400e9a80915a928f4603582e5e8bf7656a85",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "A+L (navigation ablation)",
            "name_full": "Action-history plus Language inputs ablation (vision removed)",
            "brief_description": "A unimodal ablation of published multimodal navigation models that receives the agent's previous action and the language instruction/question but no visual input; used to test how much navigation can be driven by language and action priors alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Author's released navigation model (LSTM-based decoder with LSTM language encoder)",
            "model_size": null,
            "model_description": "LSTM decoder that (in the full model) attends over an LSTM language encoder and consumes visual encodings (ResNet/CNN) plus previous action; in this ablation vision inputs are zeroed while architecture and parameters are preserved. Trained with teacher/student forcing depending on experiment.",
            "task_name": "Matterport Room-to-Room navigation (and related simulated navigation benchmarks)",
            "task_description": "Given a natural language route/instruction, an embodied agent must navigate a discretized, photo-real environment from a start location to a specified destination by producing a sequence of actions (forward, turn left, turn right, etc.). The environment provides minimally sensed availability of actions (W).",
            "task_type": "navigation / instruction following",
            "knowledge_type": "spatial+procedural (language encodes route/spatial relations and procedural action sequences)",
            "knowledge_source": "fine-tuning on the task dataset (language-route pairs) and learned priors from training set; implicit dataset biases",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning of the multimodal model with vision inputs zeroed; training used teacher/student forcing variants",
            "knowledge_representation": "implicit in LSTM weights and learned sequence models (language encoder + decoder), with action-history vector; no explicit spatial map beyond learned sequence/statistical correlations; minimal sensing W provided externally",
            "performance_metric": "success rate (proximity to target) in % (validation)",
            "performance_result": "Matterport Seen: 15.4% success; Matterport Unseen: 13.9% success (A+L ablation, Table 2). In some settings A+L outperforms published baselines and, for Matterport Unseen, outperformed the Full Model under teacher-forcing in the paper's experiments.",
            "success_patterns": "Exploits linguistic route regularities and correlations between language instructions and typical action sequences; uses action-history to continue plausible sequences (procedural priors) and minimal sensing W to avoid impossible actions.",
            "failure_patterns": "Lacks visual grounding so fails when language alone is ambiguous or when visual disambiguation is required (poorer generalization where visual cues are essential); performance can drop in seen/unseen variance and noisy visual domains where vision would provide disambiguating info.",
            "baseline_comparison": "Published baseline (random direction with up to five forwards/turn-right-on-block) achieved 15.9% seen / 16.3% unseen; Full Model achieved 38.6% seen / 21.8% unseen. A+L (15.4/13.9) compares favorably to the baseline on seen/unseen in some settings (see paper).",
            "ablation_results": "Removing vision (i.e., A+L) while preserving language and action-history yields nontrivial navigation ability, sometimes outperforming simple published baselines and even outperforming or matching the full multimodal model in certain unseen conditions; this shows language+action priors alone encode strong procedural/spatial biases.",
            "key_findings": "Language plus action-history (without vision) encodes strong procedural and topological biases that can drive reasonable navigation trajectories by exploiting dataset regularities; such unimodal performance reveals that language and action-history alone can supply much spatial/procedural signal in these benchmarks.",
            "uuid": "e525.0",
            "source_info": {
                "paper_title": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "A only (action-history only)",
            "name_full": "Action-history only ablation (no language, no vision)",
            "brief_description": "An ablation where the model receives only its previous action inputs and minimal sensing W (which actions are available), testing how much navigation can be driven purely by action-history and minimal sensing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Author's released navigation model (same architecture as full model but with language and vision zeroed)",
            "model_size": null,
            "model_description": "Same multimodal architecture as used in the original papers; language and vision vectors set to zero, preserving architecture and parameters. Learns transitions and action correlations from training trajectories.",
            "task_name": "Matterport Room-to-Room and THOR-Nav",
            "task_description": "Embodied navigation task where agent must reach a target specified by a language instruction or question, but in this ablation the instruction is removed and only previous action and minimal sensing are available.",
            "task_type": "navigation",
            "knowledge_type": "procedural+spatial (topological biases induced by action sequences and minimal sensing)",
            "knowledge_source": "fine-tuning on navigation trajectories; statistical regularities in training action sequences and environment topology",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning with language and vision inputs zeroed; training via teacher/student forcing",
            "knowledge_representation": "implicit in network weights capturing action-transition statistics and topological priors; minimal sensing W provides an availability mask but no visual scene encoding",
            "performance_metric": "success rate (%)",
            "performance_result": "Matterport Seen: 4.1% ; Unseen: 3.2% (A only, Table 2). The A-only agent nonetheless outperforms some published baselines in certain environments (paper notes A-only outperforms baseline in Matterport and THOR-Nav in some cases).",
            "success_patterns": "Learns simple navigation correlations such as average number of forward steps, not turning left immediately after turning right, and other short-range transition regularities; uses minimal sensing to avoid impossible actions.",
            "failure_patterns": "Cannot follow instructions or utilize scene-specific cues; fails at goal-directed navigation requiring semantic or spatial understanding beyond action-statistics.",
            "baseline_comparison": "Baseline published random/programmatic walks: baseline ~15.9% seen / 16.3% unseen in Matterport; A-only (4.1/3.2) is lower than the baseline's nominal success but paper reports A-only outperforms baseline in some THOR-Nav/Matterport derived baselines depending on setup.",
            "ablation_results": "Removing both language and vision leaves only action-history and minimal sensing; performance is low but non-zero, showing that trajectory/topological biases are encoded and can be exploited.",
            "key_findings": "Action-history and minimal sensing alone encode topological and procedural biases that afford some navigation ability; this reveals that even nearly zero-input agents can outperform naive baselines by learning environment/action regularities.",
            "uuid": "e525.1",
            "source_info": {
                "paper_title": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "A+V (vision+action ablation)",
            "name_full": "Action-history plus Vision inputs ablation (language removed)",
            "brief_description": "A unimodal ablation that preserves visual inputs and previous action but removes language, testing navigation driven by visual scene structure and action priors without language instructions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Author's released navigation model (CNN-encoded vision + LSTM decoder architecture)",
            "model_size": null,
            "model_description": "CNN-encoded image/semantic segmentation (ResNet/CNN) concatenated with action-history input into LSTM or controller; language vector zeroed. Trained on navigation episodes (teacher/student forcing variants).",
            "task_name": "THOR-Nav, EQA navigation, Matterport navigation",
            "task_description": "Agent navigates in a simulated environment to approach a target object/room; in this ablation the model receives visual observations (or ground-truth semantic masks) and previous action but no language/question.",
            "task_type": "navigation",
            "knowledge_type": "spatial+object-relational (scene layout priors and object location regularities gleaned from visuals)",
            "knowledge_source": "fine-tuning on environment visual observations and navigation episodes; ground-truth semantic object locations in some QA experiments",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning of CNN+LSTM model on navigation; teacher/student forcing during training",
            "knowledge_representation": "implicit visual feature representations in CNN encodings and learned associations between visual scene structure and navigation actions; no explicit metric map—relies on learned correlations/topology",
            "performance_metric": "success rate (%) and distance to target (meters) depending on benchmark",
            "performance_result": "Matterport Seen: 30.6% ; Unseen: 13.3% (A+V, Table 2). On EQA navigation, A+V often outperformed baselines and sometimes the full model (paper reports A+V reduces final distance and in some settings outperforms Full Model; overall EQA improvement ~0.06 m reduction compared to full model in some reported averages).",
            "success_patterns": "Latches onto dataset biases in scene structure and object layouts; can navigate by recognizing visual landmarks and common spatial layouts (especially in limited, repeated kitchen layouts in THOR/AI2-THOR).",
            "failure_patterns": "Without language, cannot target objects specified only linguistically or disambiguate between visually similar objects that are distinguished only by the question; suffers when target depends on semantic language cues absent from visuals.",
            "baseline_comparison": "Baseline (random long action sequences) performed poorly; A+V outperforms these baselines in THOR-Nav and EQA in the paper's experiments; compared to Full Model, A+V sometimes matches or outperforms in seen/unseen depending on dataset (e.g., ties/outperforms Full Model in several EQA settings).",
            "ablation_results": "Removing language retains strong performance in some simulated environments because visual scene regularities suffice; the ablation demonstrates that vision+action encodes spatial and object-relational priors that can drive navigation without language.",
            "key_findings": "Vision with action-history can encode spatial and object-relational knowledge sufficient for navigation in many benchmark settings, exploiting scene layout biases; absence of language does limit goal-targeting when linguistic disambiguation is necessary.",
            "uuid": "e525.2",
            "source_info": {
                "paper_title": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "L only (QA ablation)",
            "name_full": "Language-only ablation for Question Answering (vision removed)",
            "brief_description": "A unimodal QA ablation that receives only the language question (no visual or navigation input), probing whether language priors alone can predict answers in embodied QA benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Published QA model variants (question encoder feeding classifier / concatenation architectures)",
            "model_size": null,
            "model_description": "Question encoded with an LSTM (tiled/question embedding) and passed through the QA classification head while visual/top-down scene inputs are zeroed or removed; trained on QA pairs with ground-truth navigation/visual info removed for this ablation.",
            "task_name": "IQUAD V1 QA (decoupled) and EQA QA (decoupled, oracle views)",
            "task_description": "Given a question about an environment or object (existence, counting, spatial relation, color, etc.), produce the correct answer; in the ablation only language is available (no image or scene-state inputs).",
            "task_type": "egocentric question answering / VQA (decoupled from navigation)",
            "knowledge_type": "object-relational and priors about answers (statistical language priors over object properties/answers); some procedural priors for existence/counting questions encoded in the question distribution",
            "knowledge_source": "fine-tuning on QA dataset (question-answer pairs); statistical biases in dataset due to question-answer distributions",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning / supervised QA training on the dataset with vision removed",
            "knowledge_representation": "implicit in language encoder weights (LSTM) and classifier; knowledge is represented as language-conditioned statistical priors over answers rather than grounded visual features",
            "performance_metric": "top-1 QA accuracy (%)",
            "performance_result": "IQUAD V1: Unseen 41.7% / Seen 41.7% (L-only equals majority baseline there). EQA (decoupled QA with oracle views removed): Unseen 48.8% top-1 accuracy (Table 4), outperforming the majority-class baseline (19.8% for EQA baseline in paper).",
            "success_patterns": "Predicts high-probability answers from language priors (e.g., common colors or object attributes frequently associated with question templates); leverages dataset answer-distribution biases.",
            "failure_patterns": "Lacks grounding so cannot resolve questions that require visual information (object presence, specific colors in a scene when question distributions are uniform); when dataset is properly randomized (IQUAD V1) L-only performance is near chance.",
            "baseline_comparison": "Majority-class baseline: IQUAD V1 ~41.7% (varies by question type), EQA baseline ~19.8%; L-only matched or exceeded baselines (notably +29.0 percentage points over baseline for EQA in the paper's breakdown). Full multimodal QA model: IQUAD V1 ~88.3/89.3 (Unseen/Seen) and EQA ~64.0 (Full Model with visual/top-down inputs).",
            "ablation_results": "Removing vision leaves language-only QA which performs surprisingly well on EQA due to answer priors (near 48.8%); in IQUAD V1, which was randomized to reduce language-only signal, L-only performs near chance, showing dataset construction matters.",
            "key_findings": "Language-only models can encode object-relational priors and answer distributions that yield high QA performance when datasets contain biases; careful dataset construction (randomization) is required to ensure visual grounding is necessary.",
            "uuid": "e525.3",
            "source_info": {
                "paper_title": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "V only (vision-only QA ablation)",
            "name_full": "Vision-only ablation for QA (language removed)",
            "brief_description": "A unimodal QA ablation that receives only visual/top-down scene inputs (including ground-truth object locations in some experiments) but no language question, testing what visual priors alone can predict about answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Published QA model (CNN/top-down image processing + classification head) with language inputs zeroed",
            "model_size": null,
            "model_description": "Model consumes a top-down view or semantic segmentation mask of scene object locations (ground-truth in experiments) and feeds through convolutions and classifier; the question input is zeroed for this ablation.",
            "task_name": "IQUAD V1 QA and EQA QA (decoupled)",
            "task_description": "Answer visual questions about scenes; in this ablation only the visual scene description (top-down object locations or images) is given without the language question.",
            "task_type": "egocentric question answering / VQA",
            "knowledge_type": "object-relational and spatial (object locations, colors, and room features)",
            "knowledge_source": "fine-tuning on visual QA datasets and access to ground-truth object location/top-down maps in some experiments",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning on QA with language inputs removed; leveraging ground-truth segmentation/top-down views in experiments",
            "knowledge_representation": "implicit visual feature maps from CNN/top-down encodings and a spatial sum operation (paper's QA architecture); no explicit symbolic reasoning beyond convolutional features",
            "performance_metric": "top-1 QA accuracy (%)",
            "performance_result": "IQUAD V1: Unseen 43.5% / Seen 42.8% (V-only, Table 4) — a modest improvement over random/majority baselines. EQA: V-only 44.2% top-1 accuracy (Table 4).",
            "success_patterns": "Identifies salient colors and room features and uses object-location maps to eliminate unlikely answers; benefits when visual features correlate strongly with answer choices.",
            "failure_patterns": "Without the question, cannot focus on the relevant subset of visual information; fails when the answer depends on specific relations referenced by the question or when visual cues are subtle and require question-conditioned attention.",
            "baseline_comparison": "Majority baseline (IQUAD): ~41.7%; V-only slightly improves (~43.5/42.8). Full multimodal QA achieves much higher accuracy (IQUAD full ~88.3/89.3; EQA full ~64.0) when question and visual inputs are combined.",
            "ablation_results": "Removing language leaves only vision; V-only can slightly outperform chance/majority in some QA benchmarks due to salient visual priors, but lacks the focused conditional reasoning that combining vision+language yields.",
            "key_findings": "Vision-only models can exploit visual priors (colors, room structure) to answer some questions without language, but they cannot replace the conditional grounding that language provides; both modalities together are needed for high QA accuracy.",
            "uuid": "e525.4",
            "source_info": {
                "paper_title": "Shifting the Baseline: Single Modality Performance on Visual Navigation & QA",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
            "rating": 2
        },
        {
            "paper_title": "Embodied Question Answering",
            "rating": 2
        },
        {
            "paper_title": "Iqa: Visual question answering in interactive environments",
            "rating": 2
        },
        {
            "paper_title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
            "rating": 1
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 1
        }
    ],
    "cost": 0.01415925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Shifting the Baseline: Single Modality Performance on Visual Navigation \&amp; QA</h1>
<p>Jesse Thomason Daniel Gordon Yonatan Bisk<br>Paul G. Allen School of Computer Science and Engineering<br>{jdtho, xkcd, ybisk}@cs.washington.edu</p>
<h4>Abstract</h4>
<p>We demonstrate the surprising strength of unimodal baselines in multimodal domains, and make concrete recommendations for best practices in future research. Where existing work often compares against random or majority class baselines, we argue that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques. We present unimodal ablations on three recent datasets in visual navigation and QA, seeing an up to $29 \%$ absolute gain in performance over published baselines.</p>
<h2>1 Introduction</h2>
<p>All datasets have biases. Baselines should capture these regularities so that outperforming them indicates a model is actually solving a task. In multimodal domains, bias can occur in any subset of the modalities. To address this, we argue it is not sufficient for researchers to provide random or majority class baselines; instead we recommend presenting results for unimodal models. We investigate visual navigation and question answering tasks, where agents move through simulated environments using egocentric (first person) vision. We find that unimodal ablations (e.g., language only) in these seemingly multimodal tasks can outperform corresponding full models (§4.1).</p>
<p>This work extends observations made in both the Computer Vision (Goyal et al., 2018; Cirik et al., 2018) and Natural Language (Mudrakarta et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Gururangan et al., 2018; Kaushik and Lipton, 2018) communities that complex models often perform well by fitting to simple, unintended correlations in the data, bypassing the complex grounding and reasoning that experimenters hoped was necessary for their tasks.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Navigating without vision leads to sensible navigation trajectories in response to commands like "walk past the bar and turn right". At $t_{3}$, "forward" is unavailable as the agent would collide with the wall.</p>
<p>We ablate models from three recent papers: (1) navigation (Figure 1) using images of real homes paired with crowdsourced language descriptions (Anderson et al., 2018); and (2, 3) navigation and egocentric question answering (Gordon et al., 2018; Das et al., 2018a) in simulation with synthetic questions. We find that unimodal ablations often outperform the baselines that accompany these tasks.</p>
<p>Recommendation for Best Practices: Our findings show that in the new space of visual navigation and egocentric QA, all modalities, even an agent's action history, are strongly informative. Therefore, while many papers ablate either language or vision, new results should ablate both. Such baselines expose possible gains from unimodal biases in multimodal datasets irrespective of training and architecture details.</p>
<h2>2 Ablation Evaluation Framework</h2>
<p>In the visual navigation and egocentric question answering tasks, at each timestep an agent receives an observation and produces an action. Actions can move the agent to a new location or heading</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: $P($ act $=$ col $|\operatorname{prev}=$ row $)$ and marginal action distributions in Matterport training. Peaked distributions enable agents to memorize simple rules like not turning left immediately after turning right, or moving forward an average number of steps.
(e.g., turn left), or answer questions (e.g., answer 'brown'). At timestep $t$, a multimodal model $\mathcal{M}$ takes in a visual input $\mathcal{V}<em t="t">{t}$ and language question or navigation command $\mathcal{L}$ to predict the next action $a</em>$, and 'minimally sensed' world information $W$ specifying which actions are available (e.g., that forward is unavailable if the agent is facing a wall).}$. The navigation models we examine also take in their action from the previous timestep, $a_{t-1</p>
<p>$$
a_{t} \leftarrow \mathcal{M}\left(\mathcal{V}<em t-1="t-1">{t}, \mathcal{L}, a</em> ; W\right)
$$</p>
<p>In each benchmark, $\mathcal{M}$ corresponds to the author's released code and training paradigm. In addition to their full model, we evaluate the role of each input modality by removing those inputs and replacing them with zero vectors. Formally, we define the full model and three ablations:</p>
<p>$$
\begin{array}{lll}
\text { Full Model } &amp; \text { is } &amp; \mathcal{M}\left(\mathcal{V}<em t-1="t-1">{t}, \mathcal{L}, a</em> ; W\right) \
\mathcal{A} &amp; \text { is } &amp; \mathcal{M}(\overrightarrow{0}, \overrightarrow{0}, a_{t-1} ; W) \
\mathcal{A}+\mathcal{V} &amp; \text { is } &amp; \mathcal{M}\left(\mathcal{V}<em t-1="t-1">{t}, \overrightarrow{0}, a</em> ; W\right) \
\mathcal{A}+\mathcal{L} &amp; \text { is } &amp; \mathcal{M}\left(\overrightarrow{0}, \mathcal{L}, a_{t-1} ; W\right)
\end{array}
$$</p>
<p>corresponding to models with access to $\mathcal{A}$ ction inputs, $\mathcal{V}$ ision inputs, and $\mathcal{L}$ anguage inputs. These ablations preserve the architecture and number of parameters of $\mathcal{M}$ by changing only its inputs.</p>
<h2>3 Benchmark Tasks</h2>
<p>We evaluate on navigation and question answering tasks across three benchmark datasets: Matterport Room-to-Room (no question answering component), and IQUAD V1 and EQA (question answering that requires navigating to the relevant scene in the environment) (Anderson et al., 2018; Gordon
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: IQA data construction attempts to make both the question and image necessary for QA.
et al., 2018; Das et al., 2018a). We divide the latter two into separate navigation and question answering components. We then train and evaluate models separately per subtask to analyze accuracy.</p>
<h3>3.1 Matterport Room-to-Room</h3>
<p>An agent is given a route in English and navigates through a discretized map to the specified destination (Anderson et al., 2018). This task includes high fidelity visual inputs and crowdsourced natural language routes.</p>
<p>Published Full Model: At each timestep an LSTM decoder uses a ResNet-encoded image $\mathcal{V}<em t-1="t-1">{t}$ and previous action $a</em>$ (seen in Figure 2).}$ to attend over the states of an LSTM language encoder $(\mathcal{L})$ to predict navigation action $a_{t</p>
<p>Published Baseline: The agent chooses a random direction and takes up to five forward actions, turning right when no forward action is available.</p>
<h3>3.2 Interactive Question Answering</h3>
<p>IQUAD V1 (Gordon et al., 2018) contains three question types: existence (e.g., Is there a ...?), counting (e.g., How many ...?) where the answer ranges from 0 to 3 , and spatial relation: (e.g., Is there a ... in the ...?). The data was constructed via randomly generated configurations to weaken majority class baselines (Figure 3). To evaluate the navigation subtask, we introduce a new THOR-Nav benchmark. ${ }^{1}$ The agent is placed in a random location in the room and must approach one of fridge, garbage can, or microwave in response to a natural language question.</p>
<p>Although we use the same full model as Gordon et al. (2018), our QA results are not directly comparable. In particular, Gordon et al. (2018) do not quantify the effectiveness of the QA component independent of the scene exploration (i.e. navigation and interaction). To remove the scene explo-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ration steps of Gordon et al. (2018), we provide a complete ground-truth view of the environment. ${ }^{2}$ We use ground-truth rather than YOLO (Redmon et al., 2016) due to speed constraints.</p>
<p>Nav Full Model: The image and ground-truth semantic segmentation mask $\mathcal{V}<em t-1="t-1">{t}$, tiled question $\mathcal{L}$, and previous action $a</em>$ are encoded via a CNN which outputs a distribution over actions. Optimal actions are learned via teacher forcing.</p>
<p>Nav Baseline: The agent executes 100 randomly chosen navigation actions then terminates. In AI2THOR (Kolve et al., 2017), none of the kitchens span more than 5 meters. With a step-size of 0.25 meters, we observed that 100 actions was significantly shorter than the shortest path length.</p>
<p>Published QA Full Model: The question encoding $\mathcal{L}$ is tiled and concatenated with a topdown view of the ground truth location of all objects in the scene $\mathcal{V}$. This is fed into several convolutions, a spatial sum, and a final fully connected layer which outputs a likelihood for each answer.</p>
<p>Published QA Baseline: We include the majority class baseline from Gordon et al. (2018).</p>
<h3>3.3 Embodied Question Answering</h3>
<p>EQA (Das et al., 2018a) questions are programmatically generated to refer to a single, unambiguous object for a specific environment, and are filtered to avoid easy questions (e.g., What room is the bathtub in?). At evaluation, an agent is placed a fixed number of actions away from the object.</p>
<p>Published Nav Full Model: At each timestep, a planner LSTM takes in a CNN encoded image $\mathcal{V}<em t-1="t-1">{t}$, LSTM encoded question $\mathcal{L}$, and the previous action $a</em>$ again or returning control to the planner.}$ and emits an action $a_{t}$. The action is executed in the environment, and then a lower-level controller LSTM continues to take in new vision observations and $a_{t}$, either repeating $a_{t</p>
<p>Published Nav Baseline: This baseline model is trained and evaluated with the same inputs as the full model, but does not pass control to a lowerlevel controller, instead predicting a new action using the planner LSTM at each timestep (i.e., no hierarchical control). Das et al. (2018a) name this baseline LSTM+Question.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Navigation success (Matterport, THOR-Nav) (\%) and remaining distance to target (EQA) (m). Best unimodal in bold; better than reported baseline; *better than full model.</p>
<p>Published QA Full Model: Given the last five image encodings along the gold standard navigation trajectory, $\mathcal{V}<em t="t">{t-4} \ldots \mathcal{V}</em>$ and used to predict the answer. Das et al. (2018a) name this oracle-navigation model ShortestPath+VQA.}$, and the question encoding $\mathcal{L}$, image-question similarities are calculated via a dot product and converted via attention weights to a summary weight $\overline{\mathcal{V}}$, which is concatenated with $\mathcal{L</p>
<p>QA Baseline: Das et al. (2018a) provide no explicit baseline for the VQA component alone. We use a majority class baseline inspired by the data's entropy based filtering.</p>
<h2>4 Experiments</h2>
<p>Across all benchmarks, unimodal baselines outperform baseline models used in or derived from the original works. Navigating unseen environments, these unimodal ablations outperform their corresponding full models on the Matterport (absolute $\uparrow 2.5 \%$ success rate) and EQA ( $\downarrow 0.06 m$ distance to target).</p>
<h3>4.1 Navigation</h3>
<p>We evaluate our ablation baselines on Matterport, ${ }^{3}$ THOR-Nav, and EQA (Table 1), ${ }^{4}$ and discover that some unimodal ablations outperform their corresponding full models. For Matterport and THOR-Nav, success rate is defined by proximity to the target. For EQA, we measure absolute distance from the target in meters.</p>
<p>Unimodal Performance: Across Matterport, THOR-Nav, and EQA, either $\mathcal{A}+\mathcal{V}$ or $\mathcal{A}+\mathcal{L}$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Matterport $\uparrow$ <br> (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
</tr>
<tr>
<td style="text-align: left;">$\stackrel{\text { ® }}{\text { a }}$</td>
<td style="text-align: center;">Full Model</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: left;">$\stackrel{\text { ® }}{\text { a }}$</td>
<td style="text-align: center;">$\mathcal{A}$</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathcal{A}+\mathcal{V}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 6}$</td>
<td style="text-align: center;">13.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathcal{A}+\mathcal{L}$</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">$\mathbf{1 3 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">$\Delta$</td>
<td style="text-align: center;">Uni - Base</td>
<td style="text-align: center;">$+14.7$</td>
<td style="text-align: center;">-2.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Navigation results for Matterport when trained using student forcing. Best unimodal in bold; better than reported baseline.
achieves better performance than existing baselines. In Matterport, the $\mathcal{A}+\mathcal{L}$ ablation performs better than the Full Model in unseen environments. The diverse scenes in this simulator may render the vision signal more noisy than helpful in previously unseen environments. The $\mathcal{A}+\mathcal{V}$ model in THOR-Nav and EQA is able to latch onto dataset biases in scene structure to navigate better than chance (for IQA), and the nonhierarchical baseline (in EQA). In EQA, $\mathcal{A}+\mathcal{V}$ also outperforms the Full Model; ${ }^{5}$ the latent information about navigation from questions may be too distant for the model to infer.</p>
<p>The agent with access only to its action history $(\mathcal{A})$ outperforms the baseline agent in Matterport and THOR-Nav environments, suggesting it learns navigation correlations that are not captured by simple random actions (THOR-Nav) or programmatic walks away from the starting position (Matterport). Minimal sensing (which actions are available, $W$ ) coupled with the topological biases in trajectories (Figure 2), help this nearly zero-input agent outperform existing baselines. ${ }^{6}$</p>
<p>Matterport Teacher vs Student forcing With teacher forcing, at each timestep the navigation agent takes the gold-standard action regardless of what action it predicted, meaning it only sees steps along gold-standard trajectories. This paradigm is used to train the navigation agent in THOR-Nav and EQA. Under student forcing, the agent samples the action to take from its predictions, and loss is computed at each time step against the action that would have put the agent on the shortest</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Final distances to targets $\left(\mathbf{d}<em _min="{min" _text="\text">{\mathbf{T}}\right)$ and minimum distance from target achieved along paths $\left(\mathbf{d}</em>$ tied with full model.
path to the goal. Thus, the agent sees more of the scene, but can take more training iterations to learn to move to the goal.}}\right)$ in EQA navigation. Best unimodal in bold; better than reported baseline; *better than full model; ${ }^{\dagger</p>
<p>Table 2 gives the highest validation success rates across all epochs achieved in Matterport by models trained using student forcing. The unimodal ablations show that the Full Model, possibly because with more exploration and more training episodes, is better able to align the vision and language signals, enabling generalization in unseen environments that fails with teacher forcing.</p>
<p>EQA Navigation Variants Table 3 gives the average final distance from the target ( $\mathbf{d}<em _min="{min" _text="\text">{\mathbf{T}}$, used as the metric in Table 1) and average minimum distance from target achieved along the path $\left(\mathbf{d}</em>$ ablation performs best among the ablations, and ties with or outperforms the Full Model in all but one setting, suggesting that the EQA Full Model is not taking advantage of language information under any variant.}}\right)$ during EQA episodes for agents starting 10, 30, and 50 actions away from the target in the EQA navigation task. At 10 actions away, the unimodal ablations tend to outperform the full model on both metrics, possibly due to the shorter length of the episodes (less data to train the joint parameters). The $\mathcal{A}+\mathcal{V</p>
<h3>4.2 Question Answering</h3>
<p>We evaluate our ablation baselines on IQUAD V1 and EQA, reporting top-1 QA accuracy (Table 4) given gold standard navigation information as $\mathcal{V}$. These decoupled QA models do not take in a previous action, so we do not consider $\mathcal{A}$ ONLY ablations for this task.</p>
<p>Unimodal Performance: On IQUAD V1, due to randomization in its construction, model ab-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">IQUAD V1 $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EQA $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
</tr>
<tr>
<td style="text-align: left;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: left;">Full Model</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">19.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathcal{V}$ ONLY</td>
<td style="text-align: center;">$\mathbf{4 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 8}$</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathcal{L}$ ONLY</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Uni - Base</td>
<td style="text-align: center;">$\mathbf{+ 1 . 8}$</td>
<td style="text-align: center;">$\mathbf{+ 1 . 1}$</td>
<td style="text-align: center;">$\mathbf{+ 2 9 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Top-1 QA accuracy. Best unimodal in bold; better than reported baseline.
lations perform nearly at chance. ${ }^{7}$ The $\mathcal{V}$ ONLY model with access to the locations of all scene objects only improves by $2 \%$ over random guessing.</p>
<p>For EQA, single modality models perform significantly better than the majority class baseline. The vision-only model is able to identify salient colors and basic room features that allow it to reduce the likely set of answers to an unknown question. The language only models achieve nearly $50 \%$, suggesting that despite the entropy filtering in Das et al. (2018a) each question has one answer that is as likely as all other answers combined (e.g. $50 \%$ of the answers for What color is the bathtub? are grey, and other examples in Figure 4).</p>
<h2>5 Related Work</h2>
<p>Historically, semantic parsing was used to map natural language instructions to visual navigation in simulation environments (Chen and Mooney, 2011; MacMahon et al., 2006). Modern approaches use neural architectures to map natural language to the (simulated) world and execute actions (Paxton et al., 2019; Chen et al., 2018; Nguyen et al., 2018; Blukis et al., 2018; Fried et al., 2018; Mei et al., 2016). In visual question answering (VQA) (Antol et al., 2015; Hudson and Manning, 2019) and visual commonsense reasoning (VCR) (Zellers et al., 2019), input images are accompanied with natural language questions. Given the question, egocentric QA requires an agent to navigate and interact with the world to gather the relevant information to answer the question. In both cases, end-to-end neural architectures make progress on these tasks.</p>
<p>For language annotations, task design, difficulty, and annotator pay can introduce unintended artifacts which can be exploited by models to "cheat" on otherwise complex tasks (Glockner</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative results on the EQA task. The language only model can pick out the most likely answer for a question. The vision only model finds salient color and room features, but is unaware of the question.
et al., 2018; Poliak et al., 2018). Such issues also occur in multimodal data like VQA (Goyal et al., 2018), where models can answer correctly without looking at the image. In image captioning, work has shown competitive models relying only on nearest-neighbor lookups (Devlin et al., 2015) as well as exposed misalignment between caption relevance and text-based metrics (Rohrbach et al., 2018). Our unimodal ablations of visual navigation and QA benchmarks uncover similar biases, which deep architectures are quick to exploit.</p>
<h2>6 Conclusions</h2>
<p>In this work, we introduce an evaluation framework and perform the missing analysis from several new datasets. While new state-of-the-art models are being introduced for several of these domains (e.g., Matterport: (Ma et al., 2019a; Ke et al., 2019; Wang et al., 2019; Ma et al., 2019b; Tan et al., 2019; Fried et al., 2018), and EQA: (Das et al., 2018b)), they lack informative, individual unimodal ablations (i.e., ablating both language and vision) of the proposed models.</p>
<p>We find a performance gap between baselines used in or derived from the benchmarks examined in this paper and unimodal models, with unimodal models outperforming those baselines across all benchmarks. These unimodal models can even outperform their multimodal counterparts. In light of this, we recommend all future work include unimodal ablations of proposed multimodal models to vet and highlight their learned representations.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by NSF IIS-1524371, 1703166, NRI-1637479, IIS-1338054, 1652052, ONR N00014-13-1-0720, and the DARPA CwC program through ARO (W911NF-15-1-0543).</p>
<h2>References</h2>
<p>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018. Vision-and-Language Navigation: Interpreting visuallygrounded navigation instructions in real environments. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV).</p>
<p>Valts Blukis, Dipendra Misra, Ross A. Knepper, and Yoav Artzi. 2018. Mapping navigation instructions to continuous control actions with position visitation prediction. In Proceedings of the Conference on Robot Learning.</p>
<p>David L. Chen and Raymond J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Howard Chen, Alane Shur, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2018. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In NeurIPS Workshop on Visually Grounded Interaction and Language (ViGIL).</p>
<p>Volkan Cirik, Louis-Philippe Morency, and Taylor Berg-Kirkpatrick. 2018. Visual referring expression recognition: What do systems actually learn? In Proc. of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018a. Embodied Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018b. Neural Modular Control for Embodied Question Answering. In Conference on Robot Learning (CoRL).</p>
<p>Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, and C Lawrence Zitnick. 2015. Exploring nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467.</p>
<p>Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. 2018. Speaker-follower models for vision-and-language navigation. In Neural Information Processing Systems (NeurIPS).</p>
<p>Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI systems with sentences that require simple lexical inferences. In Proc. of the Association for Computational Linguistics (ACL).</p>
<p>Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. 2018. Iqa: Visual question answering in interactive environments. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2018. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. International Journal of Computer Vision (IJCV).</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proc. of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Drew A. Hudson and Christopher D. Manning. 2019. GQA: a new dataset for compositional question answering over real-world images. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Divyansh Kaushik and Zachary C. Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. 2019. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. AI2THOR: An Interactive 3D Environment for Visual AI. arXiv.</p>
<p>Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. 2019a. Self-aware visual-textual cogrounded navigation agent. In International Conference on Learning Representations (ICLR).</p>
<p>Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. 2019b. The regretful agent: Heuristic-aided navigation through progress estimation. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamdhere. 2018. Did the model understand the question? In Proc. of the Association for Computational Linguistics (ACL).</p>
<p>Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. 2018. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Jason M O'Kane and Steven M. LaValle. 2006. On comparing the power of mobile robots. In Robotics: Science and Systems (RSS).</p>
<p>Chris Paxton, Yonatan Bisk, Jesse Thomason, Arunkumar Byravan, and Dieter Fox. 2019. Prospection: Interpretable plans from language by predicting the future. In International Conference on Robotics and Automation (ICRA).</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis Only Baselines in Natural Language Inference. In Joint Conference on Lexical and Computational Semantics (StarSem).</p>
<p>Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learning to navigate unseen environments: Back translation with environmental dropout. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. 2019. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Majority class and chance for IQUAD V1 both achieve $50 \%, 50 \%, 25 \%$ when conditioned on question type; our Baseline model achieves the average of these.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ We report on Matterport-validation since this allows comparing Seen versus Unseen house performance.
${ }^{4}$ For consistency with THOR-Nav and EQA, we here evaluate Matterport using teacher forcing.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
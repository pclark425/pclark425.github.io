<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-446 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-446</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-446</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-232092484</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2103.01373v1.pdf" target="_blank">DeepMerge II: Building Robust Deep Learning Algorithms for Merging Galaxy Identification Across Domains</a></p>
                <p><strong>Paper Abstract:</strong> In astronomy, neural networks are often trained on simulation data with the prospect of being used on telescope observations. Unfortunately, training a model on simulation data and then applying it to instrument data leads to a substantial and potentially even detrimental decrease in model accuracy on the new target dataset. Simulated and instrument data represent different data domains, and for an algorithm to work in both, domain-invariant learning is necessary. Here we employ domain adaptation techniques$-$ Maximum Mean Discrepancy (MMD) as an additional transfer loss and Domain Adversarial Neural Networks (DANNs)$-$ and demonstrate their viability to extract domain-invariant features within the astronomical context of classifying merging and non-merging galaxies. Additionally, we explore the use of Fisher loss and entropy minimization to enforce better in-domain class discriminability. We show that the addition of each domain adaptation technique improves the performance of a classifier when compared to conventional deep learning algorithms. We demonstrate this on two examples: between two Illustris-1 simulated datasets of distant merging galaxies, and between Illustris-1 simulated data of nearby merging galaxies and observed data from the Sloan Digital Sky Survey. The use of domain adaptation techniques in our experiments leads to an increase of target domain classification accuracy of up to ${\sim}20\%$. With further development, these techniques will allow astronomers to successfully implement neural network models trained on simulation data to efficiently detect and study astrophysical objects in current and future large-scale astronomical surveys.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e446.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e446.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kernel-based nonparametric statistic that measures the distance between mean embeddings of two probability distributions in a reproducing kernel Hilbert space (RKHS); used here as a transfer loss to align latent feature distributions of source (simulated) and target (noisy/observational) galaxy image domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Maximum Mean Discrepancy (MMD) domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute an unbiased estimator of the squared RKHS distance between mean embeddings of the source and target distributions using a linear combination of Gaussian RBF kernels; use this quantity as a transfer loss term L_TL,mmd added to the classifier total loss so that backpropagation minimizes domain discrepancy in latent feature space and draws source and target feature distributions together.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (domain adaptation loss)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / domain adaptation / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — automated classification of merging vs non-merging galaxy images (simulated → simulated noisy; simulated → real)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Implemented MMD as an additional transfer loss inside CNN training; used a linear combination of Gaussian RBF kernels (multi-bandwidth) as the kernel; applied the unbiased estimator discretized over minibatches of latent features; combined MMD with other loss terms (cross-entropy, Fisher trace-ratio loss, entropy-minimization) and tuned a TL weight; used MMD on features extracted by two architectures (DeepMerge and ResNet18).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful in simulation-to-simulation experiments (e.g., DeepMerge: target accuracy improved from ~58% to ~77%; ResNet18: target improved from ~60% to ~75% with MMD variants). In the harder simulated-to-real task, MMD alone gave only small improvement (target acc. ~0.53 vs baseline 0.50) and failed unless combined with transfer learning; with transfer learning + MMD target accuracy reached ~0.69 (≈19% increase over baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Large domain discrepancy (simulated vs real) and small target dataset sizes limited effectiveness; sensitive to hyperparameter choices (kernel widths, TL weight); interactions with other loss terms can be complex; requires careful batch composition and sufficient representative target samples.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Using multiple RBF kernels (multi-bandwidth) to cover scales; integrating with Fisher loss and entropy minimization when appropriate; pretraining/transfer learning from a related simulation-trained model; preprocessing simulations to include observational effects (PSF, noise, dust) to reduce domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires ability to extract latent features from networks, labeled source and unlabeled target data during training, compute resources for kernel evaluations and backpropagation (GPUs), and hyperparameter search to set TL weight and kernel parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately generalizable to other astronomy cross-domain classification problems, but success depends strongly on domain similarity, quantity/variety of target data, and hyperparameter tuning; likely applicable to other scientific domains where distributions can be embedded in RKHS.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps; computational method; theoretical principles</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e446.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DANN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Adversarial Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial domain adaptation architecture that uses a feature extractor, a label predictor, and a domain classifier with a gradient reversal layer so the feature extractor learns representations that are discriminative for the task yet invariant to domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain adversarial training (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train a joint network comprising (F) feature extractor, (L) label predictor, and (D) domain classifier; add a gradient reversal layer before D so that optimizing the overall loss (L_class - λ L_domain) forces F to produce features that confuse the domain classifier while still allowing L to correctly predict labels, thereby learning domain-invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / domain adaptation method</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / domain adaptation / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — galaxy merger classification across simulated and observational image domains</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Specified a three-layer domain classifier sized to match extracted feature dimensionality (first layer = feature dim, second = 1024 with ReLU + dropout, third = 1 with sigmoid); tuned adversarial weight TL (λ) and domain-classifier learning-rate multiplier to avoid mode collapse; combined adversarial loss with Fisher loss and entropy minimization in some experiments; used early stopping and differential learning rates for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Partially successful: in simulation-to-simulation experiments DANN produced large target accuracy gains (DeepMerge target up to ~79%, an increase of ≈21% over no-DA baseline); in simulation-to-real experiments adversarial training failed to improve target performance unless combined with additional strategies (transfer learning), i.e., unsuccessful on the hardest cross-domain task without further intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mode collapse of domain classifier (predicting single domain) was common without careful objective design and LR tuning; sensitivity to λ and domain-classifier LR multiplier; overfitting in large architectures (ResNet18) reduced generalization; negative transfer when domains were substantially different.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Gradient reversal layer to implement adversarial objective; tuning of λ and separate LR multipliers; combination with Fisher loss/entropy minimization when appropriate; careful early stopping and model checkpointing.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of unlabeled target samples during training; label availability in source; ability to build and train multi-branch networks; hyperparameter tuning infrastructure and GPU compute; monitoring for domain-classifier collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable across many domain adaptation tasks but requires careful tuning; effectiveness drops for large domain gaps and limited target data; generalizable to other scientific image-classification transfers with similar caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps; instrumental/technical skills</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e446.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fisher loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fisher trace-ratio loss</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A loss term that encourages intra-class compactness and inter-class separability in latent feature space by minimizing the trace ratio of within-class scatter to between-class scatter (tr(S_w) / tr(S_b)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Fisher loss (trace-ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute within-class scatter S_w and between-class scatter S_b over labeled source features, form the trace ratio L_FL = w * tr(S_w) / (b * tr(S_b)) with weights w and b to scale contributions; add this loss to the training objective so that class centroids become compact and well-separated in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / loss function (feature discriminability)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / deep learning classification</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — to improve class discriminability of galaxy merger classifiers across domains</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Implemented trace-ratio Fisher loss as an additive term in total loss with tunable FL weight and separate w and b scaling of S_w and S_b; used only on labeled source domain; combined with MMD or DANN and entropy minimization; tuned w and b values (examples in experiments: w on order 0.01 or 1.0 depending on setup).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Conditionally helpful: improved source-domain class compactness and, in some cases, aided target-domain adaptation (notably when paired with MMD in ResNet18 experiments); however, effects varied by architecture — in some combinations it did not help or degraded target performance, indicating sensitivity to hyperparameters and architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires source labels (cannot be applied to unlabeled target); interactions with other loss terms are complex and hyperparameter-sensitive; may worsen results for some architectures if not tuned properly.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Creates compact, well-separated source clusters that make transfer objectives (MMD or DANN) easier to achieve; complements entropy minimization for target alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Accurate labels for source domain, ability to compute per-class centroids and scatter matrices (batched computation), hyperparameter tuning (w, b, FL weight).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to other DA problems where source labels exist, but effect size and stability depend on dataset size, class balance, architecture, and other losses.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps; theoretical principles</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e446.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy minimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy minimization loss (semi‑supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised loss that encourages low-entropy (confident) class predictions on unlabeled target samples, thereby pushing target examples toward a small set of compact, discriminative class regions learned from source data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Entropy minimization</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute Shannon entropy of the softmax class outputs for each unlabeled target sample L_EM = - sum_c p(c|h) log p(c|h) and add it (weighted by EM) to the total loss to encourage the network to make confident (low-entropy) predictions on target data, aiding alignment to source classes.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / loss function (semi-supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / semi-supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — alignment of unlabeled observational galaxy images to simulated class structure</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used as an additive loss weighted by EM (e.g., values like 0.05 or 0.0001 depending on experiment) combined with MMD or DANN and Fisher loss; applied only to unlabeled target features as part of total loss L_TOT.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Partially successful: in some experiments (e.g., combined with MMD and Fisher for ResNet18) it helped improve target performance, whereas in other configurations it had little or negative effect — overall outcome sensitive to weighting and architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>If domain mismatch is large, entropy minimization can push target samples to wrong source class clusters (risk of confirmation bias); requires careful weighting and complementary losses to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Works well when source clusters are compact (e.g., due to Fisher loss) and when domain discrepancy is not extreme; complements Fisher loss.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Unlabeled target samples available during training, stability via hyperparameter tuning for EM, and compatible batch construction.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Widely applicable as a semi-supervised regularizer across domains, but effectiveness strongly depends on domain gap and prior source cluster quality.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles; explicit procedural steps</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e446.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer learning (pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer learning from pre-trained models (simulation and ImageNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Loading weights from a model pre-trained on a related dataset (either ImageNet or a simulation-to-simulation trained model) and fine-tuning on the target task to provide a better initialization and mitigate small-data and domain-shift issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Transfer learning (pretrained weights from simulation-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Initialize the classifier with weights from a network previously trained on a related dataset (here: a DeepMerge model trained on simulation-to-simulation experiments) and then continue training on the new source+target setup with domain adaptation (MMD); evaluate strategies of freezing convolutional & batch-norm layers vs. full fine-tuning and found full fine-tuning worked better for this small astrophysical network.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / training protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision (ImageNet pretraining) and simulation domain (sim-to-sim trained models)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — simulated-to-real galaxy classification</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (pretraining on domain-similar data instead of general natural images)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used pretrained weights from successful simulation-to-simulation DeepMerge runs rather than ImageNet in order to provide features more relevant to galaxy morphology; tried both freezing lower layers and fine-tuning all weights, with full fine-tuning producing better results; combined with MMD transfer loss and hyperparameter search (LR, weight decay).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful: critical to enabling DA on the simulated-to-real problem — with transfer learning + MMD the target accuracy reached ~69% compared with 50% baseline (no DA), i.e., a ~19% absolute improvement; MMD without transfer learning failed to meaningfully improve the real-target accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Small number of real labeled post-mergers, large domain gap, and need to choose whether and which layers to freeze; potential for negative transfer if pretrained features are not sufficiently relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Pretraining on a simulation-trained model that is closer in domain than ImageNet; data augmentation to expand limited classes; careful hyperparameter tuning and allowing full fine-tuning enabled adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Availability of pretrained checkpoints (from sim-to-sim training), matching network architectures, compute resources for fine-tuning, and augmentation strategies for limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable approach — pretraining on domain-similar synthetic data is likely beneficial for many scientific imaging tasks where ImageNet features are less appropriate; effectiveness depends on similarity between pretraining and target domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills; tacit know-how (deciding freeze vs fine-tune)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e446.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>t-SNE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>t-Distributed Stochastic Neighbor Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonlinear dimensionality reduction and visualization method used to project high-dimensional learned features into 2D to inspect class separation and domain overlap during/after training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>t-SNE feature visualization</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Project latent features from the last layer into 2D using t-SNE to visualize how source and target features and class clusters move during training; used to qualitatively assess domain overlap and the impact of domain adaptation losses.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data analysis technique / visualization</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / data visualization</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — diagnostic interpretability for domain adaptation of galaxy classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Integrated t-SNE plotting into training pipeline to render epoch-by-epoch latent space projections; authors caution about t-SNE parameter dependence and interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Useful and successful as an interpretability/diagnostic tool: showed that DA (MMD/DANN) causes source and target feature distributions to overlap while Fisher+entropy produce better class separation.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>t-SNE plots are parameter sensitive and non-linear (cannot quantitatively compare cluster sizes); computational cost for repeated visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of latent feature vectors from CNNs and standard implementations of t-SNE; usefulness for qualitative model debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Choice of t-SNE hyperparameters and computational budget for multiple projections; interpretive care.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Widely generalizable as a visualization tool across scientific ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps; interpretive frameworks</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e446.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grad-CAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-weighted Class Activation Mapping (Grad-CAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to produce visual explanations by computing class-specific importance maps from gradients of the output w.r.t. last convolutional activation maps, used here to show which image regions the network attends to before and after domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Grad-CAM saliency mapping</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute global-average-pooled gradients of the class score w.r.t. activation maps at the last convolutional layer to obtain channel weights, form a weighted sum of activation maps and apply ReLU to produce a coarse heatmap that highlights image regions contributing to a particular class decision; compare Grad-CAMs for models trained with and without DA to interpret attention shifts (e.g., from noise to central galaxy regions).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data analysis technique / interpretability method</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>machine learning / model interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — interpreting CNN-based galaxy merger classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied standard Grad-CAM to DeepMerge (avoided ResNet18 low-res maps for interpretability); used logarithmic color mapping on some source images to enhance visibility; compared CAMs across DA conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful: demonstrated that without DA networks often attend to noise in target images while DA-trained models focus on central galaxy features visible in both domains; supported claims about what DA changes in representations.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Lower-resolution feature maps in deeper networks (ResNet18) made CAMs harder to interpret; Grad-CAMs provide coarse localization only.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Easy access to intermediate activations and gradients in PyTorch; helpful for qualitative model validation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to model internals (activations, gradients), and target/source images for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Broadly applicable for interpretability in scientific imaging ML.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills; interpretive frameworks</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e446.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observational realism augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-to-observation realism injection (PSF, noise, dust, re-binning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedures that apply telescope/instrument observational effects to simulated images (PSF convolution, sky shot noise, dust attenuation models, re-binning to instrument pixel scales) to reduce domain discrepancy between synthetic and real images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Observational realism augmentation of simulated images</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Modify simulated galaxy images to mimic specific instrument characteristics: convolve with instrument PSF (HST or Gaussian FWHM appropriate for SDSS), add random sky shot (Gaussian) noise to achieve target limiting surface brightness or SNR, apply dust slab attenuation based on gas/metal density along the line of sight, re-bin to instrument pixel scale, and order/align filters to match observational RGB channels; use these realistic simulations as source or target in DA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data preprocessing / experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>observational astronomy / instrument modeling</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>computational astrophysics / machine learning (simulated image datasets used for training classifiers applied to real observations)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>For Illustris z=2 experiments: applied PSF and added random sky shot noise to simulate HST; specified 5σ limiting surface brightness 25 mag/arcsec^2. For Illustris z=0→SDSS experiments: applied dust slab model, convolved with Gaussian PSF (FWHM=1 kpc), re-binned to 0.24 kpc pixel scale (1 arcsec), added pixelwise Gaussian noise to reach SNR~25, and aligned filters to (g,r,i) order; used augmentation (mirroring, rotations, random rotations/zoom) to balance small merger class.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Partially successful: essential to reduce domain gap and make domain adaptation feasible; despite these realism adjustments, substantial residual differences remained (especially sim-to-real), necessitating transfer learning and more sophisticated DA; in sim-to-sim case (noise injection only) DA techniques successfully recovered up to ~20% target accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Even realistic instrument effect injection may not capture crowding, background object contamination, or subtle observational biases; small real sample sizes and differing morphological distributions (e.g., post-merger prevalence) limit complete alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Close modeling of instrument PSF, noise levels, and dust physics enabled better overlap of features across domains and made DA methods more effective.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to simulation physical properties (gas/metal densities), knowledge of instrument specifications (PSF, pixel scale, noise characteristics), and coding pipeline to inject these effects reproducibly.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Highly generalizable across astrophysical simulation→observation transfers; recommended as a prerequisite for successful domain adaptation in scientific imaging.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills; explicit procedural steps</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e446.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e446.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepHyper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepHyper hyperparameter optimization framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel asynchronous model-based search framework used to perform hyperparameter optimization (mixed integer, categorical, continuous) for training DA networks, including objectives designed to avoid domain-classifier collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Hyperparameter optimization with DeepHyper</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Run parallel asynchronous searches over learning rates, weight decay, TL weights, Fisher/entropy weights, cycle lengths, and domain-classifier LR multipliers; use specific objective functions (e.g., minimize L_TOT for MMD searches and minimize L_CL*(1+ΔA_s+ΔA_t) for adversarial training to penalize domain-classifier mode collapse) to find robust training hyperparameters for DA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / optimization protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>HPC-driven machine learning optimization</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>astronomy — tuning domain-adaptive galaxy classification models</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Customized objective for adversarial training to penalize deviations of domain-classifier accuracies from 0.5 (confusion) to reduce mode collapse; searched across model- and experiment-specific ranges (different settings for DeepMerge vs ResNet18 and sim-to-sim vs sim-to-real).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Helpful: enabled systematic hyperparameter exploration and informed the parameter choices used across experiments; however, authors still observed sensitivity and acknowledged hyperparameter search was imperfect and may explain some mixed results.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Resource requirements for extensive searches; remaining sensitivity of results to hyperparameters and per-experiment idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Parallel search capability and flexible mixed-type search space allowed practical tuning of many interacting hyperparameters including adversarial stability terms.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to compute cluster/GPUs, integration of training code with DeepHyper, and design of appropriate objective functions for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable to other scientific ML tasks requiring complex hyperparameter tuning, particularly when adversarial objectives are present.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills; explicit procedural steps</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain-Adversarial Training of Neural Networks <em>(Rating: 2)</em></li>
                <li>A Kernel Two-Sample Test <em>(Rating: 2)</em></li>
                <li>Semi-supervised learning by entropy minimization <em>(Rating: 2)</em></li>
                <li>A Comprehensive Survey on Domain Adaptation for Visual Applications <em>(Rating: 2)</em></li>
                <li>Semi‑supervised and transfer learning for astronomical image classification (Ackermann et al. 2018) <em>(Rating: 1)</em></li>
                <li>Practical considerations for inserting simulated galaxies into realistic skies (Bottrell et al. 2019) <em>(Rating: 1)</em></li>
                <li>Class-aware Contrastive Domain Discrepancy networks (Kang et al. 2019) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-446",
    "paper_id": "paper-232092484",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "MMD",
            "name_full": "Maximum Mean Discrepancy",
            "brief_description": "A kernel-based nonparametric statistic that measures the distance between mean embeddings of two probability distributions in a reproducing kernel Hilbert space (RKHS); used here as a transfer loss to align latent feature distributions of source (simulated) and target (noisy/observational) galaxy image domains.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Maximum Mean Discrepancy (MMD) domain adaptation",
            "procedure_description": "Compute an unbiased estimator of the squared RKHS distance between mean embeddings of the source and target distributions using a linear combination of Gaussian RBF kernels; use this quantity as a transfer loss term L_TL,mmd added to the classifier total loss so that backpropagation minimizes domain discrepancy in latent feature space and draws source and target feature distributions together.",
            "procedure_type": "computational method / data analysis technique (domain adaptation loss)",
            "source_domain": "machine learning / domain adaptation / computer vision",
            "target_domain": "astronomy — automated classification of merging vs non-merging galaxy images (simulated → simulated noisy; simulated → real)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Implemented MMD as an additional transfer loss inside CNN training; used a linear combination of Gaussian RBF kernels (multi-bandwidth) as the kernel; applied the unbiased estimator discretized over minibatches of latent features; combined MMD with other loss terms (cross-entropy, Fisher trace-ratio loss, entropy-minimization) and tuned a TL weight; used MMD on features extracted by two architectures (DeepMerge and ResNet18).",
            "transfer_success": "Successful in simulation-to-simulation experiments (e.g., DeepMerge: target accuracy improved from ~58% to ~77%; ResNet18: target improved from ~60% to ~75% with MMD variants). In the harder simulated-to-real task, MMD alone gave only small improvement (target acc. ~0.53 vs baseline 0.50) and failed unless combined with transfer learning; with transfer learning + MMD target accuracy reached ~0.69 (≈19% increase over baseline).",
            "barriers_encountered": "Large domain discrepancy (simulated vs real) and small target dataset sizes limited effectiveness; sensitive to hyperparameter choices (kernel widths, TL weight); interactions with other loss terms can be complex; requires careful batch composition and sufficient representative target samples.",
            "facilitating_factors": "Using multiple RBF kernels (multi-bandwidth) to cover scales; integrating with Fisher loss and entropy minimization when appropriate; pretraining/transfer learning from a related simulation-trained model; preprocessing simulations to include observational effects (PSF, noise, dust) to reduce domain gap.",
            "contextual_requirements": "Requires ability to extract latent features from networks, labeled source and unlabeled target data during training, compute resources for kernel evaluations and backpropagation (GPUs), and hyperparameter search to set TL weight and kernel parameters.",
            "generalizability": "Moderately generalizable to other astronomy cross-domain classification problems, but success depends strongly on domain similarity, quantity/variety of target data, and hyperparameter tuning; likely applicable to other scientific domains where distributions can be embedded in RKHS.",
            "knowledge_type": "explicit procedural steps; computational method; theoretical principles",
            "uuid": "e446.0"
        },
        {
            "name_short": "DANN",
            "name_full": "Domain Adversarial Neural Network",
            "brief_description": "An adversarial domain adaptation architecture that uses a feature extractor, a label predictor, and a domain classifier with a gradient reversal layer so the feature extractor learns representations that are discriminative for the task yet invariant to domain.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Domain adversarial training (DANN)",
            "procedure_description": "Train a joint network comprising (F) feature extractor, (L) label predictor, and (D) domain classifier; add a gradient reversal layer before D so that optimizing the overall loss (L_class - λ L_domain) forces F to produce features that confuse the domain classifier while still allowing L to correctly predict labels, thereby learning domain-invariant features.",
            "procedure_type": "computational method / domain adaptation method",
            "source_domain": "machine learning / domain adaptation / computer vision",
            "target_domain": "astronomy — galaxy merger classification across simulated and observational image domains",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Specified a three-layer domain classifier sized to match extracted feature dimensionality (first layer = feature dim, second = 1024 with ReLU + dropout, third = 1 with sigmoid); tuned adversarial weight TL (λ) and domain-classifier learning-rate multiplier to avoid mode collapse; combined adversarial loss with Fisher loss and entropy minimization in some experiments; used early stopping and differential learning rates for stability.",
            "transfer_success": "Partially successful: in simulation-to-simulation experiments DANN produced large target accuracy gains (DeepMerge target up to ~79%, an increase of ≈21% over no-DA baseline); in simulation-to-real experiments adversarial training failed to improve target performance unless combined with additional strategies (transfer learning), i.e., unsuccessful on the hardest cross-domain task without further intervention.",
            "barriers_encountered": "Mode collapse of domain classifier (predicting single domain) was common without careful objective design and LR tuning; sensitivity to λ and domain-classifier LR multiplier; overfitting in large architectures (ResNet18) reduced generalization; negative transfer when domains were substantially different.",
            "facilitating_factors": "Gradient reversal layer to implement adversarial objective; tuning of λ and separate LR multipliers; combination with Fisher loss/entropy minimization when appropriate; careful early stopping and model checkpointing.",
            "contextual_requirements": "Availability of unlabeled target samples during training; label availability in source; ability to build and train multi-branch networks; hyperparameter tuning infrastructure and GPU compute; monitoring for domain-classifier collapse.",
            "generalizability": "Applicable across many domain adaptation tasks but requires careful tuning; effectiveness drops for large domain gaps and limited target data; generalizable to other scientific image-classification transfers with similar caveats.",
            "knowledge_type": "explicit procedural steps; instrumental/technical skills",
            "uuid": "e446.1"
        },
        {
            "name_short": "Fisher loss",
            "name_full": "Fisher trace-ratio loss",
            "brief_description": "A loss term that encourages intra-class compactness and inter-class separability in latent feature space by minimizing the trace ratio of within-class scatter to between-class scatter (tr(S_w) / tr(S_b)).",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Fisher loss (trace-ratio)",
            "procedure_description": "Compute within-class scatter S_w and between-class scatter S_b over labeled source features, form the trace ratio L_FL = w * tr(S_w) / (b * tr(S_b)) with weights w and b to scale contributions; add this loss to the training objective so that class centroids become compact and well-separated in latent space.",
            "procedure_type": "computational method / loss function (feature discriminability)",
            "source_domain": "machine learning / deep learning classification",
            "target_domain": "astronomy — to improve class discriminability of galaxy merger classifiers across domains",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Implemented trace-ratio Fisher loss as an additive term in total loss with tunable FL weight and separate w and b scaling of S_w and S_b; used only on labeled source domain; combined with MMD or DANN and entropy minimization; tuned w and b values (examples in experiments: w on order 0.01 or 1.0 depending on setup).",
            "transfer_success": "Conditionally helpful: improved source-domain class compactness and, in some cases, aided target-domain adaptation (notably when paired with MMD in ResNet18 experiments); however, effects varied by architecture — in some combinations it did not help or degraded target performance, indicating sensitivity to hyperparameters and architecture.",
            "barriers_encountered": "Requires source labels (cannot be applied to unlabeled target); interactions with other loss terms are complex and hyperparameter-sensitive; may worsen results for some architectures if not tuned properly.",
            "facilitating_factors": "Creates compact, well-separated source clusters that make transfer objectives (MMD or DANN) easier to achieve; complements entropy minimization for target alignment.",
            "contextual_requirements": "Accurate labels for source domain, ability to compute per-class centroids and scatter matrices (batched computation), hyperparameter tuning (w, b, FL weight).",
            "generalizability": "Generalizable to other DA problems where source labels exist, but effect size and stability depend on dataset size, class balance, architecture, and other losses.",
            "knowledge_type": "explicit procedural steps; theoretical principles",
            "uuid": "e446.2"
        },
        {
            "name_short": "Entropy minimization",
            "name_full": "Entropy minimization loss (semi‑supervised)",
            "brief_description": "An unsupervised loss that encourages low-entropy (confident) class predictions on unlabeled target samples, thereby pushing target examples toward a small set of compact, discriminative class regions learned from source data.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Entropy minimization",
            "procedure_description": "Compute Shannon entropy of the softmax class outputs for each unlabeled target sample L_EM = - sum_c p(c|h) log p(c|h) and add it (weighted by EM) to the total loss to encourage the network to make confident (low-entropy) predictions on target data, aiding alignment to source classes.",
            "procedure_type": "computational method / loss function (semi-supervised)",
            "source_domain": "machine learning / semi-supervised learning",
            "target_domain": "astronomy — alignment of unlabeled observational galaxy images to simulated class structure",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Used as an additive loss weighted by EM (e.g., values like 0.05 or 0.0001 depending on experiment) combined with MMD or DANN and Fisher loss; applied only to unlabeled target features as part of total loss L_TOT.",
            "transfer_success": "Partially successful: in some experiments (e.g., combined with MMD and Fisher for ResNet18) it helped improve target performance, whereas in other configurations it had little or negative effect — overall outcome sensitive to weighting and architecture.",
            "barriers_encountered": "If domain mismatch is large, entropy minimization can push target samples to wrong source class clusters (risk of confirmation bias); requires careful weighting and complementary losses to be effective.",
            "facilitating_factors": "Works well when source clusters are compact (e.g., due to Fisher loss) and when domain discrepancy is not extreme; complements Fisher loss.",
            "contextual_requirements": "Unlabeled target samples available during training, stability via hyperparameter tuning for EM, and compatible batch construction.",
            "generalizability": "Widely applicable as a semi-supervised regularizer across domains, but effectiveness strongly depends on domain gap and prior source cluster quality.",
            "knowledge_type": "theoretical principles; explicit procedural steps",
            "uuid": "e446.3"
        },
        {
            "name_short": "Transfer learning (pretraining)",
            "name_full": "Transfer learning from pre-trained models (simulation and ImageNet)",
            "brief_description": "Loading weights from a model pre-trained on a related dataset (either ImageNet or a simulation-to-simulation trained model) and fine-tuning on the target task to provide a better initialization and mitigate small-data and domain-shift issues.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Transfer learning (pretrained weights from simulation-trained model)",
            "procedure_description": "Initialize the classifier with weights from a network previously trained on a related dataset (here: a DeepMerge model trained on simulation-to-simulation experiments) and then continue training on the new source+target setup with domain adaptation (MMD); evaluate strategies of freezing convolutional & batch-norm layers vs. full fine-tuning and found full fine-tuning worked better for this small astrophysical network.",
            "procedure_type": "computational method / training protocol",
            "source_domain": "computer vision (ImageNet pretraining) and simulation domain (sim-to-sim trained models)",
            "target_domain": "astronomy — simulated-to-real galaxy classification",
            "transfer_type": "adapted/modified for new context (pretraining on domain-similar data instead of general natural images)",
            "modifications_made": "Used pretrained weights from successful simulation-to-simulation DeepMerge runs rather than ImageNet in order to provide features more relevant to galaxy morphology; tried both freezing lower layers and fine-tuning all weights, with full fine-tuning producing better results; combined with MMD transfer loss and hyperparameter search (LR, weight decay).",
            "transfer_success": "Successful: critical to enabling DA on the simulated-to-real problem — with transfer learning + MMD the target accuracy reached ~69% compared with 50% baseline (no DA), i.e., a ~19% absolute improvement; MMD without transfer learning failed to meaningfully improve the real-target accuracy.",
            "barriers_encountered": "Small number of real labeled post-mergers, large domain gap, and need to choose whether and which layers to freeze; potential for negative transfer if pretrained features are not sufficiently relevant.",
            "facilitating_factors": "Pretraining on a simulation-trained model that is closer in domain than ImageNet; data augmentation to expand limited classes; careful hyperparameter tuning and allowing full fine-tuning enabled adaptation.",
            "contextual_requirements": "Availability of pretrained checkpoints (from sim-to-sim training), matching network architectures, compute resources for fine-tuning, and augmentation strategies for limited data.",
            "generalizability": "Generalizable approach — pretraining on domain-similar synthetic data is likely beneficial for many scientific imaging tasks where ImageNet features are less appropriate; effectiveness depends on similarity between pretraining and target domains.",
            "knowledge_type": "instrumental/technical skills; tacit know-how (deciding freeze vs fine-tune)",
            "uuid": "e446.4"
        },
        {
            "name_short": "t-SNE",
            "name_full": "t-Distributed Stochastic Neighbor Embedding",
            "brief_description": "A nonlinear dimensionality reduction and visualization method used to project high-dimensional learned features into 2D to inspect class separation and domain overlap during/after training.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "t-SNE feature visualization",
            "procedure_description": "Project latent features from the last layer into 2D using t-SNE to visualize how source and target features and class clusters move during training; used to qualitatively assess domain overlap and the impact of domain adaptation losses.",
            "procedure_type": "data analysis technique / visualization",
            "source_domain": "machine learning / data visualization",
            "target_domain": "astronomy — diagnostic interpretability for domain adaptation of galaxy classifiers",
            "transfer_type": "direct application without modification",
            "modifications_made": "Integrated t-SNE plotting into training pipeline to render epoch-by-epoch latent space projections; authors caution about t-SNE parameter dependence and interpretation.",
            "transfer_success": "Useful and successful as an interpretability/diagnostic tool: showed that DA (MMD/DANN) causes source and target feature distributions to overlap while Fisher+entropy produce better class separation.",
            "barriers_encountered": "t-SNE plots are parameter sensitive and non-linear (cannot quantitatively compare cluster sizes); computational cost for repeated visualizations.",
            "facilitating_factors": "Availability of latent feature vectors from CNNs and standard implementations of t-SNE; usefulness for qualitative model debugging.",
            "contextual_requirements": "Choice of t-SNE hyperparameters and computational budget for multiple projections; interpretive care.",
            "generalizability": "Widely generalizable as a visualization tool across scientific ML tasks.",
            "knowledge_type": "explicit procedural steps; interpretive frameworks",
            "uuid": "e446.5"
        },
        {
            "name_short": "Grad-CAM",
            "name_full": "Gradient-weighted Class Activation Mapping (Grad-CAM)",
            "brief_description": "A technique to produce visual explanations by computing class-specific importance maps from gradients of the output w.r.t. last convolutional activation maps, used here to show which image regions the network attends to before and after domain adaptation.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Grad-CAM saliency mapping",
            "procedure_description": "Compute global-average-pooled gradients of the class score w.r.t. activation maps at the last convolutional layer to obtain channel weights, form a weighted sum of activation maps and apply ReLU to produce a coarse heatmap that highlights image regions contributing to a particular class decision; compare Grad-CAMs for models trained with and without DA to interpret attention shifts (e.g., from noise to central galaxy regions).",
            "procedure_type": "data analysis technique / interpretability method",
            "source_domain": "machine learning / model interpretability",
            "target_domain": "astronomy — interpreting CNN-based galaxy merger classifiers",
            "transfer_type": "direct application without modification",
            "modifications_made": "Applied standard Grad-CAM to DeepMerge (avoided ResNet18 low-res maps for interpretability); used logarithmic color mapping on some source images to enhance visibility; compared CAMs across DA conditions.",
            "transfer_success": "Successful: demonstrated that without DA networks often attend to noise in target images while DA-trained models focus on central galaxy features visible in both domains; supported claims about what DA changes in representations.",
            "barriers_encountered": "Lower-resolution feature maps in deeper networks (ResNet18) made CAMs harder to interpret; Grad-CAMs provide coarse localization only.",
            "facilitating_factors": "Easy access to intermediate activations and gradients in PyTorch; helpful for qualitative model validation.",
            "contextual_requirements": "Access to model internals (activations, gradients), and target/source images for comparison.",
            "generalizability": "Broadly applicable for interpretability in scientific imaging ML.",
            "knowledge_type": "instrumental/technical skills; interpretive frameworks",
            "uuid": "e446.6"
        },
        {
            "name_short": "Observational realism augmentation",
            "name_full": "Simulation-to-observation realism injection (PSF, noise, dust, re-binning)",
            "brief_description": "Procedures that apply telescope/instrument observational effects to simulated images (PSF convolution, sky shot noise, dust attenuation models, re-binning to instrument pixel scales) to reduce domain discrepancy between synthetic and real images.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Observational realism augmentation of simulated images",
            "procedure_description": "Modify simulated galaxy images to mimic specific instrument characteristics: convolve with instrument PSF (HST or Gaussian FWHM appropriate for SDSS), add random sky shot (Gaussian) noise to achieve target limiting surface brightness or SNR, apply dust slab attenuation based on gas/metal density along the line of sight, re-bin to instrument pixel scale, and order/align filters to match observational RGB channels; use these realistic simulations as source or target in DA experiments.",
            "procedure_type": "data preprocessing / experimental protocol",
            "source_domain": "observational astronomy / instrument modeling",
            "target_domain": "computational astrophysics / machine learning (simulated image datasets used for training classifiers applied to real observations)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "For Illustris z=2 experiments: applied PSF and added random sky shot noise to simulate HST; specified 5σ limiting surface brightness 25 mag/arcsec^2. For Illustris z=0→SDSS experiments: applied dust slab model, convolved with Gaussian PSF (FWHM=1 kpc), re-binned to 0.24 kpc pixel scale (1 arcsec), added pixelwise Gaussian noise to reach SNR~25, and aligned filters to (g,r,i) order; used augmentation (mirroring, rotations, random rotations/zoom) to balance small merger class.",
            "transfer_success": "Partially successful: essential to reduce domain gap and make domain adaptation feasible; despite these realism adjustments, substantial residual differences remained (especially sim-to-real), necessitating transfer learning and more sophisticated DA; in sim-to-sim case (noise injection only) DA techniques successfully recovered up to ~20% target accuracy improvements.",
            "barriers_encountered": "Even realistic instrument effect injection may not capture crowding, background object contamination, or subtle observational biases; small real sample sizes and differing morphological distributions (e.g., post-merger prevalence) limit complete alignment.",
            "facilitating_factors": "Close modeling of instrument PSF, noise levels, and dust physics enabled better overlap of features across domains and made DA methods more effective.",
            "contextual_requirements": "Access to simulation physical properties (gas/metal densities), knowledge of instrument specifications (PSF, pixel scale, noise characteristics), and coding pipeline to inject these effects reproducibly.",
            "generalizability": "Highly generalizable across astrophysical simulation→observation transfers; recommended as a prerequisite for successful domain adaptation in scientific imaging.",
            "knowledge_type": "instrumental/technical skills; explicit procedural steps",
            "uuid": "e446.7"
        },
        {
            "name_short": "DeepHyper",
            "name_full": "DeepHyper hyperparameter optimization framework",
            "brief_description": "A parallel asynchronous model-based search framework used to perform hyperparameter optimization (mixed integer, categorical, continuous) for training DA networks, including objectives designed to avoid domain-classifier collapse.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Hyperparameter optimization with DeepHyper",
            "procedure_description": "Run parallel asynchronous searches over learning rates, weight decay, TL weights, Fisher/entropy weights, cycle lengths, and domain-classifier LR multipliers; use specific objective functions (e.g., minimize L_TOT for MMD searches and minimize L_CL*(1+ΔA_s+ΔA_t) for adversarial training to penalize domain-classifier mode collapse) to find robust training hyperparameters for DA experiments.",
            "procedure_type": "computational method / optimization protocol",
            "source_domain": "HPC-driven machine learning optimization",
            "target_domain": "astronomy — tuning domain-adaptive galaxy classification models",
            "transfer_type": "direct application without modification",
            "modifications_made": "Customized objective for adversarial training to penalize deviations of domain-classifier accuracies from 0.5 (confusion) to reduce mode collapse; searched across model- and experiment-specific ranges (different settings for DeepMerge vs ResNet18 and sim-to-sim vs sim-to-real).",
            "transfer_success": "Helpful: enabled systematic hyperparameter exploration and informed the parameter choices used across experiments; however, authors still observed sensitivity and acknowledged hyperparameter search was imperfect and may explain some mixed results.",
            "barriers_encountered": "Resource requirements for extensive searches; remaining sensitivity of results to hyperparameters and per-experiment idiosyncrasies.",
            "facilitating_factors": "Parallel search capability and flexible mixed-type search space allowed practical tuning of many interacting hyperparameters including adversarial stability terms.",
            "contextual_requirements": "Access to compute cluster/GPUs, integration of training code with DeepHyper, and design of appropriate objective functions for stability.",
            "generalizability": "Applicable to other scientific ML tasks requiring complex hyperparameter tuning, particularly when adversarial objectives are present.",
            "knowledge_type": "instrumental/technical skills; explicit procedural steps",
            "uuid": "e446.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain-Adversarial Training of Neural Networks",
            "rating": 2,
            "sanitized_title": "domainadversarial_training_of_neural_networks"
        },
        {
            "paper_title": "A Kernel Two-Sample Test",
            "rating": 2,
            "sanitized_title": "a_kernel_twosample_test"
        },
        {
            "paper_title": "Semi-supervised learning by entropy minimization",
            "rating": 2,
            "sanitized_title": "semisupervised_learning_by_entropy_minimization"
        },
        {
            "paper_title": "A Comprehensive Survey on Domain Adaptation for Visual Applications",
            "rating": 2,
            "sanitized_title": "a_comprehensive_survey_on_domain_adaptation_for_visual_applications"
        },
        {
            "paper_title": "Semi‑supervised and transfer learning for astronomical image classification (Ackermann et al. 2018)",
            "rating": 1,
            "sanitized_title": "semisupervised_and_transfer_learning_for_astronomical_image_classification_ackermann_et_al_2018"
        },
        {
            "paper_title": "Practical considerations for inserting simulated galaxies into realistic skies (Bottrell et al. 2019)",
            "rating": 1,
            "sanitized_title": "practical_considerations_for_inserting_simulated_galaxies_into_realistic_skies_bottrell_et_al_2019"
        },
        {
            "paper_title": "Class-aware Contrastive Domain Discrepancy networks (Kang et al. 2019)",
            "rating": 1,
            "sanitized_title": "classaware_contrastive_domain_discrepancy_networks_kang_et_al_2019"
        }
    ],
    "cost": 0.026303999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DeepMerge II: Building Robust Deep Learning Algorithms for Merging Galaxy Identification Across Domains
2021</p>
<p>A Ćiprĳanović 
Fermi National Accelerator Laboratory
P.O. Box 50060510BataviaILUSA</p>
<p>D Kafkes 
Fermi National Accelerator Laboratory
P.O. Box 50060510BataviaILUSA</p>
<p>K Downey 
Department of Astronomy and Astrophysics
University of Chicago
60637ILUSA</p>
<p>S Jenkins 
Department of Astronomy and Astrophysics
University of Chicago
60637ILUSA</p>
<p>G N Perdue 
Fermi National Accelerator Laboratory
P.O. Box 50060510BataviaILUSA</p>
<p>S Madireddy 
Argonne National Laboratory
9700 S Cass Ave60439LemontILUSA</p>
<p>T Johnston 
Oak Ridge National Laboratory
1 Bethel Valley Rd37830Oak RidgeTNUSA</p>
<p>G F Snyder 
Space Telescope Science Institute
3700 San Martin Drive21218BaltimoreMDUSA</p>
<p>B Nord 
Fermi National Accelerator Laboratory
P.O. Box 50060510BataviaILUSA</p>
<p>Department of Astronomy and Astrophysics
University of Chicago
60637ILUSA</p>
<p>Kavli Institute for Cosmological Physics
University of Chicago
60637ChicagoILUSA</p>
<p>DeepMerge II: Building Robust Deep Learning Algorithms for Merging Galaxy Identification Across Domains</p>
<p>MNRAS
0002021Accepted XXX. Received YYY; in original form ZZZPreprint 3 March 2021 Compiled using MNRAS L A T E X style file v3.0galaxies: interactionsmethods: data analysismethods: statisticaltechniques: image processing
In astronomy, neural networks are often trained on simulation data with the prospect of being used on telescope observations. Unfortunately, training a model on simulation data and then applying it to instrument data leads to a substantial and potentially even detrimental decrease in model accuracy on the new target dataset. Simulated and instrument data represent different data domains, and for an algorithm to work in both, domain-invariant learning is necessary. Here we employ domain adaptation techniques-Maximum Mean Discrepancy (MMD) as an additional transfer loss and Domain Adversarial Neural Networks (DANNs)-and demonstrate their viability to extract domain-invariant features within the astronomical context of classifying merging and non-merging galaxies. Additionally, we explore the use of Fisher loss and entropy minimization to enforce better in-domain class discriminability. We show that the addition of each domain adaptation technique improves the performance of a classifier when compared to conventional deep learning algorithms. We demonstrate this on two examples: between two Illustris-1 simulated datasets of distant merging galaxies, and between Illustris-1 simulated data of nearby merging galaxies and observed data from the Sloan Digital Sky Survey. The use of domain adaptation techniques in our experiments leads to an increase of target domain classification accuracy of up to ∼20%. With further development, these techniques will allow astronomers to successfully implement neural network models trained on simulation data to efficiently detect and study astrophysical objects in current and future large-scale astronomical surveys.</p>
<p>INTRODUCTION</p>
<p>Studies of galaxy mergers are crucial for understanding the evolution of galaxies as astronomical objects, their star formation rates, chemistry, particle acceleration and other properties. Moreover, they are equally important for cosmology, understanding structure formation and the study of evolution of matter in the universe. Being able to leverage large samples of merging galaxies and to connect the knowledge obtained from large-scale simulations and astronomical surveys will play an important role in these studies.</p>
<p>Standard methods for classifying merging galaxies using visual inspection (Lin et al. 2004) or extraction of parametric measurements of structure such as the Sérsic index (Sérsic 1963), Gini coefficient, M20 -the second-order moment of the brightest 20% percent of the galaxy's flux (Lotz et al. 2004), CAS -Concentration, Asymmetry, Clumpiness (Conselice et al. 2003) etc., can be time consuming, prone to biases, or require the use of high-quality images. Due to these limitations, it has been shown that machine learning can greatly ad-★ E-mail: aleksand@fnal.gov vance the study of merging galaxies (Ackermann et al. 2018;Snyder et al. 2019;Pearson et al. 2019;Ćiprĳanović et al. 2020), improving both the quality of the results and the speed of working with big datasets. Studies of merging galaxies using machine learning can greatly benefit from models trained on simulated data, which can then be successfully applied to newly observed images from present and future large-scale surveys.</p>
<p>Simulated and observed images have different origins and represent different data domains. In this case, labeled simulation images represent the source domain we are starting from, while observed data (often unlabeled) is the target domain. While images produced by simulations are made to mimic real observations from a particular telescope, unavoidable small differences can cause the model trained on simulated images to perform substantially worse when applied to real data. This substandard performance has been demonstrated directly in the case of merging galaxies by Ćiprĳanović et al. (2020). The authors show that even when the only difference between the two merging galaxy datasets is the inclusion of noise, convolutional neural networks (CNNs) trained on one dataset cannot classify the other dataset at all. In the paper the classification accuracy of in-domain images was 79%, while the accuracy for out-of-domain images was around 50%, equivalent to random guessing. Additionally, Pearson et al. (2019) use a dataset from the EAGLE simulation (Schaye et al. 2015), which is made to mimic Sloan Digital Sky Survey (SDSS) observations, and real SDSS images (Lintott et al. 2008;Darg et al. 2010). Their work provides further evidence that the performance of the classifier trained on one dataset has much lower accuracy when classifying the other dataset. They achieve out-of-domain accuracies between 53 − 65%, with the classifier trained on real SDSS images classifying EAGLE simulation images performing particularly poorly. These two examples are indicative of a great need for more sophisticated deep learning methods to be applied to cross-domain studies in astrophysical contexts.</p>
<p>An important area of deep learning research includes the development of Domain Adaptation (DA) techniques (Csurka 2017;Wang &amp; Deng 2018;Wilson &amp; Cook 2020). They allow the model to learn the invariant features shared between the domains and align the extracted latent feature distributions. This allows the model to successfully find the decision boundary that distinguishes between different classes in multiple domains at the same time. One group of divergence-based DA methods includes finding and minimizing some divergence criterion between the source and target data distributions. Some of the most well known methods include Maximum Mean Discrepancy (MMD; Gretton et al. (2012b)), Correlation Alignment (CORAL; ; ), Contrastive Domain Discrepancy (CDD; Kang et al. (2019)), and the Wasserstein metric (Shen et al. 2018). On the other hand, adversarial-based DA methods use either generative models (Liu &amp; Tuzel 2016) to create synthetic target data related to the source domain or more simple models that utilize domain-confusion loss (Ganin et al. 2016), which measures how well the model distinguishes between different data domains.</p>
<p>In this paper we employ two different domain adaptation techniques-Maximum Mean Discrepancy (MMD) and domain adversarial training with a domain-confusion loss-to improve crossdomain applications of deep learning models to the problem of distinguishing between merging and non-merging galaxies. Maximum Mean Discrepancy works by minimizing a distance measure of the mean embeddings of the two domain distributions in latent feature space (Gretton et al. 2012b). It is applied to standard classification networks as a transfer loss. Domain adversarial neural networks (DANNs; Ganin et al. (2016)), use adversarial training between a label classifier-which distinguishes mergers from non-mergers-and a domain classifier-which classifies the source and target domain of images. This kind of training employs a gradient reversal layer within the domain classifier, thereby maximizing the loss in this branch and leading to the extraction of domain-invariant features from both sets of images. Following methods from Zhang et al. (2020a), we also add Fisher loss and entropy minimization (Grandvalet &amp; Bengio 2005), which can be used as additional losses for both MMD or domain adversarial training, to improve the overall performance of the classifier. Both of these loss functions enforce additional discriminability between the classes in source (Fisher loss) and target (entropy minimization) domains, by producing more compact classes in the latent feature space.</p>
<p>We test two networks for a comparison of technique results across architectures: DeepMerge, a simple convolutional network for classification of galaxies presented in Ćiprĳanović et al. (2020), as well as the more complex and well-known ResNet18 (He et al. 2015). We demonstrate both methods on a dataset similar to the one from Ćiprijanović et al. (2020), using simulated distant merging galaxies from Illustris-1 (Vogelsberger et al. 2014) at redshift z = 2, both without (source) and with (target) the addition of random sky shot noise to mimic observations from the Hubble Space Telescope.</p>
<p>Additionally, we test these methods on a harder and more realistic application example, where the source domain includes simulated galaxies at = 0 from the Illustris-1 simulation (Vogelsberger et al. 2014) made to mimic SDSS observations and real SDSS images of merging galaxies (Lintott et al. 2008;Darg et al. 2010). These two domains exhibit a much larger discrepancy, and simply applying MMD and adversarial training does not perform well. We demonstrate that combining MMD with transfer learning from the model trained on the first dataset of distant merging galaxies can be used to solve this harder domain adaptation problem.</p>
<p>With the use of domain adaptation techniques mentioned above, we manage to increase the target domain classification accuracy up to ∼20% in our experiments, which allowed the model to be successfully used in both domains. It is our hope that the use and continued development of these techniques will allow astronomers and cosmologists to develop deep learning algorithms that can combine information from either simulations and real data, or to combine observations from different telescopes.</p>
<p>The remainder of the paper is structured as follows: In Section 2, we introduce and explain the domain adaptation methods used in this paper. We explain the neural network architectures we use in Section 3. In Section 4, we give details about the images we use in our experiments and talk more about the experimental setup in Section 5. Finally, our results are given in Section 6, followed by a discussion in Section 7.</p>
<p>METHODS</p>
<p>Deep learning is already bringing advances in astronomy and survey science, as in other academic fields and industry. Many astronomical applications often require these models to perform well on new datasets, requiring the applicability of features learned from simulations to data that the model was not initially trained on, including newly available observed data and cross-telescope applications. Since labelling new data is slow and prone to errors, retraining these neural networks on new datasets in order to maintain high performance is often impractical. In these situations, a discriminative model that is able to transfer knowledge between training (source domain) and new data (target domain) is necessary. This can be achieved by using domain adaptation techniques, which extract invariant features between two domains, so that a neural network classifier trained on the source domain can also be applied successfully on a target domain. As previously underscored, this functionality is very useful in situations often found in astronomy, where the target domain is comprised of new observational data that has very few identified objects or is completely unlabeled. Here we will test several DA techniques that can be effective in the situation where the target domain is unlabeled. These techniques include adding transfer loss to the widely-adopted cross-entropy loss used for standard classification of images.</p>
<p>Cross-entropy loss is given as:
L CL = − M ∑︁ =1 log ( ),(1)
where is a particular class and the total number of classes is M. The true label for class is given as , and ( ) is the neural network assigned score, i.e. the output prediction from the last layer, for a given class . Minimizing cross-entropy loss leads to output predictions approaching real label values, which results in an increase in classification accuracy.</p>
<p>The inclusion of transfer loss allows DA techniques to impact the way the network learns via backpropagation. We explore two different transfer losses in this paper: Maximum Mean Discrepancy (MMD; Gretton et al. (2012a)) and using the loss of the discriminator from a Domain Adversarial Neural Network (DANN; Ganin et al. (2016)). Additionally, we explore adding Fisher loss (Zhang et al. 2020a), which enforces feature discriminability between classes in the source domain, and entropy minimization, which forces a target sample to move toward one of the compacted and separated source classes (Grandvalet &amp; Bengio 2005), leading to better class separability in the target domain.</p>
<p>The resultant total classifier loss (Zhang et al. 2020a) has multiple components:
L TOT = L CL + FL L FL + EM L EM + TL L TL ,(2)
where we define L TOT , L CL , L FL , L EM , L TL as total loss, classifier loss, Fisher loss, entropy minimization, and transfer loss, respectively. The contribution of these additional losses can be weighted using weights FL , EM and TL . Further details about the different losses used are given below.</p>
<p>Transfer Loss</p>
<p>The transfer loss L TL is calculated from the DA technique, whose goal is to decrease discrimination between the source and the target domains. This involves the representation of data from both domains in a higher-dimensional latent feature space. In this paper, we explore the use of both MMD and domain adversarial training as transfer criteria. MMD frames the domain problem in terms of high-dimensional statistics and involves calculating the distance between the mean embeddings of the source and target domain distributions. In the case of adversarial training, the domain discrepancy problem is addressed by adopting DANN, a neural network that seeks to find the common feature space between the source and target distributions by jointly minimizing the training loss in the source domain while maximizing the loss of the domain classifier. We will denote the source and target domains as D s and D t respectively. Source domain images are labeled, so we have s pairs of images and labels {x s , y s }, while in the case of the target domain we have t unlabeled images x t . Images from both domains are associated with domain labels d s for source domain and d t for target domain.</p>
<p>Maximum Mean Discrepancy (MMD)</p>
<p>Maximum Mean Discrepancy (MMD) is a statistical technique that calculates a nonparametric distance between mean embeddings of the source and target probability distributions from the ∞ norm. Following Smola et al. (2007) and Gretton et al. (2012b), we designate the source probability distribution as P s and the target probability distribution as P t .</p>
<p>It is possible to estimate densities of P s and P t from the observed source and target data using kernel methods, but this estimation, which is often computationally expensive and introduces bias, is unnecessary in practice (Pan et al. 2011). Instead, we use kernel methods to determine their means for subtraction instead of estimating the full distributions:
(P s , P t , F ) := sup ∈ F E P s [ ( s )] − sup ∈ F E P t [ ( t )],(3)
where denotes the kernel distance as a proxy for discrepancy, s and t are random variables drawn from P s and P t respectively, function class F closely resembles the set of CDF functions in vector space with total variance less than one operating on the domain (−∞, IR] and the supremum is the least element in F greater than or equal to the chosen , i.e. the max of the subset. By this definition, if P s = P t , then (P s , P t , F ) = 0 (Gretton et al. 2012a;Pan et al. 2011). If P s ≠ P t , then there must exist some such that the distance between the two means is maximized. This becomes an optimization problem in which the criterion aims to maximize the discrepancy by separating the two distributions as far as possible in some high-dimensional feature space. Kernel methods are well suited to this task since they are able to map means into higher-dimensional Reproducing Kernel Hilbert Spaces (RKHSs). Furthermore, this embedding linearizes the metric by mapping the input space into a feature vector space.</p>
<p>RKHSs possess several properties that facilitate the calculation of , including a property of norms that rescale the output of a function to fit within a unit ball-which greatly restricts the many possibilities of function class F -and a reproducing property that reduces calculations to the inner product of the output of functions. For example, , ( , ·) = ( ), where ( , ·) is a kernel that has one argument fixed at , and the second free. Performing this calculation in an RKHS means there is no need to explicitly calculate the mapping function ( ) that maps s and t into an RKHS feature space due to equivalence between ( ) and ( , ·) : ( ), ( ) = ( , ) = ( , ·), ( , ·) . Therefore, Eq. 3 can be re-expressed as two inner products with kernels mean embeddings in an RKHS:
(P s , P t , F ) := sup | | | | ≤1 E P s [ ( s ), ] − sup | | | | ≤1 E P t [ ( t ), ] (4) = sup s , − sup t , = sup s − t , ,
where s and t are the source and target distribution's mean embeddings and is still bounded by the unit ball of the RKHS. Clearly, the inner product is maximized for the identity , = 1. Therefore, to maximize the mean discrepancy we need = st , leaving us with the final formula:
(P s , P t , F ) := E P s [ ( s , s )] − 2E P s,t [ ( s , t )] + E P t [ ( t , t )],(5)
where all kernel functions come from the simplification of the inner product ( , ·), ( , ·) , following the logic of the equivalence between the mapping function and the kernel established previously.</p>
<p>Here it is clear that the distance is expressed as the difference between the self-similarities of source and target domains and their cross-similarity. In practice, this is discretized to give the the unbiased estimator :
= 1 ( − 1) ∑︁ != ( s ( ), s ( )) − ( s ( ), t ( ))− (6) ( t ( ), s ( )) + ( t ( ), t ( )),
where is the sample number of s and t . While in practice, can be considered a general kernel, we follow Zhang et al. (2020a) and substitute with , where is a linear combination of multiple Gaussian Radial Basis Function (RBF) kernels to extend across a range of mean embeddings. Gaussian RBF kernel can be written as:
( , ) = − || − || 2
where || − || is the Euclidean distance norm (where can be s or t depending on the domain), and is the free parameter which determines the width of the kernel. Finally, we use MMD as our transfer loss: L TL,mmd = , effectively drawing the source and target distributions together in latent space as the network aims to minimize the loss via backpropagation.</p>
<p>Domain Adversarial Training</p>
<p>Domain adversarial training employs a Domain Adversarial Neural Network (DANN) to distinguish between the source and target domains (Ganin et al. 2016). DANNs are comprised of three parts: a feature extractor ( F ), label predictor ( L ), and domain classifier ( D ). The first two parts can be found in any Convolutional Neural Network (CNN)-the feature extractor is built from convolutional layers which extract features from images, while the label predictor usually has fully-connected (dense) layers which output the class label. The last part, the domain classifier, is unique to DANNs. It is built from dense layers and optimized to predict the domain labels. This domain classifier is added after the feature extractor as a parallel branch to the label predictor. It includes a gradient reversal layer which maximizes the loss for this branch of the neural network, thus achieving the adversarial objective of confusing the discriminator. When the domain classifier fails to distinguish latent features from the two domains, the domain invariant features, i.e. the shared feature space, is found.</p>
<p>Compared to regular CNNs which can learn the best features for classification, training DANNs can lead to a slight drop in classification accuracy for the source domain because only the domaininvariant features are used. However, this will also lead to an increase in the classification accuracy in the new target domain which is our objective. The total loss for a DANN is L DANN = L class − L d , where L class is the loss for the image class label predictor L , while L d is the loss from the domain classifier D . Fine-tuning the trade-off between these two quantities during the learning process is done with the regularization parameter . Domain classifier loss is calculated as:
L d = 1 s ∑︁ x s ∈ D s ( D ( F (x s )), s ) + 1 t ∑︁ x t ∈ D t ( D ( F (x t )), t ),(8)
where ( D ( F (x s )), s ) and ( D ( F (x t )), t ) are the output scores for the source domain and target domain labels, respectively. Similarly to the class label predictor, the output scores for domain labels are also calculated using cross-entropy loss on domain labels. Finally, we can use domain classifier loss as our transfer loss:
L TL,adv = max{−L d }.(9)</p>
<p>Fisher Loss</p>
<p>The addition of Fisher loss to the classification and transfer losses was demonstrated to further improve classification performance for source domain images in Zhang et al. (2020a). This improvement in source classification can aid the performance of both MMD and domain adversarial training transfer criteria. It is more generally applicable than Scatter Component Analysis (Ghifary et al. 2017), which also results in class compactness, but can only be used in conjunction with MMD and would not be practical for use with adversarial training methods (Zhang et al. 2020a). Minimizing Fisher loss leads to within-class compactness and between-class separability in the latent feature space, which makes the distinction between classes easier in the source domain. Fisher loss produces a centroid for each class and effectively pushes labeled classes toward their respective centroids, thereby creating more tightly clustered classes further apart from each other. It can be defined as a function :
L FL = (tr(S w ), tr(S b ))).(10)
Here, To achieve the intended result of intra-class compactness and interclass separability, Eq. 10 must be monotonically increasing with respect to the trace of the intra-class matrix S w and monotonically decreasing with respect to the trace of the inter-class matrix S b . Thus, as the loss is minimized via backpropagation, the distances within classes will grow smaller and the distances between classes will grow larger. There are two simple ways one can construct a Fisher loss function obeying these constraints: Fisher trace ratio L FL = tr(S w )/tr(S b )) or Fisher trace difference L FL = tr(S w ) − tr(S b )). In this paper, we have chosen to use the Fisher trace ratio as our Fisher loss.
S w = M =1 m =1 (h −c )(h −c ) T
As we mentioned in Eq. 2 for total loss, we can control the contribution of all additional losses using the weight parameter . In the case of Fisher loss, we can separately weight the importance of the two matrices using w and b . Since we have chosen to use the trace ratio Fisher loss L FL = w tr(S w )/ b tr(S b ), this gives FL = w / b .</p>
<p>Entropy Minimization</p>
<p>Fisher loss can only be used in the source domain since it requires ground-truth labels to calculate intra-class centroids and the between class global center. However, Fisher loss can aid the discrimination between classes within the unlabeled target domain as well through entropy minimization loss. Entropy minimization loss pushes examples from the target domain toward source domain class centroids. Therefore, entropy minimization ensures better generalization of the decision boundary between optimally discriminative and compact source domain classes to the target domain as well (Grandvalet &amp; Bengio 2005).</p>
<p>Entropy minimization loss is defined as:
L EM = − m ∑︁ =1 M ∑︁ =1 ( |h ) log ( |h ),(11)
where ( |h ) is the classifier output and the true label is not needed. The above formula is based on Shannon's entropy (Shannon 1948), which for a discrete probability distribution can be written as
( ) = − N =1 log 2 .</p>
<p>NEURAL NETWORK ARCHITECTURES</p>
<p>We present the performance of domain adaptation using the aforementioned techniques in two neural networks-the simpler Deep-Merge architecture with 174,626 trainable parameters (Ćiprĳanović et al. 2020) and the more complex and well known network ResNet18 with 22,484,866 trainable parameters (He et al. 2015)-to compare results across architectures. We decided to use the smallest standard ResNet architecture, in order to more easily tackle possible overfitting of the model, due to small sizes of merging galaxies datasets. The DeepMerge network, first introduced by Ćiprĳanović et al. (2020), is a simple CNN comprised of three convolutional layers followed by batch normalization, max pooling, and dropout, and three dense layers. In this paper, the dropout layers have been removed such that the only regularization happens via L2 regularization of the weight decay parameter in the optimizer. Additionally, the last layer of the original DeepMerge network was updated to include two neurons rather than one. For more details about the architecture check Table A1 in the Appendix.</p>
<p>ResNets were first proposed in the seminal paper He et al. (2015) and have become one of the most widely-used network architectures for image recognition. They are comprised of residual blocks; in the case of ResNet18, blocks of two 3x3 convolutional layers are followed by a ReLU nonlinearity. The chaining of these residual blocks enables the network to retain high training accuracy performance even with increasing network depth.</p>
<p>The domain classifier used in adversarial domain training to calculate transfer loss comprises of three dense layers, the first of which is the same dimension of the extracted features in the base network (either DeepMerge or ResNet18), such that these features form the input into the domain classifier. The second layer has 1024 neurons, followed by ReLU activation and dropout of 0.5, and the third has one output neuron followed by Sigmoid activation, conveying the domain chosen by the network.</p>
<p>Details about training the networks can be found in Appendix A. We also list all hyperparameters used for training in different experiments in Table A2 and Table A3. Our parameter choice for each experiment was informed by running hyperparameter searches using DeepHyper (Balaprakash et al. 2018;Balaprakash et al. 2019).</p>
<p>DATA</p>
<p>Here we present two dataset pairs for classifying distant merging galaxies ( = 2) and nearby merging galaxies ( &lt; 0.1).</p>
<p>Simulation-to-Simulation: Distant Merging Galaxies from Illustris</p>
<p>It is often very difficult to obtain real-sky observational data of labeled mergers for deep learning models, especially at higher redshifts. Therefore, both the source and target domain of our distant merging galaxy dataset are simulated. We use the same dataset as in Ćiprĳanović et al. (2020), where the authors extract galaxies at redshift = 2 from Illustris-1 cosmological simulation (Vogelsberger et al. 2014). The objects in this dataset are labeled as mergers if they underwent a major merger (stellar mass ratios of 10 : 1) in a time window of 0.5 Gyr around when the Illustris snapshot was taken. This means our merger sample includes both past (happened before the snapshot) and future mergers (happened after the snapshot). In Ćiprĳanović et al. (2020), images contain two filters, which mimic Hubble Space Telescope (HST) observations. In this paper we add a third (middle) filter to produce three filter HST images (ACS F814W, NC F356W, WFC3 F160W). This allows us to use the images even with a more complex ResNet18 architecture.</p>
<p>We produce two groups of images: source "pristine" images are convolved with a point-spread function (PSF); and target "noisy" images are convolved with the PSF, with added random sky shot noise. This sky shot noise produces a 5 limiting surface brightness of 25 magnitudes per square arc-second. More details about the dataset can be found in Ćiprĳanović et al. (2020) and Snyder et al. (2019). The source and target domains contain 8120 mergers and 7306 non-mergers, respectively. Each image is 75 × 75 pixels. We divide these datasets into training, validation, and testing samples: 70% : 10% : 20%. See Figure 1 for example images from this Illustris dataset: mergers are shown in the left column and non-mergers in the right column. The top row shows images from the source domain, while the middle row shows the target galaxies with the added noise. The bottom row shows the same group of top-row source images with logarithmic color mapping in order to make the galaxies more visible to the human eye.</p>
<p>Simulation-to-Real: Nearby Merging Galaxies from Illustris and Real SDSS Images</p>
<p>To test the ability of MMD and adversarial training in the astronomical situations where they show the most promise-training on simulated data with the prospect of applying the models to real data-we need to use merging galaxies at lower redshifts where more real data is available.</p>
<p>Source dataset: simulated images</p>
<p>In this scenario, attempting to make simulated images resemble real observations from a particular telescope is an important step for domain adaptation techniques, since decreasing differences between domains will make domain adaptation easier. Here our source data is comprised of simulated merging galaxies (major mergers) from the final snapshot at = 0 of Illustris-1, in a time window of 0.25 Gyr before the time the snapshot was taken. The fact that we use the final snapshot of the simulation is an extremely important difference between this source domain dataset and the one described earlier: this means that only past mergers (also called post-mergers) are included instead of both past and future mergers. Since the simulation was stopped after this snapshot, images of future mergers that would have merged during 0.25 Gyr after the snapshot are not available. This dataset was originally produced in Snyder et al. (2015). Here, images also include effects of dust implemented as a slab model based on the gas and metal density along the line of sight to each pixel, similar to models by Nelson et al. (2018), De Lucia &amp; Blaizot (2007), and Kitzbichler &amp; White (2007). Images have three SDSS filters ( , , ) and are also convolved with a Gaussian PSF (FWHM = 1 kpc) and re-binned to a constant pixel scale of 0.24 kpc. This scale corresponds to 1 arcsec seeing for an object observed by SDSS at = 0.05. Finally, random sky shot noise was added to these images by independently drawing from a Gaussian distribution for each pixel, which produces average signal-to-noise ratio of 25.</p>
<p>The simulated galaxies in this dataset contain a lower number of mergers compared to the = 2 snapshot used in our simulation-tosimulation experiments. Observational evidence shows that merger rates today are much lower compared to the merger rate peak during the "cosmic high noon" at ∼ 2 − 3 (Madau &amp; Dickinson 2014), which is where our galaxies from the previous example are located. Our source domain dataset contains only 44 post-mergers and 5625 non-mergers. We employ data augmentation in order to make a larger source dataset, particularly focusing on mergers to make the classes balanced. We first augment mergers by using mirroring (vertical and horizontal), and rotation by 90 • and 180 • (which produces 220 images). Finally, these images are additionally augmented by random angle rotation or zooming in/out. The final source dataset we use contains 3000 : 3000 post-mergers and non-mergers (we truncate non-mergers to make the classes balanced).</p>
<p>Target dataset: observed images</p>
<p>Our target dataset is composed of observational SDSS images. We follow dataset selection from Ackermann et al. (2018) and use the SDSS online image cutout server to get RGB (red, green, blue) JPEG images of both merging and non-merging galaxies. These RGB images correspond to ( , , ) SDSS filters, as opposed to ( , , ) in our source domain. We later align the order of filters in our source domain to correspond to this filter order. All of the galaxies selected are from the Galaxy Zoo project (Lintott et al. 2008(Lintott et al. , 2010, which used crowd-sourcing to generate labels for 900,000 galaxies. We use the 3003 mergers identified in the Darg et al. (2010) catalogue; three of these mergers were unable to be retrieved due to faulty weblinks. This catalogue identified mergers through the weighted-merger-vote fraction,</p>
<p>, which describes the confidence in the crowd-sourced label. Mergers were defined as galaxies with an &gt; 0.4, where = 0 describes objects that are not merger-like and = 1 describes merger-like objects. Mergers included in Galaxy Zoo were also required to be between redshifts 0.005 &lt; &lt; 0.1.</p>
<p>All available SDSS mergers also include a merger stage: separated mergers (167 images), interacting (2523 images), post-mergers (310 images). Since our source domain includes only post-mergers, we restrict our target dataset to only include the post-merger subclass from SDSS. To obtain 3000 mergers, we augment the SDSS postmerger images using the same techniques used in the source domain. To complete our target dataset, another 3000 non-merger galaxies in the 0.005 &lt; &lt; 0.1 redshift range were randomly selected from the Galaxy Zoo project's entire dataset by requiring &lt; 0.2. We resize images from both domains to the same size as in simulation-to-simulation example (75 × 75 pixels), and use the same split into training, validation and testing samples: 70% : 10% : 20%. In Figure 2 we plot images from both domains. In the top row, we plot post-mergers (left) and non-mergers (right) from Illustris simulation at = 0, while in the bottom row we plot post-mergers (left) and non-mergers (right) from SDSS.</p>
<p>Images from the target domain were visually classified, and most of the target post-mergers clearly exhibit two bright galaxy cores, while images from the source domain display a greater variety of characteristics. Consequently, the two domains are extremely dissimilar relative to the two domains in the simulation-to-simulation example. The choices we detail above-using only post-mergers in both domains; including observational and dust effects and choosing a small time window to avoid the inclusion of very relaxed merger systems in the source domain-were made in an attempt to make the two domains as similar as possible. Still, the fact that the number of individual mergers in both domains is very small, paired with the fact that their appearance can be quite different makes any domain adaptation efforts quite challenging.</p>
<p>EXPERIMENTS</p>
<p>CNNs outperform other machine learning methods for classification of merging galaxies (Snyder et al. 2019;Ćiprĳanović et al. 2020). However, in both Ćiprĳanović et al. (2020) and Pearson et al. (2019), it was shown that, even though training and evaluating a CNN on images from the same domain gives very good results, a simple model trained in one domain cannot perform classification in a different domain with high accuracy. To increase the performance of deep learning classifiers on a target dataset, we use the DA techniques described in Section 2.</p>
<p>We first train neural networks without the implementation of any DA techniques to determine the base performance for source and target images for each pair of datasets: simulated-to-simulated (Illustris = 2 with and without noise) and simulated-to-real (Illustris = 0 and SDSS). While we possess labels for both the source and target domain in both scenarios, this training is performed using labeled source images exclusively. We only use the target image labels in the testing phase to asses accuracy. We seek these performance accuracies as the metric to improve upon through domain adaptation.</p>
<p>We then run several domain adaptation experiments-using MMD as transfer loss, adversarial training with DANN domain discriminator loss as transfer loss, MMD as transfer loss with Fisher loss and entropy minimization, and finally DANN adversarial training with Fisher loss and entropy minimization-with both DeepMerge and ResNet18 architectures on the simulation-to-simulation dataset. Training with domain adaptation is performed using labeled source data and unlabeled target data. In the case of adversarial training, an The left column shows post-merger galaxies, while the right column shows non-mergers. Source domain images in the top row were plotted with a logarithmic color map to make features more visible. Even when we select only post-mergers from SDSS we can still see that the merger class is different across the two domains. While the source domain contains more relaxed systems, the target contains galaxies near each other, with two bright clearly visible cores. additional domain classifier branch is added to receive an input of features from base network. The parameters used for training both DeepMerge and ResNet18 for all simulation-to-simulation experiments are given in the Appendix in Table A2.</p>
<p>Since larger networks are prone to overfitting, given the limited size of both source and target datasets in our simulation-to-real experiments, we decided to only test it with the the smaller Deep-Merge network. Similar to the experiments described above for the simulation-to-simulation dataset, we first train DeepMerge without any domain adaptation in order to determine the base performance. We then tried to improve target domain accuracy by training using MMD and adversarial training, both with and without Fisher loss and entropy minimization. However, despite performing hyperparameter searches, domain adaptation was not successful, i.e. the target domain accuracy was no better than random guessing. We then turned our trials to combining MMD and adversarial training with transfer learning from models successfully trained in simulation-tosimulation experiments. Hyperparameters used in training the model in the simulation-to-real experiments are given in the Appendix in Table A3.</p>
<p>To ensure reproducibility of our results prior to training, we fix the random seeds used for image shuffling (before division into training, validation and testing samples), as well as for random weight initialization of our neural networks. The same images were used across experiments for training, as well as for testing afterwards to produce the reported results. In Section 6 we report results for a fixed seed=1.</p>
<p>RESULTS</p>
<p>Throughout this paper we consider mergers the positive class (label 1), and non-mergers the negative class (label 0). Consequently, correctly/incorrectly classified merger are true positives (TP)/false negatives (FN), while correctly/incorrectly classified non-mergers are true negatives (TN)/false positives (FP).</p>
<p>We report classification accuracy, precision or purity: TP/(TP + FP), recall or completeness: TP/(TP + FN), and F1 score = 2 Precision×Recall Precision+Recall . We also report the Area Under the Curve (AUC) score-the area under the Receiver Operating Characteristic (ROC) curve, which conveys the trade-off between true-positive rate and false-positive rate. Finally, we provide Brier score values, which measure the mean squared error between the predicted scores and the true labels; a perfect classifier would have a Brier score of zero.</p>
<p>Simulation-to-Simulation Experiments</p>
<p>Results of training the two classifiers without domain adaptation are given in the first row of Table 1. Training was performed on the source domain images, and test accuracy on source images is high for both networks in the base case without DA: 85% for DeepMerge and 81% for ResNet18. As was expected, without any domain adaptation both classifiers are almost unable to classify target domain images; test accuracy for this domain are only 58% (DeepMerge) and 60% (ResNet18). Additionally, as expected, we noticed that the more complex ResNet18 was much more prone to overfitting earlier in the training than DeepMerge. We therefore implemented early stopping in our training in all experiments, as well as saving of the best model before the training stops due to substantial overfitting.</p>
<p>We then trained DeepMerge and ResNet18 with both domain adaptation techniques, each with and without Fisher loss and entropy minimization. Results from these DA experiments are also given in Table 1. We conclude that it is difficult to determine a single best technique across architectures. Additionally, inclusion of the Fisher loss and entropy minimization, implemented for within class compactness in both the source and target domains, does not always help. This might be due to the fact that multiple losses interact differently depending on the network architecture and complexity of the feature space. In short, simple hyperparameter grid searches, which informed our parameter choices, are not a perfect solution to find the optimal hyperparameters for different experiments (a non-trivial task that we leave for future studies). Despite our imperfect hyperparamter choices, we assert that the results presented here convey an overall demonstration of the performance and improvements of domain adaptation techniques for cross-domain studies.</p>
<p>Next we take a closer look at experiments that were most successful for DeepMerge and ResNet18. The best-performing DeepMerge network experiments-MMD, adversarial training, and adversarial training with Fisher loss and entropy minimization-reached source domain accuracies of 87%. The accuracy in the target domain was largest with adversarial training at 79%, while with MMD it reached 77%. Consequently, the highest increase in target domain accuracy was 21% compared to the classifier without domain adaptation. Again, we assert that each experiment's results could potentially be further improved with a different set of hyperparameters.</p>
<p>For ResNet18, we see a slightly smaller increase in target domain accuracies compared to improvements by DeepMerge. Because it is a more complex network, it is harder to stop ResNet18 from learning more intricate details that are only found in the source domain. Target domain accuracies increase from 60% without domain adaptation to 75% in the best performing experiment, which was MMD with ad- In experiments without DA, we allow the network to learn from all available features that can be extracted rather than restricting to the set of domain-invariant features. It follows that, in training without DA, a perfectly optimized network should be able to reach its highest source accuracy when not being forced to learn domain-invariant features. In contrast to this expectation, we observe that the addition of transfer and other losses slightly increases source domain accuracies in almost all of the experiments we ran. We posit that this is first and foremost the consequence of not finding the best set of hyperparameters and that the additional transfer loss serves as a good regularizer, enabling longer training without overfitting. This is more prominent in case of ResNet18, where source domain accuracies increase from 81% for no domain adaptation case to the highest value of 92% in the case of adversarial training.</p>
<p>For easier comparison, in Figure 3 we plot the performance values from Table 1  See Appendix B for additional performance comparison between training with and without domain adaptation for both networks on the simulation-to-simulation dataset.</p>
<p>Simulation-to-Real Experiments</p>
<p>We also evaluated the performance of these DA methods in the situation where the classifier is trained on simulated source domain images and tested on a target domain of observational telescope images. Examples like this are much more complex than our simulation-tosimulation experiments due to the larger discrepancy between domains, as discussed at length in Subsection 4.2.</p>
<p>Due to the perils of training a large network on such a small dataset, we decided to test domain adaptation techniques with only the smaller DeepMerge network. Without DA, DeepMerge reached an accuracy of 92% in the source domain and 50% in the target domain in the testing phase. This was our baseline we try to improve upon in the simulation-to-real DA experiments. Due to the extreme discrepancy between domains, we also tested trained the DeepMerge network on the target domain directly, which resulted in an accuracy of 96% on the target domain images. This is higher than for the source domain of simulation images, confirming that the target domain is easier to train on since it is comprised of visually more apparent merging features.</p>
<p>We then tried running hyperparameter grid searches with MMD and adversarial training, but were unable to successfully use domain adaptation to improve target domain accuracies. This led us to conclude that problems with the size of our dataset, as well as the difference between domains, was preventing the successful learning of domain-invariant features. In the top row of Table 2, we report all performance metrics for no domain adaptation case and in the middle row we report results of using MMD only as transfer loss. Since no other method was successful, we omit reporting numbers for all other DA setups tested in the simulation-to-real experiments. Larger simulated training samples, more sophisticated domain adaptation methods that allow for better domain overlap of discrepant feature distributions, or a combination of the two will advance the study of merging galaxies across the simulated-to-real domain in the future.</p>
<p>Transfer Learning</p>
<p>The approach we took to overcome our small dataset limitation was to use transfer learning, where the weights from a neural network pre-trained on different data are loaded before training the classifier on the data of interest. Transfer learning has been used in previous studies of merging galaxies. For example, in Ackermann et al. (2018), authors use Xception (Chollet 2016), a large deep learning model, pre-trained on images of everyday objects from ImageNet (Deng et al. 2009). They successfully trained the model on observed images of merging galaxies from SDSS and report classification precision, recall, and F1 score of 0.97, 0.96, 0.97. Similarly, in Wang et al. (2020), authors use a VGG network (Simonyan &amp; Zisserman 2015) pre-trained on ImageNet to train on simulated images from the IllustrisTNG simulation at = 0.15 (Springel et al. 2018;Pillepich et al. 2018). They report classification accuracy of 72% on simulated images, and then use the simulation-trained model to detect major mergers in KiDS (de Jong et al. 2013) and GAMA (Driver et al. 2009) observations. We decided to test if our simulated-to-real DA setup would benefit from transfer learning from a more similar dataset than ImageNet. Rather than proceeding with random weight initialization, we load the weights from our successfully trained DeepMerge networks in our simulated-to-simulated experiments. This way we can utilize extracted features that relate to distant merging galaxies, which are much more similar to nearby merging galaxies, than features extracted from everyday objects.</p>
<p>We tried training without freezing layers (allowing all weights in the network to fine-tune to the new datasets), and freezing of convolutional and batch normalization layers. The training performed much better when all weights of the model were allowed to train from their loaded checkpoint. This may be due to both the smaller  size of the DeepMerge network as well as the possible differences in the appearance of galaxies in our experiments, with a particular emphasis on the very different appearance of real SDSS mergers compared to simulated ones. This probably led to the necessity of the network finding better-suited domain invariant features when real data is included, which can be more easily found when convolutional layers are allowed to train. We also performed a hyperparameter search for both MMD and adversarial domain adaptation with transfer learning, and were able to find a configuration for successful DA with MMD. Since the domain discrepancy in the case of simulated-to-real images was large, successfully learning common features led to the reduction of source domain accuracy from 92% without domain adaptation to 83% with MMD and transfer learning. However, and most significantly, we were able to achieve 69% in the target domain during the testing phase of MMD with transfer learning-an increase of 19% compared to noDA. In the bottom row of Table 2, we report performance metrics for this transfer learning case.</p>
<p>For ease of comparison, we also plot performance metric values in the testing phase in Figure 4. Bar plots on the left show all performance metrics (accuracy, precision, recall, F1 score, Brier score, and AUC) for our simulated-to-real experiments-no domain adaptation (navy blue), MMD (purple), MMD with transfer learning (yellow). ROC curves for these experiments are presented on the right, with the same color coding as in the bar plots. In both panels, the solid bars and lines show values for the target domain while the dashed bars and lines show source domain performance.</p>
<p>This experiment demonstrates that domain adaptation techniques are very powerful. However, to be useful in a scientific context, we conclude that very careful data preprocessing to reduce domain discrepancies and/or transfer learning to mitigate the problem of small datasets is necessary. It is our hope that the introduction of these techniques to the astronomy community will spur innovation and encourage the use of more sophisticated DA methods that optimize domain alignment for this sort of difficult astronomical tasks.</p>
<p>DISCUSSION</p>
<p>We have demonstrated how MMD and domain adversarial training substantially increase the performance of simulated-to-simulated learning in the context of galaxy merger classification. For Deep-Merge, the average accuracy benefit of these techniques was 18.75% in the target domain; for ResNet18, the average benefit was 12.75%. While unable to show positive results with adversarial domain adaptation training on our simulated-to-real dataset, pairing MMD with transfer learning achieved a substantial increase in target domain accuracy of 19% with DeepMerge.</p>
<p>We believe that both techniques show great promise for use in astronomy. Here we discuss their interpretability with the aid of t-Distributed Stochastic Neighbor Embeddings (t-SNEs) and Gradient-Class Activation Mappings (Grad-CAMs); and provide an outlook on their potential for use within the scientific community.</p>
<p>Model Interpretability: Understanding the Extracted Features with t-SNEs</p>
<p>To better understand the effect of domain adaptation, we visualize the distribution of the extracted features with t-Distributed Stochastic Neighbor Embeddings (t-SNE) plots by projecting the high-dimensional feature space to a more familiar two-dimensional plane (van der Maaten &amp; Hinton 2008). This method calculates the probability distribution over data point pairs, assigning a higher probability to similar objects and a lower probability to dissimilar pairs, in both the latent feature space and in the two-dimensional mapping. By minimizing the Kullback-Leibler (KL) divergence (Kullback &amp; Leibler 1951) between the two distributions, the t-SNE method ensures the similarity between the actual distribution and the projection. Despite its usefulness, we emphasize that t-SNE is a non-linear algorithm and adapts to data by performing different transformations in each region. This can lead to clumps with highly-concentrated points appearing as very large groups, i.e. it is difficult to compare relative sizes of clusters in t-SNE plot renderings. Additionally, the outputted two-dimensional embeddings are entirely dependent on several userdefined parameters. For more details on t-SNE best practices, see Wattenberg et al. (2016). We implemented an option to plot t-SNEs in our training method to demonstrate the changes in extracted features from the source and target domain across a series of epochs. In Figure 5 we plot t-SNE plots for DeepMerge before the start of the training in the first panel, and t-SNE plots after some training-when no domain adaptation is implemented in the second panel, when MMD as transfer loss is used in the third panel, when MMD with Fisher loss and entropy minimization is used in the fourth panel. We confirm that domain adversarial training t-SNEs are virtually indistinguishable from MMD plots, so we omit the repeat here. On all t-SNE plots, red and blue colored dots represent the two classes-mergers and non-mergers, respectively-and transparent dots represent the source domain, while opaque dots represent the target domain.</p>
<p>Before the start of training, classes are completely mixed together and domains are separated (first panel). With no domain adaptation, we see that even after some training, domains remain separated (second panel). With domain adaptation but no Fisher loss or entropy minimization (third panel), we see that features from both domains completely overlap. We can see that both classes exhibit some clumping but the structure of both classes is quite complex. Finally, in the fourth panel, we also include Fisher loss and entropy minimization which helps the two classes separate more in both domains.</p>
<p>Model Interpretability: Visualizing Salient Regions in Input Images with Grad-CAMs</p>
<p>Another way of probing deep neural network models is by identifying regions in the input images that proved most important for classification as a particular class. Domain adaptation should lead to differences in these important regions. In particular, without domain adaptation, the neural network can often identify incorrect or spurious regions in images it was not trained on in the target domain, while the classifier that works correctly should focus on regions that contain useful information for the given classification task.</p>
<p>Here we will use the Gradient-weighted Class Activation Mapping (Grad-CAM) method (Selvaraju et al. 2020) to visualize the regions which differently trained models identify as the most salient information in the image. This method calculates class -specific gradients of the output score with respect to the activation maps (i.e. feature maps) of the last convolutional layer in the network. Here activation map dimensions are × = pixels and lists the number of feature maps. These gradients are global-average-pooled to calculate the importance weights for a particular class :
= 1 ∑︁ ∑︁ .(12)
Grad-CAMs are then produced by applying a ReLU function (to extract positive activation regions for the particular class ) to the weighted combination of feature maps in the last convolutional layer:
Grad−CAM, = ReLU ∑︁ .(13)
In Figure 6, we plot the last convolutional layer Grad-CAMs for simulation-to-simulation experiments with the DeepMerge network. We display plots for DeepMerge instead of ResNet18 due to the fact that the dimension of the last convolutional layer in ResNet18 is smaller, resulting in low-resolution Grad-CAMs that are much harder to interpret. The first column shows an example of a merging galaxy from the source domain at the top and from the target domain at the bottom; recall that these two domains differ only by the inclusion of noise. The second, third, and fourth columns show Grad-CAMs for the images in question for classification into a merger class, when training without DA, with MMD, and with MMD, Fisher loss, and entropy minimization, respectively.</p>
<p>In the case of training without domain adaptation in the second column, the network is focusing on the periphery of the galaxy in the source domain, exactly where a lot of interesting asymmetric and clumpy features are expected to appear in the case of mergers. These features are faint, and a lot of this information is lost in the target domain due to the inclusion of mimicked observational noise. As expected, the classifier does not work in the target domain: we can see that the network focuses on the noise instead. When domain adaptation is introduced-MMD in the third row and MMD with Fisher loss and entropy minimization in the fourth column-the network learns to focus on the central brightest regions of the galaxy, which are visible in both domains, and successfully performs classification in both cases.</p>
<p>Likewise, we plot Grad-CAMs for the simulated-to-real experiments with the DeepMerge network in Figure 7. Here we show multiple true merger images from the source and target domains in the top left and right columns, respectively. The second and third row show Grad-CAMs for these images when training without domain adaptation to highlight what the network focuses on for both merger and non-merger classes. Finally, the fourth and fifth rows show merger class and non-merger class Grad-CAMs for training with MMD with transfer learning.</p>
<p>In the source domain Grad-CAMs-where the classifier works with and without domain adaptation-the neural network searches the periphery when classifying an example as a merger, while it focuses at the bright center when classifying non-mergers. This is to be expected, since mergers often have a lot of useful information on the periphery, while non-mergers are often very compact and only have a bright center in the middle of the image. On the other hand, the Grad-CAMs for the target domain without domain adaptation, i.e. for the unsuccessful classifier, demonstrate the network's focus on the noise. This even leads to the inverted characteristics from those described in the source domain: here classification as a merger depends on the bright center and classification as a non-merger depends on the peripheral information. This leads to the classifier failing to successfully distinguish mergers and non-mergers in the target domain. Finally, when training with MMD and transfer learning, the Grad-CAMs start to resemble those in the source domain and classification is successful in the target domain as well.</p>
<p>Issues for Deployment in the Sciences: Avoiding Negative Transfer and Dealing With Small Datasets</p>
<p>The ability to achieve success in combining cross-domain knowledge will largely depend on data availability and quality. We stress the importance of making efforts to achieve likeness between domains, particularly in the sciences. Most domain adaptation techniques were developed for use across very similar domains, such as pictures of office supplies in different settings in the Office-31 dataset (Saenko et al. 2010;Venkateswara et al. 2017). Scientists hoping to deploy these DA techniques face a much more nontrivial task, demonstrated by the challenges we encountered when trying to apply MMD and ad-versarial domain adaptation to the simulated-to-real datasets, which were both small and considerably different across domains.</p>
<p>Substantial domain divergence can result in suboptimal performance or even render domain adaptation techniques virtually useless. The presence of additional classes in the target domain or outliers within the same class make domain adaptation very difficult. Training a model on a demanding problem across dissimilar domains, which occurs frequently in the sciences, may lead to what is known as negative transfer. Negative transfer is when, rather than aiding domain adaptation, the knowledge learned from the source domain actually negatively impacts the performance of the classifier in the target domain. For a comprehensive survey of negative transfer and common methods to mitigate it see Zhang et al. (2020b).</p>
<p>Therefore, despite its promise to help mitigate the issue of dealing with large unlabeled datasets, applying domain adaptation in the sciences may not be very straightforward. Scientists that wish to use DA techniques should strive to make their domains as similar as possible through careful dataset selection and image pre-processing (adding realistic noise, PSF, and other observational effects to simulated astronomical images). In crafting the simulated-to-real dataset in this paper, we made several choices to increase the likeness between our two domains, including choosing to use a small merger window to remove relaxed merger systems from our source domain, which were not present in our target domain. However, even in the case where our target distribution includes only post-mergers, DA was not very successful without the inclusion of transfer learning. Other clever approaches may be taken to decrease domain discrepancy. For example, it was shown in Bottrell et al. (2019) that standard deep learning algorithms trained to distinguish between galaxy merger stages in realistic data without domain adaption do not perform well when trained on pristine simulated images or even simulated images with included realistic PSF and noise. The authors achieve their best performance when simulated images were inserted into the realistic sky in order to introduce examples of crowding from nearby sources. This enabled the model to learn the distinction between crowding of nearby sources and close merging pairs. In this paper, the authors also show that achieving this observational realism is more important for good classification across domains than attempting to add more intricate details to computationally expensive simulations.</p>
<p>Beyond dealing with negative transfer, data augmentation and transfer learning can aid substantially in training with small datasets.</p>
<p>In this paper, we demonstrated that a combination of MMD and transfer learning from our simulated-to-simulated dataset enhanced the learning of correct features in our extremely small simulated-toreal dataset. Even in the case of small and quite discrepant domains, domain adaptation techniques can be successfully used to improve performance of deep learning algorithms on unlabeled observational data.</p>
<p>In the future, more refined domain adaptation techniques will likely be needed in the sciences. In the all too prevalent case when classes look very different across source and target domains, future work will include applying methods such as class-aware Contrastive Domain Discrepancy networks (Kang et al. 2019), that promote class compactness and overlap between class distributions from different domains-rather than the overlap of entire source and target distributions performed with MMD-to achieve even higher accuracies in the target domain.</p>
<p>Despite the issues mentioned above, we maintain that the prospect for domain adaptation's use in the sciences is extremely promising. With ongoing domain adaptation development both within computer science and by those who leverage it creatively in other sciences, we are confident that DA will soon become a staple in the natural scientist's toolkit to leverage all available data.</p>
<p>CONCLUSION</p>
<p>In this paper, we focus on applying domain adaptation techniques to the astronomical context of studying galaxy mergers. Galaxy mergers are crucial in the study of galaxy morphology, evolution, star formation, as well as particle acceleration and the evolution of matter in the universe. Finding comprehensive samples of merging galaxies in different merger stages is very important for the study of these long processes, and we are excited about what the next era of large-scale survey data will bring.</p>
<p>MMD and adversarial training using DANNs show great promise for use in classifying galaxy mergers across domains. We showed here how they can help improve classification accuracies in an unlabeled target domain, thereby allowing models trained on simulated labeled data to be successfully applied on both mimicked and real observational data. While we demonstrate successful implementation of both methods in the simulated-to-simulated dataset for both Deep-Merge and ResNet18, we also present promising results for MMD combined with transfer learning in the simulated-to-real dataset in DeepMerge. In both types of experiments we were able to increase the target dataset accuracy in the testing phase by up to ∼ 20%.</p>
<p>While we found that MMD and adversarial training can be challenging to fine-tune for nontrivial scientific tasks, we conclude that domain adaptation techniques will soon flourish as a necessary tool in astronomy and other natural sciences. These techniques will play an important role in successful use of deep learning algorithms on huge datasets from future large astronomical surveys, and might even help in real-time detection of transient objects and other interesting phenomena. We affirm that domain adaptation techniques will prove essential to building deep learning models that can combine and harness all available observational and simulated data, a tantalizing prospect in the sciences. The authors of this paper have committed themselves to performing this work in an equitable, inclusive, and just environment, and we hold ourselves accountable, believing that the best science is contingent on a good research environment. We acknowledge the Deep Skies Lab as a community of multi-domain experts and collaborators who have facilitated an environment of open discussion, idea-generation, and collaboration. This community was important for the development of this project.</p>
<p>ACKNOWLEDGEMENTS</p>
<p>We also thank K. Pedro, N. Tran, W. J. Pearson, Y. Zhang and M. Vasist for valuable discussion and comments.</p>
<p>Author Contributions</p>
<p>A. Ćiprĳanović-Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Resources, Software, Visualization, Project administration, Supervision, Writing of original draft; D. Kafkes-Formal analysis, Investigation, Methodology, Resources, Software, Visualization, Writing of original draft; K. Downey-Data curation, Formal analysis, Investigation, Software, Visualization; S. Jenkins-Formal analysis, Investigation, Software, Visualization, Writing (review &amp; editing); G. N. Perdue-Investigation, Methodology, Project administration, Resources, Software, Supervision, Writing (review &amp; editing); S. Madireddy-Resources, Software, Methodology, Supervision, Writing (review &amp; editing); T. Johnson-Methodology, Supervision, Writing (review &amp; editing); G. F. Snyder-Methodology, Conceptualization, Data curation, Writing (review &amp; editing); B. Nord-Methodology, Conceptualization, Supervision, Writing (review &amp; editing).</p>
<p>DATA AND CODE AVAILABILITY</p>
<p>All simulated Illustris datasets and observed SDSS dataset are available on Zenodo. For code see our GitHub page.</p>
<p>APPENDIX A: NEURAL NETWORK HYPERPARAMETERS AND TRAINING DETAILS</p>
<p>Here we list all relevant details related to training the neural networks used in this paper. In Table A1, we give details about the architecture of DeepMerge. For ResNet18 architecture, see He et al. (2015).</p>
<p>For both DeepMerge and ResNet18, we performed hyperparameter optimization experiments with DeepHyper (Balaprakash et al. 2018;Balaprakash et al. 2019), an open source mixed-integer nonlinear optimization framework which employs a parallel asynchronousmodel-based search approach to find the high-performing parameter configuration in this mixed (categorical, continuous, integer) search space. Our objective is to minimize the total loss L TOT for MMD hyperparameter searches and to minimize a more complex objective for adversarial domain adaptation: L CL (1 + ΔA s + ΔA t ), where ΔA s = (0.5 − A s ) 2 and ΔA t = (0.5 − A t ) 2 . This was done in an attempt to prevent domain classifier mode collapse (all images classified as one domain) and limit how much the domain classifier's source domain accuracy A s and target domain accuracy A t differed from 0.5, the ideal result for a perfectly confused domain classifier. Different hyperparameters were needed for different experiments, i.e. their values are highly dependent not only on the network, but also on the domain adaptation technique used and the dataset in question. We list all hyperparameters used in the simulated-to-simulated experiments in Table A2 and the simulated-to-real experiments in Table A3.</p>
<p>For training both DeepMerge and ResNet18, we use the Adam optimizer (Kingma &amp; Ba 2015). Additionally, we implement "onecycle" learning rate scheduling (Smith &amp; Topin 2019), which splits a given cycle length in half and includes equal length linear scaling up and down of values from a minimum to a maximum value (like a sawtooth pattern). This technique offers the best of both worlds: higher learning rates assist in regularization by enabling egress from saddle points and lower learning rates prevent the training from diverging Smith &amp; Topin (2019). It was shown by Smith &amp; Topin (2019) that this one-cycle learning leads to much faster convergence of training accuracy. If present within the chosen optimizer, momentum also follows this scaling with optional annihilation. If specified, annihilation requires rapid scaling to small learning rate values following the cycling down period in order to enable a steeper fall into a local minimum in the loss-landscape.</p>
<p>Following Smith &amp; Topin (2019), the choice of maximum and minimum learning rates for this technique were performed using a Learning Rate Range Scan in which we linearly increased values over several orders of magnitude during the first few epochs of training and plotted the associated total loss against the epoch. The minimum of these curves was taken as the maximum value of the learning rate, with the minimum learning rate taken as one order of magnitude lower than the maximum. As for cycle length, results of Smith &amp; Topin (2019) suggest that learning should be relatively robust to choices of between 2-10 epochs per cycle length. Here, we select cycle length through the aforementioned DeepHyper hyperparameter search for both DeepMerge and ResNet18.</p>
<p>In the case of domain adversarial training, we found the performance of the domain classifier to be quite finicky: mode collapse in the domain classifier was common despite training achieving high classification accuracy in the base network. We added the ability to multiply the learning rate of this part of the network by a user-defined constant in order to implement differential learning rates across the networks. For tasks similar to ours, we believe that this factor should often be less than one-indeed small fractions proved quite help-ful to the domain adversarial training, especially with Fisher loss included.</p>
<p>All experiments were run on Google Colab GPU instances and Google Console virtual machine with a NVIDIA Tesla T4 GPU. Our code uses the PyTorch framework and a fixed random seed=1; it is important to note that completely reproducible results are not guaranteed across PyTorch releases or between CPU and GPU executions, even when using fixed random seeds. In the case of training with GPUs, some PyTorch functions that use CUDA can introduce nondeterministic results. We make sure that this is fixed in the backend so that rerunning the code always leads to the same results. Yet we still must note that, even though our code will produce deterministic results when running on a single machine, slight differences will be present in the case of running on different machines.</p>
<p>APPENDIX B: MORE ON NETWORK PERFORMANCE</p>
<p>In this section we provide more details to further compare both Deep-Merge and Resnet18 performance in experiments with and without domain adaptation. To complement the results presented in Section 6.1 for simulated-to-simulated experiments, in Table B1 we compactly display confusion matrices-for the test phase using both source and target data-for all simulated-to-simulated experiments performed with DeepMerge at the top and ResNet18 at the bottom.</p>
<p>We also plot histograms of the output scores for DeepMerge in Figure B1 (left two columns) and ResNet18 (right two columns) for the simulated-to-simulated dataset. In these figures, the source domain is on the left with mergers as dark purple and non-mergers as yellow, and the target domain is on the right with mergers as navy blue and non-mergers as pink. Histograms from top to bottom are ordered as: no domain adaptation, MMD, MMD with Fisher loss and entropy minimization, adversarial domain adaptation, and adversarial domain adaptation with Fisher loss and entropy minimization.</p>
<p>From Figure B1, it is clear that adversarial training, for both Deep-Merge and ResNet18, leads to outputs more tightly concentrated around 0 and 1, i.e. the network seems more confident about the classification, while using MMD leads to slightly more spread out results. Still, confidence is not a guarantee of best classification accuracy and indeed ResNet18 performs the best in the target domain with MMD with Fisher loss and entropy minimization (although, we note that overall the differences in performance between different methods are not large). Also, in the case of the labeled source domain, it is evident that the inclusion of additional losses works as a regularization mechanism, allowing the network to train for longer without overfitting. This effect is particularly noticeable in the case of ResNet18, which is a larger network and thus more prone to overfitting. As a result, in the source domain, we can see that the ResNet18 histograms have more outputs very close to 0 or 1 in experiments with domain adaptation, than without it.</p>
<p>Using the confusion matrices and histograms for the simulatedto-simulated experiments we notice that, depending on the network, inclusion of different losses leads to different behaviours. In the case of DeepMerge, the best performing model uses adversarial training. The inclusion of Fisher loss and entropy minimization with this technique leads to slightly worse performance. On the other hand, in the case of ResNet18, the best performance is the result of using MMD with Fisher loss and entropy minimization. When these additional losses are added to the adversarial trained ResNet18, performance drops significantly: the merger class gets classified incorrectly 46% of the time.</p>
<p>More details related to the results presented in Section 6.2 for We use "channel first" image data format.</p>
<p>simulated-to-real experiments can be found in Table B3, where we present confusion matrices for classification of test source and target datasets with DeepMerge in the case of training without domain adaptation, with MMD, and with MMD with transfer learning. We also plot histograms of the output scores for simulated-to-real experiments in Figure B2-from top to bottom we present no domain adaptation, MMD, and MMD with transfer learning, respectively. It is clear that the addition of MMD results in a broadening of the source histogram distributions, especially for the non-merger class. When transfer learning is employed, distributions are further spread. This provides a visual of how a network trained with domain adaption is restricted to learn the set of domain-invariant features, rather than exploiting all available features in the source domain, which would result in greater confidence. Meanwhile, in the target domain, we see that classifier does not work without DA and with simple MMD, with many examples from both classes classified incorrectly. Inclusion of transfer learning allows correct classification even in the target domain, but with large spread of the output values, especially for the non-merger class.</p>
<p>Finally, we checked the stability of neural network performance to the choice of the random seed. To test this, we trained the DeepMerge architecture, trained on each dataset pair, ten times using ten different seeds for each type of experiment. Table B2 shows the means and standard deviations for all relevant reported performance metrics for the simulation-to-simulation dataset of distant merging galaxies. Since our dataset is very small in the simulated-to-real experiments, we expect a larger spread in performance metric results in the testing stage when different random seeds are used. In Table B4, we again give the mean and standard deviations of different performance metrics, in order to show this slightly larger spread. This paper has been typeset from a T E X/L A T E X file prepared by the author.  Figure B1. Simulated-to-Simulated: Histograms of the output scores of DeepMerge (left two columns) and ResNet18 (right two columns) for the test set of images with the source domain (simulated pristine images) on the left-mergers as dark purple and non-mergers as yellow-and target domain (simulated noisy images) on the right-mergers as navy blue and non-mergers as pink. From top to bottom, results are given for training without domain adaptation, MMD, MMD with Fisher loss and entropy minimization, adversarial training, and adversarial training with Fisher loss and entropy minimization. We plot the histogram of the output scores for all images that represent true mergers, and 1-score for all non-merger images in order to separate the classes for better visibility. Note that the vertical axis range is the same for all experiments except for no domain adaptation for DeepMerge and adversarial domain adaptation for ResNet18 (in order to accommodate larger bars).  Figure B2. Simulated-to-Real: Histograms of the output scores of DeepMerge for the test set of images with the source domain on the left (simulated Illustris images)-mergers as dark purple and non-mergers as yellow-and target domain (real SDSS images) on the right-mergers as navy blue and non-mergers as pink. From top to bottom, results are given for training without domain adaptation, MMD, and MMD with transfer learning. We plot histograms of the output scores for all images that represent true mergers, and 1-score of all non-merger images in order to separate the classes for better visibility. Note that the vertical axis range is the same for all experiments except for no domain adaptation (in order to accommodate larger bars). Table B4. Results from running DeepMerge simulated-to-real experiments with ten different random seeds. Seeds are used for image shuffling, weight initialization, and CUDA backend. We present means and standard deviations for all aforementioned performance metrics.</p>
<p>captures the intraclass dispersion of samples within each class, where h is the latent feature of the -th sample of the -th class (with M being the total number of classes). On the other hand, S b = M =1 (c − c) (c − c) T describes the distances of all class centroids c to the global center c = 1 M M =1 c . This global center, c, is meant to be optimized such that the centroids of the classes are pushed as far away as possible. Traces are used in the computation of the Fisher loss since they are computationally efficient.</p>
<p>Figure 1 .
1Galaxy images from Illustris-1 simulation at = 2. The left column shows merging galaxies and the right column shows non-mergers. The same objects are repeated across rows, with the top showing the source domain, the middle showing the target domain, and the bottom displaying the source objects with logarithmic color map normalization for enhanced visibility.</p>
<p>Figure 2 .
2Galaxy images from Illustris simulation at = 0 mimicking SDSS observations (top row) and real SDSS images (bottom row).</p>
<p>for our test set of images. The top row of plots shows results for the DeepMerge network, while the bottom row is for ResNet18. Bar plots on the left show all performance metrics (accuracy, precision, recall, F1 score, Brier score, and AUC) for our simulated-to-simulated experiments: no domain adaptation (navy blue), MMD (purple), MMD with Fisher and entropy minimization (dark purple), adversarial training (yellow), and adversarial training with Fisher and entropy (pink). The two right panels show ROC curves for all DeepMerge and ResNet18 experiments, with the same color coding as in the bar plots. In all four panels, the solid bars and lines show values for the target domain while the dashed bars and lines show source domain performance.</p>
<p>ResNet18 Figure 3 .
ResNet183The top panel shows classification results for DeepMerge network and the bottom panel for ResNet18. Left: Performance metrics for no domain adaptation experiment (labeled "noDA") in navy blue, MMD in purple, MMD wih Fisher loss and entropy minimization (labeled "MMD+F") in dark purple, adversarial training (labeled "ADA") in yellow and adversarial training with Fisher loss and entropy minimization (labeled "ADA+F") in pink. We plot values for accuracy, precision, recall, F1 score, Brier score, and AUC. Dashed bars show results for the source domain and solid colored bars for the target domain. Right: ROC curves with the same color and line style scheme. In the legend we also give AUC values for all five experiments.</p>
<p>DeepMergeFigure 4 .
4Left: Performance metrics for DeepMerge network for no domain adaptation experiment (labeled "noDA") in navy blue, MMD in purple, MMD with transfer learning (labeled "MMD+TL") in yellow. We plot values for accuracy, precision, recall, F1 score, Brier score, and AUC. Dashed bars show results for the source domain and solid colored bars for the target domain. Right: ROC curves with same color and line style scheme. In legend we also give AUC values for all three experiments.</p>
<p>Figure 5 .Figure 6 .
56t-SNE plots for the DeepMerge network. Red and blue colored dots represent mergers and non-mergers, respectively; transparent dots represent the source domain, while opaque dots represent the target domain. The first panel shows classes before any training, while all other panels show embeddings after 40 epochs. The second panel represents training without any domain adaptation, the third panel show training using MMD as transfer loss and the fourth panel shows MMD with Fisher loss and entropy minimization. Grad-CAMs for simulation-to-simulation experiments in DeepMerge network made from feature maps in the final convolutional layer. The top left image shows an example galaxy merger image from the source domain (plotted with logarithmic colormap to enhance visibility), while the bottom left image is the same galaxy from the target domain. The second, third, and fourth column show Grad-CAMs for those images for classification into a merger class when training without domain adaptation, with MMD, and with MMD, Fisher loss, and entropy minimization, respectively. In the case of training without domain adaptation, the network is focusing on the periphery of the galaxy in the source domain network, and focuses on the noise in the target domain. When domain adaptation is introduced, the network learns to focus on the central brightest regions of the galaxy, which is visible in both domains.</p>
<p>Figure 7 .
7Grad-CAMs for simulation-to-real experiments in DeepMerge network. The top row of images shows examples of true mergers from simulated source dataset on the left and real target dataset on the right. The second row shows Grad-CAMs for these designated example images for classification into the merger class, while row three shows Grad-CAM for the same image for the non-merger class. We can see that, in case of the source domain, the peripheries are important for positive classification in the merger class, while central regions are important for positive classification as a non-merger. In the case of the target domain, both mergers and non-mergers look very different, so Grad-CAMs become noisy and display inverted behavior compared with the source domain. Finally, the third and fourth rows show merger and non-merger Grad-CAMs for the model trained with MMD and transfer learning. The successful domain adaptation is apparent, as the network performs both source and target domain classification in a similar manner as in the case of source domain classification without DA.</p>
<p>This manuscript has been supported by Fermi Research Alliance, LLC under Contract No. DE-AC02-07CH11359 with the U.S. Department of Energy, Office of Science, Office of High Energy Physics. This research has been partially supported by the High Velocity Artificial Intelligence grant as part of the Department of Energy High Energy Physics Computational HEP sessions program. This research used resources of the Argonne Leadership Computing Facility at Argonne National Laboratory, which is a user facility supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357.</p>
<p>Table 1 .
1Performance metrics of the DeepMerge and ResNet18 CNNs, on source and target domain test sets, without domain adaptation (first row) and when domain adaptation techniques are used during training (all other rows). The table shows AUC, Accuracy, Precision, Recall, F1 score, and Brier score.Simulated-to-Simulated </p>
<p>Table 2 .
2Performance metrics of DeepMerge, on source simulated data and target observational data in the testing phase: without domain adaptation (first row), MMD only (middle row), and MMD with tranfer learning (bottom row). The table shows AUC, accuracy, precision, recall, F1 score, and Brier score.Simulated-to-Real </p>
<p>Loss 
Metric 
DeepMerge 
Source Target </p>
<p>No Domain Adaptation </p>
<p>AUC 
0.97 
0.58 
Accuracy 
0.92 
0.50 
Precision 
0.91 
0.50 
Recall 
0.92 
0.80 
F1 score 
0.92 
0.62 
Brier score 
0.06 
0.49 </p>
<p>MMD </p>
<p>AUC 
0.98 
0.60 
Accuracy 
0.94 
0.53 
Precision 
0.92 
0.53 
Recall 
0.95 
0.63 
F1 score 
0.94 
0.58 
Brier score 
0.05 
0.40 </p>
<p>Transfer Learning + MMD </p>
<p>AUC 
0.90 
0.76 
Accuracy 
0.83 
0.69 
Precision 
0.89 
0.68 
Recall 
0.74 
0.74 
F1 score 
0.80 
0.71 
Brier score 
0.13 
0.23 </p>
<p>Table A1 .
A1Architecture of the DeepMerge CNN used in this paper.Layers 
Properties 
Stride Padding Output Shape Parameters </p>
<p>Input 
3 × 75 × 75 
-
-
(3, 75, 75) 
0 
Convolution (2D) 
Filters: 8 
1 
2 
(8, 75, 75) 
608 
Kernel: 5 × 5 
-
-
-
-
Activation: ReLU 
-
-
-
-
Batch Normalization 
-
-
-
(8, 75, 75) 
16 
MaxPooling 
Kernel: 2 × 2 
2 
0 
(8, 37, 37) 
0 
Convolution (2D) 
Filters: 16 
1 
1 
(16, 37, 37) 
1168 
Kernel: 3 × 3 
-
-
-
-
Activation: ReLU 
-
-
-
-
Batch Normalization 
-
-
-
(16, 37, 37) 
32 
MaxPooling 
Kernel: 2 × 2 
2 
0 
(16, 18, 18) 
0 
Convolution (2D) 
Filters: 32 
1 
1 
(32, 18, 18) 
4640 
Kernel: 3 × 3 
-
-
-
-
Activation: ReLU 
-
-
-
-
Batch Normalization 
-
-
-
(32, 18, 18) 
64 
MaxPooling 
Kernel: 2 × 2 
2 
0 
(32, 9, 9) 
0 
Flatten 
-
-
-
(2592) 
-
Fully connected 
Activation: ReLU 
-
-
(64) 
165952 
Fully connected 
Activation: ReLU 
-
-
(32) 
2080 
Fully connected 
Activation: Softmax -
-
(2) 
66 </p>
<p>Table A2 .
A2The hyperparameters used to train in the simulated-to-simulated dataset experiments. The third and fourth columns list parameters for DeepMerge and ResNet18, respectively. The first row shows parameters for our baseline case without domain adaptation, while all other rows give parameters used in different domain adaptation experiments.Table A3. The hyperparameters used to train DeepMerge on the simulated-to-real dataset. The second column of the table shows values for the baseline without domain adaptation, while the third column gives parameters for MMD and the fourth for MMD with transfer learning from the simulated-to-simulated dataset.Simulated-to-Simulated 
Experiment 
Hyperparameters 
DeepMerge 
ResNet18 </p>
<p>No Domain Adaptations </p>
<p>Learning rate 
0.001 
0.00023 
Beta 
(0.7, 0.8) 
(0.7, 0.8) 
Weight Decay 
0.01 
0.1 
Epsilon 
10 −8 
10 −8 
Cycle Length 
8 
2 
Early stopping patience 
20 
15 
Dropout 
N/A 
0.6 </p>
<p>MMD </p>
<p>Learning rate 
0.001 
0.00023 
Beta 
(0.7, 0.8) 
(0.7, 0.8) 
Weight Decay 
0.0001 
0.08 
Epsilon 
10 −8 
10 −8 
Cycle Length 
8 
2 
Early stopping patience 
20 
15 </p>
<p>TL </p>
<p>1 
0.01 </p>
<p>MMD + Fisher + Entropy </p>
<p>Learning rate 
0.001 
0.00023 
Beta 
(0.7, 0.8) 
(0.7, 0.8) 
Weight Decay 
0.0001 
0.08 
Epsilon 
10 −8 
10 −8 
Cycle Length 
8 
2 
Early stopping patience 
20 
15 </p>
<p>TL </p>
<p>1 
1 </p>
<p>w </p>
<p>0.01 
.01 </p>
<p>b </p>
<p>1.0 
1.0 </p>
<p>EM </p>
<p>0.05 
0.05 </p>
<p>Adversarial </p>
<p>Learning rate 
0.001 
0.001 
Beta 
(0.7, 0.8) 
(0.7, 0.8) 
Weight Decay 
0.0001 
0.0001 
Epsilon 
10 −8 
10 −8 
Cycle Length 
8 
2 
Early stopping patience 
20 
15 </p>
<p>TL </p>
<p>1 
0.1 
Domain class. LR mult. 
1 
1 </p>
<p>Adversarial + Fisher + Entropy </p>
<p>Learning rate 
0.001 
0.001 
Beta 
(0.7, 0.8) 
(0.7, 0.8) 
Weight Decay 
0.0001 
0.0001 
Epsilon 
10 −8 
10 −8 
Cycle Length 
8 
8 
Early stopping patience 
20 
15 </p>
<p>TL </p>
<p>1 
0.1 </p>
<p>w </p>
<p>0.0001 
0.01 </p>
<p>b </p>
<p>1 
1 </p>
<p>EM </p>
<p>0.0001 
0.05 
Domain class. LR mult. 
0.01 
0.1 </p>
<p>DeepMerge: Simulated-to-Real 
Hyperparameters 
noDA 
MMD 
TL+MMD 
Learning rate 
0.001 
0.001 
0.01 
Beta 
(0.7, 0.8) 
(0.7, 0.8) 
(0.7, 0.8) 
Weight Decay 
0.001 
0.001 
0.0001 
Epsilon 
10 −8 
10 −8 
10 −8 
Cycle Length 
5 
5 
5 
Early stopping patience 
20 
20 
20 </p>
<p>Table B3 .
B3Normalized confusion matrices for simulated-to-real experiments with DeepMerge. True labels are presented horizontally, while predicted labels are vertical. Finally, the top row shows confusion matrices for the source domain test set of images, while the bottom row give results of the classification of the target test dataset.DeepMerge: Simulated-to-Real 
Experiment 
noDA 
MMD 
MMD + TL 
True Label 
M 
NM 
M 
NM 
M 
NM </p>
<p>Source 
Predicted Label </p>
<p>M 
0.92 0.08 
0.95 0.07 
0.74 0.09 
NM 
0.08 0.92 
0.05 0.93 
0.26 0.91 </p>
<p>Target 
M 
0.80 0.81 
0.63 0.58 
0.74 0.37 
NM 
0.20 0.19 
0.37 0.42 
0.26 0.63 </p>
<p>MNRAS 000, 1-17(2021)
2 ,(7)MNRAS 000, 1-17(2021)</p>
<p>. S Ackermann, K Schawinski, C Zhang, A K Weigel, M D Turp, 10.1093/mnras/sty1398MNRAS. 479415Ackermann S., Schawinski K., Zhang C., Weigel A. K., Turp M. D., 2018, MNRAS, 479, 415</p>
<p>P Balaprakash, M Salim, T D Uram, V Vishwanath, S M Wild, 2018 IEEE 25th International Conference on High Performance Computing (HiPC). Balaprakash P., Salim M., Uram T. D., Vishwanath V., Wild S. M., 2018, in 2018 IEEE 25th International Conference on High Performance Com- puting (HiPC). pp 42-51</p>
<p>P Balaprakash, R Egele, M Salim, S Wild, V Vishwanath, F Xia, T Brettin, R Stevens, 10.1145/3295500.3356202Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '19. the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '19New York, NY, USAAssociation for Computing MachineryBalaprakash P., Egele R., Salim M., Wild S., Vishwanath V., Xia F., Brettin T., Stevens R., 2019, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '19. Association for Computing Machinery, New York, NY, USA, doi:10.1145/3295500.3356202, https://doi.org/ 10.1145/3295500.3356202</p>
<p>. C Bottrell, 10.1093/mnras/stz2934Monthly Notices of the Royal Astronomical Society. 4905390Bottrell C., et al., 2019, Monthly Notices of the Royal Astronomical Society, 490, 5390</p>
<p>. F Chollet, CoRR, abs/1610.02357Chollet F., 2016, CoRR, abs/1610.02357</p>
<p>. A Ćiprĳanović, G F Snyder, B Nord, J E G Peek, 10.1016/j.ascom.2020.100390Astronomy and Computing. 32100390Ćiprĳanović A., Snyder G. F., Nord B., Peek J. E. G., 2020, Astronomy and Computing, 32, 100390</p>
<p>. C J Conselice, M A Bershady, M Dickinson, C Papovich, 10.1086/377318AJ. 1261183Conselice C. J., Bershady M. A., Dickinson M., Papovich C., 2003, AJ, 126, 1183</p>
<p>A Comprehensive Survey on Domain Adaptation for Visual Applications. G Csurka, 10.1007/978-3-319-58347-1_1Springer International PublishingChamCsurka G., 2017, A Comprehensive Survey on Domain Adaptation for Visual Applications. Springer International Publishing, Cham, pp 1- 35, doi:10.1007/978-3-319-58347-1_1, https://doi.org/10.1007/ 978-3-319-58347-1_1</p>
<p>. D W Darg, 10.1111/j.1365-2966.2009.15686.xMonthly Notices of the Royal Astronomical Society. 4011043Darg D. W., et al., 2010, Monthly Notices of the Royal Astronomical Society, 401, 1043</p>
<p>. De Lucia, G Blaizot, J , 10.1111/j.1365-2966.2006.11287.xMNRAS. 3752De Lucia G., Blaizot J., 2007, MNRAS, 375, 2</p>
<p>J Deng, W Dong, R Socher, L Li, Kai Li Li Fei-Fei, 10.1109/CVPR.2009.52068482009 IEEE Conference on Computer Vision and Pattern Recognition. Deng J., Dong W., Socher R., Li L., Kai Li Li Fei-Fei 2009, in 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp 248-255, doi:10.1109/CVPR.2009.5206848</p>
<p>. S P Driver, 10.1111/j.1468-4004.2009.50512.xAstronomy and Geophysics. 5012Driver S. P., et al., 2009, Astronomy and Geophysics, 50, 5.12</p>
<p>. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M March, V Lempitsky, Journal of Machine Learning Research. 171Ganin Y., Ustinova E., Ajakan H., Germain P., Larochelle H., Laviolette F., March M., Lempitsky V., 2016, Journal of Machine Learning Research, 17, 1</p>
<p>. M Ghifary, D Balduzzi, W B Kleĳn, M Zhang, 10.1109/TPAMI.2016.2599532IEEE TPAMI. 39Ghifary M., Balduzzi D., Kleĳn W. B., Zhang M., 2017, IEEE TPAMI, 39, 1414-1430</p>
<p>Y Grandvalet, Y Bengio, Advances in Neural Information Processing Systems 17. Saul L. K., Weiss Y., Bottou L.MIT PressGrandvalet Y., Bengio Y., 2005, in Saul L. K., Weiss Y., Bottou L., eds, , Advances in Neural Information Processing Systems 17. MIT Press, pp 529-536, http://papers.nips.cc/paper/ 2740-semi-supervised-learning-by-entropy-minimization. pdf</p>
<p>A Gretton, D Sejdinovic, H Strathmann, S Balakrishnan, M Pontil, K Fukumizu, B K Sriperumbudur, Advances in Neural Information Processing Systems 25. Pereira F., Burges C. J. C., Bottou L., Weinberger K. Q.Curran Associates, IncGretton A., Sejdinovic D., Strathmann H., Balakrishnan S., Pontil M., Fukumizu K., Sriperumbudur B. K., 2012a, in Pereira F., Burges C. J. C., Bottou L., Weinberger K. Q., eds, , Advances in Neural Information Processing Systems 25. Curran Asso- ciates, Inc., pp 1205-1213, http://papers.nips.cc/paper/ 4727-optimal-kernel-choice-for-large-scale-two-sample-tests. pdf</p>
<p>. A Gretton, K M Borgwardt, M J Rasch, B Schölkopf, A Smola, Journal of Machine Learning Research. 13723Gretton A., Borgwardt K. M., Rasch M. J., Schölkopf B., Smola A., 2012b, Journal of Machine Learning Research, 13, 723</p>
<p>. K He, X Zhang, S Ren, J Sun, arXiv:1512.03385He K., Zhang X., Ren S., Sun J., 2015, arXiv e-prints, p. arXiv:1512.03385</p>
<p>. G Kang, L Jiang, Y Yang, A G Hauptmann, arXiv:1901.00976Kang G., Jiang L., Yang Y., Hauptmann A. G., 2019, arXiv e-prints, p. arXiv:1901.00976</p>
<p>D P Kingma, J Ba, 3rd International Conference on Learning Representations. Bengio Y., LeCun Y.San Diego, CA, USAConference Track ProceedingsKingma D. P., Ba J., 2015, in Bengio Y., LeCun Y., eds, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. http://arxiv. org/abs/1412.6980</p>
<p>. M G Kitzbichler, S D M White, 10.1111/j.1365-2966.2007.11458.xMNRAS. 3762Kitzbichler M. G., White S. D. M., 2007, MNRAS, 376, 2</p>
<p>. S Kullback, R A Leibler, 10.1214/aoms/1177729694Ann. Math. Statist. 2279Kullback S., Leibler R. A., 1951, Ann. Math. Statist., 22, 79</p>
<p>. L Lin, 10.1086/427183ApJ. 6179Lin L., et al., 2004, ApJ, 617, L9</p>
<p>. C J Lintott, 10.1111/j.1365-2966.2008.13689.xMonthly Notices of the Royal Astronomical Society. 3891179Lintott C. J., et al., 2008, Monthly Notices of the Royal Astronomical Society, 389, 1179</p>
<p>. C Lintott, 10.1111/j.1365-2966.2010.17432.xMonthly Notices of the Royal Astronomical Society. 410166Lintott C., et al., 2010, Monthly Notices of the Royal Astronomical Society, 410, 166</p>
<p>M Liu, O Tuzel, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Lee D. D., Sugiyama M., von Luxburg U., Guyon I., Garnett R.Barcelona, SpainLiu M., Tuzel O., 2016, in Lee D. D., Sugiyama M., von Luxburg U., Guyon I., Garnett R., eds, Advances in Neural Information Pro- cessing Systems 29: Annual Conference on Neural Information Pro- cessing Systems 2016, December 5-10, 2016, Barcelona, Spain. pp 469-477, https://proceedings.neurips.cc/paper/2016/hash/ 502e4a16930e414107ee22b6198c578f-Abstract.html</p>
<p>. J M Lotz, J Primack, P Madau, 10.1086/421849AJ. 128163Lotz J. M., Primack J., Madau P., 2004, AJ, 128, 163</p>
<p>. P Madau, M Dickinson, 10.1146/annurev-astro-081811-125615Annual Review of Astronomy and Astrophysics. 52415Madau P., Dickinson M., 2014, Annual Review of Astronomy and Astro- physics, 52, 415</p>
<p>. D Nelson, 10.1093/mnras/stx3040MNRAS. 475624Nelson D., et al., 2018, MNRAS, 475, 624</p>
<p>. S J Pan, I W Tsang, J T Kwok, Q Yang, 10.1109/TNN.2010.2091281Trans. Neur. Netw. 22Pan S. J., Tsang I. W., Kwok J. T., Yang Q., 2011, Trans. Neur. Netw., 22, 199-210</p>
<p>. W J Pearson, L Wang, J W Trayford, C E Petrillo, F F S Van Der Tak, 10.1051/0004-6361/201935355A&amp;A. 62649Pearson W. J., Wang L., Trayford J. W., Petrillo C. E., van der Tak F. F. S., 2019, A&amp;A, 626, A49</p>
<p>. A Pillepich, 10.1093/mnras/stx2656MNRAS. 4734077Pillepich A., et al., 2018, MNRAS, 473, 4077</p>
<p>K Saenko, B Kulis, M Fritz, T Darrell, Proceedings of the 11th European Conference on Computer Vision: Part IV. ECCV'10. the 11th European Conference on Computer Vision: Part IV. ECCV'10Berlin, HeidelbergSpringer-VerlagSaenko K., Kulis B., Fritz M., Darrell T., 2010, in Proceedings of the 11th European Conference on Computer Vision: Part IV. ECCV'10. Springer- Verlag, Berlin, Heidelberg, p. 213-226</p>
<p>. J Schaye, 10.1093/mnras/stu2058MNRAS. 446521Schaye J., et al., 2015, MNRAS, 446, 521</p>
<p>. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, 10.1007/s11263-019-01228-7Int. J. Comput. Vis. 128336Selvaraju R. R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D., 2020, Int. J. Comput. Vis., 128, 336</p>
<p>Boletin de la Asociacion Argentina de Astronomia La Plata Argentina. J L Sérsic, 699Sérsic J. L., 1963, Boletin de la Asociacion Argentina de Astronomia La Plata Argentina, 6, 99</p>
<p>The Bell System Technical Journal. C E Shannon, 27379Shannon C. E., 1948, The Bell System Technical Journal, 27, 379</p>
<p>J Shen, Y Qu, W Zhang, Y Yu, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). McIlraith S. A., Weinberger K. Q.the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressShen J., Qu Y., Zhang W., Yu Y., 2018, in McIlraith S. A., Weinberger K. Q., eds, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press, pp 4058-4065, https://www. aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17155</p>
<p>K Simonyan, A Zisserman, arXiv:1409.1556Very Deep Convolutional Networks for Large-Scale Image Recognition. Simonyan K., Zisserman A., 2015, Very Deep Convolutional Networks for Large-Scale Image Recognition (arXiv:1409.1556)</p>
<p>L N Smith, N Topin, 10.1117/12.2520589Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. SPIE. Pham T., ed.11006Smith L. N., Topin N., 2019, in Pham T., ed., Vol. 11006, Artifi- cial Intelligence and Machine Learning for Multi-Domain Operations Applications. SPIE, pp 369 -386, doi:10.1117/12.2520589, https: //doi.org/10.1117/12.2520589</p>
<p>A Smola, A Gretton, L Song, B Schölkopf, Algorithmic Learning Theory. Berlin, GermanySpringer4754Smola A., Gretton A., Song L., Schölkopf B., 2007, in Algorithmic Learn- ing Theory, Lecture Notes in Computer Science 4754. Springer, Berlin, Germany, pp 13-31</p>
<p>. G F Snyder, 10.1093/mnras/stv2078MNRAS. 4541886Snyder G. F., et al., 2015, MNRAS, 454, 1886</p>
<p>. G F Snyder, V Rodriguez-Gomez, J M Lotz, P Torrey, A C N Quirk, L Hernquist, M Vogelsberger, P E Freeman, 10.1093/mnras/stz1059MNRAS. 4863702Snyder G. F., Rodriguez-Gomez V., Lotz J. M., Torrey P., Quirk A. C. N., Hernquist L., Vogelsberger M., Freeman P. E., 2019, MNRAS, 486, 3702</p>
<p>. V Springel, 10.1093/mnras/stx3304MNRAS. 475676Springel V., et al., 2018, MNRAS, 475, 676</p>
<p>B Sun, K Saenko, 10.1007/978-3-319-49409-8_35Computer Vision -ECCV 2016 Workshops. Hua G., Jégou H.Amsterdam, The Netherlands9915Proceedings, Part III. pp 443-450Sun B., Saenko K., 2016, in Hua G., Jégou H., eds, Lecture Notes in Computer Science Vol. 9915, Computer Vision -ECCV 2016 Work- shops -Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III. pp 443-450, doi:10.1007/978-3-319-49409-8_35, https://doi.org/10.1007/978-3-319-49409-8_35</p>
<p>B Sun, J Feng, K Saenko, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI'16. the Thirtieth AAAI Conference on Artificial Intelligence. AAAI'16AAAI PressSun B., Feng J., Saenko K., 2016, in Proceedings of the Thirtieth AAAI Con- ference on Artificial Intelligence. AAAI'16. AAAI Press, p. 2058-2065</p>
<p>. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, abs/1706.07522Venkateswara H., Eusebio J., Chakraborty S., Panchanathan S., 2017, CoRR, abs/1706.07522</p>
<p>. M Vogelsberger, 10.1093/mnras/stu1536MNRAS. 4441518Vogelsberger M., et al., 2014, MNRAS, 444, 1518</p>
<p>. M Wang, W Deng, 10.1016/j.neucom.2018.05.083Neurocomputing. 312135Wang M., Deng W., 2018, Neurocomputing, 312, 135</p>
<p>. L Wang, W J Pearson, V Rodriguez-Gomez, 10.1051/0004-6361/202038084A&amp;A. 64487Wang L., Pearson W. J., Rodriguez-Gomez V., 2020, A&amp;A, 644, A87</p>
<p>. M Wattenberg, F Viégas, I Johnson, G Wilson, D J Cook, 10.1145/3400066ACM Trans. Intell. Syst. Technol. 11Wattenberg M., Viégas F., Johnson I., 2016, Distill Wilson G., Cook D. J., 2020, ACM Trans. Intell. Syst. Technol., 11</p>
<p>. Y Zhang, Y Zhang, Y Wei, K Bai, Y Song, Q Yang, arXiv:2003.05636Zhang Y., Zhang Y., Wei Y., Bai K., Song Y., Yang Q., 2020a, arXiv e-prints, p. arXiv:2003.05636</p>
<p>. W Zhang, L Deng, L Zhang, D Wu, arXiv:2009.00909arXiv e-printsZhang W., Deng L., Zhang L., Wu D., 2020b, arXiv e-prints, p. arXiv:2009.00909</p>
<p>. J T A De Jong, The Messenger15444de Jong J. T. A., et al., 2013, The Messenger, 154, 44</p>
<p>. L Van Der Maaten, G Hinton, Journal of Machine Learning Research. 92579van der Maaten L., Hinton G., 2008, Journal of Machine Learning Research, 9, 2579</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3091 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3091</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3091</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-266755880</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.02404v3.pdf" target="_blank">Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Generative AI including large language models (LLMs) has recently gained significant interest in the geoscience community through its versatile task-solving capabilities including programming, arithmetic reasoning, generation of sample data, time-series forecasting, toponym recognition, or image classification. Existing performance assessments of LLMs for spatial tasks have primarily focused on ChatGPT, whereas other chatbots received less attention. To narrow this research gap, this study conducts a zero-shot correctness evaluation for a set of 76 spatial tasks across seven task categories assigned to four prominent chatbots, that is, ChatGPT-4, Gemini, Claude-3, and Copilot. The chatbots generally performed well on tasks related to spatial literacy, GIS theory, and interpretation of programming code and functions, but revealed weaknesses in mapping, code writing, and spatial reasoning. Furthermore, there was a significant difference in the correctness of results between the four chatbots. Responses from repeated tasks assigned to each chatbot showed a high level of consistency in responses with matching rates of over 80% for most task categories in the four chatbots.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3091.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3091.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial reasoning tasks (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial reasoning tasks (including Tic-Tac-Toe placement, box-ordering, and RCC-8 qualitative reasoning) evaluated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluated zero-shot spatial puzzle-like tasks (e.g., placing X's on Tic-Tac-Toe, ordering boxes, RCC-8 queries) using four chatbots to measure spatial reasoning ability and failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Gemini, Claude-3, Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer-based chatbots evaluated as provided by vendors in May 2024; architectures are generative pre-trained transformer variants with proprietary training corpora (details largely closed). Copilot is Microsoft Prometheus variant with additional access to code/web data; Claude-3 is Anthropic's Claude family; Gemini is Google's multimodal chatbot.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Tic-Tac-Toe placement task; ordering boxes; RCC-8 qualitative spatial reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Discrete-grid and relational spatial puzzles requiring identification/placement of tokens on a 3x3 Tic-Tac-Toe board without making 3-in-a-row (place six X's), deduction of vertical ordering of boxes from relational statements, and reasoning about possible RCC-8 relations between regions (qualitative topology). These require combinatorial spatial arrangement, local/global constraint reasoning, and qualitative topological inference.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual prompts only (no images or diagrams); puzzles described in plain text (e.g., 'Place six X's on a Tic Tac Toe board without making three-in-a-row').</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot prompting (each task presented without examples), each task repeated twice in a fresh chat to measure consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative analysis of outputs showed systematic failures: models often produced incorrect arrangements or incorrect logical inferences. The authors note instances where models gave correct answers with incorrect or illogical reasoning. Consistency of repeated responses was measured (matching rates >80% for many categories, but lower for mapping and some spatial reasoning tasks). No internal model interpretability (attention/ablation) was performed; analysis was based on qualitative inspection of responses and correctness counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Spatial reasoning category results (10 tasks per model, counted correct only if both repeated attempts correct): GPT-4 2/10 (20.0%), Gemini 4/10 (40.0%), Claude-3 3/10 (30.0%), Copilot 4/10 (40.0%). Specific task: 'Place six X's...' — all four chatbots failed. Consistency: GPT-4 spatial-reasoning matching rate reported at 70% for repeated tasks (lower than typical >80%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Models failed on multi-step spatial constraint tasks: could not reliably place tokens on grids to satisfy global constraints (e.g., placing six X's without making three-in-a-row), failed certain RCC-8 inference tasks, and failed ordering puzzles (boxes). Failures included incorrect final configurations, incorrect intermediate reasoning, or contradictions across repeats. Authors attribute failures to weak spatial reasoning/planning capabilities of current LLMs and to reliance on pattern/statistical associations rather than structured spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Within this study, GPT-4 performed worst among the four on spatial reasoning (2/10) despite being best overall on other spatial categories; Copilot and Gemini outperformed GPT-4 in this category (4/10). No human baseline is provided in this study. Authors reference other work (Liga & Pasetto, 2023) showing Claude-2 outperformed GPT-4 on 5x5 Tic-Tac-Toe and GPT-4 slightly higher on 3x3, indicating mixed comparative performance across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3091.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3091.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Liga & Pasetto tic-tac-toe study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Testing spatial reasoning of Large Language Models: the case of tic-tac-toe (Liga & Pasetto, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that compared GPT-3.5, GPT-4 and Claude-2 on Tic-Tac-Toe spatial tasks on 3x3 and 5x5 grids and reported differing strengths by model and board size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Testing spatial reasoning of Large Language Models: the case of tic-tac-toe</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Previous-generation LLMs (GPT-3.5 and GPT-4 from OpenAI; Claude-2 from Anthropic). The current paper cites this prior work's empirical comparison but does not reproduce its methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Tic-Tac-Toe on 3x3 and 5x5 grids</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Grid-based game requiring spatial placement reasoning to avoid or create three-in-a-row patterns; larger 5x5 version increases combinatorial complexity and planning horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Described in referenced study (textual description of board and moves) — in this paper it is only cited, not re-run.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Not specified in this paper (referenced study).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Referenced finding: Claude-2 relatively superior in 5x5 scenario, GPT-4 slightly better in 3x3; no attention/interpretability work described in this paper about that study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Summary cited: Claude-2 outperformed GPT-4 on 5x5; GPT-4 slightly higher on 3x3 (exact metrics not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>The paper cites the prior work to illustrate model variability across board sizes; detailed failure modes are in the referenced study, not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Direct model-to-model comparison is reported in the referenced study; this paper uses that citation to contextualize its own spatial reasoning results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3091.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3091.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCC-8 qualitative reasoning (Cohn 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8 (Cohn, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced evaluation of GPT-4's ability to answer RCC-8 qualitative spatial relation queries; used as prior work motivating inclusion of RCC-8 tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 evaluated on RCC-8 qualitative spatial reasoning tasks in the referenced study; in the present paper RCC-8 tasks were included in the task set.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>RCC-8 qualitative spatial reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Logical/relational spatial reasoning tasks concerning topological relations between regions (e.g., DC, EC, TPP) requiring inference of possible relations between entities given constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual symbolic relational statements (e.g., 'If DC(x,y) and TPPi(y,z) then what are the possible relationships between x and z?').</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>In this paper: zero-shot textual prompts; in the referenced Cohn study prompting specifics exist in that work (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>This paper reports that GPT-4 failed the three RCC-8 tasks included in its battery, while Gemini and Claude-3 succeeded on two and Copilot on one; thus GPT-4's RCC-8 performance here contradicts some prior positive results and highlights variability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper's RCC-8 subset: GPT-4 failed all three RCC-8 items included; other chatbots had mixed success (two successes for Gemini and Claude-3, one for Copilot).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>RCC-8 tasks exposed logical-inference weaknesses: GPT-4 produced incorrect answers for items that prior work had reported as solvable, indicating instability and possible sensitivity to prompt formulation or version differences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Referenced prior work (Cohn) reported some success with GPT-4 on RCC-8; this paper's results show worse performance for GPT-4 relative to some other chatbots, underscoring inconsistent cross-study behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3091.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3091.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path planning benchmark (Aghzal et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning (Aghzal, Plaku, & Yao, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work introducing a path-planning benchmark used to evaluate LLMs, including GPT-4 with different prompting methodologies and other models (BART, T5) via fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, BART, T5 (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluations included few-shot and prompt-augmented GPT-4 and fine-tuned sequence models (BART, T5) to test path planning and spatial-temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Path planning benchmark (trajectory / route planning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Tasks require planning feasible paths through spatial environments, combining spatial reasoning with temporal/sequential decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Referenced study uses environment descriptions (likely textual or structured), but this paper only cites the work and does not reproduce its methods.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Referenced: few-shot GPT-4 and fine-tuning for BART/T5; specifics are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>The cited study examined prompting methodologies and fine-tuning; the present paper references it as evidence that few-shot and action-oriented prompting can improve spatial reasoning for planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in detail in this paper — referenced study reported that few-shot GPT-4 showed promise; details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not detailed here; cited as evidence that specialized benchmarks and prompting can help but that challenges remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Referenced comparison across GPT-4 (few-shot) and fine-tuned BART/T5; the present paper uses this to motivate advanced prompt strategies (CoT, ReAct, ToT) for spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3091.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3091.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StepGame / Step-based benchmarks (F. Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark (F. Li, D. C. Hogg, A. G. Cohn, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced recent benchmark (StepGame) and associated study that evaluate and propose enhancements for spatial reasoning in LLMs using structured stepwise tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (including GPT-4 in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>StepGame provides a structured benchmark to probe multi-step spatial reasoning; referenced study evaluates LLMs and explores prompting/augmentation methods to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame benchmark (stepwise spatial reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Benchmarks that decompose spatial tasks into sequential reasoning steps to evaluate multi-step inference and planning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual, stepwise tasks in the referenced study; this paper cites StepGame as related work and suggests chain-of-thought / tree-of-thought prompting as promising.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Referenced study uses enhanced prompting (stepwise/coT-like approaches) to improve performance; the present paper used zero-shot but suggests these methods for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Cited as demonstration that structured stepwise benchmarks and prompting (CoT, ToT, ReAct) can improve spatial reasoning; the paper points to these as directions to address observed failures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper; metrics and improvements are in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>This paper notes that zero-shot LLMs struggled on multi-step spatial tasks and identifies CoT/ToT/ReAct as potential mitigations highlighted by the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Referenced work compares LLM responses under enhanced prompting; this paper references it to contrast zero-shot performance with potential improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Testing spatial reasoning of Large Language Models: the case of tic-tac-toe <em>(Rating: 2)</em></li>
                <li>Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. <em>(Rating: 2)</em></li>
                <li>Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark <em>(Rating: 2)</em></li>
                <li>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8 <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 1)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3091",
    "paper_id": "paper-266755880",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "Spatial reasoning tasks (this study)",
            "name_full": "Spatial reasoning tasks (including Tic-Tac-Toe placement, box-ordering, and RCC-8 qualitative reasoning) evaluated in this paper",
            "brief_description": "The paper evaluated zero-shot spatial puzzle-like tasks (e.g., placing X's on Tic-Tac-Toe, ordering boxes, RCC-8 queries) using four chatbots to measure spatial reasoning ability and failure modes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, Gemini, Claude-3, Copilot",
            "model_description": "Decoder-only transformer-based chatbots evaluated as provided by vendors in May 2024; architectures are generative pre-trained transformer variants with proprietary training corpora (details largely closed). Copilot is Microsoft Prometheus variant with additional access to code/web data; Claude-3 is Anthropic's Claude family; Gemini is Google's multimodal chatbot.",
            "model_size": null,
            "puzzle_name": "Tic-Tac-Toe placement task; ordering boxes; RCC-8 qualitative spatial reasoning tasks",
            "puzzle_description": "Discrete-grid and relational spatial puzzles requiring identification/placement of tokens on a 3x3 Tic-Tac-Toe board without making 3-in-a-row (place six X's), deduction of vertical ordering of boxes from relational statements, and reasoning about possible RCC-8 relations between regions (qualitative topology). These require combinatorial spatial arrangement, local/global constraint reasoning, and qualitative topological inference.",
            "input_representation": "Textual prompts only (no images or diagrams); puzzles described in plain text (e.g., 'Place six X's on a Tic Tac Toe board without making three-in-a-row').",
            "prompting_method": "Zero-shot prompting (each task presented without examples), each task repeated twice in a fresh chat to measure consistency.",
            "spatial_reasoning_analysis": "Qualitative analysis of outputs showed systematic failures: models often produced incorrect arrangements or incorrect logical inferences. The authors note instances where models gave correct answers with incorrect or illogical reasoning. Consistency of repeated responses was measured (matching rates &gt;80% for many categories, but lower for mapping and some spatial reasoning tasks). No internal model interpretability (attention/ablation) was performed; analysis was based on qualitative inspection of responses and correctness counts.",
            "performance_metrics": "Spatial reasoning category results (10 tasks per model, counted correct only if both repeated attempts correct): GPT-4 2/10 (20.0%), Gemini 4/10 (40.0%), Claude-3 3/10 (30.0%), Copilot 4/10 (40.0%). Specific task: 'Place six X's...' — all four chatbots failed. Consistency: GPT-4 spatial-reasoning matching rate reported at 70% for repeated tasks (lower than typical &gt;80%).",
            "limitations_or_failure_modes": "Models failed on multi-step spatial constraint tasks: could not reliably place tokens on grids to satisfy global constraints (e.g., placing six X's without making three-in-a-row), failed certain RCC-8 inference tasks, and failed ordering puzzles (boxes). Failures included incorrect final configurations, incorrect intermediate reasoning, or contradictions across repeats. Authors attribute failures to weak spatial reasoning/planning capabilities of current LLMs and to reliance on pattern/statistical associations rather than structured spatial planning.",
            "comparison_to_other_models_or_humans": "Within this study, GPT-4 performed worst among the four on spatial reasoning (2/10) despite being best overall on other spatial categories; Copilot and Gemini outperformed GPT-4 in this category (4/10). No human baseline is provided in this study. Authors reference other work (Liga & Pasetto, 2023) showing Claude-2 outperformed GPT-4 on 5x5 Tic-Tac-Toe and GPT-4 slightly higher on 3x3, indicating mixed comparative performance across model families.",
            "uuid": "e3091.0",
            "source_info": {
                "paper_title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Liga & Pasetto tic-tac-toe study",
            "name_full": "Testing spatial reasoning of Large Language Models: the case of tic-tac-toe (Liga & Pasetto, 2023)",
            "brief_description": "A referenced study that compared GPT-3.5, GPT-4 and Claude-2 on Tic-Tac-Toe spatial tasks on 3x3 and 5x5 grids and reported differing strengths by model and board size.",
            "citation_title": "Testing spatial reasoning of Large Language Models: the case of tic-tac-toe",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5, GPT-4, Claude-2",
            "model_description": "Previous-generation LLMs (GPT-3.5 and GPT-4 from OpenAI; Claude-2 from Anthropic). The current paper cites this prior work's empirical comparison but does not reproduce its methods.",
            "model_size": null,
            "puzzle_name": "Tic-Tac-Toe on 3x3 and 5x5 grids",
            "puzzle_description": "Grid-based game requiring spatial placement reasoning to avoid or create three-in-a-row patterns; larger 5x5 version increases combinatorial complexity and planning horizon.",
            "input_representation": "Described in referenced study (textual description of board and moves) — in this paper it is only cited, not re-run.",
            "prompting_method": "Not specified in this paper (referenced study).",
            "spatial_reasoning_analysis": "Referenced finding: Claude-2 relatively superior in 5x5 scenario, GPT-4 slightly better in 3x3; no attention/interpretability work described in this paper about that study.",
            "performance_metrics": "Summary cited: Claude-2 outperformed GPT-4 on 5x5; GPT-4 slightly higher on 3x3 (exact metrics not reproduced here).",
            "limitations_or_failure_modes": "The paper cites the prior work to illustrate model variability across board sizes; detailed failure modes are in the referenced study, not reproduced here.",
            "comparison_to_other_models_or_humans": "Direct model-to-model comparison is reported in the referenced study; this paper uses that citation to contextualize its own spatial reasoning results.",
            "uuid": "e3091.1",
            "source_info": {
                "paper_title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "RCC-8 qualitative reasoning (Cohn 2023)",
            "name_full": "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8 (Cohn, 2023)",
            "brief_description": "A referenced evaluation of GPT-4's ability to answer RCC-8 qualitative spatial relation queries; used as prior work motivating inclusion of RCC-8 tasks in this study.",
            "citation_title": "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "GPT-4 evaluated on RCC-8 qualitative spatial reasoning tasks in the referenced study; in the present paper RCC-8 tasks were included in the task set.",
            "model_size": null,
            "puzzle_name": "RCC-8 qualitative spatial reasoning tasks",
            "puzzle_description": "Logical/relational spatial reasoning tasks concerning topological relations between regions (e.g., DC, EC, TPP) requiring inference of possible relations between entities given constraints.",
            "input_representation": "Textual symbolic relational statements (e.g., 'If DC(x,y) and TPPi(y,z) then what are the possible relationships between x and z?').",
            "prompting_method": "In this paper: zero-shot textual prompts; in the referenced Cohn study prompting specifics exist in that work (not specified here).",
            "spatial_reasoning_analysis": "This paper reports that GPT-4 failed the three RCC-8 tasks included in its battery, while Gemini and Claude-3 succeeded on two and Copilot on one; thus GPT-4's RCC-8 performance here contradicts some prior positive results and highlights variability.",
            "performance_metrics": "In this paper's RCC-8 subset: GPT-4 failed all three RCC-8 items included; other chatbots had mixed success (two successes for Gemini and Claude-3, one for Copilot).",
            "limitations_or_failure_modes": "RCC-8 tasks exposed logical-inference weaknesses: GPT-4 produced incorrect answers for items that prior work had reported as solvable, indicating instability and possible sensitivity to prompt formulation or version differences.",
            "comparison_to_other_models_or_humans": "Referenced prior work (Cohn) reported some success with GPT-4 on RCC-8; this paper's results show worse performance for GPT-4 relative to some other chatbots, underscoring inconsistent cross-study behavior.",
            "uuid": "e3091.2",
            "source_info": {
                "paper_title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Path planning benchmark (Aghzal et al. 2024)",
            "name_full": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning (Aghzal, Plaku, & Yao, 2024)",
            "brief_description": "Referenced work introducing a path-planning benchmark used to evaluate LLMs, including GPT-4 with different prompting methodologies and other models (BART, T5) via fine-tuning.",
            "citation_title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning.",
            "mention_or_use": "mention",
            "model_name": "GPT-4, BART, T5 (various sizes)",
            "model_description": "Evaluations included few-shot and prompt-augmented GPT-4 and fine-tuned sequence models (BART, T5) to test path planning and spatial-temporal reasoning.",
            "model_size": null,
            "puzzle_name": "Path planning benchmark (trajectory / route planning tasks)",
            "puzzle_description": "Tasks require planning feasible paths through spatial environments, combining spatial reasoning with temporal/sequential decision-making.",
            "input_representation": "Referenced study uses environment descriptions (likely textual or structured), but this paper only cites the work and does not reproduce its methods.",
            "prompting_method": "Referenced: few-shot GPT-4 and fine-tuning for BART/T5; specifics are in the cited paper.",
            "spatial_reasoning_analysis": "The cited study examined prompting methodologies and fine-tuning; the present paper references it as evidence that few-shot and action-oriented prompting can improve spatial reasoning for planning tasks.",
            "performance_metrics": "Not reported in detail in this paper — referenced study reported that few-shot GPT-4 showed promise; details are in the cited work.",
            "limitations_or_failure_modes": "Not detailed here; cited as evidence that specialized benchmarks and prompting can help but that challenges remain.",
            "comparison_to_other_models_or_humans": "Referenced comparison across GPT-4 (few-shot) and fine-tuned BART/T5; the present paper uses this to motivate advanced prompt strategies (CoT, ReAct, ToT) for spatial reasoning.",
            "uuid": "e3091.3",
            "source_info": {
                "paper_title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "StepGame / Step-based benchmarks (F. Li et al., 2024)",
            "name_full": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark (F. Li, D. C. Hogg, A. G. Cohn, 2024)",
            "brief_description": "Referenced recent benchmark (StepGame) and associated study that evaluate and propose enhancements for spatial reasoning in LLMs using structured stepwise tasks.",
            "citation_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (including GPT-4 in referenced work)",
            "model_description": "StepGame provides a structured benchmark to probe multi-step spatial reasoning; referenced study evaluates LLMs and explores prompting/augmentation methods to improve performance.",
            "model_size": null,
            "puzzle_name": "StepGame benchmark (stepwise spatial reasoning tasks)",
            "puzzle_description": "Benchmarks that decompose spatial tasks into sequential reasoning steps to evaluate multi-step inference and planning capabilities.",
            "input_representation": "Textual, stepwise tasks in the referenced study; this paper cites StepGame as related work and suggests chain-of-thought / tree-of-thought prompting as promising.",
            "prompting_method": "Referenced study uses enhanced prompting (stepwise/coT-like approaches) to improve performance; the present paper used zero-shot but suggests these methods for future work.",
            "spatial_reasoning_analysis": "Cited as demonstration that structured stepwise benchmarks and prompting (CoT, ToT, ReAct) can improve spatial reasoning; the paper points to these as directions to address observed failures.",
            "performance_metrics": "Not reported in this paper; metrics and improvements are in the referenced work.",
            "limitations_or_failure_modes": "This paper notes that zero-shot LLMs struggled on multi-step spatial tasks and identifies CoT/ToT/ReAct as potential mitigations highlighted by the referenced work.",
            "comparison_to_other_models_or_humans": "Referenced work compares LLM responses under enhanced prompting; this paper references it to contrast zero-shot performance with potential improvements.",
            "uuid": "e3091.4",
            "source_info": {
                "paper_title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Testing spatial reasoning of Large Language Models: the case of tic-tac-toe",
            "rating": 2,
            "sanitized_title": "testing_spatial_reasoning_of_large_language_models_the_case_of_tictactoe"
        },
        {
            "paper_title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_good_path_planners_a_benchmark_and_investigation_on_spatialtemporal_reasoning"
        },
        {
            "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
            "rating": 2,
            "sanitized_title": "advancing_spatial_reasoning_in_large_language_models_an_indepth_evaluation_and_enhancement_using_the_stepgame_benchmark"
        },
        {
            "paper_title": "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8",
            "rating": 2,
            "sanitized_title": "an_evaluation_of_chatgpt4s_qualitative_spatial_reasoning_capabilities_in_rcc8"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        }
    ],
    "cost": 0.014088,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks</p>
<p>Hartwig H Hochmair hhhochmair@ufl.edu 
School of Forest, Fisheries, and Geomatics Sciences
Fort Lauderdale Research and Education Center
University of Florida
DavieFLU.S.A</p>
<p>Levente Juhász 
GIS Center
Florida International University
MiamiFLU.S.A</p>
<p>Takoda Kemp 
School of Forest, Fisheries, and Geomatics Sciences
Fort Lauderdale Research and Education Center
University of Florida
DavieFLU.S.A</p>
<p>Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks
64586251DC750A40317FB817133BEDF210.1111/tgis.13233Generative AIzero-shotcodingmappinglarge language modelsspatial tasks
Generative AI including large language models (LLMs) has recently gained significant interest in the geo-science community through its versatile tasksolving capabilities including programming, arithmetic reasoning, generation of sample data, time-series forecasting, toponym recognition, or image classification.Existing performance assessments of LLMs for spatial tasks have primarily focused on ChatGPT, whereas other chatbots received less attention.To narrow this research gap, this study conducts a zero-shot correctness evaluation for a set of 76 spatial tasks across seven task categories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini, Claude-3, and Copilot.The chatbots generally performed well on tasks related to spatial literacy, GIS theory, and interpretation of programming code and functions, but revealed weaknesses in mapping, code writing, and spatial reasoning.Furthermore, there was a significant difference in correctness of results between the four chatbots.Responses from repeated tasks assigned to each chatbot showed a high level of consistency in responses with matching rates of over 80% for most task categories in the four chatbots.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), which simulate human-like conversations, have recently gained widespread popularity due to their versatility and use in conversational agents, text summarization, information retrieval, coding, computer science tasks, translation, reasoning, or solving arithmetic problems, among others.LLMs, such as OpenAI's ChatGPT, have performed well on academic and professional exams across various disciplines, such as mathematics, science, medicine, law (Kung et al., 2023;Ray, 2023;Rudolph, Tan, &amp; Tan, 2023) and geographic information systems (Mooney, Cui, Guan, &amp; Juhász, 2023).Despite these significant advancements, LLMs come with limitations and persistent challenges, including generating factually incorrect information due to hallucination or imperfect mathematical abilities including difficulties with unit conversions, handling numbers in scientific format, calculating descriptive statistics, or misplacing decimal points in arithmetic operations (Tyson, 2023).Their reliance on statistical patterns from training data also means that they lack human-like reasoning and understanding of context (Hadi et al., 2023).Other issues include limited generalizability, performance was found when evaluating the quality of responses to myopia (nearsightedness) related queries (Lim et al., 2023).Another comparison study, which also included Claude, evaluated LLMs in 1002 questions encompassing 27 categories (Borji &amp; Mohammadian, 2023).It demonstrated a success rate of 84.1% for GPT-4, 78.3% for GPT-3.5, 64.5 % for Claude, and 62.4% for Bard.Chatbots demonstrated proficiency in language understanding (spelling, grammar, translation, vocabulary, etc.) and facts, but encountered difficulties in mathematics, coding, and reasoning.Comparison of the performance of three LLMs on a benchmark set of 147 undergraduate-level control problems, which combine mathematical theory and engineering, revealed that Claude 3 Opus (58.5% baseline accuracy) outperformed  and Gemini 1.0 Ultra: (34.0%) (Kevian et al., 2024).All three LLMs faced difficulties in handling problems involving visual elements such as Bode plots and Nyquist plots.</p>
<p>A research gap identified in reviewing previous work is that the evaluation of LLMs on spatial tasks (e.g.spatial reasoning, spatial literacy, GIS operations, spatial data acquisition, toponym recognition, mapping, urban geography, time series forecasting) was primarily focused on OpenAI's chatbots, i.e.,  or GPT-4 (Cohn, 2023;Juhász, Mooney, Hochmair, &amp; Guan, 2023;F. Li, Hogg, &amp; Cohn, 2024;Mai et al., acc.;Mooney et al., 2023;Tao &amp; Xu, 2023) with only few exceptions (Manvi et al., 2023;Yin, Li, &amp; Goldberg, 2023).One study, for example, compared the spatial reasoning activities of GPT-3.5, GPT-4 and Claude-2 in the context of the tic-tac-toe game on 3x3 and 5x5 grids, demonstrating a relative superiority of Claude-2 over GPT-4 in the 5x5 scenario but a slightly higher performance of GPT-4 in the 3x3 grid scenario (Liga &amp; Pasetto, 2023).A new benchmark for path planning tasks was used in (Aghzal, Plaku, &amp; Yao, 2024) to evaluate LLMs including GPT-4 via different prompting methodologies as well as BART and T5 of various sizes via fine-tuning, showing promise of few-shot GPT-4 in spatial reasoning.</p>
<p>Evaluation of 14 LMMs, including GPT-4V(ision) and Gemini, identified significant challenges in disciplines which present more complex visual data and require intricate reasoning, such as business and science (Yue et al., 2023).The proprietary GPT-4V achieved an accuracy of 55.7%, whereas open-source models, such as BLIP2-FLAN-T5-XXL achieved lower accuracies of approximately 34%.GPT-4V was also found to perform best in an evaluation of 12 prominent foundation models with MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks, with an overall accuracy of 49.9%, outperforming Multimodal Bard by 15.1% (Lu et al., 2024).Solving geospatial tasks often requires multi-modal approaches (Mooney et al., 2023).Foundation models for GeoAI should therefore address these challenges by integrating various data types such as text, images, and spatial vectors to enhance performance and applicability across different geospatial domains (Mai et al., acc.).In fact, the new version of GPT-4V already demonstrated proficiency in executing fundamental map reading and analysis tasks (J.Xu &amp; Tao, 2024).</p>
<p>Research objectives</p>
<p>This research aims to provide a closer insight into the correctness of responses from four chatbots for 76 tasks across seven spatial and GIS related categories, using quantitative and qualitative analyses.This will enhance findings from previous work which focused on individual LLMs in evaluating their spatial capabilities.Task definitions in our experiments use a zero-shot prompting approach and thus rely on the model knowledge gained from training data without providing additional examples or demonstrations as part of the prompt.</p>
<p>Each task is presented twice which allows subsequent quantification of consistency in the results.For two task categories verbosity of responses will be examined which allows to better understand which chatbot to consult if more elaborate responses with additional background information are preferred.</p>
<p>Materials and Methods</p>
<p>The study analyses responses to tasks and questions provided to the four analyzed chatbots in their chat interface.For each task a new chat was initiated to prevent use of context for solving subsequent tasks.Each task was given twice so that consistency between responses could be assessed.The experiment was conducted at the beginning of May 2024 using chatbots based on LLM versions that were available at that time.</p>
<p>Task categories</p>
<p>Tasks were divided into seven categories with a spatial context and chosen to evaluate factual knowledge (i.e.spatial literacy, GIS theory), visualization (mapping) and code writing capabilities, and interpretation (of functions and code examples) and spatial reasoning skills.The categories are drawn from earlier LLM evaluation studies (Borji &amp; Mohammadian, 2023;F. Li et al., 2024;Tao &amp; Xu, 2023) using a subset of categories that are suitable for spatial tasks and questions.Questions for some categories (e.g.GIS concepts, spatial reasoning) were adopted from other studies (Cohn, 2023;Mooney et al., 2023).For the spatial literacy category, we aimed to replicate the structure of some questions used in GeoQA systems (Punjani et al., 2018).For the other categories (e.g.coding, function interpretation), tasks were created anew or adopted and modified from other sources.</p>
<p>Each task category contains 10 given tasks except for the spatial literacy category with 16 tasks which cover a wide range of question for different geographic areas.Tasks were provided as text prompt so that LLM analysis capabilities could be assessed using a single input mode without the intermediate step of image analysis which is prone to introducing perceptual errors (Yue et al., 2023).The URL to download the complete set of 76 tasks with their responses for each chatbot is provided in the data availability statement.</p>
<p>Spatial literacy</p>
<p>This category contained 13 questions and 3 tasks on geographic knowledge related to points of interest, countries, highways, rivers, elevations, distances, temperatures, and population.Some questions and tasks involved the relationship between geographic features, such as topological (e.g., bordering countries, crossing of rivers and roads, islands), distance (e.g., closest cities), directional (e.g., cardinal directions between states) or order (e.g., sort cities by elevation or temperature).No instructions about the desired response length were given.Sample questions and tasks include:  In which German city crosses A60 the Rhine river? Are more than 5 countries bordering the Baltic Sea?  What is the single most important geographic commonality between Mauritius, Sardegna, Guernsey and Bali?  Which two cities among Dallas, Atlanta, Memphis, and Oklahoma City are closest to each other considering great circle distances?</p>
<p> In which order does the Danube flow through Bratislava, Budapest, Linz and Vienna? List the UNESCO world heritage sites in Oman.</p>
<p>GIS concepts</p>
<p>This category contains a selection of six true/false and four multiple choice questions.These questions were adopted from an earlier study which examined the knowledge of GIS theory of ChatGPT-3.5 and ChatGPT-4 (Mooney et al., 2023) based on GIS exam questions in a popular textbook for introductory GIS courses (Bolstad &amp; Manson, 2022)</p>
<p>Mapping</p>
<p>LLMs can leverage existing programming language libraries for map design and create executable programming code.They can also generate a URL that points towards a thirdparty online mapping service.In both cases it is necessary for an LLM to provide correct parameter information and follow syntax rules.In this study, chatbot capabilities to generate maps leveraging Mapbox resources (four tasks), Python libraries (three tasks) and R packages (three tasks) were examined.The Mapbox related set of tasks involved the Mapbox Static Images API and the Mapbox GL JS client-side JavaScript library.The following list provides one example from each task group:</p>
<p> Create a Mapbox map link of Vienna with a marker at (16.3692, 48.2034)  Generate code based on the matplotlib library which creates a map that shows the population and location of the 5 largest cities in the U.S.  Generate code that uses the R tidyverse package to plot a world map</p>
<p>Function interpretation</p>
<p>This task category explores chatbot capabilities to explain the purpose of functions commonly used in the spatial sciences.Named labels, such as "distance" or "NDVI", were replaced with more generic labels (such as "a", "b").This forced the LLM to solve a given task through analyzing the function structure rather than names.The ten functions given evolve around coordinate transformation, distance computation, spherical trigonometry, multispectral image analysis, and spatial regression.Two tasks (identify Epanechnikov kernel and spatial lag model) are as follows:</p>
<p> Provide a term commonly used for this equation: K(x) = 3/4 (1-x²) for |x| ≤ 1  Explain in one sentence what this equation is called: =++</p>
<p>Code explanation</p>
<p>This task category is similar to the previous one except that the input is a programming algorithm expressed as Python code (five tasks) and R code (fives tasks).Some provided code snippets use programming libraries and packages, such as ArcPy, math, pathlib (Python) or sf (R).As before, function and variables names were replaced with generic labels.The provided Python code snippets include among others algorithms which create a binary slope map from an elevation raster, compute the shortest path using Dijkstra's algorithm, and find the point of intersection of two lines.The provided R code snippets compute among others the centroid of a polygon, produce a raster grid that overlaps with a given polygon provided as a shapefile, or find the X, Y tile coordinates in the web map tile system based on a given point location.The task associated with the latter algorithm is provided as an example below:</p>
<p>Explain in one sentence the purpose of this function:
my_calc &lt;-function(lat, lon, z){ l &lt;-lat * pi /180 n &lt;-2.0 ^ z x &lt;-floor((lon + 180.0) / 360.0 * n) y = floor((1.0 -log(tan(l) + (1 / cos(l))) / pi) / 2.0 * n) return(c(x, y)) } my_calc(20, -80, 15)</p>
<p>Coding</p>
<p>This task category explores chatbot capabilities to write, modify, and complete programming code (four tasks in Python and two tasks in R), and to translate programming code from Python to R (two tasks) and from R to Python (two tasks).Some tasks involve handling spatial programming libraries and packages, such as ArcPy, shapely (Python), spatstat or sp (R).The six code writing/modification/completion tasks include the following:</p>
<p> Generate Python code which: (1) sorts a set of points with given geographic coordinates from East to West; (2) uses the ArcPy library to generate a line geometry from a set of points with given geographic coordinates and saves the result as a shapefile  Enhance provided Python code with the following functionality: (3) print the id (memory address) of local and global variables; (4) update values for two additional fields of the specified record in a given point feature class  Complete the provided R code to: (5) plot three polygons based on given x/y point coordinates; (6) create and plot a raster map using indirect distance weighting with a power of 0.5</p>
<p>As an example, the input for task ( 5) is as follows: Complete this code to plot three polygons.install.packages("sp")library(sp) cs1 &lt;-rbind(c(7, 5), c(10, 5), c(10, 0), c(5, 0)) cs2 &lt;-rbind(c(5, 5), c(10, 5), c(9, 8)) cs3 &lt;-rbind(c(7, 5), c(3, 5), c(5, 0))</p>
<p>Code translation tasks involve various challenges, such as handling spatial reference information and recursive functions.Following code algorithms needed to be translated:</p>
<p> From R to Python: (1) generate polygons from points with x/y coordinates and plot them;</p>
<p>(2) compute the Haversine distance between two points with given geographic coordinates  From Python to R: (3) sort given point coordinates by longitude and save them as points in a shapefile; (4) sort numbers using a Quicksort algorithm</p>
<p>Spatial reasoning</p>
<p>Tasks in this category were primarily drawn from previous studies on spatial reasoning (Borji &amp; Mohammadian, 2023;Cohn, 2023).Upon initial examination of explanations provided in chatbot responses we noticed that the reasoning behind some tasks was incorrect but still resulted in the correct answer due to the lack of specificity of the question.To avoid these situations, some tasks were either re-written to require provision of a solution (as opposed to only asking a question about the solution) or formulated as a multiple-choice question with a smaller chance of randomly picking the correct answer.For example, the question "Can you place six X's on a Tic Tac Toe board without making three-in-a-row in any direction?"was re-written as "Place six X's on a Tic Tac Toe board without making three-in-a-row in any direction".Tasks in this category required the identification of spatial relationships between objects (e.g.boxes or persons) based on a set of relative position descriptions, point connection and arrangement tasks on grid-like "worlds" (see Tic Tac Toe example above), and qualitative spatial reasoning tasks on the region connection calculus 8 (RCC-8).For the latter, three tasks were chosen from (Cohn, 2023), two of which were previously correctly answered in ChatGPT-4 in that study.One example of the qualitative spatial reasoning tasks on the RCC-8 reads as follows (some parts removed for brevity):</p>
<p> Consider the following set of eight pairwise disjoint and mutually exhaustive binary spatial relations.These relations form part of the wellknown RCC-8 qualitative spatial reasoning calculus.DC(x,y) means that x and y are disconnected and share no spatial parts.EC(x,y) means that x and y touch at a boundary but do not share any interior parts […].TPP(x,y) means that x is part of y and touches y's boundary.TPPi(x,y) is the same as TPP(y,x).[…].If DC(x,y) and TPPi(y,z) then what are the possible relationships between x and z?</p>
<p>Analysis methods</p>
<p>The correctness of chatbot responses was analyzed both statistically and qualitatively.A task was counted as completed correctly if the responses in both attempts were correct.Open-ended tasks, such as code and function interpretation were considered correct if the relevant technical terms and essential behavior were present in the answer.URLs and programming code generated in response to mapping and coding tasks were tested in a Web browser (URL, HTML) or an integrated development environment, i.e., PyCharm (Python) and RStudio (R), respectively.Besides a summary table which reports the percentage of correctly completed tasks in the seven task categories for the four analyzed chatbots, statistical significance of differences in the proportion of correct results between chatbots and between task categories were analyzed using a two-tailed Pearson's chi-square test.This was followed by post-hoc tests which adjust the significance level (α) for multiple testing using the Bonferroni correction method to determine statistically significant differences in percent correct between individual pairs of chatbots and task categories.</p>
<p>Difference in the length of responses between spatial literacy tasks and GIS concepts tasks were analyzed using a Wilcoxon rank sum test.Differences in response length between the four chatbots for each of these two task categories were analyzed using a Kruskal-Wallis test, followed by a Dunn post-hoc test which compared response lengths between individual pairs of chatbots.Correctness outcomes of repeated tasks were used to compute the consistency (matching) of responses.Matching rates were summarized for chatbots and task categories.</p>
<p>Qualitative analysis of results discussed observed difficulties and challenges in the completion of tasks together with illustrations and examples.</p>
<p>Results</p>
<p>Statistical analysis</p>
<p>Table 1 summarizes the number of correctly completed tasks for each task category (rows) and chatbot (columns), with percentage correct shown in parentheses.Numbers in boldface highlight for each task category the chatbots with the highest percentage of correct responses, which are GPT-4 for five out of seven categories (some ties with Claude-3 and Copilot), Copilot for coding tasks, and both Gemini and Copilot for spatial reasoning.Column totals reveal that GPT-4 completes most tasks correctly (76.3%), whereas Gemini completes fewest tasks (55.3%) correctly.This pattern is also reflected in Figure 1a.</p>
<p>Percent correct varies between task categories (see Category total column in Table 1 and Figure 1b).It is highest for tasks related to GIS concepts (95.0%) and code explanation (95.0%), and lowest for mapping tasks (25.0%), followed by spatial reasoning tasks (32.5%).A chi-square test of independence was performed to examine the relation between chatbot and response correctness.Results show that there is a significant relationship between the two variables, X 2 (3, N = 304) = 8.47, p = 0.037.Post hoc comparisons were conducted using pairwise chi-square tests of independence between chatbots and response correctness applying a Bonferroni correction for multiple testing.Assuming a p-value of less than 0.05 to be considered statistically significant, an adjusted alpha level of .008(0.05/6) was used for this purpose.Using the adjusted alpha level threshold of 0.008, no difference between any pair of chatbots was statistically significant, although the p value for the difference between GPT-4 and Gemini was close to that threshold, X 2 (1, N = 152) = 6.577, p = 0.010.</p>
<p>[insert Table 1 about here]</p>
<p>A chi-square test of independence showed that there was a significant association between task category and response correctness, X 2 (6, N =304) = 98.6, p &lt; 0.001.Post hoc comparisons, which applied a Bonferroni correction for multiple testing with an adjusted alpha level of .0024(0.05/21) for individual chi-square tests showed that differences were significant for 12 task pairs which reveal a distinct pattern (Table 2).That is, tasks related to spatial literacy, GIS concepts, function interpretation and code explanation resulted significantly more often in correct responses than mapping, coding, and spatial reasoning tasks.The task category pairs with significant correctness differences are visualized as horizontal bars in Figure 1b.</p>
<p>[insert Table 2 about here]</p>
<p>Figure 2a and b plot the percentage of matching correctness results in response to repeated tasks aggregated by task category for each chatbot.Consistency was 80% or higher in 25 out of 28 task categories evaluated across the four chatbots.Only mapping tasks for Claude-3 (60%) and Copilot (60%) and spatial reasoning tasks for GPT-4 (70%) had lower consistency rates, meaning that 4 out of 10 repeated mapping tasks for Claude-3 and Copilot and 3 out of 10 repeated spatial reasoning tasks for GPT-4 yielded different outcomes.Table 3 lists the mean and median number of words in responses to the spatial literacy tasks (top half) and GIS concept tasks (bottom half) for the four compared chatbots, revealing that GPT-4 and Gemini provide most concise and Claude-3 provides most verbose answers, with Copilot in-between.For Copilot, which offers three conversation styles, the "more precise" option, which tends to provide shorter answers, was chosen for this study.A Wilcoxon rank sum test with continuity correction indicated that output was significantly longer for GIS concept tasks (MD = 90.0)than for spatial literacy tasks (MD = 59.3),W = 760, p &lt; 0.001, demonstrating that the question category affects response length.</p>
<p>[insert Table 3 about here]</p>
<p>A Kruskal-Wallis test showed that the difference in central tendency of word counts between the four compared chatbots was significant both for spatial literacy tasks, H (3, n = 64) = 9.632, p = .022,and GIS concepts tasks, H (3, n = 40) = 12.666, p = .005.Post hoc comparisons using Dunn's method with a Bonferroni correction for multiple tests with an adjusted alpha level of .008(0.05/6) showed a significant difference  between GPT-4 and Claude-3, p = 0.007, for spatial literacy questions, and  between GPT-4 and Claude-3, p = 0.002, and Gemini and Claude-3, p = 0.002, for GIS concepts questions.</p>
<p>These three chatbot pairs with significant differences in response lengths are annotated with small superscript letters in Table 3.</p>
<p>Qualitative analysis</p>
<p>Spatial literacy</p>
<p>Most tasks in this category (12 out of 16) were completed correctly by at least three chatbots.The four remaining tasks which revealed some challenges were the following: (1) Identify the German city in which the A60 intersects the Rhine river; (2) List the order in which the Danube flows through Bratislava, Budapest, Linz and Vienna; (3) Determine the cities of 250,000 or more residents within a 100 mile drive from Lexington, KY; (4) List the UNESCO world heritage sites in Oman.For the latter task both GPT-4 and Claude-3 missed one UNESCO world heritage site in Oman, i.e., Ancient City of Qalhat, which was added in 2018, long before these LLMs were trained.The reason for this omission is therefore unclear.For task (1) the incorrect result is based on factual errors as chatbot explanations in Gemini and Copilot revealed by stating that the A60 highway in Germany does not cross the Rhine River.In addition, Copilot demonstrated problems in sorting cities by average daytime temperature in July although it reported the correct temperature for each city.</p>
<p>GIS concepts</p>
<p>The only question which was not correctly answered by all chatbots was if grid north is in the direction of the north pole.Both Gemini and Claude-3 provided an incorrect response in both attempts.</p>
<p>Mapping</p>
<p>The tasks in this category proved to be the most challenging ones.GPT-4 was the only chatbot which completed more than half of the tasks correctly (6/10) and which also successfully handled at least one Mapbox mapping task in repeated attempts.That successful Mapbox task was the most complex one ("Create a Mapbox map with a line from Vienna to Munich") since it required generating an HTML script that integrated the Mapbox GL JS client-side JavaScript library (Figure 3a).None of the chatbots successfully completed the task to generate code that uses the R tmap package to map worldwide cities.Other Python libraries (e.g.matplotlib, pandas) and R packages (e.g., tmap) were more successfully applied to generate maps and scatterplots of largest cities in the U.S. (Figure 3b and c) and world maps (Figure 3d-f).No specific instructions about the map design were provided in the prompt.Therefore, produced map layouts do not necessarily meet strict cartographic mapping guidelines.Gemini was the only chatbot which failed in completing all 10 tasks.</p>
<p>Figure 3. Selected results of mapping tasks.GPT-4: Mapbox map with a line from Vienna to Munich (a); GPT-4: map showing the locations of the 5 largest cities in the U.S. using matplotlib library (b); GPT-4: scatter plot showing the locations of the 5 largest cities in the U.S. using pandas library (c); Claude-3: World map with orthographic map projection using matplotlib library (d); Claude-3: World map using R tmap package (e); Copilot: World map using R tidyverse package (f).</p>
<p>Function interpretation</p>
<p>GPT-4 identified and labelled all 10 provided functions correctly, with Claude-3 (8/10), Copilot (8/10), and Gemini (7/10) following closely.The only function which was not correctly recognized by two chatbots at the same time (Gemini and Copilot) was that for a semi-variogram (Equation 1):
g (h)= 1 2 E (Z (s )−Z ( s+h)) 2 (1)
Functions captioning 2-D Euclidean distance, NDVI as well as great circle distance and Law of Cosines in spherical trigonometry were correctly identified by all chatbots in both attempts.Equations for affine transformation, spatial lag model, Epanechnikov kernel, Mercator Projection and spherical excess were identified correctly by three chatbots.</p>
<p>Python, find the centroid of a polygon in R).For example, for the line intersection task, Gemini describes the purpose of the code too generally as code which "solves a system of two linear equations using Cramer's rule".</p>
<p>Coding</p>
<p>The 10 tasks in this category proved to be challenging overall, with Copilot performing strongest (7/10) and Gemini performing poorest (3/10).The two code modification tasks in Python and the translation of the Haversine distance code snippet from R to Python are the only three coding tasks which were successfully completed in all four chatbots.As opposed to this, all four chatbots failed to translate a Quicksort algorithm implementation from Python to R. Three chatbots (GPT-4, Claude-3, Copilot) managed the task to complete R code to plot three polygons.Results varied slightly between GPT-4 (Figure 4) and Claude-3/Copilot (Figure 5b).Copilot was the only chatbot to successfully complete the code snippet shown below to create and plot a raster map using indirect distance weighting with a power of 0.5 (Figure 7c).It was also the only chatbot generating Python code that correctly sorted three points with their given geographic coordinates from East to West.As opposed to this, the remaining three chatbots incorrectly sorted points from West to East.library(spatstat) library(sp)</p>
<p>xlist &lt;-1:10 ylist &lt;-1:10 comb &lt;-expand.grid(xlist,ylist)labels &lt;-runif(100)<em>(comb$Var1/10)</em>(comb$Var2/10) w = owin(c(0,11),c(0,11)) ppp_rd&lt;-ppp(comb$Var1,comb$Var2, marks=labels,window=w)</p>
<p>Spatial reasoning</p>
<p>Correctness was low for this task category with a range between 2/10 (GPT-4) and 4/10 (Gemini, Copilot).The only question correctly answered by all four chatbots was if two circles that both have a radius of 5 and whose distance between their centres is 9 intersect.All four chatbots failed in the three following tasks: (1) Provide the order of three cubic boxes (A, B, C) from top to bottom if C is immediately below A, and B is higher up than C;</p>
<p>(2) Place six X's on a Tic Tac Toe board without making three-in-a-row in any direction;</p>
<p>(3) RCC-8 qualitative spatial reasoning calculus: If NTPP(x,y) and TPP(y,z) then what are the possible relationships between x and z? GPT-4 was unable to solve any of the three RCC-8 tasks, whereas Gemini and Claude-3 successfully completed two of them and Copilot one, respectively.</p>
<p>Discussion</p>
<p>Chatbot performance metrics</p>
<p>Results presented in this study are a reflection of LLMs' current capabilities.Recent benchmark studies that compare GPT-3.5 and GPT-4 (Ali et al., 2023;Koubaa, 2023) indicate that the capabilities of LLMs improve rapidly.This was already shown in the case of an introductory GIS exam between these two variants of ChatGPT (Mooney et al., 2023).Thus, we expect that the performance related to spatial tasks will also improve over time.However, it must be noted that it is difficult to predict how LLMs will evolve, especially when it comes to their capabilities in spatial reasoning.Previous studies found GPT-4's performance to be superior compared to that of other chatbots in different disciplines such as business, mathematics, history, medicine, reasoning and language (Borji &amp; Mohammadian, 2023;Lim et al., 2023;Rudolph et al., 2023).Our study adds a spatial focus to chatbot evaluations and reveals a more nuanced picture.That is, in line with previous findings, GPT-4 performed also best with spatial tasks and handled more tasks correctly (58/76) than Copilot (54/76), Claude-3 (49/76) and Gemini (42/76).Whereas the difference in percent correct between the chatbots was statistically significant (p = 0.037), comparison of percent correct between individual pairs of chatbots did not show any significant difference in post-hoc tests.Although GPT-4 was the single best performer in three task categories (spatial literacy, mapping, function interpretation) it performed poorest in spatial reasoning, and delivered also more erroneous results than Copilot in coding tasks.</p>
<p>Analyzing results across all four chatbots revealed that tasks on spatial literacy, GIS concepts, function interpretation and code explanation were significantly more often correct than mapping, coding, and spatial reasoning tasks.Mapping tasks revealed that chatbots faced difficulties in using the Mapbox APIs, with parameter values missing or incorrectly set.Similarly, coding tasks revealed problems in using functions from libraries or packages due to incorrect use of function parameters.Earlier studies showed that LLMs struggle with coding questions (Borji &amp; Mohammadian, 2023), and that Bard (now Gemini) was the most erroneous performer in this category.This matches our findings.Despite these flaws and even occasional problems in running their own code, LLMs are commonly used as codewriting assistant (Stokel-Walker &amp; Noorden, 2023) for tasks such as creating generic functions.</p>
<p>Spatial reasoning tasks were found to be challenging for LLMs in other experiments as well although they do better than chance (Cohn, 2023).Though LLMs can learn spatial concepts from text (Abdou et al., 2021) and possess a degree of abstract reasoning skills, they demonstrate weaknesses in spatial reasoning and planning.While current LLMs may possess some abstract task-solving skills, they often also rely on narrow, non-transferable procedures for task-solving (Wu et al., 2023).Similarly, investigation of the trustworthiness of ChatGPT and GPT-4 showed that these models frequently fall short of generating logically consistent predictions as measured by semantic, negation, symmetric, and transitive consistency, and that they also exhibit high levels of self-contradiction (Jang &amp; Lukasiewicz, 2023).</p>
<p>The tasks in the spatial literacy and GIS concepts task categories did not stipulate a certain response length and thus allowed us to compare the verbosity of responses between chatbots and between task categories.Findings do partially support previous work which showed that GPT-4 provided shorter answers than Claude (Borji &amp; Mohammadian, 2023).Hence, Claude-3 might be more suitable for chatbot users who are interested in obtaining more background information related to a task or question.While Bing Chat (now Copilot) was shown to provide brief answers compared to GPT-4 and Bard (Gemini) (Rudolph et al., 2023) this pattern was not observed in our experiments.That is, Copilot answers were longer than those of GPT-4 and Gemini, but this difference was not statistically significant.An earlier study (Scheider, Bartholomeus, &amp; Verstegen, 2023) had 41 university teachers evaluate ChatGPT-3.5 responses to Geography and GIScience related questions along four quality scores.Results showed that 80% of answers had correctness scores, and 74% had completeness and clarity scores that would allow ChatGPT to pass the exam.As opposed to this, conciseness scores were much lower at 47% pointing towards overly long answers.While correctness, completeness, and conciseness were to some extent evaluated in our study setup, assessment of response clarity might be considered for chatbot evaluations in future work as well.</p>
<p>Chatbot architecture and performance</p>
<p>All chatbots included in this study are based on decoder-only transformers (Vaswani et al., 2017).While their general architecture is similar, each chatbot and their respective LLMs have distinct design philosophies, training datasets and optimization strategies, which contribute to their strengths and weaknesses in various task categories presented in this paper.Table 4 lists some details about these chatbots.ChatGPT-4 (OpenAI, 2023), Claude-3 (Anthropic, 2024) and Copilot are based on generative pre-trained transformer (GPT) models originally developed by OpenAI (Radford, Narasimhan, Salimans, &amp; Sutskever, 2018).</p>
<p>[Insert Table 4 about here]</p>
<p>These models face challenges in spatial reasoning and advanced mapping tasks, which originates from their design and purpose.Due to their commercial nature, the exact architecture, training dataset and other technical details are not always available, which is for example the case with GPT-4 where the technical report published by its creators focuses on its capabilities (OpenAI, 2023).This closed structure makes it challenging to assess the effect of model architecture on performance.A detailed discussion of this association is therefore inherently speculative as it can only draw from industry news and user experience.For example, both ChatGPT and Copilot are based on the same model variant, however their performance across task categories is different.Two potential contributing factors are 1) the different setup of the chat interface that could use the models with different parameters (e.g.token length, temperature, etc.), and 2) Microsoft's access to a wider ecosystem, including emails, calendars, and web searches.Also, Microsoft acquired GitHub, a widely popular open-source code repository and version control system, which provides Copilot more training data for coding and programming related tasks.This might explain why Copilot performed best in coding related tasks (Table 1).To gain a deeper technical understanding of the relationship between each LLM and its spatial capabilities, more targeted approaches and surveys are needed, which are beyond the scope of this paper.However, our results demonstrate varied performance across different spatial task categories, highlighting the need to optimize LLM architectures for specific GeoAI applications.To improve the capabilities of future chatbots in managing complex spatial tasks we offer the following suggestions:</p>
<p> Multimodal integration: Incorporate additional data types such as images, maps, and spatial vectors. Specialized training: Fine-tune models on domain-specific datasets that include geospatial information and spatial reasoning tasks. Advanced prompt engineering: Utilize techniques such as chain-of-thought and selfchecking prompts to improve reasoning and decision-making capabilities.</p>
<p>Limitations of the study</p>
<p>This research analyzed problem solving capabilities of LLM-based chatbots for a crosssection of spatial tasks from a variety of categories.Some of these categories are inherently spatial (e.g.spatial literacy, GIS concepts, spatial reasoning) and related tasks were already examined for chatbots earlier.As opposed to this, other categories (e.g., function interpretation, coding) are more general and can be applied to many (including aspatial) disciplines.For these categories we identified or constructed spatial tasks for evaluation.The list of categories examined in this study is not exhaustive since previous studies have analyzed tasks from other spatial categories before, including toponym recognition (Mai et al., acc.), geospatial knowledge extraction (e.g.population density) for a given location (Manvi et al., 2023), and map reading (J.Xu &amp; Tao, 2024).To keep the presented study concise, additional spatial task categories may be considered in follow-up chatbot comparison studies in the future.In addition, this study did not consider complex questions previously described as indirect QA (Scheider et al., 2021).These questions require determining appropriate data sources and tools as well as computation algorithms for an answer.Currently, there is no universally accepted approach to handle these questions in GeoQA.While there are efforts to overcome these limitations, e.g. by applying a grammar to interpret geo-analytical questions and therefore enable machines to solve these kinds of tasks (H.Xu, Nyamsuren, Scheider, &amp; Top, 2023), more research is needed to implement these in the context of LLMs and chatbots.</p>
<p>The current trend of large models is to be multimodal.Spatial tasks and GIS related questions may also include diagrams, flowcharts, and maps (Bolstad &amp; Manson, 2022).Therefore, for future work, some tasks presented in this study may be modified to be presented as multimodal expressions, such as stating a textual question together with a chart, map, or image (J.Xu &amp; Tao, 2024).</p>
<p>Whereas a zero-shot prompt directly instructs the model to perform a task without any additional examples fine-tuning allows developers to train the model on a small dataset for a specific target application or domain (e.g.legal, medical), leading to more accurate and relevant responses (Kasneci et al., 2023).The construction of the prompt, i.e., specialized pretraining through provision of context or personalization, can have an impact on the obtained results as well (Kocoń et al., 2023).For example, few-shot prompting enables in-context learning where the input provides demonstrations in the prompt to steer the model to better performance (Brown et al., 2020).The Chain-of-Thought (CoT) prompt (Wei et al., 2022) includes intermediate steps of reasoning within the prompt besides the task input and output and thus requires the LLM to reason about its strategy before taking actions.It has been found to shown to be effective for tasks that require multiple steps of reasoning and decision-making (F.Li et al., 2024).The Tree of Thoughts (ToT) framework (Yao, Yu, et al., 2023) generalizes over the CoT approach to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next step.The Graph of Thoughts (GoT) framework provides further prompting advancements through modelling the information generated by an LLM as an arbitrary graph, where LLM thoughts are vertices, and edges correspond to dependencies between these vertices (Besta et al., 2024).The Re-Act prompt (Aghzal et al., 2024;Yao, Zhao, et al., 2023) synergizes reasoning traces (e.g. to track and update action plans, correct a mistaken trajectory) and task-specific actions (e.g. to interface with external sources, such as knowledge bases or environments) in LLMs.A self-checking prompt instructing the LLM to check its answer allows the LLM to identify and rectify its previous mistakes (Kevian et al., 2024;Kojima, Gu, Reid, Matsuo, &amp; Iwasawa, 2022).While in this study a zero-shot prompt approach was applied, future work will consider prompt engineering for tasks that were not successfully completed or answered in this study.</p>
<p>Conclusions</p>
<p>LLMs, especially OpenAI's ChatGPT platforms, already demonstrated a significant addition to recent advancements of geo-data analysis methods, sources, and tools, such as crowd-sourcing, citizen science, social media, GIS cloud computing, or blockchain technology (Hochmair, Navratil, &amp; Huang, 2023).The aim of the presented comparison study was to enhance our understanding of the current chatbot ecosystem and its performance in different spatial task categories.The ongoing enhancement of foundation models will improve performance in the multimodal nature of GIS, mapping, and other spatial tasks due to their improved capabilities to reason over various types of geospatial data (e.g., image, video, text, sound) through geospatial alignments (Iyer, Ganguli, &amp; Pandey, 2023;Mai et al., acc.) and the use of geospatial knowledge graphs (Gao et al., 2023;Mai et al., 2022).Evaluation and comparison of new AI technologies and data structures for solving geospatial tasks is part of future work.</p>
<p>Tables</p>
<p>Figure 1 .
1
Figure 1.Percentage of correct task completion grouped by chatbot (a) and task category (b).Pairs of tasks with significant differences in percent correct are connected through horizontal bars in (b).</p>
<p>Figure 2 .
2
Figure 2. Consistency of responses for task categories, grouped by chatbot (a) and task category (b).</p>
<p>Figure 6 .
6
Figure 6.Selected results of coding tasks: Complete R code to plot three polygons in GPT-4 (a) and Claude-3 and Copilot (b); Complete R code to plot a raster map using indirect distance weighting in Copilot (c).</p>
<p>Figure captions Figure 8 .
captions8
Figure captionsFigure 8. Percentage of correct task completion grouped by chatbot (a) and task category (b).Pairs of tasks with significant differences in percent correct are connected through horizontal bars in (b).</p>
<p>Figure 9 .
9
Figure 9. Consistency of responses for task categories, grouped by chatbot (a) and task category (b).</p>
<p>Figure 10 .
10
Figure 10.Selected results of mapping tasks.GPT-4: Mapbox map with a line from Vienna to Munich (a); GPT-4: map showing the locations of the 5 largest cities in the U.S. using matplotlib library (b); GPT-4: scatter plot showing the locations of the 5 largest cities in the U.S. using pandas library (c); Claude-3: World map with orthographic map projection using matplotlib library (d); Claude-3: World map using R tmap package (e); Copilot: World map using R tidyverse package (f) Figure 11.Selected results of coding tasks: Complete R code to plot three polygons in GPT-4 (a) and Claude-3 and Copilot (b); Complete R code to plot a raster map using indirect distance weighting in Copilot (c).</p>
<p>Table 1 .
1
Correctness of responses in seven task categories, separated by chatbot.
Category Spatial literacy (SL) GIS concepts (GIS) Mapping (Map) Function interpretation (Func) Code explanation (ExpC) Coding (Code) Spatial reasoning (SR) Chatbot totalGPT-4 15/16 (93.8%) 10/10 (100.0%) 6/10 (60.0%) 10/10 (100.0%) 10/10 (100.0%) 5/10 (50.0%) 2/10 (20.0%) 58/76 (76.3%)Gemini 11/16 (68.8%) 9/10 (90.0%) 0/10 (0.0%) 7/10 (70.0%) 8/10 (80.0%) 3/10 (30.0%) 4/10 (40.0%) 42/76 (55.3%)Claude-3 Copilot Category total 13/16 (81.3%) 13/16 (81.3%) 52/64 (81.3%) 9/10 (90.0%) 10/10 (100.0% 38/40 (95.0%) ) 2/10 (20.0%) 2/10 (20.0%) 10/40 (25.0%) 8/10 (80.0%) 8/10 (80.0%) 33/40 (82.5%) 10/10 (100.0%) 10/10 (100.0% 38/40 (95.0%) ) 4/10 (40.0%) 7/10 (70.0%) 19/40 (47.5%) 3/10 (30.0%) 4/10 (40.0%) 13/40 (32.5%) 49/76 (64.5%) 54/76 (71.1%)</p>
<p>Table 2 .
2
Results of pairwise chi-square post-hoc tests examining the significance of differences in percent correct between task categories
Category 1 Spatial literacy Spatial literacy Spatial literacy GIS concepts GIS concepts GIS concepts Function interpretation Function interpretation Function interpretation Code explanation Code explanation Code explanationCategory 2 Mapping Coding Spatial reasoning Mapping Coding Spatial reasoning Mapping Coding Spatial reasoning Mapping Coding Spatial reasoningdf 1 1 1 1 1 1 1 1 1 1 1 1p 4.20E-08 7.20E-04 1.69E-06 7.19E-10 8.73E-06 2.38E-08 8.00E-07 0.0023 1.73E-05 7.19E-10 8.73E-06 2.38E-08X 2 30.1 11.4 22.9 38.0 19.8 31.2 24.3 9.3 18.5 38.0 19.8 31.2</p>
<p>Table 3 .
3
Word count of responses to spatial literacy questions (n=16) and GIS concepts questions (n=10) in four chatbots; standard deviation shown in parentheses.Small letters indicate pairs of chatbots with significant differences in word count identified in post-hoc tests.<em>Microsoft's Prometheus model is based on OpenAI's GPT-4 complemented by Microsoft ecosystem, including search results(Microsoft, 2023)
GPT-4 Mean (SD) 66.3 (54.9) a 68.1 (63.2) Gemini Spatial literacy Median 45.8 50.2Claude-3 103.0 (47.2) a 116.0Copilot 74.4 (29.0) 73.5All 78.1 (51.3) 59.3GIS concepts Mean (SD) 84.1 (25.4) b 84.9 (37.8) c 172.4 (70.8) bc 105.7 (35.3) 111.8 (57.1) Median 82.5 76.5 177.3 95.8 90.0Table 4. Chatbots and their underlying model architectures Chatbot Underlying LLMArchitectureChatGPT-4 GeminiGPT-4 Gemini </em>GPT MoEClaude-3Claude-3GPTCopilotMicrosoft Prometheus *<em>GPT</em> Gemini is multimodal by design.
*</p>
<p>Code explanationWhereas  and  Copilot provided correct explanations for all 10 code snippets, Gemini failed to do so for two tasks (find point of intersection of two lines inData availability statementThe complete set of tasks assigned to chatbots in this study and their responses can be downloaded from https://doi.org/10.6084/m9.figshare.25903729Disclosure statementNo potential conflict of interest was reported by the authors.
Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. M Abdou, A Kulmizev, D Hershcovich, S Frank, E Pavlick, A Søgaard, 25th Conference on Computational Natural Language Learning. 2021</p>
<p>M Aghzal, E Plaku, Z Yao, Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. 2024arXiv preprint</p>
<p>R Ali, O Y Tang, I D Connolly, P L Zadnik Sullivan, J H Shin, J S Fridley, . . Telfeian, A E , Performance of ChatGPT and GPT-4 on Neurosurgery Written Board Examinations. 202393</p>
<p>The Claude 3 Model Family: Opus, Sonnet, Haiku. Anthropic, 2024</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, . . Hoefler, T , The Thirty-Eighth AAAI Conference on Artificial Intelligence. 2024AAAI-24</p>
<p>P Bolstad, S Manson, GIS Fundamentals: A First Text on Geographic Information Systems. MNEider Press20227th ed.. White Bear Lake</p>
<p>A Borji, M Mohammadian, 10.2139/ssrn.4476855Battle of the Wordsmiths: Comparing ChatGPT, GPT-4. Claude2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, . . Amodei, D , Advances in Neural Information Processing Systems. 202033</p>
<p>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8. arXiv preprint. A G Cohn, 2023</p>
<p>Y Feng, L Ding, G Xiao, GeoQAMap -Geographic Question Answering with Maps Leveraging LLM and Open Knowledge Base. 2023. GIScience 2023at the 12th International Conference on Geographic Information Science</p>
<p>GeoAI Methodological Foundations: Deep Neural Networks and Knowledge Graphs. S Gao, J Rao, Y Liang, Y Kang, J Zhu, R Zhu, Handbook of Geospatial Artificial Intelligence. S Gao, Y Hu, &amp; W Li, Boca RatonCRC Press2023</p>
<p>Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. Y Gu, R Tinn, H Cheng, M Lucas, X Liu, T Naumann, . . Poon, H , 10.1145/3458754ACM Transactions on Computing for Healthcare. 312021</p>
<p>Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects. M U Hadi, R Qureshi, A Shah, M Irfan, A Zafar, M B Shaikh, . . Mirjalili, S , 10.36227/techrxiv.23589741.v42023TechRxiv</p>
<p>H H Hochmair, G Navratil, H Huang, Perspectives on Advanced Technologies in Spatial Data Collection and Analysis. geographies. 20233</p>
<p>Perspectives on Geospatial Artificial Intelligence Platforms for Multimodal Spatiotemporal Datasets. C V K Iyer, S Ganguli, V Pandey, Advances in Scalable and Intelligent Geospatial Analytics. S S Durbha, J Sanyal, L Yang, S S Chaudhari, U Bhangale, U Bharambe, K Kurte, Boca Raton, FLCRC Press2023</p>
<p>Consistency Analysis of ChatGPT. M E Jang, T Lukasiewicz, 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs. L Juhász, P Mooney, H H Hochmair, B Guan, Fourth Spatial Data Science Symposium. 2023</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, . . Kasneci, G , Learning and Individual Differences. 2023</p>
<p>Benchmarking Geospatial Question Answering Engines Using the Dataset GEOQUESTIONS1089. S.-A Kefalidis, D Punjani, E Tsalapati, K Plas, M Pollali, M Mitsios, . . Maret, P , The Semantic Web -ISWC 2023. T R Payne, V Presutti, G Qi, M Poveda-Villalón, G Stoilos, L Hollink, Z Kaoudi, G Cheng, &amp; J Li, BerlinSpringer202314266</p>
<p>D Kevian, U Syed, X Guo, A Havens, G Dullerud, P Seiler, . . Hu, B , Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra. 2024arXiv preprint</p>
<p>ChatGPT: Jack of all trades, master of none. Information Fusion, 99, 101861. J Kocoń, I Cichecki, O Kaszyca, M Kochanek, D Szydło, J Baran, . . Kazienko, P Kojima, T Gu, S S Reid, M Matsuo, Y Iwasawa, Y , 36th Conference on Neural Information Processing Systems. 2023. 2022. NeurIPS 2022Tree of Thoughts: Deliberate Problem Solving with Large Language Models</p>
<p>GPT-4 vs. GPT-3.5: A Concise Showdown. Preprints, 2023030422. A Koubaa, 10.20944/preprints202303.0422.v12023</p>
<p>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepaño, . . Tseng, V , PLOS Digit Health. 22e00001982023</p>
<p>Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark. F Li, D C Hogg, A G Cohn, The Thirty-Eighth AAAI Conference on Artificial Intelligence. 2024AAAI-24</p>
<p>BLIP-2: Bootstrapping Language-Image Pretraining with Frozen Image Encoders and Large Language Models. J Li, D Li, S Savarese, S Hoi, 2023arXiv preprint</p>
<p>Testing spatial reasoning of Large Language Models: the case of tic-tac-toe. Paper presented at the AIxPAC 2023, 1st Workshop on Artificial Intelligence for Perception and Artificial Consciousness. D Liga, L Pasetto, 2023Rome, Italy</p>
<p>Benchmarking large language models' performances for myopia care: a comparative analysis of ChatGPT-3. Z W Lim, K Pushpanathan, S M E Yew, Y Lai, C.-H Sun, J S H Lam, . . Tham, Y.-C , 20235ChatGPT-4.0, and Google Bard. eBioMedicine, 95, 104770</p>
<p>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, . . Gao, J , Twelfth International Conference on Learning Representations. Vienna Austria2024ICLR 2024</p>
<p>Symbolic and subsymbolic GeoAI: Geospatial knowledge graphs and spatially explicit machine learning. G Mai, Y Hu, S Gao, L Cai, B Martins, J Scholz, . . Janowicz, K , Transactions in GIS. 262022</p>
<p>On the Opportunities and Challenges of Foundation Models for GeoAI (Vision Paper). G Mai, W Huang, J Sun, S Song, D Mishra, N Liu, . . Lao, N , ACM Transactions on Spatial Algorithms and Systems. </p>
<p>R Manvi, S Khanna, G Mai, M Burke, D Lobell, S Ermon, GeoLLM: Extracting Geospatial Knowledge from Large Language Models. 2023arXiv preprint</p>
<p>How Copilot works, technically speaking. 2023Microsoft</p>
<p>Towards Understanding the Spatial Literacy of ChatGPT. P Mooney, W Cui, B Guan, L Juhász, ACM SIGSPATIAL International Conference. Hamburg, GermanyACM Press2023</p>
<p>. Openai, 2023arXiv preprint</p>
<p>The Question Answering System GeoQA2. D Punjani, S A Kefalidis, K Plas, E Tsalapati, M Koubarakis, P Maret, Proceedings of the 2nd International Workshop on Geospatial Knowledge Graphs and GeoAI: Methods, Models, and Resources. the 2nd International Workshop on Geospatial Knowledge Graphs and GeoAI: Methods, Models, and ResourcesLeeds, UK.2023</p>
<p>Template-Based Question Answering over Linked Geospatial Data. D Punjani, K Singh, A Both, M Koubarakis, I Angelidis, K Bereta, . . Stamouli, G , Proceedings of the 12th Workshop on Geographic Information Retrieval. the 12th Workshop on Geographic Information Retrieval201818Paper presented at the GIR</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. P P Ray, Internet of Things and Cyber-Physical Systems. 32023</p>
<p>War of the chatbots: Bard, Bing Chat, ChatGPT, Ernie and beyond. The new AI gold rush and its impact on higher education. J Rudolph, S Tan, S Tan, Journal of Applied Learning &amp; Teaching. 612023</p>
<p>ChatGPT is not a pocket calculator -Problems of AI-chatbots for teaching Geography. S Scheider, H Bartholomeus, J Verstegen, 2023arXiv preprint</p>
<p>Geo-analytical questionanswering with GIS. S Scheider, E Nyamsuren, H Kruiger, H Xu, International Journal of Digital Earth. 1412021</p>
<p>R Shewale, 62 Chatbot Statistics For 2024 (Usage, Challenges &amp; Trends. 2023</p>
<p>What ChatGPT and generative AI mean for science. C Stokel-Walker, R V Noorden, Nature. 6142023</p>
<p>Mapping with ChatGPT. R Tao, J Xu, ISPRS International Journal of Geo-Information. 1272842023</p>
<p>Shortcomings of ChatGPT. J Tyson, Journal of Chemical Education. 10082023</p>
<p>Attention Is All You Need. A Vaswani, Noamshazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Advances in Neural Information Processing Systems. 2017. 201730Polosukhin, I</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, . . Zhou, D , 36th Conference on Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, &amp; A Oh, 202235</p>
<p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, . . Kim, Y , 2023arXiv preprint</p>
<p>Extracting interrogative intents and concepts from geo-analytic questions. H Xu, E Hamzei, E Nyamsuren, H Kruiger, S Winter, M Tomko, S Scheider, AGILE GIScience Series. 12020</p>
<p>A grammar for interpreting geoanalytical questions as concept transformations. H Xu, E Nyamsuren, S Scheider, E Top, International Journal of Geographical Information Science. 3722023</p>
<p>Map Reading and Analysis with GPT-4V(ision). J Xu, R Tao, ISPRS International Journal of Geo-Information. 1341272024</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 2023. NeurIPS 202336</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, Eleventh International Conference on Learning Representations (ICLR 2023). Kigali, Rwanda2023</p>
<p>Is ChatGPT a game changer for geocoding -a benchmark for geocoding address parsing techniques. Z Yin, D Li, D W Goldberg, GeoSearch '23: 2nd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data. 2023</p>
<p>X Yue, Y Ni, K Zhang, T Zheng, R Liu, G Zhang, . . Chen, W , MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3358 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3358</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3358</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-8575d181953d63646d3f0453910ea30ff13e09af</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8575d181953d63646d3f0453910ea30ff13e09af" target="_blank">Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work presents Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced, and on par with the accuracy of a rule-based clue solver.</p>
                <p><strong>Paper Abstract:</strong> Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. Cryptonite is a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6% accuracy, on par with the accuracy of a rule-based clue solver (8.6%).</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3358",
    "paper_id": "paper-8575d181953d63646d3f0453910ea30ff13e09af",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0029089999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</h1>
<p>Avia Efrat<em> Uri Shaham</em> Dan Kilman Omer Levy<br>Tel Aviv University<br>{avia.efrat, uri.shaham1}@gmail.com</p>
<h4>Abstract</h4>
<p>Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost $100 \%$ accuracy. Cryptonite is a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only $7.6 \%$ accuracy, on par with the accuracy of a rule-based clue solver ( $8.6 \%$ ).</p>
<h2>1 Introduction</h2>
<p>The ambiguity of natural language is one of the most fundamental challenges in NLP research. While there are works and datasets specifically targeting ambiguity (Levesque et al., 2011; Raganato et al., 2017; Sakaguchi et al., 2020), these can be solved by a native speaker with relative ease. Can we design a dataset with ambiguities that pose a challenge even to competent native speakers?</p>
<p>We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Cryptonite's 523 K examples are taken from professionallyauthored cryptic crosswords, making them less prone to artifacts and biases than examples created by crowdsourcing (Gururangan et al., 2018; Geva et al., 2019). Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, which poses a challenge even for humans experienced in cryptic crossword solving. A cryptic clue usually consists of two underlying parts: wordplay and definition. The</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: How to solve the cryptic clue "One doesn't like shifting earth (5)": Solving usually starts by figuring out which of the clue's words belong to the definition (blue) and which to the wordplay (orange). Next, one needs to figure out the type of wordplay, which is often hinted by an indicator (purple). In our case, "shifting" hints that the answer is an anagram of some part of the wordplay. As the enumeration (gray) states the answer is a five-letter word, "earth" is a promising candidate for anagraming. Finally, given that "hater" is both an anagram of "earth" and a synonym of the definition, we conclude it to be the correct answer.
clue's answer is both a disambiguation of the wordplay and, at the same time, directly answers the definition. While solving cryptic clues requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge, clues are designed to have only one possible answer. See Section 1 for an example clue and its solution.</p>
<p>We provide a standard baseline by fine-tuning the generic T5-Large conditional language model (Raffel et al., 2020) on Cryptonite, achieving only $7.6 \%$ accuracy. For comparison, a rule-based cryptic clue solver (Deits, 2021) achieves $8.6 \%$ accuracy. These results highlight the challenge posed by Cryptonite, making it a candidate for assessing the disambiguation capabilities of future models.</p>
<p>Analyzing the results of both baselines, we find a correlation between performance on individual clues and a human assessment of the clue's difficulty, and that the enumeration (answer length) is highly informative. Finally, we show that ensuring that the answers of train and test examples are mutually exclusive is critical for a candid estimation of T5's ability to solve cryptic clues in general.</p>
<h2>2 Cryptic Crosswords</h2>
<p>A cryptic crossword, just like a regular (noncryptic) crossword, is a puzzle designed to entertain humans, comprised of clues whose answers are to be filled in a letter grid. Unlike regular crosswords, where answers are typically synonyms or hyponyms of the clues (Severyn et al., 2015), cryptic clues have a misleading surface reading, and solving them requires disambiguating wordplays. A cryptic clue has only one possible answer, even when taken outside the context of the letter grid. ${ }^{1}$</p>
<p>Generally, a cryptic clue (henceforth, "clue") consists of two parts: wordplay and definition. The wordplay-definition split is not given to the solver, and parsing it is usually the first step in solving a clue. Both the wordplay and the definition lead to the answer, but each in a different manner. While the definition is directly related to the answer (e.g. a synonym or a hypernym), the wordplay needs to be deciphered, usually with the help of an indicator that hints at the wordplay's type.</p>
<p>Figure 2 walks through several clues with different types of wordplays and their solution. These examples provide only a glimpse into the rich and diverse world of cryptic crosswords. For a deeper dive, see the guidebook by Moorey (2018).</p>
<h2>3 Dataset</h2>
<p>We introduce Cryptonite, a dataset of 523,114 cryptic clues from 17,375 English-language crosswords published in The Times ${ }^{2}$ and The Telegraph ${ }^{3}$ between October 2000 and October 2020. ${ }^{4}$</p>
<p>For preprocessing, we remove clue-answer duplicates and examples whose answer and enumeration do not match. In addition, we remove any examples with the same clue but with a different answer. While this occurred in less than $0.1 \%$ of the data, these examples violate the principle that a clue must have a single solution once the wordplay is deciphered (see Section 2).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Model of car and every train (5)</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) A clue with a relatively simple additive wordplay, also requiring world knowledge. One can decipher the wordplay by identifying "and" as a concatenation indicator.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Clues can also have phonetic wordplays. Here, "it's said" implies that the wordplay is a homophone of the answer.</p>
<h2>Got staff back in case of blockage (6)</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(c) Many clues combine more than one type of wordplay. This clue composes three: reversing the letters of a word ("back"), and inserting it ("in") into the boundary letters of another ("case of").</p>
<h2>Getting fed up about midday $(2,5)$</h2>
<p>A possible surface reading is "someone being mad about it being noon".
A potential source of ambiguity is "fed up". Try finding a different, perhaps more literal reading if the clue.</p>
<h2>at lunch</h2>
<p>(d) Although many wordplays can be roughly clustered into types and deciphered based on indicators, there is no silver bullet for solving cryptic crosswords. For this clue, even the standard wordplay-definition split does not apply; instead, the entire clue points to the answer.</p>
<p>Figure 2: Various examples of cryptic clues and how to solve them. Answers are in green, definitions in light blue (dashed frame), wordplays in orange, and indicators are in purple.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Examples</th>
<th style="text-align: center;">Unique <br> Answers</th>
<th style="text-align: center;">Clue <br> Length</th>
<th style="text-align: center;">Answer <br> Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">train</td>
<td style="text-align: center;">470,803</td>
<td style="text-align: center;">80,837</td>
<td style="text-align: center;">7.76</td>
<td style="text-align: center;">1.22</td>
</tr>
<tr>
<td style="text-align: left;">valid</td>
<td style="text-align: center;">26,156</td>
<td style="text-align: center;">4,534</td>
<td style="text-align: center;">7.77</td>
<td style="text-align: center;">1.20</td>
</tr>
<tr>
<td style="text-align: left;">test</td>
<td style="text-align: center;">26,157</td>
<td style="text-align: center;">4,538</td>
<td style="text-align: center;">7.77</td>
<td style="text-align: center;">1.24</td>
</tr>
</tbody>
</table>
<p>Table 1: Overview of Cryptonite. Reported lengths are mean values. Words are delimited by spaces.</p>
<p>We follow the recent findings of Lewis et al. (2020), and split Cryptonite into train, validation, and test sets, where no answer is shared between them. Answer splitting creates a far more challenging benchmark for supervised models than naive random splits (see Section 4.3). Table 1 shows some basic statistics of the final Cryptonite dataset.</p>
<h2>4 Experiments</h2>
<p>We provide initial results on Cryptonite using two baselines: T5-Large (Raffel et al., 2020) and a rule-based cryptic clue solver (Deits, 2021). Despite training on half a million clues (T5) or being tailored to the task (rule-based solver), both approaches solve only a small portion of the test data, demonstrating that Cryptonite is indeed a challenging task. We further investigate two properties of the data: how difficulty (as perceived by humans) correlates with accuracy, and the informativeness of enumeration. In addition, we analyze how a naive data split affects the performance of T5, demonstrating that partitioning by answers is crucial for obtaining a candid estimate of the neural model's ability to generalize to new cryptic clues.</p>
<h3>4.1 Baselines</h3>
<p>T5-Large Following current NLP methodology, we fine-tune the 770M parameter T5-Large (Raffel et al., 2020) on Cryptonite. The model's encoder takes the clue as input, and uses the decoder to predict the answer using teacher forcing during training and beam search $(b=5)$ during inference.</p>
<p>We use HuggingFace (Wolf et al., 2020) with the recommended settings (Raffel et al., 2020), optimizing with AdaFactor (Shazeer and Stern, 2018) at a constant learning rate of 0.001 . We train until convergence with a patience of 10 epochs and a batch size of 7000 tokens, selecting the best model checkpoint using validation set accuracy.</p>
<p>T5 uses SentencePiece tokenization (Kudo and Richardson, 2018), which might incur some information loss, as many clues require character-level manipulations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Baseline</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Validation</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">$7.44 \%$</td>
<td style="text-align: center;">$7.64 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Rule-based Solver</td>
<td style="text-align: center;">$8.26 \%$</td>
<td style="text-align: center;">$8.58 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Baseline performance on Cryptonite. Accuracy is measured using exact string match.</p>
<p>Rule-based Solver We also gauge the abilities of a rule-based solver with a manually-crafted probabilistic grammar (Deits, 2021). Building on the assumption that a clue can usually be split into a wordplay and a definition (Section 2), the solver tries to find the most probable parse such that the wordplay yields a semantically-similar result to the definition. The similarity between the definition and the parsed wordplay is calculated using expert-authored resources such as WordNet (Miller, 1995). Some less frequent wordplay types, such as homophones (Figure 2b) and hidden-at-intervals (Moorey, 2018, Chapter 3), are not implemented in the solver's grammar.</p>
<h3>4.2 Main Benchmark</h3>
<p>We first evaluate our baselines on the main dataset. Table 2 shows that both approaches are able to solve a small portion of the clues. Even though the T5 model is trained on roughly half a million examples, it does not exceed the performance of the rule-based solver. For comparison, top-tier human experts are able to solve even very hard clues with almost $100 \%$ accuracy (Friedlander and Fine, 2016, 2018), though this expertise is acquired through significant training. Appendix A shows a selection of examples and the respective predictions of T5.</p>
<h3>4.3 Analysis</h3>
<p>Correlation with human perception of difficulty Quick cryptic crosswords is a subgenre of cryptic crosswords aimed at beginners, with clues designed to be easier to solve. Cryptonite's test set contains 2,081 such clues. Examining the results of our main benchmark, Table 3 shows that both baselines perform better on quick clues, suggesting a correlation between human assessment of linguistic difficulty and the models' performance on clues. ${ }^{5}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Quick Clues</th>
<th style="text-align: center;">Non-Quick Clues</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">$12.83 \%$</td>
<td style="text-align: center;">$3.40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Rule-based Solver</td>
<td style="text-align: center;">$13.50 \%$</td>
<td style="text-align: center;">$5.78 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on quick cryptic clues vs non-quick cryptic clues. Both baselines perform better on clues that were deemed easier by human experts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">With Enum.</th>
<th style="text-align: center;">Without Enum.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">$7.64 \%$</td>
<td style="text-align: center;">$4.90 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Rule-based Solver</td>
<td style="text-align: center;">$8.58 \%$</td>
<td style="text-align: center;">$3.58 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of baseline accuracy when enumeration is provided and when it is not provided.</p>
<p>The effect of enumeration The enumeration is the number (or numbers) in parentheses at the end of a clue indicating the number of letters in its answer, e.g. (7) or (5,4). To measure the informativeness of enumeration, we run our main experiment again, this time without providing the enumeration. Table 4 shows an accuracy drop in both baselines when the enumeration is not provided. ${ }^{6}$ While it is to be expected that enumeration helps the rulebased solver, we see that T5 is able to leverage this information as well.</p>
<p>Why do we split the data by answer? Many clues that share the same answer are paraphrases of each other (Appendix B). A neural model such as T5 might exploit this information and by copying answers from memorized training examples. Therefore, to test whether a model has learnt a general process for solving cryptic clues, we follow Lewis et al. (2020) and make Cryptonite's default split the answer split, in which the answers of the train, validation, and test sets are mutually exclusive.</p>
<p>We compare the answer split with a naive (random) partition of the data. Table 5 shows that a naive split of Cryptonite will grossly overestimate the performance of T5; while the rule-based solver's performance barely changes, T5-Large is now able to solve an additional $50 \%$ of the entire test set. Further analyzing the naive test set (Figure 3), we observe that the probability of T5 solving a clue is highly correlated with the number of times its answer appeared in the training set. This result indicates that that a significant part of the performance difference is due to the paraphrasing artifact,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 3: The naive split exhibits a strong correlation between T5's accuracy on clues from the test set (vertical axis) and the number of times their answer appears in the train set (horizontal). Each dot's size represents the number of clues from the test set whose answer appears in the train set $n$ times. Trend line is logarithmic.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data Split</th>
<th style="text-align: center;">T5-Large</th>
<th style="text-align: center;">Rule-based Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: center;">$7.64 \%$</td>
<td style="text-align: center;">$8.58 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Naive</td>
<td style="text-align: center;">$56.16 \%$</td>
<td style="text-align: center;">$8.43 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparing test set accuracy of our answer split (where the answers of test examples do not appear during training) and the naive random split.
and that ensuring unseen test answers is critical for establishing a true estimate of a model's ability to solve cryptic clues.</p>
<h2>5 Related Work</h2>
<p>Cryptic crosswords Williams and Woodhead (1979) attempt to devise a formal language for describing cryptic clues. Hart and Davis (1992) define four stages of rule-based solving, and implement the second stage - "syntactic identification". In our work we focus on creating a large-scale dataset of a cryptic clues and apply neural and rule-based methods to establish a strong baseline. Hardcastle $(2001,2007)$ focuses on rule-based approaches for creating cryptic clues given a word as an answer. Although in our work we test solving abilities, the reverse direction of creating a clue from an answer is also challenging, and the Cryptonite dataset could prove useful in this direction as well.</p>
<p>Language disambiguation In addition to works and datasets specifically targeting disambiguation on the word level (Levesque et al., 2011; Raganato et al., 2017; Sakaguchi et al., 2020), there are other domains strongly related to language disambiguation. Among them are pun disambiguation (Miller and Gurevych, 2015; Miller et al., 2017),</p>
<p>and sarcasm detection (Joshi et al., 2017; Oprea and Magdy, 2020). However, to the best of our knowledge Cryptonite is the first dataset both large in scale (unlike pun disambiguation), and containing a variety of wordplays (unlike sarcasm detection).</p>
<p>Non-cryptic crosswords As described in Section 2, non-cryptic ("regular") crosswords are the common crosswords found in most newspapers. There are works introducing regular crossword datasets, some even containing a small percentage of more "tricky" clues ${ }^{7}$ (Littman et al., 2002). However, identifying this small portion of clues requires human effort, whereas Cryptonite is already guaranteed to consist entirely of cryptic clues. In addition, works on solving regular crosswords typically rely on an external database of clues (Ernandes et al., 2005; Barlacchi et al., 2014; Severyn et al., 2015). When given a clue as an input, these systems search the database for the most similar clues, in hope they share the answer with the input clue. In Cryptonite, the answers of the train, validation, and test sets are mutually exclusive (Section 3). In doing so, we hope to shift the focus of solving from memorization to reasoning, which is especially interesting in the setting of cryptic clues.</p>
<h2>6 Conclusion</h2>
<p>We presente Cryptonite, a large-scale dataset based on cryptic crosswords, whose solving requires disambiguating a variety of wordplays. We saw that the standard approach of fine-tuning T5-Large on Cryptonite does not outperform an existing rulebased model, achieving $7.6 \%$ and $8.6 \%$ accuracy respectively, while human experts achieve close to $100 \%$ accuracy. These results highlight the challenge posed by Cryptonite, and will hopefully encourage further research on disambiguation tasks that are not easily solved by a native speaker.</p>
<h2>References</h2>
<p>Gianni Barlacchi, Massimo Nicosia, and Alessandro Moschitti. 2014. Learning to rank answer candidates for automatic resolution of crossword puzzles. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 39-48, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Robin Deits. 2021. rdeits/crypticcrosswords.jl: v0.1.1.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Marco Ernandes, Giovanni Angelini, and Marco Gori. 2005. Webcrow: A web-based system for crossword solving. In AAAI, pages 1412-1417.</p>
<p>Kathryn J. Friedlander and Philip A. Fine. 2016. The grounded expertise components approach in the novel area of cryptic crossword solving. Frontiers in Psychology, 7:567.</p>
<p>Kathryn J. Friedlander and Philip A. Fine. 2018. "the penny drops": Investigating insight through the medium of cryptic crosswords. Frontiers in Psychology, 9:904.</p>
<p>Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 1161-1166, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>D Hardcastle. 2001. Using the bnc to produce dialectic cryptic crossword clues. In Corpus Linguistics 2001, pages 256-265.</p>
<p>David Hardcastle. 2007. Riddle posed by computer (6): the computer generation of cryptic crossword clues. Ph.D. thesis, Citeseer.</p>
<p>M Hart and RH Davis. 1992. Cryptic crossword clue interpreter. Information and Software Technology, 34(1):16 - 27.</p>
<p>Aditya Joshi, Pushpak Bhattacharyya, and Mark J. Carman. 2017. Automatic sarcasm detection: A survey. ACM Comput. Surv., 50(5).</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.
H. Levesque, E. Davis, and L. Morgenstern. 2011. The winograd schema challenge. In $K R$.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2020. Question and answer test-train overlap in open-domain question answering datasets.</p>
<p>Michael L Littman, Greg A Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. Artificial Intelligence, 134(1-2):23-55.</p>
<p>George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39-41.</p>
<p>Tristan Miller and Iryna Gurevych. 2015. Automatic disambiguation of English puns. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 719-729, Beijing, China. Association for Computational Linguistics.</p>
<p>Tristan Miller, Christian Hempelmann, and Iryna Gurevych. 2017. SemEval-2017 task 7: Detection and interpretation of English puns. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 58-68, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Tim Moorey. 2018. How to Crack Cryptic Crosswords. Collins.</p>
<p>Silviu Oprea and Walid Magdy. 2020. iSarcasm: A dataset of intended sarcasm. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1279-1289, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Alessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017. Word sense disambiguation: A unified evaluation framework and empirical comparison. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99-110, Valencia, Spain. Association for Computational Linguistics.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. In $A A A I$.</p>
<p>Aliaksei Severyn, Massimo Nicosia, Gianni Barlacchi, and Alessandro Moschitti. 2015. Distributional neural networks for automatic resolution of crossword puzzles. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 199-204, Beijing, China. Association for Computational Linguistics.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 4596-4604, Stockholmsm√§ssan, Stockholm Sweden. PMLR.
P. W. Williams and D. Woodhead. 1979. Computer assisted analysis of cryptic crosswords. The Computer Journal, 22(1):67-70.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<h1>A Example Predictions</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Clue</th>
<th style="text-align: left;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Act like tragic heroine with cold extremity</td>
<td style="text-align: left;">mimic</td>
</tr>
<tr>
<td style="text-align: left;">Group of musicians prohibited on the radio</td>
<td style="text-align: left;">band</td>
</tr>
<tr>
<td style="text-align: left;">Assumed diamonds to be shelved</td>
<td style="text-align: left;">put on ice</td>
</tr>
<tr>
<td style="text-align: left;">Is in control of distant armies abroad</td>
<td style="text-align: left;">administrates</td>
</tr>
<tr>
<td style="text-align: left;">Second parasite</td>
<td style="text-align: left;">tick</td>
</tr>
</tbody>
</table>
<p>Table 6: Examples of correct predictions of T5-Large.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Clue</th>
<th style="text-align: left;">Answer</th>
<th style="text-align: left;">Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Suffer death at riverside</td>
<td style="text-align: left;">endure</td>
<td style="text-align: left;">strand</td>
</tr>
<tr>
<td style="text-align: left;">Travel free heading for eastbourne</td>
<td style="text-align: left;">ride</td>
<td style="text-align: left;">trip</td>
</tr>
<tr>
<td style="text-align: left;">Toast gordon!</td>
<td style="text-align: left;">brown</td>
<td style="text-align: left;">gobbi</td>
</tr>
<tr>
<td style="text-align: left;">Pair of braces</td>
<td style="text-align: left;">four</td>
<td style="text-align: left;">pair</td>
</tr>
<tr>
<td style="text-align: left;">Performing insect begged for food</td>
<td style="text-align: left;">eggs</td>
<td style="text-align: left;">beef</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">benedict</td>
<td style="text-align: left;">wellington</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples incorrect predictions of T5-Large.</p>
<h2>B Similar Clues</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Clue</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">greek upper chamber <br> greek for 'upper room'</td>
<td style="text-align: center;">attic</td>
</tr>
<tr>
<td style="text-align: left;">flat race maybe failing to finish <br> flat race possibly unfinished</td>
<td style="text-align: center;">even</td>
</tr>
<tr>
<td style="text-align: left;">one playing minor part in run <br> one with small part in film run?</td>
<td style="text-align: center;">extra</td>
</tr>
<tr>
<td style="text-align: left;">think to make changes in partner <br> contemplate change in partner</td>
<td style="text-align: center;">meditate</td>
</tr>
<tr>
<td style="text-align: left;">beginning assault <br> start of an attack</td>
<td style="text-align: center;">onset</td>
</tr>
<tr>
<td style="text-align: left;">what we learn by accepting established award <br> what we learn by accepting fixed award</td>
<td style="text-align: center;">rosette</td>
</tr>
<tr>
<td style="text-align: left;">animal in forest, a grizzly <br> animal in forest, a gazelle</td>
<td style="text-align: center;">stag</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of pairs of similar clues from the naive split, one from the train set and one from the test set.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Compare Florida fruit (6) $\rightarrow$ orange to Where to get a date (4) $\rightarrow$ palm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
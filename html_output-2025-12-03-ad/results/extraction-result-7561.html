<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7561 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7561</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7561</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-428f0dafd33eb083eb77c6b002aaad9e8ad468f8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/428f0dafd33eb083eb77c6b002aaad9e8ad468f8" target="_blank">AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7561",
    "paper_id": "paper-428f0dafd33eb083eb77c6b002aaad9e8ad468f8",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0050852499999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing</h1>
<p>Matei Bejan ${ }^{* \dagger}$<br>University of Bucharest<br>matei.bejan@s.unibuc.ro</p>
<h2>Andrei Manolache ${ }^{*}$ Bitdefender<br>University of Stuttgart<br>amanolache@bitdefender.com</h2>
<p>Marius Popescu<br>University of Bucharest<br>popescunmarius@gmail.com</p>
<h4>Abstract</h4>
<p>Deep learning models have reignited the interest in Anomaly Detection research in recent years. Methods for Anomaly Detection in text have shown strong empirical results on ad-hoc anomaly setups that are usually made by downsampling some classes of a labeled dataset. This can lead to reproducibility issues and models that are biased toward detecting particular anomalies while failing to recognize them in more sophisticated scenarios. In the present work, we provide a unified benchmark for detecting various types of anomalies, focusing on problems that can be naturally formulated as Anomaly Detection in text, ranging from syntax to stylistics. In this way, we are hoping to facilitate research in Text Anomaly Detection. We also evaluate and analyze two strong shallow baselines, as well as two of the current state-of-the-art neural approaches, providing insights into the knowledge the neural models are learning when performing the anomaly detection task. We provide the code for evaluation, downloading, and preprocessing the dataset at https: //github.com/mateibejan1/ad-nlp/.</p>
<h2>1 Introduction</h2>
<p>An anomaly, sometimes referred to as an outlier, discordant, or novelty, can be intuitively described as an observation that appears to be inconsistent with the remainder of that set of data (Ord, 1996) to the degree that it arouses suspicion (Hawkins, 1980). Such an observation can be described as being atypical, irregular, erroneous, or simply strange (Ruff et al., 2020). Anomalies are relative to the data distribution at hand and can be perceived as having characteristics that are not definitory to the vast majority of the population. Researchers have</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>been interested in the Anomaly Detection (AD) problem for several decades (Knorr and Ng, 1997; Chandola et al., 2009; Aggarwal and Reddy, 2014), some of the first formal mentions of "discordant observations" going back to the 19th century (Edgeworth, 1887). Both general anomaly detectors (Schölkopf et al., 1999; Liu et al., 2008; Manevitz and Yousef, 2002; Ruff et al., 2018) and narrowscoped ones (Wang et al., 2019c,b; Ruff et al., 2019; Manolache et al., 2021) have shown promising results in various domains, such as network monitoring (Stolfo et al., 2000; Sharafaldin et al., 2018; Radford et al., 2018), time series (Braei and Wagner, 2020), computer vision (Wang et al., 2019c; Yu et al., 2020), and natural language processing (Ruff et al., 2019; Manolache et al., 2021).</p>
<p>The advent of deep learning methods for detecting anomalies amplified the interest in the field, producing a great variety of models such as ones based on generative networks (Wang et al., 2019b; Zhou and Paffenroth, 2017), self-supervised learning (Wang et al., 2019c; Manolache et al., 2021), or one-class classification (Ruff et al., 2018). Although the field has seen a sprout of activity, most of the introduced datasets for Anomaly Detection are being specifically crafted for Computer Vision, Intrusion Detection Systems (Sharafaldin et al., 2018) or network traffic (Stolfo et al., 2000; Radford et al., 2018). Approaches in NLP are usually benchmarked on ad-hoc setups, typically by making use of an annotated dataset and downsampling some classes to produce outliers (Wang et al., 2019c; Ruff et al., 2019; Manolache et al., 2021). This does not represent an impediment in computer vision, due to the large number and variety of annotated datasets. In natural language processing, however, deciding what an anomaly is and what it is not is a delicate matter. This problem is rooted in the intrinsic complexity of the text: the</p>
<p>form the language of a text takes is influenced not only by the style and lexicon of the author but also by the literary or historical era it was written in and its purpose (e.g., newspaper article, novel, letter, satire, etc.). Thus, anomalies can be defined through a multitude of factors, ranging from the concrete syntax, style, and grammar, to the more abstract semantics, metaphorical meaning, and contextual topic. A notable aspect of certain linguistic phenomena that can be effectively analyzed with Anomaly Detection (AD) is the challenge posed by the lack of clear negative examples. For instance, while there are countless texts that were not written by Shakespeare, determining which ones serve as a good representation of non-Shakespearean texts can be tricky. In a similar vein, identifying a typical non-scientific text can also be challenging.</p>
<p>We introduce AD-NLP: an anomaly detection setup for Natural Language Processing which can be used to benchmark AD systems on syntactic anomalies, semantic anomalies, pragmatic anomalies and stylistic anomalies, by aggregating different tasks and treating them as a general anomaly detection task, such as: sentence acceptability detection, topic detection, metaphor detection and autorship detection. Furthermore, we experiment with two strong classical baselines and two recent deep methods for detecting anomalies in text and make observations on both the quantitative and qualitative results of the models.</p>
<p>In the following, we will use the terms "anomaly" and "outlier" interchangeably to refer to the same concept of divergent observation with respect to the overall data distribution.</p>
<p>The paper is organized as follows: Section 2 covers related work. In Section 3, we outline our task definition, data collection approach, and datasets. Sections 4 and 5 delve into the models we used and our experimental assessment. Finally, we wrap up with our conclusions in Section 6.</p>
<h2>2 Related Work</h2>
<h3>2.1 AD Datasets</h3>
<p>Most of the AD benchmarks were historically used in the domain of Anomaly-based Network Intrusion Detection Systems (A-NIDS). Some of these datasets are synthetic, such as KDD99 (Stolfo et al., 2000), CIC-IDS (Sharafaldin et al., 2018) and the LANL log dataset (Turcotte et al., 2018), while others are obtained using honeypots, like the Kyoto IDS dataset (Song et al., 2011).</p>
<p>In recent years, there has been a focus on Computer Vision, especially on video anomaly detection that uses such benchmarks as Avenue ( Lu et al., 2013), ShanghaiTech (Zhang et al., 2016) and UCSD Ped 2 (Mahadevan et al., 2010).</p>
<p>In NLP we note TAP-DLND 2.0 (Tirthankar Ghosal, 2022), a document-level novelty classification dataset, which focuses on semantic and syntactic novelty and tests multiple baselines on a singular task. All datasets of the TAP benchmark are in the newswire domain, we wish to offer a larger variety in terms of anomaly types. On top of this, TAP-DLND 2.0 focuses on detecting the degree of novelty of paraphrased or plagiarized text with respect to one or multiple ground truth texts, while we target out-of-distribution samples compared to an overarching distribution.</p>
<p>Additionally, out-of-distribution detection setups (Arora et al., 2021) can be used to construct ad-hoc outliers in the validation or test data, albeit these distribution-shifted samples are artificial.</p>
<h3>2.2 NLP Datasets</h3>
<p>Anomaly detection can be viewed as a particular case of the one-class classification setting. Classification datasets can be ideal for AD tasks due to their ease of being adapted for one-class unsupervised classification (Ruff et al., 2019; Manolache et al., 2021). Various NLP classification benchmarks are widely used, their purpose varying and including news topic detection tasks (Sam Dobbins, 1987; Lang, Ken and Rennie, Jason, 2008; Zhang et al., 2015), sentiment analysis (Maas et al., 2011; Socher et al., 2013; He and McAuley, 2016) or authorship verification (Bevendorff et al., 2020).</p>
<p>Recently, more general NLP datasets that contain multi-task challenges have emerged due to the desire to obtain general NLP models. decaNLP (McCann et al., 2018) provides a general framework for multitask learning as question answering and is proposing ten tasks together with a leaderboard to facilitate research in areas such as multitask learning, transfer learning, and general purpose question answering. GLUE (Wang et al., 2018) is a multi-task benchmark and analysis platform for Natural Language Understanding. GLUE provides a suite of nine sentences or sentence-pair NLU tasks, an evaluation leaderboard, and a socalled "diagnostic evaluation dataset". The performance on the GLUE benchmark has surpassed the</p>
<p>Table 1: Data statistics for our benchmark. By the "Several" domain we denote a blend of Politics, Science, Fiction, Academia, and News. The Avg #Words column denotes each dataset's average number of words per sample.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Corpus</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Avg #Words</th>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Anomaly</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">20Newsgroups</td>
<td style="text-align: center;">10.996</td>
<td style="text-align: center;">8.819</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">News</td>
<td style="text-align: center;">Semantic</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">120.000</td>
<td style="text-align: center;">7.600</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">News</td>
<td style="text-align: center;">Semantic</td>
</tr>
<tr>
<td style="text-align: left;">COLA</td>
<td style="text-align: center;">8.551</td>
<td style="text-align: center;">1.043</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Linguistics</td>
<td style="text-align: center;">Syntactic</td>
</tr>
<tr>
<td style="text-align: left;">VUA</td>
<td style="text-align: center;">8.485</td>
<td style="text-align: center;">3.637</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Several</td>
<td style="text-align: center;">Pragmatic</td>
</tr>
<tr>
<td style="text-align: left;">Song Genres</td>
<td style="text-align: center;">15.120</td>
<td style="text-align: center;">3.780</td>
<td style="text-align: center;">249</td>
<td style="text-align: center;">Music</td>
<td style="text-align: center;">Stylistic</td>
</tr>
<tr>
<td style="text-align: left;">Gutenberg Categories</td>
<td style="text-align: center;">5.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">618</td>
<td style="text-align: center;">Several</td>
<td style="text-align: center;">Semantic</td>
</tr>
<tr>
<td style="text-align: left;">Gutenberg Authors</td>
<td style="text-align: center;">4.765</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">619</td>
<td style="text-align: center;">Fiction+Politics</td>
<td style="text-align: center;">Stylistic</td>
</tr>
</tbody>
</table>
<p>level of non-human experts in just twelve months, thus leading to the release of an updated variant called SuperGLUE (Wang et al., 2019a).</p>
<h2>3 Data</h2>
<h3>3.1 Task Definition</h3>
<p>Problem Setting Our benchmark is aimed at addressing a broad spectrum of anomaly detection scenarios. In this regard, we concentrate on three crucial elements for our benchmark: diversity in the domain of natural language where anomalies occur, diversity within each of these domains (see Table 2), covering various settings that might arise within the same NLP domain, and diversity in terms of sample counts. The final aspect explores the variation in both train and test sample counts across datasets, as well as the variation in sample numbers for each class within each dataset.</p>
<p>Dataset Properties Firstly, our benchmark covers four outlier classes, as can be seen in Table 2. On top of this, it also aims at delivering intra-class variety by supplying multiple datasets for our content category. We believe this is important so as to not lock an anomaly class to a specific instance of outlier distribution. As an example, content anomalies can appear in news data through a minority of articles with a diverging topic, whereas when it comes to music genres, lyricism, or fiction writing, the outliers can present a cluster of multiple similar subjects which are different from the ones of the majority class. Some dataset statistics are available in Table 1, with more detailed information in our benchmark's datasheet ${ }^{1}$. Similarly, training</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and validation code can be obtained through our GitHub repository.</p>
<h3>3.2 Data Collection</h3>
<p>Our data acquisition methodology was designed with the clear goal of providing a large diversity of what we can define as anomalies. This has been done so the data mimics the in-the-wild distribution of classes as well as possible. Our benchmark consists of four already-available datasets: 20Newsgroups (Lang, Ken and Rennie, Jason, 2008), AG News (Zhang et al., 2015), CoLA (Warstadt et al., 2018) and VUA (Steen et al., 2010), as well as novel datasets: Song Genres, Gutenberg Categories, and Gutenberg Authors. Table 3 provides an overview of the data through examples of outliers and inliers.</p>
<h3>3.3 Available Datasets</h3>
<p>We selected a set of representative existing datasets due to their prior utilization in literature (Ruff et al., 2019; Manolache et al., 2021). The 20Newsgroups and AGNews datasets have been frequently used in various experimental setups, hence, we included them to ensure comprehensiveness. Additionally, we incorporated COLA for syntactic anomaly detection and VUA for metaphor detection.</p>
<p>20Newsgroups. The 20Newsgroups dataset (Lang, Ken and Rennie, Jason, 2008) amounts to almost 20,000 news documents clustered into twenty groups, each of which corresponds to a different topic. Some newsgroups are closely related, such as comp.sys.ibm.pc.hardware and comp.sys.mac.hardware, while others are highly unconnected, namely, misc.forsale and soc.religion.christian.</p>
<p>Table 2: The outlier classes contained in our benchmark and a succinct definition of their data type.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Syntactic</td>
<td style="text-align: left;">A pronounced divergence in the arrangement of words and phrases.</td>
</tr>
<tr>
<td style="text-align: left;">Semantic</td>
<td style="text-align: left;">A difference in the subject or content appearing throughout the sample.</td>
</tr>
<tr>
<td style="text-align: left;">Pragmatic</td>
<td style="text-align: left;">The presence of metaphors or figures of speech in the sample.</td>
</tr>
<tr>
<td style="text-align: left;">Stylistic</td>
<td style="text-align: left;">A distinctive manner of expression, including but not limited to: repetition of <br> verbs or phrases, use of stopwords or punctuation.</td>
</tr>
</tbody>
</table>
<p>We have extracted six classes from the initial twenty: computer, recreation, science, miscellaneous, politics, and religion, as done in (Ruff et al., 2019). Each category is represented by a range of 577-2.856 training samples and 382-1.909 validation samples. Despite the relatively small size of the dataset, its classical relevance for NLP tasks prompted us to include it in our analysis.</p>
<p>AG News. The AG News dataset (Zhang et al., 2015) encompasses 496.835 categorized news articles from over 2.000 news sources. This topic classification corpus was gathered from multiple news sources for over a year. Out of all the classes, we have chosen the four largest classes in the dataset: business, sci, sports, and world. We are using the full 30.000 training samples per class, selecting only the title and description fields.</p>
<p>CoLA. CoLA (Warstadt et al., 2018) consists of a corpus of 10.657 sentences from 23 linguistics publications, books, and articles, annotated as being grammatically acceptable (in other words, grammatically correct) or not. Each sample is a sequence of words annotated with whether it is a grammatical English sentence. The public version of this dataset we used contains 8.551 sentences belonging to the training set and 1.043 sentences as part of the development set. As the test is not publicly available, we have used the development set as a de-facto test set for our work.</p>
<p>VUA. VUA (Steen et al., 2010) consists of 117 fragments sampled across four genres from the British National Corpus: Academic, News, Conversation, and Fiction and contain word-level all content-word metaphors annotations. The train set contains 12.122 lines or sentences and is the only publicly available subset of VUA. Under these circumstances, we applied an 80-20 train-test split on this solely open data subset and produced 8.485 train samples and 3.637 test samples.</p>
<p>The data is annotated according to the MIPVU procedure described by its authors. As a conse-
quence, the words annotated as metaphors have been prefixed with the "M_" string in the original annotation setting. To transform this initial problem setup of text segmentation into one of anomaly detection and for the data to comply with our methodology, we removed the word-level annotations and instead labeled the whole sentences as containing a metaphor or not.</p>
<h3>3.4 Newly-Proposed Datasets</h3>
<p>We introduce three new datasets: Song Genres, Gutenberg Categories, and Gutenberg Authors. The latter two were extracted from the Project Gutenberg website ${ }^{2}$. We scraped the entire website and parsed all bookshelves, which stored the book texts, their authors, titles, and the category in which Project Gutenberg placed them. We annotated the books for said category. The result is a corpus of over 15.000 literary texts ${ }^{3}$, along with their authors, titles, and titles and bookshelves (a term that Gutenberg maintainers use for categories). We then filtered this dataset to produce Gutenberg Categories and Gutenberg Authors.</p>
<p>Song Genres. The Song Lyrics is a dataset ${ }^{4}$ composed of four sources and consists of over 290.000 multilingual song lyrics and their respective genres. The initial data was forwarded from the 2018 Textract Hackathon ${ }^{5}$. This was enhanced with data collected from three other datasets from Kaggle: 150K Lyrics Labeled with Spotify Valence, dataset lyrics musics, and AZLyrics song lyrics.</p>
<p>To deal with the lack of labels, we have built a labeling system using the spotipy library, which uses the Spotify API to retrieve an Artist's genre. The Spotify API returns a list of genres for one artist, so we consider the mode of that list to be the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Data samples for each anomaly domain included in our benchmark. Apart from Pragmatic and Syntactic, we used only a chunk of the sample. We did not mention the inlier class in this table, as the majority class is constructed by clustering all non-outlier classes from the dataset as inliers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Outlier Class</th>
<th style="text-align: left;">Outlier Sample</th>
<th style="text-align: left;">Inlier Sample</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Semantic</td>
<td style="text-align: left;">computers</td>
<td style="text-align: left;">Apple has patented their <br> implementation of regions, <br> which presumably includes <br> the internal data structure.</td>
<td style="text-align: left;">I am looking for an inex- <br> pensive motorcycle, nothing <br> fancy, have to be able to do <br> all maintenance my self.</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic</td>
<td style="text-align: left;">Gramatically Unacceptable</td>
<td style="text-align: left;">They caused him to become <br> angry by making him.</td>
<td style="text-align: left;">Bill coughed his way out of <br> the restaurant.</td>
</tr>
<tr>
<td style="text-align: left;">Pragmatic</td>
<td style="text-align: left;">Metaphor</td>
<td style="text-align: left;">Mr Franklin went there at the <br> end of the 1970s, after the col- <br> lapse of Keyser Ullman.</td>
<td style="text-align: left;">It would be a criticism if I was <br> doing it to impoverish myself.</td>
</tr>
<tr>
<td style="text-align: left;">Stylistic</td>
<td style="text-align: left;">Arthur Conan Doyle</td>
<td style="text-align: left;">You may place considerable <br> confidence in Mr. Holmes, sir, <br> said the police agent loftily.</td>
<td style="text-align: left;">Mars, I scarcely need remind <br> the reader, revolves around <br> the sun at a mean distance of <br> $140,000,000$ miles.</td>
</tr>
</tbody>
</table>
<p>dominant genre of the lyrics of said artist. Additionally, we used the langdetect library to label the lyrics with a language automatically. In total, the lyrics come in 34 languages. Please note that we have only used the lyrics as our training data, with their respective genres as labels, leaving aside the corresponding artist, year, or song name. We've applied this procedure for all the data apart from the original 2018 Textract data.</p>
<p>From the original dataset, we have curated our subsequent Song Genres subset, which encompasses nine genres: Pop, Hip-Hop, Electronic, Indie, Rock, Metal, Country, Folk, and Jazz. Song Genres is designed to present an anomaly setup where crucial data aspects (e.g., melody, rhythm, etc.) are obscured or absent. This accentuates the necessity of discerning subtle text variations in songs to distinguish between different groups. Through this, we aim to foster the advancement of more robust models for AD in NLP.</p>
<p>Gutenberg Categories. The initial subset derived from the original Gutenberg data is termed as the Gutenberg Categories dataset. It comprises texts corresponding to 10 categories from the Gutenberg project website: Biology, Botany, United States Central Intelligence Agency, Canada, Detective, Historical, Mystery, Science, Children's, and Harvard. It's important to note that the categories are not inherently distinct by nature. Some, like CIA and Children's, are expected to be eas-
ily distinguishable, while others, such as Biology and Botany or Science and Harvard might exhibit significant overlap. We specifically included the CIA category to offer a class that stands distinctly separable from the rest in the text distribution.</p>
<p>We have selected 500 train samples from each class for the train set, and 100 samples per class for the test set. The samples have been extracted from multiple authors for each category, as to offer a wider distribution of styles, syntax, and grammar.</p>
<p>Gutenberg Authors. The Gutenberg Authors dataset represents our second subset generated from the project Gutenberg data. It comprises the texts respective to 10 authors: Arthur Conan Doyle, Charles Dickens, Charles Darwin, Mark Twain, Edgar Allan Poe, Walter Scott, United States Central Intelligence Agency, H. G. Wells, L. M. Montgomery, and Agatha Christie. Again, we aimed at providing different levels of complexity throughout our data, the reason for which we included authors whose novels are within the same genre, namely Doyle and Christie, as well as those writing about the same historical era or similar historical events, such as Twain and Dickens, and female authors which supposedly share a common sensibility, meaning Montgomery and Christie. We've added the CIA and Darwin classes with the same purpose as for Gutenberg Categories.</p>
<p>We have sampled between 400 and 500 train text chunks for each author and 100 test samples.</p>
<p>The samples have been extracted through multiple books for each author we consider for this experiment. This has been done to avoid the possibility of the event in which a particular anomaly class might be locked into a repetitive word or phrasing, e.g., character names (Sherlock Holmes, Huckleberry Finn, etc.), places (London, Washington, etc.) or simply by the sample length.</p>
<h2>4 Models</h2>
<h3>4.1 Experimental Methodology</h3>
<p>Our methodology consists of creating multiple data splits for each dataset within our benchmark, running our models on all splits, and finally aggregating the results. By data split, we refer to a separation of classes into one inlier class and a cluster of classes that are considered to be outliers in this setup. To achieve this, we iterate through all the classes for every dataset and, at each iteration, choose one of them as the inlier, while the rest are treated as outliers by the models. Through this, we achieve an important objective: we unfold an exhaustive series of experiments over all possible combinations of outliers and inliers, outlining which of the former are the most prominent and which are the hardest to detect. We ran a hyperparameter search for the two classical models on each split, thus finding the best parameters for detecting each outlier choice. The deep models were trained with a limited set of hyperparameters, as can be seen in Subsections 4.2 and 4.3.</p>
<p>Evaluation Metrics. We use AUROC in Table 4, as well as AUPR-In, and AUPR-Out in Table 6 and Supplementary Tables 7 and 8. AUROC (Area Under the Receiver Operating Characteristic) is the area under the curve where the false positive rate is on the X -axis and the true positive rate on the Y-axis. AUPR-In (Area Under the Precision-Recall for Inliers) is the area under the curve where the recall is on the X -axis and the precision is on the Yaxis. AUPR-Out (Area Under the Precision-Recall for Outliers) has the same definition as AUPR-In but is computed on inverted labels.</p>
<h3>4.2 Classical approaches</h3>
<p>The SVM classifier is a versatile model adaptable for outlier detection tasks, known as the One Class Support Vector Machine, as detailed in (Schölkopf et al., 1999). The OC-SVM aims to learn from an inlier class, designating test samples as anomalies when they deviate from the train-
ing dataset. The Isolation Forest technique is another outlier detection approach, drawing inspiration from the Random Forest model, as described in (Liu et al., 2008). In an n-dimensional space, inliers typically form denser clusters, whereas outliers tend to be more dispersed.</p>
<p>To optimize the performance of traditional methods, we conducted a comprehensive hyperparameter tuning. For the OC-SVM, we explored different kernels, namely rbf, polynomial, and linear, and assessed a range of $\nu$ values: $\nu \in 0.05,0.1,0.2,0.5$. For the Isolation Forest, we evaluated various numbers of estimators, specifically $64,100,128,256$. For both models, we compared the effectiveness of two embedding methods: FastText and GloVe, each with an embedding size of 300 .</p>
<h3>4.3 Neural approaches</h3>
<p>CVDD. Context Vector Data Description (CVDD) (Ruff et al., 2019) is a method that takes advantage of pre-trained word embeddings to perform AD on text. CVDD jointly learns the so-called "context vectors" and a multi-head self-attention mechanism that projects the word representations near these "context vectors" by minimizing the cosine distance between them.</p>
<p>CVDD allows the disentanglement of the context vectors such that they can provide more interpretable results and penalize non-orthogonal context vectors. The resulting projection function and context vectors act like a clustering method and cluster centroids, respectively. Anomalies are detected based on the mean sample distance from the sequence projection to the context vectors. We only search for the optimal number of context vectors like in (Ruff et al., 2019) $(c \in{3,5,10})$ and report the best performing models.</p>
<p>DATE. Detecting Anomalies in Text using ELECTRA (DATE) is an approach that uses selfsupervision for training Transformer networks using two pretext tasks tailored for detecting anomalies - Replaced Token Detection (RTD) (Clark et al., 2020) and Replaced Mask Detection (RMD). The method uses a generator to sample masked tokens and a discriminator to identify the replaced tokens and the masking patterns. The generator can be any distribution over the vocabulary. The discriminator is a BERT (Devlin et al., 2018) model trained from scratch. The Replaced Token Detection head ouputs a token-wise anomaly score which</p>
<p>Table 4: Best AUROC scores for each model and each split. We also provide the mean and standard deviations between the splits for all datasets apart from COLA and VUA, where we use a single split as our inlier class.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inlier Class</th>
<th style="text-align: center;">iForest</th>
<th style="text-align: center;">OCSVM</th>
<th style="text-align: center;">CVDD</th>
<th style="text-align: center;">DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">20 News</td>
<td style="text-align: center;">comp</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rec</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">83.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sci</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">69.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">misc</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">86.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pol</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rel</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean $\pm$ std</td>
<td style="text-align: center;">$66.5 \pm 7.8$</td>
<td style="text-align: center;">$71.3 \pm 6.6$</td>
<td style="text-align: center;">$55.6 \pm 4.7$</td>
<td style="text-align: center;">$83.2 \pm 6.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">business</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sci</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sports</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">95.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">world</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">90.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean $\pm$ std</td>
<td style="text-align: center;">$77.6 \pm 4.0$</td>
<td style="text-align: center;">$84.9 \pm 3.1$</td>
<td style="text-align: center;">$69.9 \pm 5.9$</td>
<td style="text-align: center;">$90.0 \pm 4.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Indie</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">53.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pop</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metal</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">51.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hip-Hop</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Electronic</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Country</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">66.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Folk</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R\&amp;B</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rock</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">54.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jazz</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean $\pm$ std</td>
<td style="text-align: center;">$52.3 \pm 7.8$</td>
<td style="text-align: center;">$51.1 \pm 7.4$</td>
<td style="text-align: center;">$50.1 \pm 6.7$</td>
<td style="text-align: center;">$58.0 \pm 7.2$</td>
</tr>
<tr>
<td style="text-align: center;">Gutenberg Categories</td>
<td style="text-align: center;">Detective</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">82.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Botany</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIA</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mystery</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">85.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Children's</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">79.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Harvard</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">82.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Canada</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">53.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Science</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Historical</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">66.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean $\pm$ std</td>
<td style="text-align: center;">$68.2 \pm 13.8$</td>
<td style="text-align: center;">$68.0 \pm 16.2$</td>
<td style="text-align: center;">$64.6 \pm 12.3$</td>
<td style="text-align: center;">$77.4 \pm 12.0$</td>
</tr>
<tr>
<td style="text-align: center;">Gutenberg Authors</td>
<td style="text-align: center;">C. Dickens</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A.C. Doyle</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M. Twain</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C. Darwin</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">W. Scott</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">11.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A. Christie</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E.A. Poe</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIA</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L.M. Montgomery</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">61.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H.G. Wells</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">11.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean $\pm$ std</td>
<td style="text-align: center;">$76.9 \pm 13.2$</td>
<td style="text-align: center;">$77.0 \pm 15.0$</td>
<td style="text-align: center;">$78.4 \pm 17.2$</td>
<td style="text-align: center;">$55.2 \pm 32.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 (Acceptable)</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0 (Non-Metaphor)</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">51.1</td>
</tr>
</tbody>
</table>
<p>Table 5: CVDD's ability to cluster semantically related words from the Hip-Hop Music Genre subset. The model effectively identifies variations in verb and pronoun usage, recognizes foreign language terms, and associates specific contexts with obscene terms. We highlight second context as it is the most meaningful one when detecting anomalies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Context 1</th>
<th style="text-align: center;">Context 2</th>
<th style="text-align: center;">Context 3</th>
<th style="text-align: center;">Context 4</th>
<th style="text-align: center;">Context 5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">'m</td>
<td style="text-align: center;">livin</td>
<td style="text-align: center;">to</td>
<td style="text-align: center;">the</td>
<td style="text-align: center;">que</td>
</tr>
<tr>
<td style="text-align: center;">i</td>
<td style="text-align: center;">turnin</td>
<td style="text-align: center;">can</td>
<td style="text-align: center;">of</td>
<td style="text-align: center;">mas</td>
</tr>
<tr>
<td style="text-align: center;">'re</td>
<td style="text-align: center;">shakin</td>
<td style="text-align: center;">could</td>
<td style="text-align: center;">in</td>
<td style="text-align: center;">como</td>
</tr>
<tr>
<td style="text-align: center;">y'</td>
<td style="text-align: center;">waitin</td>
<td style="text-align: center;">make</td>
<td style="text-align: center;">'s</td>
<td style="text-align: center;">porque</td>
</tr>
<tr>
<td style="text-align: center;">somebody</td>
<td style="text-align: center;">walkin</td>
<td style="text-align: center;">will</td>
<td style="text-align: center;">that</td>
<td style="text-align: center;">sabe</td>
</tr>
<tr>
<td style="text-align: center;">myself</td>
<td style="text-align: center;">keepin</td>
<td style="text-align: center;">would</td>
<td style="text-align: center;">is</td>
<td style="text-align: center;">desde</td>
</tr>
<tr>
<td style="text-align: center;">everybody</td>
<td style="text-align: center;">slippin</td>
<td style="text-align: center;">pray</td>
<td style="text-align: center;">this</td>
<td style="text-align: center;">boca</td>
</tr>
<tr>
<td style="text-align: center;">obscenity1</td>
<td style="text-align: center;">lickin</td>
<td style="text-align: center;">try</td>
<td style="text-align: center;">surrounds</td>
<td style="text-align: center;">se</td>
</tr>
<tr>
<td style="text-align: center;">obscenity2</td>
<td style="text-align: center;">standin</td>
<td style="text-align: center;">come</td>
<td style="text-align: center;">descends</td>
<td style="text-align: center;">encontrar</td>
</tr>
</tbody>
</table>
<p>Table 6: The Area under the Precision-Recall curve for all of the datasets, averaged over the subsets. We highlight the best scores in bold. Abbreviations: 20NG - 20Newsgroups, AGN - AGNews, SG - Song Genres, GC - Gutenberg Categories, GA - Gutenberg Authors, CA - COLA, VA - VUA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">OCSVM</th>
<th style="text-align: center;">iForest</th>
<th style="text-align: center;">CVDD</th>
<th style="text-align: center;">DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">90.8 $\pm$ 5.0</td>
<td style="text-align: center;">89.6 $\pm$ 5.6</td>
<td style="text-align: center;">85.0 $\pm$ 7.3</td>
<td style="text-align: center;">41.4 $\pm 14.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">40.1 $\pm 17.5$</td>
<td style="text-align: center;">32.6 $\pm 13.5$</td>
<td style="text-align: center;">22.5 $\pm 7.7$</td>
<td style="text-align: center;">96.0 $\pm$ 1.7</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">93.1 $\pm$ 1.0</td>
<td style="text-align: center;">88.9 $\pm 3.3$</td>
<td style="text-align: center;">85.5 $\pm 3.6$</td>
<td style="text-align: center;">76.9 $\pm 9.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">70.4 $\pm 6.7$</td>
<td style="text-align: center;">57.5 $\pm 6.9$</td>
<td style="text-align: center;">46.1 $\pm 8.9$</td>
<td style="text-align: center;">95.9 $\pm$ 1.7</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">90.6 $\pm 2.3$</td>
<td style="text-align: center;">90.7 $\pm$ 1.9</td>
<td style="text-align: center;">90.3 $\pm 2.3$</td>
<td style="text-align: center;">12.8 $\pm 3.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">12.3 $\pm 7.0$</td>
<td style="text-align: center;">11.7 $\pm 4.0$</td>
<td style="text-align: center;">10.2 $\pm 2.3$</td>
<td style="text-align: center;">92.3 $\pm$ 1.9</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">94.0 $\pm 4.8$</td>
<td style="text-align: center;">94.1 $\pm$ 4.3</td>
<td style="text-align: center;">93.9 $\pm 3.2$</td>
<td style="text-align: center;">36.8 $\pm 26.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">25.1 $\pm 24.0$</td>
<td style="text-align: center;">22.6 $\pm 16.3$</td>
<td style="text-align: center;">19.4 $\pm 11.2$</td>
<td style="text-align: center;">96.3 $\pm$ 2.7</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">96.3 $\pm 2.5$</td>
<td style="text-align: center;">96.4 $\pm 2.2$</td>
<td style="text-align: center;">96.3 $\pm 4.1$</td>
<td style="text-align: center;">48.5 $\pm 33.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">39.4 $\pm 25.3$</td>
<td style="text-align: center;">36.8 $\pm 21.1$</td>
<td style="text-align: center;">37.9 $\pm 17.7$</td>
<td style="text-align: center;">97.2 $\pm$ 3.5</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">31.8 $\pm$ N/A</td>
<td style="text-align: center;">31.6 $\pm$ N/A</td>
<td style="text-align: center;">34.2 $\pm$ N/A</td>
<td style="text-align: center;">36.4 $\pm$ N/A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">68.0 $\pm$ N/A</td>
<td style="text-align: center;">66.9 $\pm$ N/A</td>
<td style="text-align: center;">69.9 $\pm$ N/A</td>
<td style="text-align: center;">73.5 $\pm$ N/A</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">AUPR-In</td>
<td style="text-align: center;">77.2 $\pm$ N/A</td>
<td style="text-align: center;">78.1 $\pm$ N/A</td>
<td style="text-align: center;">76.1 $\pm$ N/A</td>
<td style="text-align: center;">55.7 $\pm$ N/A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUPR-Out</td>
<td style="text-align: center;">73.3 $\pm$ N/A</td>
<td style="text-align: center;">74.8 $\pm$ N/A</td>
<td style="text-align: center;">75.0 $\pm$ N/A</td>
<td style="text-align: center;">48.4 $\pm$ N/A</td>
</tr>
</tbody>
</table>
<p>can also be used as an explainability mechanism. The anomalies are then detected by averaging the $P L_{R T D}$ score over the entire sequence.</p>
<p>We use the same experimental setup and hyperparameters as in (Manolache et al., 2021) for the 20Newsgroups and AG News datasets, and the AG News setup for the other datasets.</p>
<h2>5 Experiments</h2>
<h3>5.1 Quantitative results</h3>
<p>We test the performance of all models on all the datasets within our benchmark and report the scores using the best hyperparameters following
the search methodology mentioned in Sec. 4. The models were trained on an inlier class, and every other class was considered anomalous at test time.</p>
<p>The results of the experiments are in Tables 4 and 6 , calculated as averages over all splits for each dataset. We can observe that DATE has achieved the best AUROC scores on six out of our seven datasets, with a total average score of $71.11 \%$, followed by CVDD, which scored the best on VUA. The average AUROC scores are almost equal across the three models: OC-SVM - $64.9 \%$, IsoForest $63.2 \%$, CVDD - $64.3 \%$. Interestingly, every other model is close to $50 \%$ AUROC on COLA. Results on 20Newsgroups and AGNews are similar across all splits; thus, it makes sense to consider either split as the inlier.</p>
<p>The Area under the Precision-Recall Curve (AUPR) results are summarized in Table 6, with detailed results for each split being available in the Appendix in Tables 7 and 8, where we provide an exhaustive table with the metrics for every split and for each dataset.</p>
<h3>5.2 Qualitative results</h3>
<p>We provide a sample of qualitative results in Figure 1, where we show how DATE can detect semantic nuances for the Botany subset of Gutenberg Categories, and in the Supplementary Figure 2, where we show how DATE can detect stylistic changes when correctly profiling an author. The model is suspicious of certain punctuation and word usage. When scoring an inlier entry, DATE seems to think that some stopword usage is anomalous; this could indicate a change of stylistic approach when an</p>
<h2>Botany Detected Inlier</h2>
<p>thirty flowers on the crossed plants were crossed with ##pol ##len from other crossed plants of the same generation, and the ##1 ##wen ##ty - six capsule ##s thus produced contained, on an average, 4.3 seeds; whilst thirty flowers on the self - fe ##r5 ##lis ##ed plants, fe ##r5 ##lis ##ed with the ##pol ##len from the same flower, produced twenty - three capsule ##s, each ##con ##tain ##ing 4.3 seeds . thus the average number of seeds in the crossed ##cap ##out ##es was to that in the self - fe ##r5 ##lis ##ed capsule ##s as to, ah ##und ##red of the crossed seeds weighed [SEP]</p>
<h2>Botany Detected Outlier</h2>
<p>it would have been an old look for a child of ##1 ##we ##1 ##ve, and sars crewe was only seven. the fact was, however, that she ##was always dreaming and thinking odd things and could not herself ##rem ##em ##ber any time when she had not been thinking things about grown - up ##pe ##ople and the world they belonged to, she felt as if she had lived along, long time. I this moment she was remembering the voyage she had just made from ##bo ##mba ##y with her father, captain crewe, she was thinking of the big ##ship, of the las ##cars passing in [SEP]</p>
<p>Figure 1: We provide a qualitative examples for outliers and inliers detected by DATE for the Botany subset of the Guttemberg Categories dataset. A stronger red highlight indicates a greater anomaly score. We can see that, in the case of the inlier sample, there is no strong indication of abnormalities, since the contents relate to Botany. For the outlier sample, words such as "ship" and "cars" are detected as being anomalous.
author is writing texts in registers.
We show in Table 5 how CVDD creates clusters of semantically similar words when trained on the Hip-Hop Music Genre subset. The context vectors act as topic centroids. CVDD cannot only distinguish between colloquial usage of verbs, pronouns, and foreign languages but also associates unreproducible words (e.g., obscene, insulting, etc.) with certain contexts.</p>
<h2>6 Conclusions</h2>
<p>We introduce AD-NLP, a benchmark for anomaly detection in text over an extensive assortment of outlier scenarios, covering syntactic, semantic, pragmatic, and stylistic language anomalies. Additionally, we introduce three new datasets as part of AD-NLP: Song Genres, Gutenberg Categories, and Gutenberg Authors. Song Genres provides a complex setting in which part of information about the data has been obscured, enforcing a distinct focus on the subtle differences between texts, while the two datasets derived from the Gutenberg data are meant to provide variety on multiple levels: syntax, style, genre, and literary movement. We find that anomalies that depend solely on semantic or stylistic aspects of the text are easier to recognize, whereas those that only partially depend on the text,
like song lyrics, are harder to detect and separate. We have also disclosed our results on various models and found out that the neural models react well to domain-specific words, author idiosyncrasies and punctuation as being anomalous or not. We hope that the proposed benchmark and tools will facilitate research in Text Anomaly Detection.</p>
<h2>7 Limitations \&amp; Further Work</h2>
<p>Some of our more simple baselines managed to outperform more sophisticated anomaly detection methods in some scenarios. For instance, we did observe that on datasets such as COLA and VUA, both CVDD and DATE obtain weak results - as an example, in Tables 7 and 8 of the Supplementary Material, we can observe that the OC-SVM and the Isolation Forest outperform DATE on the metaphor detection task from VUA. Moreover, we can observe that for every dataset in the benchmark, there are instances where the Isolation Forest and the OC-SVM outperform the more sophisticated CVDD and DATE methods. Therefore, we believe that it's very important to be able to analyze the limitations and inductive biases on a wide range of scenarios while developing an Anomaly Detection methodology. One way of accomplishing this would be studying AD-NLP at an even more granular level. For example, one could determine various linguistic properties of texts written by Edgar Allan Poe, aiming to discover the reasons behind the poor performance of DATE compared to the other authors, as can be observed in Table 4. Further linguistic analysis would benefit the quality of AD-NLP, and we leave this undertaking for further work.</p>
<p>Benchmark updates. We commit to enhancing AD-NLP with future datasets that would expand the intra or inter-domain variety, either by adding new datasets to AD-NLP or scraping, aggregating, and labeling new data off the web ourselves. We also commit to making the dataset more accessible through multiple hosting services, as well as updating our GitHub repository.</p>
<h2>Acknowledgments</h2>
<p>AM acknowledges the International Max Planck Research School for Intelligent Systems (IMPRSIS) for the support provided.</p>
<h2>References</h2>
<p>C. C. Aggarwal and C. K. Reddy, editors. Data Clustering: Algorithms and Applications. CRC Press, 2014. ISBN 978-1-46-655821-2. URL http://www. crcpress.com/product/isbn/9781466558212.
U. Arora, W. Huang, and H. He. Types of out-ofdistribution texts and how to detect them. CoRR, abs/2109.06827, 2021. URL https://arxiv.org/ abs/2109.06827.
J. Bevendorff, B. Ghanem, A. Giachanou, M. Kestemont, E. Manjavacas, I. Markov, M. Mayerl, M. Potthast, F. Rangel Pardo, P. Rosso, G. Specht, E. Stamatatos, B. Stein, M. Wiegmann, and E. Zangerle. Overview of PAN 2020: Authorship Verification, Celebrity Profiling, Profiling Fake News Spreaders on Twitter, and Style Change Detection, pages 372383. 09 2020. ISBN 978-3-030-58218-0. doi: 10.1007/978-3-030-58219-7_25.
M. Braei and S. Wagner. Anomaly detection in univariate time-series: A survey on the state-of-the-art. 04 2020.
V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Comput. Surv., 41 (3), July 2009. ISSN 0360-0300. doi: 10.1145/ 1541880.1541882. URL https://doi.org/10. 1145/1541880.1541882.
K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. ArXiv, abs/2003.10555, 2020.
J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805.
F. Y. Edgeworth. Xli. on discordant observations. Philosophical Magazine Series 1, 23:364-375, 1887. URL https://api.semanticscholar.org/ CorpusID:120568135.
D. M. Hawkins. Identification of outliers, volume 11. Springer, 1980.
R. He and J. McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In Proceedings of the 25th International Conference on World Wide Web, WWW '16, page 507-517, Republic and Canton of Geneva, CHE, 2016. International World Wide Web Conferences Steering Committee. ISBN 9781450341431. doi: 10.1145/2872427.2883037. URL https://doi. org/10.1145/2872427.2883037.
E. M. Knorr and R. T. Ng. A unified notion of outliers: Properties and computation. In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, KDD'97, page 219-222. AAAI Press, 1997.</p>
<p>Lang, Ken and Rennie, Jason. 20 newsgroups dataset, 2008. URL http://people.csail.mit. edu/jrennie/20Newsgroups/.
F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM '08, page 413-422, USA, 2008. IEEE Computer Society. ISBN 9780769535029. doi: 10.1109/ICDM.2008.17. URL https://doi.org/10.1109/ICDM.2008.17.
C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps in matlab. In 2013 IEEE International Conference on Computer Vision, pages 2720-2727, 2013. doi: 10.1109/ICCV.2013.338.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng , and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015.
V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos. Anomaly detection in crowded scenes. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1975-1981, 2010. doi: 10.1109/CVPR.2010.5539872.
L. M. Manevitz and M. Yousef. One-class svms for document classification. J. Mach. Learn. Res., 2: 139-154, Mar. 2002. ISSN 1532-4435.
A. Manolache, F. Brad, and E. Burceanu. DATE: Detecting anomalies in text via self-supervision of transformers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 267-277, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.25. URL https: //aclanthology.org/2021.naacl-main. 25.
B. McCann, N. Keskar, C. Xiong, and R. Socher. The natural language decathlon: Multitask learning as question answering. 062018.
K. Ord. Outliers in statistical data : V. Barnett and T. Lewis, 1994, 3rd edition, (John Wiley \&amp; Sons, Chichester), 584 pp., [UK pound]55.00, ISBN 0-471-93094-6. International Journal of Forecasting, 12(1):175-176, March 1996. URL https://ideas.repec.org/a/eee/intfor/ v12y1996i1p175-176.html.
B. Radford, L. Apolonio, A. Trias, and J. Simpson. Network traffic anomaly detection using recurrent neural networks. 032018.
L. Ruff, R. A. Vandermeulen, N. Görnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft. Deep one-class classification. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 4393-4402, 2018.</p>
<p>L. Ruff, Y. Zemlyanskiy, R. Vandermeulen, T. Schnake, and M. Kloft. Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 40614071, jul 2019.
L. Ruff, J. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek, M. Kloft, T. G. Dietterich, and K. Muller. A unifying review of deep and shallow anomaly detection. ArXiv, abs/2009.11732, 2020.
S. W. Sam Dobbins, Mike Topliss. Reuters-21578 dataset, 1987. URL http://kdd.ics.uci.edu/ databases/reuters21578/reuters21578.html.
B. Schölkopf, R. Williamson, A. Smola, J. ShaweTaylor, and J. Platt. Support vector method for novelty detection. volume 12, pages 582-588, 011999.
I. Sharafaldin, A. Habibi Lashkari, and A. Ghorbani. Toward generating a new intrusion detection dataset and intrusion traffic characterization. pages 108-116, 01 2018. doi: 10.5220/0006639801080116.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. EMNLP, 1631:1631-1642, 012013.
J. Song, H. Takakura, Y. Okabe, M. Eto, D. Inoue, and K. Nakao. Statistical analysis of honeypot data and building of kyoto 2006+ dataset for nids evaluation. In Proceedings of the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security, BADGERS '11, page 29-36, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450307680. doi: 10.1145/1978672.1978676. URL https://doi. org/10.1145/1978672.1978676.
G. Steen, L. Dorst, J. Herrmann, A. Kaal, T. Krennmayr, and T. Pasma. A method for linguistic metaphor identification: From MIP to MIPVU. 06 2010. doi: 10.1075/celcr. 14.
S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. Cost-based modeling for fraud and intrusion detection: results from the jam project. In Proceedings DARPA Information Survivability Conference and Exposition. DISCEX'00, volume 2, pages 130-144 vol.2, 2000. doi: 10.1109/DISCEX.2000.821515.
T. B. A. E. P. B. Tirthankar Ghosal, Tanik Saikh. Novelty Detection: A Perspective from Natural Language Processing. Computational Linguistics JOurnal, April 2022. URL https://direct. mit.edu/coli/article/48/1/77/108847/ Novelty-Detection-A-Perspective-from-Natural.
M. J. M. Turcotte, A. D. Kent, and C. Hash. Unified Host and Network Data Set, chapter Chapter 1, pages 1-22. World Scientific, nov 2018. doi: 10.1142/9781786345646_ 001. URL https://www.worldscientific.com/ doi/abs/10.1142/9781786345646_001.
A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018.
A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. 05 2019a.
H. Wang, M. Li, F. Ma, S.-L. Huang, and L. Zhang. Unsupervised anomaly detection via generative adversarial networks: Poster abstract. In Proceedings of the 18th International Conference on Information Processing in Sensor Networks, IPSN '19, page 313-314, New York, NY, USA, 2019b. Association for Computing Machinery. ISBN 9781450362849. doi: 10.1145/3302506.3312605. URL https://doi. org/10.1145/3302506.3312605.
S. Wang, Y. Zeng, X. Liu, E. Zhu, J. Yin, C. Xu, and M. Kloft. Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network. In NeurIPS, 2019c.
A. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.
G. Yu, S. Wang, Z. Cai, E. Zhu, C. Xu, J. Yin, and M. Kloft. Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events, page 583-591. Association for Computing Machinery, New York, NY, USA, 2020. ISBN 9781450379885. URL https://doi.org/10. 1145/3394171.3413973.
X. Zhang, J. Zhao, and Y. LeCun. Character-level convolutional networks for text classification. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, page 649-657, Cambridge, MA, USA, 2015. MIT Press.
Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma. Singleimage crowd counting via multi-column convolutional neural network. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 589-597, 2016. doi: 10.1109/CVPR.2016.70.
C. Zhou and R. C. Paffenroth. Anomaly detection with robust deep autoencoders. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17, page 665-674, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983.3098052. URL https://doi. org/10.1145/3097983.3098052.</p>
<h1>8 Supplementary Materials</h1>
<h3>8.1 More quantitative results</h3>
<p>In Tables 7 and 8 we present the Area Under the Precision-Recall curve (AUPR) results for all our models and datasets. This provides a clear measure of how each model performed across different datasets.</p>
<p>Table 7: Best AUPR-In scores for each model and for each split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inlier Class</th>
<th style="text-align: center;">iForest</th>
<th style="text-align: center;">OCSVM</th>
<th style="text-align: center;">CVDD</th>
<th style="text-align: center;">DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">comp</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rec</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">83.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sci</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">69.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">misc</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">86.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pol</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rel</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">business</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">74.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sci</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sports</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">world</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Indie</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">10.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pop</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">11.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metal</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">10.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hip-Hop</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">9.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Electronic</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Country</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">15.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Folk</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R\&amp;B</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">18.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rock</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jazz</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">19.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Detective</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Botany</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">59.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIA</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mystery</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">28.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Children's</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Harvard</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Canada</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">11.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Science</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">18.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Historical</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">13.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C. Dickens</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A.C. Doyle</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">7.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M. Twain</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">61.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C. Darwin</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">W. Scott</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">11.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A. Christie</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E.A. Poe</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIA</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L.M. Montgomery</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">55.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H.G. Wells</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">11.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 (Acceptable)</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0 (Non-Metaphor)</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">55.7</td>
</tr>
</tbody>
</table>
<p>Table 8: Best AUPR-Out scores for each model and for each split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inlier Class</th>
<th style="text-align: center;">iForest</th>
<th style="text-align: center;">OCSVM</th>
<th style="text-align: center;">CVDD</th>
<th style="text-align: center;">DATE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">comp</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rec</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">83.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sci</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">69.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">misc</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">86.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pol</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">rel</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">business</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">96.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sci</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">93.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sports</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">98.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">world</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">95.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Indie</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">91.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pop</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">92.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metal</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hip-Hop</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">91.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Electronic</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">90.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Country</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Folk</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">91.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R\&amp;B</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">94.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rock</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">91.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jazz</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Detective</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">97.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Botany</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIA</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">45.22</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mystery</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">98.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">97.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Children's</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">97.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Harvard</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">97.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Canada</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">90.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Science</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">95.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Historical</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">95.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Charles Dickens</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">98.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A.C. Doyle</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">89.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M. Twain</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">99.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C. Darwin</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">W. Scott</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">94.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A. Christie</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E.A. Poe</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">97.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIA</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L.M. Montgomery</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H.G. Wells</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">94.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 (Acceptable)</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">73.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0 (Non-Metaphor)</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">48.4</td>
</tr>
</tbody>
</table>
<h1>9 More qualitative examples</h1>
<p>In this section, we provide additional qualitative examples to for DATE in Figure 2. These examples are derived from the Gutenberg dataset, focusing specifically on three distinct subsets: Biology, works of Agatha Christie, and CIA-related texts. The aim is to showcase how DATE identifies and highlights anomalous patterns within different authors and styles of text.</p>
<h2>Biology True Positive</h2>
<p>these two ##fis ##cu ##ff ##ies determine all the others . a creature that feels and moves ##re ##qui ##res a stomach to carry food in . food requires instruments tod ##iv ##ide it, liquids to digest it . plants, which do not feel and do not ##mo ##ve, have no need of a stomach, but have roots instead . thus the " animal functions " of feeling and moving determine the character of ##the organs of the second order, the organs of digest ##ion . these int ##hei ##r turn are prior to the organs of circulation, which are a means ##to the end of di ##st ##ri ##bu ##tin [SEP]</p>
<h2>Agata Christie True Negative</h2>
<p>then ##he turned to whitaker 's alma ##nac ##k to brows ##e upon the statistics of the ##gre ##at european armies, he was rouse ##d from this by the breakfast gong. I breakfast there was no talk of anything but war. hugh was as excited ##as a cat in thunder ##y weather, and the small boys wanted information ##ab ##out flags . the russian and the partisan flag were in dispute, and the ##fi ##ag page of webster 's dictionary had to be consulted. newspapers and ##lette ##rs were both abnormal ##ly late, and mr . brit ##ling . ti ##ring of supplying ##tri ##vial I [SEP]</p>
<h2>CIA True Negative</h2>
<p>here again we see how much more vigorous the ##cross ##ed plants are than the self - fe ##rti ##lis ##ed . ross ##ed and self - fe ##rti ##lis ##ed plants of the seventh generation, he ##se were raised as here ##to ##for ##e with the following result : - - table 2 / 8 . ip ##omo ##ea pu ##rp ##ure ##a ( seventh generation) . eight ##s of plants in inches : column : number ( name ) of pot . ol ##um ##n : crossed plants . ol ##um ##n 3 : self - fe</p>
<p>Figure 2: Qualitative examples of DATE on the Gutenberg data. From top to bottom: Biology True Positive sample, Agatha Christie True Negative sample, and CIA True Negative sample. The darker the colour of the highlight, the more greater the anomaly score for the word.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.gutenberg.org/
${ }^{3}$ https://www.kaggle.com/mateibejan/
15000-gutenberg-books
${ }^{4}$ https://www.kaggle.com/mateibejan/ multilingual-lyrics-for-genre-classification
${ }^{5}$ https://www.sparktech.ro/textract-2018/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
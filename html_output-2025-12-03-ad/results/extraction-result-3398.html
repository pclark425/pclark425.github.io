<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-270062293</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.16661v3.pdf" target="_blank">RLSF: Fine-tuning LLMs via Symbolic Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have transformed AI but often struggle with tasks that require domain-specific reasoning and logical alignment. Traditional fine-tuning methods do not leverage the vast amount of symbolic domain-knowledge available to us via symbolic reasoning tools (e.g., provers), and are further limited by sparse rewards and unreliable reward models. We introduce Reinforcement Learning via Symbolic Feedback (RLSF), a novel fine-tuning paradigm where symbolic reasoning tools (e.g., solvers, provers, and algebra systems) provide fine-grained feedback to LLMs. RLSF uses poly-sized certificates (e.g., proofs) generated by symbolic tools to identify and correct errors in model outputs, offering token-level guidance without requiring differentiable reasoning systems. This paradigm bridges the gap between symbolic reasoning and LLM fine-tuning, enabling precise alignment with domain-specific constraints while addressing key limitations of traditional reward signals. Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on five different applications (that have some associated logical or domain constraints), namely, program synthesis from natural language pseudo-code to programming language, three chemistry tasks, and solving the Game of 24. A key takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7-billion-parameter transformer LLM used in this paper as a baseline and as a model fine-tuned with RLSF for three chemistry tasks (molecule generation, forward synthesis, retrosynthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mistral-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM with ~7 billion parameters (obtained from [20] in the paper). Used as a pre-trained base model and then fine-tuned via supervised fine-tuning (SFT), RL with Boolean feedback, and RLSF (token-level symbolic feedback). LoRA adapters were applied during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES sequence generation from natural language prompts; models were fine-tuned via supervised learning and then via reinforcement learning using RLSF which supplies token-level rewards derived from RDKit error/validation outputs (PPO used for RL).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry: molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS) (relevant for drug discovery / reaction prediction / molecular design).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact Match (EM) to gold outputs, Fingerprint Tanimoto Similarity (FTS) using Morgan fingerprints, and Validity (SMILES grammar + chemical valence rules).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>After supervised fine-tuning (SFT) the model's performance improved; applying RL with Boolean scalar feedback produced further gains (for mistral-7b: +2.9% EM and +10.9% Validity on MG, +3.2% EM and +1.2% Validity on FS, +5.2% EM and no change in Validity on RS). The paper reports that applying RLSF (token-level RDKit feedback) on SFT-tuned models produced the best results (RLSF yields further improvements over Boolean-RL), though the paper gives explicit RLSF delta numbers principally for galactica-1.3b (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared against (a) zero-shot GPT-4 and open models' zero-shot performance, (b) supervised fine-tuning (SFT), and (c) RL with Boolean scalar feedback. RL-Boolean improved over SFT as reported; RLSF (token-level feedback) produced the best results overall for the two open models evaluated (mistral-7b and galactica-1.3b).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper does not report full per-task RLSF numeric breakdown for mistral-7b in the main text snippet; chemistry tasks have multiple valid outputs per input which lowers Exact Match (EM) even for chemically valid/purposeful outputs; symbolic feedback requires RDKit calls (though RDKit latency is small per example); potential remaining gaps between validity and exact match (valid outputs may not match specific gold-standard SMILES even if chemically plausible).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLSF: Fine-tuning LLMs via Symbolic Feedback', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>galactica-1.3b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.3-billion-parameter open-source science-focused LLM from Meta used in experiments and fine-tuned with RLSF to generate SMILES and perform forward/reverse reaction prediction, achieving notable gains over SFT and comparisons to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>galactica-1.3b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM (≈1.3B parameters) from Meta AI; used as a pre-trained base then supervised fine-tuned and further fine-tuned with RLSF using LoRA during experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation and reaction prediction from natural-language prompts; fine-tuning pipeline: SFT → optional RL with Boolean feedback → RLSF (token-level feedback) where RDKit analyzes generated SMILES and produces token-level error signals used as dense reward vectors for PPO fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry tasks: molecular generation from NL descriptions, forward synthesis (product prediction), retrosynthesis (reactant prediction) — applications relevant to molecular design and synthetic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact Match (EM) against gold outputs, Fingerprint Tanimoto Similarity (FTS) of Morgan fingerprints, and Validity (SMILES grammar and chemical valence checks using RDKit).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RLSF (token-level RDKit feedback) applied to SFT-tuned galactica-1.3b produced the best reported results: compared to SFT, RLSF gave +8.0% EM and +58.3% Validity for Molecule Generation (MG), +11.8% EM and +2.1% Validity for Forward Synthesis (FS), and +12.2% EM and +3.2% Validity for Retrosynthesis (RS). The paper states RLSF-tuned galactica-1.3b outperformed GPT-4 on these chemistry tasks despite being ~1000× smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Benchmarked vs (1) zero-shot GPT-4 (closed-source) which had high validity but low EM (e.g., GPT-4 zero-shot: MG Validity 90% with EM 3%; FS Validity 93.1% with EM 0.4%; RS Validity 88.2% with EM 0.8%), (2) supervised fine-tuning (SFT), and (3) RL with Boolean scalar feedback. RLSF on galactica-1.3b improved EM and Validity substantially over SFT and RL-Boolean and is reported to surpass GPT-4 in exact-match metrics on the selected datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Key limitations noted: (a) Exact Match is a strict metric and multiple chemically-correct outputs exist per input, so EM can underrepresent chemically valid generation, (b) symbolic feedback depends on RDKit analysis — RDKit flags valence/syntax errors but cannot fully capture downstream application utility, (c) the study focuses on fine-tuning only (not inference-time multi-step symbolic interaction), (d) for problems with many valid outputs, supervised memorization can mask generalization, and (e) while RDKit latency is low (~0.003s per generation reported), symbolic-tool overhead and dataset curation remain practical considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLSF: Fine-tuning LLMs via Symbolic Feedback', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4-turbo (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source ultra-large LLM from OpenAI used as a strong baseline in zero-shot evaluations for the three chemistry tasks; reported to produce high SMILES validity but low exact-match rates to gold outputs in these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source very large LLM (paper cites an approximate parameter order-of-magnitude of ~1.76 trillion as a rumor); accessed via API and evaluated in zero-shot for chemistry SMILES tasks. Used only as a baseline comparator (no fine-tuning reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Zero-shot SMILES generation and reaction prediction from natural language prompts; outputs were assessed by RDKit for validity and compared to gold outputs for EM/FTS.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry: molecule generation, forward synthesis, retrosynthesis (used as a baseline to compare to RLSF-fine-tuned smaller open models).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact Match (EM), Fingerprint Tanimoto Similarity (FTS), and Validity (SMILES grammar and valence checks via RDKit).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Zero-shot GPT-4 produced high validity but very low exact-match rates on the chosen test splits: for Molecule Generation (MG) GPT-4 valid SMILES ~90% with EM 3%; for Forward Synthesis (FS) Validity ~93.1% with EM 0.4%; for Retrosynthesis (RS) Validity ~88.2% with EM 0.8%. RLSF-fine-tuned smaller models (e.g., galactica-1.3b) achieved higher EM and improved validity in reported experiments, leading the authors to state that RLSF-tuned smaller models outperformed GPT-4 on the evaluated EM metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>GPT-4 served as a closed-source baseline; while GPT-4 had better zero-shot validity than unfine-tuned open models, RLSF fine-tuning on smaller open models yielded higher exact-match performance on the curated datasets used in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Although GPT-4 produces chemically valid SMILES at high rates, it achieved low Exact Match (EM) to the dataset gold outputs — likely because many valid alternative SMILES/reaction outcomes exist and EM is strict; the paper cautions that EM can understate useful chemical generation. Also GPT-4 was only evaluated zero-shot (no fine-tuning applied), so direct comparisons to fine-tuned open models have caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RLSF: Fine-tuning LLMs via Symbolic Feedback', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. <em>(Rating: 2)</em></li>
                <li>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language. <em>(Rating: 2)</em></li>
                <li>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. <em>(Rating: 2)</em></li>
                <li>A review of transformers in drug discovery and beyond. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3398",
    "paper_id": "paper-270062293",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "Mistral-7B",
            "name_full": "mistral-7b",
            "brief_description": "An open-source 7-billion-parameter transformer LLM used in this paper as a baseline and as a model fine-tuned with RLSF for three chemistry tasks (molecule generation, forward synthesis, retrosynthesis).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mistral-7b",
            "model_description": "Open-source transformer LLM with ~7 billion parameters (obtained from [20] in the paper). Used as a pre-trained base model and then fine-tuned via supervised fine-tuning (SFT), RL with Boolean feedback, and RLSF (token-level symbolic feedback). LoRA adapters were applied during fine-tuning.",
            "generation_method": "Direct SMILES sequence generation from natural language prompts; models were fine-tuned via supervised learning and then via reinforcement learning using RLSF which supplies token-level rewards derived from RDKit error/validation outputs (PPO used for RL).",
            "application_domain": "Chemistry: molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS) (relevant for drug discovery / reaction prediction / molecular design).",
            "evaluation_metrics": "Exact Match (EM) to gold outputs, Fingerprint Tanimoto Similarity (FTS) using Morgan fingerprints, and Validity (SMILES grammar + chemical valence rules).",
            "results_summary": "After supervised fine-tuning (SFT) the model's performance improved; applying RL with Boolean scalar feedback produced further gains (for mistral-7b: +2.9% EM and +10.9% Validity on MG, +3.2% EM and +1.2% Validity on FS, +5.2% EM and no change in Validity on RS). The paper reports that applying RLSF (token-level RDKit feedback) on SFT-tuned models produced the best results (RLSF yields further improvements over Boolean-RL), though the paper gives explicit RLSF delta numbers principally for galactica-1.3b (see separate entry).",
            "comparison_to_baselines": "Compared against (a) zero-shot GPT-4 and open models' zero-shot performance, (b) supervised fine-tuning (SFT), and (c) RL with Boolean scalar feedback. RL-Boolean improved over SFT as reported; RLSF (token-level feedback) produced the best results overall for the two open models evaluated (mistral-7b and galactica-1.3b).",
            "limitations_challenges": "Paper does not report full per-task RLSF numeric breakdown for mistral-7b in the main text snippet; chemistry tasks have multiple valid outputs per input which lowers Exact Match (EM) even for chemically valid/purposeful outputs; symbolic feedback requires RDKit calls (though RDKit latency is small per example); potential remaining gaps between validity and exact match (valid outputs may not match specific gold-standard SMILES even if chemically plausible).",
            "uuid": "e3398.0",
            "source_info": {
                "paper_title": "RLSF: Fine-tuning LLMs via Symbolic Feedback",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Galactica-1.3B",
            "name_full": "galactica-1.3b",
            "brief_description": "A 1.3-billion-parameter open-source science-focused LLM from Meta used in experiments and fine-tuned with RLSF to generate SMILES and perform forward/reverse reaction prediction, achieving notable gains over SFT and comparisons to GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "galactica-1.3b",
            "model_description": "Open-source transformer LLM (≈1.3B parameters) from Meta AI; used as a pre-trained base then supervised fine-tuned and further fine-tuned with RLSF using LoRA during experiments.",
            "generation_method": "Direct SMILES generation and reaction prediction from natural-language prompts; fine-tuning pipeline: SFT → optional RL with Boolean feedback → RLSF (token-level feedback) where RDKit analyzes generated SMILES and produces token-level error signals used as dense reward vectors for PPO fine-tuning.",
            "application_domain": "Chemistry tasks: molecular generation from NL descriptions, forward synthesis (product prediction), retrosynthesis (reactant prediction) — applications relevant to molecular design and synthetic planning.",
            "evaluation_metrics": "Exact Match (EM) against gold outputs, Fingerprint Tanimoto Similarity (FTS) of Morgan fingerprints, and Validity (SMILES grammar and chemical valence checks using RDKit).",
            "results_summary": "RLSF (token-level RDKit feedback) applied to SFT-tuned galactica-1.3b produced the best reported results: compared to SFT, RLSF gave +8.0% EM and +58.3% Validity for Molecule Generation (MG), +11.8% EM and +2.1% Validity for Forward Synthesis (FS), and +12.2% EM and +3.2% Validity for Retrosynthesis (RS). The paper states RLSF-tuned galactica-1.3b outperformed GPT-4 on these chemistry tasks despite being ~1000× smaller.",
            "comparison_to_baselines": "Benchmarked vs (1) zero-shot GPT-4 (closed-source) which had high validity but low EM (e.g., GPT-4 zero-shot: MG Validity 90% with EM 3%; FS Validity 93.1% with EM 0.4%; RS Validity 88.2% with EM 0.8%), (2) supervised fine-tuning (SFT), and (3) RL with Boolean scalar feedback. RLSF on galactica-1.3b improved EM and Validity substantially over SFT and RL-Boolean and is reported to surpass GPT-4 in exact-match metrics on the selected datasets.",
            "limitations_challenges": "Key limitations noted: (a) Exact Match is a strict metric and multiple chemically-correct outputs exist per input, so EM can underrepresent chemically valid generation, (b) symbolic feedback depends on RDKit analysis — RDKit flags valence/syntax errors but cannot fully capture downstream application utility, (c) the study focuses on fine-tuning only (not inference-time multi-step symbolic interaction), (d) for problems with many valid outputs, supervised memorization can mask generalization, and (e) while RDKit latency is low (~0.003s per generation reported), symbolic-tool overhead and dataset curation remain practical considerations.",
            "uuid": "e3398.1",
            "source_info": {
                "paper_title": "RLSF: Fine-tuning LLMs via Symbolic Feedback",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 (baseline)",
            "name_full": "gpt-4-turbo (GPT-4)",
            "brief_description": "A closed-source ultra-large LLM from OpenAI used as a strong baseline in zero-shot evaluations for the three chemistry tasks; reported to produce high SMILES validity but low exact-match rates to gold outputs in these datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo)",
            "model_description": "Closed-source very large LLM (paper cites an approximate parameter order-of-magnitude of ~1.76 trillion as a rumor); accessed via API and evaluated in zero-shot for chemistry SMILES tasks. Used only as a baseline comparator (no fine-tuning reported in this paper).",
            "generation_method": "Zero-shot SMILES generation and reaction prediction from natural language prompts; outputs were assessed by RDKit for validity and compared to gold outputs for EM/FTS.",
            "application_domain": "Chemistry: molecule generation, forward synthesis, retrosynthesis (used as a baseline to compare to RLSF-fine-tuned smaller open models).",
            "evaluation_metrics": "Exact Match (EM), Fingerprint Tanimoto Similarity (FTS), and Validity (SMILES grammar and valence checks via RDKit).",
            "results_summary": "Zero-shot GPT-4 produced high validity but very low exact-match rates on the chosen test splits: for Molecule Generation (MG) GPT-4 valid SMILES ~90% with EM 3%; for Forward Synthesis (FS) Validity ~93.1% with EM 0.4%; for Retrosynthesis (RS) Validity ~88.2% with EM 0.8%. RLSF-fine-tuned smaller models (e.g., galactica-1.3b) achieved higher EM and improved validity in reported experiments, leading the authors to state that RLSF-tuned smaller models outperformed GPT-4 on the evaluated EM metrics.",
            "comparison_to_baselines": "GPT-4 served as a closed-source baseline; while GPT-4 had better zero-shot validity than unfine-tuned open models, RLSF fine-tuning on smaller open models yielded higher exact-match performance on the curated datasets used in this work.",
            "limitations_challenges": "Although GPT-4 produces chemically valid SMILES at high rates, it achieved low Exact Match (EM) to the dataset gold outputs — likely because many valid alternative SMILES/reaction outcomes exist and EM is strict; the paper cautions that EM can understate useful chemical generation. Also GPT-4 was only evaluated zero-shot (no fine-tuning applied), so direct comparisons to fine-tuned open models have caveats.",
            "uuid": "e3398.2",
            "source_info": {
                "paper_title": "RLSF: Fine-tuning LLMs via Symbolic Feedback",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models.",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset.",
            "rating": 2,
            "sanitized_title": "llasmol_advancing_large_language_models_for_chemistry_with_a_largescale_comprehensive_highquality_instruction_tuning_dataset"
        },
        {
            "paper_title": "Translation between molecules and natural language.",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy.",
            "rating": 2,
            "sanitized_title": "predicting_retrosynthetic_pathways_using_transformerbased_models_and_a_hypergraph_exploration_strategy"
        },
        {
            "paper_title": "A review of transformers in drug discovery and beyond.",
            "rating": 1,
            "sanitized_title": "a_review_of_transformers_in_drug_discovery_and_beyond"
        }
    ],
    "cost": 0.013713749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RLSF: Fine-tuning LLMs via Symbolic Feedback
27 Jun 2025</p>
<p>Piyush Jha piyush.jha@gatech.edu 
Georgia Institute of Technology
USA</p>
<p>Prithwish Jana 
Georgia Institute of Technology
USA</p>
<p>Pranavkrishna Suresh 
Georgia Institute of Technology
USA</p>
<p>Arnav Arora 
Georgia Institute of Technology
USA</p>
<p>Vijay Ganesh 
Georgia Institute of Technology
USA</p>
<p>RLSF: Fine-tuning LLMs via Symbolic Feedback
27 Jun 202503FC8F037650108AC3C8BA8D6CE1FA83arXiv:2405.16661v3[cs.CL]
Large Language Models (LLMs) have transformed AI but often struggle with tasks that require domain-specific reasoning and logical alignment.Traditional fine-tuning methods do not leverage the vast amount of symbolic domain-knowledge available to us via symbolic reasoning tools (e.g., provers), and are further limited by sparse rewards and unreliable reward models.We introduce Reinforcement Learning via Symbolic Feedback (RLSF), a novel fine-tuning paradigm where symbolic reasoning tools (e.g., solvers, provers, and algebra systems) provide finegrained feedback to LLMs.RLSF uses poly-sized certificates (e.g., proofs) generated by symbolic tools to identify and correct errors in model outputs, offering token-level guidance without requiring differentiable reasoning systems.This paradigm bridges the gap between symbolic reasoning and LLM fine-tuning, enabling precise alignment with domain-specific constraints while addressing key limitations of traditional reward signals.Via extensive evaluations, we show that our RLSF-based finetuning of LLMs outperforms traditional approaches on five different applications (that have some associated logical or domain constraints), namely, program synthesis from natural language pseudocode to programming language (+31.43% in functional correctness for Google's CodeGemma-2b compared to supervised fine-tuning, +17.01% in functional correctness compared to GPT-3.5 -100× larger), three chemistry tasks (+5.5% exact match for molecule generation, +19.4% exact match for forward synthesis, +33.7% exact match for retrosynthesis, using Meta's Galactica-1.3b,compared to GPT-4 -1000× larger), and solving the Game of 24 (+25% success rate using Meta's Llama2-7b compared to traditional methods, and +7% success rate compared to GPT-3.5 -25× larger).A key takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs) have had a dramatic impact on many sub-fields of AI [6].Tasks that seemed impossible just a few years ago, are now routinely solved by LLMs.Examples include language translation [39,34], text-to-image generation [58], coding assistants [29], and open-ended text generation [2].</p>
<p>Despite their impressive performance, these models often struggle with tasks that require reasoning or formal domain knowledge [32,44].This limitation has sparked a growing interest in exploring methods that can better align LLM outputs with logical or domain constraints, particularly through the incorporation of corrective feedback loops between learning and reasoning processes [14,25].The image depicts two distinct fine-tuning paradigms.(Top) RLHF operates within an environment governed by a black-box reward model, typically offering scalar feedback.(Bottom) By contrast, the environment in RLSF leverages sound symbolic reasoning tools and also provides tokenlevel feedback that is, in turn, based on poly-sized certificates produced by these symbolic tools.</p>
<p>For example, Kambhampati et al. [25] state that "LLMs cannot plan themselves but can play a variety of constructive roles in solving planning tasks-especially as approximate knowledge sources and candidate plan generators in the so-called LLM-Modulo Frameworks in conjunction with external sound model-based verifiers."</p>
<p>The concept of incorporating reasoning tools into machine learning in general, and LLMs in particular, is rooted in the idea of combining the strengths of these two sub-fields of AI [18,14,25,4].While LLMs excel at capturing statistical patterns and generating fluent text, they can fail to perform sound reasoning and generate text that is logically coherent.In fact, the logical or code errors introduced by LLMs in the objects they generate can be very subtle, and not immediately obvious upon inspection.This motivates the need to use reasoning and verification tools at different stages of an LLM's life cycle (from data curation, training, fine-tuning, and inference).For instance, using program analysis tools during inference [3], and integrating solvers into neural network layers [52] or during gradient descent [5] have shown promising results in terms of convergence and data utilization.</p>
<p>By contrast to LLMs, symbolic reasoning systems, such as theorem provers and constraint solvers 1 , are adept at handling sound log-ical reasoning, perform symbolic mathematical operations and maintain coherence, but they do not seem to possess the impressive generative capabilities of LLMs.By integrating these two approaches, a hybrid system can be created that can leverage the strengths of both paradigms, potentially leading to more robust and capable AI systems.</p>
<p>One popular approach to fine-tuning LLMs is Reinforcement Learning from Human Feedback (RLHF) [37,45], which relies on manually collecting correct/incorrect cases and creating an approximate (possibly unsound) black-box reward model.Such an approach can be expensive, error-prone, and may not accurately capture the nuances of a reasoning task.Moreover, the reward signal thus generated can be sparse and scalar in nature.Such sparse reward signals can fall short of fully capturing those aspects of an LLM-generated object that may be incorrect with respect to a well-defined specification (or inconsistent with domain knowledge).</p>
<p>To address these challenges, we propose a new fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF) that is designed to improve the performance of LLMs compared to traditional methods in complex reasoning tasks.Figure 1 highlights the differences between RLHF and RLSF.In the RLSF setting, the LLM is considered as the RL agent to be finetuned, while the environment is allowed access to reasoning tools, that can generate poly-sized certificates of their analyses.</p>
<p>In the forward direction (LLM to environment), formal objects generated (such as programs, proofs, molecules, or theories) by the LLM are fed to the RLSF environment, which leverages reasoning tools to perform analysis or verification of such objects against specifications or domain knowledge.In the reverse direction (environment to LLM), any certificate (that identifies what is wrong with the LLM-generated object) produced by symbolic tools is used as part of a reward function to provide corrective feedback to the LLM.Leveraging symbolic tools to generate poly-sized certificates2 , which provide sound, token-level feedback to LLMs, eliminates the need for manual preference data collection and addresses limitations of traditional reward models.Moreover, our RLSF approach does not require the reasoning systems to be differentiable, unlike traditional neurosymbolic RL approaches [18], adding to its versatility.</p>
<p>Contributions.</p>
<p>• Reinforcement Learning via Symbolic Feedback: We present the RLSF paradigm and evaluate the effectiveness of RLSF-based fine-tuning methodologies on LLMs across five distinct tasks that present unique challenges regarding their domain-specific understanding.• Translation of natural language pseudo-code to C++ code:</p>
<p>Our results show a significant improvement over the supervised fine-tuning (SFT) approach for widely-used code LLMs (such as stable-code-instruct-3b [38], deepseek-coder-1.3b-base[16] and Google's code-gemma-2b [10]).For example, our RLSF-tuned code-gemma-2b shows an improvement of +52.64% in compilation accuracy and +31.43% in functional correctness accuracy over SFT and also achieves superior results compared to GPT-3.5 (∼100× larger) [2].(Section 4)</p>
<p>• Three chemistry tasks (molecule generation, forward synthesis, retrosynthesis): For the three chemistry tasks, mistral-7b [23] from Mistral AI and galactica-1.3b[46] from Meta AI show an improvement upto 13% in exact match and 58% in validity compared to traditional approaches, and upto 33% improvement in exact match compared to GPT-4 (∼1000× larger).(Section 5) • Game of 24: We observe significant improvement using RLSF on popular LLMs -Google's gemma-2b-it [15] and Meta's llama2-7b-chat [47] with +15% and +25% success respectively, compared to traditional methods.Notably, post RLSF finetuning, llama2-7b-chat also outperforms (+7%) GPT-3.5 (∼25× larger).This underscores the importance of RLSF finetuning that facilitates relatively smaller LLMs to significantly outperform models, such as ChatGPT, which are orders of magnitude larger.(Section 6)</p>
<p>Related Work</p>
<p>The idea of integrating symbolic feedback into reinforcement learning for RLSF has philosophical origins in the Learning Rate Based (LRB) branching heuristic for SAT solvers [28].In their work, they model the variable selection problem as an online multi-armed bandit, a special case of reinforcement learning, to learn branching variables such that the learning rate of the solver is maximized.LRB uses a structured feedback signal based on the solver's performance and conflict analysis, guiding the optimization of variable selection.This is one of the key inspirations behind the RLSF approach, where we provide feedback to LLMs via poly-sized certificates generated by symbolic reasoning tools.</p>
<p>Neurosymbolic Reinforcement Learning (NRL).We refer the reader to the Compendium of Neurosymbolic AI for a rich and recent literature survey on combinations of learning and reasoning systems [18].There has been considerable work in NRL [1].However, all the NRL work that we are aware of focuses on combining symbolic reasoning tools with the RL agent, often requiring reasoning systems to be differentiable and also limiting the agent's ability to handle large symbolic representations.In contrast, RLSF integrates symbolic tools with the environment and eliminates the need for differentiability.Further, in the RLSF paradigm, a key innovation is that we leverage the symbolic tools in the environment to generate poly-sized certificates and provide a feedback signal to the RL agent.This allows for a modular framework with more expressive and interpretable feedback during training.</p>
<p>Combinations of LLMs with Symbolic Tools.Previous research has explored the integration of program analysis tools during inference [3].Existing efforts have often relied on binary or scalar signals for utilizing symbolic feedback during training or finetuning [7,21,22].Recent methods like DeepSeek-R1 [17] have also demonstrated the promise of symbolic feedback in RL, but similarly focus on scalar feedback.In contrast, the RLSF paradigm leverages richer token-level (dense) symbolic feedback, thus enabling significantly improved fine-tuning performance as it enables more detailed and fine-grained correction, pinpointing specific areas in the output that need improvement.This allows the model to learn more effectively, as it receives clearer, more precise guidance.Kambhampati et al. [25] introduced the concept of LLM-Modulo Frameworks, which proposes using external symbolic reasoning tools during inference.In contrast, our work focuses on the fine-tuning phase and how external symbolic reasoning tools can be effectively incorporated into the RLSF paradigm to guide the model during fine-tuning.for batchi in D do 3:</p>
<p>responsei ∼ Model (batchi )
4:
certi ← SymbolicReasoner (batchi , responsei )
5:
vectori ← RewardFunc(certi )</p>
<p>6:
Model ′ ← ppostep(Model , batchi , responsei , vectori ) 7:
Model ← Model ′ 8:</p>
<p>end for 9: end for LLM-based Feedback.Another line of work that has explored using LLMs as part of a corrective feedback loop [42,31] where the feedback is usually provided as in-context (not RL), under the assumption that LLMs are better at verification compared to generation.However, recent studies have challenged this assumption, suggesting that LLMs may not be as effective at verification/self-critiquing tasks as previously thought [32,44].By contrast, in our work, we use sound symbolic systems to provide corrective feedback and do so via an RL approach.</p>
<p>3 Reinforcement Learning via Symbolic Feedback (RLSF)</p>
<p>In this section, we introduce the Reinforcement Learning via Symbolic Feedback (RLSF) algorithm.The algorithm incorporates feedback generated by reasoning or domain knowledge tools, thereby addressing the limitations of traditional reward-based methods.</p>
<p>The RLSF algorithm, outlined in Algorithm 1, fine-tunes a pretrained language model Model using reinforcement learning (RL) with the help of the certificate provided by a symbolic reasoning tool SymbolicReasoner .The framework leverages a reward function (RewardFunc) to compute vector (token-level) feedback (vector i) based on the certificate generated by the symbolic reasoning tool.This feedback aligns with the shape of the response generated by the language model, facilitating fine-grained adjustments during finetuning.The algorithm operates over a specified number of epochs N epochs , iterating through a dataset D. Inputs and Outputs.The algorithm takes as input the pre-trained model (RL agent to be fine-tuned) Model , the symbolic reasoning tool SymbolicReasoner and the reward function RewardFunc (to be used as the RL environment), and the dataset D that consists of input prompts for Model .The output is a fine-tuned model Model ′ .The algorithm performs the following steps: Epoch Iteration.For each epoch from 1 to N epochs , the algorithm iterates through D. Batch Processing.For each batch batchi in the dataset D, the algorithm performs the following: • Response generation (response i ): The Model generates a response response i given the input prompt batchi.</p>
<p>• Certificate computation (certi): The symbolic reasoning tool</p>
<p>SymbolicReasoner computes a certificate certi corresponding to the response response i .The certificate includes fine-grained error/non-error messages extracted from the symbolic analysis of the prompt-response pair.</p>
<p>• Token-level (dense) feedback computation (vector i): The reward function (RewardFunc) calculates a vector feedback (vector i) based on the certificate certi.The reward function processes this certificate to generate token-level vector feedback.This feedback provides detailed guidance to the language model during finetuning, facilitating precise adjustments.The vector feedback has the same shape as the response tokens (computed using the tokenizer provided by Model).Figures for respective tasks in the main text and appendix give a concrete example of such vector feedback for the different tasks.• Model update (Model ′ ): The Model is updated to Model ′ using the Proximal Policy Optimization (PPO) algorithm, using the input prompt batchi, response response i , and certificate certi.Generalizing RLSF to different reasoning tasks.Our RLSF paradigm can be applied to any reasoning task, provided that: (a) the final output is in a formal language, and (b) a SymbolicReasoner can furnish segment-wise feedback based on a chosen delimiter.In Algorithm 1, we fine-tune an LLM for a specific reasoning task where the output is expressed in a formal language.Since each responsei must conform to a formal language F , logical delimiters are used to segment responsei into lines (using \n), words (using spaces), characters, or parser-tokens based on the grammar of F .When validated by the SymbolicReasoner , certi verifies each segment.Then RewardFunc maps the segment-level certificate to vectori , providing token-level feedback for responsei .In Sections 4, 5 and 6, we demonstrate the RLSF paradigm across five reasoning tasks from different domains.The implementation details of the RLSF algorithm are described in Appendix D. Overhead of symbolic feedback.One concern with symbolic feedback is the potential training overhead caused by querying external tools.We observe that the latency varies across domains: for chemistry and math tasks, symbolic feedback is generated rapidly.For example, using RDKit in retrosynthesis incurs just 0.003s per generation, resulting in under 5 minutes of overhead per 12-hour training iteration.Similarly, SymPy-based arithmetic checks take 0.002s on average.Code generation tasks, which involve invoking g++, are comparatively slower (≈0.5s per generation).However, by asynchronously parallelizing these symbolic queries across batches using Popen, we keep the cumulative overhead manageable (e.g., ≈1 hour per 24-hour iteration for code synthesis).</p>
<p>Reasoning Task A: Natural Language</p>
<p>Pseudo-code to C++ Code Automated code synthesis from natural language (NL) descriptions is a crucial task in software engineering that has garnered considerable attention.Recent efforts, such as [48], have explored using LLMs for this.We evaluate how our RLSF paradigm improves LLMbased translation of an NL pseudo-code into a C++ code.To be correct, the code must be both (a) syntactically correct per the g++ compiler and (b) functionally correct (or equivalent3 ) w.r.t. a test suite.</p>
<p>Benchmark and RLSF setup</p>
<p>For training and evaluating the LLM, we utilize the SPoC dataset [26], which includes 16,326 accepted C++ solutions for 677
&lt; L N &gt; \ n &lt; L 1 &gt; \ n <BOS> <EOS> &lt; L k &gt; \ n &lt; L j &gt; \ n
Test Suite (TS)</p>
<p>Figure 2: RLSF for translation of NL pseudo-code to code: Given the generated C++ code (with N lines), the symbolic environment uses the g++ compiler to detect erroneous lines (E) and compute the pass rate r from the test suite, providing fine-grained symbolic feedback for fine-tuning the LLM.</p>
<p>problems from a competitive programming website [9].However, the codes in the SPoC dataset lack #include preprocessor directives and using namespace lines.As such, during fine-tuning, the LLM is not trained to generate such lines.To resolve this, we prepend a reasonably comprehensive and fixed set of 21 preprocessor directives along with a using namespace std; line to each LLM-generated code snippet before calculating CompAcc, FCorrAcc, and the token-level feedback.This ensures that the LLMgenerated code compiles correctly with the g++ compiler.</p>
<p>In the supervised learning setup, we assume a training dataset D = {(pc i , ci, TS i)}, consisting of pseudo-code pc i , gold-standard code ci, and test-suite TS i.During fine-tuning, the LLM receives pc i as input (with prompt in Figure 2 and generates code ci, which may contain syntactic errors.This code is passed through the compiler (here, g++) to identify lines with errors (E).If ci compiles, it is tested on TS i to compute the pass rate (r ∈ [0, 1]).If it does not compile, r = 0. Our token-level feedback assigns a high reward p = 1 + r to tokens in syntactically correct lines (positive case) and a low reward n = 0 to tokens in erroneous lines (negative case).</p>
<p>Our setup features a gradual progression of rewards and symbolic fine-grained feedback.Tokens in erroneous lines receive a reward of 0, while those in syntactically correct lines of non-compiling code get a reward of 1 (since p = 1 + r and r = 0).Tokens in a compiling code receive rewards p ∈ [1, 2], based on the value of r.The special tokens <BOS> and <EOS> get a reward of p if ci compiles, and n otherwise.This token-level feedback fine-tunes the LLM through RL using PPO (Algorithm 1).</p>
<p>Evaluation Metrics</p>
<p>We use three metrics to evaluate pseudo-code to C++ code translation with LLMs.CompAcc and FCorrAcc provide more nuanced insights, while Pass@1 is a stricter pass/fail metric.In Section 4.4, we present performance using all three metrics, focusing on the first two for comparison of methods.</p>
<p>Compilation Accuracy (CompAcc).The percentage of generated C++ codes that are syntactically correct, indicating that it compiles without errors using a g++ compiler.Functional Correctness Accuracy (FCorrAcc).The percentage of test cases demonstrating the expected input-output behavior for each generated C++ code, averaged across all the generated codes.If a generated code contains syntactical errors, FCorrAcc is zero for that code.Pass@1.The percentage of generated C++ codes generated on the first attempt by the LLM that is syntactically correct and passes all test cases in the problem's test suite, with each code evaluated as true if it passes all tests and false otherwise.</p>
<p>Comparative Models Used</p>
<p>We perform RLSF fine-tuning on three recent open-source code LLMs that we obtain from [20]: code-gemma-2b from Google, stable-code-instruct-3b from StabilityAI, and deepseek-coder-1.3b-basefrom DeepSeekAI.We also compare results with the closed-source gpt-3.5-turbo-0301model i.e., GPT-3.5 (ChatGPT), that we access via the API of [36].For reproducibility, temperature and top_p are set to 0. The three open-source models have 2 billion, 2.7 billion, and 1.3 billion parameters, respectively.In comparison, GPT-3.5 is rumored to have around 175 billion parameters -about 100 times more.</p>
<p>Results and Ablation Study</p>
<p>As shown in Table 1, we first evaluate the LLMs in a zeroshot setting.GPT-3.5 generates 29.13% compilable C++ code, with 24.29% of test cases showing correct inputoutput behavior.In contrast, none of the open-source models (code-gemma-2b, stable-code-instruct-3b, and deepseek-coder-1.3b-base)produce a single compilable or functionally correct code.Next, we train the open-source models by supervised fine-tuning (SFT) to minimize the cross-entropy loss between the generated and gold-standard C++ code.After SFT, code-gemma-2b and stable-code-instruct-3b achieve about 12% CompAcc and 10% FCorrAcc, while deepseek-coder-1.3b-basereaches only around 2% in both metrics.</p>
<p>We further fine-tune the SFT-tuned LLMs through RL with Boolean feedback, where models receive a binary scalar reward (0 or 1) based on whether the generated code com- They also surpass GPT-3.5 (ChatGPT), despite using 100 times fewer parameters.Additional results comparing Boolean compiler and success feedback in RL-based fine-tuning can be found in the Appendix A.</p>
<p>5 Reasoning Tasks B: Chemistry (Molecule Generation, Forward Synthesis and Retrosynthesis)</p>
<p>Chemistry tasks, such as molecule generation, forward synthesis, and retrosynthesis, are critical benchmarks for assessing the capabilities of LLMs in real-world scientific applications [59,8].These tasks require models to navigate the intricate rules of chemical structures and reactions, making them excellent challenges to evaluate domainspecific understanding.Leveraging LLMs in chemistry has the potential to significantly accelerate fields like drug discovery and materials science, enabling faster innovation and discovery [24].</p>
<p>Benchmark and RLSF setup</p>
<p>We focus on three tasks, namely, Molecular Generation (MG), Forward Synthesis (FS), and Retrosynthesis (RS).MG involves generating molecules given a description in natural language.FS involves predicting the product of a chemical reaction given specific reactants and reagents.RS focuses on identifying the reactants necessary to produce a specific target product.All output molecules are generated in the SMILES format [53], a widely used method for encoding molecular structures as a sequence of symbols, making it a popular choice for LLM-based chemistry models [57].We use a high-quality version of popular Mol-Instructions, USPTO-full, and ChEBI-20 datasets from LlaSMol [57], ensuring the removal of chemically invalid SMILES and inaccurate information.We focus on a subset of the dataset where multiple outputs are possible for a given input, as this poses a significant challenge for LLMs.In such cases, the model must better capture the syntax and semantics of a given task, rather than relying on simple input-output mapping through supervised fine-tuning (SFT).This setup pushes the model beyond memorization, testing its ability to generalize, making it a key evaluation of its capabilities.We ensure that same input samples are not shared across dataset splits for the same and also across the three tasks.More details can be found in Appendix B.</p>
<p>For the RLSF feedback, we handle both syntax and semantic errors.In our chemical modeling tasks, we use RDKit [40], a widely adopted cheminformatics toolkit, as the symbolic feedback generator.RDKit analyzes model outputs, such as SMILES strings, and generates error logs that we transform into symbolic token-level rewards whenever applicable.For instance, consider the modelgenerated SMILES string C(C)(C)N(C)(C)(C).RDKit identifies an issue with the nitrogen atom, flagging it for violating valence rules.Specifically, RDKit's parser outputs the error: "Explicit valence for atom # 3 N, 4, is greater than permitted."This indicates that the nitrogen atom (N, indexed as atom 3) has a valence of 4, which exceeds the permissible limit for nitrogen.To correct this error, two possible modifications can be applied: either adding a positive charge to nitrogen to form a quaternary ammonium ion (e.g., C(C)(C)N+(C)(C)) or reducing the number of bonds to nitrogen by trimming one methyl group, resulting in a valid structure like C(C)(C)N(C)(C).In this case, erroneous tokens include N (penalized for exceeding valence limits) and the parenthetical C groups following N (penalized as contributors to the invalid valence state).Figure 3 shows an example for the MG task.</p>
<p>In addition to valence issues, RDKit helps detect a wide range of errors, including hallucinated (non-existent) atoms, syntax errors (e.g., invalid characters, unbalanced parentheses, or unclosed rings), and semantic violations, such as the introduction of elements that violate conservation laws.RDKit's consistent error format allows for straightforward parsing and categorization of issues, enabling targeted feedback.For example, in FS, we ensure adherence to the first law of chemistry by penalizing outputs that introduce atoms not present in the reactants.If the reactants contain hydrogen and carbon but the product introduces nitrogen, this discrepancy triggers a penalty.Similarly, for molecule generation, we verify the presence of specified functional groups and penalize outputs that omit them.The symbolic token-level feedback mechanism ensures that the model learns precise corrections by addressing both syntax and semantic errors effectively.By integrating detailed checks and aligning feedback with domain-specific requirements, our approach significantly enhances the model's ability to generate accurate, chemically valid outputs.</p>
<p>Evaluation metrics</p>
<p>Three main metrics are used to assess performance on the FS, RS and MG tasks: Exact Match (EM), Fingerprint Tanimoto Similarity (FTS), and Validity (Valid).EM measures the proportion of predicted results that exactly match the gold standards.FTS quantifies structural similarities between molecules using Tanimoto similarities of their Morgan fingerprints [35].Validity assesses the ratio Figure 3: RLSF for one of the chemistry tasks -Molecule Generation: In this illustration, the symbolic environment utilizes RDKit to generate a token-level reward vector as feedback based on the presence or absence of any syntactical errors.Moreover, for the semantic errors, we again use RDKit to check for the presence of the required functional groups mentioned in the input natural language description and penalize the entire generated molecule if it lacks the required functional groups.Each element in the reward vector corresponds to a token in the response, where erroneous tokens are penalized with a value of 0 and correct ones are assigned 1.The last element of the reward vector (corresponding to the <EOS> token) is 1 only if the entire response is correct, otherwise, it is 0. of valid predictions following SMILES grammar and the chemical valence rules [57].These metrics are commonly used in the field of cheminformatics and molecular prediction [41,8,57].</p>
<p>Comparative Models Used</p>
<p>We conduct RLSF fine-tuning on two recent open-source code LLMs that we obtain from [20]: mistral-7b [23] from Mistral AI and galactica-1.3b[46] from Meta AI.We also compare results with the closed-source gpt-4-turbo model i.e., GPT-4 (Chat-GPT), that we access via the API of [36].The two open-source models have 7 billion and 1.3 billion parameters, respectively.In comparison, GPT-4 is rumored to have around 1.76 trillion parametersabout 1000 times more.</p>
<p>Results and Ablation Study</p>
<p>As shown in Table 2, we first evaluated the LLMs in a zero-shot setting.GPT-4 generated valid SMILES for 90% of the MG inputs but achieved only 3% exact match (EM) with the gold outputs.For the FS task, GPT-4 produced 93.1% valid SMILES and only 0.4% EM, while in the RS task, it generated 88.2% valid SMILES with just 0.8% EM.In contrast, none of the open-source models achieved any EM in the zero-shot setting.Next, we trained two open-source models using supervised finetuning (SFT), which improved their performance in terms of EM, FTS, and valid SMILES generation.Following SFT, we applied RL with Boolean feedback, where the model received a binary reward (0 or 1) based on whether the generated output adhered to task specifications as described in Section 5.1.This further improved performance: for mistral-7b, we observed a +2.9% increase in EM and a +10.9% boost in validity for the MG task, +3.2% EM and +1.2% validity for FS, and +5.2% EM with no change in validity for RS.Similarly, for galactica-1.3b,we recorded a +2.2% increase in EM and +52.3% improvement in validity for MG, +3.7% EM and +1.1% validity for FS, and +3.8% EM with +2.7% validity for RS, compared to the SFT-tuned models.</p>
<p>Moreover, our RLSF with token-level feedback approach on the SFT-tuned LLMs, achieved the best results in terms of EM, FTS, and validity.For example, RLSF on galactica-1.3bresulted in +8% EM and +58.3% validity for MG, +11.8% EM and +2.1% validity for FS, and +12.2% EM and +3.2% validity for RS, compared to SFT-tuned models.We attribute this superior performance to tokenlevel feedback, which allows for more fine-grained corrections than Boolean feedback, leading to significant improvements.Note that RLSF-tuned models outperformed GPT-4 despite using 1000 times fewer parameters, as shown in Table 2. 6 Reasoning Task C: Game of 24 using Tree of Thoughts (ToT)</p>
<p>The Game of 24 is a well-known benchmark that involves basic arithmetic operations such as addition, subtraction, multiplication, and division aimed at testing the mathematical capabilities of LLMs.Briefly, the idea behind the Game of 24 is as follows: given 4 numbers and basic arithmetic operations, obtain the target number 24.We refer the reader to the paper on Tree of Thoughts (ToT) by [56] and Appendix C for more details.</p>
<p>Benchmark and RLSF setup</p>
<p>Similar to ToT [56], we collect data from 4nums.com, a website hosting mathematical games, selecting 1,362 games sorted by human solving time from easy to hard.For testing, we use the same games as ToT, indexed 901-1,000.Success in each task is defined as producing a valid equation that equals 24 while using each input number exactly once.For the RLSF fine-tuning phase, we use games indexed 800-900.Fine-tuning occurs during the "propose prompt" steps (using prompt styles from ToT).Pairs of "propose prompts" and LLM responses are periodically evaluated using SymPy [33] to identify syntax and semantic errors.This feedback is used to finetune the LLM with Proximal Policy Optimization (PPO) following Algorithm 1.For more details on different error categories and converting symbolic feedback into fine-grained signals for fine-tuning, refer to Appendix C.</p>
<p>Comparative Methods and Models Used</p>
<p>We incorporate the benchmarks previously used by ToT, i.e., standard Input-Output (IO) prompting, Chain-of-Thought (CoT) prompting, and ToT prompting.IO prompting uses five in-context examples, while CoT includes three intermediate equations for these in-context examples.Both IO and CoT prompting are sampled 100 times per game for average performance assessment.To show the improvement due to symbolic fine-grained token-level feedback, we perform an ablation study where we compare the scalar binary and token-level versions of feedback for the RL fine-tuning.After RL fine-tuning, we evaluate the performance on the test set using ToT with the updated LLM.We perform RLSF fine-tuning on two smaller open-source LLMs (gemma-2b-it [15] and llama2-7b-chat [47]) that we obtain from [20] and also compare against two closed-source models GPT-3.5 and GPT-4 [2].</p>
<p>Results</p>
<p>We conduct a comparative analysis of different methods applied across various LLMs to tackle the Game of 24 task (Table 3).As observed by [56], the Tree of Thoughts (ToT) prompt method emerges as the most successful approach for both closed-source models GPT-3.5 and GPT-4, achieving success rates of 19% and 69%, respectively.This performance surpasses that of both the IO and CoT prompt methods.However, gemma-2b-it and llama2-7b-chat exhibit poor performance across all the three prompting methods.</p>
<p>We explore the use of Boolean scalar feedback for RL fine-tuning, where the Computer Algebra System (CAS) provides binary feedback based on the correctness of the generated response.However, we observe no improvement with this feedback mechanism (Table 3).Consequently, we transition to a symbolic token-level feedback approach using RLSF, where the symbolic environment provides tokenlevel feedback (Figure in Appendix C), resulting in a significant improvement in performance.Specifically, after employing RLSF finetuning, gemma-2b-it demonstrates a 15% increase in success rate, while llama2-7b-chat exhibits a 25% improvement in success rate compared to ToT prompting prior to RLSF fine-tuning.Notably, the 7-billion-parameter llama2-7b-chat outperforms (+7%) the 175-billion-parameter GPT-3.5 model, highlighting RLSF's effectiveness in enhancing model performance.However, GPT-4 demonstrates the best performance across all methods.We attribute this to its ultra-large-scale pre-training and architecture advancements.Looking ahead, future investigations can explore the application of RLSF on larger open-source LLMs.</p>
<p>Conclusions, Limitations, and Future Work</p>
<p>In this paper, we introduced RLSF, a fine-tuning paradigm that incorporates symbolic feedback into the fine-tuning process of LLMs.While we do not claim to improve general reasoning capabilities, RLSF leverages symbolic reasoning tools to improve downstream domain-specific tasks where syntax and semantics play a critical role.Our results show a significant improvement in all five tasks, over different traditional prompting and fine-tuning methods.Notably, the RLSF-tuned galactica-1.3bachieves superior results compared to GPT-4 (1000× larger) on the three chemistry tasks, RLSFtuned code-gemma-2b outperforms GPT-3.5 (100× larger) on the program synthesis task.Similarly, RLSF-tuned llama2-7b-chat also outperforms GPT-3.5 (25× larger) on Game of 24.Additionally, unlike traditional neuro-symbolic RL approaches, RLSF does not require differentiable reasoning systems, making it more versatile and practical.</p>
<p>Limitations and future work.This study demonstrates the initial potential of integrating symbolic feedback into LLM fine-tuning frameworks, with empirical improvements in specific domains such as program synthesis, chemistry and mathematical tasks.While we do not aim to enhance the overall reasoning capabilities of LLMs, our focus has been on developing a new fine-tuning paradigm that outperforms traditional methods within specific domains.In tasks where symbolic tools are slow or resource-intensive, DPO-style approaches with precomputed symbolic rewards may offer a practical alternative.Future research may explore theoretical guarantees, its impact across other reasoning tasks, and broader LLM reasoning capabilities.Lastly, our focus has been solely on fine-tuning, but we believe that combining RLSF with multi-step symbolic feedback during inference could further boost performance.</p>
<p>B Additional details for reasoning tasks B: Chemistry</p>
<p>B.1 Overview of the three chemistry tasks</p>
<p>Forward synthesis involves predicting the product of a chemical reaction based on given reactants and reagents.In computational chemistry, forward synthesis prediction allows for the simulation of chemical reactions, allowing chemists to plan experiments without conducting them physically.For example, given the reactants phenoxazine (NC1=CC=C2OCOC2=C1) and formaldehyde (O=CO), the model predicts the product as O=CNC1=CC=C2OCOC2=C1.</p>
<p>Retrosynthesis involves determining the reactants required to create a specific product.It is essential for planning synthetic pathways, especially for complex molecules.For instance, the product CC1=CC=C(N)N=C1N can be derived from the reactants CC(C#N)CCC#N and ammonia (NH3).</p>
<p>Molecular generation involves generating a molecule that meets specific requested chemical and biological properties in natural language.This process is widely utilized in drug discovery and materials science to create compounds with specific characteristics, such as the presence of certain functional groups, binding affinity, stability, or bioactivity.For example, a molecule described as "a red-colored pigment with antibiotic properties..." is generated as CCCCCC1=C(C)NC(/C=C2\N=C(C3=CC=CN3)C=C2OC)=C1.</p>
<p>B.2 Dataset split</p>
<p>We focus on three key tasks: Molecule Generation, Forward Synthesis, and Retrosynthesis, each of which presents the challenge of having multiple valid outputs for a given input.We take datapoints from existing datasets [30,12,13,57] where multiple outputs are possible for a given input, as this scenario presents a significant challenge for LLMs.In such cases, the model must not only generate syntactically valid outputs but also grasp domain-specific rules and context to accurately capture task details.Simple supervised fine-tuning (SFT), which directly maps inputs to outputs, often struggles with this complexity, relying more on memorization.By handling tasks with multiple potential outputs for a single input, the model is required to go beyond basic input-output mapping, forcing it to learn complex relationships within the data.This setup is crucial for testing the model's ability to generalize.</p>
<p>To ensure proper evaluation, we identify sample input-output pairs across tasks that correspond to the same molecules or reactions, and group these samples consistently in either the training, evaluation, or test set.In addition, samples with the same input but different outputs are handled with care.For instance, in the RS task, a single product can be synthesized from multiple sets of reactants, so we ensure that all samples with identical inputs remain in the same split to avoid data leakage.For the Molecule Generation task, we used 3367 training samples, 354 validation samples, and 933 test samples.The Forward Synthesis task consists of 10207 training samples, 1452   C Additional details for reasoning task C: Game of 24 using Tree of Thoughts (ToT)</p>
<p>C.1 Game of 24 using ToT</p>
<p>To solve the Game of 24 using ToT, the process involves decomposing the problem into three steps, each representing an intermediate equation.Starting with the given input numbers, the LLM is prompted to propose possible next steps (or "thoughts") using a "propose prompt".Similar to [56], we employ a breadth-first search (BFS) approach in ToT, maintaining the top 5 candidates at each step.Now, we prompt the LLM using the value prompt to evaluate the "thoughts."The score given by the LLM using the value prompt helps prune the "thoughts" generated by the "propose prompt."These two prompts are repeated, starting with the three remaining numbers (from all thoughts accepted after using the value prompt) to build the ToTs.This process is repeated until you arrive at the final equation that results in the number 24.</p>
<p>C.2 Converting the symbolic feedback to fine-grained signal</p>
<p>In Game of 24, we use a combination of SymPy and regex-based checks to validate model outputs and identify errors.SymPy performs syntax and semantic validation, ensuring the logical and mathematical correctness of the expressions, while regex is employed to verify numerical compliance with the game's rules.</p>
<p>Errors are classified into several categories for precise feedback.First, we detect instances where invalid numbers are used in the expression.For example, if the input numbers are (3,8,3,3) and the model generates 3+8-3+10=18, the use of 10 is flagged as an error because it is not part of the game input or an intermediate step.</p>
<p>Next, we verify the correctness of the equation using SymPy to ensure that the left-hand side equals the right-hand side of the equation.Discrepancies such as 3×(8+3)=27 are flagged as errors.Regexbased checks are also used to identify missing or extra numbers in the final expression.For example, if the input is [3,8,3,3] and the model outputs (3×8)+3=27, one of the 3s is missing.In contrast, if the model produces (3×8)+3+2, the presence of extraneous 2 is flagged.Additional syntax issues, such as unbalanced parentheses, missing operators, or division by zero, are also detected and flagged.</p>
<p>To provide fine-grained feedback, we perform token-level pinpointing whenever possible, assigning penalties to erroneous tokens.For example, in the expression (3×8)+3 3, SymPy raises a syntax error due to the missing operator between the two 3s, and regex flags both 3s as repeated without justification.This allows errors to be traced back to specific tokens, enabling targeted corrections.This detailed feedback ensures that the model learns from precise and actionable signals, improving its ability to generate accurate solutions in subsequent attempts.</p>
<p>D Implementation, TRL Library, and Hardware Details</p>
<p>The RLSF algorithm is implemented by modifying the Transformer Reinforcement Learning (TRL) library [49], a popular and comprehensive framework integrated with Huggingface transformers [54] designed for training transformer language models with reinforcement learning (RL).We use the Proximal Policy Optimization (PPO) algorithm, commonly used in Reinforcement Learning from Human Feedback (RLHF) [55,37,45], for fine-tuning the language model.However, TRL only allows for scalar reward signals during the RL fine-tuning process.We modify the library to support reward vec-tor (dense) signals, allowing fine-grained feedback at the token level.This enhancement enables RLSF to leverage symbolic feedback effectively during the RL process.We use LoRA [19] applied to all linear layers in the self-attention and FFN modules with lora_r and lora_alpha set to 16 during our fine-tuning.All our experiments were conducted on a high-performance CentOS V7 cluster equipped with Intel E5-2683 v4 Broadwell processors running at 2.10 GHz and 64 GiB of memory.We used 4 NVIDIA V100 GPUs for the tasks in NL2PL translation and Chemistry tasks, and 1 NVIDIA V100 GPU for the Game of 24 task.</p>
<p>Figure 1 :
1
Figure 1: Contrasting RLHF with RLSF: The image depicts two distinct fine-tuning paradigms.(Top) RLHF operates within an environment governed by a black-box reward model, typically offering scalar feedback.(Bottom) By contrast, the environment in RLSF leverages sound symbolic reasoning tools and also provides tokenlevel feedback that is, in turn, based on poly-sized certificates produced by these symbolic tools.</p>
<p>Output a C++ code for the following pseudo-code in NL. ###Pseudo-code in NL: p,...,p, ... , p,...,p, p] L k ∈ E L j ∈ {L 1, L 2,..., L N } ∖ E E = φ?..., n,...,n, ... , p,...,p, ..., n]</p>
<p>... 1, 0, ... 0, 1, ... , 0] len(feedback) == len(response_tokens)</p>
<p>6 + 4 =
4
10 (left: 1 4 10) 4 * 2 = 14 (left: 1 6 14) {...} [1, 1, ... , 0, 1, ... , 0, 1, ... , 0] len(feedback) == len(response_tokens)</p>
<p>Figure 4 :
4
Figure 4: RLSF for the Game of 24: In this illustration, the symbolic environment utilizes the Computer Algebra System (CAS) library SymPy to generate a token-level reward vector as feedback.Each element in the vector corresponds to a token in the response, where erroneous tokens are penalized with a value of 0 and correct ones are assigned 1.The last element of the reward vector (corresponding to the EOS token) is 1 only if the entire response is correct, otherwise, it is 0. validation samples, and 2908 test samples.The Retrosynthesis task has 77561 training samples, 2051 validation samples, and 4382 test samples.</p>
<p>[1,average, each program has 14.7 lines, ranging in[1, 457].They employed 59 crowd-workers from Amazon Mechanical Turk to write NL pseudocode for each line of C++ program, ensuring a line-level granularity of textual description.Each problem is accompanied by a test suite of multiple test cases, curated by the problem-setter.We evaluate on the TESTP partition of SPoC, consisting of 158 problems (∼23.34%)and 1,778 pairs of pseudo-code and C++ code.The remaining 14,548 pairs from 519 problems (∼76.66%)are used for training the LLMs.</p>
<p>Table 1 :
1
Natural Language Pseudo-code to Code Translation Results: Performance comparison of ZS (Zero-shot), SFT (Supervised fine-tuning), RL Boolean (SFT + RL with Boolean scalar f/b), RLSF (our method with token-level f/b) over different LLMs.and +17.78%, and FCorrAcc by +14.84%, +0.13%, and +6.17% for code-gemma-2b, stable-code-instruct-3b, and deepseek-coder-1.3b-base,respectively.Conversely, we use the proposed scheme of RLSF with token-level feedback to fine-tune the SFT-tuned LLMs.This yields even better results, improving CompAcc by +52.64%, +42.23%, and +36.73%, and FCorrAcc by +31.43%, +7.66%, and +23.99% for the same mod-
LLMConfigCompAcc (%) FCorrAcc (%) Pass@1 (%)GPT-3.5ZS29.1324.2920.98ZS0.000.000.00code-gemma-2bSFT11.319.879.00RL Boolean54.8924.7112.77RLSF63.9541.3028.80ZS0.000.000.00stable-code-3bSFT12.0410.789.96RL Boolean48.4310.919.22RLSF54.2718.4416.09ZS0.000.000.00deepseek-coder-1.3bSFT2.191.901.63RL Boolean19.978.073.88RLSF38.9225.8914.51piles -a common approach in various recent code genera-tion techniques [50, 11]. This improves CompAcc by +43.58%,+36.39%,
els compared to SFT.We believe RLSF's superior performance is due to its symbolic token-level feedback.It provides more granular rewards based on line-level syntactical correctness and overall test case pass rate, unlike Boolean feedback which only indicates if code compiles.As a result, RLSF-tuned models outperform those trained with Boolean feedback by +9.06%, +5.84%, and +18.95% in CompAcc, and +16.59%, +7.53%, and +17.82% in FCorrAcc.</p>
<p>Table 2 :
2
Chemistry Tasks Results: Performance comparison of ZS (Zero-shot), SFT (Supervised fine-tuning), RL Boolean (SFT + RL with Boolean scalar f/b), RLSF (our method with token-level f/b) over different LLMs for three chemistry tasks -Molecule Generation (MG), Forward synthesis (FS), and Retrosynthesis (RS).
Task &amp; Metric</p>
<p>Table 3 :
3
Game of 24 Results: Performance comparison of methods over different LLMs
LLMConfigurationSuccessIO prompt6%GPT-3.5CoT prompt3%ToT prompt19%IO prompt7.3%GPT-4CoT prompt4%ToT prompt69%IO prompt1%CoT prompt3%gemma-2b-itToT prompt2%RL Boolean1%RLSF17%IO prompt5%CoT prompt1%llama2-7b-chatToT prompt1%RL Boolean1%RLSF26%</p>
<p>Table 4 :
4
Additional Results for Natural Language Pseudo-code to Code Translation: Comparison of scalar feedback schemes and vectorized feedback in RLSF for fine-tuning LLMs.
LLMConfigurationCompAcc (%) FCorrAcc (%) Pass@1 (%)SFT + RL (with Boolean success f/b)50.1122.3010.29code-gemma-2b [10]SFT + RL (with Boolean compiler f/b)54.8924.7112.77SFT + RLSF (with token-level f/b)63.9541.3028.80SFT + RL (with Boolean success f/b)40.047.826.69stable-code-instruct-3b [38]SFT + RL (with Boolean compiler f/b)48.4310.919.22SFT + RLSF (with token-level f/b)54.2718.4416.09SFT + RL (with Boolean success f/b)19.126.923.43deepseek-coder-1.3b-base [16]SFT + RL (with Boolean compiler f/b)19.978.073.88SFT + RLSF (with token-level f/b)38.9225.8914.51Propose Prompt:PPOResponseComputer Algebra{examples}LanguageSystem (CAS)Input: 6 4 1 2ModelPossible next steps:(LLM)Symbolicfeedback
We define the term symbolic reasoning systems broadly to include solvers, provers, computer algebra systems, program analysis tools, knowledge bases, and simulators. The only requirement is that they analyze/solve inputs that are formally defined, and can produce poly-sized certificates of their analysis.
Examples of poly-sized certificates include unsatisfiability proofs, compiler feedback, equivalence testing, etc. Our approach is not limited to any one type of symbolic tool, as long as these certificates can be converted into appropriate rewards. It is possible that the problem addressed by these symbolic tools is NP-hard, and therefore, in general, we cannot always expect certificates that are polynomial in the input size. Having said that, many of these tools, such as compilers, computer algebra systems, or solvers, work well in practice.
Note that in general the task of determining whether a C++ code is functionally equivalent to pseudo-code (even when it is specified in formal logic) is undecidable. Hence, we consider a weaker functional equivalence property, namely, that a generated C++ code is deemed functionally correct if it passes all test cases corresponding to a given pseudo-code.
AppendixA Additional details for reasoning task A: Natural Language Pseudo-code to C++ CodeA.1 Additional ResultsMost state-of-the-art code generation LLMs[50,27,43,11,51]employing RL-based fine-tuning rely on either Boolean compiler feedback (e.g., +1 if the code compiles, −1 if it does not), Boolean success feedback (e.g., +x if all test cases pass, −y if at least one test case fails), or a combination of both.In our natural language pseudocode to code translation experiments, presented in main paper, our aim was to compare these conventional binary scalar reward-based methods with the proposed fine-grained vector reward in RLSF.For this comparison, we evaluated two types of scalar rewards: success feedback and compiler feedback, consistent with the binary reward schemes commonly used in state-of-the-art methods.Notably, success feedback provides an even sparser signal than compiler feedback.This is because the likelihood of LLM-generated code either (1) passing all test cases vs. failing at least one test case or (2) successfully compiling vs. failing to compile, results in a more balanced distribution in the second scenario.In other words, the disparity between success and failure cases is higher for success feedback compared to compiler feedback.As shown in Table4, LLMs fine-tuned with Boolean success feedback underperform compared to those fine-tuned with Boolean compiler feedback.The proposed vectorized feedback in RLSF enhances the binary scalar feedback schemes used in state-of-the-art methods in two key ways: (a) it refines the success reward by incorporating the number of test cases passed, moving beyond a binary outcome and, (b) it introduces a vectorized symbolic feedback mechanism that assigns lower rewards to tokens in code lines flagged by the compiler and higher rewards to tokens in unflagged lines.These innovations provide a more nuanced and effective fine-tuning paradigm for code generation LLMs, as demonstrated by our results.
Neurosymbolic reinforcement learning and planning: A survey. K Acharya, W Raza, C Dourado, A Velasquez, H H Song, IEEE Transactions on Artificial Intelligence. 2023</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Monitor-guided decoding of code lms with static analysis of repository context. L A Agrawal, A Kanade, N Goyal, S Lahiri, S Rajamani, Advances in Neural Information Processing Systems. 362024</p>
<p>From shallow to deep interactions between knowledge representation, reasoning and machine learning. K R Amel, arXiv:1912.066122019arXiv preprint</p>
<p>A solver+ gradient descent training algorithm for deep neural networks. D Ashok, V Nagisetty, C Srinivasa, V Ganesh, arXiv:2207.032642022arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Program synthesis using deduction-guided reinforcement learning. Y Chen, C Wang, O Bastani, I Dillig, Y Feng, International Conference on Computer Aided Verification. Springer2020</p>
<p>G 2 retro as a two-step graph generative models for retrosynthesis prediction. Z Chen, O R Ayinde, J R Fuchs, H Sun, X Ning, Communications Chemistry. 611022023</p>
<p>. Codeforces, 2023. codeforces.com. </p>
<p>Codegemma, Codegemma, Open code models based on gemma. 2024</p>
<p>S Dou, Y Liu, H Jia, L Xiong, E Zhou, J Shan, C Huang, W Shen, arXiv:2402.01391Improve Code Generation with Reinforcement Learning from Compiler Feedback. 2024arXiv preprint</p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Y Fang, X Liang, N Zhang, K Liu, R Huang, Z Chen, X Fan, H Chen, arXiv:2306.08018Mol-instructions: A large-scale biomolecular instruction dataset for large language models. 2023arXiv preprint</p>
<p>Machine learning and logic: a new frontier in artificial intelligence. V Ganesh, S A Seshia, S Jha, Formal Methods in System Design. 6032022</p>
<p>T Gemma-Team, C Mesnard, R Hardin, S Dadashi, S Bhupatiraju, L Pathak, M Sifre, M S Rivière, J Kale, Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Deepseek-coder: When the large language model meets programming-the rise of code intelligence. D Guo, Q Zhu, D Yang, Z Xie, K Dong, W Zhang, G Chen, X Bi, Y Wu, Y Li, arXiv:2401.141962024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Compendium of Neurosymbolic Artificial Intelligence. P Hitzler, M K Sarker, A Eberhart, 2023IOS Press369</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>. HuggingFace. The AI Community Building the Future. 2024</p>
<p>Cotran: An llm-based code translator using reinforcement learning with feedback from compiler and symbolic execution. P Jana, P Jha, H Ju, G Kishore, A Mahajan, V Ganesh, arXiv:2306.067552023arXiv preprint</p>
<p>Bertrlfuzzer: A bert and reinforcement learning based fuzzer (student abstract). P Jha, J Scott, J S Ganeshna, M Singh, V Ganesh, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>A review of transformers in drug discovery and beyond. J Jiang, L Chen, L Ke, B Dou, C Zhang, H Feng, Y Zhu, H Qiu, B Zhang, G Wei, Journal of Pharmaceutical Analysis. 1010812024</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. S Kambhampati, K Valmeekam, L Guan, K Stechly, M Verma, S Bhambri, L Saldyt, A Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>Spoc: Search-based pseudocode to code. S Kulal, P Pasupat, K Chandra, M Lee, O Padon, A Aiken, P S Liang, Advances in Neural Information Processing Systems. 201932</p>
<p>CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. H Le, Y Wang, A D Gotmare, S Savarese, S C H Hoi, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning rate based branching heuristic for sat solvers. J H Liang, V Ganesh, P Poupart, K Czarnecki, Theory and Applications of Satisfiability Testing-SAT 2016: 19th International Conference. Bordeaux, FranceSpringerJuly 5-8, 2016. 201619</p>
<p>A large-scale survey on the usability of ai programming assistants: Successes and challenges. J T Liang, C Yang, B A Myers, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>D Lowe, Chemical reactions from US patents. 1976-Sep2016), 6 2017</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, Advances in Neural Information Processing Systems. 202436</p>
<p>Champ: A competition-level dataset for fine-grained analyses of llms' mathematical reasoning capabilities. Y Mao, Y Kim, Y Zhou, arXiv:2401.069612024arXiv preprint</p>
<p>Sympy: symbolic computing in python. A Meurer, C P Smith, M Paprocki, O Čertík, S B Kirpichev, Others , 10.7717/peerj-cs.103PeerJ Computer Science. 2376-59923e103Jan. 2017</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, E Agirre, I Heintz, D Roth, 2023ACM Computing Surveys56</p>
<p>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. H L Morgan, Journal of Chemical Documentation. 51965</p>
<p>. Openai, Chatgpt, </p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, Advances in neural information processing systems. 202235</p>
<p>N Pinnaparaju, R Adithyan, D Phung, J Tow, J Baicoianu, A Datta, M Zhuravinskyi, arXiv:2404.01226Stable code technical report. 2024arXiv preprint</p>
<p>L Qin, Q Chen, Y Zhou, Z Chen, Y Li, L Liao, M Li, W Che, P S Yu, arXiv:2404.04925Multilingual large language model: A survey of resources, taxonomy and frontiers. 2024arXiv preprint</p>
<p>Open-source cheminformatics software. Rdkit, Rdkit, 2023</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, R Petraglia, V Zullo, V H Nair, Chemical science. 11122020</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Execution-based Code Generation using Deep Reinforcement Learning. P Shojaee, A Jain, S Tipirneni, C K Reddy, Transactions on Machine Learning Research. 2835-88562023</p>
<p>On the selfverification limitations of large language models on reasoning and planning tasks. K Stechly, K Valmeekam, S Kambhampati, arXiv:2402.081152024arXiv preprint</p>
<p>Learning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, P F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, arXiv:2307.092882023arXiv preprint</p>
<p>Improving llm code generation with grammar augmentation. S Ugare, T Suresh, H Kang, S Misailovic, G Singh, arXiv:2403.016322024arXiv preprint</p>
<p>L Werra, Y Belkada, L Tunstall, E Beeching, T Thrush, N Lambert, S Huang, Trl: Transformer reinforcement learning. 2020</p>
<p>Compilable Neural Code Generation with Compiler Feedback. X Wang, Y Wang, Y Wan, F Mi, Y Li, P Zhou, J Liu, H Wu, X Jiang, Q Liu, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Y Wang, Y Wang, D Guo, J Chen, R Zhang, Y Ma, Z Zheng, arXiv:2407.19487RLCoder: Reinforcement Learning for Repository-level Code Completion. 2024arXiv preprint</p>
<p>Grounding neural inference with satisfiability modulo theories. Z Wang, S Vijayakumar, K Lu, V Ganesh, S Jha, M Fredrikson, Advances in Neural Information Processing Systems. 202436</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. D Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Transformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOct. 2020Association for Computational Linguistics</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, X Han, Q Feng, H Jiang, S Zhong, B Yin, X Hu, ACM Transactions on Knowledge Discovery from Data. 1862024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. B Yu, F N Baker, Z Chen, X Ning, H Sun, arXiv:2402.093912024arXiv preprint</p>
<p>Text-to-image diffusion model in generative ai: A survey. C Zhang, C Zhang, M Zhang, I S Kweon, arXiv:2303.079092023arXiv preprint</p>
<p>Uni-mol: A universal 3d molecular representation learning framework. G Zhou, Z Gao, Q Ding, H Zheng, H Xu, Z Wei, L Zhang, G Ke, ICLR 20232023</p>            </div>
        </div>

    </div>
</body>
</html>
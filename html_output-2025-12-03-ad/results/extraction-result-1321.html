<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-240354417</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2111.00956v2.pdf" target="_blank">Robot Learning From Randomized Simulations: A Review</a></p>
                <p><strong>Paper Abstract:</strong> The rise of deep learning has caused a paradigm shift in robotics research, favoring methods that require large amounts of data. Unfortunately, it is prohibitively expensive to generate such data sets on a physical platform. Therefore, state-of-the-art approaches learn in simulation where data generation is fast as well as inexpensive and subsequently transfer the knowledge to the real robot (sim-to-real). Despite becoming increasingly realistic, all simulators are by construction based on models, hence inevitably imperfect. This raises the question of how simulators can be modified to facilitate learning robot control policies and overcome the mismatch between simulation and reality, often called the “reality gap.” We provide a comprehensive review of sim-to-real research for robotics, focusing on a technique named “domain randomization” which is a method for learning from randomized simulations.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1321.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1321.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose rigid-body physics engine commonly used for learning control policies in simulated articulated robotics; models joint dynamics, actuators, and contact with approximate friction models and efficient integrators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Rigid-body dynamics simulator designed for articulated robots; efficient contact and actuator models suitable for reinforcement learning research.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-high fidelity for kinematics and control; contact and friction are approximate (engine-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>models joint dynamics, actuators, and contacts with approximate friction/contact laws and numerical integrators; may under- or over-estimate contact dynamics; not a full continuum or soft-body solver</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reinforcement learning robot control policies (general deep NN policies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network policies (model-free RL) trained for locomotion/manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>control and decision-making for robotic locomotion and manipulation (not classical scientific reasoning tasks like thermodynamics/circuits/biology)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robots (sim-to-real) and cross-simulator evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Reported generally good for kinematics and control aspects but transfer degrades on dynamic robot-object interactions; success often requires domain randomization or additional calibration</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>When compared across engines, MuJoCo (like other engines) models kinematics/control accurately but shows deficits in dynamic contacts; no single engine uniformly best — differences arise from solver and contact/friction modeling</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The review argues that simply increasing simulator accuracy is not sufficient; minimum useful fidelity depends on task (e.g., contact and friction modeling matter for manipulation, terrain/contact for locomotion) and randomized parameter distributions help bridge remaining gaps</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Underestimated friction or inaccurate contact modeling can cause policies that perform well in MuJoCo to fail on the real robot; dynamic robot-object interactions are a common failure source.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1321.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bullet / PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bullet Physics Engine (PyBullet interface)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source rigid-body physics engine widely used in robotics and reinforcement learning; provides efficient collision detection and contact handling with several numerical solver options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Bullet / PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>General-purpose rigid-body simulator with collision detection and contact models; often used through the PyBullet Python bindings for RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium fidelity for kinematics and control; contact/friction approximations vary with solver settings</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>discrete-time integration, collision detection, contact and friction approximations; different coordinate representations and solver tolerances can affect outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reinforcement learning policies for locomotion, manipulation, and other control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep neural network policies trained with model-free RL</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>robot control (locomotion, manipulation), sim-to-sim and sim-to-real transfer studies</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real robots and other simulators (sim-to-sim cross-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Mixed — review cites studies (Muratore et al. 2018 / Collins et al. 2019) showing that transfer depends on contact/friction fidelity; policies trained in Bullet sometimes fail when moved to other engines or reality without randomization</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Benchmarks show Bullet models control and kinematics well but can deviate on dynamic interactions; differences versus Vortex/MuJoCo/ODE/ Newton arise from solvers and contact models</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Need at least reasonable contact/friction modeling for manipulation and terrain interactions; renderer quality matters for vision-based tasks</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Differences in coordinate representations, solver implementations, and friction/contact models led to transfer failures between Bullet and other engines in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1321.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vortex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vortex Physics Engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial/general-purpose rigid-body simulator used in robotics for dynamics and contact-rich scenarios; noted in the review for exhibiting behavioral differences when policies are transferred between engines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Vortex</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Rigid-body dynamics and contact simulator with specific contact/friction implementations, used in robot simulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-high for some dynamics aspects; contact/friction behavior can differ substantially from other engines</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>implements its own numerical solvers and contact/friction models which can produce different trajectories/contacts compared to other engines</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reinforcement learning controllers / policies</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Policy parameterizations trained in simulation (e.g., neural network controllers)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>robot control policy learning, sim-to-sim and sim-to-real transfer evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>other physics engines (e.g., Bullet) and real robots</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Muratore et al. (2018) observed that policies trained in Vortex did not always transfer well to Bullet and vice versa — indicating inter-engine fidelity differences affect transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Significant deviations between Vortex and Bullet observed; sources include different coordinate representations, numerical solvers, and friction/contact models; thus fidelity differences hurt transfer</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Ensuring plausibility and consistency of contact/friction models across simulator instances is important; low-level differences can defeat transfer</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Inter-engine differences caused transfer failures when moving controllers between Vortex and Bullet.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1321.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODE / Newton / SimBody (other classical engines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open Dynamics Engine (ODE), Newton, SimBody (classical rigid-body physics engines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Several widely-used rigid-body physics engines (ODE, Newton, SimBody) referenced as common options for robot simulation; each has differing implementations of solvers, contact and friction handling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ODE / Newton / SimBody</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Classic rigid-body physics engines providing articulated dynamics, collision detection, and contact models used in robotics experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium fidelity; good for kinematics and basic dynamics, contact fidelity varies</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>discrete-time integrators, contact and friction approximations, solver-specific numerical behavior; accuracy varies by scenario and implementation</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RL agents / control policies used for locomotion and manipulation research</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep RL policies or trajectory optimization controllers</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>robot control and policy learning</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robots and cross-engine comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Benchmarking (Collins et al., 2019; Erez et al., 2015) shows adequate control/kinematics prediction but deficits in dynamic object interactions — transfer depends on the task and contact dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Benchmarks indicate no single engine models all phenomena perfectly; kinematics and control often accurate, dynamic interactions are more problematic</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Task-dependent: contact and friction are necessary to model for manipulation; simply higher nominal fidelity is not a panacea without uncertainty treatment</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Simulators diverged from motion-capture ground truth in dynamic robot-object interactions; inaccurate contact modeling led to transfer failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1321.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable simulators (Tiny Differentiable Simulator / NeuralSim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tiny Differentiable Simulator (Tiny Differentiable Simulator) and NeuralSim (augmenting differentiable simulators with neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differentiable physics engines that permit gradient propagation through dynamics, enabling gradient-based learning and integration of learned components (e.g., neural net residuals) into simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Tiny Differentiable Simulator / NeuralSim (differentiable simulation frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulators designed to be differentiable w.r.t. state and parameters; used for gradient-based training, system identification, and combining analytical models with learned components.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics; soft-body extensions</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>variable — can be high fidelity for modeled dynamics but often limited to the domains implemented (rigid/soft bodies); enables learning residuals to improve fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>propagate gradients through dynamics; can be extended with neural networks to model residuals; soft-body differentiable simulators (e.g., ChainQueen) exist</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Differentiable-model-based controllers, neural residual models, policies trained with gradient-based objectives</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines analytical differentiable dynamics with learned neural network corrections; used for model-based RL or system identification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>model-based control, system identification, improving sim-to-real via learned simulator components</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world systems (via improved simulator fidelity / residual models) and training pipelines that require gradients</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Described as facilitating learning and identification; specifics depend on task — differentiable augmentations can improve prediction accuracy and thereby potential transfer</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Differentiability aids learning and calibration but fidelity still task-dependent; combining analytic models and learned residuals helps capture unmodeled effects</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not directly enumerated in the review; general caveat that simulators remain approximations and inaccurate components (contacts/friction) still hamper transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1321.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChainQueen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChainQueen (real-time differentiable soft-body simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-time differentiable physical simulator targeted at soft robotics and deformable bodies, enabling gradient-based learning for soft-body dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ChainQueen</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Real-time differentiable simulator focused on soft-body dynamics, useful for soft robotics and differentiable learning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>soft-body mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity for certain soft-body approximations; real-time, differentiable; models continuum-like soft dynamics approximately</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>differentiable continuum/particle-based soft-body dynamics, enables gradients for learning; may not capture all complex real-world material nonlinearities</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Controllers and learned models for deformable object manipulation / soft robotics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model-based controllers, differentiable residual models, or RL policies using differentiable simulation gradients</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>control and planning involving deformable objects or soft-body dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world soft robotics tasks; sim-to-real for deformable object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Used to enable learning for deformable objects; specifics depend on task and calibration; review cites ChainQueen as part of differentiable-simulator developments</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Soft-body-specific fidelity is necessary to model deformable objects; differentiable soft simulators help but require calibration for real materials</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Real-world material complexities and contact interactions can still cause mismatch; not enumerated in detail in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1321.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Collins2019 benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantifying the Reality gap in Robotic Manipulation Tasks (Collins et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark study cited in the review that measured accuracy of several physics engines (ODE, PyBullet, Newton, Vortex, MuJoCo) against motion-capture ground truth and concluded that engines model kinematics/control well but struggle with dynamic interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Benchmark of ODE, PyBullet, Newton, Vortex, MuJoCo (as evaluated in Collins et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Empirical comparison of multiple rigid-body simulation engines using motion-capture ground truth to compute accumulated MSE in Cartesian position.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>evaluative: finds engines have high fidelity for kinematics/control but lower fidelity for dynamic robot-object interactions</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>benchmark uses accumulated mean-squared error between simulated Cartesian positions and motion-capture ground truth; highlights contact/dynamic interaction discrepancies</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>N/A (benchmark study rather than a trained agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>assessing simulator fidelity for robotic manipulation and implications for sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world motion-capture-measured trajectories (ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Collins et al. conclude control and kinematics are modeled accurately but dynamic interactions are not — indicating transfer limitations for tasks involving contacts/impacts</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Measured accumulated MSE shows all evaluated engines adequate for kinematics/control but deficits arise for dynamic robot-object interactions; no single engine uniformly superior across all metrics</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Contact and dynamic interaction fidelity are key bottlenecks for manipulation transfer; achieving perfect physical fidelity is not mandatory but modeling contact/friction and uncertainty is critical</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Deficits during dynamic robot-object interactions caused substantial simulated-vs-real discrepancies in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1321.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differing-simulator-transfer (Muratore et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment (Muratore et al., 2018) — inter-engine observations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reference cited that observed inter-engine differences when transferring policies trained in one engine (Vortex) to another (Bullet) and introduced measures for estimating Simulation Optimization Bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Vortex and Bullet (inter-engine transfer case study)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Empirical case study of policy transfer between two physics engines where differences in solvers/contact models affected transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>illustrative: shows that fidelity differences across engines manifest in transfer behavior</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>differences in coordinate representation, numerical solvers, friction and contact models affect behavior</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Robot control policies (learned via policy optimization in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Policy search / RL controllers whose transferability is measured</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>assessment of transferability of controllers across simulators and to reality</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>other simulator engines and real robot domains (general sim-to-sim and sim-to-real concerns)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Observed failures / performance drops when transferring between Vortex and Bullet without accounting for inter-engine differences; developed Simulation Optimization Bias as a metric</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Inter-engine transferability degraded due to solver/contact differences; optimistic simulation performance (SOB) quantifies expected overestimation</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper and review stress that modeling key phenomena (contacts, friction) and accounting for uncertainty (e.g., via domain randomization) is more important than aiming for a single ultra-accurate engine</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Policies exploiting simulator-specific artifacts failed to transfer between engines and to real systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1321.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1321.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-domain-specific-simulators-found</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absence of thermodynamics / circuits / biology simulators in this review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The reviewed paper focuses on robotics simulators and sim-to-real for control (mechanics); it does not present examples of simulators used to train models/agents for thermodynamics, electrical circuits, or biology reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robot Learning From Randomized Simulations: A Review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>none (no thermodynamics/circuits/biology simulators discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>The review lists many physics engines for rigid and soft-body mechanics but contains no specific simulators for thermodynamics, electrical-circuit simulation, or biological-system simulation used to train agents for scientific reasoning in those domains.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>N/A (explicitly absent: thermodynamics, circuits, biology)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>While the review extensively discusses fidelity for mechanics/contact/vision simulators, it does not discuss minimum fidelity or transfer for thermodynamics, circuit, or biology simulators — none are presented</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not applicable — the paper does not report experiments or transfer cases in thermodynamics, circuits, or biology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robot Learning From Randomized Simulations: A Review', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying the Reality gap in Robotic Manipulation Tasks <em>(Rating: 2)</em></li>
                <li>Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning Agile Locomotion for Quadruped Robots <em>(Rating: 1)</em></li>
                <li>A Differentiable Physics Engine for Deep Learning in Robotics <em>(Rating: 2)</em></li>
                <li>Chainqueen: A Real-Time Differentiable Physical Simulator for Soft Robotics <em>(Rating: 2)</em></li>
                <li>Bayessim: Adaptive Domain Randomization via Probabilistic Inference for Robotics Simulators <em>(Rating: 2)</em></li>
                <li>Learning Dexterous In-Hand Manipulation <em>(Rating: 1)</em></li>
                <li>Sim-to-(multi)-real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1321",
    "paper_id": "paper-240354417",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A general-purpose rigid-body physics engine commonly used for learning control policies in simulated articulated robotics; models joint dynamics, actuators, and contact with approximate friction models and efficient integrators.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "Rigid-body dynamics simulator designed for articulated robots; efficient contact and actuator models suitable for reinforcement learning research.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium-high fidelity for kinematics and control; contact and friction are approximate (engine-dependent)",
            "fidelity_characteristics": "models joint dynamics, actuators, and contacts with approximate friction/contact laws and numerical integrators; may under- or over-estimate contact dynamics; not a full continuum or soft-body solver",
            "model_or_agent_name": "Reinforcement learning robot control policies (general deep NN policies)",
            "model_description": "Neural-network policies (model-free RL) trained for locomotion/manipulation tasks",
            "reasoning_task": "control and decision-making for robotic locomotion and manipulation (not classical scientific reasoning tasks like thermodynamics/circuits/biology)",
            "training_performance": null,
            "transfer_target": "real-world robots (sim-to-real) and cross-simulator evaluation",
            "transfer_performance": "Reported generally good for kinematics and control aspects but transfer degrades on dynamic robot-object interactions; success often requires domain randomization or additional calibration",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "When compared across engines, MuJoCo (like other engines) models kinematics/control accurately but shows deficits in dynamic contacts; no single engine uniformly best — differences arise from solver and contact/friction modeling",
            "minimal_fidelity_discussion": "The review argues that simply increasing simulator accuracy is not sufficient; minimum useful fidelity depends on task (e.g., contact and friction modeling matter for manipulation, terrain/contact for locomotion) and randomized parameter distributions help bridge remaining gaps",
            "failure_cases": "Underestimated friction or inaccurate contact modeling can cause policies that perform well in MuJoCo to fail on the real robot; dynamic robot-object interactions are a common failure source.",
            "uuid": "e1321.0",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Bullet / PyBullet",
            "name_full": "Bullet Physics Engine (PyBullet interface)",
            "brief_description": "An open-source rigid-body physics engine widely used in robotics and reinforcement learning; provides efficient collision detection and contact handling with several numerical solver options.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "Bullet / PyBullet",
            "simulator_description": "General-purpose rigid-body simulator with collision detection and contact models; often used through the PyBullet Python bindings for RL experiments.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium fidelity for kinematics and control; contact/friction approximations vary with solver settings",
            "fidelity_characteristics": "discrete-time integration, collision detection, contact and friction approximations; different coordinate representations and solver tolerances can affect outputs",
            "model_or_agent_name": "Reinforcement learning policies for locomotion, manipulation, and other control tasks",
            "model_description": "Deep neural network policies trained with model-free RL",
            "reasoning_task": "robot control (locomotion, manipulation), sim-to-sim and sim-to-real transfer studies",
            "training_performance": null,
            "transfer_target": "real robots and other simulators (sim-to-sim cross-evaluation)",
            "transfer_performance": "Mixed — review cites studies (Muratore et al. 2018 / Collins et al. 2019) showing that transfer depends on contact/friction fidelity; policies trained in Bullet sometimes fail when moved to other engines or reality without randomization",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Benchmarks show Bullet models control and kinematics well but can deviate on dynamic interactions; differences versus Vortex/MuJoCo/ODE/ Newton arise from solvers and contact models",
            "minimal_fidelity_discussion": "Need at least reasonable contact/friction modeling for manipulation and terrain interactions; renderer quality matters for vision-based tasks",
            "failure_cases": "Differences in coordinate representations, solver implementations, and friction/contact models led to transfer failures between Bullet and other engines in reported experiments.",
            "uuid": "e1321.1",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Vortex",
            "name_full": "Vortex Physics Engine",
            "brief_description": "A commercial/general-purpose rigid-body simulator used in robotics for dynamics and contact-rich scenarios; noted in the review for exhibiting behavioral differences when policies are transferred between engines.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "Vortex",
            "simulator_description": "Rigid-body dynamics and contact simulator with specific contact/friction implementations, used in robot simulation experiments.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium-high for some dynamics aspects; contact/friction behavior can differ substantially from other engines",
            "fidelity_characteristics": "implements its own numerical solvers and contact/friction models which can produce different trajectories/contacts compared to other engines",
            "model_or_agent_name": "Reinforcement learning controllers / policies",
            "model_description": "Policy parameterizations trained in simulation (e.g., neural network controllers)",
            "reasoning_task": "robot control policy learning, sim-to-sim and sim-to-real transfer evaluations",
            "training_performance": null,
            "transfer_target": "other physics engines (e.g., Bullet) and real robots",
            "transfer_performance": "Muratore et al. (2018) observed that policies trained in Vortex did not always transfer well to Bullet and vice versa — indicating inter-engine fidelity differences affect transfer.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Significant deviations between Vortex and Bullet observed; sources include different coordinate representations, numerical solvers, and friction/contact models; thus fidelity differences hurt transfer",
            "minimal_fidelity_discussion": "Ensuring plausibility and consistency of contact/friction models across simulator instances is important; low-level differences can defeat transfer",
            "failure_cases": "Inter-engine differences caused transfer failures when moving controllers between Vortex and Bullet.",
            "uuid": "e1321.2",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "ODE / Newton / SimBody (other classical engines)",
            "name_full": "Open Dynamics Engine (ODE), Newton, SimBody (classical rigid-body physics engines)",
            "brief_description": "Several widely-used rigid-body physics engines (ODE, Newton, SimBody) referenced as common options for robot simulation; each has differing implementations of solvers, contact and friction handling.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "ODE / Newton / SimBody",
            "simulator_description": "Classic rigid-body physics engines providing articulated dynamics, collision detection, and contact models used in robotics experiments.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "medium fidelity; good for kinematics and basic dynamics, contact fidelity varies",
            "fidelity_characteristics": "discrete-time integrators, contact and friction approximations, solver-specific numerical behavior; accuracy varies by scenario and implementation",
            "model_or_agent_name": "RL agents / control policies used for locomotion and manipulation research",
            "model_description": "Deep RL policies or trajectory optimization controllers",
            "reasoning_task": "robot control and policy learning",
            "training_performance": null,
            "transfer_target": "real-world robots and cross-engine comparisons",
            "transfer_performance": "Benchmarking (Collins et al., 2019; Erez et al., 2015) shows adequate control/kinematics prediction but deficits in dynamic object interactions — transfer depends on the task and contact dynamics",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Benchmarks indicate no single engine models all phenomena perfectly; kinematics and control often accurate, dynamic interactions are more problematic",
            "minimal_fidelity_discussion": "Task-dependent: contact and friction are necessary to model for manipulation; simply higher nominal fidelity is not a panacea without uncertainty treatment",
            "failure_cases": "Simulators diverged from motion-capture ground truth in dynamic robot-object interactions; inaccurate contact modeling led to transfer failures.",
            "uuid": "e1321.3",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Differentiable simulators (Tiny Differentiable Simulator / NeuralSim)",
            "name_full": "Tiny Differentiable Simulator (Tiny Differentiable Simulator) and NeuralSim (augmenting differentiable simulators with neural networks)",
            "brief_description": "Differentiable physics engines that permit gradient propagation through dynamics, enabling gradient-based learning and integration of learned components (e.g., neural net residuals) into simulators.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "Tiny Differentiable Simulator / NeuralSim (differentiable simulation frameworks)",
            "simulator_description": "Simulators designed to be differentiable w.r.t. state and parameters; used for gradient-based training, system identification, and combining analytical models with learned components.",
            "scientific_domain": "mechanics / robotics; soft-body extensions",
            "fidelity_level": "variable — can be high fidelity for modeled dynamics but often limited to the domains implemented (rigid/soft bodies); enables learning residuals to improve fidelity",
            "fidelity_characteristics": "propagate gradients through dynamics; can be extended with neural networks to model residuals; soft-body differentiable simulators (e.g., ChainQueen) exist",
            "model_or_agent_name": "Differentiable-model-based controllers, neural residual models, policies trained with gradient-based objectives",
            "model_description": "Combines analytical differentiable dynamics with learned neural network corrections; used for model-based RL or system identification",
            "reasoning_task": "model-based control, system identification, improving sim-to-real via learned simulator components",
            "training_performance": null,
            "transfer_target": "real-world systems (via improved simulator fidelity / residual models) and training pipelines that require gradients",
            "transfer_performance": "Described as facilitating learning and identification; specifics depend on task — differentiable augmentations can improve prediction accuracy and thereby potential transfer",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Differentiability aids learning and calibration but fidelity still task-dependent; combining analytic models and learned residuals helps capture unmodeled effects",
            "failure_cases": "Not directly enumerated in the review; general caveat that simulators remain approximations and inaccurate components (contacts/friction) still hamper transfer.",
            "uuid": "e1321.4",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "ChainQueen",
            "name_full": "ChainQueen (real-time differentiable soft-body simulator)",
            "brief_description": "A real-time differentiable physical simulator targeted at soft robotics and deformable bodies, enabling gradient-based learning for soft-body dynamics.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "ChainQueen",
            "simulator_description": "Real-time differentiable simulator focused on soft-body dynamics, useful for soft robotics and differentiable learning approaches.",
            "scientific_domain": "soft-body mechanics / robotics",
            "fidelity_level": "high-fidelity for certain soft-body approximations; real-time, differentiable; models continuum-like soft dynamics approximately",
            "fidelity_characteristics": "differentiable continuum/particle-based soft-body dynamics, enables gradients for learning; may not capture all complex real-world material nonlinearities",
            "model_or_agent_name": "Controllers and learned models for deformable object manipulation / soft robotics",
            "model_description": "Model-based controllers, differentiable residual models, or RL policies using differentiable simulation gradients",
            "reasoning_task": "control and planning involving deformable objects or soft-body dynamics",
            "training_performance": null,
            "transfer_target": "real-world soft robotics tasks; sim-to-real for deformable object manipulation",
            "transfer_performance": "Used to enable learning for deformable objects; specifics depend on task and calibration; review cites ChainQueen as part of differentiable-simulator developments",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Soft-body-specific fidelity is necessary to model deformable objects; differentiable soft simulators help but require calibration for real materials",
            "failure_cases": "Real-world material complexities and contact interactions can still cause mismatch; not enumerated in detail in the review.",
            "uuid": "e1321.5",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Collins2019 benchmark",
            "name_full": "Quantifying the Reality gap in Robotic Manipulation Tasks (Collins et al., 2019)",
            "brief_description": "A benchmark study cited in the review that measured accuracy of several physics engines (ODE, PyBullet, Newton, Vortex, MuJoCo) against motion-capture ground truth and concluded that engines model kinematics/control well but struggle with dynamic interactions.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "Benchmark of ODE, PyBullet, Newton, Vortex, MuJoCo (as evaluated in Collins et al. 2019)",
            "simulator_description": "Empirical comparison of multiple rigid-body simulation engines using motion-capture ground truth to compute accumulated MSE in Cartesian position.",
            "scientific_domain": "mechanics / robotics (robotic manipulation)",
            "fidelity_level": "evaluative: finds engines have high fidelity for kinematics/control but lower fidelity for dynamic robot-object interactions",
            "fidelity_characteristics": "benchmark uses accumulated mean-squared error between simulated Cartesian positions and motion-capture ground truth; highlights contact/dynamic interaction discrepancies",
            "model_or_agent_name": "N/A (benchmark study rather than a trained agent)",
            "model_description": "N/A",
            "reasoning_task": "assessing simulator fidelity for robotic manipulation and implications for sim-to-real transfer",
            "training_performance": null,
            "transfer_target": "real-world motion-capture-measured trajectories (ground truth)",
            "transfer_performance": "Collins et al. conclude control and kinematics are modeled accurately but dynamic interactions are not — indicating transfer limitations for tasks involving contacts/impacts",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Measured accumulated MSE shows all evaluated engines adequate for kinematics/control but deficits arise for dynamic robot-object interactions; no single engine uniformly superior across all metrics",
            "minimal_fidelity_discussion": "Contact and dynamic interaction fidelity are key bottlenecks for manipulation transfer; achieving perfect physical fidelity is not mandatory but modeling contact/friction and uncertainty is critical",
            "failure_cases": "Deficits during dynamic robot-object interactions caused substantial simulated-vs-real discrepancies in the benchmark.",
            "uuid": "e1321.6",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Differing-simulator-transfer (Muratore et al. 2018)",
            "name_full": "Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment (Muratore et al., 2018) — inter-engine observations",
            "brief_description": "Reference cited that observed inter-engine differences when transferring policies trained in one engine (Vortex) to another (Bullet) and introduced measures for estimating Simulation Optimization Bias.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "Vortex and Bullet (inter-engine transfer case study)",
            "simulator_description": "Empirical case study of policy transfer between two physics engines where differences in solvers/contact models affected transfer.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "illustrative: shows that fidelity differences across engines manifest in transfer behavior",
            "fidelity_characteristics": "differences in coordinate representation, numerical solvers, friction and contact models affect behavior",
            "model_or_agent_name": "Robot control policies (learned via policy optimization in simulation)",
            "model_description": "Policy search / RL controllers whose transferability is measured",
            "reasoning_task": "assessment of transferability of controllers across simulators and to reality",
            "training_performance": null,
            "transfer_target": "other simulator engines and real robot domains (general sim-to-sim and sim-to-real concerns)",
            "transfer_performance": "Observed failures / performance drops when transferring between Vortex and Bullet without accounting for inter-engine differences; developed Simulation Optimization Bias as a metric",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Inter-engine transferability degraded due to solver/contact differences; optimistic simulation performance (SOB) quantifies expected overestimation",
            "minimal_fidelity_discussion": "Paper and review stress that modeling key phenomena (contacts, friction) and accounting for uncertainty (e.g., via domain randomization) is more important than aiming for a single ultra-accurate engine",
            "failure_cases": "Policies exploiting simulator-specific artifacts failed to transfer between engines and to real systems.",
            "uuid": "e1321.7",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "No-domain-specific-simulators-found",
            "name_full": "Absence of thermodynamics / circuits / biology simulators in this review",
            "brief_description": "The reviewed paper focuses on robotics simulators and sim-to-real for control (mechanics); it does not present examples of simulators used to train models/agents for thermodynamics, electrical circuits, or biology reasoning tasks.",
            "citation_title": "Robot Learning From Randomized Simulations: A Review",
            "mention_or_use": "mention",
            "simulator_name": "none (no thermodynamics/circuits/biology simulators discussed)",
            "simulator_description": "The review lists many physics engines for rigid and soft-body mechanics but contains no specific simulators for thermodynamics, electrical-circuit simulation, or biological-system simulation used to train agents for scientific reasoning in those domains.",
            "scientific_domain": "N/A (explicitly absent: thermodynamics, circuits, biology)",
            "fidelity_level": "N/A",
            "fidelity_characteristics": "N/A",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "While the review extensively discusses fidelity for mechanics/contact/vision simulators, it does not discuss minimum fidelity or transfer for thermodynamics, circuit, or biology simulators — none are presented",
            "failure_cases": "Not applicable — the paper does not report experiments or transfer cases in thermodynamics, circuits, or biology.",
            "uuid": "e1321.8",
            "source_info": {
                "paper_title": "Robot Learning From Randomized Simulations: A Review",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying the Reality gap in Robotic Manipulation Tasks",
            "rating": 2,
            "sanitized_title": "quantifying_the_reality_gap_in_robotic_manipulation_tasks"
        },
        {
            "paper_title": "Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_simulationbased_policy_optimization_with_transferability_assessment"
        },
        {
            "paper_title": "Sim-to-real: Learning Agile Locomotion for Quadruped Robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "A Differentiable Physics Engine for Deep Learning in Robotics",
            "rating": 2,
            "sanitized_title": "a_differentiable_physics_engine_for_deep_learning_in_robotics"
        },
        {
            "paper_title": "Chainqueen: A Real-Time Differentiable Physical Simulator for Soft Robotics",
            "rating": 2,
            "sanitized_title": "chainqueen_a_realtime_differentiable_physical_simulator_for_soft_robotics"
        },
        {
            "paper_title": "Bayessim: Adaptive Domain Randomization via Probabilistic Inference for Robotics Simulators",
            "rating": 2,
            "sanitized_title": "bayessim_adaptive_domain_randomization_via_probabilistic_inference_for_robotics_simulators"
        },
        {
            "paper_title": "Learning Dexterous In-Hand Manipulation",
            "rating": 1,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Sim-to-(multi)-real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors",
            "rating": 1,
            "sanitized_title": "simtomultireal_transfer_of_lowlevel_robust_control_policies_to_multiple_quadrotors"
        }
    ],
    "cost": 0.0240395,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robot Learning From Randomized Simulations: A Review</p>
<p>Fabio Muratore 
Intelligent Autonomous Systems Group
Technical University of Darmstadt
Darmstadt, Germany</p>
<p>Honda Research Institute Europe
Offenbach am Main
Germany</p>
<p>Fabio Ramos 
School of Computer Science
University of Sydney
SydneyNSWAustralia</p>
<p>NVIDIA
SeattleWAUnited States</p>
<p>Greg Turk 
Georgia Institute of Technology
AtlantaGAUnited States</p>
<p>Wenhao Yu 
Robotics at Google
Mountain ViewCAUnited States</p>
<p>Michael Gienger 
Honda Research Institute Europe
Offenbach am Main
Germany</p>
<p>Jan Peters 
Intelligent Autonomous Systems Group
Technical University of Darmstadt
Darmstadt, Germany</p>
<p>Robot Learning From Randomized Simulations: A Review
10.3389/frobt.2022.799893roboticssimulationreality gapsimulation optimization biasreinforcement learningdomain randomizationsim-to-real
The rise of deep learning has caused a paradigm shift in robotics research, favoring methods that require large amounts of data. Unfortunately, it is prohibitively expensive to generate such data sets on a physical platform. Therefore, state-of-the-art approaches learn in simulation where data generation is fast as well as inexpensive and subsequently transfer the knowledge to the real robot (sim-to-real). Despite becoming increasingly realistic, all simulators are by construction based on models, hence inevitably imperfect. This raises the question of how simulators can be modified to facilitate learning robot control policies and overcome the mismatch between simulation and reality, often called the "reality gap." We provide a comprehensive review of sim-to-real research for robotics, focusing on a technique named "domain randomization" which is a method for learning from randomized simulations.</p>
<p>INTRODUCTION</p>
<p>Given that machine learning has achieved super-human performance in image classification (Ciresan et al., 2012;Krizhevsky et al., 2012) and games (Mnih et al., 2015;Silver et al., 2016), the question arises why we do not see similar results in robotics. There are several reasons for this. First, learning to act in the physical world is orders of magnitude more difficult. While the data required by modern (deep) learning algorithms could be acquired directly on a real robot , this solution is too expensive in terms of time and resources to scale up. Alternatively, the data can be generated in simulation faster, cheaper, safer, and with unmatched diversity. In doing so, we have to cope with unavoidable approximation errors that we make when modeling reality. These errors, often referred to as the "reality gap," originate from omitting physical phenomena, inaccurate parameter estimation, or the discretized numerical integration in typical solvers. Compounding this issue, state-of-the-art (deep) learning methods are known to be brittle (Szegedy et al., 2014;Goodfellow et al., 2015;Huang et al., 2017), that is, sensitive to shifts in their input domains. Additionally, the learner is free to exploit the simulator, overfitting to features which do not occur in the real world. For example, Baker et al. (2020) noticed that the agents learned to exploit the physics engine to gain an unexpected advantage. While this exploitation is an interesting observation for studies made entirely in simulation, it is highly undesirable in sim-to-real scenarios. In the best case, the reality gap manifests itself as a performance drop, giving a lower success rate or reduced tracking accuracy. More likely, the learned policy is not transferable to the robot because of unknown physical effects. One effect that is difficult to model is friction, often leading to an underestimation thereof in simulation, which can result in motor commands that are not strong enough to get the robot moving. Another reason for failure are parameter estimation errors, which can quickly lead to unstable system dynamics. This case is particularly dangerous for the human and the robot. For these reasons, bridging the reality gap is the essential step to endow robots with the ability to learn from simulated experience.</p>
<p>There is a consensus that further increasing the simulator's accuracy alone will not bridge this gap (Höfer et al., 2020). Looking at breakthroughs in machine learning, we see that deep models in combination with large and diverse data sets lead to better generalization (Russakovsky et al., 2015;Radford et al., 2019). In a similar spirit, a technique called domain randomization has recently gained momentum ( Figure 1). The common characteristic of such approaches is the perturbation of simulator parameters, state observations, or applied actions. Typical quantities to randomize include the bodies' inertia and geometry, the parameters of the friction and contact models, possible delays in the actuation, efficiency coefficients of motors, levels of sensor noise, as well as visual properties such as colors, illumination, position and orientation of a camera, or additional artifacts to the image (e.g., glare). Domain randomization can be seen as a regularization method that prevents the learner from overfitting to individual simulation instances. From the Bayesian perspective, we can interpret the distribution over simulators as a representation of uncertainty.</p>
<p>In this paper, we first introduce the necessary nomenclature and mathematical fundamentals for the problem (Section 2). Next, we review early approaches for learning from randomized simulations, state the practical requirements, and describe measures for sim-to-real transferability (Section 3). Subsequently, we discuss the connections between research on sim-to-real transfer and related fields (Section 4). Moreover, we introduce a taxonomy for domain randomization and categorize the current state of the art (Section 5). Finally, we conclude and outline possible future research directions (Section 6). For those who want to first become more familiar with robot policy learning as well as policy search, we recommend these surveys: Kober et al. (2013), Deisenroth et al. (2013), and Chatzilygeroudis et al. (2020).</p>
<p>PROBLEM FORMULATION AND NOMENCLATURE</p>
<p>We begin our discussion by defining critical concepts and nomenclature used throughout this article.</p>
<p>Markov Decision Processes (MDPs): Consider a discrete-time dynamical system s t+1~Pξ s t+1 |s t , a t ( ) , s 0~μ ξ s 0 ( ), a t~πθ a t |s t ( ), ξ~p ξ ( ), (1) with the continuous state s t ∈ S ξ ⊆ R n s and continuous action a t ∈ A ξ ⊆ R n a at time step t. The environment, also called domain, is characterized by its parameters ξ ∈ R n ξ (e.g., masses, friction coefficients, time delays, or surface appearance properties) which are in general assumed to be random variables distributed according to an unknown probability distribution p(ξ): R n ξ → R + . A special case of this is the common assumption that the domain parameters obey a parametric distribution p ϕ (ξ) with unknown parameters ϕ (e.g., mean and variance). The domain parameters determine the transition probability density function P ξ : S ξ × A ξ × S ξ → R + that describes the system's stochastic dynamics. The initial state s 0 is drawn from the start state distribution μ ξ : S ξ → R + . In general, the instantaneous reward is a random variable depending on the current state and action as well as the next state. Here we make the common simplification that the reward is a deterministic function of the current state and action r ξ : S ξ × A ξ → R Together with the temporal discount factor γ ∈ [0, 1], the system forms a MDP described by the tuple M ξ 〈S ξ , A ξ , P ξ , μ ξ , r ξ , γ〉. Reinforcement Learning (RL): The goal of a RL agent is to maximize the expected (discounted) return, a numeric scoring function which measures the policy's performance. The expected discounted return of a policy π θ (a t |s t ) with the parameters θ ∈ Θ ⊆ R n θ is defined as FIGURE 1 | Examples of sim-to-real robot learning research using domain randomization: (left) Multiple simulation instances of robotic in-hand manipulation (OpenAI et al., 2020), (middle top) transformation to a canonical simulation (James et al., 2019), (middle bottom) synthetic 3D hallways generated for indoor drone flight (Sadeghi and Levine, 2017), (right top) ball-in-a-cup task solved with adaptive dynamics randomization (Muratore et al., 2021a), (right bottom) quadruped locomotion (Tan et al., 2018).
J θ, ξ ( ) E s0~μ ξ s0 ( ) E st+1~P ξ st,at ( ), at~π θ at|st ( ) T−1 t 0 γ t ξ s t , a t ( ) θ | , ξ, s 0 .
(2)</p>
<p>While learning from experience, the agent adapts its policy parameters. The resulting state-action-reward tuples are collected in trajectories, a.k.a. rollouts, τ {s t , a t , r t } T−1 t 0 ∈ T with r t r ξ (s t , a t ). In a partially observable MDP, the policy's input would not be the state but observations there of o t ∈ O ξ ⊆ R n o , which are obtained through an environmentspecific mapping o t = f obs (s t ).</p>
<p>Domain randomization: When augmenting the RL setting with domain randomization, the goal becomes to maximize the expected (discounted) return for a distribution of domain parameters
J θ ( ) E ξ~p ξ ( ) J θ, ξ ( ) [ ] E ξ~p ξ ( ) E τ~p τ ( ) T−1 t 0 γ t ξ s t , a t ( ) θ | , ξ, s 0 .(3)
The outer expectation with respect to the domain parameter distribution p(ξ) is the key difference compared to the standard MDP formulation. It enables the learning of robust policies, in the sense that these policies work for a whole set of environments instead of overfitting to a particular problem instance.</p>
<p>FOUNDATIONS OF SIM-TO-REAL TRANSFER</p>
<p>Modern research on learning from (randomized) physics simulations is based on solid foundation of prior work (Section 3.1). Parametric simulators are the core component of every sim-to-real method (Section 3.2). Even though the details of their randomization are crucial, they are rarely discussed (Section 3.3). Estimating the sim-to-real transferability during or after learning allows one to assess or predict the policy's performance in the target domain (Section 3.4).</p>
<p>Early Methods</p>
<p>The roots of randomized simulations trace back to the invention of the Monte Carlo method (Metropolis and Ulam, 1949), which computes its results based on repeated random sampling and subsequent statistical analysis. Later, the concept of common random numbers, also called correlated sampling, was developed as a variance reduction technique (Kahn and Marshall, 1953;Wright and Ramsay, 1979). The idea is to synchronize the random numbers for all stochastic events across the simulation runs to achieve a (desirably positive) correlation between random variables reducing the variance of an estimator based on a combination of them. Many of the simto-real challenges which are currently tackled have already been identified by Brooks (1992). In particular, Brooks addresses the overfitting to effects which only occur in simulation as well as the idealized modeling on sensing and actuation. To avoid overfitting, he advocated for reactive behavior-based programming which is deeply rooted in, hence tailored to, the embodiment. Focusing on RL, Sutton (1991) introduced the Dyna architecture which revolves around predicting from a learned world model and updating the policy from this hypothetical experience. Viewing the data generated from randomized simulators as "imaginary," emphasizes the parallels of domain randomization to Dyna. As stated by Sutton, the usage of "mental rehearsal" to predict and reason about the effect of actions dates back even further in other fields of research such as psychology (Craik, 1943;Dennett, 1975). Instead of querying a learned internal model, Jakobi et al. (1995) added random noise the sensors and actuators while learning, achieving the arguably first sim-to-real transfer in robotics. In follow-up work, Jakobi (1997) formulated the radical envelope of noise hypothesis which states that "it does not matter how inaccurate or incomplete [the simulations] are: controllers that have evolved to be reliably fit in simulation will still transfer into reality." Picking up on the idea of common random numbers, Ng and Jordan (2000) suggested to explicitly control the randomness of a simulator, i.e., the random number generator's state, rendering the simulator deterministic. This way the same initial configurations can be (re-)used for Monte Carlo estimations of different policies' value functions, allowing one to conduct policy search in partially observable problems. Bongard et al. (2006) bridged the sim-to-real gap through iterating model generation and selection depending on the short-term state-action history. This process is repeated for a given number of iterations, and then yields the self-model, i.e., a simulator, which best explains the observed data. Inspired by these early approaches, the systematic analysis of randomized simulations for robot learning has become a highly active research direction. Moreover, the prior work above also falsifies the common belief that domain randomization originated recently with the rise of deep learning. Nevertheless, the current popularity of domain randomization can be explained by its widespread use in the computer vision and locomotion communities as well as its synergies with deep learning methods.</p>
<p>The key difference between the early and the recent domain randomization methods (Section 5) is that the latter (directly) manipulate the simulators' parameters.</p>
<p>Constructing Stochastic Simulators</p>
<p>Simulators can be obtained by implementing a set of physical laws for a particular system. Given the challenges in implementing an efficient simulator for complex systems, it is common to use general purpose physics engines such as ODE, DART, Bullet, Newton, SimBody, Vortex, MuJoCo, Havok, Chrono, RaiSim, PhysX, FleX, or Brax. These simulators are parameterized generative models, which describe how multiple bodies or particles evolve over time by interacting with each other. The associated physics parameters can be estimated by system identification (Section 4.6), which generally involves executing experiments on the physical platform and recording associated measurement. Additionally, using the Gauss-Markov theorem one could also compute the parameters' covariance and hence construct a normal distribution for each domain parameter. Differentiable simulators facilitate deep learning for robotics (Degrave et al., 2019;Coumans, 2020;Heiden et al., 2021) by propagating the gradients though the dynamics. Current research extends the differentiability to soft body dynamics (Hu et al., 2019). Alternatively, the system dynamics can be captured using nonparametric methods like Gaussian Processes (GPs) (Rasmussen and Williams, 2006) as for example demonstrated by Calandra et al. (2015). It is important to keep in mind that even if the domain parameters have been identified very accurately, simulators are nevertheless just approximations of the real world and are thus always imperfect.</p>
<p>Several comparisons between various physics engines were made (Ivaldi et al., 2014;Erez et al., 2015;Chung and Pollard, 2016;Collins et al., 2019;Körber et al., 2021). However, note that these results become outdated quickly due to the rapid development in the field, or are often limited to very few scenarios and partially introduce custom metrics to measure their performance or accuracy.</p>
<p>Apart from the physics engines listed above, there is an orthogonal research direction investigating human-inspired learning of the physics laws from visual input (Battaglia et al., 2013;Wu et al., 2015) as well as physical reasoning given a configuration of bodies (Battaglia et al., 2016), which is out of the scope of this review.</p>
<p>Randomizing a Simulator</p>
<p>Learning from randomized simulations entails significant design decisions:</p>
<p>Which parameters should be randomized? Depending on the problem, some domain parameters have no influence (e.g., the mass of an idealized rolling ball) while others are pivotal (e.g., the pendulum length for a stabilization task). It is recommended to first identify the essential parameters (Xie et al., 2020). For example, most robot locomotion papers highlight the importance of varying the terrain and contact models, while applications such as drone control benefit from adding perturbations, e.g., to simulate a gust of wind. Injecting random latency and noise to the actuation is another frequent modeling choice. Starting from a small set of randomized domain parameters, identified from prior knowledge, has the additional benefit of shortening the evaluation time which involves approximating an expectation over domains, which scales exponentially with the number of parameters. Moreover, including at least one visually observable parameter (e.g., an extent of a body) helps to verify if the values are set as expected.</p>
<p>When should the parameters be randomized? Episodic dynamics randomization, without a rigorous theoretical justification, is the most common approach. Randomizing the domain parameters at every time step instead would drastically increase the variance, and pose a challenge to the implementations since this typically implies recreating the simulation at every step. Imagine a stack of cubes standing on the ground. If we now vary the cubes' side lengths individually while keeping their absolute positions fixed, they will either lose contact or intersect with their neighboring cube(s). In order to keep the stack intact, we need to randomize the cubes with respect to their neighbors, additionally moving them in space. Executing this once at the beginning is fine, but doing this at every step creates artificial "movement" which would almost certainly be detrimental. Orthogonal to the argumentation above, alternative approaches apply random disturbance forces and torques at every time step. In these cases, the distribution over disturbance magnitudes is chosen to be constant until the randomization scheme is updated. To the best of our knowledge, event-triggered randomization has not been explored yet.</p>
<p>How should the parameters be randomized? Answering this question is what characterizes a domain randomization method (Section 5). There are a few aspects that needs to be considered in practice when designing a domain randomization scheme, such as the numerical stability of the simulation instances. Low masses for example quickly lead to stiff differential equations which might require a different (implicit) integrator. Furthermore, the noise level of the introduced randomness needs to match the precision of the state estimation. If the noise is too low, the randomization is pointless. On the other side, if the noise level is too high, the learning procedure will fail. To find the right balance between these considerations, we can start by statistically analyzing the incoming measurement signals.</p>
<p>What about physical plausibility? The application of pseudorandom color patterns, e.g., Perlin noise (Perlin, 2002), has become a frequent choice for computer vision applications. Despite that these patterns do not occur on real-world objects, this technique has improved the robustness of object detectors (James et al., 2017;Pinto et al., 2018). Regarding the randomization of dynamics parameters, no research has so far hinted that physically implausible simulations (e.g., containing bodies with negative masses) are useful. On the other hand, it is safe to say that these can cause numerical instabilities. Thus, ensuring feasibility of the resulting simulator is highly desirable. One solution is to project the domain parameters into a different space, guaranteeing physical plausibility via the inverse projection. For example, a body's mass could be learned in the log-space such that the subsequent exp-transformation, applied before setting the new parameter value, yields strictly positive numbers. However, most of the existing domain randomization approaches can not guarantee physical plausibility.</p>
<p>Even in the case of rigid body dynamics there are notable differences between physics engines, as was observed by Muratore et al. (2018) when transferring a robot control policy trained using Vortex to Bullet and vice versa. Typical sources for deviations are different coordinate representations, numerical solvers, friction and contact models. Especially the latter two are decisive for robot manipulation. For vision-based tasks, Alghonaim and Johns (2020) found a strong correlation between the renderer's quality and sim-to-real transferability. Additionally, the authors emphasize the importance of randomizing both distractor objects and background textures for generalizing to unseen environments.</p>
<p>Measuring and Predicting the Reality Gap</p>
<p>Coining the term "reality gap," Koos et al. (2010) hypothesize that the fittest solutions in simulation often rely on poorly simulated phenomena. From this, they derive a multi-objective formulation for sim-to-real transfer where performance and transferability Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893 need to be balanced. In subsequent work, Koos et al. (2013) defined a transferability function that maps controller parameters to their estimated target domain performance. A surrogate model of this function is regressed from the real-world fitness values that are obtained by executing the controllers found in simulation. The Simulation Optimization Bias (SOB) (Muratore et al., 2018;) is a quantitative measure for the transferability of a control policy from a set of source domains to a different target domain originating from the same distribution. Building on the formulation of the optimality gap from convex optimization (Mak et al., 1999;Bayraksan and Morton, 2006), Muratore et al. (2018) proposed a Monte Carlo estimator of the SOB as well as an upper confidence bound, tailored to reinforcement learning settings. This bound can be used as an indicator to stop training when the predicted transferability exceeds a threshold. Moreover, the authors show that the SOB is always positive, i.e., optimistic, and in expectation monotonically decreases with an increasing number of domains. Collins et al. (2019) quantify the accuracy of ODE, (Py)Bullet, Newton, Vortex, and MuJoCo in a real-world robotic setup. The accuracy is defined as the accumulated mean-squared error between the Cartesian ground truth position, tracked by a motion capture system, and the simulators' prediction. Based on this measure, they conclude that simulators are able to model the control and kinematics accurately, but show deficits during dynamic robot-object interactions.</p>
<p>To obtain a quantitative estimate of the transferability, Zhang et al. (2020) suggest to learn a probabilistic dynamics model which is evaluated on a static set of target domain trajectories. This dynamics model is trained jointly with the policy in the same randomized simulator. The transferability score is chosen to be the average negative log-likelihood of the model's output given temporal state differences from the real-world trajectories. Thus, the proposed method requires a set of pre-recorded target domain trajectories, and makes the assumption that for a given domain the model's prediction accuracy correlates with the policy performance.</p>
<p>With robot navigation in mind, Kadian et al. (2020) define the Sim-vs-Real Correlation Coefficient (SRCC) to be the Pearson correlation coefficient on data pairs of scalar performance metrics. The data pairs consist of the policy performance achieved in a simulator instance as well as in the real counterpart. Therefore, in contrast to the SOB (Muratore et al., 2018), the SRCC requires real-world rollouts. A high SRCC value, i.e., close to 1, predicts good transferability, while low values, i.e., close to 0, indicates that the agent is exploited the simulation during learning. Kadian et al. (2020) also report tuning the domain parameters with grid search to increase the SRCC. By using the Pearson correlation, the SRCC is restricted to linear correlation, which might not be a notable restriction in practice.</p>
<p>RELATION OF SIM-TO-REAL TO OTHER FIELDS</p>
<p>There are several research areas that overlap with sim-to-real in robot learning, more specifically domain randomization ( Figure 2). In the following, we describe those that either share the same goal, or employ conceptually similar methods.</p>
<p>Curriculum Learning</p>
<p>The key idea behind curriculum learning is to increase the sample efficiency by scheduling the training process such that the agent first encounters "easier" tasks and gradually progresses to "harder" ones. Hence, the agent can bootstrap from the knowledge it gained at the beginning, before learning to solve more difficult task instances. Widely known in supervised learning (Bengio et al., 2009;Kumar et al., 2010), curriculum learning has been applied to RL (Asada et al., 1996;Erez and Smart, 2008;Klink et al., 2019Klink et al., , 2021. The connection between curriculum learning and domain randomization can be highlighted by viewing the task as a part of the domain, i.e., the MDP, rendering the task parameters a subspace of the domain parameters. From this point of view, the curriculum learning schedule describes how the domain parameter distribution is updated. There are several challenges to using a curriculum learning approach for sim-to-real transfer. Three such challenges are: 1) we can not always assume to have an assessment of the difficulty level of individual domain parameter configurations, 2) curriculum learning does not aim at finding solutions robust to model uncertainty, and 3) curriculum learning methods may require a target distribution which is not defined in the domain randomization setting. However, adjustments can be made to circumvent these problems. OpenAI et al. (2019) suggested a heuristic for the domain randomization schedule that increases the boundaries of each domain parameter individually until the return drops more than a predefined threshold. Executing this approach on a computing cluster, the authors managed to train a policy and a vision system which in combination solve a Rubik's cube with a tendondriven robotic hand. Another intersection point of curriculum learning and sim-to-real transfer is the work by Morere et al. (2019), where a hierarchical planning method for discrete domains with unknown dynamics is proposed. Learning abstract skills based on a curriculum enables the algorithm to outperform planning and RL baselines, even in domains with a very large number of possible states.</p>
<p>Meta Learning</p>
<p>Inspired by the human ability to quickly master new tasks by leveraging the knowledge extracted from solving other tasks, meta learning (Santoro et al., 2016;Finn et al., 2017) seeks to make use of prior experiences gained from conceptually similar tasks. The field of meta learning currently enjoys high popularity, leading to abundant follow-up work. Grant et al. (2018) for example casts meta learning as hierarchical Bayesian inference. Furthermore, the meta learning framework has been adapted to the RL setting (Wang et al., 2017;Nagabandi et al., 2019). The optimization over an ensemble of tasks can be translated to the optimization over an ensemble of domain instances, modeled by different MDPs (Section 2). Via this duality one can view domain randomization as a special form of meta learning where the robot's task remains qualitatively unchanged but the environment varies. Thus, the tasks seen during the meta training phase are analogous to domain instances experienced earlier in the training process. However, when looking at the complete procedure, meta learning and domain randomization are fundamentally different. The goal of meta learning, i.e., Finn et al. (2017), is to find a suitable set of initial weights, which when updated generalizes well to a new task. Domain randomization on the other hand strives to directly solve a single task, generalizing over domain instances.</p>
<p>Transfer Learning</p>
<p>The term transfer learning covers a wide range of machine learning research, aiming at using knowledge learned in the source domain to solve a task in the target domain. Rooted in classification, transfer learning is categorized in several subfields by for example differentiating 1) if labeled data is available in the source or target domain, and 2) if the tasks in both domains are the same (Pan and Yang, 2010;Zhuang et al., 2021). Domain adaptation is one of the resulting subfields, specifying the case where ground truth information is only available in the target domain which is not equal to the source domain while the task remains the same. Thus, domain adaptation methods are in general suitable to tackle sim-to-real problems. However, the research fields evolved at different times in different communities, with different goals in mind. The keyword "simto-real" specifically concerns regression and control problems where the focus lies on overcoming the mismatch between simulation and reality. In contrast, most domain adaptation techniques are not designed for a dynamical system as the target domain.</p>
<p>Knowledge Distillation</p>
<p>When executing a controller on a physical device operating at high frequencies, it is of utmost importance that the forward pass finishes with the given time frame. With deep Neural Network (NN) policies, and especially with ensembles of these, this requirement can become challenging to meet. Distilling the knowledge of a larger network into a smaller one reduces the evaluation time. Knowledge distillation (Hinton et al., 2015) has been successfully applied to several machine learning applications such as natural language processing (Cui et al., 2017), and object detection (Chen et al., 2017). In the context of RL, knowledge distillation techniques can be used to compress the learned behavior of one or more teachers into a single student (Rusu et al., 2016a). Based on samples generated by the teachers, the student is trained in a supervised manner to imitate them. This idea can be applied to sim-to-real robot learning in a straightforward manner, where the teachers can be policies optimal for specific domain instances (Brosseit et al., 2021). Complementarily, knowledge distillation has been applied to multitask learning (Parisotto et al., 2016;Teh et al., 2017), promising to improve sample efficiency when learning a new task. A technical comparison of policy distillation methods for RL is provided by Czarnecki et al. (2019).</p>
<p>Distributional Robustness</p>
<p>The term robustness is overloaded with different meanings, such as the ability to (quickly) counteract external disturbances, or the resilience against uncertainties in the underlying model's parameters. The field of robust control aims at designing controllers that explicitly deal with these uncertainties (Zhou and Doyle, 1998). Within this field, distributional robust optimization is a framework to find the worst-case probabilistic model from a so-called ambiguity set, and subsequently set a policy which acts optimally in this worst case. Mathematically, the problem is formulated as bilevel optimization, which is solved iteratively in practice. By restricting the model selection to the ambiguity set, distributional robust optimization regularizes the adversary to prevent the process from yielding solutions that are overly conservative policies. Under the lens of domain randomization, the ambiguity set closely relates to the distribution over domain parameters. Abdulsamad et al. (2021) for example define the ambiguity set as a Kullback-Leibler (KL) ball the nominal distribution. Other approaches use a moment-based ambiguity set (Delage and Ye, 2010) or introduce chance constrains (Van Parys et al., 2016). For a review of distributional robust optimization, see Zhen et al. (2021). Chatzilygeroudis et al. (2020) point out that performing policy search under an uncertain model is equivalent to finding a policy that can perform well under various dynamics models. Hence, they argue that "model-based policy search with probabilistic models is performing something similar to dynamics randomization."</p>
<p>System Identification</p>
<p>The goal of system identification is to find the set of model parameters which fit the observed data best, typically by minimizing the prediction-dependent loss such as the meansquared error. Since the simulator is the pivotal element in every domain randomization method, the accessible parameters and their nominal values are of critical importance. When a manufacturer does not provide data for all model parameters, or when an engineer wants to deploy a new model, system identification is typically the first measure to obtain an estimate of the domain parameters. In principle, a number of approaches can be applied depending on the assumptions on the internal structure of the simulator. The earliest approaches in robotics recognized the linearity of the rigid body dynamics with respect to combinations of physics parameters such as masses, moments of inertia, and link lengths, thus proposed to use linear regression (Atkeson et al., 1986), and later Bayesian linear regression (Ting et al., 2006). However, it was quickly observed that the inferred parameters may be physically implausible, leading to the development of methods that can account for this (Ting et al., 2011). With the advent of deep learning, such structured physics-based approaches have been enhanced with NNs, yielding nonlinear system identification methods such as the ones based on the Newton-Euler forward dynamics (Sutanto et al., 2020;Lutter et al., 2021b). Alternatively, the simulator can be augmented with a NN to learn the domain parameter residuals, minimizing the one step prediction error (Allevato et al., 2019). On another front, system identification based on the classification loss between simulated and real samples has been investigated (Du et al., 2021;Jiang et al., 2021). System identification can also be interpreted as an episodic RL problem by treating the trajectory mismatch as the cost function and iteratively updating a distribution over models (Chebotar et al., 2019). Recent simulation-based inference methods yield highly expressive posterior distributions that capture multi-modality as well as correlations between the domain parameters (Section 4.8).</p>
<p>Adaptive Control</p>
<p>The well-established field of adaptive control is concerned with the problem of adapting a controller's parameters at runtime to operate initially uncertain or varying systems (e.g., aircraft reaching supersonic speed). A prominent method is model reference adaptive control, which tracks a reference model's output specifying the desired closed-loop behavior. Model Identification Adaptive Control (MIAC) is a different variant, which includes an online system identification component that continuously estimates the system's parameters based on the prediction error of the output signal (Åström and Wittenmark, 2008;Landau et al., 2011). Given the identified system, the controller is updated subsequently. Similarly, there exists a line of sim-to-real reinforcement learning approaches that condition the policy on the estimated domain parameters (Yu et al., , 2019bMozifian et al., 2020) or a latent representation thereof (Yu et al., 2019a;Peng et al., 2020;Kumar et al., 2021). The main difference to MIAC lies in the adaption mechanism. Adaptive control techniques typically define the parameters' gradient proportional to the prediction error, while the approaches referenced above make the domain parameters an input to the policy.</p>
<p>Simulation-Based Inference</p>
<p>Simulators are predominantly used as forward models, i.e., to make predictions. However, with the increasing fidelity and expressiveness of simulators, there is a growing interest to also use them for probabilistic inference (Cranmer et al., 2020). In the case of simulation-based inference, the simulator and its parameters define the statistical model. Inference tasks differ by the quantity to be inferred. Regarding sim-to-real transfer, the most frequent task is to infer the simulation parameters from real-world time series data. Similarly to system identification (Section 4.6), the result can be a point estimate, or a posterior distribution. Likelihood-Free Inference (LFI) methods are a type of simulation-based inference approaches which are particularly well-suited when we can make very little assumptions about the underlying generative model, treating it as an implicit function. These approaches only require samples from the model (e.g., a non-differentiable black-box simulator) and a measure of how likely real observations could have been generated from the simulator. Approximate Bayesian computation is well-known class of LFI methods that applies Monte Carlo sampling to infer the parameters by comparing summary statistics of synthetically generated and observed data. There exist plenty of variants for approximate Bayesian computation (Marjoram et al., 2003;Beaumont et al., 2009;Sunnåker et al., 2013) as well as studies on the design of low-dimensional summary statistics (Fearnhead and Prangle, 2012). In order to increase the efficiency and thereby scale LFI higher-dimensional problems, researchers investigated amortized approaches, which conduct the inference over multiple sequential rounds. Sequential neural posterior estimation approaches (Papamakarios and Murray, 2016;Lueckmann et al., 2017;Greenberg et al., 2019) approximate the conditional posterior, allowing for direct sampling from the posterior. Learning the likelihood (Papamakarios et al., 2019) can be useful in the context for hypothesis testing. Alternatively, posterior samples can be generated from likelihood-ratios (Durkan et al., 2020;Hermans et al., 2020). However, simulation-based inference does not explicitly consider policy optimization or domain randomization. Recent approaches connected all three techniques, and closed the reality gap by inferring a distribution over simulators while training policies in simulation Barcelos et al., 2020;Muratore et al., 2021c).</p>
<p>DOMAIN RANDOMIZATION FOR SIM-TO-REAL TRANSFER</p>
<p>We distinguish between static (Section 5.1), adaptive (Section 5.2), and adversarial (Section 5.3) domain randomization (Figure 3). Static, as well as adaptive, methods are characterized by randomly sampling a set of domain parameters ξ~p(ξ) at the beginning of Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893 each simulated rollout. A randomization scheme is categorized as adaptive if the domain parameter distribution is updated during learning, otherwise the scheme is called static. The main advantage of adaptive schemes is that they alleviate the need for hand-tuning the distributions of the domain parameters, which is currently a decisive part of the hyper-parameter search in a static scheme. Nonetheless, the prior distributions still demand design decisions. On the downside, every form of adaptation requires data from the target domain, typically the real robot, which is significantly more expensive to obtain. Another approach for learning robust policies in simulation is to apply adversarial disturbances during the training process. We classify these perturbations as a form of domain randomization, since they either depend on a highly stochastic adversary learned jointly with the policy, or directly contain a random process controlling the application of the perturbation. Adversarial approaches may yield exceptionally robust control strategies. However, without any further restrictions, it is always possible to create scenarios in which the protagonist agent can never win, i.e., the policy can not learn the task. Balancing the adversary's power is pivotal to an adversarial domain randomization method, adding a sensitive hyperparameter.</p>
<p>Another way to distinguish domain randomization concepts is the representation of the domain parameter distribution. The vast majority of algorithms assume a specific probability distribution (e.g., normal or uniform) independently for every parameter. This modeling decision has the benefit of greatly reducing the complexity, but at the same time severely limits the expressiveness. Novel LFI methods (Section 5.2) estimate the complete posterior, hence allow the recognition of correlations between the domain parameters, multi-modality, and skewness.</p>
<p>Static Domain Randomization</p>
<p>Approaches that sample from a fixed domain parameter distribution typically aim at performing sim-to-real transfer without using any real-world data ( Figure 4). Since running the policy on a physical device is generally the most difficult and time-consuming part, static approaches promise quick and relatively easy to obtain results. In terms of final policy performance in the target domain, these methods are usually inferior to those that adapt the domain parameter distribution. Nevertheless, static domain randomization has bridged the reality gap in several cases.</p>
<p>Randomizing Dynamics Without Using Real-World Data at Runtime</p>
<p>More than a decade ago, Wang et al. (2010) proposed to randomize the simulator in which the training data is generated. The authors examined the randomization of initial states, external disturbances, goals, and actuator noise, clearly showing an improved robustness of the learned locomotion controllers in simulated experiments (sim-to-sim). Mordatch et al. (2015) used a finite model ensembles to run (offline) trajectory optimization on a small-scale humanoid robot, achieving one of the first sim-to-real transfers in robotics powered by domain randomization. Similarly, Lowrey et al. (2018) employed the Natural Policy Gradient (Kakade, 2001) to learn a continuous controller for a three-finger positioning task, after carefully identifying the system's parameters. Conforming with Mordatch et al. (2015), their results showed that the policy learned from the identified model was able to perform the sim-to-real transfer, but the policies learned from an ensemble of models was more robust to modeling errors. In contrast, Peng et al. (2018) combined model-free RL with recurrent NN policies that were trained using hindsight experience replay (Andrychowicz et al., 2017) in order to push an object by controlling a robotic arm. Tan et al. (2018) presented an example for learning quadruped gaits from randomized simulations, where particular efforts were made to conduct a prior system identification. They empirically found that sampling domain parameters from a uniform distribution together with applying random forces and regularizing the observation space can be enough to cross the reality gap. For quadrotor control, Molchanov et al. (2019) trained feedforward NN policies which generalize over different physical drones. The suggested randomization includes a custom model for motor lag and noise based on an Ornstein-Uhlenbeck process. Rajeswaran et al. (2017) explored the use of a risk-averse objective function, optimizing a lower quantile of the return. The method was only evaluated on simulated MuJoCo tasks, however it was also one of the first methods that draws upon the Bayesian perspective. Moreover, this approach was employed as a baseline by Muratore et al. (2021b), who introduced a measure for the inter-domain transferability of controllers together with a risk-neutral randomization scheme. The resulting policies have the unique feature of providing a (probabilistic) guarantee on the estimated transferability and managed to directly transfer to the real platform in two different experiments. Siekmann et al. (2021) achieved the sim-to-real transfer of a recurrent NN policy for bipedal walking. The policy was trained using model-free RL in simulation with uniformly distributed dynamics parameters as well as randomized task-specific terrain. According to the authors, the recurrent architecture and the terrain randomization were pivotal.</p>
<p>Randomizing Dynamics Using Real-World Data at Runtime</p>
<p>The work by Cully et al. (2015) can be seen as both static and adaptive domain randomization, where a large set of hexapod locomotion policies is learned before execution on the physical robot, and subsequently evaluated in simulation. Every policy is associated with one configuration of the so-called behavioral descriptors, which can be interpreted as domain parameters. Instead of retraining or fine-tuning, the proposed algorithm Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893 reacts to performance drops, e.g., due to damage, by querying Bayesian Optimization (BO) to sequentially select one of the pretrained policies and measure its performance on the robot. Instead of randomizing the simulator parameters, Cutler and How (2015) explored learning a probabilistic model, chosen to be a GP, of the environment using data from both simulated and real-world dynamics. A key feature of this method is to incorporate the simulator as a prior for the probabilistic model, and subsequently use this information of the policy updates with PILCO (Deisenroth and Rasmussen, 2011). The authors demonstrated policy transfer for a inverted pendulum task. In follow-up work, Cutler and How (2016) extended the algorithm to make a remote-controlled toy car learn how to drift in circles. Antonova et al. (2019) propose a sequential Variational AutoEncoder (VAE) to embed trajectories into a compressed latent space which is used with BO to search for controllers. The VAE and the domain-specific high-level controllers are learned jointly, while the randomization scheme is left unchanged. Leveraging a custom kernel which measures the KL divergence between trajectories and the data efficiency of BO, the authors report successful sim-to-real transfers after 10 target domain trials for a hexapod locomotion task as well as 20 trials for a manipulation task. Kumar et al. (2021) learned a quadruped locomotion policy that passed joint positions to a lower level PD controller without using any real-wold data. The essential components of this approach are the encoder that projects the domain parameters to a latent space and the adaption module which is trained to regress the latent state from the recent history of measured states and actions. The policy is conditioned on the current state, the previous actions, and the latent state which needs to be reconstructed during deployment in the physical world. Emphasizing the importance of the carefully engineered reward function, the authors demonstrate the method's ability to transfer from simulation to various outdoor terrains.</p>
<p>Randomizing Visual Appearance and Configurations</p>
<p>Tobin et al. (2017) learned an object detector for robot grasping using a fixed domain parameter distribution, and bridged the gap with a deep NN policy trained exclusively on simulated RGB images. Similarly, James et al. (2017) added various distracting shapes as well as structured noise (Perlin, 2002) when learning a robot manipulation task with an end-to-end controller that mapped pixels to motor velocities. The approach presented by Pinto et al. (2018) combines the concepts of static domain randomization and actor-critic training , enabling the direct sim-to-real transfer of the abilities to pick, push, or move objects. While the critic has access to the simulator's full state, the policy only receives images of the environment, creating an information asymmetry. Matas et al. (2018) used the asymmetric actor-critic idea from Pinto et al. (2018) as well as several other improvements to train a deep NN policy end-to-end, seeded with prior demonstrations. Solving three variations of a tissue folding task, this work scales sim-toreal visuomotor manipulation to deformable objects. Purely visual domain randomization has also been applied to aerial robotics, where Sadeghi and Levine (2017) achieved sim-to-real transfer for learning to fly a drone through indoor environments. The resulting deep NN policy was able to map from monocular images to normalized 3D drone velocities. Similarly, Polvara et al. (2020) demonstrated landing of a quadrotor trained in end-toend fashion using randomized environments. Dai et al. (2019) investigated the effect of domain randomization on visuomotor policies, and observed that this leads to more redundant and entangled representations accompanied with significant statistical changes in the weights. Yan et al. (2020) apply Model Predictive Control (MPC) to manipulate of deformable objects using a forward model based on visual input. The novelty of this approach is that the predictive model is trained jointly with an embedding to minimizing a contrastive loss (van den Oord et al., 2018) in the latent space. Finally, domain randomization was applied to transfer the behavior from simulation to the real robot.</p>
<p>Randomizing Dynamics, Randomizing Visual Appearance, and Configurations</p>
<p>Combining Generative Adversarial Networks (GANs) and domain randomization, Bousmalis et al. (2018) greatly reduced the number of necessary real-world samples for learning a robotic grasping task. The essence of their method is to transform simulated monocular RGB images in a way that is closely matched to the real counterpart. Extensive evaluation on the physical robot showed that domain randomization as well as the suggested pixel-level domain adaptation technique were important to successfully transfer. Despite the pixel-level domain adaptation technique being learned, the policy optimization in simulation is done with a fixed randomization scheme. In related work James et al. (2019) train a GAN to transform randomized images to so-called canonical images, such that a corresponding real image would be transformed to the same one. This approach allowed them to train purely from simulated images, and optionally fine-tune the policy on target domain data. Notably, the robotic in-hand manipulation conducted by OpenAI et al. (2020) demonstrated that domain randomization in combination with careful model engineering and the usage of recurrent NNs enables sim-to-real transfer on an unprecedentedly difficulty level.</p>
<p>Adaptive Domain Randomization</p>
<p>Static domain randomization (Section 5.1) is inherently limited and implicitly assumes knowledge of the true mean of the domain parameters or accepts biased samples ( Figure 5). Adapting the randomization scheme allows the training to narrow or widen the search distribution in order to fulfill one or multiple criteria Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893 which can be chosen freely. The mechanism devised for updating the domain parameter distribution as well as the procedure to collect meaningful target domain data are typically the center piece of adaptive randomization algorithms. In this process the execution of intermediate policies on the physical device is the most likely point of failure. However, approaches that update the distribution solely based on data from the source domain are less flexible and generally less effective.</p>
<p>Conditioning Policies on the Estimated Domain Parameters</p>
<p>Yu et al. (2017) suggested the use of a NN policy that is conditioned on the state and the domain parameters. Since these parameters are not assumed to be known, they have to be estimated, e.g., with online system identification. For this purpose, a second NN is trained to regress the domain parameters from the observed rollouts. By applying this approach to simulated continuous control tasks, the authors showed that adding the online system identification module can enable an adaption to sudden changes in the environment. In subsequent research, Yu et al. (2019a) intertwined policy optimization, system identification, and domain randomization. The proposed method first identifies bounds on the domain parameters which are later used for learning from the randomized simulator. In a departure from their previous approach, the policy is conditioned on a latent space projection of the domain parameters. After training in simulation, a second system identification step runs BO for a fixed number of iterations to find the most promising projected domain parameters. The algorithm was evaluated on sim-to-real bipedal robot walking. Mozifian et al. (2020) also introduce a dependence of the policy w.r.t. to the domain parameters. These are updated by gradient ascent on the average return over domains, regularized by a penalty proportional to the KL divergence. Similar to Ruiz et al. (2019), the authors update the domain parameter distribution using the score function gradient estimator. Mozifian et al. (2020) tested their method on sim-to-sim robot locomotion tasks. It remains unclear whether this approach scales to sim-to-real scenarios since the adaptation is done based on the return obtained in simulation, thus is not physically grounded. Bootstrapping from pre-recorded motion capture data of animals, Peng et al. (2020) learned quadruped locomotion skills with a synthesis of imitation learning, domain randomization, and domain adaptation (Section 4.3). The introduced method is conceptually related to the approach of Yu et al. (2019b), but adds an information bottleneck. According to the authors, this bottleneck is necessary because without it, the policy has access to the underlying dynamics parameters and becomes overly dependent on them, which leads to brittle behavior. To avoid this overfitting, Peng et al. (2020) limit the mutual information between the domain parameters and their encoding, realized as penalty on the KL divergence from a zero-mean Gaussian prior on the latent variable.</p>
<p>The Bilevel Optimization Perspective</p>
<p>Muratore et al. (2021a) formulated adaptive domain randomization as a bilevel optimization that consists of an upper and a lower level problem. In this framework, the upper level is concerned with finding the domain parameter distribution, which when used for training in simulation leads to a policy with maximal real-world return. The lower level problem seeks to find a policy in the current randomized source domain. Using BO for the upper level and model-free RL for the lower level, Muratore et al. (2021a) compare their method in two underactuated sim-to-real robotic tasks against two baselines. Picturing the real-world return analogous to the probability for optimality, this approach reveals parallels to control as inference (Rawlik et al., 2012;Levine and Koltun, 2013;Watson et al., 2021), where the control variates are the parameters of the domain distribution. BO has also been employed by Paul et al. (2019) to adapt the distribution of domain parameters such that using these for the subsequent training maximizes the policy's return. Their method models the relation between the current domain parameters, the current policy and the return of the updated policy with a GP. Choosing the domain parameters that maximize the return in simulation is critical, since this creates the possibility to adapt the environment such that it is easier for the agent to solve. This design decision requires the policy parameters to be fed into the GP which is prohibitively expensive if the full set of parameters are used. Therefore, abstractions of the policy, so-called fingerprints, are created. These handcrafted features, e.g., a Gaussian approximation of the stationary state distribution, replace the policy to reduce the input dimension. Paul et al. (2019) tested the suggested algorithm on three sim-to-sim tasks, focusing on the handling of so-called significant rare events. Embedding the domain parameters into the mean function of a GP which models the system dynamics, Chatzilygeroudis and Mouret (2018) extended a black-box policy search algorithm (Chatzilygeroudis et al., 2017) with a simulator as prior. The approach explicitly searches for parameters of the simulator that fit the real-world data in an upper level loop, while optimizing the GP's hyper-parameters in a lower level loop. This method allowed a damage hexapod robot to walk in less than 30 s. Ruiz et al. (2019) proposed a meta-algorithm which is based on a bilevel optimization problem and updates the domain parameter distribution using REINFORCE (Williams, 1992). The approach has been evaluated in simulation on synthetic data, except for a semantic segmentation task. Thus, there was no dynamics-dependent interaction of the learned policy with the real world. Mehta et al. (2019) also formulated the adaption of the domain parameter distribution as an RL problem where different simulation instances are sampled and compared against a reference environment based on the resulting trajectories. This comparison is done by a discriminator which yields rewards proportional to the difficulty of distinguishing the simulated and real environments, hence providing an incentive to generate distinct domains. Using this reward signal, the domain parameters of the simulation instances are updated via Stein Variational Policy Gradient . Mehta et al. (2019) evaluated their method in a sim-to-real experiment where a robotic arm had to reach a desired point. In contrast, Chebotar et al. (2019) presented a trajectory-based framework for closing the reality gap, and validated it on two sim-to-real Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893 robotic manipulation tasks. The proposed procedure adapts the domain parameter distribution's parameters by minimizing discrepancy between observations from the real-world system and the simulation. To measure the discrepancy, Chebotar et al.</p>
<p>(2019) use a linear combination of the L 1 and L 2 norm between simulated and real trajectories. These values are then plugged in as costs for Relative Entropy Policy Search (REPS) (Peters et al., 2010) to update the simulator's parameters, hence turning the simulator identification into an episodic RL problem. The policy optimization was done using Proximal Policy Optimization (PPO) (Schulman et al., 2017), a step-based model-free RL algorithm.</p>
<p>Removing Restrictions on the Domain Parameter Distribution</p>
<p>Ramos et al. (2019) perform a fully Bayesian treatment of the simulator's parameters by employing Likelihood-Free Inference (LFI) with a Mixture Density Network (MDN) as model for the density estimator. Analyzing the obtained posterior over domain parameters, they showed that the proposed method is, in a sim-tosim scenario, able to simultaneously infer different parameter configurations which can explain the observed trajectories. An evaluation over a gird of domain parameters confirms that the policies trained with the inferred posterior are more robust model uncertainties.</p>
<p>The key benefit over previous approaches is that the domain parameter distribution is not restricted to belong to a specific family, e.g., normal or uniform. Instead, the true posterior is approximated by the density estimator, fitted using LFI (Papamakarios and Murray, 2016). In follow-up work, Possas et al. (2020) addressed the problem of learning the behavioral policies which are required for the collection of target domain data. By describing the integration policy optimization via modelfree RL, the authors created an online variant of the original method.</p>
<p>The sim-to-real experiments were carried out using MPC where (only) the model parameters are updated based on the result from the LFI routine. Matl et al. (2020) scaled the Bayesian inference procedure of Ramos et al. (2019) to the simulation of granular media, estimating parameters such as friction and restitution coefficients. Barcelos et al. (2020) presented a method that interleaves domain randomization, LFI, and policy optimization. The controller is updated via nonlinear MPC while using the unscented transform to simulate different domain instances for the control horizon. Hence, this algorithm allows one to calibrate the uncertainty as the system evolves with the passage of time, attributing higher costs to more uncertain paths. For performing the essential LFI, the authors build upon the work of Ramos et al. (2019) to identify the posterior domain parameters, which are modeled by a mixture of Gaussians. The approach was validated on a simulated inverted pendulum swing-up task as well as a real trajectory following task using a wheeled robot. Since the density estimation problem is the center piece of LFI-based domain randomization, improving the estimator's flexibility is of great interest. Muratore et al. (2021c) employed a sequential neural posterior estimation algorithm (Greenberg et al., 2019) which uses normalizing flows to estimate the (conditional) posterior over simulators. In combination with a segment-wise synchronization between the simulations and the recorded real-world trajectories, Muratore et al. (2021c) demonstrated the neural inference method's ability to learn the posterior belief over contact-rich black-box simulations. Moreover, the proposed approach was evaluated with policy optimization in the loop on an underactuated swing-up and balancing task, showing improved results compared to BayesSim  as well as Bayesian linear regression.</p>
<p>Adversarial Domain Randomization</p>
<p>Extensive prior studies have shown that deep NN classifiers are vulnerable to imperceptible perturbations their inputs, obtained via adversarial optimization, leading to significant drops in accuracy (Szegedy et al., 2014;Fawzi et al., 2015;Goodfellow et al., 2015;Kurakin et al., 2017;Ilyas et al., 2019). This line of research has been extended to reinforcement learning, showing that small (adversarial) perturbations are enough to significantly degrade the policy performance (Huang et al., 2017). To defend against such attacks, the training data can be augmented with adversarially-perturbed examples, or the adversarial inputs can be detected and neutralized at test-time ( Figure 6). However, studies of existing defenses have shown that adversarial examples are harder to detect than originally believed (Carlini and Wagner, 2017). It is safe to assume that this insight gained from computer vision problems transfers to the RL setting, on which we focus here. Mandlekar et al. (2017) proposed physically plausible perturbations by randomly deciding when to add a scaled gradient of the expected return w.r.t. the state. Their sim-tosim evaluation on four MuJoCo tasks showed that agents trained with the suggested adversarial randomization generalize slightly better to domain parameter configurations than agents trained with a static randomization scheme. Lutter et al. (2021a) derived the optimal policy together with different optimal disturbances from the value function in a continuous state, action, and time RL setting. Despite outstanding sim-to-real transferability of the resulting policies, the presented approach is conceptually restricted by assuming access to a compact representation of the state domain, typically obtained through exhaustive sampling, which hinders the scalability to high-dimensional tasks.</p>
<p>Adversary Available Analytically</p>
<p>Adversary Learned via Two-Player Games</p>
<p>Domain randomization can be described using a game theoretic framework. Focusing on two-player games for model-based RL, Rajeswaran et al. (2020) define a "policy player" which maximizes rewards in the learned model and a "model player" which minimizes prediction error of data collected by policy player. This formulation can be transferred to the sim-to-real scenario in different ways. One example is to make the "policy player" modelagnostic and to let the "model player" control the domain parameters. Pinto et al. (2017) introduced the idea of a second agent whose goal it is to hinder the first agent from fulfilling its task. This adversary has the ability to apply force disturbances at predefined locations of the robot's body, while the domain parameters remain unchanged. Both agents are trained in alternation using RL make this a zero-sum game. Similarly, Zhang et al. (2021) aim to train an agent using adversarial examples such that it becomes robust against test-time attacks. As in the approach presented by Pinto et al. (2017), the adversary and the protagonist are trained alternately until convergence at every meta-iteration. Unlike prior work, Zhang et al. (2021) build on state-adversarial MDPs manipulating the observations but not the simulation state. Another key property of their approach is that the perturbations are applied after a projection to a bounded set. The proposed observation-based attack as well as training algorithm is supported by four sim-to-sim validations in MuJoCo environments. Jiang et al. (2021) employed GANs to distinguish between source and target domain dynamics, sharing the concept of a learned domain discriminator with Mehta et al. (2019). Moreover, the authors proposed to augment an analytical physics simulator with a NN that is trained to maximize the similarity between simulated and real trajectories, turning the identification of the hybrid simulator into an RL problem. The comparison on a sim-to-real quadruped locomotion task showed an advantage over static domain randomization baselines. On the other hand, this method added noise to the behavioral policy in order to obtain diverse target domain trajectories for the simulator identification, which can be considered dangerous.</p>
<p>DISCUSSION AND OUTLOOK</p>
<p>To conclude this review, we discuss practical aspects of choosing among the existing domain randomization approaches (Section 6.1), emphasizing that sim-to-real transfer can also be achieved without randomizing (Section 6.2). Finally, we sketch out several promising directions for future sim-to-real research (Section 6.3).</p>
<p>Choosing a Suitable Domain Randomization Approach</p>
<p>Every publication on sim-to-real robot learning presents an approach that surpasses its baselines. So, how should we select the right algorithm given a task? Up to now, there is no benchmark for sim-to-real methods based on the policy's target domain performance, and it is highly questionable if such a comparison could be fair, given that these algorithms have substantially different requirements and goals. The absence of one common benchmark is not necessarily bad, since bundling a set of environments to define a metric would bias research to pursue methods which optimize solely for that metric. A prominent example for this mechanism is the OpenAI Gym (Brockman et al., 2016), which became the de facto standard for RL. Contrarily, a similar development for sim-to-real research is not desirable since the overfitting to a small set of scenarios would be detrimental to the desired transferability and the vast amount of other scenarios. When choosing from the published algorithms, the practitioner is advised to check if the approach has been tested on at least two different sim-to-real tasks, and if the (sometimes implicit) assumptions can be met. Adaptive domain randomization methods, for example, will require operating the physical device in order to collect real-world data. After all, we can expect that approaches with randomization will be more robust than the ones only trained on a nominal model. This has been shown consistently (Section 5). However, we can not expect that these approaches work out of the box on novel problems without adjusting the hyper-parameters. Another starting point could be the set of sim-to-sim benchmarks released by Mehta et al. (2020), targeting the problem of system identification for state-of-the-art domain randomization algorithms.</p>
<p>Sim-To-Real Transfer Without Domain Randomization</p>
<p>Domain randomization is one way to successfully transfer control policies learned in simulation to the physical device, but by no means the only way.</p>
<p>Action Transformation</p>
<p>In order to cope with the inaccuracies of a simulator, Christiano et al. (2016) propose to train a deep inverse dynamics model to map the action commanded by policy to a transformed action. When applying the original action to the real system and the transformed action to the simulated system, they would lead to the same next robot state, thus bridging the reality gap. To generate the data for training the inverse dynamics model, preliminary policies are augmented with hand-tuned exploration noise and executed in the target domain. Their approach is based on the observation that a policy's high-level strategy remains valid after sim-to-real transfer, and assumes that the simulator provides a reasonable estimate of the next state. With the same goal in mind, Hanna and Stone (2017) suggest an action transformation that is learned such that applying the transformed actions in simulation has the same effects as applying the original actions had on the real system. At the core approach is the estimation of neural forward and inverse models based on rollouts executed with the real robot.</p>
<p>Novel Neural Policy Architectures</p>
<p>Rusu et al. (2017) employ a progressively growing NN architecture (Rusu et al., 2016b) to learn an end-to-end approach mapping from pixels to discretized joint velocities. This NN framework enables the reuse of previously gained knowledge as well as the adaptation to new input modalities. The first part of the NN policy is trained in simulation, while the part added when transferring needs to be trained using real-world data. For a relatively simple reaching task, the authors reported requiring approximately 4 h of runtime on the physical robot.  Kaspar et al. (2020) propose to combine operational space control and RL. After carefully identifying the simulator's parameters, the RL agent learns to control the end-effector via forces on a unit mass-spring-damper system. The constrains and nullspace behavior are abstracted away from the agent, making the RL problem easier and the policy more transferable.</p>
<p>Promising Future Research Directions</p>
<p>Learning from randomized simulations still offers abundant possibilities to enable or improve the sim-to-real transfer of control policies. In the following section, we describe multiple opportunities for future work in this area of research.</p>
<p>Real-To-Sim-To-Real Transfer</p>
<p>Creating randomizable simulation environments is timeintensive, and the initial guesses for the domain parameters as well as their variances are typically very inaccurate. It is of great interest to automate this process grounded by real-world data.</p>
<p>One viable scenario could be to record an environment with a RGBD camera, and subsequently use the information to reconstruct the scene. Moreover, the recorded data can be processed to infer the domain parameters, which then specifies the domain parameter distributions. When devising such a framework, we could start from prior work on 3D scene reconstruction Kolev et al. (2009), Haefner et al. (2018 as well as methods to estimate the degrees of freedom for rigid bodies (Martin-Martin and Brock, 2014). A data-based automatic generation of simulation environments (real-to-sim-to-real) not only promises to reduce the workload, but would also yields a meaningful initialization for domain distribution parameters.</p>
<p>Policy Architectures With Inductive Biases</p>
<p>tDeep NNs are by far the most common policy type, favored because of their flexibility and expressiveness. However, they are also brittle w.r.t. changes in their inputs (Szegedy et al., 2014;Goodfellow et al., 2015;Huang et al., 2017). Due to the inevitable domain shift in sim-to-real scenarios this input sensitivity is magnified. The success of domain randomization methods for robot learning can largely be attributed to their ability of regularizing deep NN policies by diversifying the training data. Generally, one may also introduce regularization to the learning by designing alternative models for the control policies, e.g., linear combination of features and parameters, (time varying) mixtures of densities, or movement primitives. All of these have their individual strengths and weaknesses. We believe that pairing the expressiveness of deep NNs with physically-grounded prior knowledge leads to controllers that achieve high performance and suffer less from transferring to the real world, since they are able to bootstrap from their prior. There are multiple ways to incorporate abstract knowledge about physics. We can for example restrict the policy to obey stable system dynamics derived from first principles (Greydanus et al., 2019;Lutter et al., 2019). Another approach is to design the model class such that the closed-loop system is passive for all parameterizations of the learned policy, thus guaranteeing stability in the sense of Lyapunov as well as bounded output energy given bounded input energy (Brogliato et al., 2007;Yang et al., 2013;Dai et al., 2021). All these methods would require significant exploration in the environment, making it even more challenging to learn successful controllers in the real-world directly. Leveraging randomized simulation is likely going to be a critical component in demonstrating solving sequential problems on real robots.</p>
<p>Towards Dual Control via Neural Likelihood-Free Inference</p>
<p>Continuing the direction of adaptive domain randomization, we are convinced that neural LFI powered by normalizing flows are auspicious approaches. The combination of highly flexible density estimators with widely applicable and sampleefficient inference methods allows one to identify multimodal distributions over simulators with very mild assumptions Barcelos et al., 2021;Muratore et al., 2021c). By introducing an auxiliary optimality variable and making the policy parameters subject to the inference, we obtain the posterior over policies quantifying their likelihood of being optimal. While this idea is well-known in the control-as-inference community (Rawlik et al., 2012;Levine and Koltun, 2013;Watson et al., 2021), prior methods were limited to less powerful density estimation procedures. Taking this idea one step further, we could additionally include the domain parameters for inference, and thereby establish connections to dual control (Feldbaum, 1960;Wittenmark, 1995).</p>
<p>Accounting for the Cost of Information Collection</p>
<p>Another promising direction for future research is the combination of simulated and real-world data collection with explicit consideration of the different costs when sampling from the two domains, subject to a restriction of the overall computational budget. One part of this problem was already addressed by Marco et al. (2017), showing how simulation can be used to alleviate the need for real-world samples when finding a set of policy parameters. However, the question of how to schedule the individual (simulated or real) experiments and when to stop the procedure, i.e., when does the cost of gathering information exceed its expected benefit, is not answered for sim-to-real transfer yet. This question relates to the problems of optimal stopping (Chow and Robbins, 1963) as well as multi-fidelity optimization (Forrester et al., 2007), and can be seen as a reformulation thereof in the context of simulationbased learning.</p>
<p>Solving Sequential Problems</p>
<p>The problem settings considered in the overwhelming majority of related publications, are (continuous) control tasks which do not have a sequential nature. In contrast, most real-world tasks such as the ones posed at the DARPA Robotics Challenge (Krotkov et al., 2017) consist of (disconnected) segments, e.g., a robot needs to turn the knob before it can open a door. One possible way to address these more complicated tasks is by splitting the control into high and low level policies, similar to the options framework (Sutton et al., 1999). The higher level policy is trained to orchestrate the lowlevel policies which could be learned or fixed. Existing approaches typically realize this with discrete switches between the low-level policies, leading to undesirable abrupt changes in the behavior. An alternative would be a continuous blending of policies, controlled by a special kind of recurrent NN which has originally been proposed by Amari (1977) to model activities in the human brain. Used as policy architectures they can be constructed to exhibit asymptotically stable nonlinear dynamics (Kishimoto and Amari, 1979). The main benefits of this structure are its easy interpretability via exhibition and inhibition of neural potentials, as well as the relatively low number of parameters necessary to create complex and adaptive behavior. A variation of this idea with hand-tuned parameters, i.e., without machine learning, has been applied by Luksch et al. (2012) to coordinate the activation pre-defined movement primitives.</p>
<p>SELECTION OF REFERENCES</p>
<p>We chose the references based on multiple criteria: 1) Our primary goal was to covering all milestones of the sim-to-real research for robotics. 2) In the process, we aimed at diversifying over subfields and research groups. 3) A large proportion of papers came to our attention by running Google Scholar alerts on "sim-to-real" and "reality gap" since 2017. 4) Another source were reverse searches starting from highly influential publications. 5) Some papers came to our attention because of citation notifications we received on our work. 6) Finally, a few of the selected publications are recommendations from reviewers, colleagues, or researchers met at conferences. 7) Peer-reviewed papers were strongly preferred over pre-prints. </p>
<p>AUTHOR CONTRIBUTIONS</p>
<p>FIGURE 2 |
2Topological overview of the sim-to-real research and a selection of related fields.Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893</p>
<p>FIGURE 3 |
3Topological overview of domain randomization methods.</p>
<p>FIGURE 4 |
4Conceptual illustration of static domain randomization.</p>
<p>FIGURE 5 |
5Conceptual illustration of adaptive domain randomization.</p>
<p>FIGURE 6 |
6Conceptual illustration of adversarial domain randomization. Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893</p>
<dl>
<dt>FM</dt>
<dd>main author; FR: added and edited text, suggested publications, proofread; GT: added and edited text, suggested publications, proofread; WY: added and edited text, suggested publications, proofread; MG: edited text, proofread, (Ph.D. supervisor of FM); JP: added and edited text, suggested publications, proofread, (Ph.D. supervisor of FM).FUNDING FM gratefully acknowledges the financial support from Honda Research Institute Europe. JP received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 640554. WY and GT have been supported by NSF award IIS-1514258.</dd>
</dl>
<p>Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893 6.2.3 Identifying and Improving the Simulator Xie et al. (2019) describe an iterative process including motion tracking, system identification, RL, and knowledge distillation, to learn control policies for humanoid walking on the physical system. This way, the authors can rely on known building blocks resulting in initial and intermediate policies which are reasonably safe to execute. To run a policy on the real robot while learning without the risk of damaging or stopping the device,
Muratore et al.  Robot Learning From Randomized Simulations <br />
The authors declare that this study received funding from the Honda Research Institute Europe. The funder had the following involvement in the study: the structuring and improvement of this article jointly with the authors, and the decision to submit it for publication.Conflict of Interest: Author FM was employed by the Technical University of Darmstadt in collaboration with the Honda Research Institute Europe. Author FR was employed by NVIDIA. Author WY was employed by Google. Author MG was employed by the Honda Research Institute Europe.The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.
. H Abdulsamad, T Dorau, B Belousov, J Zhu, J Peters, Abdulsamad, H., Dorau, T., Belousov, B., Zhu, J., and Peters, J. (2021).</p>
<p>Distributionally Robust Trajectory Optimization under Uncertain Dynamics via Relative-Entropy Trust Regions. arXiv 2103.15388Distributionally Robust Trajectory Optimization under Uncertain Dynamics via Relative-Entropy Trust Regions. arXiv 2103.15388</p>
<p>Benchmarking Domain Randomisation for Visual Sim-To-Real Transfer. R Alghonaim, E Johns, arXiv 2011.07112Alghonaim, R., and Johns, E. (2020). Benchmarking Domain Randomisation for Visual Sim-To-Real Transfer. arXiv 2011.07112</p>
<p>Tunenet: One-Shot Residual Tuning for System Identification and Sim-To-Real Robot Task Transfer. A Allevato, E S Short, M Pryor, A Thomaz, Conference on Robot Learning (CoRL). Osaka, Japan100PMLR)Allevato, A., Short, E. S., Pryor, M., and Thomaz, A. (2019). Tunenet: One-Shot Residual Tuning for System Identification and Sim-To-Real Robot Task Transfer. In Conference on Robot Learning (CoRL), Osaka, Japan, October 30 -November 1 (PMLR), vol. 100 of Proc. Machine Learn. Res., 445-455.</p>
<p>Dynamics of Pattern Formation in Lateral-Inhibition Type Neural fields. S Amari, 10.1007/bf00337259Biol. Cybern. 27Amari, S.-i. (1977). Dynamics of Pattern Formation in Lateral-Inhibition Type Neural fields. Biol. Cybern. 27, 77-87. doi:10.1007/bf00337259</p>
<p>Hindsight Experience Replay. M Andrychowicz, D Crow, A Ray, J Schneider, R Fong, P Welinder, Conference on Neural Information Processing Systems (NIPS). Long Beach, CA, USAAndrychowicz, M., Crow, D., Ray, A., Schneider, J., Fong, R., Welinder, P., et al. (2017). "Hindsight Experience Replay," in Conference on Neural Information Processing Systems (NIPS), December 4-9 (Long Beach, CA, USA, 5048-5058.</p>
<p>Learning Dexterous In-Hand Manipulation. O M Andrychowicz, B Baker, M Chociej, R Józefowicz, B Mcgrew, J Pachocki, 10.1177/0278364919887447Int. J. Robotics Res. 39Andrychowicz, O. M., Baker, B., Chociej, M., Józefowicz, R., McGrew, B., Pachocki, J., et al. (2020). Learning Dexterous In-Hand Manipulation. Int. J. Robotics Res. 39, 3-20. doi:10.1177/0278364919887447</p>
<p>Bayesian Optimization in Variational Latent Spaces with Dynamic Compression. R Antonova, A Rai, T Li, D Kragic, Research.100Conference on Robot Learning (CoRL). Osaka, Japanof Proceedings of Machine LearningAntonova, R., Rai, A., Li, T., and Kragic, D. (2019). "Bayesian Optimization in Variational Latent Spaces with Dynamic Compression," in Conference on Robot Learning (CoRL), October 30 -November 1 (Osaka, Japan: PMLR), 456-465. of Proceedings of Machine Learning Research.100</p>
<p>Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning. M Asada, S Noda, S Tawaratsumida, K Hosoda, 10.1023/A:101823700882310.1007/bf00117447Mach. Learn. 23Asada, M., Noda, S., Tawaratsumida, S., and Hosoda, K. (1996). Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning. Mach. Learn. 23, 279-303. doi:10.1023/A:101823700882310.1007/bf00117447</p>
<p>Adaptive Control. 2 edn. K J Åström, B Wittenmark, Dover PublicationsÅström, K. J., and Wittenmark, B. (2008). Adaptive Control. 2 edn. Dover Publications.</p>
<p>Estimation of Inertial Parameters of Manipulator Loads and Links. C G Atkeson, H , C A , An , C H , 10.1177/027836498600500306doi:10.1177/ 027836498600500306Int. J. Robotics Res. 5Atkeson, C. G., H, C. A., and An, C. H. (1986). Estimation of Inertial Parameters of Manipulator Loads and Links. Int. J. Robotics Res. 5, 101-119. doi:10.1177/ 027836498600500306</p>
<p>Emergent Tool Use from Multi-Agent Autocurricula. B Baker, I Kanitscheider, T M Markov, Y Wu, G Powell, B Mcgrew, 26-30OpenReview.net.International Conference on Learning Representations (ICLR)April. Addis Ababa, EthiopiaBaker, B., Kanitscheider, I., Markov, T. M., Wu, Y., Powell, G., McGrew, B., et al. (2020). "Emergent Tool Use from Multi-Agent Autocurricula," in (Addis Ababa, Ethiopia. OpenReview.net.International Conference on Learning Representations (ICLR)April 26-30</p>
<p>Dual Online Stein Variational Inference for Control and Dynamics. L Barcelos, A Lambert, R Oliveira, P Borges, B Boots, F Ramos, 10.15607/RSS.2021.XVII.068Robotics: Science and Systems (RSS). Barcelos, L., Lambert, A., Oliveira, R., Borges, P., Boots, B., and Ramos, F. (2021). "Dual Online Stein Variational Inference for Control and Dynamics," in Robotics: Science and Systems (RSS), July 12-16. Virtual Event. doi:10.15607/RSS.2021.XVII.068</p>
<p>DISCO: Double Likelihood-free Inference Stochastic Control. L Barcelos, R Oliveira, R Possas, L Ott, F Ramos, 10.1109/ICRA40945.2020.9196931International Conference on Robotics and Automation (ICRA). Paris, FranceIEEEBarcelos, L., Oliveira, R., Possas, R., Ott, L., and Ramos, F. (2020). "DISCO: Double Likelihood-free Inference Stochastic Control," in International Conference on Robotics and Automation (ICRA), May 31 -August 31 (Paris, France: IEEE), 10969-10975. doi:10.1109/ICRA40945.2020.9196931</p>
<p>Simulation as an Engine of Physical Scene Understanding. P W Battaglia, J B Hamrick, J B Tenenbaum, 10.1073/pnas.1306572110Proc. Natl. Acad. Sci. 110Battaglia, P. W., Hamrick, J. B., and Tenenbaum, J. B. (2013). Simulation as an Engine of Physical Scene Understanding. Proc. Natl. Acad. Sci. 110, 18327-18332. doi:10.1073/pnas.1306572110</p>
<p>Interaction Networks for Learning about Objects, Relations and Physics. P W Battaglia, R Pascanu, M Lai, D J Rezende, K Kavukcuoglu, Conference on Neural Information Processing Systems (NIPS). Barcelona, SpainBattaglia, P. W., Pascanu, R., Lai, M., Rezende, D. J., and Kavukcuoglu, K. (2016). "Interaction Networks for Learning about Objects, Relations and Physics," in Conference on Neural Information Processing Systems (NIPS), December 5-10 (Barcelona, Spain, 4502-4510.</p>
<p>Assessing Solution Quality in Stochastic Programs. G Bayraksan, D P Morton, 10.1007/s10107-006-0720-xMath. Program. 108Bayraksan, G., and Morton, D. P. (2006). Assessing Solution Quality in Stochastic Programs. Math. Program 108, 495-514. doi:10.1007/s10107-006-0720-x</p>
<p>Adaptive Approximate Bayesian Computation. M A Beaumont, J.-M Cornuet, J.-M Marin, C P Robert, 10.1093/biomet/asp052doi:10.1093/ biomet/asp052Biometrika. 96Beaumont, M. A., Cornuet, J.-M., Marin, J.-M., and Robert, C. P. (2009). Adaptive Approximate Bayesian Computation. Biometrika 96, 983-990. doi:10.1093/ biomet/asp052</p>
<p>Curriculum Learning. Y Bengio, J Louradour, R Collobert, Weston , J , 10.1145/1553374.1553380International Conference on Machine Learning (ICML). Montreal, Quebec, CanadaACMACM International Conference Proceeding SeriesBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). "Curriculum Learning," in International Conference on Machine Learning (ICML), June 14-18 (Montreal, Quebec, Canada: ACM), 41-48. of ACM International Conference Proceeding Series. doi:10.1145/1553374.1553380382</p>
<p>Learning Agile Robotic Locomotion Skills by Imitating Animals. X Bin Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, 10.15607/RSS.2020.XVI.064Robotics: Science and Systems (RSS), Virtual Event/Corvalis. Oregon, USABin Peng, X., Coumans, E., Zhang, T., Lee, T.-W., Tan, J., and Levine, S. (2020). "Learning Agile Robotic Locomotion Skills by Imitating Animals," in Robotics: Science and Systems (RSS), Virtual Event/Corvalis, July 12-16 (Oregon, USA. doi:10.15607/RSS.2020.XVI.064</p>
<p>Resilient Machines through Continuous Self-Modeling. J Bongard, V Zykov, H Lipson, 10.1126/science.1133687Science. 314Bongard, J., Zykov, V., and Lipson, H. (2006). Resilient Machines through Continuous Self-Modeling. Science 314, 1118-1121. doi:10.1126/science. 1133687</p>
<p>Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, 10.1109/ICRA.2018.8460875doi:10. 1109/ICRA.2018.8460875International Conference on Robotics and Automation. Brisbane, Australia: ICRA)Bousmalis, K., Irpan, A., Wohlhart, P., Bai, Y., Kelcey, M., Kalakrishnan, M., et al. (2018). "Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping," in International Conference on Robotics and Automation, May 21-25 (Brisbane, Australia: ICRA), 4243-4250. doi:10. 1109/ICRA.2018.8460875</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, Openai Gym. arXiv 1606.01540Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., et al. (2016). Openai Gym. arXiv 1606.01540</p>
<p>Dissipative Systems Analysis and Control. B Brogliato, B Maschke, R Lozano, O Egeland, 10.1007/978-1-84628-517-2Theor. Appl. 2Brogliato, B., Maschke, B., Lozano, R., and Egeland, O. (2007). Dissipative Systems Analysis and Control. Theor. Appl. 2. doi:10.1007/978-1-84628-517-2</p>
<p>Artificial Life and Real Robots. R A Brooks, European Conference on Artificial Life (ECAL). Paris, FranceBrooks, R. A. (1992). "Artificial Life and Real Robots," in European Conference on Artificial Life (ECAL), December 11-13 (Paris, France, 3-10.</p>
<p>Destilled Domain Randomization, 2112. J Brosseit, B Hahner, F Muratore, M Gienger, J Peters, 3149Brosseit, J., Hahner, B., Muratore, F., Gienger, M., and Peters, J. (2021). Destilled Domain Randomization, 2112, 03149.</p>
<p>Learning Inverse Dynamics Models with Contacts. R Calandra, S Ivaldi, M P Deisenroth, E Rueckert, J Peters, 10.1109/ICRA.2015.7139638International Conference on Robotics and Automation (ICRA). Seattle, WA, USAIEEECalandra, R., Ivaldi, S., Deisenroth, M. P., Rueckert, E., and Peters, J. (2015). "Learning Inverse Dynamics Models with Contacts," in International Conference on Robotics and Automation (ICRA), 26-30 May (Seattle, WA, USA: IEEE), 3186-3191. doi:10.1109/ICRA.2015.7139638</p>
<p>Adversarial Examples Are Not Easily Detected. N Carlini, D Wagner, 10.1145/3128572.3140444Workshop on Artificial Intelligence and Security (AISec). Dallas, TX, USAACMCarlini, N., and Wagner, D. (2017). "Adversarial Examples Are Not Easily Detected," in Workshop on Artificial Intelligence and Security (AISec), November 3 (Dallas, TX, USA: ACM), 3-14. doi:10.1145/3128572.3140444</p>
<p>Using Parameterized Black-Box Priors to Scale up Model-Based Policy Search for Robotics. K Chatzilygeroudis, J.-B Mouret, 10.1109/ICRA.2018.8461083International Conference on Robotics and Automation (ICRA). Brisbane, AustraliaIEEEChatzilygeroudis, K., and Mouret, J.-B. (2018). "Using Parameterized Black-Box Priors to Scale up Model-Based Policy Search for Robotics," in International Conference on Robotics and Automation (ICRA), May 21-25 (Brisbane, Australia: IEEE), 1-9. doi:10.1109/ICRA.2018.8461083</p>
<p>Black-box Data-Efficient Policy Search for Robotics. K Chatzilygeroudis, R Rama, R Kaushik, D Goepp, V Vassiliades, J.-B Mouret, 10.1109/IROS.2017.8202137International Conference on Intelligent Robots and Systems (IROS). Vancouver, BCCanadaIEEEChatzilygeroudis, K., Rama, R., Kaushik, R., Goepp, D., Vassiliades, V., and Mouret, J.-B. (2017). "Black-box Data-Efficient Policy Search for Robotics," in International Conference on Intelligent Robots and Systems (IROS), September 24-28 (Vancouver, BC: CanadaIEEE), 51-58. doi:10.1109/IROS.2017.8202137</p>
<p>A Survey on Policy Search Algorithms for Learning Robot Controllers in a Handful of Trials. K Chatzilygeroudis, V Vassiliades, F Stulp, S Calinon, J.-B Mouret, 10.1109/TRO.2019.2958211IEEE Trans. Robot. 36Chatzilygeroudis, K., Vassiliades, V., Stulp, F., Calinon, S., and Mouret, J.-B. (2020). A Survey on Policy Search Algorithms for Learning Robot Controllers in a Handful of Trials. IEEE Trans. Robot. 36, 328-347. doi:10.1109/TRO.2019.2958211</p>
<p>Closing the Sim-To-Real Loop: Adapting Simulation Randomization with Real World Experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, 10.1109/ICRA.2019.8793789doi:10. 1109/ICRA.2019.8793789International Conference on Robotics and Automation (ICRA). Montreal, QC, CanadaChebotar, Y., Handa, A., Makoviychuk, V., Macklin, M., Issac, J., Ratliff, N., et al. (2019). "Closing the Sim-To-Real Loop: Adapting Simulation Randomization with Real World Experience," in International Conference on Robotics and Automation (ICRA), May 20-24 (Montreal, QC, Canada, 8973-8979. doi:10. 1109/ICRA.2019.8793789</p>
<p>Learning Efficient Object Detection Models with Knowledge Distillation. G Chen, W Choi, X Yu, T X Han, M Chandraker, Conference on Neural Information Processing Systems (NIPS). Long Beach, CA, USAChen, G., Choi, W., Yu, X., Han, T. X., and Chandraker, M. (2017). "Learning Efficient Object Detection Models with Knowledge Distillation," in Conference on Neural Information Processing Systems (NIPS), December 4-9 (Long Beach, CA, USA, 742-751.</p>
<p>On Optimal Stopping Rules. Y S Chow, H Robbins, 10.1007/bf00535296Z. Wahrscheinlichkeitstheorie Verw Gebiete. 2Chow, Y. S., and Robbins, H. (1963). On Optimal Stopping Rules. Z. Wahrscheinlichkeitstheorie Verw Gebiete 2, 33-49. doi:10.1007/bf00535296</p>
<p>P F Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, arXiv 1610.03518Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model. Christiano, P. F., Shah, Z., Mordatch, I., Schneider, J., Blackwell, T., Tobin, J., et al. (2016). Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model. arXiv 1610.03518</p>
<p>Predictable Behavior during Contact Simulation: a Comparison of Selected Physics Engines. S.-J Chung, N Pollard, 10.1002/cav.1712Comp. Anim. Virtual Worlds. 27Chung, S.-J., and Pollard, N. (2016). Predictable Behavior during Contact Simulation: a Comparison of Selected Physics Engines. Comp. Anim. Virtual Worlds 27, 262-270. doi:10.1002/cav.1712</p>
<p>Multi-column Deep Neural Networks for Image Classification. D Ciresan, U Meier, J Schmidhuber, 10.1109/CVPR.2012.6248110Conference on Computer Vision and Pattern Recognition (CVPR). RI, USAIEEE Computer SocietyCiresan, D., Meier, U., and Schmidhuber, J. (2012). "Multi-column Deep Neural Networks for Image Classification," in Conference on Computer Vision and Pattern Recognition (CVPR), June 16-21 (RI, USA: IEEE Computer Society), 3642-3649. doi:10.1109/CVPR.2012.6248110</p>
<p>Quantifying the Reality gap in Robotic Manipulation Tasks. J Collins, D Howard, J Leitner, 10.1109/ICRA.2019.8793591International Conference on Robotics and Automation (ICRA). Montreal, QC, CanadaIEEECollins, J., Howard, D., and Leitner, J. (2019). "Quantifying the Reality gap in Robotic Manipulation Tasks," in International Conference on Robotics and Automation (ICRA), May 20-24 (Montreal, QC, Canada: IEEE), 6706-6712. doi:10.1109/ICRA.2019.8793591</p>
<p>Tiny Differentiable Simulator. E Coumans, Coumans, E. (2020). Tiny Differentiable Simulator. Available at: https://github. com/google-research/tiny-differentiable-simulator.</p>
<p>The Nature of Explanation. K J W Craik, Craik, K. J. W. (1943). The Nature of Explanation.</p>
<p>The Frontier of Simulation-Based Inference. K Cranmer, J Brehmer, G Louppe, 10.1073/pnas.1912789117Proc. Natl. Acad. Sci. USA. 117Cranmer, K., Brehmer, J., and Louppe, G. (2020). The Frontier of Simulation-Based Inference. Proc. Natl. Acad. Sci. USA 117, 30055-30062. doi:10.1073/pnas.1912789117</p>
<p>Knowledge Distillation across Ensembles of Multilingual Models for Low-Resource Languages," in Knowledge distillation across ensembles of multilingual models for low-resource languages. J Cui, B Kingsbury, B Ramabhadran, G Saon, T Sercu, K Audhkhasi, 10.1109/ICASSP.2017.7953073IEEENew Orleans, LA, USACui, J., Kingsbury, B., Ramabhadran, B., Saon, G., Sercu, T., Audhkhasi, K., et al. (2017). "Knowledge Distillation across Ensembles of Multilingual Models for Low-Resource Languages," in Knowledge distillation across ensembles of multilingual models for low-resource languages, March 5-9 (ICASSP, New Orleans, LA, USA: IEEE), 4825-4829. doi:10.1109/ICASSP.2017.7953073</p>
<p>Robots that Can Adapt like Animals. A Cully, J Clune, D Tarapore, J.-B Mouret, 10.1038/nature14422Nature. 521Cully, A., Clune, J., Tarapore, D., and Mouret, J.-B. (2015). Robots that Can Adapt like Animals. Nature 521, 503-507. doi:10.1038/nature14422</p>
<p>Autonomous Drifting Using Simulation-Aided Reinforcement Learning. M Cutler, J P How, 10.1109/ICRA.2016.7487756International Conference on Robotics and Automation (ICRA). Stockholm, SwedenIEEECutler, M., and How, J. P. (2016). "Autonomous Drifting Using Simulation-Aided Reinforcement Learning," in International Conference on Robotics and Automation (ICRA), May 16-21 (Stockholm, Sweden: IEEE), 5442-5448. doi:10.1109/ICRA.2016.7487756</p>
<p>Efficient Reinforcement Learning for Robots Using Informative Simulated Priors. M Cutler, J P How, 10.1109/ICRA.2015.7139550International Conference on Robotics and Automation (ICRA). Seattle, WA, USAIEEECutler, M., and How, J. P. (2015). "Efficient Reinforcement Learning for Robots Using Informative Simulated Priors," in International Conference on Robotics and Automation (ICRA), 26-30 May (Seattle, WA, USA: IEEE), 2605-2612. doi:10.1109/ICRA.2015.7139550</p>
<p>Distilling Policy Distillation. W M Czarnecki, R Pascanu, S Osindero, S M Jayakumar, G Swirszcz, Jaderberg , M , International Conference on Artificial Intelligence and Statistics (AISTATS). Naha, Okinawa89of Proceedings of Machine Learning ResearchCzarnecki, W. M., Pascanu, R., Osindero, S., Jayakumar, S. M., Swirszcz, G., and Jaderberg, M. (2019). "Distilling Policy Distillation," in International Conference on Artificial Intelligence and Statistics (AISTATS) (Naha, Okinawa, Japan16-18 April: PMLR), 1331-1340. of Proceedings of Machine Learning Research.89</p>
<p>Lyapunov-stable Neural-Network Control. H Dai, B Landry, L Yang, M Pavone, R Tedrake, 10.15607/RSS.2021.XVII.063Robotics: Science and Systems (RSS). Dai, H., Landry, B., Yang, L., Pavone, M., and Tedrake, R. (2021). "Lyapunov-stable Neural-Network Control," in Robotics: Science and Systems (RSS), July 12-16. Virtual Event. doi:10.15607/RSS.2021.XVII.063</p>
<p>Analysing Deep Reinforcement Learning Agents Trained with Domain Randomisation. T Dai, K Arulkumaran, S Tukra, F Behbahani, A A Bharath, arXiv 1912.08324Dai, T., Arulkumaran, K., Tukra, S., Behbahani, F., and Bharath, A. A. (2019). Analysing Deep Reinforcement Learning Agents Trained with Domain Randomisation. arXiv 1912.08324</p>
<p>A Differentiable Physics Engine for Deep Learning in Robotics. J Degrave, M Hermans, J Dambre, F Wyffels, 10.3389/fnbot.2019.00006doi:10. 3389/fnbot.2019.00006Front. Neurorobot. 136Degrave, J., Hermans, M., Dambre, J., and Wyffels, F. (2019). A Differentiable Physics Engine for Deep Learning in Robotics. Front. Neurorobot. 13, 6. doi:10. 3389/fnbot.2019.00006</p>
<p>A Survey on Policy Search for Robotics. M P Deisenroth, G Neumann, J Peters, 10.1561/2300000021FNT in Robotics. 2Deisenroth, M. P., Neumann, G., and Peters, J. (2011). A Survey on Policy Search for Robotics. FNT in Robotics 2, 1-142. doi:10.1561/2300000021</p>
<p>PILCO: a Model-Based and Data-Efficient Approach to Policy Search. M P Deisenroth, C E Rasmussen, International Conference on Machine Learning (ICML). Bellevue, Washington, USADeisenroth, M. P., and Rasmussen, C. E. (2011). "PILCO: a Model-Based and Data- Efficient Approach to Policy Search," in International Conference on Machine Learning (ICML), June 28 -July 2 (Bellevue, Washington, USA, 465-472.</p>
<p>Distributionally Robust Optimization under Moment Uncertainty with Application to Data-Driven Problems. E Delage, Ye , Y , 10.1287/opre.1090.0741Operations Res. 58Delage, E., and Ye, Y. (2010). Distributionally Robust Optimization under Moment Uncertainty with Application to Data-Driven Problems. Operations Res. 58, 595-612. doi:10.1287/opre.1090.0741</p>
<p>Why the Law of Effect Will Not Go Away. D C Dennett, 10.1111/j.1468-5914.1975.tb00350.xJ. Theor. Soc. Behav. Dennett, D. C. (1975). Why the Law of Effect Will Not Go Away. J. Theor. Soc. Behav. doi:10.1111/j.1468-5914.1975.tb00350.x</p>
<p>Auto-tuned Sim-To-Real Transfer. Y Du, O Watkins, T Darrell, P Abbeel, D Pathak, arXiv 2104.07662Du, Y., Watkins, O., Darrell, T., Abbeel, P., and Pathak, D. (2021). Auto-tuned Sim- To-Real Transfer. arXiv 2104.07662.</p>
<p>On Contrastive Learning for Likelihood-free Inference. C Durkan, I Murray, Papamakarios , G , Virtual Eventof Proceedings of Machine Learning Research. PMLR119International Conference on Machine Learning (ICML)Durkan, C., Murray, I., and Papamakarios, G. (2020). "On Contrastive Learning for Likelihood-free Inference," in International Conference on Machine Learning (ICML), July 13-18 (PMLR), 2771-2781. Virtual Eventof Proceedings of Machine Learning Research.119</p>
<p>What Does Shaping Mean for Computational Reinforcement Learning?. T Erez, W D Smart, International Conference on Development and Learning (ICDL). Monterey, CA, USAIEEEErez, T., and Smart, W. D. (2008). "What Does Shaping Mean for Computational Reinforcement Learning?," in International Conference on Development and Learning (ICDL) (Monterey, CA, USA: IEEE), 215-219.</p>
<p>Simulation Tools for Model-Based Robotics: Comparison of Bullet, Havok, Mujoco, ODE and Physx. T Erez, Y Tassa, E Todorov, 10.1109/ICRA.2015.7139807International Conference on Robotics and Automation (ICRA). Seattle, WA, USAErez, T., Tassa, Y., and Todorov, E. (2015). "Simulation Tools for Model-Based Robotics: Comparison of Bullet, Havok, Mujoco, ODE and Physx," in International Conference on Robotics and Automation (ICRA), May 26-30 (Seattle, WA, USA, 4397-4404. doi:10.1109/ICRA.2015.7139807</p>
<p>Fundamental Limits on Adversarial Robustness. A Fawzi, O Fawzi, P Frossard, International Conference on Machine Learning (ICML), Workshop on Deep Learning. Fawzi, A., Fawzi, O., and Frossard, P. (2015). "Fundamental Limits on Adversarial Robustness," in International Conference on Machine Learning (ICML), Workshop on Deep Learning.</p>
<p>Constructing Summary Statistics for Approximate Bayesian Computation: Semi-automatic Approximate Bayesian Computation. P Fearnhead, D Prangle, 10.1111/j.1467-9868.2011.01010.xDual Control Theory. I. Avtomatika i Telemekhanika. x Feldbaum, A. A.74J. R. Stat. Soc.Fearnhead, P., and Prangle, D. (2012). Constructing Summary Statistics for Approximate Bayesian Computation: Semi-automatic Approximate Bayesian Computation. J. R. Stat. Soc. 74, 419-474. doi:10.1111/j.1467-9868.2011.01010.x Feldbaum, A. A. (1960). Dual Control Theory. I. Avtomatika i Telemekhanika 21, 1240-1249.</p>
<p>Model-agnostic Meta-Learning for Fast Adaptation of Deep Networks. C Finn, P Abbeel, S Levine, International Conference on Machine Learning. Sydney, NSW, AustraliaFinn, C., Abbeel, P., and Levine, S. (2017). "Model-agnostic Meta-Learning for Fast Adaptation of Deep Networks," in International Conference on Machine Learning, 6-11 August (Sydney, NSW, Australia: ICML), 1126-1135.</p>
<p>Multi-fidelity Optimization via Surrogate Modelling. A I J Forrester, A Sóbester, A J Keane, 10.1098/rspa.2007.1900Proc. R. Soc. A. 463Forrester, A. I. J., Sóbester, A., and Keane, A. J. (2007). Multi-fidelity Optimization via Surrogate Modelling. Proc. R. Soc. A. 463, 3251-3269. doi:10.1098/rspa.2007.1900</p>
<p>Explaining and Harnessing Adversarial Examples. I J Goodfellow, J Shlens, C Szegedy, International Conference on Learning Representations (ICLR). San Diego, CA, USAConference TrackGoodfellow, I. J., Shlens, J., and Szegedy, C. (2015). "Explaining and Harnessing Adversarial Examples," in International Conference on Learning Representations (ICLR), May 7-9 (San Diego, CA, USA. Conference Track.</p>
<p>Frontiers in Robotics and AI | www.frontiersin.org. 9799893Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893</p>
<p>Recasting Gradient-Based Meta-Learning as Hierarchical Bayes. E Grant, C Finn, S Levine, T Darrell, T L Griffiths, International Conference on Learning Representations (ICLR). Vancouver, BC, CanadaConference Track (OpenReview.netGrant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T. L. (2018). "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes," in International Conference on Learning Representations (ICLR), April 30 -May 3, 2018 (Vancouver, BC, Canada. Conference Track (OpenReview.net).</p>
<p>Automatic Posterior Transformation for Likelihood-free Inference. D S Greenberg, M Nonnenmacher, J H Macke, Research.97International Conference on Machine Learning (ICML). Long Beach, California, USAof Proceedings of Machine LearningGreenberg, D. S., Nonnenmacher, M., and Macke, J. H. (2019). "Automatic Posterior Transformation for Likelihood-free Inference," in International Conference on Machine Learning (ICML), 9-15 June (Long Beach, California, USA: PMLR), 2404-2414. of Proceedings of Machine Learning Research.97</p>
<p>Hamiltonian Neural Networks. S Greydanus, M Dzamba, Yosinski , J , Conference on Neural Information Processing Systems (NeurIPS). Vancouver, BC, CanadaGreydanus, S., Dzamba, M., and Yosinski, J. (2019). "Hamiltonian Neural Networks," in Conference on Neural Information Processing Systems (NeurIPS), December 8-14 (Vancouver, BC, Canada, 15353-15363.</p>
<p>S Höfer, K E Bekris, A Handa, J C G Higuera, F Golemo, M Mozifian, Perspectives on Sim2real Transfer for Robotics: A Summary of the R:SS 2020 Workshop. arXiv 2012. 3806Höfer, S., Bekris, K. E., Handa, A., Higuera, J. C. G., Golemo, F., Mozifian, M., et al. (2020). Perspectives on Sim2real Transfer for Robotics: A Summary of the R:SS 2020 Workshop. arXiv 2012.03806.</p>
<p>Fight Ill-Posedness with Ill-Posedness: Single-Shot Variational Depth Superresolution from Shading. B Haefner, Y Queau, T Mollenhoff, D Cremers, 10.1109/CVPR.2018.00025Conference on Computer Vision and Pattern Recognition (CVPR). Salt Lake City, UT, USAIEEE Computer SocietyHaefner, B., Queau, Y., Mollenhoff, T., and Cremers, D. (2018). "Fight Ill- Posedness with Ill-Posedness: Single-Shot Variational Depth Super- resolution from Shading," in Conference on Computer Vision and Pattern Recognition (CVPR), June 18-22 (Salt Lake City, UT, USA: IEEE Computer Society), 164-174. doi:10.1109/CVPR.2018.00025</p>
<p>Grounded Action Transformation for Robot Learning in Simulation. J P Hanna, P Stone, AAAI Conference on Artificial Intelligence. San Francisco, California, USAHanna, J. P., and Stone, P. (2017). "Grounded Action Transformation for Robot Learning in Simulation," in AAAI Conference on Artificial Intelligence, February 4-9 (San Francisco, California, USA, 3834-3840.</p>
<p>NeuralSim: Augmenting Differentiable Simulators with Neural Networks. E Heiden, D Millard, E Coumans, Y Sheng, G S Sukhatme, 10.1109/icra48506.2021.9560935International Conference on Robotics and Automation (ICRA). Xi'an, ChinaHeiden, E., Millard, D., Coumans, E., Sheng, Y., and Sukhatme, G. S. (2021). "NeuralSim: Augmenting Differentiable Simulators with Neural Networks," in International Conference on Robotics and Automation (ICRA), May 30 -June 5 (Xi'an, China. doi:10.1109/icra48506.2021.9560935</p>
<p>J Hermans, V Begy, G Louppe, PMLRof Proceedings of Machine Learning Research.Likelihood-free MCMC with Amortized Approximate Ratio EstimatorsInternational Conference on Machine Learning (ICML). 119Hermans, J., Begy, V., and Louppe, G. (2020)., 119. PMLR, 4239-4248. of Proceedings of Machine Learning Research.Likelihood-free MCMC with Amortized Approximate Ratio EstimatorsInternational Conference on Machine Learning (ICML), Virtual Event13-18 July</p>
<p>G E Hinton, O Vinyals, J Dean, Distilling the Knowledge in a Neural Network. arXiv 1503. 2531Hinton, G. E., Vinyals, O., and Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv 1503.02531.</p>
<p>Chainqueen: A Real-Time Differentiable Physical Simulator for Soft Robotics. Y Hu, J Liu, A Spielberg, J B Tenenbaum, W T Freeman, J Wu, 10.1109/ICRA.2019.8794333International Conference on Robotics and Automation (ICRA). Montreal, QC, CanadaIEEEHu, Y., Liu, J., Spielberg, A., Tenenbaum, J. B., Freeman, W. T., Wu, J., et al. (2019). "Chainqueen: A Real-Time Differentiable Physical Simulator for Soft Robotics," in International Conference on Robotics and Automation (ICRA), May 20-24 (Montreal, QC, Canada: IEEE), 6265-6271. doi:10.1109/ICRA. 2019.8794333</p>
<p>S H Huang, N Papernot, I J Goodfellow, Y Duan, Abbeel , P , International Conference on Learning Representations (ICLR) Toulon. France. OpenReviewAdversarial Attacks on Neural Network Policies, Workshop Track. netHuang, S. H., Papernot, N., Goodfellow, I. J., Duan, Y., and Abbeel, P. (2017). "Adversarial Attacks on Neural Network Policies, Workshop Track," in International Conference on Learning Representations (ICLR) Toulon, April 24-26 (France. OpenReview.net).</p>
<p>Adversarial Examples Are Not Bugs, They Are Features. A Ilyas, S Santurkar, D Tsipras, L Engstrom, B Tran, A Madry, Conference on Neural Information Processing Systems (NeurIPS). Vancouver, BC, CanadaIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. (2019). "Adversarial Examples Are Not Bugs, They Are Features," in Conference on Neural Information Processing Systems (NeurIPS), December 8-14 (Vancouver, BC, Canada, 125-136.</p>
<p>Tools for Simulating Humanoid Robot Dynamics: A Survey Based on User Feedback. S Ivaldi, J Peters, V Padois, F Nori, 10.1109/HUMANOIDS.2014.7041462doi:10.1109/ HUMANOIDS.2014.7041462Tools for simulating humanoid robot dynamics: A survey based on user feedback. Humanoids, Madrid, SpainIEEEIvaldi, S., Peters, J., Padois, V., and Nori, F. (2014). "Tools for Simulating Humanoid Robot Dynamics: A Survey Based on User Feedback," in Tools for simulating humanoid robot dynamics: A survey based on user feedback, November 18-20 (Humanoids, Madrid, Spain: IEEE), 842-849. doi:10.1109/ HUMANOIDS.2014.7041462</p>
<p>Evolutionary Robotics and the Radical Envelope-Of-Noise Hypothesis. N Jakobi, 10.1177/105971239700600205Adaptive Behav. 6Jakobi, N. (1997). Evolutionary Robotics and the Radical Envelope-Of-Noise Hypothesis. Adaptive Behav. 6, 325-368. doi:10.1177/105971239700600205</p>
<p>Noise and the Reality gap: The Use of Simulation in Evolutionary Robotics. N Jakobi, P Husbands, I Harvey, 10.1007/3-540-59496-5_337Advances in Artificial Life. Granada, SpainJakobi, N., Husbands, P., and Harvey, I. (1995). "Noise and the Reality gap: The Use of Simulation in Evolutionary Robotics," in Advances in Artificial Life, June 4-6 (Granada, Spain, 704-720. 704-720. doi:10.1007/3-540-59496-5_337</p>
<p>Transferring End-To-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task. S James, A J Davison, E Johns, Research.78Conference on Robot Learning (CoRL). Mountain View, California, USAof Proceedings of Machine LearningJames, S., Davison, A. J., and Johns, E. (2017). "Transferring End-To-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task," in Conference on Robot Learning (CoRL), November 13-15 (Mountain View, California, USA: PMLR), 334-343. of Proceedings of Machine Learning Research.78</p>
<p>Sim-to-real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, 10.1109/CVPR.2019.01291doi:10. 1109/CVPR.2019.01291Conference on Computer Vision and Pattern Recognition (CVPR). Long Beach, CA, USAComputer Vision Foundation/IEEEJames, S., Wohlhart, P., Kalakrishnan, M., Kalashnikov, D., Irpan, A., Ibarz, J., et al. (2019). "Sim-to-real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks," in Conference on Computer Vision and Pattern Recognition (CVPR), June 16-20 (Long Beach, CA, USA: Computer Vision Foundation/IEEE), 12627-12637. doi:10. 1109/CVPR.2019.01291</p>
<p>Simgan: Hybrid Simulator Identification for Domain Adaptation via. Y Jiang, T Zhang, D Ho, Y Bai, C K Liu, S Levine, Adversarial Reinforcement Learning. arXiv. 21016005Jiang, Y., Zhang, T., Ho, D., Bai, Y., Liu, C. K., Levine, S., et al. (2021). Simgan: Hybrid Simulator Identification for Domain Adaptation via Adversarial Reinforcement Learning. arXiv 2101.06005</p>
<p>M Körber, J Lange, S Rediske, S Steinmann, R Glück, Comparing Popular Simulation Environments in the Scope of Robotics and Reinforcement Learning. arXiv 2103. 4616Körber, M., Lange, J., Rediske, S., Steinmann, S., and Glück, R. (2021). Comparing Popular Simulation Environments in the Scope of Robotics and Reinforcement Learning. arXiv 2103.04616</p>
<p>Sim2real Predictivity: Does Evaluation in Simulation Predict Real-World Performance? IEEE Robot. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, 10.1109/LRA.2020.3013848Autom. Lett. 5Kadian, A., Truong, J., Gokaslan, A., Clegg, A., Wijmans, E., Lee, S., et al. (2020). Sim2real Predictivity: Does Evaluation in Simulation Predict Real-World Performance? IEEE Robot. Autom. Lett. 5, 6670-6677. doi:10.1109/LRA. 2020.3013848</p>
<p>Methods of Reducing Sample Size in Monte Carlo Computations. H Kahn, A W Marshall, 10.1287/opre.1.5.263Or. 1Kahn, H., and Marshall, A. W. (1953). Methods of Reducing Sample Size in Monte Carlo Computations. Or 1, 263-278. doi:10.1287/opre.1.5.263</p>
<p>VancouverBritish Columbia, Canada. S M Kakade, A Natural Policy GradientConference on Neural Information Processing Systems (NIPS) December. Kakade, S. M. (2001). VancouverBritish Columbia, Canada, 1531-1538.A Natural Policy GradientConference on Neural Information Processing Systems (NIPS) December 3-8.</p>
<p>Sim2real Transfer for Reinforcement Learning without Dynamics Randomization. M Kaspar, J D Munoz Osorio, J Bock, 10.1109/IROS45743.2020.9341260International Conference on Intelligent Robots and Systems (IROS). Las Vegas, NV, USAIEEEKaspar, M., Munoz Osorio, J. D., and Bock, J. (2020). "Sim2real Transfer for Reinforcement Learning without Dynamics Randomization," in International Conference on Intelligent Robots and Systems (IROS), October 24 -January 24 (Las Vegas, NV, USA: IEEE), 4383-4388. doi:10.1109/IROS45743.2020.9341260</p>
<p>Existence and Stability of Local Excitations in Homogeneous Neural fields. K Kishimoto, Amari , S , 10.1007/bf00275151J. Math. Biol. 7Kishimoto, K., and Amari, S. (1979). Existence and Stability of Local Excitations in Homogeneous Neural fields. J. Math. Biol. 7, 303-318. doi:10.1007/bf00275151</p>
<p>A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. P Klink, H Abdulsamad, B Belousov, C D&apos;eramo, J Peters, J Pajarinen, Klink, P., Abdulsamad, H., Belousov, B., D'Eramo, C., Peters, J., and Pajarinen, J. (2021). A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning, 13176. arXiv 2102.</p>
<p>Self-paced Contextual Reinforcement Learning. P Klink, H Abdulsamad, B Belousov, J Peters, PMLRof Proceedings of Machine Learning Research. Osaka, Japan100Conference on Robot Learning (CoRL)Klink, P., Abdulsamad, H., Belousov, B., and Peters, J. (2019). "Self-paced Contextual Reinforcement Learning," in Conference on Robot Learning (CoRL), October 30 -November 1 (Osaka, Japan: PMLR), 513-529. of Proceedings of Machine Learning Research.100.</p>
<p>Reinforcement Learning in Robotics: A Survey. J Kober, J A Bagnell, J Peters, 10.1177/0278364913495721Int. J. Robotics Res. 32Kober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement Learning in Robotics: A Survey. Int. J. Robotics Res. 32, 1238-1274. doi:10.1177/0278364913495721</p>
<p>Continuous Global Optimization in Multiview 3d Reconstruction. K Kolev, M Klodt, T Brox, D Cremers, 10.1007/s11263-009-0233-1Int. J. Comput. Vis. 84Kolev, K., Klodt, M., Brox, T., and Cremers, D. (2009). Continuous Global Optimization in Multiview 3d Reconstruction. Int. J. Comput. Vis. 84, 80-96. doi:10.1007/s11263-009-0233-1</p>
<p>Crossing the Reality gap in Evolutionary Robotics by Promoting Transferable Controllers. S Koos, J.-B Mouret, S Doncieux, 10.1145/1830483.1830505Genetic and Evolutionary Computation Conference (GECCO). Portland, Oregon, USAACMKoos, S., Mouret, J.-B., and Doncieux, S. (2010). "Crossing the Reality gap in Evolutionary Robotics by Promoting Transferable Controllers," in Genetic and Evolutionary Computation Conference (GECCO), July 7-11 (Portland, Oregon, USA: ACM), 119-126. doi:10.1145/1830483.1830505</p>
<p>The Transferability Approach: Crossing the Reality gap in Evolutionary Robotics. S Koos, J.-B Mouret, S Doncieux, 10.1109/TEVC.2012.2185849IEEE Trans. Evol. Computat. 17Koos, S., Mouret, J.-B., and Doncieux, S. (2013). The Transferability Approach: Crossing the Reality gap in Evolutionary Robotics. IEEE Trans. Evol. Computat. 17, 122-145. doi:10.1109/TEVC.2012.2185849</p>
<p>Imagenet Classification with Deep Convolutional Neural Networks. A Krizhevsky, I Sutskever, G E Hinton, Conference on Neural Information Processing Systems (NIPS). Lake TahoeKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). "Imagenet Classification with Deep Convolutional Neural Networks," in Conference on Neural Information Processing Systems (NIPS), 1106-1114.Lake Tahoe, Nev. United States December3-6</p>
<p>The DARPA Robotics challenge Finals: Results and Perspectives. E Krotkov, D Hackett, L Jackel, M Perschbacher, J Pippine, J Strauss, 10.1002/rob.21683J. Field Robotics. 34Krotkov, E., Hackett, D., Jackel, L., Perschbacher, M., Pippine, J., Strauss, J., et al. (2017). The DARPA Robotics challenge Finals: Results and Perspectives. J. Field Robotics 34, 229-240. doi:10.1002/rob.21683</p>
<p>RMA: Rapid Motor Adaptation for Legged Robots. A Kumar, Z Fu, D Pathak, Malik , J , 10.15607/RSS.2021.XVII.011Robotics: Science and Systems (RSS), Virtual Event. Kumar, A., Fu, Z., Pathak, D., and Malik, J. (2021). "RMA: Rapid Motor Adaptation for Legged Robots," in Robotics: Science and Systems (RSS), Virtual Event, July 12-16. doi:10.15607/RSS.2021.XVII.011</p>
<p>Self-paced Learning for Latent Variable Models. M P Kumar, B Packer, D Koller, Conference on Neural Information Processing Systems (NIPS). Vancouver, British Columbia, CanadaKumar, M. P., Packer, B., and Koller, D. (2010). "Self-paced Learning for Latent Variable Models," in Conference on Neural Information Processing Systems (NIPS), 6-9 December (Vancouver, British Columbia, Canada, 1189-1197.</p>
<p>Adversarial Examples in the Physical World. A Kurakin, I J Goodfellow, S Bengio, International Conference on Learning Representations (ICLR) Toulon. France. Workshop Track (OpenReview.netKurakin, A., Goodfellow, I. J., and Bengio, S. (2017). "Adversarial Examples in the Physical World," in International Conference on Learning Representations (ICLR) Toulon, April 24-26 (France. Workshop Track (OpenReview.net).</p>
<p>Adaptive Control: Algorithms, Analysis and Applications. 2 edn. I D Landau, R Lozano, M Saad, A Karimi, Springer Science &amp; Business MediaLandau, I. D., Lozano, R., M'Saad, M., and Karimi, A. (2011). Adaptive Control: Algorithms, Analysis and Applications. 2 edn. Springer Science &amp; Business Media.</p>
<p>Variational Policy Search via Trajectory Optimization. S Levine, V Koltun, Conference on Neural Information Processing Systems (NIPS). Lake Tahoe, Nevada, USALevine, S., and Koltun, V. (2013). "Variational Policy Search via Trajectory Optimization," in Conference on Neural Information Processing Systems (NIPS), December 5-8 (Lake Tahoe, Nevada, USA, 207-215.</p>
<p>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, 10.1177/0278364917710318doi:10.1177/ 0278364917710318Int. J. Robotics Res. 37Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J., and Quillen, D. (2018). Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large- Scale Data Collection. Int. J. Robotics Res. 37, 421-436. doi:10.1177/ 0278364917710318</p>
<p>Frontiers in Robotics and AI | www.frontiersin.org. 9799893Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893</p>
<p>Continuous Control with Deep Reinforcement Learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, International Conference on Learning Representations (ICLR). San Juan, Puerto RicoConference Track (OpenReview.netLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2016). "Continuous Control with Deep Reinforcement Learning," in International Conference on Learning Representations (ICLR), May 2-4 (San Juan, Puerto Rico. Conference Track (OpenReview.net).</p>
<p>Stein Variational Policy Gradient. Y Liu, P Ramachandran, Q Liu, J Peng, Association for Uncertainty in Artificial Intelligence (UAI). Sydney, AustraliaLiu, Y., Ramachandran, P., Liu, Q., and Peng, J. (2017). "Stein Variational Policy Gradient," in Association for Uncertainty in Artificial Intelligence (UAI) (Sydney, Australia, August 11-15.</p>
<p>Reinforcement Learning for Non-prehensile Manipulation: Transfer from Simulation to Physical System. K Lowrey, S Kolev, J Dao, A Rajeswaran, E Todorov, 10.1109/SIMPAR.2018.8376268Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR). Brisbane, AustraliaLowrey, K., Kolev, S., Dao, J., Rajeswaran, A., and Todorov, E. (2018). "Reinforcement Learning for Non-prehensile Manipulation: Transfer from Simulation to Physical System," in Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR), May 16-19 (Brisbane, Australia, 35-42. doi:10.1109/SIMPAR.2018.8376268</p>
<p>Flexible Statistical Inference for Mechanistic Models of Neural Dynamics. J Lueckmann, P J Gonçalves, G Bassetto, K Öcal, M Nonnenmacher, J H Macke, Conference on Neural Information Processing Systems. Long Beach, CA, USALueckmann, J., Gonçalves, P. J., Bassetto, G., Öcal, K., Nonnenmacher, M., and Macke, J. H. (2017). "Flexible Statistical Inference for Mechanistic Models of Neural Dynamics," in Conference on Neural Information Processing Systems, December 4-9 (Long Beach, CA, USA: NIPS), 1289-1299.</p>
<p>Adaptive Movement Sequences and Predictive Decisions Based on Hierarchical Dynamical Systems. T Luksch, M Gienger, M Mühlig, Yoshiike , T , 10.1109/iros.2012.6385651International Conference on Intelligent Robots and Systems (IROS). Vilamoura, Algarve, PortugalIEEELuksch, T., Gienger, M., Mühlig, M., and Yoshiike, T. (2012). "Adaptive Movement Sequences and Predictive Decisions Based on Hierarchical Dynamical Systems," in International Conference on Intelligent Robots and Systems (IROS), October 7-12 (Vilamoura, Algarve, Portugal: IEEE), 2082-2088. doi:10.1109/iros.2012.6385651</p>
<p>M Lutter, S Mannor, J Peters, D Fox, A Garg, Robust Value Iteration for Continuous Control Tasks. arXiv 2105. 12189Lutter, M., Mannor, S., Peters, J., Fox, D., and Garg, A. (2021a). Robust Value Iteration for Continuous Control Tasks. arXiv 2105.12189.</p>
<p>Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning. M Lutter, C Ritter, J Peters, International Conference on Learning Representations (ICLR). New Orleans, LA, USAConference Track (OpenReview.netLutter, M., Ritter, C., and Peters, J. (2019). "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning," in International Conference on Learning Representations (ICLR), May 6-9 (New Orleans, LA, USA. Conference Track (OpenReview.net).</p>
<p>Differentiable Physics Models for Real-World Offline Model-Based Reinforcement Learning. M Lutter, J Silberbauer, J Watson, J Peters, 1734Lutter, M., Silberbauer, J., Watson, J., and Peters, J. (2021b). Differentiable Physics Models for Real-World Offline Model-Based Reinforcement Learning. arXiv 2011.01734</p>
<p>Monte Carlo Bounding Techniques for Determining Solution Quality in Stochastic Programs. W.-K Mak, D P Morton, R K Wood, 10.1016/S0167-6377(98)00054-6Operations Res. Lett. 24Mak, W.-K., Morton, D. P., and Wood, R. K. (1999). Monte Carlo Bounding Techniques for Determining Solution Quality in Stochastic Programs. Operations Res. Lett. 24, 47-56. doi:10.1016/S0167-6377(98)00054-6</p>
<p>A Mandlekar, Y Zhu, A Garg, L Fei-Fei, S Savarese, 10.1109/IROS.2017.82062458206245Adversarially Robust Policy Learning: Active Construction of Physically-Plausible PerturbationsInternational Conference on Intelligent Robots and Systems (IROS). Vancouver, BC: CanadaMandlekar, A., Zhu, Y., Garg, A., Fei-Fei, L., and Savarese, S. (2017). Vancouver, BC: Canada. September 24-28. 3932-3939. doi:10.1109/IROS.2017. 8206245Adversarially Robust Policy Learning: Active Construction of Physically-Plausible PerturbationsInternational Conference on Intelligent Robots and Systems (IROS)</p>
<p>Virtual vs. Real: Trading off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization. A Marco, F Berkenkamp, P Hennig, A P Schoellig, A Krause, S Schaal, 10.1109/icra.2017.7989186International Conference on Robotics and Automation (ICRA). Marina Bay Sands; SingaporeMarco, A., Berkenkamp, F., Hennig, P., Schoellig, A. P., Krause, A., Schaal, S., et al. (2017). "Virtual vs. Real: Trading off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization," in International Conference on Robotics and Automation (ICRA), May 29 -Jun 3 (Marina Bay Sands, Singapore. doi:10.1109/icra.2017.7989186</p>
<p>Markov Chain Monte Carlo without Likelihoods. P Marjoram, J Molitor, V Plagnol, S Tavare, 10.1073/pnas.0306899100doi:10. 1073/pnas.0306899100Proc. Natl. Acad. Sci. 100Marjoram, P., Molitor, J., Plagnol, V., and Tavare, S. (2003). Markov Chain Monte Carlo without Likelihoods. Proc. Natl. Acad. Sci. 100, 15324-15328. doi:10. 1073/pnas.0306899100</p>
<p>Online Interactive Perception of Articulated Objects with Multi-Level Recursive Estimation Based on Taskspecific Priors. Martin Martin, R Brock, O , 10.1109/IROS.2014.6942902doi:10.1109/ IROS.2014.6942902International Conference on Intelligent Robots and Systems (IROS). Chicago, IL, USAIEEEMartin Martin, R., and Brock, O. (2014). "Online Interactive Perception of Articulated Objects with Multi-Level Recursive Estimation Based on Task- specific Priors," in International Conference on Intelligent Robots and Systems (IROS), September 14-18 (Chicago, IL, USA: IEEE), 2494-2501. doi:10.1109/ IROS.2014.6942902</p>
<p>Sim-to-real Reinforcement Learning for Deformable Object Manipulation. J Matas, S James, A J Davison, PMLRof Proceedings of Machine Learning Research. Zürich, Switzerland87Conference on Robot Learning (CoRL)Matas, J., James, S., and Davison, A. J. (2018). "Sim-to-real Reinforcement Learning for Deformable Object Manipulation," in Conference on Robot Learning (CoRL), October 29-31 (Zürich, Switzerland: PMLR), 734-743. of Proceedings of Machine Learning Research.87</p>
<p>Inferring the Material Properties of Granular media for Robotic Tasks. C Matl, Y Narang, R Bajcsy, F Ramos, D Fox, 10.1109/ICRA40945.2020.9197063International Conference on Robotics and Automation (ICRA). Paris, FranceMay 31 -AugustIEEE31Matl, C., Narang, Y., Bajcsy, R., Ramos, F., and Fox, D. (2020). "Inferring the Material Properties of Granular media for Robotic Tasks," in International Conference on Robotics and Automation (ICRA) (Paris, FranceMay 31 - August 31: IEEE), 2770-2777. doi:10.1109/ICRA40945.2020.9197063</p>
<p>Active Domain Randomization. B Mehta, M Diaz, F Golemo, C J Pal, L Paull, Research.100Conference on Robot Learning (CoRL). Osaka, Japanof Proceedings of Machine LearningMehta, B., Diaz, M., Golemo, F., Pal, C. J., and Paull, L. (2019). "Active Domain Randomization," in Conference on Robot Learning (CoRL), October 30 - November 1 (Osaka, Japan: PMLR), 1162-1176. of Proceedings of Machine Learning Research.100</p>
<p>A User's Guide to Calibrating Robotics Simulators. B Mehta, A Handa, D Fox, F Ramos, Proceedings of Machine Learning Research. Machine Learning ResearchPMLRConference on Robot Learning (CoRL)Mehta, B., Handa, A., Fox, D., and Ramos, F. (2020). "A User's Guide to Calibrating Robotics Simulators," in Conference on Robot Learning (CoRL), Virtual Event, November 16 -18 (PMLR). Proceedings of Machine Learning Research.</p>
<p>The Monte Carlo Method. N Metropolis, S Ulam, 10.1080/01621459.1949.10483310J. Am. Stat. Assoc. 44Metropolis, N., and Ulam, S. (1949). The Monte Carlo Method. J. Am. Stat. Assoc. 44, 335-341. doi:10.1080/01621459.1949.10483310</p>
<p>Human-level Control through Deep Reinforcement Learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, 10.1038/nature14236Nature. 518Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level Control through Deep Reinforcement Learning. Nature 518, 529-533. doi:10.1038/nature14236</p>
<p>Sim-to-(multi)-real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors. A Molchanov, T Chen, W Honig, J A Preiss, N Ayanian, G S Sukhatme, 10.1109/IROS40897.2019.8967695doi:10. 1109/IROS40897.2019.8967695International Conference on Intelligent Robots and Systems (IROS). Macau, SAR, ChinaIEEEMolchanov, A., Chen, T., Honig, W., Preiss, J. A., Ayanian, N., and Sukhatme, G. S. (2019). "Sim-to-(multi)-real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors," in International Conference on Intelligent Robots and Systems (IROS), November 3-8 (Macau, SAR, China: IEEE), 59-66. doi:10. 1109/IROS40897.2019.8967695</p>
<p>Ensemble-cio: Full-Body Dynamic Motion Planning that Transfers to Physical Humanoids. I Mordatch, K Lowrey, E Todorov, 10.1109/IROS.2015.7354126doi:10.1109/ IROS.2015.7354126International Conference on Intelligent Robots and Systems (IROS). Hamburg, GermanyMordatch, I., Lowrey, K., and Todorov, E. (2015). "Ensemble-cio: Full-Body Dynamic Motion Planning that Transfers to Physical Humanoids," in International Conference on Intelligent Robots and Systems (IROS), September 28 -October 2 (Hamburg, Germany, 5307-5314. doi:10.1109/ IROS.2015.7354126</p>
<p>Learning to Plan Hierarchically from Curriculum. P Morere, L Ott, F Ramos, 10.1109/LRA.2019.2920285IEEE Robot. Autom. Lett. 4Morere, P., Ott, L., and Ramos, F. (2019). Learning to Plan Hierarchically from Curriculum. IEEE Robot. Autom. Lett. 4, 2815-2822. doi:10.1109/LRA.2019. 2920285</p>
<p>Learning Domain Randomization Distributions for Training Robust Locomotion Policies. M Mozian, J Camilo Gamboa Higuera, D Meger, G Dudek, 10.1109/IROS45743.2020.9341019International Conference on Intelligent Robots and Systems (IROS) Las Vegas. NV, USAIEEEMozian, M., Camilo Gamboa Higuera, J., Meger, D., and Dudek, G. (2020). "Learning Domain Randomization Distributions for Training Robust Locomotion Policies," in International Conference on Intelligent Robots and Systems (IROS) Las Vegas, October 24 -January 24 (NV, USA: IEEE), 6112-6117. doi:10.1109/IROS45743.2020.9341019</p>
<p>Data-efficient Domain Randomization with Bayesian Optimization. F Muratore, C Eilers, M Gienger, J Peters, 10.1109/LRA.2021.3052391IEEE Robot. Autom. Lett. 6Muratore, F., Eilers, C., Gienger, M., and Peters, J. (2021a). Data-efficient Domain Randomization with Bayesian Optimization. IEEE Robot. Autom. Lett. 6, 911-918. doi:10.1109/LRA.2021.3052391</p>
<p>Assessing Transferability from Simulation to Reality for Reinforcement Learning. F Muratore, M Gienger, J Peters, 10.1109/TPAMI.2019.2952353IEEE Trans. Pattern Anal. Mach. Intell. 43Muratore, F., Gienger, M., and Peters, J. (2021b). Assessing Transferability from Simulation to Reality for Reinforcement Learning. IEEE Trans. Pattern Anal. Mach. Intell. 43, 1172-1183. doi:10.1109/TPAMI.2019.2952353</p>
<p>Neural Posterior Domain Randomization. F Muratore, T Gruner, F Wiese, B B M Gienger, J Peters, Conference on Robot Learning (CoRL). London, EnglandVirtual EventMuratore, F., Gruner, T., Wiese, F., Gienger, B. B. M., and Peters, J. (2021c). "Neural Posterior Domain Randomization," in Conference on Robot Learning (CoRL), Virtual Event, November 8-11 (London, England.</p>
<p>Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment. F Muratore, F Treede, M Gienger, J Peters, of Proceedings of Machine Learning Research. Zürich, SwitzerlandOctoberPMLR87Conference on Robot Learning (CoRL)Muratore, F., Treede, F., Gienger, M., and Peters, J. (2018). "Domain Randomization for Simulation-Based Policy Optimization with Transferability Assessment," in Conference on Robot Learning (CoRL) (Zürich, SwitzerlandOctober 29-31: PMLR), 700-713. of Proceedings of Machine Learning Research.87</p>
<p>Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning. A Nagabandi, I Clavera, S Liu, R S Fearing, P Abbeel, S Levine, International Conference on Learning Representations (ICLR). New Orleans; LA, USANagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., et al. (2019). "Learning to Adapt in Dynamic, Real-World Environments through Meta- Reinforcement Learning," in International Conference on Learning Representations (ICLR) New Orleans, May 6-9 (LA, USA. OpenReview.net).</p>
<p>PEGASUS: a Policy Search Method for Large Mdps and Pomdps. A Y Ng, Jordan , M I , UAI. Stanford, California, USAMorgan KaufmannNg, A. Y., and Jordan, M. I. (2000). "PEGASUS: a Policy Search Method for Large Mdps and Pomdps," in UAI, June 30 -July 3 (Stanford, California, USA: Morgan Kaufmann), 406-415.</p>
<p>Solving Rubik's Cube with a Robot Hand. I Openaiakkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, arXiv 1910.07113OpenAIAkkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., et al. (2019). Solving Rubik's Cube with a Robot Hand. arXiv 1910.07113</p>
<p>A Survey on Transfer Learning. S J Pan, Yang , Q , 10.1109/TKDE.2009.191IEEE Trans. Knowl. Data Eng. 22Pan, S. J., and Yang, Q. (2010). A Survey on Transfer Learning. IEEE Trans. Knowl. Data Eng. 22, 1345-1359. doi:10.1109/TKDE.2009.191</p>
<p>Fast ϵ-free Inference of Simulation Models with Bayesian Conditional Density Estimation. G Papamakarios, Murray , I , Conference on Neural Information Processing Systems (NIPS). Barcelona, SpainPapamakarios, G., and Murray, I. (2016). "Fast ϵ-free Inference of Simulation Models with Bayesian Conditional Density Estimation," in Conference on Neural Information Processing Systems (NIPS), December 5-10 (Barcelona, Spain, 1028-1036.</p>
<p>Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows. G Papamakarios, D C Sterratt, Murray , I , PMLRInternational Conference on Artificial Intelligence and Statistics (AISTATS). Naha, Okinawa, Japan89of Proceedings of Machine Learning ResearchPapamakarios, G., Sterratt, D. C., and Murray, I. (2019). "Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows," in International Conference on Artificial Intelligence and Statistics (AISTATS), April 16-18 (Naha, Okinawa, Japan: PMLR), 837-848. of Proceedings of Machine Learning Research.89</p>
<p>Actor-mimic: Deep Multitask and Transfer Reinforcement Learning. E Parisotto, L J Ba, R Salakhutdinov, International Conference on Learning Representations (ICLR). San Juan; Puerto RicoConference TrackParisotto, E., Ba, L. J., and Salakhutdinov, R. (2016). "Actor-mimic: Deep Multitask and Transfer Reinforcement Learning," in International Conference on Learning Representations (ICLR) San Juan, May 2-4 (Puerto Rico. Conference Track.</p>
<p>Fingerprint Policy Optimisation for Robust Reinforcement Learning. S Paul, M A Osborne, S Whiteson, International Conference on Machine Learning (ICML). Long Beach California, USA; June97Paul, S., Osborne, M. A., and Whiteson, S. (2019). Fingerprint Policy Optimisation for Robust Reinforcement Learning. In International Conference on Machine Learning (ICML), Long Beach California, USA, 9-15 June (PMLR), vol. 97</p>
<p>Sim-to-real Transfer of Robotic Control with Dynamics Randomization. X B Peng, M Andrychowicz, W Zaremba, Abbeel , P , 10.1109/ICRA.2018.8460528International Conference on Robotics and Automation (ICRA). Brisbane, AustraliaPeng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018). "Sim-to-real Transfer of Robotic Control with Dynamics Randomization," in International Conference on Robotics and Automation (ICRA), May 21-25 (Brisbane, Australia, 1-8. doi:10.1109/ICRA.2018.8460528</p>
<p>Improving Noise. K Perlin, 10.1145/566654.566636doi:10.1145/ 566654.566636ACM Trans. Graph. 21Perlin, K. (2002). Improving Noise. ACM Trans. Graph. 21, 681-682. doi:10.1145/ 566654.566636</p>
<p>Frontiers in Robotics and AI | www.frontiersin.org. 9799893Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893</p>
<p>Relative Entropy Policy Search. J Peters, K Mülling, Altun , Y , AAAI Conference on Artificial Intelligence. Atlanta, Georgia, USAPeters, J., Mülling, K., and Altun, Y. (2010). "Relative Entropy Policy Search," in AAAI Conference on Artificial Intelligence, July 11-15 (Atlanta, Georgia, USA.</p>
<p>Asymmetric Actor Critic for Image-Based Robot Learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, Abbeel , P , 10.15607/RSS.2018.XIV.008Robotics: Science and Systems (RSS). Pittsburgh, Pennsylvania, USAPinto, L., Andrychowicz, M., Welinder, P., Zaremba, W., and Abbeel, P. (2018). "Asymmetric Actor Critic for Image-Based Robot Learning," in Robotics: Science and Systems (RSS), June 26-30 (Pittsburgh, Pennsylvania, USA. doi:10.15607/RSS.2018.XIV.008</p>
<p>Robust Adversarial Reinforcement Learning. L Pinto, J Davidson, R Sukthankar, A Gupta, PMLRInternational Conference on Machine Learning (ICML). Sydney, NSW, AustraliaPinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017). "Robust Adversarial Reinforcement Learning," in International Conference on Machine Learning (ICML), August 6-11 (Sydney, NSW, Australia: PMLR), 2817-2826.</p>
<p>Sim-to-real Quadrotor landing via Sequential Deep Q-Networks and Domain Randomization. R Polvara, M Patacchiola, M Hanheide, G Neumann, 10.3390/robotics9010008Robotics. 9Polvara, R., Patacchiola, M., Hanheide, M., and Neumann, G. (2020). Sim-to-real Quadrotor landing via Sequential Deep Q-Networks and Domain Randomization. Robotics 9, 8. doi:10.3390/robotics9010008</p>
<p>Online Bayessim for Combined Simulator Parameter Inference and Policy Improvement. R Possas, L Barcelos, R Oliveira, D Fox, F Ramos, 10.1109/IROS45743.2020.9341401International Conference on Intelligent Robots and Systems (IROS) Las Vegas. NV, USAIEEEPossas, R., Barcelos, L., Oliveira, R., Fox, D., and Ramos, F. (2020). "Online Bayessim for Combined Simulator Parameter Inference and Policy Improvement," in International Conference on Intelligent Robots and Systems (IROS) Las Vegas, October 24 -January 24 (NV, USA: IEEE), 5445-5452. doi:10.1109/IROS45743.2020.9341401</p>
<p>Language Models Are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models Are Unsupervised Multitask Learners.</p>
<p>Epopt: Learning Robust Neural Network Policies Using Model Ensembles. A Rajeswaran, S Ghotra, B Ravindran, S Levine, International Conference on Learning Representations (ICLR). ToulonFrance. Conference Track (OpenReview.netRajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S. (2017). "Epopt: Learning Robust Neural Network Policies Using Model Ensembles," in International Conference on Learning Representations (ICLR), Toulon, April 24-26 (France. Conference Track (OpenReview.net).</p>
<p>A Game Theoretic Framework for Model Based Reinforcement Learning. A Rajeswaran, I Mordatch, V Kumar, International Conference on Machine Learning (ICML). 119of Proceedings of Machine Learning ResearchRajeswaran, A., Mordatch, I., and Kumar, V. (2020). "A Game Theoretic Framework for Model Based Reinforcement Learning," in International Conference on Machine Learning (ICML), Virtual Event, 13-18 July (PMLR), 7953-7963. of Proceedings of Machine Learning Research.119</p>
<p>Bayessim: Adaptive Domain Randomization via Probabilistic Inference for Robotics Simulators. F Ramos, R Possas, D Fox, 10.15607/RSS.2019.XV.029Robotics: Science and Systems (RSS). Germany: Freiburg im BreisgauRamos, F., Possas, R., and Fox, D. (2019). "Bayessim: Adaptive Domain Randomization via Probabilistic Inference for Robotics Simulators," in Robotics: Science and Systems (RSS), June 22-26 (Germany: Freiburg im Breisgau). doi:10.15607/RSS.2019.XV.029</p>
<p>C E Rasmussen, C K I Williams, Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning. MIT PressRasmussen, C. E., and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning. MIT Press.</p>
<p>K Rawlik, M Toussaint, S Vijayakumar, 10.15607/RSS.2012.VIII.045Stochastic Optimal Control and Reinforcement Learning by Approximate InferenceRobotics: Science and SystemsJuly. Sydney, NSW, Australia9Rawlik, K., Toussaint, M., and Vijayakumar, S. (2012). Sydney, NSW, Australia: RSS. doi:10.15607/RSS.2012.VIII.045On Stochastic Optimal Control and Reinforcement Learning by Approximate InferenceRobotics: Science and SystemsJuly 9-13</p>
<p>Learning to Simulate. N Ruiz, S Schulter, M Chandraker, International Conference on Learning Representations (ICLR). New Orleans, LA, USAOpenReview.netRuiz, N., Schulter, S., and Chandraker, M. (2019). "Learning to Simulate," in International Conference on Learning Representations (ICLR), May 6-9 (New Orleans, LA, USA. (OpenReview.net).</p>
<p>. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015).</p>
<p>Imagenet Large Scale Visual Recognition challenge. 10.1007/s11263-015-0816-yInt. J. Comput. Vis. 115Imagenet Large Scale Visual Recognition challenge. Int. J. Comput. Vis. 115, 211-252. doi:10.1007/s11263-015-0816-y</p>
<p>Policy Distillation. A A Rusu, S G Colmenarejo, Ç Gülçehre, G Desjardins, J Kirkpatrick, R Pascanu, Conference Track.International Conference on Learning Representations (ICLR)May. San Juan, Puerto RicoRusu, A. A., Colmenarejo, S. G., Gülçehre, Ç., Desjardins, G., Kirkpatrick, J., Pascanu, R., et al. (2016a). "Policy Distillation," in (San Juan, Puerto Rico. Conference Track.International Conference on Learning Representations (ICLR)May 2-4</p>
<p>A A Rusu, N C Rabinowitz, G Desjardins, H Soyer, J Kirkpatrick, K Kavukcuoglu, 1606.04671Progressive Neural Networks. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., et al. (2016b). Progressive Neural Networks. arXiv 1606.04671</p>
<p>Sim-to-real Robot Learning from Pixels with Progressive Nets. A A Rusu, M Vecerik, T Rothörl, N Heess, R Pascanu, R Hadsell, Research.78Conference on Robot Learning (CoRL). Mountain View; California, USAof Proceedings of Machine LearningRusu, A. A., Vecerik, M., Rothörl, T., Heess, N., Pascanu, R., and Hadsell, R. (2017). "Sim-to-real Robot Learning from Pixels with Progressive Nets," in Conference on Robot Learning (CoRL), Mountain View, November 13-15 (California, USA: PMLR), 262-270. of Proceedings of Machine Learning Research.78</p>
<p>CAD2RL: Real Single-Image Flight without a Single Real Image. F Sadeghi, S Levine, 10.15607/RSS.2017.XIII.034Robotics: Science and Systems (RSS). Cambridge, Massachusetts, USASadeghi, F., and Levine, S. (2017). "CAD2RL: Real Single-Image Flight without a Single Real Image," in Robotics: Science and Systems (RSS), July 12-16 (Cambridge, Massachusetts, USA. doi:10.15607/RSS.2017.XIII.034</p>
<p>Meta-learning with Memory-Augmented Neural Networks. A Santoro, S Bartunov, M Botvinick, D Wierstra, T P Lillicrap, International Conference on Machine Learning (ICML). New York City, NY, USA48Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. P. (2016). "Meta-learning with Memory-Augmented Neural Networks," in International Conference on Machine Learning (ICML), June 19-24 (New York City, NY, USA: JMLR.org), 1842-1850.48</p>
<p>. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).</p>
<p>Proximal Policy Optimization Algorithms. arXiv 1707. 6347Proximal Policy Optimization Algorithms. arXiv 1707.06347.</p>
<p>Blind Bipedal Stair Traversal via Sim-To-Real Reinforcement Learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, 10.15607/RSS.2021.XVII.061Robotics: Science and Systems (RSS), Virtual Event. Siekmann, J., Green, K., Warila, J., Fern, A., and Hurst, J. (2021). "Blind Bipedal Stair Traversal via Sim-To-Real Reinforcement Learning," in Robotics: Science and Systems (RSS), Virtual Event, July 12-16. doi:10.15607/RSS.2021.XVII.061</p>
<p>Mastering the Game of Go with Deep Neural Networks and Tree Search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, 10.1038/nature16961Nature. 529Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature 529, 484-489. doi:10.1038/nature16961</p>
<p>Approximate Bayesian Computation. M Sunnåker, A G Busetto, E Numminen, J Corander, M Foll, C Dessimoz, 10.1371/journal.pcbi.1002803Plos Comput. Biol. 91002803Sunnåker, M., Busetto, A. G., Numminen, E., Corander, J., Foll, M., and Dessimoz, C. (2013). Approximate Bayesian Computation. Plos Comput. Biol. 9, e1002803. doi:10.1371/journal.pcbi.1002803</p>
<p>Encoding Physical Constraints in Differentiable newton-euler Algorithm. G Sutanto, A S Wang, Y Lin, M Mukadam, G S Sukhatme, A Rai, Research.120of Proceedings of Machine Learning. Berkeley, CA, USAL4DC, Virtual EventSutanto, G., Wang, A. S., Lin, Y., Mukadam, M., Sukhatme, G. S., Rai, A., et al. (2020). "Encoding Physical Constraints in Differentiable newton-euler Algorithm," in L4DC, Virtual Event, 11-12 June (Berkeley, CA, USA: PMLR), 804-813. of Proceedings of Machine Learning Research.120</p>
<p>R S Sutton, 10.1145/122344.122377Dyna, an Integrated Architecture for Learning, Planning, and Reacting. SIGART Bull. 2Sutton, R. S. (1991). Dyna, an Integrated Architecture for Learning, Planning, and Reacting. SIGART Bull. 2, 160-163. doi:10.1145/122344.122377</p>
<p>Between Mdps and Semi-mdps: A Framework for Temporal Abstraction in Reinforcement Learning. R S Sutton, D Precup, S Singh, 10.1016/S0004-3702(99)00052-1Artif. Intelligence. 112Sutton, R. S., Precup, D., and Singh, S. (1999). Between Mdps and Semi-mdps: A Framework for Temporal Abstraction in Reinforcement Learning. Artif. Intelligence 112, 181-211. doi:10.1016/S0004-3702(99)00052-1</p>
<p>Intriguing Properties of Neural Networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I J Goodfellow, Conference Track.International Conference on Learning Representations (ICLR)April. Banff, CanadaSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., et al. (2014). "Intriguing Properties of Neural Networks," in (Banff, Canada. Conference Track.International Conference on Learning Representations (ICLR)April 14-16</p>
<p>Sim-toreal: Learning Agile Locomotion for Quadruped Robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, 10.15607/RSS.2018.XIV.010doi:10.15607/ RSS.2018.XIV.010Robotics: Science and Systems (RSS). Pittsburgh, Pennsylvania, USATan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., et al. (2018). "Sim-to- real: Learning Agile Locomotion for Quadruped Robots," in Robotics: Science and Systems (RSS), June 26-30 (Pittsburgh, Pennsylvania, USA. doi:10.15607/ RSS.2018.XIV.010</p>
<p>Distral: Robust Multitask Reinforcement Learning. Y W Teh, V Bapst, W M Czarnecki, J Quan, J Kirkpatrick, R Hadsell, Conference on Neural Information Processing Systems (NIPS). Long Beach, CA, USATeh, Y. W., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., et al. (2017). "Distral: Robust Multitask Reinforcement Learning," in Conference on Neural Information Processing Systems (NIPS) (Long Beach, CA, USA, 4496-4506.</p>
<p>Bayesian Robot System Identification with Input and Output Noise. J.-A Ting, A Souza, S Schaal, 10.1016/j.neunet.2010.08.011Neural Networks. 24Ting, J.-A., D'Souza, A., and Schaal, S. (2011). Bayesian Robot System Identification with Input and Output Noise. Neural Networks 24, 99-108. doi:10.1016/j.neunet.2010.08.011</p>
<p>A Bayesian Approach to Nonlinear Parameter Identification for Rigid Body Dynamics. J Ting, M Mistry, J Peters, S Schaal, J Nakanishi, 10.15607/RSS.2006.II.032Robotics: Science and Systems (RSS). Philadelphia, Pennsylvania, USAThe MIT PressTing, J., Mistry, M., Peters, J., Schaal, S., and Nakanishi, J. (2006). "A Bayesian Approach to Nonlinear Parameter Identification for Rigid Body Dynamics," in Robotics: Science and Systems (RSS), August 16-19 (Philadelphia, Pennsylvania, USA: The MIT Press). doi:10.15607/RSS. 2006.II.032</p>
<p>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, Abbeel , P , 10.1109/IROS.2017.8202133International Conference on Intelligent Robots and Systems (IROS). Vancouver, BC: CanadaTobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. (2017). "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World," in International Conference on Intelligent Robots and Systems (IROS), September 24-28 (Vancouver, BC: Canada), 23-30. doi:10.1109/IROS.2017.8202133</p>
<p>A Van Den Oord, Y Li, O Vinyals, arXiv 1807.03748Representation Learning with Contrastive Predictive Coding. van den Oord, A., Li, Y., and Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. arXiv 1807.03748</p>
<p>Distributionally Robust Control of Constrained Stochastic Systems. B Van Parys, D Kuhn, P Goulart, M Morari, 10.1109/TAC.2015.2444134IEEE Trans. Automat. Contr. 61Van Parys, B., Kuhn, D., Goulart, P., and Morari, M. (2015). Distributionally Robust Control of Constrained Stochastic Systems. IEEE Trans. Automat. Contr. 61, 1. doi:10.1109/TAC.2015.2444134</p>
<p>Learning to Reinforcement Learn. J Wang, Z Kurth-Nelson, H Soyer, J Z Leibo, D Tirumala, R Munos, Cognitive Science. London, UK. cognitivesciencesociety.orgWang, J., Kurth-Nelson, Z., Soyer, H., Leibo, J. Z., Tirumala, D., Munos, R., et al. (2017). "Learning to Reinforcement Learn," in Cognitive Science, 16-29 July (London, UK. cognitivesciencesociety.org).</p>
<p>Optimizing Walking Controllers for Uncertain Inputs and Environments. J M Wang, D J Fleet, A Hertzmann, 10.1145/1833351.177881010.1145/1778765.1778810ACM Trans. Graphics. 29Wang, J. M., Fleet, D. J., and Hertzmann, A. (2010). Optimizing Walking Controllers for Uncertain Inputs and Environments. ACM Trans. Graphics 29, 73-78. doi:10.1145/1833351.177881010.1145/1778765.1778810</p>
<p>J Watson, H Abdulsamad, R Findeisen, J Peters, Stochastic Control through Approximate Bayesian Input Inference. arXiv 2105. 7693Watson, J., Abdulsamad, H., Findeisen, R., and Peters, J. (2021). Stochastic Control through Approximate Bayesian Input Inference. arXiv 2105.07693</p>
<p>Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. R J Williams, 10.1007/BF00992696doi:10. 1007/BF00992696Mach Learn. 8Williams, R. J. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Mach Learn. 8, 229-256. doi:10. 1007/BF00992696</p>
<p>Adaptive Dual Control Methods: An Overview. B Wittenmark, 10.1016/b978-0-08-042375-3.50010-xAdaptive Systems in Control and Signal Processing. ElsevierWittenmark, B. (1995). "Adaptive Dual Control Methods: An Overview," in Adaptive Systems in Control and Signal Processing 1995 (Elsevier), 67-72. doi:10.1016/b978-0-08-042375-3.50010-x</p>
<p>On the Effectiveness of Common Random Numbers. R D Wright, T E Ramsay, 10.1287/mnsc.25.7.649Manag. Sci. 25Wright, R. D., and Ramsay, T. E. (1979). On the Effectiveness of Common Random Numbers. Manag. Sci. 25, 649-656. doi:10.1287/mnsc.25.7.649</p>
<p>Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning. J Wu, I Yildirim, J J Lim, B Freeman, J B Tenenbaum, Conference on Neural Information Processing Systems (NIPS). Montreal, Quebec, CanadaWu, J., Yildirim, I., Lim, J. J., Freeman, B., and Tenenbaum, J. B. (2015). "Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning," in Conference on Neural Information Processing Systems (NIPS), December 7-12 (Montreal, Quebec, Canada, 127-135.</p>
<p>Learning Locomotion Skills for Cassie: Iterative Design and Sim-To-Real. Z Xie, P Clary, J Dao, P Morais, J W Hurst, M Van De Panne, Research.100Conference on Robot Learning (CoRL). Osaka, Japanof Proceedings of Machine LearningXie, Z., Clary, P., Dao, J., Morais, P., Hurst, J. W., and van de Panne, M. (2019). "Learning Locomotion Skills for Cassie: Iterative Design and Sim-To-Real," in Conference on Robot Learning (CoRL), October 30 -November 1 (Osaka, Japan: PMLR), 317-329. of Proceedings of Machine Learning Research.100</p>
<p>Frontiers in Robotics and AI | www.frontiersin.org. 9799893Frontiers in Robotics and AI | www.frontiersin.org April 2022 | Volume 9 | Article 799893</p>
<p>Dynamics Randomization Revisited: A Case Study for Quadrupedal Locomotion. Z Xie, X Da, M Van De Panne, B Babich, A Garg, arXiv 2011.02404Xie, Z., Da, X., van de Panne, M., Babich, B., and Garg, A. (2020). Dynamics Randomization Revisited: A Case Study for Quadrupedal Locomotion. arXiv 2011.02404</p>
<p>Learning Predictive Representations for Deformable Objects Using Contrastive Estimation. W Yan, A Vangipuram, P Abbeel, L Pinto, PMLRof Proceedings of Machine Learning Research. Virtual Event/Cambridge, MA, USA155Conference on Robot Learning (CoRL)Yan, W., Vangipuram, A., Abbeel, P., and Pinto, L. (2020). "Learning Predictive Representations for Deformable Objects Using Contrastive Estimation," in Conference on Robot Learning (CoRL), Virtual Event, November 16 -18 (Virtual Event/Cambridge, MA, USA: PMLR), 564-574. of Proceedings of Machine Learning Research.155.</p>
<p>Lyapunov Stability and strong Passivity Analysis for Nonlinear Descriptor Systems. C Yang, J Sun, Q Zhang, X Ma, 10.1109/TCSI.2012.2215396IEEE Trans. Circuits Syst. 60Yang, C., Sun, J., Zhang, Q., and Ma, X. (2013). Lyapunov Stability and strong Passivity Analysis for Nonlinear Descriptor Systems. IEEE Trans. Circuits Syst. 60, 1003-1012. doi:10.1109/TCSI.2012.2215396</p>
<p>Sim-to-real Transfer for Biped Locomotion. W Yu, V C Kumar, G Turk, C K Liu, 10.1109/IROS40897.2019.8968053doi:10. 1109/IROS40897.2019.8968053International Conference on Intelligent Robots and Systems (IROS). Macau, SAR, ChinaIEEEYu, W., Kumar, V. C., Turk, G., and Liu, C. K. (2019a). "Sim-to-real Transfer for Biped Locomotion," in International Conference on Intelligent Robots and Systems (IROS), November 3-8 (Macau, SAR, China: IEEE), 3503-3510. doi:10. 1109/IROS40897.2019.8968053</p>
<p>Policy Transfer with Strategy Optimization. W Yu, C K Liu, G Turk, International Conference on Learning Representations (ICLR). New Orleans, LA, USAConference Track (OpenReview.netYu, W., Liu, C. K., and Turk, G. (2019b). "Policy Transfer with Strategy Optimization," in International Conference on Learning Representations (ICLR), May 6-9 (New Orleans, LA, USA. Conference Track (OpenReview.net).</p>
<p>Preparing for the Unknown: Learning a Universal Policy with Online System Identification. W Yu, J Tan, Karen Liu, C Turk, G , 10.15607/RSS.2017.XIII.048doi:10. 15607/RSS.2017.XIII.048Robotics: Science and Systems (RSS). Cambridge, Massachusetts, USAYu, W., Tan, J., Karen Liu, C., and Turk, G. (2017). "Preparing for the Unknown: Learning a Universal Policy with Online System Identification," in Robotics: Science and Systems (RSS), July 12-16 (Cambridge, Massachusetts, USA. doi:10. 15607/RSS.2017.XIII.048</p>
<p>Robust Reinforcement Learning on State Observations with Learned Optimal Adversary. H Zhang, H Chen, D S Boning, C Hsieh, International Conference on Learning Representations (ICLR), Virtual Event. Zhang, H., Chen, H., Boning, D. S., and Hsieh, C. (2021). "Robust Reinforcement Learning on State Observations with Learned Optimal Adversary," in International Conference on Learning Representations (ICLR), Virtual Event, May 3-7 (Austria. OpenReview.net).</p>
<p>Predicting Sim-To-Real Transfer with Probabilistic Dynamics Models. L M Zhang, M Plappert, W Zaremba, 12864Zhang, L. M., Plappert, M., and Zaremba, W. (2020). Predicting Sim-To-Real Transfer with Probabilistic Dynamics Models, 12864. arXiv 2009.</p>
<p>Mathematical Foundations of Robust and Distributionally Robust Optimization. J Zhen, D Kuhn, W Wiesemann, arXiv 2105.00760Zhen, J., Kuhn, D., and Wiesemann, W. (2021). Mathematical Foundations of Robust and Distributionally Robust Optimization. arXiv 2105.00760</p>
<p>Essentials of Robust Control. K Zhou, J C Doyle, Prentice-Hall104Zhou, K., and Doyle, J. C. (1998). Essentials of Robust Control, 104. Prentice-Hall.</p>
<p>. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, 10.1109/JPROC.2020.3004555doi:10. 1109/JPROC.2020.3004555Comprehensive Survey on Transfer Learning. Proc. IEEE. 109Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., et al. (2021). A Comprehensive Survey on Transfer Learning. Proc. IEEE 109, 43-76. doi:10. 1109/JPROC.2020.3004555</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8760 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8760</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8760</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-266999677</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29774/31334" target="_blank">Small Language Model Can Self-correct</a></p>
                <p><strong>Paper Abstract:</strong> Generative Language Models (LMs) such as ChatGPT have exhibited remarkable performance across various downstream tasks. Nevertheless, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. Previous studies have devised sophisticated pipelines and prompts to induce large LMs to exhibit the capability for self-correction. However, large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans. Moreover, these complex prompts are extremely challenging for small LMs to follow. In this paper, we introduce the \underline{I}ntrinsic \underline{S}elf-\underline{C}orrection (ISC) in generative language models, aiming to correct the initial output of LMs in a self-triggered manner, even for those small LMs with 6 billion parameters. Specifically, we devise a pipeline for constructing self-correction data and propose Partial Answer Masking (PAM), aiming to endow the model with the capability for intrinsic self-correction through fine-tuning. We conduct experiments using LMs with parameters sizes ranging from 6 billion to 13 billion in two tasks, including commonsense reasoning and factual knowledge reasoning. Our experiments demonstrate that the outputs generated using ISC outperform those generated without self-correction. We believe that the output quality of even small LMs can be further improved by empowering them with the ability to intrinsic self-correct.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8760.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8760.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsic Self-Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-fine-tuning approach that endows LMs with an intrinsic, self-triggered capability to verify and, if necessary, modify their own initial answers (self-verification + self-modification) using a single Self-Correction Prompt and training data constructed to mimic human self-correction; trained with Partial Answer Masking (PAM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CuteGPT-7B, CuteGPT-13B, ChatGLM-6B, Vicuna-7B, Vicuna-13B (also reported experiments on Llama2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned models of scale 6B–13B parameters: CuteGPT family (7B, 13B; LLaMA-based, Chinese-vocab expansion), ChatGLM-6B (bilingual ~6B), Vicuna (7B, 13B; fine-tuned LLaMA), Llama2-7B (Meta, 7B). Models were further fine-tuned for ISC via IFT (full fine-tune for CuteGPT, LoRA for Vicuna/Llama2, prompt-tuning for ChatGLM).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Intrinsic Self-Correction (ISC) with Self-Correction Prompt (SCP)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates an initial answer plus (optionally) chain-of-thought, then internally produces a binary self-verification signal (v) and, if v indicates an error, produces a revised answer; training uses constructed self-correction examples (good/bad cases) and Partial Answer Masking so the model learns to verify and modify without relying on external verifiers; at inference models are given up to two attempts (initial + one correction).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>OpenBookQA, CommonsenseQA (and zero-shot test on StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice question answering datasets: OpenBookQA (elementary science multiple-choice, ~5,957 questions, 4 options); CommonsenseQA (~12,102 questions, 5 options). StrategyQA used as a held-out zero-shot test requiring true/false answers and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Per-model aggregated results (after ISC self-correction, using PAM unless noted): CuteGPT-7B — OpenBookQA ACC after correction 29.0 (ACC-First 25.2), CommonsenseQA ACC after correction 26.9 (ACC-First 23.2). CuteGPT-13B — OpenBookQA ACC after correction 42.0 (ACC-First 37.2), CommonsenseQA ACC after correction 37.9 (ACC-First 36.6). ChatGLM-6B — OpenBookQA ACC after correction 42.6 (ACC-First 37.0; improvement +5.6 percentage points), CommonsenseQA ACC after correction 38.7 (ACC-First 34.3). Vicuna-7B — OpenBookQA ACC after correction 28.8 (ACC-First 28.6), CommonsenseQA ACC after correction 26.0 (ACC-First 25.9). Vicuna-13B — OpenBookQA ACC after correction 34.0 (ACC-First 33.8), CommonsenseQA ACC after correction 32.6 (ACC-First 33.8). (Numbers taken from the paper's reported ACC-First and ACC columns; ISC also improved zero-shot StrategyQA accuracy after one correction for all LMs.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>The initial-answer accuracies (ACC-First) before any correction: CuteGPT-7B OpenBookQA 25.2, CommonsenseQA 23.2; CuteGPT-13B OpenBookQA 37.2, CommonsenseQA 36.6; ChatGLM-6B OpenBookQA 37.0, CommonsenseQA 34.3; Vicuna-7B OpenBookQA 28.6, CommonsenseQA 25.9; Vicuna-13B OpenBookQA 33.8, CommonsenseQA 33.8. (These are the ACC-First baseline numbers reported in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented via instruction fine-tuning on synthetic self-correction examples (Question + generated COT + initial answer + verification signal + corrected answer). Self-correction realized by: (1) prompting at inference with Self-Correction Prompt (SCP) that asks model to double-check before finalizing, and (2) training with Partial Answer Masking (PAM) so that bad-case incorrect initial answers are masked out of loss and only verification + corrected answer drive learning. No external verifier or separate critic model is required at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements reported across multiple base models and tasks: e.g., ChatGLM-6B OpenBookQA ACC improved from 37.0% (ACC-First) to 42.6% after ISC (a +5.6 percentage-point gain). Across other models and datasets ISC typically increases final ACC relative to ACC-First (tables report ACC-First vs ACC). Zero-shot StrategyQA also showed accuracy improvements after the second-round correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) models with high self-confidence (e.g., Vicuna variants) often refuse to change answers and thus show limited gains; (2) single-step correction is sometimes insufficient — many modifications remain incorrect (substantial W2W counts), indicating multiple iterations may be needed; (3) persistent errors may stem from knowledge insufficiency (lack of factual knowledge) rather than just contextual mis-generation; (4) small models can struggle with self-verification accuracy; (5) imbalanced constructed training data (more bad than good cases) can harm learning if PAM is not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with Chain-of-Thought (COT): ISC is positioned as different (it critiques and revises an initial output rather than only producing multi-step reasoning before an answer). Compared to prior iterative self-feedback approaches (e.g., Self-Refine), ISC is designed to be intrinsic (single model, trained to self-trigger correction) and to work for smaller LMs via PAM; ablations show ISC + PAM outperforms naively training on entire answers without PAM in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation on Partial Answer Masking (PAM): training with PAM generally yields higher ACC and better self-verification (EvalACC) versus training without PAM. Example: CuteGPT-13B w/ PAM OpenBookQA ACC increased to 42.0 from 32.4 w/o PAM; ChatGLM-6B w/ PAM OpenBookQA ACC 42.6 vs 36.0 w/o PAM. Exception: CuteGPT-7B showed a slightly higher ACC-First without PAM on OpenBookQA, but overall PAM improved final ACC and EvalACC. The paper attributes PAM's benefits to preventing the model from learning to reproduce incorrect answers during IFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_info_in_paper</strong></td>
                            <td>The method, training pipeline, and PAM are introduced and evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8760.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8760.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partial Answer Masking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning technique introduced in this paper where, for 'bad' training cases (initial incorrect model outputs), the incorrect portion of the output is masked out of the loss so the model learns from verification and the corrected answer rather than from reproducing incorrect text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied during IFT of the evaluated 6B–13B models (CuteGPT, ChatGLM, Vicuna, Llama2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See ISC entry (PAM used as a training modification during instruction fine-tuning of the listed small LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Partial Answer Masking (PAM) — training component to enable intrinsic self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During fine-tuning, for examples labeled as 'bad' (where the model's initial answer is incorrect), the incorrect answer tokens are excluded from loss computation; only the self-verification signal and the corrected (ground-truth) answer tokens contribute gradients, encouraging the model to learn to verify and then produce a corrected answer rather than learning to generate the incorrect initial output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>OpenBookQA, CommonsenseQA (used in IFT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ISC entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Using PAM during ISC fine-tuning led to higher final ACC and improved self-verification. Example: CuteGPT-13B OpenBookQA ACC w/ PAM 42.0 vs w/o PAM 32.4; ChatGLM-6B OpenBookQA ACC w/ PAM 42.6 vs w/o PAM 36.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>When PAM is not used, models sometimes learn from incorrect initial answers; this reduced EvalACC and final ACC in many cases (see numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Training-time masking of incorrect output tokens (loss exclusion) for bad cases; combined with self-correction training examples and Self-Correction Prompt at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation experiments show consistent ACC gains and higher EvalACC for models fine-tuned with PAM compared to identical training without PAM (see Table 4 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In one case (CuteGPT-7B on OpenBookQA) PAM led to a slight reduction in ACC-First relative to no PAM, though final ACC and self-correction gains generally favored PAM; authors note data imbalance (more bad cases) could affect results if PAM is not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>PAM is presented as complementary to ISC and contrasted with standard IFT where all answer tokens contribute to loss; PAM avoids teaching the model to reproduce incorrect answers and yields better self-verification behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported in Table 4: across multiple models, training with PAM increased final ACC and EvalACC relative to training without PAM (detailed per-model numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8760.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8760.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of simple inference-time prompts (e.g., 'double-check your response for accuracy before proceeding to submit') used to trigger the ISC behavior so that the model performs verification and possible modification in one pass.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used with the fine-tuned models in ISC experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See ISC entry models.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction Prompt (SCP)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Instructional prompt appended to the question/instruction that directs the model to verify and, if necessary, correct its answer before finalizing; the model is trained to follow SCP during IFT so verification and modification occur spontaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applied across the same QA tasks (OpenBookQA, CommonsenseQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ISC entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>SCP is the inference instruction used to activate ISC; performance improvements reported for models trained with ISC (see ISC entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline without SCP corresponds to ACC-First; ISC training aims to enable models to respond to SCP appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (instruction added at input) combined with IFT to internalize the behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>When models are trained with SCP in the self-correction dataset, they produce verification signals and corrected answers leading to higher ACC versus initial answers (ACC-First).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SCP alone (without PAM and appropriate training) may be insufficient for small LMs; paper highlights that complex multi-step prompts used for large LMs are difficult for small LMs to follow.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SCP replaces the multi-step external prompting pipelines (feedback prompt + modification prompt) used in prior large-LM self-refinement pipelines, aiming for a single spontaneous correction step.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8760.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8760.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback (Madaan et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-refinement algorithm that alternates between generating feedback about an initial response and refining that response using that feedback (reported in prior work, applied primarily to large LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large LMs (e.g., ChatGPT / GPT-4 referenced in paper as prior focus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work applied to large-capacity models; details reported in the referenced paper (Madaan et al. 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-refinement (feedback + refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Few-shot prompts guide the model to produce feedback on its initial answer; the feedback is then fed back to the same model to refine the answer, repeating iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (few-shot), iterative feedback loop using the same model as verifier and rewriter.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as prior evidence that large LMs can improve via iterative self-feedback; no direct numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes such methods focus on large LMs and are difficult to migrate to small LMs because the complex prompts used for self-verification and self-modification are hard for small models to follow.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as a prior external iterative method that ISC seeks to internalize and make effective for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8760.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8760.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning (Shinn et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method where language agents use verbal self-reflection and reinforcement learning-style updates to improve behavior iteratively (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language agents (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior agent-framework LMs that iteratively reflect and update behavior; details in referenced paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Verbal reflection / iterative improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents generate reflections about past actions/responses and use them to improve subsequent generations, sometimes with RL-style updates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Agent-style reflection, external memory or learning updates across episodes (described in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as related work that explores self-improvement via reflection; no numbers provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper; referenced to indicate prior focus on larger models/agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned among iterative self-improvement approaches that ISC differs from by focusing on single-model intrinsic correction for smaller LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8760.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8760.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Verification (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification / Self-Feedback (Weng et al. 2022 et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior line of work showing LMs can verify their own outputs (e.g., by predicting masked tokens or computing verification scores) and use that signal to detect hallucinations or select higher-quality answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are reasoners with self-verification.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large LMs (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Works typically analyze model-internal ability to detect inconsistencies or verify answers (various model sizes reported in cited literature).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-verification / internal consistency checking</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Methods where the model evaluates its own outputs, e.g., by masked-token prediction consistency, voting across multiple answers, or producing explicit verification signals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-internal verification processes, often implemented through prompt-based checks or internal consistency metrics; used as prior motivation for ISC.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as evidence that self-verification can be a prerequisite for self-correction in LMs; no numeric results are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that self-verification capability often depends on model size and may be weak in small models, motivating PAM and ISC.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>ISC uses an explicit binary verification signal in training data, inspired by prior self-verification work but adapted to small-model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8760.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8760.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (COT) (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits multi-step intermediate reasoning (COT) before producing a final answer; used in this paper to generate initial diverse analysis when constructing self-correction training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used as part of data generation (gpt-3.5-turbo and instruction-following LMs used to produce COT during dataset construction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>COT is a prompting technique rather than a specific model; the authors used diverse COT prompts with an instruction-following model to generate reasoning traces for their training data.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought prompting (used as part of data construction, contrasted with ISC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompts such as 'Please give the detailed solving process' or 'Let's think step by step' used to make the model produce intermediate reasoning prior to answers; the paper says ISC is distinct from COT because ISC critiques and revises initial outputs rather than only producing multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>used in generating self-correction training data for QA tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ISC entry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to elicit multi-step reasoning traces used in constructing training examples (and to improve quality of generated candidate answers).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>COT was used to ensure quality of generated answers in the dataset; authors explicitly state ISC is not a COT strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not the target method for self-correction; authors emphasize ISC differs from COT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>COT aims to improve answers via multi-step reasoning; ISC focuses on critique + revision of initial answer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Model Can Self-correct', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative Refinement with Self-Feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning. <em>(Rating: 2)</em></li>
                <li>Large language models are reasoners with self-verification. <em>(Rating: 2)</em></li>
                <li>Generating Sequences by Learning to Self-Correct. <em>(Rating: 1)</em></li>
                <li>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8760",
    "paper_id": "paper-266999677",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "ISC",
            "name_full": "Intrinsic Self-Correction",
            "brief_description": "An instruction-fine-tuning approach that endows LMs with an intrinsic, self-triggered capability to verify and, if necessary, modify their own initial answers (self-verification + self-modification) using a single Self-Correction Prompt and training data constructed to mimic human self-correction; trained with Partial Answer Masking (PAM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CuteGPT-7B, CuteGPT-13B, ChatGLM-6B, Vicuna-7B, Vicuna-13B (also reported experiments on Llama2-7B)",
            "model_description": "Open-source instruction-tuned models of scale 6B–13B parameters: CuteGPT family (7B, 13B; LLaMA-based, Chinese-vocab expansion), ChatGLM-6B (bilingual ~6B), Vicuna (7B, 13B; fine-tuned LLaMA), Llama2-7B (Meta, 7B). Models were further fine-tuned for ISC via IFT (full fine-tune for CuteGPT, LoRA for Vicuna/Llama2, prompt-tuning for ChatGLM).",
            "reflection_method_name": "Intrinsic Self-Correction (ISC) with Self-Correction Prompt (SCP)",
            "reflection_method_description": "The model generates an initial answer plus (optionally) chain-of-thought, then internally produces a binary self-verification signal (v) and, if v indicates an error, produces a revised answer; training uses constructed self-correction examples (good/bad cases) and Partial Answer Masking so the model learns to verify and modify without relying on external verifiers; at inference models are given up to two attempts (initial + one correction).",
            "task_name": "OpenBookQA, CommonsenseQA (and zero-shot test on StrategyQA)",
            "task_description": "Multiple-choice question answering datasets: OpenBookQA (elementary science multiple-choice, ~5,957 questions, 4 options); CommonsenseQA (~12,102 questions, 5 options). StrategyQA used as a held-out zero-shot test requiring true/false answers and multi-step reasoning.",
            "performance_with_reflection": "Per-model aggregated results (after ISC self-correction, using PAM unless noted): CuteGPT-7B — OpenBookQA ACC after correction 29.0 (ACC-First 25.2), CommonsenseQA ACC after correction 26.9 (ACC-First 23.2). CuteGPT-13B — OpenBookQA ACC after correction 42.0 (ACC-First 37.2), CommonsenseQA ACC after correction 37.9 (ACC-First 36.6). ChatGLM-6B — OpenBookQA ACC after correction 42.6 (ACC-First 37.0; improvement +5.6 percentage points), CommonsenseQA ACC after correction 38.7 (ACC-First 34.3). Vicuna-7B — OpenBookQA ACC after correction 28.8 (ACC-First 28.6), CommonsenseQA ACC after correction 26.0 (ACC-First 25.9). Vicuna-13B — OpenBookQA ACC after correction 34.0 (ACC-First 33.8), CommonsenseQA ACC after correction 32.6 (ACC-First 33.8). (Numbers taken from the paper's reported ACC-First and ACC columns; ISC also improved zero-shot StrategyQA accuracy after one correction for all LMs.)",
            "performance_without_reflection": "The initial-answer accuracies (ACC-First) before any correction: CuteGPT-7B OpenBookQA 25.2, CommonsenseQA 23.2; CuteGPT-13B OpenBookQA 37.2, CommonsenseQA 36.6; ChatGLM-6B OpenBookQA 37.0, CommonsenseQA 34.3; Vicuna-7B OpenBookQA 28.6, CommonsenseQA 25.9; Vicuna-13B OpenBookQA 33.8, CommonsenseQA 33.8. (These are the ACC-First baseline numbers reported in the paper.)",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented via instruction fine-tuning on synthetic self-correction examples (Question + generated COT + initial answer + verification signal + corrected answer). Self-correction realized by: (1) prompting at inference with Self-Correction Prompt (SCP) that asks model to double-check before finalizing, and (2) training with Partial Answer Masking (PAM) so that bad-case incorrect initial answers are masked out of loss and only verification + corrected answer drive learning. No external verifier or separate critic model is required at inference.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Quantitative improvements reported across multiple base models and tasks: e.g., ChatGLM-6B OpenBookQA ACC improved from 37.0% (ACC-First) to 42.6% after ISC (a +5.6 percentage-point gain). Across other models and datasets ISC typically increases final ACC relative to ACC-First (tables report ACC-First vs ACC). Zero-shot StrategyQA also showed accuracy improvements after the second-round correction.",
            "limitations_or_failure_cases": "Reported limitations include: (1) models with high self-confidence (e.g., Vicuna variants) often refuse to change answers and thus show limited gains; (2) single-step correction is sometimes insufficient — many modifications remain incorrect (substantial W2W counts), indicating multiple iterations may be needed; (3) persistent errors may stem from knowledge insufficiency (lack of factual knowledge) rather than just contextual mis-generation; (4) small models can struggle with self-verification accuracy; (5) imbalanced constructed training data (more bad than good cases) can harm learning if PAM is not used.",
            "comparison_to_other_methods": "Contrasted with Chain-of-Thought (COT): ISC is positioned as different (it critiques and revises an initial output rather than only producing multi-step reasoning before an answer). Compared to prior iterative self-feedback approaches (e.g., Self-Refine), ISC is designed to be intrinsic (single model, trained to self-trigger correction) and to work for smaller LMs via PAM; ablations show ISC + PAM outperforms naively training on entire answers without PAM in most cases.",
            "ablation_study_results": "Ablation on Partial Answer Masking (PAM): training with PAM generally yields higher ACC and better self-verification (EvalACC) versus training without PAM. Example: CuteGPT-13B w/ PAM OpenBookQA ACC increased to 42.0 from 32.4 w/o PAM; ChatGLM-6B w/ PAM OpenBookQA ACC 42.6 vs 36.0 w/o PAM. Exception: CuteGPT-7B showed a slightly higher ACC-First without PAM on OpenBookQA, but overall PAM improved final ACC and EvalACC. The paper attributes PAM's benefits to preventing the model from learning to reproduce incorrect answers during IFT.",
            "citation_info_in_paper": "The method, training pipeline, and PAM are introduced and evaluated in this paper.",
            "uuid": "e8760.0",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PAM",
            "name_full": "Partial Answer Masking",
            "brief_description": "A fine-tuning technique introduced in this paper where, for 'bad' training cases (initial incorrect model outputs), the incorrect portion of the output is masked out of the loss so the model learns from verification and the corrected answer rather than from reproducing incorrect text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied during IFT of the evaluated 6B–13B models (CuteGPT, ChatGLM, Vicuna, Llama2-7B)",
            "model_description": "See ISC entry (PAM used as a training modification during instruction fine-tuning of the listed small LMs).",
            "reflection_method_name": "Partial Answer Masking (PAM) — training component to enable intrinsic self-correction",
            "reflection_method_description": "During fine-tuning, for examples labeled as 'bad' (where the model's initial answer is incorrect), the incorrect answer tokens are excluded from loss computation; only the self-verification signal and the corrected (ground-truth) answer tokens contribute gradients, encouraging the model to learn to verify and then produce a corrected answer rather than learning to generate the incorrect initial output.",
            "task_name": "OpenBookQA, CommonsenseQA (used in IFT)",
            "task_description": "See ISC entry.",
            "performance_with_reflection": "Using PAM during ISC fine-tuning led to higher final ACC and improved self-verification. Example: CuteGPT-13B OpenBookQA ACC w/ PAM 42.0 vs w/o PAM 32.4; ChatGLM-6B OpenBookQA ACC w/ PAM 42.6 vs w/o PAM 36.0.",
            "performance_without_reflection": "When PAM is not used, models sometimes learn from incorrect initial answers; this reduced EvalACC and final ACC in many cases (see numbers above).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Training-time masking of incorrect output tokens (loss exclusion) for bad cases; combined with self-correction training examples and Self-Correction Prompt at inference.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Ablation experiments show consistent ACC gains and higher EvalACC for models fine-tuned with PAM compared to identical training without PAM (see Table 4 in the paper).",
            "limitations_or_failure_cases": "In one case (CuteGPT-7B on OpenBookQA) PAM led to a slight reduction in ACC-First relative to no PAM, though final ACC and self-correction gains generally favored PAM; authors note data imbalance (more bad cases) could affect results if PAM is not used.",
            "comparison_to_other_methods": "PAM is presented as complementary to ISC and contrasted with standard IFT where all answer tokens contribute to loss; PAM avoids teaching the model to reproduce incorrect answers and yields better self-verification behavior.",
            "ablation_study_results": "Reported in Table 4: across multiple models, training with PAM increased final ACC and EvalACC relative to training without PAM (detailed per-model numbers provided).",
            "uuid": "e8760.1",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SCP",
            "name_full": "Self-Correction Prompt",
            "brief_description": "A family of simple inference-time prompts (e.g., 'double-check your response for accuracy before proceeding to submit') used to trigger the ISC behavior so that the model performs verification and possible modification in one pass.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "used with the fine-tuned models in ISC experiments",
            "model_description": "See ISC entry models.",
            "reflection_method_name": "Self-Correction Prompt (SCP)",
            "reflection_method_description": "Instructional prompt appended to the question/instruction that directs the model to verify and, if necessary, correct its answer before finalizing; the model is trained to follow SCP during IFT so verification and modification occur spontaneously.",
            "task_name": "Applied across the same QA tasks (OpenBookQA, CommonsenseQA)",
            "task_description": "See ISC entry.",
            "performance_with_reflection": "SCP is the inference instruction used to activate ISC; performance improvements reported for models trained with ISC (see ISC entry).",
            "performance_without_reflection": "Baseline without SCP corresponds to ACC-First; ISC training aims to enable models to respond to SCP appropriately.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering (instruction added at input) combined with IFT to internalize the behavior.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "When models are trained with SCP in the self-correction dataset, they produce verification signals and corrected answers leading to higher ACC versus initial answers (ACC-First).",
            "limitations_or_failure_cases": "SCP alone (without PAM and appropriate training) may be insufficient for small LMs; paper highlights that complex multi-step prompts used for large LMs are difficult for small LMs to follow.",
            "comparison_to_other_methods": "SCP replaces the multi-step external prompting pipelines (feedback prompt + modification prompt) used in prior large-LM self-refinement pipelines, aiming for a single spontaneous correction step.",
            "ablation_study_results": null,
            "uuid": "e8760.2",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Refine (mentioned)",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback (Madaan et al. 2023)",
            "brief_description": "An iterative self-refinement algorithm that alternates between generating feedback about an initial response and refining that response using that feedback (reported in prior work, applied primarily to large LMs).",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback.",
            "mention_or_use": "mention",
            "model_name": "large LMs (e.g., ChatGPT / GPT-4 referenced in paper as prior focus)",
            "model_description": "Prior work applied to large-capacity models; details reported in the referenced paper (Madaan et al. 2023).",
            "reflection_method_name": "Iterative self-refinement (feedback + refinement)",
            "reflection_method_description": "Few-shot prompts guide the model to produce feedback on its initial answer; the feedback is then fed back to the same model to refine the answer, repeating iteratively.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering (few-shot), iterative feedback loop using the same model as verifier and rewriter.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as prior evidence that large LMs can improve via iterative self-feedback; no direct numbers provided in this paper.",
            "limitations_or_failure_cases": "Paper notes such methods focus on large LMs and are difficult to migrate to small LMs because the complex prompts used for self-verification and self-modification are hard for small models to follow.",
            "comparison_to_other_methods": "Presented as a prior external iterative method that ISC seeks to internalize and make effective for smaller models.",
            "ablation_study_results": null,
            "uuid": "e8760.3",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Reflexion (mentioned)",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning (Shinn et al. 2023)",
            "brief_description": "A prior method where language agents use verbal self-reflection and reinforcement learning-style updates to improve behavior iteratively (mentioned in related work).",
            "citation_title": "Reflexion: Language Agents with Verbal Reinforcement Learning.",
            "mention_or_use": "mention",
            "model_name": "large language agents (prior work)",
            "model_description": "Prior agent-framework LMs that iteratively reflect and update behavior; details in referenced paper.",
            "reflection_method_name": "Verbal reflection / iterative improvement",
            "reflection_method_description": "Agents generate reflections about past actions/responses and use them to improve subsequent generations, sometimes with RL-style updates.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Agent-style reflection, external memory or learning updates across episodes (described in prior work).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as related work that explores self-improvement via reflection; no numbers provided here.",
            "limitations_or_failure_cases": "Not discussed in detail in this paper; referenced to indicate prior focus on larger models/agents.",
            "comparison_to_other_methods": "Positioned among iterative self-improvement approaches that ISC differs from by focusing on single-model intrinsic correction for smaller LMs.",
            "ablation_study_results": null,
            "uuid": "e8760.4",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Verification (mentioned)",
            "name_full": "Self-Verification / Self-Feedback (Weng et al. 2022 et al.)",
            "brief_description": "Prior line of work showing LMs can verify their own outputs (e.g., by predicting masked tokens or computing verification scores) and use that signal to detect hallucinations or select higher-quality answers.",
            "citation_title": "Large language models are reasoners with self-verification.",
            "mention_or_use": "mention",
            "model_name": "large LMs (prior work)",
            "model_description": "Works typically analyze model-internal ability to detect inconsistencies or verify answers (various model sizes reported in cited literature).",
            "reflection_method_name": "Self-verification / internal consistency checking",
            "reflection_method_description": "Methods where the model evaluates its own outputs, e.g., by masked-token prediction consistency, voting across multiple answers, or producing explicit verification signals.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Model-internal verification processes, often implemented through prompt-based checks or internal consistency metrics; used as prior motivation for ISC.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as evidence that self-verification can be a prerequisite for self-correction in LMs; no numeric results are reported in this paper.",
            "limitations_or_failure_cases": "Paper notes that self-verification capability often depends on model size and may be weak in small models, motivating PAM and ISC.",
            "comparison_to_other_methods": "ISC uses an explicit binary verification signal in training data, inspired by prior self-verification work but adapted to small-model fine-tuning.",
            "ablation_study_results": null,
            "uuid": "e8760.5",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chain-of-Thought (COT) (mentioned)",
            "name_full": "Chain-of-Thought reasoning",
            "brief_description": "A prompting strategy that elicits multi-step intermediate reasoning (COT) before producing a final answer; used in this paper to generate initial diverse analysis when constructing self-correction training examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "used as part of data generation (gpt-3.5-turbo and instruction-following LMs used to produce COT during dataset construction)",
            "model_description": "COT is a prompting technique rather than a specific model; the authors used diverse COT prompts with an instruction-following model to generate reasoning traces for their training data.",
            "reflection_method_name": "Chain-of-Thought prompting (used as part of data construction, contrasted with ISC)",
            "reflection_method_description": "Prompts such as 'Please give the detailed solving process' or 'Let's think step by step' used to make the model produce intermediate reasoning prior to answers; the paper says ISC is distinct from COT because ISC critiques and revises initial outputs rather than only producing multi-step reasoning.",
            "task_name": "used in generating self-correction training data for QA tasks",
            "task_description": "See ISC entry tasks.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering to elicit multi-step reasoning traces used in constructing training examples (and to improve quality of generated candidate answers).",
            "number_of_iterations": null,
            "evidence_for_improvement": "COT was used to ensure quality of generated answers in the dataset; authors explicitly state ISC is not a COT strategy.",
            "limitations_or_failure_cases": "Not the target method for self-correction; authors emphasize ISC differs from COT.",
            "comparison_to_other_methods": "COT aims to improve answers via multi-step reasoning; ISC focuses on critique + revision of initial answer.",
            "ablation_study_results": null,
            "uuid": "e8760.6",
            "source_info": {
                "paper_title": "Small Language Model Can Self-correct",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Large language models are reasoners with self-verification.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_reasoners_with_selfverification"
        },
        {
            "paper_title": "Generating Sequences by Learning to Self-Correct.",
            "rating": 1,
            "sanitized_title": "generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.",
            "rating": 1,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        }
    ],
    "cost": 0.016618749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Small Language Model Can Self-Correct</p>
<p>Haixia Han haixiahan03@gmail.com 
Shanghai Institute of AI for Education
School of Computer Science and Technology
East China Normal University</p>
<p>Jiaqing Liang liangjiaqing@fudan.edu 
School of Data Science
Fudan University</p>
<p>Jie Shi 
School of Computer Science
Shanghai Key Laboratory of Data Science
Fudan University</p>
<p>Qianyu He 
School of Computer Science
Shanghai Key Laboratory of Data Science
Fudan University</p>
<p>Yanghua Xiao 
Shanghai Institute of AI for Education
School of Computer Science and Technology
East China Normal University</p>
<p>School of Computer Science
Shanghai Key Laboratory of Data Science
Fudan University</p>
<p>Small Language Model Can Self-Correct
71F6FEBDEDAE4644ACBE10BED234CFEC
Generative Language Models (LMs) such as ChatGPT have exhibited remarkable performance across various downstream tasks.Nevertheless, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.Previous studies have devised sophisticated pipelines and prompts to induce large LMs to exhibit the capability for self-correction.However, large LMs are explicitly prompted to verify and modify their answers separately rather than completing all steps spontaneously like humans.Moreover, these complex prompts are extremely challenging for small LMs to follow.In this paper, we introduce the Intrinsic Self-Correction (ISC) in generative language models, aiming to correct the initial output of LMs in a self-triggered manner, even for those small LMs with 6 billion parameters.Specifically, we devise a pipeline for constructing selfcorrection data and propose Partial Answer Masking (PAM), aiming to endow the model with the capability for intrinsic self-correction through fine-tuning.We conduct experiments using LMs with parameters sizes ranging from 6 billion to 13 billion in two tasks, including commonsense reasoning and factual knowledge reasoning.Our experiments demonstrate that the outputs generated using ISC outperform those generated without self-correction.We believe that the output quality of even small LMs can be further improved by empowering them with the ability to intrinsic self-correct.</p>
<p>Introduction</p>
<p>Generative Language Models (LMs) have gained considerable attention due to their remarkable capabilities (Guo et al. 2023;Suzgun et al. 2023).Despite the convincing and realistic nature of text generated by these LMs, a concern with LMs lies in their tendency to produce fabricated facts and generate false information (Lin, Hilton, and Evans 2022).Moreover, these models deliver inaccurate information employing unequivocal expressions, which poses substantial risks as it can lead to the spread of misleading and harmful content.</p>
<p>One of the contributing factors to the hallucination lies in inadequate acquisition of knowledge (Manakul, Liusie, and Gales 2023;Huang et al. 2023).For example, consider the question Which animal is China's nation treasure?, LMs Figure 1: Two self-correction methods are demonstrated in language models in response to a query.The gray line on the left illustrates the process of self-correction employing prompt engineering in large language models like Chat-GPT.The red line shows the overall steps of our proposed Intrinsic Self-Correction, where self-verification and selfmodification occur spontaneously.may provide a different animal name like tiger instead of panda due to a lack of relevant knowledge.Considerable efforts have been made to alleviate such hallucination induced by lacking of knowledge in LMs.One approach involves supervised fine-tuning LMs with standard ground-truth answers to enhance their comprehension of relevant knowledge (Wei et al. 2022a;Ouyang et al. 2022).This method has shown promising efficacy.However, it demands a significant amount of high-quality annotated data for training.Additionally, other methods have relied on external verifier or critic model to evaluate the accuracy of a statement (Yang et al. 2022;Paul et al. 2023).Training a verifier necessitates a large number of high-quality evaluation annotations and further fine-tuning of the model, which restricts its broad applicability to other tasks and domains.</p>
<p>Another reason for an LM to provide incorrect response is intrinsically linked to the design architecture of genera-tive language models themselves (Azaria and Mitchell 2023;Paul et al. 2023;Shinn et al. 2023).It is widely acknowledged that LMs generate a sentence by maximizing the likelihood of the next token given all previous tokens.Subtle differences in the preceding sentences can potentially lead to diverse generation outcomes.For example, when the question is Who is the author of The Analects?, the model gives the correct answer as Confucius.However, when the input question becomes Is Laozi or Confucius the author of the Analects of Confucius?, the model is likely to generate an answer of Laozi.In this case, the model has the ability to rely on its knowledge to recognize false information (Schick et al. 2023).This process is akin to how humans perform self-verification of answers to minimize mistakes (Flower and Hayes 1981).Moreover, when we realize our answer is wrong, we further modify it.Motivated by this, when the model itself detects the potential hallucination, the next step is to correct the error or mistake.Once the model incorporates this inherent self-correction mechanism, it can address similar issues in other domains and achieve selfimprovement.</p>
<p>The existing work (Madaan et al. 2023;Ganguli et al. 2023) towards self-correction in LMs has mainly focused on lager models like ChatGPT and GPT4, which is challenging to migrate these self-correction methods to small LMs.Some studies indicated that the self-correction ability depends on model parameters and only emerges in models with larger parameter sizes (Azaria and Mitchell 2023).The main reason is that they devised a sophisticated pipeline and zero-shot prompts to achieve self-correction.However, these prompts crafted for self-verification and self-modification are difficult for small models to understand.As depicted in Figure 1, upon generating the initial answer to the given question, an additional feedback instruction is utilized to guide ChatGPT in generating feedback information regarding the initial answer.This information contains an evaluation of the correctness of the initial answer.A subsequent modification instruction is employed to alter or refine the initial answer based on the feedback received.</p>
<p>Nevertheless, small models typically lack self-awareness (Weng et al. 2022) and tend to exhibit greater confidence in their generated responses.Consequently, they struggle to assess the quality of their generated outcomes.The capability for self-verification serves as a prerequisite for achieving self-correction.Furthermore, the manner in which selfcorrection is achieved through multi-step prompt engineering within LMs differs from the spontaneous and one-time correction observed in humans.</p>
<p>To empower the capability for self-correction in small language models, we propose Intrinsic Self-Correction (ISC), an intrinsic mechanism that relies on two basic abilities: selfverification and self-modification.At its core, the LM provides a response and subsequently evaluates its own answer.Upon identifying an error, the same LM adjusts its initial response.Conversely, if the answer is validated as accurate, no further modifications are required.The self-correction process is not divided into two separate steps, but rather constitutes a single comprehensive step, as depicted by the red arrowed segment in Figure 1.We trained the LM to process the self-correction through Instruction Fine-Tuning (IFT).For this purpose, we design the data processing procedure to construct the self-correction data and define the data format.During the fine-tuning process, we propose Partial Answer Masking (PAM) to make the model have the ability of selfverification.Our contributions are summarized as follows:</p>
<p>• To the best of our knowledge, we are the first to demonstrate that small language models with even 6 billion parameters possess the capacity for self-correction during response generation without relying ground truth.Chain-of thought in language model.Chain-of-Thought (COT), as an alternative prompt strategy, adopts a chained reasoning approach, incorporating a multi-step reasoning path before generating the final answer.This strategy aims to enhance the model's capacity to handle complex and multistep queries.Kojima et al. (2022) introduced a novel yet simple approach to prompt the LM by using the phase Let's think step by step, which enables reasoning generation in a zero-shot manner.Zhou et al. (2023) further decomposed the questions into multiple sub-questions, encouraging the model to engage in a sequential thought process while arriving at the answer.Overall, COT aims to enhance the answer quality through a multi-step reasoning process.</p>
<p>In contrast, our proposed intrinsic self-correction in LMs involves critiquing and subsequently revising the initially generated results.It does not fall under the classification of a COT strategy.</p>
<p>Self-improvement in language model.Richer and more detailed feedback plays a crucial role in aligning the output of the LM with the user's preferences and in significantly improving the overall performance of the model.Scaling model size can increase model's various capabilities (Kaplan et al. 2020), including self-verification (or selffeedback).Recent research efforts have focused on exploring large LMs that employ self-feedback rather than relying on additional models.Madaan et al. (2023) introduced an iterative self-refinement algorithm that alternates between feedback and refinement.They guided the model to provide feedback about the initial response by employing fewshot prompts.This feedback is then passed back to the same model to help refine the initial response.Some methods used self-improvement to improve the reasoning ability of LM without supervised data.For example, the LM first generates multiple COT reasoning paths and corresponding answers for each question (Wei et al. 2022b).The answer with the highest consistency is selected by majority voting.The COT reasoning paths exhibiting high-confidence, along with the selected answers, are augmented by mixed formats to serve as the training data used for IFT.Additionally, the LM verified its multiple generated answers in turn and calculated the verification score based on the number of predicted masked values (Weng et al. 2022).Instead of requesting the LM to answer the same query multiple times, a more effective approach is to prompt the LM to rephrase the query and then answer each rephrased question individually.</p>
<p>We also focus on providing self-feedback and enabling self-modification to enhance the quality of generation of LMs.Furthermore, we extend this capability to encompass more generalized models, including smaller LMs.</p>
<p>Methods</p>
<p>Task Formulation</p>
<p>In this paper, Intrinsic Self-Correction is employed to accomplish auto-regressive tasks.Given an input sequence x, an LM M is tasked with generating an output y.Typically, to generate a correct or plausible output, the model needs to incorporate explanation or reasoning, denoted as z, as intermediate steps.The process can be described as follows:
p(z, y 0 |x) = p(y 0 |x, z)p(z|x),(1)
where y 0 denotes initial output.The same model M provides self-verification of the output to assess the accuracy of y 0 , represented as p(v|x, z, y 0 ), where v represents the verification of y 0 and it is a binary feedback signal.Upon detecting a fault, the LM M proceeds to self-modify and generate a revised answer y.If no errors are found, there is no need for any modification, and in such case, y = y 0 .Ultimately, y represents the final response of the model to the input sequence, as given by:
p(y|x) = p(y|x, z, y 0 , v)(2)
It is important to note that the stages of generating initial output, self-verification, and self-modification (if necessary) are not carried out separately.Rather, they are accomplished with just an instruction, referred to as the Self-Correction Prompt (SCP).There are some prompt examples of SCP: double-check your response for accuracy before proceeding to submit, before you finalize your answer, please reexamine it to ensure its correctness and please review your response carefully to make sure it is correct before submitting.</p>
<p>Question: [TaskP] Examine the following options carefully and select the correct one.</p>
<p>[COTP] Before providing your final answer, give the analysis steps.</p>
<p>[SCP] And you need double-check your response for accuracy before proceeding to submit.</p>
<p>[question] Where do you buy tickets at a ticket booth for games? A. train station B. cathedral C. metro station D. fairgrounds E. amusement park Answer: [A 1 1 -COT] The question mentions the keywords "buying tickets" and "games", so we can guess that this is a
• • • [A 1 1 ] Therefore,</p>
<p>Self-correction Data</p>
<p>We mimic human self-correction behavioral patterns to design self-correction data format as shown in Table 1.We define the self-correction data format as a Question-Answer pair.Next, we will elaborate on how to construct each component separately.Question preparation.We utilize an LM M , which is capable of following instructions, to generate answers for a set of questions.These questions are randomly sampled from datasets on various tasks and come with ground truth.</p>
<p>To ensure the quality of the generated answers, we leverage the COT to instruct the model M to initially generate a problem-solving process and finally provide an answer to the given question.Therefore, we design a range of diverse COT prompts (COTPs) to guide the M to generate the COT analysis.For example, we use prompts like, Please select the correct option from the provided choices and offer a comprehensive problem-solving process.In the few shot setting, the whole prompt also contains other question-COTPs and answer examples.We combine task prompts (TaskP) for responding to specific question types, instructions for producing COT analysis processes (COTP), instructions for selfcorrection (SCP), and the question itself.This constitutes the Question part for the self-correcting data.</p>
<p>Additionally, we use two methods to enhance the model's understanding of various self-correction instructions.First, we use gpt-3.5-turbo to generate diverse prompts for different types of instructions mentioned above.We present a brief prompt template below I want you act as a Prompt Rewriter.Your objective is to rewrite a given prompt to make language model to understand.The rewritten prompt must ensure that the requirement remains unchanged.#Given Prompt#:[T askP | COT P | SCP ].Second, we randomly select one instruction from TaskP, COTP and SCP sets and combine them.The order of these instructions can also be rearranged, but TaskP is positioned at the beginning.</p>
<p>Answer preparation.To ensure answer diversity, we utilize nucleus sampling (Holtzman et al. 2020) to generate multiple answers.After generating multiple answers for a question, the next step is to evaluate the accuracy of each answer by comparing it with the provided ground truth for the each question.In the case of multiple-choice questions, we extract the options of the final answer through string matching and then directly compare them with the standard answer to check the accuracy.</p>
<p>dui For a good case, the outcome of self-verification should be positive, indicating there is no need to modify the initial answers.Accordingly, given the question x, the Answer is represented as (A 1 1 -COT||A 1 1 ||P V ), where A i n represents the model attempts n times to obtain the correct answer, and the current answer is the ith response, A i n -COT represents the COT process of the answer, P V denotes the positive verification in self-correction, and || represents the concatenation.Here, we set the verification as a binary signal.A positive verification can be set like I am sure my answer is correct.</p>
<p>Conversely, for a bad case, negative self-verification result is excepted, such as Sorry, there is an error in the previous answer.It requires modifications to the initial answer.To enhance the model's ability to generate more appropriate reasoning process and correct answer, we utilize the ground truth, representing the standard answer, as the revised answer.Additionally, we employ gpt-3.5-turbo to assist in generating the COT analysis process, denoted as G.We use prompts like the answer of [Question] is [Ground Truth].Please provide a step-by-step explanation for resolving the given problem.Therefore, the data format is (A
COT||A 1 n ||N V ||A 2 n -COT||A 2 n • • • A n n ||P V ),
where N V indicates negative verification.In Table 1</p>
<p>, we provide two general examples of self-correction data, representing examples</p>
<p>where the correct answer is obtained without correction and with one correction respectively.We also provide detailed prompt examples used at each step in the Appendix.</p>
<p>This pipeline can be utilized to customize the selfcorrection data for various corrections, depending on the specific task type.The general process of constructing selfcorrection data is shown in Figure 2.</p>
<p>Partial Answer Masking</p>
<p>In the instruction fine-tuning stage, the goal is to guide the model to follow human intention by referring to annotated data.Usually, only the loss associated with the answer component of the training data is used for gradient backpropagation, while the loss related to the input is not employed in updating weights.</p>
<p>In our paper, we apply this loss calculation method to good cases.However, it is not suitable for bad cases due to two reasons.First, these bad cases contain error information, which increases the likelihood of the common issue of hallucination inherent in language models.Second, deliberately training the model to first generate incorrect answers and then correcting them does not align with our intention of teaching the LM how to self-correct.Our aim is instead to enable the model to spontaneously correct itself when it generates information inconsistent with its internal knowledge.Therefore, for bad cases, we refrain from computing the loss corresponding to the incorrect answer part in the output during training.Instead, we calculate the loss from the output on self-verification.It means that for bad cases, only the self-verification and modified correct answer contribute to the loss calculation and parameter update.We call this training method Partial Answer Masking (PAM).In Table 1, the underline part in the output is excluded from the loss calculation.</p>
<p>Experiments Experimental Settings</p>
<p>Datasets.We conduct experiments on two questionanswering datasets, including OpenBookQA1 and CommonsenseQA2 .OpenBookQA is a science questionanswering dataset, containing 5,957 elementary-level science multiple-choice questions with 4 options each.These questions evaluate human comprehension of 1,326 core science facts and their application to novel scenarios.CommonsenseQA is a single choice question-answering dataset that necessitates diverse forms of commonsense knowledge for accurate answer prediction.It comprises 12,102 questions with 5 choices each.After performing our proposed self-correction data construction process on the two datasets, the training data comprises about 15,000 self-correction samples.We use another about 1,700 examples as test data, of which 500 are from OpenBookQA and 1,200 are from CommonsenseQA.Base language models.Our goal is to evaluate whether we can improve the performance of any LMs through our proposed ISC, even to small LMs.We opt for open-source models with parameters ranging between 6 billion and 13 billion.We use CuteGPT-7B, CuteGPT-13B3 , ChatGLM-6B4 , Llama2-7B5 , Vicuna-7B6 and Vicuna-13B7 as our instruction fine-tuning base models for continuing training.These models have been fine-tuned on the instruction data and have the ability to follow instructions.CuteGPT is based on the original Llama model structure, expands the Chinese vocabulary and performs pre-training.CuteGPT has two public versions: CuteGPT-7B and CuteGPT-13B.ChatGLM-6B also is an open bilingual language model.It is trained for about 1T tokens of Chinese and English corpus, and demonstrates outstanding performance among language models of equal parameter size.Llama2 is a family of state-of-the-art open-access large language models released by Meta, ranging in scale from 7 billion to 70 billion parameters.Vicuna is also an open-source chatbot trained by fine-tuning Llama.</p>
<p>In the instruction fine-tuning stage, these models employ distinct training strategies.CuteGPT family models employ full fine-tuning, Llama2-7B and Vicuna family models utilize Low-Rank Adaptation (LORA), and ChatGLM-6B employs Prompt-tuning.</p>
<p>Metrics.In our experiment, a model is given two chances to answer questions.It only attempts to correct itself after making an error in the first response.To evaluate the accuracy of generation answer, we employ string matching to extract the LM's option answers and compare them with the ground truth provided in datasets.We use several evaluation metrics to assess the various performance of the ISC.We introduce the following evaluation metrics:</p>
<p>• ACC-First: it represents the accuracy of the initial answer of LM. • ACC: it denotes the accuracy after correction of LM.</p>
<p>• EvalACC: this metric represents the probability of the LM correctly evaluating its own initial answer.It reflects the self-verification ability of the LM.• Confidence: it refers to the level of confidence of LM, which represents the proportion of questions where LMs assess their answers as correct.</p>
<p>Besides, in our experiments, to assess the capability for self-modification of LM, we use R2R, R2W, W2R, and W2W to denote the times that the LM modifies the correct answer to the right answer, changes the right answer to the wrong answer, changes the wrong answer to another incorrect answer, and modifies the wrong answer to the right answer, respectively.</p>
<p>Results and Analysis</p>
<p>Main Results.We conduct experiments to analyze the accuracy after applying ISC on the test data.The results are presented in Table 2. Following the application of selfcorrection, a noticeable enhancement in accuracy is observed across all models for the two given tasks.This implies that the integration of ISC provides the base models with self-correction capabilities.This intrinsic ability allows the model to modify the answer when it detects an error in the initial response generation.For example, in the case of ChatGLM-6B, correcting answers yields a notable improvement of 5.6% in accuracy on the OpenBookQA dataset, improving it from 37% to 42.6%.</p>
<p>The effectiveness of enhancing the self-correction of small LMs depends on the innate abilities.The variations in effects of enhancement differ among models with similar parameter scales.For instance, among these base mod-els, ChatGLM-6B features the smallest parameter scale, yet it demonstrates the most prominent improvement.On the other hand, while Llama2-7B attains the highest accuracy in its answers, making corrections to its responses proves to be challenging.</p>
<p>Quantitative Analysis.In the part, we delve further into the analysis of the intrinsic self-correction of base models.The results are shown in Table 3.</p>
<p>Discovery 1: When base models lack strong inherent capabilities but exhibit a high degree of confidence in their generated outcomes, the accuracy of self-verification tends to be notably lower.The performance gains derived from self-correction remain constrained.Taking Vicuna-13B as an example, it shows high confidence in its responses, which makes it challenging to accurately evaluate its initially generated answer, resulting in minimal attempts to modify its initial responses.</p>
<p>Discovery 2: Observing the values in the W2R column, we find that if an LM can recognize errors in its responses and attempt to modify the initial answers, an opportunity arises to transform them into accurate solutions.This supports the earlier assumption that hallucination in LM output is not solely attributed to knowledge deficits, but is also linked to contextual texts.In this case, relying on the model itself for correction becomes feasible.</p>
<p>Discovery 3: The values in the W2W column also constitute a significant portion of the total modifications.However, despite undergoing self-correction, the models have not achieved successful revisions.Two factors account for this phenomenon.Firstly, a single corrective step is not enough to help LM answer correctly, potentially leading to persistent incorrect answers.Employing multiple iterations of corrections could potentially reduce the W2W values to some extent.Secondly, the persistence of incorrect responses by the model could primarily result from knowledge insufficiency.The infusion of relevant domain knowledge is a method to be considered.</p>
<p>Discovery 4: By examining the values within the R2R column, we discern that even though our experiments primarily focused on self-verification of the final answers rather  than the problem-solving process, we note that the models, through self-correction, could identify inadequacies in the analytical process and subsequently provide supplementary analysis.</p>
<p>We conduct case study and present several instances about W2R and R2R cases in the Appendix.</p>
<p>Ablation Analysis.In this part, we verify the impact of the PAM on the LM's self-correction through ablation analysis.Unlike PAM, a commonly used training method in IFT involves utilizing the entire answer for loss calculation.By employing identical data, model architecture, and hyper parameters settings, we conducted a comparative evaluation of the effects of the PAM on self-correction, as illustrated in Table 4.</p>
<p>Except for CuteGPT-7B without PAM, which exhibits a slightly higher initial answer accuracy on the OpenBook dataset compared to CuteGPT-7B with PAM, nearly all results indicate that employing the PAM leads to higher accuracy in generating answers.Furthermore, the improvement in answer quality through self-correction is most prominent after utilizing the PAM.</p>
<p>Training without PAM appears to reduce the accuracy of self-verification, which is particularly evident in the CuteGPT family models.This phenomenon arises from an imbalance in our constructed training data, where the number of bad cases outweighs that of good cases.However, such data imbalances do not impact the results when employing PAM.This is because, in IFT with PAM, the model effectively learns from the correct information of answer while disregarding the incorrect information.Without PAM, the model learns from the entire answer, including incorrect information as well, leading the model to evaluate answers as incorrect in most cases.The relatively low accuracy of an LM, assessing answers as incorrect helps enhance the accuracy of self-verification.Additionally, for the Vicuna family models, regardless of the training method employed, they consistently exhibit strong confidence in their generated outputs, making them refuse to self-correct.</p>
<p>Zero-shot Performance on New Task.We evaluate the generalization ability of using ISC on a novel task.We choose StrategyQA8 as the new test task, which serves as a question answering benchmark focusing open-domain inquiries.This task requires providing either "true" or "false" as a response to the given questions.The results are presented in Figure 3.</p>
<p>We discover that ISC remains effective for the new task.After the second round of correction, the accuracy of all LMs improve.</p>
<p>Conclusion</p>
<p>We introduce Intrinsic Self-Correction (ISC) in LMs, an approach that utilizes models' own capabilities to identify and further modify their initial responses autonomously.This strong capability can even be applied to smaller LMs.We first devise a general process for constructing self-correction data, applicable to generating diverse self-correction task training data.Furthermore, we introduce a novel fine-tuning method named PAM to instruct LMs to self-correct.We conduct experiments on several open-source LMs to validate the efficacy of ISC.The experimental results on two distinct tasks consistently demonstrate that the utilization of ISC empowers the models with the capability for self-correction, and improves the accuracy of generated answers.In the best case, the accuracy enhancement reaches up to 5.6%.We also conduct a comprehensive analysis of the ability of the base model to self-validate and self-correct after using ISC.These findings help us understand how ISC works better.</p>
<p>Figure 2 :
2
Figure 2: The pipeline of constructing self-correction data.</p>
<p>Welleck et al. (2022) introduced a corrector model used to improve the initial response by learning the specific mistakes made by the LM and the appropriate method for modifying them.Similarly, Paul et al. (2023) introduced a critic model to provide feedback on the LM's reasoning errors, evaluating the generated intermediate steps.The feedback, together with the original question and preceding intermediate reasoning steps, was subsequently fed back to the LM to improve the generation of the next step.</p>
<p>the correct answer is E. amusement park [PV] Thinking about the correctness of the previous answer• • • Thinking result: I am sure that the earlier answer is correct Question: [TaskP] Please choose the most appropriate one from the following options: [question] what contributes more, though less than some believe, to the lung damage caused by smoking? A. smoking less B. switching to chewing C. no filters D. switching to e-cigs [COTP] Please give the detailed solving process and [SCP] verify your response before final submission.Answer: [A 1 2 -COT] Smoking causes less lung damage than people think, but it's not completely without effect.So the answer is A. smoking less [NV] Thinking about the correctness of the previous answer • • • Thinking result: Sorry, there is an error in the previous answer.[SC-COT] Let's analyze each option: A. smoking less: The question clearly mentions that it contributes less than some people think,• • • • • • C. no filters: filters it contributes to lung damage, and to a lesser extent than some believe.Therefore, the no filter option meets the requirement.[A 2 2 ] So the correct option is C. no filter.</p>
<p>Figure 3 :
3
Figure 3: Zero-shot performance of Intrinsic Self-Correction on the StrategyQA test data.After the second round of correction, the accuracy improves obviously.</p>
<p>Table 1 :
1
Training Data samples</p>
<p>Table 2 :
2
Main results of several open-source LMs.After self-correction, their accuracy is improved on test datasets.
1 n -</p>
<p>Table 3 :
3
The quantitative results of Intrinsic Self-Correction on two tasks.
Base Models Confidence EvalACC R2R R2W W2W W2ROpenBookQACuteGPT-7B70.636.69207839CuteGPT-13B40.852.057545778Llama2-7B99.652.20020ChatGLM-6B60.427.234317659Vicuna-7B98.628.80061Vicuna-13B97.234.21445CommonsenseQACuteGPT-7B87.726.81068351CuteGPT-13B42.253.8111131111159Llama2-7B99.852.30042ChatGLM-6B52.752.37092272146Vicuna-7B98.926.012139Vicuna-13B97.533.155138</p>
<p>Table 4 :
4
The impact of Partial Answer Masking on capability for self-correction.
Base ModelsOpenBookQA ACC-First ACC Confidence EvalACCCommonsenseQA ACC-First ACC Confidence EvalACCCuteGPT-7B w/ PAM25.229.070.636.623.226.987.726.8CuteGPT-7B w/o PAM26.028.22.068.618.831.22.276.2CuteGPT-13B w/ PAM37.242.040.852.036.637.942.253.2CuteGPT-13B w/o PAM28.632.41.668.220.534.21.977.8ChatGLM-6B w/ PAM37.042.660.427.234.338.752.752.3ChatGLM-6B w/o PLM30.236.056.632.226.032.950.047.2Vicuna-7B w/ PAM28.628.898.628.825.926.298.926.0Vicuna-7B w/o PAM23.423.697.425.013.813.939.814.0Vicuna-13B w/ PAM33.834.097.234.232.432.697.533.9Vicuna-13B w/o PAM32.031.289.632.631.532.497.533.1
The Thirty-Eighth AAAI Conference on Artificial Intelligence 
http://data.allenai.org/OpenBookQA
https://www.tau-nlp.sites.tau.ac.il/commonsenseqa
https://github.com/Abbey4799/CuteGPT/
https://github.com/THUDM/ChatGLM-6B
https://huggingface.co/meta-llama/Llama-2-7b-hf
https://huggingface.co/lmsys/vicuna-7b-v1.3
https://huggingface.co/lmsys/vicuna-13b-v1.3The Thirty-Eighth AAAI Conference on Artificial Intelligence 
https://github.com/eladsegal/strategyqaThe Thirty-Eighth AAAI Conference on Artificial Intelligence 
AcknowledgementsYanghua Xiao is also a member of Research Group of Computational and AI Communication at Institute for Global Communications and Integrated Media, Fudan University.This work was supported by Science and Technology Commission of Shanghai Municipality Grant (No. 22511105902), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), and National Natural Science Foundation of China (Grant No.62102095).
A Azaria, T Mitchell, arXiv:2304.13734The Internal State of an LLM Knows When its Lying. 2023</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>A cognitive process theory of writing. College composition and communication. L Flower, J R Hayes, 198132</p>
<p>D Ganguli, A Askell, N Schiefer, T Liao, K Lukošiūtė, A Chen, A Goldie, A Mirhoseini, C Olsson, D Hernandez, arXiv:2302.07459The for moral self-correction in large language models. 2023arXiv preprint</p>
<p>Scaling laws for reward model overoptimization. L Gao, J Schulman, J Hilton, International Conference on Machine Learning. PMLR2023</p>
<p>B Guo, X Zhang, Z Wang, M Jiang, J Nie, Y Ding, J Yue, Y Wu, arXiv:2301.07597How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. 2023</p>
<p>The Curious Case of Neural Text Degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, International Conference on Learning Representations. 2020</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, International Conference On Learning Representations. 2021</p>
<p>Y Huang, X Feng, X Feng, B Qin, arXiv:2104.14839The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. 2023</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.001902021arXiv preprint</p>
<p>TruthfulQA: Measuring How Models Mimic Human Falsehoods. S Lin, J Hilton, O Evans, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, arXiv:2303.17651Self-Refine: Iterative Refinement with Self-Feedback. 2023</p>
<p>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. P Manakul, A Liusie, M J F Gales, arXiv:2303.088962023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, arXiv:2203.02155arXiv:2304.01904Paul, D.Ismayilzada, M.Peyrard, M.Borges, B.Bosselut, A.West, R.2022and Faltings, B. 2023. RE-FINER: Reasoning Feedback on Intermediate Representations</p>
<p>PEER: A Collaborative Language Model. T Schick, J Dwivedi-Yu, Z Jiang, F Petroni, P Lewis, G Izacard, Q You, C Nalmpantis, E Grave, S Riedel, Proceedings of The 11th International Conference on Learning Representations. The 11th International Conference on Learning Representations2023</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023</p>
<p>Learning to summarize with human feedback. N Stiennon, L Ouyang, J Wu, D Ziegler, R Lowe, C Voss, A Radford, D Amodei, P F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q Le, E Chi, D Zhou, J Wei, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652Finetuned Language Models Are Zero-Shot Learners. 2022a</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Generating Sequences by Learning to Self-Correct. S Welleck, X Lu, P West, F Brahman, T Shen, D Khashabi, Y Choi, ArXiv, abs/2211.000532022</p>
<p>Large language models are reasoners with self-verification. Y Weng, M Zhu, S He, K Liu, J Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Re3: Generating Longer Stories With Recursive Reprompting and Revision. K Yang, Y Tian, N Peng, D Klein, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>. Abu Dhabi, Association for Computational LinguisticsUnited Arab Emirates</p>
<p>D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, E Chi, arXiv:2205.10625Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
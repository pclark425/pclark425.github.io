<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8888 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8888</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8888</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-8b281b474c45683b7b8cb63b3f186382c5d69ef2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8b281b474c45683b7b8cb63b3f186382c5d69ef2" target="_blank">Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that, the proposed Retro*, a neural-based A*-like algorithm that finds high-quality synthetic routes efficiently outperforms existing state-of-the-art with respect to both the success rate and solution quality, while being more efficient at the same time.</p>
                <p><strong>Paper Abstract:</strong> Retrosynthetic planning is a critical task in organic chemistry which identifies a series of reactions that can lead to the synthesis of a target product. The vast number of possible chemical transformations makes the size of the search space very big, and retrosynthetic planning is challenging even for experienced chemists. However, existing methods either require expensive return estimation by rollout with high variance, or optimize for search speed rather than the quality. In this paper, we propose Retro*, a neural-based A*-like algorithm that finds high-quality synthetic routes efficiently. It maintains the search as an AND-OR tree, and learns a neural search bias with off-policy data. Then guided by this neural network, it performs best-first search efficiently during new planning episodes. Experiments on benchmark USPTO datasets show that, our proposed method outperforms existing state-of-the-art with respect to both the success rate and solution quality, while being more efficient at the same time.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8888.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8888.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retro*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-guided best-first search algorithm for multi-step retrosynthetic planning that maintains an AND-OR search tree and uses a learned neural value function to guide A*-like expansions, optimizing route cost (negative log-likelihood) and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retro* (neural-guided A*-like planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Neural-guided search (value function + AND-OR A*-style search)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses synthesis route dataset constructed from USPTO reaction data (~1.3M cleaned reactions, 299,202 training routes) and commercially-available building blocks from eMolecules to train the value function; uses a separately trained one-step template MLP trained on USPTO (380K templates).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrosynthesis planning / chemical synthesis route design (organic chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Does not directly generate novel molecules; uses a one-step retrosynthesis model B to propose candidate reactions (template-based top-50 predictions) and uses the learned value function V_m to select which frontier molecule to expand in an A*-style AND-OR search, thereby assembling multi-step synthesis routes.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not intended as a de novo molecule generator; proposed reactants are produced by applying known reaction templates from USPTO to the product — novelty is therefore limited to combinations of known templates and available building blocks; no novelty metrics (e.g., % new molecules) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tailors generation to low-cost, high-probability synthetic routes by using reaction likelihoods from the one-step model as costs (negative log-likelihood) and training the value function on extracted routes so expansions prioritize low-cost, synthesizable routes that terminate in commercially available building blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (fraction of test targets for which a route is found under a time limit measured as number of one-step model calls), total route cost (sum of negative log-likelihoods), route length (number of reactions), number of one-step model calls (time proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Retro* outperforms baseline planners (DFPN-E, MCTS, greedy DFS) on a held-out hard test set: 86.84% success rate (Retro*) vs 55.26% (DFPN-E) and ~33-36% (MCTS variants) under a 500 one-step-call limit; produces more low-cost routes (112 better routes than experts) and more shorter routes (50 shorter than expert routes). Ablation (Retro*-0 with V_m = 0) shows the learned value function gives ≈6% absolute improvement in success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly against DFPN-E, MCTS (with rollout / PUCT), and greedy DFS; Retro* finds more solutions faster and yields better-cost solutions. Authors also show replacing rollouts/initializations in baselines with the learned V_m can help MCTS but not DFPN-E in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Retro* is a planner not a generative LLM; it depends on the quality and coverage of the one-step retrosynthesis model and the available templates; value function requires training routes extracted from reaction data and assumes access to a lower bound for V_m (0 is used as universal lower bound); search can be limited by top-k template predictions (they use top-50) and one-step model coverage; the approach does not address de novo molecule generation, property optimization, or LLM-style language-based molecular design; runtime dominated by one-step model calls (~0.3s per call).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8888.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8888.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template MLP one-step model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-based MLP one-step retrosynthesis model (top-50 template predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-class MLP classifier over reaction templates that, given a product molecule, predicts the most likely reaction templates; predicted templates are applied to yield candidate reactant sets used as one-step retrosynthesis proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Template-based MLP one-step retrosynthesis model</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multi-layer perceptron (multi-class classifier over reaction templates)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on USPTO reaction dataset (after cleaning ~1.3M reactions), with ~380K distinct reaction templates extracted via RDChiral; trained/predicted templates provide top-50 candidates per product.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>One-step retrosynthesis proposal generation (used within multi-step planning)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Template classification: predict top-k reaction templates for a product, then apply templates to produce reactant sets (candidate one-step reactions).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates reactant proposals by applying known templates from the USPTO corpus; novelty limited by template set and application rules; no explicit novelty statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Ensures synthesizability by using historically observed reaction templates and filters test routes to those whose reactions are covered by top-50 predictions; cost is negative log-likelihood from the model, aligning proposals to high-likelihood (plausible) reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used as the primitive for route cost (negative log-likelihood per reaction); its calls are counted as the main time metric (number of one-step model calls).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Serves as the candidate generator within Retro*; authors report that up to 99+% of runtime is in one-step model calls (~0.3s per call) and they restrict to top-50 templates. No standalone benchmarks of this MLP vs other one-step models are reported in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper notes trade-offs in template-based approaches (tens to hundreds of thousands of templates make classification hard) and contrasts template-based methods with template-free seq2seq approaches (LSTM/Transformer) cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires large template inventory (~380K) making classification hard; templates are not always available for all reactions; coverage limited by training reactions and by the top-k cutoff; template application may fail for some products; dominates runtime of planning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8888.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8888.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value network V_m</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural value function V_m (Morgan fingerprint → single-layer FC → scalar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned scalar value function that maps a molecule (Morgan fingerprint) to an estimated synthesis cost V_m, trained offline from extracted retrosynthesis routes and used as an admissible-ish heuristic to guide Retro*'s search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>V_m value network (fingerprint → FC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Single-layer fully connected neural network taking Morgan (ECFP) fingerprint input</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>hidden dimension 128 (single hidden layer); final output scalar</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Training routes R_train constructed from USPTO reactions and eMolecules building blocks: 299,202 training routes (plus validation/test splits described); one-step candidates B(m) included during training to form consistency losses.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Heuristic value estimation for retrosynthesis planning (guiding expansion to minimize route cost)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative model for molecules; used to estimate future cost h_t by regressing route costs and enforcing pairwise consistency margins between the best true one-step solution and alternatives (regression + margin ranking loss).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — V_m estimates cost of synthesizing molecules rather than generating novel molecules; no novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Trained to predict total route costs for molecules and to rank one-step options so search is biased towards low-cost, synthesizable routes; consistency loss uses true route data to prefer historically-used one-step reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Regression loss (MSE between V_m and observed route cost v_i) and margin-based consistency loss; empirical evaluation shows that using V_m improves Retro* success rate by ≈6% vs setting V_m=0 and increases number of best-cost solutions found.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The learned V_m meaningfully improves search: Retro* with V_m (full method) achieves 86.84% success rate vs 79.47% for Retro*-0 (V_m = 0), demonstrating improved efficiency and solution quality; V_m also helps MCTS when substituted for rollouts (MCTS+).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors replaced rollout value estimates in MCTS and proof-number initializations in DFPN-E with V_m: MCTS benefits (MCTS+), DFPN-E not improved in their setup, illustrating V_m provides lower-variance, generalizable heuristics that aid search-based planners.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Relies on quality and coverage of extracted training routes; assumes availability of lower bounds for V_m (0 used as universal lower bound); may not generalize beyond distribution of training routes or to unseen reaction types; requires constructing route dataset which itself depends on one-step model predictions and building-block sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8888.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8888.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2seq models (LSTM/Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seq2seq template-free one-step retrosynthesis models (LSTM and Transformer based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Template-free one-step retrosynthesis approaches that treat reaction prediction as a sequence-to-sequence translation problem (product SMILES → reactant SMILES), implemented with LSTM-based or Transformer-based architectures in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrosynthetic reaction prediction using neural sequence-to-sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seq2seq LSTM / Transformer one-step retrosynthesis models</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Sequence-to-sequence neural networks (LSTM or Transformer architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Mentioned prior works trained on reaction corpora such as USPTO (cited works: Liu et al., 2017 for LSTM; Karpov et al., 2019 for Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>One-step retrosynthesis (template-free reactant prediction), part of broader retrosynthesis planning pipelines in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct sequence generation of reactant SMILES from product SMILES (template-free generation), as an alternative to template classification.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>These methods can propose reactants not tied to existing templates, potentially enabling novel transformations; this paper only cites them and does not report novelty statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Template-free models can directly generate reactants for specific products; this paper does not evaluate them but notes them as an alternative family of one-step methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated in this paper; prior work typically uses top-k accuracy for one-step prediction and may integrate with multi-step planners for route quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as background: template-free seq2seq models (LSTM/Transformer) exist and are an alternative to template-based predictors; this paper does not use them in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper contrasts template-based (this work's one-step model) vs template-free seq2seq approaches, noting template-based struggles with very large template sets while template-free methods do not require templates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper does not evaluate these architectures directly; known trade-offs include potential for invalid SMILES, need for large training data, and different failure modes compared to template-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Planning chemical syntheses with deep neural networks and symbolic ai <em>(Rating: 2)</em></li>
                <li>Retrosynthetic reaction prediction using neural sequence-to-sequence models <em>(Rating: 2)</em></li>
                <li>A transformer model for retrosynthesis <em>(Rating: 2)</em></li>
                <li>Computer-assisted retrosynthesis based on molecular similarity <em>(Rating: 1)</em></li>
                <li>Depth-first proof-number search with heuristic edge cost and application to chemical synthesis planning <em>(Rating: 1)</em></li>
                <li>Learning retrosynthetic planning through simulated experience <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8888",
    "paper_id": "paper-8b281b474c45683b7b8cb63b3f186382c5d69ef2",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Retro*",
            "name_full": "Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search",
            "brief_description": "A neural-guided best-first search algorithm for multi-step retrosynthetic planning that maintains an AND-OR search tree and uses a learned neural value function to guide A*-like expansions, optimizing route cost (negative log-likelihood) and efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Retro* (neural-guided A*-like planner)",
            "model_type": "Neural-guided search (value function + AND-OR A*-style search)",
            "model_size": null,
            "training_data": "Uses synthesis route dataset constructed from USPTO reaction data (~1.3M cleaned reactions, 299,202 training routes) and commercially-available building blocks from eMolecules to train the value function; uses a separately trained one-step template MLP trained on USPTO (380K templates).",
            "application_domain": "Retrosynthesis planning / chemical synthesis route design (organic chemistry)",
            "generation_method": "Does not directly generate novel molecules; uses a one-step retrosynthesis model B to propose candidate reactions (template-based top-50 predictions) and uses the learned value function V_m to select which frontier molecule to expand in an A*-style AND-OR search, thereby assembling multi-step synthesis routes.",
            "novelty_of_chemicals": "Not intended as a de novo molecule generator; proposed reactants are produced by applying known reaction templates from USPTO to the product — novelty is therefore limited to combinations of known templates and available building blocks; no novelty metrics (e.g., % new molecules) are reported.",
            "application_specificity": "Tailors generation to low-cost, high-probability synthetic routes by using reaction likelihoods from the one-step model as costs (negative log-likelihood) and training the value function on extracted routes so expansions prioritize low-cost, synthesizable routes that terminate in commercially available building blocks.",
            "evaluation_metrics": "Success rate (fraction of test targets for which a route is found under a time limit measured as number of one-step model calls), total route cost (sum of negative log-likelihoods), route length (number of reactions), number of one-step model calls (time proxy).",
            "results_summary": "Retro* outperforms baseline planners (DFPN-E, MCTS, greedy DFS) on a held-out hard test set: 86.84% success rate (Retro*) vs 55.26% (DFPN-E) and ~33-36% (MCTS variants) under a 500 one-step-call limit; produces more low-cost routes (112 better routes than experts) and more shorter routes (50 shorter than expert routes). Ablation (Retro*-0 with V_m = 0) shows the learned value function gives ≈6% absolute improvement in success rate.",
            "comparison_to_other_methods": "Compared directly against DFPN-E, MCTS (with rollout / PUCT), and greedy DFS; Retro* finds more solutions faster and yields better-cost solutions. Authors also show replacing rollouts/initializations in baselines with the learned V_m can help MCTS but not DFPN-E in their setup.",
            "limitations_and_challenges": "Retro* is a planner not a generative LLM; it depends on the quality and coverage of the one-step retrosynthesis model and the available templates; value function requires training routes extracted from reaction data and assumes access to a lower bound for V_m (0 is used as universal lower bound); search can be limited by top-k template predictions (they use top-50) and one-step model coverage; the approach does not address de novo molecule generation, property optimization, or LLM-style language-based molecular design; runtime dominated by one-step model calls (~0.3s per call).",
            "uuid": "e8888.0",
            "source_info": {
                "paper_title": "Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Template MLP one-step model",
            "name_full": "Template-based MLP one-step retrosynthesis model (top-50 template predictor)",
            "brief_description": "A multi-class MLP classifier over reaction templates that, given a product molecule, predicts the most likely reaction templates; predicted templates are applied to yield candidate reactant sets used as one-step retrosynthesis proposals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Template-based MLP one-step retrosynthesis model",
            "model_type": "Multi-layer perceptron (multi-class classifier over reaction templates)",
            "model_size": null,
            "training_data": "Trained on USPTO reaction dataset (after cleaning ~1.3M reactions), with ~380K distinct reaction templates extracted via RDChiral; trained/predicted templates provide top-50 candidates per product.",
            "application_domain": "One-step retrosynthesis proposal generation (used within multi-step planning)",
            "generation_method": "Template classification: predict top-k reaction templates for a product, then apply templates to produce reactant sets (candidate one-step reactions).",
            "novelty_of_chemicals": "Generates reactant proposals by applying known templates from the USPTO corpus; novelty limited by template set and application rules; no explicit novelty statistics reported.",
            "application_specificity": "Ensures synthesizability by using historically observed reaction templates and filters test routes to those whose reactions are covered by top-50 predictions; cost is negative log-likelihood from the model, aligning proposals to high-likelihood (plausible) reactions.",
            "evaluation_metrics": "Used as the primitive for route cost (negative log-likelihood per reaction); its calls are counted as the main time metric (number of one-step model calls).",
            "results_summary": "Serves as the candidate generator within Retro*; authors report that up to 99+% of runtime is in one-step model calls (~0.3s per call) and they restrict to top-50 templates. No standalone benchmarks of this MLP vs other one-step models are reported in detail here.",
            "comparison_to_other_methods": "Paper notes trade-offs in template-based approaches (tens to hundreds of thousands of templates make classification hard) and contrasts template-based methods with template-free seq2seq approaches (LSTM/Transformer) cited in related work.",
            "limitations_and_challenges": "Requires large template inventory (~380K) making classification hard; templates are not always available for all reactions; coverage limited by training reactions and by the top-k cutoff; template application may fail for some products; dominates runtime of planning pipeline.",
            "uuid": "e8888.1",
            "source_info": {
                "paper_title": "Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Value network V_m",
            "name_full": "Neural value function V_m (Morgan fingerprint → single-layer FC → scalar)",
            "brief_description": "A learned scalar value function that maps a molecule (Morgan fingerprint) to an estimated synthesis cost V_m, trained offline from extracted retrosynthesis routes and used as an admissible-ish heuristic to guide Retro*'s search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "V_m value network (fingerprint → FC)",
            "model_type": "Single-layer fully connected neural network taking Morgan (ECFP) fingerprint input",
            "model_size": "hidden dimension 128 (single hidden layer); final output scalar",
            "training_data": "Training routes R_train constructed from USPTO reactions and eMolecules building blocks: 299,202 training routes (plus validation/test splits described); one-step candidates B(m) included during training to form consistency losses.",
            "application_domain": "Heuristic value estimation for retrosynthesis planning (guiding expansion to minimize route cost)",
            "generation_method": "Not a generative model for molecules; used to estimate future cost h_t by regressing route costs and enforcing pairwise consistency margins between the best true one-step solution and alternatives (regression + margin ranking loss).",
            "novelty_of_chemicals": "Not applicable — V_m estimates cost of synthesizing molecules rather than generating novel molecules; no novelty metrics.",
            "application_specificity": "Trained to predict total route costs for molecules and to rank one-step options so search is biased towards low-cost, synthesizable routes; consistency loss uses true route data to prefer historically-used one-step reactions.",
            "evaluation_metrics": "Regression loss (MSE between V_m and observed route cost v_i) and margin-based consistency loss; empirical evaluation shows that using V_m improves Retro* success rate by ≈6% vs setting V_m=0 and increases number of best-cost solutions found.",
            "results_summary": "The learned V_m meaningfully improves search: Retro* with V_m (full method) achieves 86.84% success rate vs 79.47% for Retro*-0 (V_m = 0), demonstrating improved efficiency and solution quality; V_m also helps MCTS when substituted for rollouts (MCTS+).",
            "comparison_to_other_methods": "Authors replaced rollout value estimates in MCTS and proof-number initializations in DFPN-E with V_m: MCTS benefits (MCTS+), DFPN-E not improved in their setup, illustrating V_m provides lower-variance, generalizable heuristics that aid search-based planners.",
            "limitations_and_challenges": "Relies on quality and coverage of extracted training routes; assumes availability of lower bounds for V_m (0 used as universal lower bound); may not generalize beyond distribution of training routes or to unseen reaction types; requires constructing route dataset which itself depends on one-step model predictions and building-block sets.",
            "uuid": "e8888.2",
            "source_info": {
                "paper_title": "Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Seq2seq models (LSTM/Transformer)",
            "name_full": "Seq2seq template-free one-step retrosynthesis models (LSTM and Transformer based)",
            "brief_description": "Template-free one-step retrosynthesis approaches that treat reaction prediction as a sequence-to-sequence translation problem (product SMILES → reactant SMILES), implemented with LSTM-based or Transformer-based architectures in prior literature.",
            "citation_title": "Retrosynthetic reaction prediction using neural sequence-to-sequence models",
            "mention_or_use": "mention",
            "model_name": "Seq2seq LSTM / Transformer one-step retrosynthesis models",
            "model_type": "Sequence-to-sequence neural networks (LSTM or Transformer architectures)",
            "model_size": null,
            "training_data": "Mentioned prior works trained on reaction corpora such as USPTO (cited works: Liu et al., 2017 for LSTM; Karpov et al., 2019 for Transformer).",
            "application_domain": "One-step retrosynthesis (template-free reactant prediction), part of broader retrosynthesis planning pipelines in literature.",
            "generation_method": "Direct sequence generation of reactant SMILES from product SMILES (template-free generation), as an alternative to template classification.",
            "novelty_of_chemicals": "These methods can propose reactants not tied to existing templates, potentially enabling novel transformations; this paper only cites them and does not report novelty statistics.",
            "application_specificity": "Template-free models can directly generate reactants for specific products; this paper does not evaluate them but notes them as an alternative family of one-step methods.",
            "evaluation_metrics": "Not evaluated in this paper; prior work typically uses top-k accuracy for one-step prediction and may integrate with multi-step planners for route quality metrics.",
            "results_summary": "Mentioned as background: template-free seq2seq models (LSTM/Transformer) exist and are an alternative to template-based predictors; this paper does not use them in experiments.",
            "comparison_to_other_methods": "Paper contrasts template-based (this work's one-step model) vs template-free seq2seq approaches, noting template-based struggles with very large template sets while template-free methods do not require templates.",
            "limitations_and_challenges": "Paper does not evaluate these architectures directly; known trade-offs include potential for invalid SMILES, need for large training data, and different failure modes compared to template-based approaches.",
            "uuid": "e8888.3",
            "source_info": {
                "paper_title": "Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Planning chemical syntheses with deep neural networks and symbolic ai",
            "rating": 2
        },
        {
            "paper_title": "Retrosynthetic reaction prediction using neural sequence-to-sequence models",
            "rating": 2
        },
        {
            "paper_title": "A transformer model for retrosynthesis",
            "rating": 2
        },
        {
            "paper_title": "Computer-assisted retrosynthesis based on molecular similarity",
            "rating": 1
        },
        {
            "paper_title": "Depth-first proof-number search with heuristic edge cost and application to chemical synthesis planning",
            "rating": 1
        },
        {
            "paper_title": "Learning retrosynthetic planning through simulated experience",
            "rating": 1
        }
    ],
    "cost": 0.0143345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Retro<em>: Learning Retrosynthetic Planning with Neural Guided A</em> Search</h1>
<p>Binghong Chen ${ }^{1}$ Chengtao $\mathbf{L i}^{2}$ Hanjun Dai ${ }^{3}$ Le Song ${ }^{14}$</p>
<h4>Abstract</h4>
<p>Retrosynthetic planning is a critical task in organic chemistry which identifies a series of reactions that can lead to the synthesis of a target product. The vast number of possible chemical transformations makes the size of the search space very big, and retrosynthetic planning is challenging even for experienced chemists. However, existing methods either require expensive return estimation by rollout with high variance, or optimize for search speed rather than the quality. In this paper, we propose Retro<em>, a neural-based A</em>-like algorithm that finds high-quality synthetic routes efficiently. It maintains the search as an ANDOR tree, and learns a neural search bias with offpolicy data. Then guided by this neural network, it performs best-first search efficiently during new planning episodes. Experiments on benchmark USPTO datasets show that, our proposed method outperforms existing state-of-the-art with respect to both the success rate and solution quality, while being more efficient at the same time.</p>
<h2>1. Introduction</h2>
<p>Retrosynthetic planning is one of the fundamental problems in organic chemistry. Given a target product, the goal of retrosynthesis is to identify a series of reactions that can lead to the synthesis of the product, by searching backwards and iteratively applying chemical transformations to unavailable molecules. As thousands of theoretically-possible transformations can all be applied during each step of reactions, the search space of planning will be huge and makes the problem challenging even for experienced chemists.</p>
<p>The one-step retrosynthesis prediction, which predicts a list of possible direct reactants given product, serves as the foundation for realizing the multistep retrosynthetic planning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Existing methods roughly fall into two categories, either template-based or template-free. Each chemical reaction is associated with a reaction template that encodes how atoms and bonds change during the reaction. Given a target product, template-based methods predict the possible reaction templates, and subsequently apply the predicted reaction templates to target molecule to get corresponding reactants. Existing methods include retrosim (Coley et al., 2017), neuralsym (Segler \&amp; Waller, 2017) and GLN (Dai et al., 2019). Though conceptually straightforward, template-based methods need to deal with tens or even hundreds of thousands of possible reaction templates, making the classification task hard. Besides, templates are not always available for chemical reactions. Due to these reasons, people have also been developing template-free methods that could directly predict reactants. Most of existing methods employ seq2seq models like LSTM (Liu et al., 2017) or Transformer (Karpov et al., 2019) from neural machine translation literature.</p>
<p>While one-step methods are continuously being improved, most molecules in real world cannot be synthesized within one step. Possible number of synthesis steps could go up to 60 or even more. Since each molecule could be synthesized by hundreds of different possible reactants, the possible synthesis routes becomes countless for a single product. Such huge space poses challenges for efficient searching and planning, even with advanced one-step approaches.</p>
<p>Besides the huge search space, another challenge is the ambiguity in performance measure and benchmarking. It has been extremely hard to quantitatively analyze the performance of any multi-step retrosynthesis algorithms due to the ambiguous definition of 'good synthesis routes', nor are there any benchmark datasets for analyzing designed algorithms. Most common ways for quantitative analysis is to employ domain experts and let them judge if one synthesis route is better than the other based solely on their experiences, which is both time-consuming and costly.</p>
<p>Due to aforementioned challenges, there are less work proposed in the field of multi-step retrosynthetic planning. Previous works using Monte Carlo Tree Search (MCTS) (Segler et al., 2018; 2017) have achieved superior results over neuralor heuristic-based Breadth First Search (BFS). However, MCTS-based methods has several limitations in this setting:</p>
<ul>
<li>Each tree node corresponds to a set of molecules instead</li>
</ul>
<p>of single molecule. This addtional combinatorial aspect make the representation of tree node, and the estimation of its value even harder. Furthermore, reactions do not explicilty appear as nodes in the tree, which prevents their algorithm from exploiting the structure of subproblems.</p>
<ul>
<li>As the algorithm depends on online value estimation, the full rollout from vanilla MCTS may not be efficient for the planning need. Furthermore, the algorithm can not exploit historical data in that many good retrosynthesis plans may have been found previously, and "intuitions" on how to plan efficiently may be learned from these histories.</li>
</ul>
<p>For quantitative evaluation, they have employed numerous domain experts to conduct A-B tests over methods proposed by their algorithm and other baselines.</p>
<p>In this paper, we present a novel neural-guided tree search method, called Retro*1, for chemical retrosynthesis planning. In our method,</p>
<ul>
<li>We explicitly maintain information about reactions as nodes in an AND-OR tree, where a node with "AND" type corresponds to a reaction, and a node with "OR" type corresponds to a molecule. The tree captures the relations between candidate reactions and reactant molecules, which allows us to exploit structure of subproblems corresponding to a single molecule.</li>
<li>Based on the AND-OR tree representation, we propose an A*-like planning algorithm which is guided by a neural network learned from past retrosynthesis planning experiences. More specifially, The neural network learns a synthesis cost for each molecule, and it helps the search algorithm to pick the most promising molecule node to expand.</li>
</ul>
<p>Furthermore, we also propose a method for constructing benchmark synthesis routes data given reactions and chemical building blocks. Based on this, we construct a synthesis route dataset from benchmark reaction dataset USPTO. The route dataset is not only useful for quantitative analysis for predicted synthesis routes, but also work as training data for the neural network components in our method.</p>
<p>Below we summarize our contributions:</p>
<ul>
<li>We propose a novel learning-based retrosynthetic planning algorithm to learn from previous planning experience. The proposed algorithm outperforms state-of-theart methods by a large margin on a realworld benchmark dataset.</li>
<li>Our algorithm framework can induce a search algorithm that guarantees the optimal solution.</li>
<li>We propose a method for constructing synthesis route</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>datasets for quantitative analysis of multistep retorsynthetic planning methods.</p>
<p>Our planning algorithm is general in the sense that it can also be applied to other machine learning problems such as theorem proving (Yang \&amp; Deng, 2019) and hierarchical task planning (Erol, 1996). A synthetic task planning experiment is included in Appendix D to demonstrate the idea. Most related works have been mentioned in the first two sections. For more related works, please refer to Appendix E.</p>
<h2>2. Background</h2>
<p>In this section, we first state the problem and its background we are tackling in Section 2.1. Then in Section 2.2 and Section 2.3 we describe how MCTS and proof number search fit in the problem setting.</p>
<h3>2.1. Problem Statement</h3>
<p>One-step retrosynthesis: Denote the space of all molecule as $\mathcal{M}$. The one-step retrosynthesis takes a target molecule $t \in \mathcal{M}$ as input, and predicts a set of source reactants $\mathcal{S} \subset \mathcal{M}$ that can be used to synthesize $t$. This is the reverse problem of reaction outcome prediction. In our paper, we assume the existence of such one-step retrosynthesis model (or one-step model for simplicity in the rest of the paper) $B$,</p>
<p>$$
B(\cdot): \quad t \rightarrow\left{R_{i}, \mathcal{S}<em i="i">{i}, c\left(R</em>
$$}\right)\right}_{i=1}^{k</p>
<p>which outputs at most $k$ reactions $R_{i}$, the corresponding reactant sets $\mathcal{S}<em i="i">{i}$ and costs $c\left(R</em>}\right)$. The cost can be the actual price of the reaction $R_{i}$, or simply the negative loglikelihood of this reaction under model $B$. A one-step retrosynthesis model can be learned from a dataset of chemical reactions $\mathcal{D<em i="i">{\text {train }}=\left{\mathcal{S}</em>$ which have already been discovered by chemists in the past (Coley et al., 2017; Segler \&amp; Waller, 2017; Liu et al., 2017; Dai et al., 2019; Karpov et al., 2019).}, t_{i}\right}^{2</p>
<p>Retrosynthesis planning. Given a single target molecule $t \in \mathcal{M}$ and an initial set of molecules $\mathcal{I} \subset \mathcal{M}$, we are interested in synthesizing $t$ via a sequence of chemical reactions using reactants that are from or can be synthesized by $\mathcal{I}$. In this case, $\mathcal{I}$ corresponds to a set of molecules that are commercially available. The goal of retrosynthesis planning is to predict a sequence of reactions with reactants in $\mathcal{I}$ and will ultimately arrive at product $t$.</p>
<p>Instead of performing forward chaining like reasoning that starts from $\mathcal{I}$, a more efficient and commonly used method is to perform backward chaining that starts from the molecule $t$, and perform a series of one-step retrosynthesis prediction until all the reactants required are from $\mathcal{I}$. Beyond just finding such a synthesis route, our goal is to find the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>retrosynthesis plan that are:</p>
<ul>
<li>High-quality:</li>
<li>The entire retrosynthesis plan should be chemically sound with high probability;</li>
<li>The reactants or chemical reactions required should have as low cost as possible;</li>
<li>Efficient: Due to the synthesis effort, the number of retrosynthesis steps should be limited.</li>
</ul>
<p>Our proposed Retro* is aiming at finding the best retrosynthesis plan with respect to above criteria in limited time. To achieve this, we also assume that the quality of a solution can be measured by the reaction cost, where such cost is known to our model.</p>
<h3>2.2. Monte Carlo Tree Search</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Left: MCTS (Segler et al., 2018) for retrosynthesis planning. Each node represents a set of molecules. Orange nodes/molecules are available building blocks; Right: AND-OR stump illustration of $B(m)=P, Q$. Reaction $P$ requires molecule $c$ and $d$. Reaction $Q$ requires molecule $f$. Either $P$ or $Q$ can be used to synthesize $m$.</p>
<p>The Monte Carlo Tree Search (MCTS) has achieved ground breaking successes in two player games, such as GO (Silver et al., 2016; 2017). Its variant, UCT (Kocsis \&amp; Szepesvári, 2006), is especially powerful for balancing exploration and exploitation in online learning setting, and has been employed in Segler et al. (2018) for retrosynthesis planning. Specifically, as illustrated in Figure 1, the tree search start from the target molecule $t$. Each node $u$ in the current search tree $T$ represents a set of molecules $\mathcal{M}<em u="u">{u}$. Each child node $v \in \operatorname{ch}(u)$ of $u$ is obtained by selecting one molecule $m \in \mathcal{M}</em>}$ and a one-step retrosynthesis reaction $\left(R_{u v}, \mathcal{S<em u="u" v="v">{u v}, c\left(R</em>}\right)\right) \in B(m)$, where the resulting node $v$ contains molecule set $\mathcal{M<em u="u" v="v">{v}=\left(\mathcal{S}</em>$.
Despite its good performance, MCTS formulation for retrosynthesis planning has several limitations. First, the rollout needed in MCTS makes it time-consuming, and unlike in two-player zero-sum games, the retrosynthesis planning is essentially a single player game where the return estimated by random rollouts could be highly inaccurate. Second, since each tree node is a set of molecules instead of a single molecule, the combinatorial nature of this representation brings the sparsity in the variance estimation.} \cup \mathcal{M}_{u}\right) \backslash{m} \backslash \mathcal{I</p>
<h3>2.3. Proof Number Search and Variants</h3>
<p>The proof-number search (PNS) (Allis et al., 1994) is a game tree search that is designed for two-player game with binary goal. It tries to either prove or disprove the root node as fast as possible. In the retrosynthesis planning scenario, this corresponds to either proving the target molecule $t$ by finding a feasible planning path, or concluding that it is not synthesizable.</p>
<p>AND-OR Tree: The search tree of PNS is an AND-OR tree $T$, where each AND node needs all its children to be proved, while OR node requires at least one to be satisfied. Each node $u \in T$ is associated with a proof number $p n(u)$ that defines the minimum number of leaf nodes to be proved in order to prove $u$. Similarly, the disproof number $d n(u)$ finds the minimum number of leaf nodes needed to disprove $u$. With such definition, we can recursively define these numbers for internal nodes. Specifically, for AND node $u$,</p>
<p>$$
\begin{gathered}
p n(u)=\sum_{v \in c h(u)} p n(v), d n(u)=\min _{v \in c h(u)} d n(v) \
\text { and for proved nodes: } p n(u)=0, d n(u)=+\infty
\end{gathered}
$$</p>
<p>and for OR node $u$, we have</p>
<p>$$
\begin{gathered}
p n(u)=\min <em _in="\in" c="c" h_u_="h(u)" v="v">{v \in c h(u)} p n(v), d n(u)=\sum</em> d n(v) \
\text { and for disproved node: } p n(u)=+\infty, d n(u)=0
\end{gathered}
$$</p>
<p>Represent retrosynthesis planning using AND-OR tree: As illustrated in Figure 1, the application of one-step retrosynthesis model $B$ on molecule $m$ can be represented using one block of AND-OR tree (denoted as AND-OR stump), with molecule node as 'OR' node and reaction node as 'AND' node. This is because a molecule $m$ can be synthesized using any one of its children reactions (or-relation), and each reaction node requires all of its children molecules (and-relation) to be ready.</p>
<p>The search of PNS starts from the root node every time, and selects the child node with either minimum proof number or minimum disproof number, depends on whether the current node is an OR node or AND node, respectively. The process ends when a leaf node is reached, which can be either reaction or molecule node to be expanded. And after one step of retrosynthesis expansion, all the $p n(\cdot)$ and $d n(\cdot)$ of nodes along the path back to the root will be updated. The two-player game in this sense comes from the interleaving behavior of selecting proof and disproof numbers, where the first 'player' tries to prove the root while the second 'player' tries to disprove it. As both of the players behave optimally when the proof/disproof numbers are accurate, such perspective would bring the efficiency for finding a feasible synthesis path or prove that it is not synthesizable.</p>
<p>Variant: There have been several variants to improve different aspects of PNS, including different traversal strategy, different initialization methods of $p n(\cdot)$ and $d n(\cdot)$ for newly added nodes. The most recent work DFPN-E (Kishimoto</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Retro</span><span class="o">^</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">t</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="o">=(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">V</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">E</span><span class="p">}</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">V</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="p">{</span><span class="err">t\</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">E</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">emptyset</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="nt">route</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">found</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">argmin</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">m</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="err">\mathcal{F</span><span class="p">}</span><span class="o">(</span><span class="nt">T</span><span class="o">)</span><span class="err">}</span><span class="w"> </span><span class="nt">V_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">(</span><span class="nt">m</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">R_{i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">c</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">B</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">Add</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">under</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">j</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">|</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">|</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">                </span><span class="nt">Add</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">under</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">V_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">(</span><span class="nt">m</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">F</span><span class="p">}</span><span class="o">(</span><span class="nt">T</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="nt">route</span><span class="o">;</span>
</code></pre></div>

<p>et al., 2019) builds on top of the depth-first variant of PNS with an additive cost in addition to classical update rule in Eq (3). Specifically, for an unsolved OR node,</p>
<p>$$
p n(u)=\min _{v \in c h(u)}(h(u, v)+p n(v))
$$</p>
<p>Here $h(u, v)$ is the function of the cost of corresponding one-step retrosynthesis. Together with manually defined thresholds, this method addresses the lopsided problem in retrosynthesis planning, i.e., the imbalance of branching factor between AND and OR nodes.</p>
<p>The variants of PNS has shown some promising results over MCTS for retrosynthesis planning. However, the twoplayer game formulation is designed for the speed of a proof, not necessarily the overall solution quality. Moreover, existing works rely on human expert to design $p n(\cdot), d n(\cdot)$ and thresholds during search. This makes it not only timeconsuming to tune, but also hard to generalize well when solving new target molecule $t$ or dealning with new one-step model or reaction data.</p>
<h2>3. Retro* Search Algorithm</h2>
<p>Our proposed Retro* is a retrosynthetic planning algorithm that works on the AND-OR search tree. It is significantly different from PNS which is also based on AND-OR tree, or other MCTS based methods in the following ways:</p>
<ul>
<li>Retro<em> utilizes AND-OR tree for single player game which only utilizes the global value estimation. This is different from PNS which models the problem as twoplayer game with both proof numbers and disproof numbers. The distinction of the objective makes Retro</em> advantageous in finding best retrosynthetic routes.</li>
<li>Retro* estimates the future value of frontier nodes with neural network that can be trained using historical retrosynthesis planning data. This is different from the expensive rollouts used in Segler et al. (2018), or the human designed heuristics in Kishimoto et al. (2019). This not only enables more accurate prediction during expansion, but also generalizes the knowledge learned from existing planning paths.</li>
</ul>
<h3>3.1. Overview of Retro*</h3>
<p>Retro* (Algorithm 1) is a best-first search algorithm, which exploits neural priors to directly optimize for the quality of the solution. The search tree $T$ is an AND-OR tree, with molecule node as 'OR' node and reaction node as 'AND' node. It starts the search tree $T$ with a single root molecule node that is the target molecule $t$. At each step, it selects a node $u$ in the frontier of $T$ (denoted as $\mathcal{F}(T)$ ) according to the value function. Then it expands $u$ with the one-step model $B(u)$ and grows $T$ with one AND-OR stump. Finally the nodes with potential dependency on $u$ will be updated. Below we first provide a big picture of the algorithm by explaining these steps one by one, then we look into details of value function design and its update in Section 3.2 and Section 3.3, respectively. Figure 2 summarizes these steps in high level.</p>
<p>Selection: Given a search tree $T$, we denote the molecule nodes as $\mathcal{V}^{m}(T)$ and reaction nodes as $\mathcal{V}^{r}(T)$, where the total nodes in $T$ will be $\mathcal{V}(T)=\mathcal{V}^{m}(T) \cup \mathcal{V}^{r}(T)$. The frontier $\mathcal{F}(T) \subseteq \mathcal{V}^{m}(T)$ contains all the molecule nodes in $T$ that haven't been expanded before. Since we want to minimize the total cost of the final solution, an ideal option to expand next would be the molecule node which is part of the best synthesis plan.</p>
<p>Suppose we already have a value function oracle $V_{t}(m \mid T)$ which tells us that under the current search tree $T$, the cost of the best plan that contains $m$ for synthesizing target $t$. We can use it to select the next node to expand:</p>
<p>$$
m_{\text {next }}=\operatorname{argmin}<em t="t">{m \in \mathcal{F}(T)} V</em>(m \mid T)
$$</p>
<p>A proper design of such $V_{t}(m \mid T)$ would not only improve search efficiency, but can also bring theoretical guarantees.</p>
<p>Expansion: After picking the node $m$ with minimum cost estimation $V_{t}(m \mid T)$, we will expand the search tree with $k$ one-step retrosynthesis proposals from $B(m)$. Specifically, for each proposed retrosynthesis reaction $\left(R_{i}, \mathcal{S}<em i="i">{i}, c\left(R</em>$, we create a molecule node under the reaction node $R$. This will create an AND-OR stump under node $m$. Unlike in MCTS (Segler et al., 2018) where multiple calls to $B(\cdot)$ is needed till a terminal state during rollout, here the expansion only requires a single call to the one-step model.}\right)\right) \in$ $B(m)$, we create a reaction node $R=R_{i}$ under node $m$, and for each molecule $m^{\prime} \in \mathcal{S}_{i</p>
<p>Update: Denote the search tree $T$ after expansion of node $m$ to be $T^{\prime}$. Such expansion obtains the corresponding cost information for one-step retrosynthesis. we utilize this more direct information to update $V_{t}\left(\cdot \mid T^{\prime}\right)$ of all other relevant nodes to provide a more accurate estimation of total cost.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Retro* algorithm framework. We use circles to represent molecule nodes, and squares to represent reaction nodes. An iteration consists of three phases. In the selection phase, one of the frontier molecule nodes is selected according to the cost estimation $V_{t}(m \mid T)$. Then the an AND-OR stump is expanded from the selected node. All the new reactions and molecules are added to the tree. Finally the values inside the tree are updated using the $V_{m} \mathrm{~s}$ from the newly added molecules. The left-most figure also serves as the illustration for computing $V_{t}(f \mid T) . V_{t}(f \mid T)=g_{t}(f \mid T)+h_{t}(f \mid T)$, where $g_{t}(f \mid T)=c(\mathcal{F})+c(R)$, and $h_{t}(f \mid T)=V_{a}+V_{c}+V_{f}+V_{k}$.</p>
<h3>3.2. Design of $V_{t}(m \mid T)$</h3>
<p>To properly design $V_{t}(m \mid T)$, we borrow the idea from A<em> algorithm. A</em> algorithm is a best-first search algorithm which uses the cost from start $g(\cdot)$ together with the estimation of future cost $h(\cdot)$ to select move. When such estimation is admissible, it will be guaranteed to return the optimal solution. Inspired by the A* algorithm, we decompose the value function into two parts:</p>
<p>$$
V_{t}(m \mid T)=g_{t}(m \mid T)+h_{t}(m \mid T)
$$</p>
<p>where $g_{t}(m \mid T)$ is the cost of current reactions that have happened in $T$, if $m$ should be in the final route, and $h_{t}(m \mid T)$ is the estimated cost for future reactions needed to complete such planning. Instead of explicitly calculate these two separately, we show an equivalent but simpler way to calculate $V_{t}(\cdot \mid T)$ directly.</p>
<p>Specifically, we first define $V_{m}(m \mid \emptyset)$, which is a boundary case of the value function oracle $V$ that simply tells how much cost is needed to synthesize molecule $m$. For the simplicity of notation, we denote it as $V_{m}$. Then we define the reaction number function $r n(\cdot \mid T): \mathcal{V}(T) \mapsto \mathbb{R}$ that is inspired by proof number but with different purpose:</p>
<p>$$
\begin{aligned}
&amp; r n(R \mid T)=c(R)+\sum_{m \in c h(R)} r n(m \mid T) \
&amp; r n(m \mid T)= \begin{cases}V_{m}, &amp; m \in \mathcal{F}(T) \
\min _{R \in c h(m)} r n(R \mid T), &amp; \text { otherwise }\end{cases}
\end{aligned}
$$</p>
<p>where $r n(R \mid T)$ and $r n(m \mid T)$ calculate for reaction node and molecule node, respectively. The reaction number tells the minimum estimated cost needed for a molecule or reaction to happen in the current tree. We further define $\operatorname{pr}(u \mid T): \mathcal{V}(T) \mapsto \mathcal{V}(T)$ to get the parent node of $u$, and $\mathcal{A}(u \mid T)$ be all the ancestors of node $u$. Note that $\operatorname{pr}(m \mid T) \in \mathcal{V}^{r}(T), \forall m \in \mathcal{V}^{m}(T)$ and vise versa. Then
function $V_{t}(m \mid T)$ will be:</p>
<p>$$
\begin{aligned}
V_{t}(m \mid T) &amp; =\sum_{r \in \mathcal{A}(m \mid T) \cap \mathcal{V}^{r}(T)} c(r) \
&amp; +\sum_{m^{\prime} \in \mathcal{V}^{m}(T), \operatorname{pr}\left(m^{\prime}\right) \in \mathcal{A}(m \mid T)} r n\left(m^{\prime} \mid T\right)
\end{aligned}
$$</p>
<p>The first summation calculates all the reaction cost that has happened along the path from node $m$ to root. Additionally, $\forall R \in \mathcal{A}(m \mid T) \cap \mathcal{V}^{r}(T)$, the child node $m^{\prime} \in c h(R)$ should also be synthesized, as each such reaction node $R$ is an AND node. This requirement is captured in the second summation of Eq (8). We can see that implicitly $g_{t}(m \mid T)$ sums up the cost associated with the reaction nodes in this route related to $m$, and $h_{t}(m \mid T)$ takes all the terms related to $V$. in Eq (7).</p>
<p>In Figure 2 we demonstrate the calculation of $V_{t}(m \mid T)$ with a simple example. Notice that we can compute the parts that relevant to $g_{t}(\cdot \mid T)$ with existing information. But we can only estimate the part of $h_{t}(\cdot \mid T)$ since the required reactions are not in the search tree yet. We will show how to learn this future estimation in Section 4.2.</p>
<h3>3.3. Updating $V_{t}(m \mid T)$</h3>
<p>After a node $m$ is expanded, there are several components needed to be updated to maintain the search tree state.</p>
<p>Update $r n(\cdot \mid T)$ : Following Eq (7), the reaction number for newly created molecule nodes $u$ under the subtree rooted at $m$ will be $V_{u}$, and the reaction nodes $R \in c h(m)$ will have the cost $c(R)$ added to the sum of reaction numbers in children. After that, all the nodes $u \in \mathcal{A}(m \mid T) \cup{m}$ would potentially have the reaction number updated following Eq (7). Thus this process requires the computation complexity to be $O(\operatorname{depth}(T))$. However in our implementation, we can update these nodes in a bottom-up fashion that starts from $m$, and stop anytime when an ancestor node</p>
<p>value doesn't change. This would speed up the update.
Update $V_{t}(\cdot \mid T)$ : Let $\mathcal{A}^{\prime}(m \mid T) \subseteq(\mathcal{A}(m \mid T) \cup{m}) \cap$ $\mathcal{V}^{m}(T)$ be the set of molecule nodes that have reaction number being updated in the stage above. From Eq (8) we can see, for any molecule node $u \in \mathcal{F}(T), V_{t}(u \mid T)$ will be recalculated if $\left{m^{\prime}: p r\left(m^{\prime}\right) \in \mathcal{A}(u \mid T)\right} \cap \mathcal{A}^{\prime}(m \mid T) \neq \emptyset$.</p>
<p>Remark: The expansion of a node $m$ can potentially affect all other nodes in $\mathcal{F}(T)$ in the worst case. However the expansion of a single molecule node $m$ will only affect another node $v$ in the frontier when it is on the current best synthesis solution that composes $V_{t}(v \mid T)$. For the actual implementation, we use efficient caching and lazy propagate mechanism, which will guarantee to only update the $V_{t}(v \mid T)$ when it is necessary. The implementation details of both above updates can be found in Appendix A.</p>
<h3>3.4. Guarantees on Finding the Optimal Solution</h3>
<p>Theorem 1 Assuming $V_{m}$ or its lowerbound is known for all encountered molecules $m$, Algorithm 1 is guaranteed to return an optimal solution, if the halting condition is changed to "the total costs of a found route is no larger than $\operatorname{argmin}<em t="t">{m \in \mathcal{F}(T)} V</em>(m)$ ".</p>
<p>The proof can be found in Appendix B.
Remark 1: If we define the cost of a reaction to be its negative log-likelihood, then 0 is the lowerbound of $V_{m}$ for any molecule $m$. The induced algorithm is guaranteed to find the optimal solution.</p>
<p>Remark 2: In practice, due to the limited time budget, we prefer the algorithm to return once a solution is found.</p>
<h3>3.5. Extension: Retro* on Graph Search Space</h3>
<p>We have been mainly illustrating the technique on a tree structured space. As the retrosynthesis planning is essentially performend on a directed graph (i.e., certain intermediate molecules may share the same reactants, which may further reduce the actual cost), the above calculation can be extended to the general bipartite graph $G$ with edges connecting $\mathcal{V}^{m}(G)$ and $\mathcal{V}^{r}(G)$. Due to the potential existence of loops, the calculation of Eq (7) will be performed using shortest path algorithm instead. As there will be no negative loops, shortest path algorithm will still converge. By viewing the search space as tree rather than graph, we may possibly find sub-optimal solution due to the repetition in state representation. However, as loopy synthesis is rare in real world, we mainly focus on the tree structured search in this paper, and will investigate this extension to bipartite graph space search in future work.</p>
<h2>4. Estimating $V_{m}$ from Planning Solutions</h2>
<p>Retro* requires the value function oracle $V_{m}$ to compute $V_{t}(\cdot \mid T)$ for expansion node selection. However in practice it is impossible to obtain the exact value of $V_{m}$ for every molecule $m$. Therefore we try to estimate it from previous planning data.</p>
<h3>4.1. Represention of $V_{m}$</h3>
<p>To parameterize $V_{m}$ for any molecule $m$, we first compute its Morgan fingerprint (Rogers \&amp; Hahn, 2010) of radius 2 with 2048 bits, and feed it into a single-layer fully connected neural network of hidden dimension 128, which then outputs a scalar representing $V_{m}$.</p>
<h3>4.2. Offline Learning of $V_{m}$</h3>
<p>Previous work has either used random rollout or human designed heuristics for estimating $V_{m}$, which may not be accurate enough to guide the search. Instead of learning it online during planning (Silver et al., 2017), we utilize the existing reactions in the training set $\mathcal{D}_{\text {train }}$ to train it.</p>
<p>Specifically, we construct retrosynthesis routes for feasible molecules in $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}$, where the available set of molecule $\mathcal{M}$ is also given beforehand. The specific construction strategy will be covered in Section 5.1.2. The resulting dataset will be $\mathcal{R}</em>$ used in the planning solution.}}=\left{r t_{i}=\left(m_{i}, v_{i}, R_{i}, B\left(m_{i}\right)\right)\right}$, where each tuple $r t_{i}$ contains the target molecule $m_{i}$, the best entire route cost $v_{i}$, the one-step retrosynthesis candidates $B\left(m_{i}\right)$ which also contains the true one-step retrosynthesis $R_{i</p>
<p>The learning of $V_{m}$ consists of two parts, namely the value fitting which is a regression loss $\mathcal{L}<em i="i">{\text {reg }}\left(r t</em>}\right)=\left(V_{m_{i}}-v_{i}\right)^{2}$ and the consistency learning which maintains the partial order relationship between best one-step solution $R_{i}$ and other solutions $\left(R_{j}, \mathcal{S<em j="j">{j}, c\left(R</em>\right)$ :
$\mathcal{L}}\right)\right) \in B\left(m_{i<em i="i">{\text {con }}\left(r t</em>}, R_{j}\right)=\max \left{0, v_{i}+\epsilon-c\left(R_{j}\right)-\sum_{m^{\prime} \in \mathcal{S<em m_prime="m^{\prime">{j}} V</em>\right}$
where $\epsilon$ is a positive constant margin to ensure $r_{i}$ has higher priority for expansion than its alternatives even if the value estimates have tolerable noise. The overall objective is:}</p>
<p>$$
\begin{aligned}
\min <em _cdot_="(\cdot)">{V</em>}} &amp; \mathbb{E<em i="i">{r t</em>} \sim \mathcal{R<em _reg="{reg" _text="\text">{\text {train }}}\left[\mathcal{L}</em>\right)+\right. \
&amp; \left.\lambda \mathbb{E}}}\left(r t_{i<em j="j">{R</em>} \sim B\left(m_{i}\right) \backslash\left{R_{i}\right}}\left[\mathcal{L<em i="i">{\text {con }}\left(r t</em>\right)\right]\right]
\end{aligned}
$$}, R_{j</p>
<p>where $\lambda$ balances these two losses. In experiment we set it to be 1 by default.</p>
<h2>5. Experiments</h2>
<h3>5.1. Creating Benchmark Dataset</h3>
<h3>5.1.1. USPTO ReACTION DATASET</h3>
<p>We use the publicly available reaction dataset extracted from United States Patent Office (USPTO) to train one-step model</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Retro*</th>
<th>Retro*-0</th>
<th>DFPN-E+</th>
<th>DFPN-E</th>
<th>MCTS+</th>
<th>MCTS</th>
<th>Greedy DFS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Success rate</td>
<td>86.84%</td>
<td>79.47%</td>
<td>53.68%</td>
<td>55.26%</td>
<td>35.79%</td>
<td>33.68%</td>
<td>22.63%</td>
</tr>
<tr>
<td>Time</td>
<td>156.58</td>
<td>208.58</td>
<td>289.42</td>
<td>279.67</td>
<td>365.21</td>
<td>370.51</td>
<td>388.15</td>
</tr>
<tr>
<td>Shorter routes</td>
<td>50</td>
<td>52</td>
<td>59</td>
<td>59</td>
<td>18</td>
<td>14</td>
<td>11</td>
</tr>
<tr>
<td>Better routes</td>
<td>112</td>
<td>102</td>
<td>22</td>
<td>25</td>
<td>46</td>
<td>41</td>
<td>26</td>
</tr>
</tbody>
</table>
<p>Table 1. Performance summary. Time is measured by the number of one-step model calls, with a hard limit of 500. The number of shorter and better routes are obtained from the comparison against the expert routes, in terms of number of reactions and the total costs.
and extract synthesis routes. The whole dataset consists of $\sim 3.8 M$ chemical reactions published up to September 2016. For reactions with multiple products, we duplicate them into multiple ones with one product each. After removing the duplications and reactions with wrong atom mappings, we further extract reaction templates with RDChiral ${ }^{3}$ for all reactions and discard those whose reactants cannot be obtained by applying reaction templates to their products. The remaining $\sim 1.3 M$ reactions are further split randomly into train/val/test sets following $80 \% / 10 \% / 10 \%$ proportions.</p>
<p>With reaction data, we train a template-based MLP model (Segler \&amp; Waller, 2017) for one-step retrosynthesis. Following literature, we formulate the one-step retrosynthesis as a multi-class classification problem, where given a molecule as product, the goal is to predict possible reaction templates. Reactants are obtained by applying the predicted templates to product molecule. There are in total $\sim 380 K$ distinct templates. Throughout all experiments, we take the top-50 templates predicted by MLP model and apply them on each product to get corresponding reactant lists.</p>
<h3>5.1.2. Extracting Synthesis Routes</h3>
<p>To train our value function and quantitatively analyze the predicted routes, we construct synthesis routes based on USPTO reaction dataset and a list of commercially available building blocks from eMolecules ${ }^{4}$. eMolecules consists of $231 M$ commercially available molecules that could work as ending points for our searching algorithm.</p>
<p>Given the list of building blocks, we take each molecule that have appeared in USPTO reaction data and analyze if it can be synthesized by existing reactions within USPTO training data. For each synthesizable molecule, we choose the shortest-possible synthesis routes with ending points being available building blocks in eMolecules.</p>
<p>We obtain validation and test route datasets with slightly different process. For validation dataset, we first combine train and validation reaction dataset, and then repeat aforementioned extraction procedure on the combined dataset. Since we extract routes with more reactions, synthesizable</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>molecules will include those who could not be synthesized with original reactions and those who have shorter routes. We exclude molecules with routes of same length as in training data, and pack the remaining as validation route dataset. We apply similar procedure to test data but make sure that there is no overlap between test and training/validation set.</p>
<p>We further clean the test route dataset by only keeping the routes whose reactions are all covered by the top- 50 predictions by the one-step model. To make the test set more challenging, we filter out the easier molecules by running a heuristic-based BFS planning algorithm, and discarding the solved molecules in a fixed time limit. After processing, we obtain 299202 training routes, 65274 validation routes, 189 test routes and the corresponding target molecules.</p>
<h3>5.2. Results</h3>
<p>We compare Retro* against DFPN-E (Kishimoto et al., 2019), MCTS (Segler et al., 2018) and greedy Depth First Search (DFS) on product molecules in test route dataset described in Section 5.1.2. Greedy DFS always prioritizes the reaction with the highest likelihood. MCTS is implemented with PUCT, where we used the reaction probability provided by the one-step model as the prior to bias the search.</p>
<p>We measure both route quality and planning efficiency to evaluate the algorithm. To measure the quality of a solution route, we compare its total cost as well as its length, i.e. number of reactions in the route. The cost function is defined as the negative log-likelihood of the reaction. Therefore, minimizing the total costs is equivalent to maximizing the likelihood of the route. To measure planning effiency, we use the number of calls to the one-step model ( $\approx 0.3 s$ per call) as a surrogate of time (since it will occupy $&gt;99 \%$ of running time) and compare the success rate under the same time limit.</p>
<p>Performance summary: The performances of all algorithms are summarized in Table 1. Under the time limit of 500 one-step calls, Retro<em> solves $31 \%$ more test molecules than the second best method, DFPN-E. Among all the solutions given by Retro</em>, 50 of them are shorter than expert routes, and 112 of them are better in terms of the total costs. We also conduct an ablation study to understand the importance of the learning component in Retro<em> by evaluating its non-learning version Retro</em>-0. Retro*-0 is obtained from</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Left: Counts of the best solutions among all algorithms in terms of length/cost; Mid: Sample solution route from Retro*. Numbers on the edges are the likelihoods of the reactions. Yellow nodes are building blocks; Right: The corresponding dotted box part in the expert route, much longer and less probable than the solution.</p>
<p>Retro<em> by setting $V_{m}$ to 0 , which is a lowerbound of any valid values. Comparing to baseline methods, Retro</em>-0 is also showing promising results. However, it is outperformed by Retro* by $6 \%$ in terms of success rate, demonstrating the performance gain brought by learning from previous planning experience.</p>
<p>To find out whether MCTS and DFPN-E can benefit from the learned value function oracle $V_{m}$ in Retro*, we replace the reward estimation by rollout in MCTS and the proof number initialization in DFPN-E by the same $V_{m}$, calling the strengthened algorithms MCTS+ and DFPN-E+. Value function helps MCTS as expected due to having a value estimate with less variance than rollout. The performance of DFPN-E is not improved because we dont have a good initialization of the disproof number.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Influence of time limit on performance.
Influence of time limit: To show the influence of time limit on performance, we plot the success rate against the number of one-step model calls in Figure 4. We can see that Retro<em> not only outperforms baseline algorithms by a large margin at the beginning, but also is improving faster than the baselines, enlarging the performance gap as the time
limit increases.
Solution quality: To evaluate the overall solution quality, for each test molecule, we collect solutions from all algorithms, and compare the route lengths and costs (see Figure 3-left). We only keep the best routes (could be multiple) for each test molecule, and count the number of best routes in total for each method. We find that in terms of total costs, Retro</em> produces $4 \times$ more best routes than the second best method. Even for the length metric, which is not the objective Retro* is optmizing for, it still achieves about the same performance as the best method.</p>
<p>As a demonstration for Retro*'s ability to find high-quality routes, we illustrate a sample solution in Figure 3-mid, where each node represents a molecule. The target molecule corresponds to the root node, and the building blocks are in yellow. The numbers on the edges indicates the likelihoods of successfully producing the corresponding reactions in realworld. The expert route provided shares the exactly the same first reaction and the same right branch with the route found by our algorithm. However, the left branch (Figure 3-right) is much longer and less probable than the corresponding part of the solution route, as shown in the dotted box region in Figure 3-mid. Please refer to Appendix C for more sample solution routes and search tree visualizations.</p>
<h2>6. Conclusion</h2>
<p>In this work, we propose Retro<em>, a learning-based retrosynthetic planning algorithm for efficiently finding high-quality routes. Retro</em> is able to utilize previous planning experience to bias the search on unseen molecules towards promising directions. We also propose a systematic approach for creating a retrosynthesis dataset from publicly available reaction datasets and novel metrics for evaluating solution routes without involving human experts. Experiments on realworld benchmark dataset demonstrate our algorithm's significant improvement over existing methods on both planning efficiency and solution quality.</p>
<h2>Acknowledgements</h2>
<p>We thank Junhong Liu, Wei Yang and Yong Liu for helpful discussions. This work is supported in part by NSF grants CDS\&amp;E-1900017 D3SC, CCF-1836936 FMitF, IIS1841351, CAREER IIS-1350983, CNS-1704701, ONR MURI grant to L.S.</p>
<h2>References</h2>
<p>Allis, L. V., van der Meulen, M., and Van Den Herik, H. J. Proof-number search. Artificial Intelligence, 66(1):91124, 1994.</p>
<p>Chen, B., Dai, B., Lin, Q., Ye, G., Liu, H., and Song, L. Learning to plan in high dimensions via neural exploration-exploitation trees. In International Conference on Learning Representations, 2020.</p>
<p>Coley, C. W., Rogers, L., Green, W. H., and Jensen, K. F. Computer-assisted retrosynthesis based on molecular similarity. ACS Central Science, 3(12):1237-1245, 2017.</p>
<p>Dai, H., Li, C., Coley, C., Dai, B., and Song, L. Retrosynthesis prediction with conditional graph logic network. In Advances in Neural Information Processing Systems, pp. 8870-8880, 2019.</p>
<p>Erol, K. Hierarchical task network planning: formalization, analysis, and implementation. PhD thesis, 1996.</p>
<p>Guez, A., Weber, T., Antonoglou, I., Simonyan, K., Vinyals, O., Wierstra, D., Munos, R., and Silver, D. Learning to search with MCTSnets. arXiv preprint arXiv:1802.04697, 2018.</p>
<p>Hart, P. E., Nilsson, N. J., and Raphael, B. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100-107, 1968.</p>
<p>Karpov, P., Godin, G., and Tetko, I. A transformer model for retrosynthesis. 2019.</p>
<p>Kishimoto, A., Buesser, B., Chen, B., and Botea, A. Depthfirst proof-number search with heuristic edge cost and application to chemical synthesis planning. In Advances in Neural Information Processing Systems, pp. 7224-7234, 2019.</p>
<p>Kocsis, L. and Szepesvári, C. Bandit based Monte-Carlo planning. In European conference on machine learning, pp. 282-293. Springer, 2006.</p>
<p>Liu, B., Ramsundar, B., Kawthekar, P., Shi, J., Gomes, J., Luu Nguyen, Q., Ho, S., Sloane, J., Wender, P., and Pande, V. Retrosynthetic reaction prediction using neural sequence-to-sequence models. ACS Central Science, 3 (10):1103-1113, 2017.</p>
<p>Rogers, D. and Hahn, M. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50 (5):742-754, 2010.</p>
<p>Schreck, J. S., Coley, C. W., and Bishop, K. J. Learning retrosynthetic planning through simulated experience. $A C S$ Central Science.</p>
<p>Segler, M., Preuß, M., and Waller, M. P. Towards" alphachem": Chemical synthesis planning with tree search and deep neural network policies. arXiv preprint arXiv:1702.00020, 2017.</p>
<p>Segler, M. H. and Waller, M. P. Neural-symbolic machine learning for retrosynthesis and reaction prediction. Chemistry-A European Journal, 23(25):5966-5971, 2017.</p>
<p>Segler, M. H., Preuss, M., and Waller, M. P. Planning chemical syntheses with deep neural networks and symbolic ai. Nature, 555(7698):604, 2018.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of GO with deep neural networks and tree search. nature, 529(7587):484, 2016.</p>
<p>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of GO without human knowledge. Nature, 550(7676):354-359, 2017.</p>
<p>Yang, K. and Deng, J. Learning to prove theorems via interacting with proof assistants. arXiv preprint arXiv:1905.09381, 2019.</p>
<h1>A. Implementation details</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Illustration for the update process. Three phases correspond to line 1-8, line 11-16, and line 17-21 in Algorithm 2.
In this section we describe the algorithm details in the update phase of Retro*. The goal of the update phase is to compute the up-to-date $V_{t}(m \mid T)$ for every molecule node $m \in \mathcal{F}(T)$. To implement efficient update, we need to cache $V_{t}(m \mid T)$ for all $m \in \mathcal{V}^{m}(T)$. Note that from Eq (8), we can observe the fact that sibling molecule nodes have the same $V_{t}(m \mid T)$, i.e. $V_{t}\left(m_{a} \mid T\right)=V_{t}\left(m_{b} \mid T\right)$ if $p r\left(m_{a} \mid T\right)=p r\left(m_{b} \mid T\right)$. Therefore instead of storing the value of $V_{t}(m \mid T)$ in every molecule node $m$, we store the value in their common parent via defining $V_{t}(R \mid T)=V_{t}(m \mid T)$ if $R=p r(m \mid T)$ for every reaction node $R \in \mathcal{V}^{r}(T)$.
In our implementation, we cache $V_{t}(R \mid T)$ for all reaction nodes $R \in \mathcal{V}^{r}(T)$ and cache $r n(v \mid T)$ for all nodes $v \in \mathcal{V}(T)$. Caching values in this way would allow us to visit each related node only once for minimal update.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">R_{i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">c</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)^</span><span class="p">{</span><span class="err">5</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">S</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="o">(</span><span class="nt">m</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">V_</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">c</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)+</span><span class="err">\</span><span class="nt">sum_</span><span class="p">{</span><span class="err">m</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="err">\mathcal{S</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="o">(</span><span class="nt">m</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">V_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">V_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">p</span><span class="w"> </span><span class="nt">r</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="nt">-r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)+</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">new</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">min</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">\in\{1,2,</span><span class="w"> </span><span class="err">\cdots,</span><span class="w"> </span><span class="err">k\</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">delta</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">new_rn-rn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">new_rn</span><span class="o">;</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{next</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="nt">delta</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">neq</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">root</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">p</span><span class="w"> </span><span class="nt">r</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)+</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">delta</span><span class="o">;</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">V_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">V_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)+</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">delta</span><span class="o">;</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">ch</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">UpdateSibling</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">m</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">delta</span><span class="w"> </span><span class="err">\</span><span class="o">()</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">p</span><span class="w"> </span><span class="nt">r</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">delta</span><span class="w"> </span><span class="err">\</span><span class="o">(=</span><span class="nt">0</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)&lt;</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="nt">delta</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="nt">-r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">R_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{current</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
</code></pre></div>

<p>The update function is summarized in Algorithm 2 and illustrated in Figure 5, which takes in the expanded node $m_{\text {next }}$ and the expansion result $\left{R_{i}, \mathcal{S}<em i="i">{i}, c\left(R</em>$, and performs updates to affected nodes. We first compute the values for new}\right)\right}_{i=1}^{k</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>reactions according to Eq (7) and (8) in line 1-8. Then we update the ancestor nodes of $m_{\text {next }}$ in a bottom-up fashion in line 9-21. We also update the molecule nodes in the sibling sub-trees in line 16 and Algorithm 3.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="nt">UpdateSibling</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">m</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">delta</span><span class="p">}</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(</span><span class="nt">1</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">T</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">r</span><span class="w"> </span><span class="nt">n</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">T</span><span class="o">)+</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">delta</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="nt">2</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">R</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">c</span><span class="w"> </span><span class="nt">h</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">T</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="nt">3</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">c</span><span class="w"> </span><span class="nt">h</span><span class="o">(</span><span class="nt">R</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">T</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="nt">4</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">quad</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">UpdateSibling</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">m</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">delta</span><span class="w"> </span><span class="err">\</span><span class="o">()</span><span class="err">\</span><span class="o">);</span>
</code></pre></div>

<p>Our implementation visits a node only when necessary. When updating along the ancestor path, it immediately stops when the influence of the expansion vanishes (line 10). When updating a single node, we use a $O(1)$ delta update by leveraging the relations derived from Eq (7) and (8), avoiding a direct computation which may require $O(k)$ or $O(\operatorname{depth}(T))$ summations.</p>
<h1>B. Guarantees on finding the optimal solution</h1>
<p>Since Retro<em> is a variant of the A</em> algorithm, we can leverage existing results to prove the theoretical guarantees for Retro<em>. In this section, we first state the assumptions we make, and then prove the admissibility (Theorem 1) of Retro</em>.</p>
<p>The theoretical results in this paper build upon the assumption that we can access $\hat{V}<em m="m">{m}$, which is a lowerbound for $V</em>$.}$ for all molecules $m$. Note that this is a weak assumption, since we know 0 is a universal lowerbound for $V_{m</p>
<p>As we describe in Eq (6), $V_{t}(m \mid T)$ can be decomposed into $g_{t}(m \mid T)$ and $h_{t}(m \mid T)$, where $g_{t}(m \mid T)$ is the exact cost of the partial route through $m$ which is already in the tree, and $h_{t}(m \mid T)$ is the future costs for frontier nodes in the route which is a summation of a series of $V_{m} \mathrm{~s}$. In practice we use $\hat{V}<em t="t">{m}$ in the summation, and arrive at $\hat{h}</em>(m \mid T)$, i.e. the following lemma.}(m \mid T)$, which is a lowerbound of $h_{t</p>
<p>Lemma 2 Assuming $V_{m}$ or its lowerbound is known for all encountered molecules $m$, then the approximated future costs $\hat{h}<em t="t">{t}(m \mid T)$ in Retro* is a lowerbound of true $h</em>(m \mid T)$.</p>
<p>We re-state the admissibility result (Theorem 1) in the main text and prove it with existing results in A* literature.</p>
<p>Theorem 1 (Admissibility) Assuming $V_{m}$ or its lowerbound is known for all encountered molecules $m$, Algorithm 1 is guaranteed to return an optimal solution, if the halting condition is changed to "the total costs of a found route is no larger than $\operatorname{argmin}<em t="t">{m \in \mathcal{F}(T)} V</em>(m)$ ".</p>
<p>Proof Combine Lemma 2 and Theorem 1 in the original A* paper (Hart et al., 1968).</p>
<h2>C. Sample search trees and solution routes</h2>
<p>In this section, we present two examples of the solution routes and the corresponding search trees for target molecule $A$ and $B$ produced by Retro*.</p>
<p>Solution route for target molecule $A / B$ is illustrated in the top/bottom sub-figure of Figure 6, where a set of edges pointing from the same product molecule to reactant molecules represents an one-step chemical reaction. Molecules on the leaf nodes are all available.</p>
<p>The search trees for molecule $A$ and $B$ are illustrated in Figure 7 and Figure 8. We use reactangular boxes to represent molecules. Yellow/grey/blue boxes indicate available/unexpanded/solved molecules. Reactangular arrows are used to represent reactions. The numbers on the edges pointing from a molecule to a reaction are the probabilities produced by the one-step model. Due to space limit, we only present the minimal tree which leads to a solution.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Top/bottom: solution route produced by Retro* for molecule $A / B$. Edges point from the same product molecule to the reactant molecules represent an one-step chemical reaction.</p>
<h1>D. Retro* for hierarchical task planning</h1>
<p>As a general planning algorithm, Retro* can be applied to other machine learning problems as well, including theorem proving (Yang \&amp; Deng, 2019) and hierarchical task planning (Erol, 1996) (or HTP), etc. Below, we conduct a synthetic experiment on HTP to demonstrate the idea. In the experiment, we are trying to search for a plan to complete a target task. The tasks (OR nodes) can be completed with different methods, and each method (AND nodes) requires a sequence of subtasks to be completed. Furthermore, each method is associated with a nonnegative cost. The goal is to find a plan with minimum total cost to realize the target task by decomposing it recursively until all the leaf task nodes represent primitive tasks that we know how to execute directly. As an example, to travel from home in city $A$ to hotel in city $B$, we can take either flight, train or ship, each with its own cost. For each method, we have subtasks such as home $\rightarrow$ airport $A$, flight $(A \rightarrow B)$, and airport $B \rightarrow$ hotel. These subtasks can be further realized by several methods.</p>
<p>As usual, we want to find a plan with small cost in limited time which is measured by the number of expansions of task nodes. We use the optimal halting condition as stated in theorem 1. We compare our algorithms against DFPN-E, the best performing baseline. The results are summarized in Table 2 and 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Time Limit</th>
<th style="text-align: center;">15</th>
<th style="text-align: center;">20</th>
<th style="text-align: center;">25</th>
<th style="text-align: center;">30</th>
<th style="text-align: center;">35</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Retro*</td>
<td style="text-align: center;">.67</td>
<td style="text-align: center;">.91</td>
<td style="text-align: center;">.96</td>
<td style="text-align: center;">.98</td>
<td style="text-align: center;">1.</td>
</tr>
<tr>
<td style="text-align: left;">Retro*-0</td>
<td style="text-align: center;">.50</td>
<td style="text-align: center;">.86</td>
<td style="text-align: center;">.95</td>
<td style="text-align: center;">.98</td>
<td style="text-align: center;">.99</td>
</tr>
<tr>
<td style="text-align: left;">DFPN-E</td>
<td style="text-align: center;">.02</td>
<td style="text-align: center;">.33</td>
<td style="text-align: center;">.74</td>
<td style="text-align: center;">.93</td>
<td style="text-align: center;">.97</td>
</tr>
</tbody>
</table>
<p>Table 2. Success rate (higher is better) vs time limit.</p>
<p>As we can see, in terms of success rate, Retro<em> is slightly better than Retro</em>-0, and both of them are significantly better than DFPN-E. In terms of solution quality, we compute the approximation ratio ( $=$ solution cost $/$ ground truth best solution cost)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Alg</th>
<th style="text-align: left;">Retro*</th>
<th style="text-align: left;">Retro*-0</th>
<th style="text-align: left;">DFPN-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Avg. AR</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">Max. AR</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">3.9</td>
</tr>
</tbody>
</table>
<p>Table 3. AR = Approximation ratio (lower is better), time limit=35.
for every solution, and verify the theoretical guarantee in theorem 1 on finding the best solution.</p>
<h1>E. Related Works</h1>
<p>Reinforcement learning algorithms (without planning) have also been considered for the retrosynthesis problem. Schreck et al. leverages self-play experience to fit a value function and uses policy iteration for learning an expansion policy. It is possible to combine it with a planning algorithm to achieve better performance in practice.</p>
<p>Learning to search from previous planning experiences has been well studied and applied to Go (Silver et al., 2016; 2017), Sokoban (Guez et al., 2018) and path planning (Chen et al., 2020). Existing methods cannot be directly applied to the retrosynthesis problem since the search space is more complicated, and the traditional representation where a node corresponds to a state is highly inefficient, as we mentioned in the discussion on MCTS in previous sections.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Search tree produced by Retro* for molecule $A$. Reactangular boxes/arrows represent molecules/reactions. Yellow/grey/blue indicate available/unexpanded/solved molecules. Numbers on the edges are the probabilities produced by the one-step model.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Search tree produced by Retro* for molecule $B$. Reactangular boxes/arrows represent molecules/reactions. Yellow/grey/blue indicate available/unexpanded/solved molecules. Numbers on the edges are the probabilities produced by the one-step model.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ For clarity, we omit the condition on $T$ in the notations.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ For simplicity we follow the common practice to ignore the reagents and other chemical reaction conditions.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4441 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4441</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4441</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-259692664</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.09841v4.pdf" target="_blank">Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning consistently plays a fundamental and significant role in the domains of knowledge engineering and artificial intelligence. Recently, Large Language Models (LLMs) have emerged as a noteworthy innovation in natural language processing (NLP). However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered. To this end, we aim to bridge this gap and provide comprehensive evaluations in this paper. First, to offer systematic evaluations, we select fifteen typical logical reasoning datasets and organize them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include 3 early-era representative LLMs and 4 trending LLMs. Second, different from previous evaluations relying only on simple metrics (e.g., <italic>accuracy</italic>), we propose fine-level evaluations in objective and subjective manners, covering both answers and explanations, including <italic>answer correctness</italic>, <italic>explain correctness</italic>, <italic>explain completeness</italic> and <italic>explain redundancy</italic>. Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., <italic>evidence selection process</italic> and <italic>reasoning process</italic>. Third, to avoid the influences of knowledge bias and concentrate purely on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content. Based on the in-depth evaluations, this paper finally forms a general evaluation scheme of logical reasoning capability from six dimensions (i.e., <italic>Correct</italic>, <italic>Rigorous</italic>, <italic>Self-aware</italic>, <italic>Active</italic>, <italic>Oriented</italic> and <italic>No hallucination</italic>). It reflects the pros and cons of LLMs and gives guiding directions for future works.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4441.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4441.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-level metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-level evaluation metrics for LLM logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of four complementary metrics (Answer Correctness, Explanation Correctness, Explanation Completeness, Explanation Redundancy) designed to assess both outputs and intermediate explanations of LLMs on logical reasoning tasks, treating explanation metrics as independent and semantically-aware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Four-dimension fine-level metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate each LLM output using: (1) Answer Correctness — semantic alignment of generated answer with ground truth (not token-match); (2) Explanation Correctness — whether the generated explanation is logically correct for the true answer; (3) Explanation Completeness — whether the explanation contains all elements needed (selected facts and intermediate facts) so the correct answer can be inferred; (4) Explanation Redundancy — whether the explanation includes unnecessary facts beyond the minimal set needed. These metrics are assessed per example and are independent (an explanation can be correct but incomplete, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Answer correctness (semantic match), Explanation correctness (logical validity), Explanation completeness (coverage of necessary premises/intermediate steps), Explanation parsimony/self-awareness measured via redundancy (absence of unnecessary facts).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language logical reasoning / AI evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory reasoning chains / logical inferences (explanations of answers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied across 15 logical-reasoning datasets and NeuLR; results show substantial drops when moving from answer-only accuracy to answer+explanation metrics (rigor). Specific aggregated numeric breakdowns reported per-dataset in the paper; no single overall scalar reported for all four metrics combined. Explanation-related metrics were obtained by manual annotation for three LLMs and automatic LLM-as-judge for four LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: manual human annotation (ten graduate students) used for three LLMs; automatic evaluation using an LLM judge (GPT-4o) for four trending LLMs after calibrating correspondence with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Pilot study comparing LLM-as-judge (GPT-4o) against human annotations on 300 samples (reported alignment / consistency in Fig.13), used to justify automated evaluation for newer models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Subjectivity in judging explanations (though instructions were formalized); annotation cost leading to sampling for some datasets; inter-annotator variability mitigated by trained annotators and LLM-as-judge calibration but detailed IRR statistics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied across 15 existing logical-reasoning datasets (deductive/inductive/abductive/mixed) and NeuLR (new neutral-content dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4441.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-dimension five-type error taxonomy for logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic taxonomy attributing incorrect/explanatory-failure cases to evidence-selection errors (wrong selection, hallucination) and reasoning-process errors (no reasoning, perspective mistake, process mistake), intended to explain sources of LLM logical failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Evidence-selection & reasoning-process error attribution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each problematic case (wrong answer or wrong explanation), annotate a single primary error in the reasoning-process dimension (no reasoning — LLM just lists facts; perspective mistake — starts from wrong viewpoint; process mistake — logical steps flawed) and optionally an evidence-selection label (wrong selection — required facts ignored/wrongly chosen; hallucination — asserted facts contradict or are unverified by context). Aggregated statistics are reported per dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Categorical attribution into five types: evidence selection: {wrong selection, hallucination}; reasoning process: {no reasoning, perspective mistake, process mistake}. Each problematic case receives one reasoning-process label and zero/one evidence-selection labels as applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP logical reasoning / diagnostic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Error analysis for generated explanations and inferred conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported aggregated shares across evaluated failure cases: wrong selection 33.26%, hallucination 27.46%; reasoning-process breakdown: no reasoning 19.33%, perspective mistakes 44.47%, process mistakes 36.20% (numbers from the paper's aggregated analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-annotated for primary experiments; automated attribution used for trending LLMs via GPT-4o after calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No formal statistical inter-rater reliability reported in main text; validity argued via careful definitions and pilot alignment between human and LLM-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Ambiguity when multiple error types co-occur; single-label restriction for reasoning-process dimension may oversimplify multi-causal failures; subjective judgments required to separate hallucination from creative but valid inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to the same 15 datasets and NeuLR; attribution visualized per dataset in Fig.6.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4441.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rigorousness metric / 'Rigorous'</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rigorous reasoning metric (Correct answer + correct & complete explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite operationalization of 'rigorous' reasoning defined as when an LLM provides the correct answer together with a correct and complete explanation; used to measure whether correct outputs reflect proper reasoning rather than lucky guesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rigorous reasoning (answer+explanation completeness & correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the subset of cases where (1) Answer Correctness = true, (2) Explanation Correctness = true, and (3) Explanation Completeness = true. Reported as counts/percentages per dataset and aggregated into heatmaps; compared against answer-only accuracy to show drop-off ('ratio of rigorous performance to answer accuracy').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Answer correctness + explanation correctness + explanation completeness simultaneously. Also alternative two-condition aggregations reported (answer+correct explanation irrespective of completeness; answer+complete explanation irrespective of explanation correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP logical reasoning / evaluation of explanatory fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Verification of explanatory chains (causal/logic explanations supporting answers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Rigorous-performance rates are substantially lower than answer-only accuracy across models and datasets; BARD had the best rigorous capability among three early LLMs while ChatGPT performed worse on deductive/inductive settings; exact per-dataset rigorous percentages reported in Table 3 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid — manual annotation for explanation labels used to compute rigorous metric for some models; automated LLM judge used for trending models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Metric grounded in concrete annotation definitions (correctness/completeness) and validated indirectly by showing consistent patterns across datasets and models; no external gold-standard for explanation completeness beyond reference explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires ground-truth 'complete explanation' annotations (expensive); completeness is partially subjective and dependent on reference explanations; may penalize alternative valid reasoning chains that differ in granularity or intermediate steps from reference.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used across 15 datasets and NeuLR; rigorous-vs-accuracy heatmaps shown in Fig.5a.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4441.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Six-dimension scheme</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Six-dimension evaluation scheme (Correct, Rigorous, Self-aware, Active, Oriented, No hallucination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A holistic evaluation framework abstracting logical reasoning capability into six quantifiable properties: Correct (answer accuracy), Rigorous (answer+explanation correctness/completeness), Self-aware (low redundancy), Active (low 'no reasoning' rate), Oriented (low perspective-mistake rate), and No hallucination (low hallucination rate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Six-dimension capability scheme</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Quantify each of six properties from annotated/automatic metrics: Correct = answer correctness rate; Rigorous = proportion of cases meeting rigorous definition; Self-aware = inverse of explanation redundancy rate; Active = inverse of 'no reasoning' error rate; Oriented = inverse of perspective-mistake error rate; No hallucination = inverse of hallucination rate. Visualize ability maps per model and per reasoning setting (deductive/inductive/abductive/mixed).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Six operational metrics as above, each derived from the fine-level metrics and error taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Meta-evaluation framework for reasoning/explanatory capability</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Ability maps (heatmaps) produced for each LLM showing strengths/weaknesses across dimensions and reasoning types; e.g., BARD scored high on Correct and Active, ChatGPT high on Oriented and No-hallucination but low on Rigorous and Active in some settings. Aggregated visualizations for early-era and trending LLMs in Figs.11-12 and 15.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Derived from combination of human annotations (for three models) and automatic LLM-judge evaluations (for trending models).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Construct validity supported by mapping to concrete lower-level metrics (fine-level metrics and error taxonomy) and demonstrating discriminative differences across models and settings; no formal external validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Choice of six properties is heuristic and domain-specific; weighting and aggregation choices may affect resulting ability maps; some dimensions (e.g., Self-aware) operationalized via single proxy (redundancy) which may not capture all aspects of self-awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Computed across 15 datasets and NeuLR; visualized per reasoning setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4441.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeuLR: Neutral-content Logical Reasoning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark of 3,000 content-neutral logical reasoning samples (1,000 each for deductive, inductive, abductive) created by replacing concrete entities/properties with randomized tokens to decouple reasoning from world-knowledge memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Neutral-content benchmarking (NeuLR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct dataset by taking existing logical-reasoning data and replacing names/species/properties with random 6-character tokens prefixed with NP/SP/ADP to remove commonsense/knowledge priors. Use prompts that explicitly describe the neutral token scheme to models and evaluate performance under zero-shot, few-shot, and chain-of-thought settings to isolate pure logical capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Answer correctness and the four fine-level explanation metrics on neutralized examples; comparison of zero/one-shot and chain-of-thought effects to identify gains that derive from reasoning vs. content memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Logical reasoning / benchmark construction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Benchmarks for testing generative reasoning and explanatory fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>NeuLR results: overall LLM performance still leaves room for improvement. In zero-shot NeuLR experiments on three LLMs, BARD > text-davinci-003 > ChatGPT; chain-of-thought and few-shot often improve accuracy (COT helps most). Notably, ordering of per-reasoning-type difficulty changed compared to classical datasets (on NeuLR inductive > deductive/abductive for some models). Exact numeric table in Table 5 (e.g., NeuLR overall stats for models reported).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Evaluations reported with same hybrid annotation setup (manual for some models, automated judge for others).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Dataset constructed from prior benchmarks and neutralized; prompts include explicit neutral-content description. No separate external validation beyond paper experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Neutralization removes useful linguistic/commonsense cues but may create artificial token distributions unfamiliar to some models; random-token replacement may still leave structural biases; sample construction details and potential leakage into training data for some LLMs need consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>NeuLR (3,000 neutralized examples: 1k deductive, 1k inductive, 1k abductive).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4441.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relative perf. metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative performance (LLM accuracy / SOTA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple relative-performance metric that divides an LLM's accuracy by the reported SOTA accuracy for the same dataset to quantify how close an LLM is to state-of-the-art specialized systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-to-SOTA relative accuracy ratio</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each dataset compute ratio = (LLM accuracy) / (SOTA accuracy) to contextualize absolute accuracy; used to create comparative visualizations across reasoning types and prompting settings (see Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dataset-specific ratio values; aggregated across datasets or weighted by dataset size to summarize per-reasoning-mode performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP benchmarking / comparative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Performance comparison / benchmark proximity</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported that many LLMs fall substantially short of SOTA smaller specialized models; relative ratios visualized per reasoning mode showing LLM weakness especially in inductive settings.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated computation from answer-correctness scores; underlying answer correctness labels obtained via human annotation or LLM-judge as per dataset/model.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Relies on reported SOTA numbers for each dataset; interpretability depends on accuracy of those SOTA baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>SOTA baselines vary in task formulation and may not be strictly comparable to LLM evaluation setups; ratio sensitive to differences in evaluation protocols (e.g., generation vs classification).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied across the 15 selected logical-reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4441.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation protocol (human + LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid evaluation protocol using human annotators and LLM-as-judge (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-mode annotation pipeline: manual human annotation for explanation-level metrics on some models/datasets, and automated LLM-as-judge (GPT-4o) for scaling to more models, validated via pilot alignment experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hybrid annotation with LLM judge calibration</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect model outputs via API, have trained human annotators label answer correctness and three explanation metrics for a subset (used for three early LLMs). For four trending LLMs use GPT-4o as automated judge: feed it the metric definitions and ask it to annotate outputs. Demonstrate alignment between human and GPT-4o on a 300-sample pilot before full automated use.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human labels used as gold for calibration; automated judge expected to follow explicit instructions and replicate human labels at acceptable agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Annotation methodology / evaluation scaling</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Meta-evaluation / annotation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Pilot (300 samples) indicated good match between GPT-4o judgements and human annotations (Fig.13), justifying automated-mode evaluation for trending LLMs; used to produce full automatic evaluations (Figs.14-15).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid by design; human annotation for high-cost/critical evaluations, automated judge for scale.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Pilot alignment study between human annotators and GPT-4o on 300 samples; exact numeric agreement statistics not tabulated in main text but reported as supportive evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Automated judge may inherit biases or errors; reliance on LLM judge risks circularity if judge shares training data/inductive biases with evaluated models; limited disclosure of inter-annotator agreement metrics for human annotators in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4441.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4441.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies & evaluation modes (zero-shot, few-shot, COT, POT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation explicitly varies prompt styles and in-context learning regimes — zero-shot, 1-shot, 3-shot, chain-of-thought (COT), and program-of-thought (POT) — to measure their differential effects on answer and explanation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompting-mode comparison (zero/one/three-shot, COT, POT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run each LLM under multiple prompting regimes: direct zero-/few-shot prompts, chain-of-thought prompting, and program-of-thought where applicable. Compare answer correctness and explanation metrics across modes to determine when and how prompting strategies improve rigorous reasoning. POT is applied selectively using external symbolic solvers (Pyke) for datasets amenable to symbolic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-mode change in answer correctness, explanation correctness/completeness/redundancy; also whether few-shot aids multi-hop reasoning; COT/POT gains reported per dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Prompt engineering / reasoning elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Methodological variations in eliciting reasoning/explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Findings: few-shot ICL yields inconsistent gains for logical reasoning; COT helps on NeuLR and some cases but can harm abductive tasks; POT benefits powerful proprietary LLMs on symbolic tasks and helps numeric/symbolic reasoning, while many open-source LLMs struggle to benefit from POT. Per-dataset numeric comparisons shown in Tables 1-2 and Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Metric outcomes measured via annotation pipeline (hybrid human/LLM-judge) as above.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison across many datasets and models; selective external symbolic solver use for POT experiments (Pyke) to validate generated symbolic programs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompting sensitivity: results vary widely with exact prompt and exemplar choice; POT requires reliable symbolic generation which many models cannot produce robustly; external solver integration nontrivial and not uniformly applicable across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Experiments across 15 datasets and NeuLR; POT applied to limited symbolic datasets (ProofWriter, RuleTaker) as noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Survey of hallucination in natural language generation <em>(Rating: 1)</em></li>
                <li>Abductive commonsense reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4441",
    "paper_id": "paper-259692664",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Fine-level metrics",
            "name_full": "Fine-level evaluation metrics for LLM logical reasoning",
            "brief_description": "A set of four complementary metrics (Answer Correctness, Explanation Correctness, Explanation Completeness, Explanation Redundancy) designed to assess both outputs and intermediate explanations of LLMs on logical reasoning tasks, treating explanation metrics as independent and semantically-aware.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Four-dimension fine-level metrics",
            "evaluation_method_description": "Evaluate each LLM output using: (1) Answer Correctness — semantic alignment of generated answer with ground truth (not token-match); (2) Explanation Correctness — whether the generated explanation is logically correct for the true answer; (3) Explanation Completeness — whether the explanation contains all elements needed (selected facts and intermediate facts) so the correct answer can be inferred; (4) Explanation Redundancy — whether the explanation includes unnecessary facts beyond the minimal set needed. These metrics are assessed per example and are independent (an explanation can be correct but incomplete, etc.).",
            "evaluation_criteria": "Answer correctness (semantic match), Explanation correctness (logical validity), Explanation completeness (coverage of necessary premises/intermediate steps), Explanation parsimony/self-awareness measured via redundancy (absence of unnecessary facts).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural language logical reasoning / AI evaluation",
            "theory_type": "Explanatory reasoning chains / logical inferences (explanations of answers)",
            "human_comparison": false,
            "evaluation_results": "Applied across 15 logical-reasoning datasets and NeuLR; results show substantial drops when moving from answer-only accuracy to answer+explanation metrics (rigor). Specific aggregated numeric breakdowns reported per-dataset in the paper; no single overall scalar reported for all four metrics combined. Explanation-related metrics were obtained by manual annotation for three LLMs and automatic LLM-as-judge for four LLMs.",
            "automated_vs_human_evaluation": "Hybrid: manual human annotation (ten graduate students) used for three LLMs; automatic evaluation using an LLM judge (GPT-4o) for four trending LLMs after calibrating correspondence with human labels.",
            "validation_method": "Pilot study comparing LLM-as-judge (GPT-4o) against human annotations on 300 samples (reported alignment / consistency in Fig.13), used to justify automated evaluation for newer models.",
            "limitations_challenges": "Subjectivity in judging explanations (though instructions were formalized); annotation cost leading to sampling for some datasets; inter-annotator variability mitigated by trained annotators and LLM-as-judge calibration but detailed IRR statistics not provided.",
            "benchmark_dataset": "Applied across 15 existing logical-reasoning datasets (deductive/inductive/abductive/mixed) and NeuLR (new neutral-content dataset).",
            "uuid": "e4441.0",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Error taxonomy",
            "name_full": "Two-dimension five-type error taxonomy for logical reasoning",
            "brief_description": "A diagnostic taxonomy attributing incorrect/explanatory-failure cases to evidence-selection errors (wrong selection, hallucination) and reasoning-process errors (no reasoning, perspective mistake, process mistake), intended to explain sources of LLM logical failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Evidence-selection & reasoning-process error attribution",
            "evaluation_method_description": "For each problematic case (wrong answer or wrong explanation), annotate a single primary error in the reasoning-process dimension (no reasoning — LLM just lists facts; perspective mistake — starts from wrong viewpoint; process mistake — logical steps flawed) and optionally an evidence-selection label (wrong selection — required facts ignored/wrongly chosen; hallucination — asserted facts contradict or are unverified by context). Aggregated statistics are reported per dataset and model.",
            "evaluation_criteria": "Categorical attribution into five types: evidence selection: {wrong selection, hallucination}; reasoning process: {no reasoning, perspective mistake, process mistake}. Each problematic case receives one reasoning-process label and zero/one evidence-selection labels as applicable.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP logical reasoning / diagnostic evaluation",
            "theory_type": "Error analysis for generated explanations and inferred conclusions",
            "human_comparison": false,
            "evaluation_results": "Reported aggregated shares across evaluated failure cases: wrong selection 33.26%, hallucination 27.46%; reasoning-process breakdown: no reasoning 19.33%, perspective mistakes 44.47%, process mistakes 36.20% (numbers from the paper's aggregated analysis).",
            "automated_vs_human_evaluation": "Human-annotated for primary experiments; automated attribution used for trending LLMs via GPT-4o after calibration.",
            "validation_method": "No formal statistical inter-rater reliability reported in main text; validity argued via careful definitions and pilot alignment between human and LLM-judge.",
            "limitations_challenges": "Ambiguity when multiple error types co-occur; single-label restriction for reasoning-process dimension may oversimplify multi-causal failures; subjective judgments required to separate hallucination from creative but valid inferences.",
            "benchmark_dataset": "Applied to the same 15 datasets and NeuLR; attribution visualized per dataset in Fig.6.",
            "uuid": "e4441.1",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Rigorousness metric / 'Rigorous'",
            "name_full": "Rigorous reasoning metric (Correct answer + correct & complete explanation)",
            "brief_description": "A composite operationalization of 'rigorous' reasoning defined as when an LLM provides the correct answer together with a correct and complete explanation; used to measure whether correct outputs reflect proper reasoning rather than lucky guesses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Rigorous reasoning (answer+explanation completeness & correctness)",
            "evaluation_method_description": "Compute the subset of cases where (1) Answer Correctness = true, (2) Explanation Correctness = true, and (3) Explanation Completeness = true. Reported as counts/percentages per dataset and aggregated into heatmaps; compared against answer-only accuracy to show drop-off ('ratio of rigorous performance to answer accuracy').",
            "evaluation_criteria": "Answer correctness + explanation correctness + explanation completeness simultaneously. Also alternative two-condition aggregations reported (answer+correct explanation irrespective of completeness; answer+complete explanation irrespective of explanation correctness).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP logical reasoning / evaluation of explanatory fidelity",
            "theory_type": "Verification of explanatory chains (causal/logic explanations supporting answers)",
            "human_comparison": false,
            "evaluation_results": "Rigorous-performance rates are substantially lower than answer-only accuracy across models and datasets; BARD had the best rigorous capability among three early LLMs while ChatGPT performed worse on deductive/inductive settings; exact per-dataset rigorous percentages reported in Table 3 of the paper.",
            "automated_vs_human_evaluation": "Hybrid — manual annotation for explanation labels used to compute rigorous metric for some models; automated LLM judge used for trending models.",
            "validation_method": "Metric grounded in concrete annotation definitions (correctness/completeness) and validated indirectly by showing consistent patterns across datasets and models; no external gold-standard for explanation completeness beyond reference explanations.",
            "limitations_challenges": "Requires ground-truth 'complete explanation' annotations (expensive); completeness is partially subjective and dependent on reference explanations; may penalize alternative valid reasoning chains that differ in granularity or intermediate steps from reference.",
            "benchmark_dataset": "Used across 15 datasets and NeuLR; rigorous-vs-accuracy heatmaps shown in Fig.5a.",
            "uuid": "e4441.2",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Six-dimension scheme",
            "name_full": "Six-dimension evaluation scheme (Correct, Rigorous, Self-aware, Active, Oriented, No hallucination)",
            "brief_description": "A holistic evaluation framework abstracting logical reasoning capability into six quantifiable properties: Correct (answer accuracy), Rigorous (answer+explanation correctness/completeness), Self-aware (low redundancy), Active (low 'no reasoning' rate), Oriented (low perspective-mistake rate), and No hallucination (low hallucination rate).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Six-dimension capability scheme",
            "evaluation_method_description": "Quantify each of six properties from annotated/automatic metrics: Correct = answer correctness rate; Rigorous = proportion of cases meeting rigorous definition; Self-aware = inverse of explanation redundancy rate; Active = inverse of 'no reasoning' error rate; Oriented = inverse of perspective-mistake error rate; No hallucination = inverse of hallucination rate. Visualize ability maps per model and per reasoning setting (deductive/inductive/abductive/mixed).",
            "evaluation_criteria": "Six operational metrics as above, each derived from the fine-level metrics and error taxonomy.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP evaluation / logical reasoning",
            "theory_type": "Meta-evaluation framework for reasoning/explanatory capability",
            "human_comparison": false,
            "evaluation_results": "Ability maps (heatmaps) produced for each LLM showing strengths/weaknesses across dimensions and reasoning types; e.g., BARD scored high on Correct and Active, ChatGPT high on Oriented and No-hallucination but low on Rigorous and Active in some settings. Aggregated visualizations for early-era and trending LLMs in Figs.11-12 and 15.",
            "automated_vs_human_evaluation": "Derived from combination of human annotations (for three models) and automatic LLM-judge evaluations (for trending models).",
            "validation_method": "Construct validity supported by mapping to concrete lower-level metrics (fine-level metrics and error taxonomy) and demonstrating discriminative differences across models and settings; no formal external validation reported.",
            "limitations_challenges": "Choice of six properties is heuristic and domain-specific; weighting and aggregation choices may affect resulting ability maps; some dimensions (e.g., Self-aware) operationalized via single proxy (redundancy) which may not capture all aspects of self-awareness.",
            "benchmark_dataset": "Computed across 15 datasets and NeuLR; visualized per reasoning setting.",
            "uuid": "e4441.3",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "NeuLR",
            "name_full": "NeuLR: Neutral-content Logical Reasoning dataset",
            "brief_description": "A new benchmark of 3,000 content-neutral logical reasoning samples (1,000 each for deductive, inductive, abductive) created by replacing concrete entities/properties with randomized tokens to decouple reasoning from world-knowledge memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Neutral-content benchmarking (NeuLR)",
            "evaluation_method_description": "Construct dataset by taking existing logical-reasoning data and replacing names/species/properties with random 6-character tokens prefixed with NP/SP/ADP to remove commonsense/knowledge priors. Use prompts that explicitly describe the neutral token scheme to models and evaluate performance under zero-shot, few-shot, and chain-of-thought settings to isolate pure logical capability.",
            "evaluation_criteria": "Answer correctness and the four fine-level explanation metrics on neutralized examples; comparison of zero/one-shot and chain-of-thought effects to identify gains that derive from reasoning vs. content memorization.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Logical reasoning / benchmark construction",
            "theory_type": "Benchmarks for testing generative reasoning and explanatory fidelity",
            "human_comparison": false,
            "evaluation_results": "NeuLR results: overall LLM performance still leaves room for improvement. In zero-shot NeuLR experiments on three LLMs, BARD &gt; text-davinci-003 &gt; ChatGPT; chain-of-thought and few-shot often improve accuracy (COT helps most). Notably, ordering of per-reasoning-type difficulty changed compared to classical datasets (on NeuLR inductive &gt; deductive/abductive for some models). Exact numeric table in Table 5 (e.g., NeuLR overall stats for models reported).",
            "automated_vs_human_evaluation": "Evaluations reported with same hybrid annotation setup (manual for some models, automated judge for others).",
            "validation_method": "Dataset constructed from prior benchmarks and neutralized; prompts include explicit neutral-content description. No separate external validation beyond paper experiments reported.",
            "limitations_challenges": "Neutralization removes useful linguistic/commonsense cues but may create artificial token distributions unfamiliar to some models; random-token replacement may still leave structural biases; sample construction details and potential leakage into training data for some LLMs need consideration.",
            "benchmark_dataset": "NeuLR (3,000 neutralized examples: 1k deductive, 1k inductive, 1k abductive).",
            "uuid": "e4441.4",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Relative perf. metric",
            "name_full": "Relative performance (LLM accuracy / SOTA)",
            "brief_description": "A simple relative-performance metric that divides an LLM's accuracy by the reported SOTA accuracy for the same dataset to quantify how close an LLM is to state-of-the-art specialized systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-to-SOTA relative accuracy ratio",
            "evaluation_method_description": "For each dataset compute ratio = (LLM accuracy) / (SOTA accuracy) to contextualize absolute accuracy; used to create comparative visualizations across reasoning types and prompting settings (see Fig.3).",
            "evaluation_criteria": "Dataset-specific ratio values; aggregated across datasets or weighted by dataset size to summarize per-reasoning-mode performance.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP benchmarking / comparative evaluation",
            "theory_type": "Performance comparison / benchmark proximity",
            "human_comparison": false,
            "evaluation_results": "Reported that many LLMs fall substantially short of SOTA smaller specialized models; relative ratios visualized per reasoning mode showing LLM weakness especially in inductive settings.",
            "automated_vs_human_evaluation": "Automated computation from answer-correctness scores; underlying answer correctness labels obtained via human annotation or LLM-judge as per dataset/model.",
            "validation_method": "Relies on reported SOTA numbers for each dataset; interpretability depends on accuracy of those SOTA baselines.",
            "limitations_challenges": "SOTA baselines vary in task formulation and may not be strictly comparable to LLM evaluation setups; ratio sensitive to differences in evaluation protocols (e.g., generation vs classification).",
            "benchmark_dataset": "Applied across the 15 selected logical-reasoning datasets.",
            "uuid": "e4441.5",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Evaluation protocol (human + LLM judge)",
            "name_full": "Hybrid evaluation protocol using human annotators and LLM-as-judge (GPT-4o)",
            "brief_description": "A two-mode annotation pipeline: manual human annotation for explanation-level metrics on some models/datasets, and automated LLM-as-judge (GPT-4o) for scaling to more models, validated via pilot alignment experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Hybrid annotation with LLM judge calibration",
            "evaluation_method_description": "Collect model outputs via API, have trained human annotators label answer correctness and three explanation metrics for a subset (used for three early LLMs). For four trending LLMs use GPT-4o as automated judge: feed it the metric definitions and ask it to annotate outputs. Demonstrate alignment between human and GPT-4o on a 300-sample pilot before full automated use.",
            "evaluation_criteria": "Human labels used as gold for calibration; automated judge expected to follow explicit instructions and replicate human labels at acceptable agreement.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Annotation methodology / evaluation scaling",
            "theory_type": "Meta-evaluation / annotation protocol",
            "human_comparison": false,
            "evaluation_results": "Pilot (300 samples) indicated good match between GPT-4o judgements and human annotations (Fig.13), justifying automated-mode evaluation for trending LLMs; used to produce full automatic evaluations (Figs.14-15).",
            "automated_vs_human_evaluation": "Hybrid by design; human annotation for high-cost/critical evaluations, automated judge for scale.",
            "validation_method": "Pilot alignment study between human annotators and GPT-4o on 300 samples; exact numeric agreement statistics not tabulated in main text but reported as supportive evidence.",
            "limitations_challenges": "Automated judge may inherit biases or errors; reliance on LLM judge risks circularity if judge shares training data/inductive biases with evaluated models; limited disclosure of inter-annotator agreement metrics for human annotators in main text.",
            "uuid": "e4441.6",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Prompting & modes",
            "name_full": "Prompting strategies & evaluation modes (zero-shot, few-shot, COT, POT)",
            "brief_description": "Evaluation explicitly varies prompt styles and in-context learning regimes — zero-shot, 1-shot, 3-shot, chain-of-thought (COT), and program-of-thought (POT) — to measure their differential effects on answer and explanation metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Prompting-mode comparison (zero/one/three-shot, COT, POT)",
            "evaluation_method_description": "Run each LLM under multiple prompting regimes: direct zero-/few-shot prompts, chain-of-thought prompting, and program-of-thought where applicable. Compare answer correctness and explanation metrics across modes to determine when and how prompting strategies improve rigorous reasoning. POT is applied selectively using external symbolic solvers (Pyke) for datasets amenable to symbolic execution.",
            "evaluation_criteria": "Per-mode change in answer correctness, explanation correctness/completeness/redundancy; also whether few-shot aids multi-hop reasoning; COT/POT gains reported per dataset and model.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Prompt engineering / reasoning elicitation",
            "theory_type": "Methodological variations in eliciting reasoning/explanations",
            "human_comparison": false,
            "evaluation_results": "Findings: few-shot ICL yields inconsistent gains for logical reasoning; COT helps on NeuLR and some cases but can harm abductive tasks; POT benefits powerful proprietary LLMs on symbolic tasks and helps numeric/symbolic reasoning, while many open-source LLMs struggle to benefit from POT. Per-dataset numeric comparisons shown in Tables 1-2 and Table 5.",
            "automated_vs_human_evaluation": "Metric outcomes measured via annotation pipeline (hybrid human/LLM-judge) as above.",
            "validation_method": "Empirical comparison across many datasets and models; selective external symbolic solver use for POT experiments (Pyke) to validate generated symbolic programs.",
            "limitations_challenges": "Prompting sensitivity: results vary widely with exact prompt and exemplar choice; POT requires reliable symbolic generation which many models cannot produce robustly; external solver integration nontrivial and not uniformly applicable across datasets.",
            "benchmark_dataset": "Experiments across 15 datasets and NeuLR; POT applied to limited symbolic datasets (ProofWriter, RuleTaker) as noted.",
            "uuid": "e4441.7",
            "source_info": {
                "paper_title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2,
            "sanitized_title": "explaining_answers_with_entailment_trees"
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Survey of hallucination in natural language generation",
            "rating": 1,
            "sanitized_title": "survey_of_hallucination_in_natural_language_generation"
        },
        {
            "paper_title": "Abductive commonsense reasoning",
            "rating": 1,
            "sanitized_title": "abductive_commonsense_reasoning"
        }
    ],
    "cost": 0.019389749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond
15 Sep 2024</p>
<p>Fangzhi Xu 
Qika Lin 
Member, IEEEJiawei Han 
Tianzhe Zhao 
Senior Member, IEEEJun Liu 
Fellow, IEEEErik Cambria 
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond
15 Sep 20243E857D74EA0E3712898F90AAA0997750arXiv:2306.09841v4[cs.CL]Logical reasoninglarge language modeldeductive reasoninginductive reasoningabductive reasoning
Logical reasoning consistently plays a fundamental and significant role in the domains of knowledge engineering and artificial intelligence.Recently, Large Language Models (LLMs) have emerged as a noteworthy innovation in natural language processing (NLP).However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered.To this end, we aim to bridge this gap and provide comprehensive evaluations in this paper.Firstly, to offer systematic evaluations, we select fifteen typical logical reasoning datasets and organize them into deductive, inductive, abductive and mixed-form reasoning settings.Considering the comprehensiveness of evaluations, we include 3 early-era representative LLMs and 4 trending LLMs.Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy ), we propose fine-level evaluations in objective and subjective manners, covering both answers and explanations, including answer correctness, explain correctness, explain completeness and explain redundancy.Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process.Thirdly, to avoid the influences of knowledge bias and concentrate purely on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content.Based on the in-depth evaluations, this paper finally forms a general evaluation scheme of logical reasoning capability from six dimensions (i.e., Correct, Rigorous, Self-aware, Active, Oriented and No hallucination).It reflects the pros and cons of LLMs and gives guiding directions for future works.</p>
<p>INTRODUCTION</p>
<p>A S a fundamental and significant topic in the domains of knowledge engineering and artificial intelligence, logical reasoning has consistently remained a subject of intense research interest [1], [2], [3].Through the integration of logical reasoning, a wide range of intelligent applications can be developed (e.g., recommendation systems [4], relation prediction [5] and question generation [6]), which not only offer powerful capabilities but also ensure natural interpretability.However, developing efficient and robust logical reasoning systems [7] remains a challenging academic pursuit due to complexities in handling intricate semantic and syntactic structures, managing symbolic knowledge, executing high-level abstractions and inferences, and navigating uncertainty and ambiguity [8], [9].</p>
<p>With the aid of large-scale pre-training, instruction finetuning, and human feedback reinforcement learning strategies [10], Large Language Models (LLMs) have made unpar-</p>
<p>• Fangzhi Xu, Qika Lin, Jiawei Han and Tianzhe Zhao are with the School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi 710049, China (email: {Leo981106, tara1208260223}@stu.xjtu.edu.cn,{qikalin, ztz8758}@foxmail.com).• Jun Liu is with the Shaanxi Provincial Key Laboratory of Big Data Knowledge Engineering, and National Engineering Lab for Big Data Analytics, Xi'an, Shaanxi 710049, China (e-mail: liukeen@xjtu.edu.cn).• Erik Cambria is with the School of Computer Science and Engineering, Nanyang Technological University (e-mail: cambria@ntu.edu.sg).• Fangzhi Xu and Qika Lin contribute equally to this paper.</p>
<p>• Corresponding author: Qika Lin.alleled strides in the artificial intelligence community, particularly in the field of natural language processing (NLP).They have achieved remarkable performance in numerous traditional NLP tasks [11], [12], such as question answering, information retrieval, and affective computing.Under this circumstance, researchers have begun to question the efficacy of LLMs in addressing complex logical reasoning tasks [13], [14]and their ability to apply this knowledge to downstream intelligent applications.As such, a natural question arises: are large language models really good logical reasoners?</p>
<p>There are already several studies to evaluate the capability of LLMs from various reasoning perspectives, e.g., multilingual reasoning [15], commonsense reasoning [16], and mathematical reasoning [17].These efforts are meaningful and inspiring.Nevertheless, all of them are confronted with one or more of the following defects: (1) There are no comprehensive evaluations, which are hampered by the issue of lacking systematic category, limited LLMs for comparison, and limited data samples for evaluation; (2) The majority of evaluation works purely report the accuracy of answers, which limits fine-level analysis on the reasoning process and fails to explore the causes of mistakes; (3) Current logical reasoning benchmarks may fail to purely evaluate logical reasoning ability since the reasoning of LLMs can be affected by the content; (4) There lack a complete evaluation system or well-defined dimensions to comprehensively conclude the logical reasoning capability of LLMs.Therefore, our work aims to fill these gaps and provide a comprehensive evaluation, where the overall architecture encompasses the five main points as illustrated in Fig. 1: category with reasoning settings, main metrics for evaluation, perspectives of error analysis, dimensions for measuring logical reasoning capability, and new benchmark NeuLR with neutral content.In detail, we address the above limitations from the following aspects.</p>
<p>Firstly, our work starts from systematic views and provides comprehensive evaluations.According to the classical definition [18], logical reasoning can be mainly categorized into three fundamental types, i.e., deductive, inductive and abductive reasoning.They together form a complete chain of reasoning, thus it is meaningful to evaluate LLMs from these views.Based on it, our comprehensive evaluation is reflected in three orthogonal dimensions.For the reasoning type view, all the evaluated datasets are categorized into four reasoning settings, i.e., deductive, inductive, abductive and mixed-forms.The former three involve the independent reasoning manner.For the dataset view, we include fifteen typical logical reasoning datasets according to the above categories.For the model view, we evaluate three previous LLMs (i.e., text-davinci-003 [19], ChatGPT [20] and BARD [21]) as well as four up-to-date LLMs (i.e., LLaMA3.1-Chat 1 , Mistral-Instruct-v0.3 2 , Claude-3 3 and GPT-4 4 ).</p>
<p>Secondly, our work fills the blank in fine-level evaluations of logical reasoning tasks.Current benchmarks only rely on a few objective metrics (e.g., accuracy) to measure the model capability.It may not be sufficient in the case of generative LLMs, since the role of LLMs is not only limited to correctly answer questions but also serves as practical tools, which are required to provide reasoning chains or explanations.Previous works [22], [23] conduct extensive experiments on popular NLP datasets, but they purely report the performance results.Since some LLMs (e.g., Chat-GPT) function as the interactive tools for human use, it is necessary to introduce subjective metrics to do fine-grained evaluations.In this paper, we employ four dimensions of metrics, covering answer correctness, explanation correctness, 1. https://llama.meta.com/ 2. https://huggingface.co/mistralai/Mistral-7B-Instruct-v0. 3 3. https://claude.ai/4. https://openai.com/index/gpt-4/explanation completeness and explanation redundancy.It can provide more meaningful and complete evaluations from both objective and subjective views.Considering problematic cases (i.e., wrong answer or wrong explanation) can reflect obvious logical flaws of LLMs, we further attribute them to several error types from two dimensions of evidence selection process and reasoning process and give in-depth analysis.Notably, the whole evaluation system can be applied under both human-annotation and automatic modes.</p>
<p>Thirdly, our work focuses on the issue of content neutrality and provides new solutions.The current benchmark for evaluating logical reasoning ability is strongly coupled with text comprehension; in other words, the rule reasoning process of LLMs may be affected by the content in the inputs, which limits the test of real logical reasoning ability.Also, LLMs are highly powerful due to their massive training data, which may overlap with popular benchmarks.As a result, testing LLMs on these benchmarks may not be entirely fair, as it can only demonstrate the fitting ability of LLMs rather than their real logical reasoning capability.Therefore, language models may be trained to learn a biased pattern from text, rather than really capture the logical reasoning capability.Some previous works [24], [25] propose to establish complete benchmarks for LLMs.But few works focus on logical reasoning and fail to attend to the contentneutral problem.To narrow this gap, we propose a new dataset named NeuLR, which contains 3,000 content-neutral samples and covers the deductive, inductive and abductive reasoning types.It is expected to offer a novel perspective for benchmarking the logical reasoning ability of LLMs.</p>
<p>Finally, we conclude the extensive performance results of LLMs and form an evaluation scheme with six key properties, i.e., Correct, Rigorous, Self-aware, Active, Oriented and No Hallucination.Among them, Correct purely measures the accuracy of the answer.Rigorous measures whether LLMs give both correct answers and complete and correct explanations.Self-aware is reflected by the redundancy of the generated content.Active is measured by the proportion of reasoning.Oriented illustrates whether LLMs can reason from the right perspectives.No hallucination measures whether LLMs are more prone to produce hallucinations.The above dimensions can all be quantified from existing evaluation experiments.For deductive, inductive, abductive, and mixed reasoning settings respectively, we visualize the ability maps for each LLM.It is meaningful to identify the strengths and weaknesses of LLMs under the four reasoning settings, thus guiding future directions.</p>
<p>The main contributions of the paper are listed as follows:</p>
<p>(1) In view of the great success of LLMs in massive NLP tasks, our work is targeted at answering are LLMs really good logical reasoners?.In this paper, we provide a comprehensive evaluation and give potential directions for future researches.</p>
<p>(2) For a comprehensive evaluation of logical reasoning, this paper classifies datasets into four reasoning manners, i.e., deductive, inductive, abductive, and mixed-form.We include fifteen typical logical reasoning datasets and evaluate seven representative LLMs under diverse prompting strategies, including 3 early-era LLMs (text-davinci-003, ChatGPT, and BARD) and 4 trending LLMs (LLaMA3.1-Chat,Mistral-Instruct-v0.3, Claude-3 and GPT-4).</p>
<p>(3) Considering the drawbacks in current objective metrics, this paper gives fine-level evaluations including four dimensions i.e., answer correctness, explanation correctness, explanation completeness, and explanation redundancy.To explore the value of failure cases, we attribute them to several error types and explore the logical flaws of LLMs.</p>
<p>(4) To provide fair evaluations with neutral content and decouple logical reasoning from text understanding, this paper proposes a new dataset named NeuLR 5 .It contains 3,000 content-neutral samples and covers deductive, inductive and abductive reasoning manners.</p>
<p>(5) In view of the evaluation results, this paper forms a general evaluation scheme for the logical reasoning capability of LLMs for the first time, which concludes six key properties, i.e, Correct, Rigorous, Self-aware, Active, Oriented and No hallucination.Furthermore, we derive the ability maps for each LLM under four reasoning settings respectively and propose future directions.</p>
<p>PRELIMINARY</p>
<p>Logical reasoning aims to generate logical implications that contain new facts using one-or multi-step inference based on given premises [26], [27], [28], i.e., premise⇒conclusion.Elements of logical reasoning typically include knowledge facts or premises and logical rules, for example:</p>
<p>• rule: All eight-year-old children are in primary school.</p>
<p>• fact1/premise1: Jordan is a child of eight years old.</p>
<p>• fact2/premise2: Jordan is in primary school.</p>
<p>According to the reasoning classification system of classical logic [18], there are three major types of logical reasoning: deductive, inductive and abductive.Drawing upon the aforementioned rule and facts, we can depict the objective of these three types of reasoning as forecasting the third item based on the two provided ones.</p>
<p>Deductive Reasoning.Deductive reasoning is the psychological process of drawing deductive inferences that start from the given premises and reason with logical rules or commonsense to obtain certain conclusions [29], [30].It can be premise1+rule⇒premise2.Fig. 2 presents an example of deductive reasoning.Its reasoning progress generates specific knowledge facts from general counterparts, e.g., premise2 and rule are specific and general knowledge, respectively.Therefore, deductive reasoning is actually a top-down way.</p>
<p>Inductive Reasoning.Distinct from deductive reasoning, inductive reasoning derives general principles from a body of observations which means making broad generalizations based on specific observations [31], [32].For example, an example of inductive reasoning can be premise1+premise2⇒rule, concluding generalized knowledge rule that is independent with specific item Jordan.Generally, the truth of the conclusion of an inductive argument is probable rather than certain in inductive reasoning.Thus, inductive reasoning is a bottom-up approach and is contrasted with deductive reasoning.</p>
<p>Abductive Reasoning.Formally, abductive reasoning is similar to deductive reasoning which seeks conclusions from a set of observations.But differently, its target is to 5. Available at https://github.com/DeepReasoning/NeuLRFig. 2: An example of using an LLM to answer reasoning questions.The red words represent the generated results, while the blue ones represent the generated explanations.</p>
<p>generate the simplest and most likely explanation for the given observations [33], [34].So the result is probable like in inductive reasoning.An example of abductive reasoning can be premise2+rule⇒premise1, which means premise1 is the most likely cause of premise2.</p>
<p>Mixed-form Reasoning.Numerous real-life reasoning scenarios may require multiple steps and the incorporation of at least two of these three types.In this paper, they are vieweds as a more intricate type of reasoning, namely, mixed-form.For example, given the following fact: 1) James is a crow and is black; 2) Sophia is a crow and is black; 3) Emily is a crow.To answer the question what color is Emily?, we should first get the conclusion that all crows are black using inductive reasoning and then conduct deductive reasoning on this conclusion and fact 3) to obtain the final answer black.</p>
<p>EVALUATION DETAILS</p>
<p>This section presents the comprehensive experimental settings, including the models, datasets and metrics.More detailed prompts for zero-or few-shot are shown in Table 3 of the Appendix 6 .</p>
<p>Evaluated Models</p>
<p>Because of the rapid emergence of LLMs, it is not realistic to include all LLMs in this paper.Thus, we mainly select three early-era representative LLMs (i.e., text-davinci-003, ChatGPT, and BARD) and four up-to-date LLMs (i.e., LLaMA3.1-Chat,Mistral-Instruct-v0.3, Claude-3 and GPT-4).The details of these models are listed in the Appendix.</p>
<p>Evaluated Datasets</p>
<p>According to the previous discussion, the evaluation is conducted systematically from deductive, inductive, abductive and mixed-form views.Therefore, this paper selects fifteen popular datasets in logical reasoning and divides them into the above four folds.Table 2 in the Appendix presents detailed information of these datasets.The selected datasets contain both generation and classification ones 6. Appendix is at: https://github.com/DeepReasoning/NeuLRand there exist diverse forms of tasks, which illustrate the comprehensiveness of our evaluation.Diverging from prior works that only employ a limited number of samples, this paper significantly expands the evaluation size.Since ChatGPT is one of the most popular LLM for the public, we give much focus on it.For parts of the datasets, we keep all the test examples for ChatGPT evaluation (i.e., EntailmentBank [35], FOLIO [36], Leap-Of-Thought [37], CLUTRR [38], ReClor [39], LogiQA [40], LogiQA 2.0 [41] and LogiQA2NLI [41]), while other large datasets (i.e., bAbI-15 [42], RuleTaker [43], bAbI-16 [42], α-NLI [44], α-NLG [44], AbductiveRules [45] and D*-Ab [46]) are sampled to 1,000 examples.As for the evaluation of text-davinci-003 and BARD, we sample 100 test examples for each dataset.</p>
<p>Selected Metrics</p>
<p>The majority of prior evaluation studies solely report the accuracy metric, which may not be comprehensive enough for evaluating the effectiveness of LLMs.Consequently, we assert that a more nuanced assessment is necessary.To this end, we propose to evaluate LLMs from both objective and subjective perspectives.Specifically, we introduce four evaluation metrics to reflect the intermediate reasoning process of LLMs: answer correctness, explanation correctness, explanation completeness and explanation redundancy.</p>
<p>• Answer Correctness.This metric measures the alignment between a generated answer and the true label, emphasizing semantic coherence over exact token matching.For example, "the car hit the tree" and "the tree was struck by the car" demonstrate this alignment despite differing expression style.</p>
<p>• Explanation Correctness.To reason towards the answer, explanations usually provide a logical and coherent narrative that connects the available evidence or premises to the ultimate conclusion.Thus this metric indicates whether the generated explanation is logically correct for the true answer.</p>
<p>• Explanation Completeness.It indicates whether there are any missing explanations to reason towards the true answer.An explanation is deemed complete if it encompasses all elements present in the true explanation, which means that in the reasoning process, the correct answer can be inferred through the selected known facts and the generated intermediate facts.But this does not necessarily cause answer correctness or explanation correctness.</p>
<p>•</p>
<p>Explanation Redundancy.This suggests that the combination of selected established facts and additional intermediate facts surpasses the necessary information for arriving at a precise conclusion, leading to the inclusion of unnecessary details in the reasoning process.For instance, only three specific facts are essential for drawing a conclusion; any extra facts beyond this set would be redundant.</p>
<p>Notably, the metric values of explanation correctness, completeness, and redundancy are independent.To identify prevalent logical flaws in LLMs, we establish error types to categorize problematic cases.This paper classifies errors along two primary dimensions: (1) evidence selection process and (2) reasoning process.The first dimension centers on evaluating the evidence selected by LLMs, whereas the second dimension emphasizes the logical reasoning process using the selected evidence.Detailedly, evidence selection process category can be further divided into wrong selection and hallucination.The former denotes that LLMs select the wrong facts or ignore the necessary facts from the beginning of the reasoning.The latter denotes that LLMs select the evidence which contradicts the given context or can not be verified by the context.Reasoning process category can be further divided into no reasoning, perspective mistake and process mistake.The first error type signifies instances where LLMs fail to conduct reasoning, instead merely listing the given facts and the final answer.The second denotes LLMs starting from an irrelevant point or focusing on an improper perspective for the correct answer.The last refers to LLMs commencing from a proper viewpoint, but making mistakes during the reasoning process.</p>
<p>OVERALL EXPERIMENTS</p>
<p>In this section, we conduct evaluation experiments on three early-era LLMs (i.e., text-davinci-003, ChatGPT and BARD) and four trending LLMs (i.e., LLaMA3.1-Chat,Mistral-Instruct-v0.3, Claude-3 and GPT-4) under various prompting methods.Table 1 presents the overall answer correctness of these three early-stage LLMs on fifteen logical reasoning datasets.Table 2 supplements the performances of four popular LLMs.Generally, LLMs' performances on logical reasoning tasks still have significant room for improvement in comparison to the state-of-the-art (SOTA) metric.Most of the results fall short of those achieved by smaller-sized SOTA models.We provide a detailed analysis of the results from the following perspectives.For simplicity, we only include text-davinci-003, ChatGPT and BARD for discussion.</p>
<p>Firstly, we conduct an analysis of LLMs' performances across four reasoning manners, solely focusing on the zeroshot results to facilitate a clear comparison.Furthermore, we introduce the relative performance metric (i.e., LLM accuracy/SOTA) to reflect the relative capability of LLMs in comparison to SOTA performances in Fig. 3.We also calculate the weighted results of four reasoning manners in Fig. 4a.From the results, ChatGPT performs worse in deductive and inductive settings compared with text-davinci-003 and BARD.In the abductive setting, three LLMs show comparable performances and BARD wins with slight advantages.In the mixed-form setting, ChatGPT performs better and BARD ranks second.Overall, BARD shows consistent superiority among deductive, inductive and abductive settings, while text-davinci-003 also does relatively well.It seems that ChatGPT struggles in the three settings, but is better at mixed-form reasoning.Also, we compare the LLM performances between deductive, inductive and abductive settings.LLMs do best in deductive setting, while they mostly struggle in inductive setting.We argue that deductive and abductive reasoning align with typical NLP scenarios, where LLMs have to provide missing facts.Conversely, inductive reasoning necessitates extracting high-level rules or knowledge from the given facts, which is more intricate and may not be readily available in the training corpus.TABLE 1: Overall results of LLMs' answer correctness across the zero-shot, one-shot and three-shot logical reasoning settings.The notations De., In., Ab. and Mix correspond to deductive, inductive, abductive and mixed-form reasoning, respectively (as in the following tables and figures).Gen. indicates whether the task is a generation one.The percentage signs (%) of performance values are omitted for simplicity in the paper.</p>
<p>Dataset</p>
<p>Gen. text-davinci-003 ChatGPT BARD SOTA 0-shot 1-shot 3-shot 0-shot 1-shot 3-shot 0-shot 1-shot  Secondly, we conduct a detailed analysis of LLMs' performances from the generation and classification perspectives in Fig. 4b.In general, classification scenarios tend to yield better performance than generation counterparts.ductiveRules and D*-Ab.This observation may result from the fact that ChatGPT is designed to improve chatting capability rather than complex reasoning, which can lead to performance degradation in pure generative logic reasoning scenarios.</p>
<p>Thirdly, few-shot in-context learning (ICL) [48] does not necessarily bring improvements in logical reasoning tasks.It is quite inconsistent with the cases in other non-reasoning NLP tasks, such as topic classification and sentiment analysis [49].We count the cases where LLMs can continuously obtain the performance gains from few-shot ICL (i.e., 0-shot &lt; 1-shot &lt; 3-shot).For text-davinci-003, only two (out of four) abductive datasets continuously benefit from the fewshot ICL.ChatGPT witnesses performance improvements only in one (out of five) deductive dataset and one (out of four) abductive dataset.For BARD, few-shot ICL helps two (out of five) deductive datasets and one (out of four) abductive datasets.Remarkably, few-shot ICL fails to provide consistent benefits for LLMs under inductive reasoning and mixed-form reasoning manners.We argue that inductive and mixed-form settings require more complex and high-order reasoning ability, which may be difficult to learn with a few question-answer samples without rationales.But the task form of deductive and abductive reasoning is easy to follow, which provides the application potential for fewshot ICL.</p>
<p>FINE-LEVEL EVALUATIONS</p>
<p>This section presents a detailed analysis of the logical reasoning capabilities of LLMs from various perspectives.</p>
<p>Are LLMs Rigorous Logical Reasoning?</p>
<p>While LLMs may produce correct answers in some cases, it is unclear whether they perform the correct logical reasoning or simply arrive at the right answer by chance.Therefore, we delve deeper into the reasoning process beyond the output answers.We view cases where LLMs provide a correct answer along with a correct and complete explanation as Rigorous.The detailed results are shown in Table 3.Compared with the simple judgment of answer correctness, all selected LLMs present obvious performance drops.For simplicity, we only take the zero-shot setting into consideration.In Fig. 5a, we calculate the ratio of rigorous performance and answer accuracy.The higher values (darker colors) mean better performance in rigorous reasoning.According to the results, BARD shows the best capability in rigorous reasoning, consistently under four reasoning manners.Meanwhile, ChatGPT still struggles in deductive and inductive settings, while text-davinci-003 comes last in both abductive and mixed-form manners.</p>
<p>Further, LLMs are best at keeping rigorous reasoning in the abductive setting, while they are weak in the deductive and inductive settings.The finding is a little different from the analysis of simple accuracy conditions in the previous section.We argue that the setting of abductive reasoning requires the LLMs to achieve the reasoning reversely, which can activate LLMs to provide sufficient reasoning process.In a deductive reasoning setting, the reasoning chain is sequential, which may cause LLMs to be in a lazy mode and harm rigorous reasoning.</p>
<p>In Table 3, we also include the two conditions (1) when correct answer and correct explanations are satisfied, and ( 2) when correct answer and complete explanations are satisfied.Results vary a lot with different datasets and different LLMs.Overall, ChatGPT performs relatively well in keeping correct explanations while it may fail to maintain complete explanations in most cases.In comparison, text-davinci-003 exhibits stronger characteristics in maintaining the completeness of explanations, compared with the correctness of explanations.These findings of the respective reasoning preferences are expected to guide the future utilization of LLMs.</p>
<p>Are LLMs Self-aware Logical Reasoners?</p>
<p>From an alternative perspective, the redundancy of the generated content by LLMs has been a frequently discussed topic, as it is deemed an important metric for assessing their practicality.In this paper, we consider LLMs with less redundant content as more self-aware, as they can effectively express the necessary information without outputting all possible answers.Table 4 presents the evaluation results of LLMs' self-awareness.Similar to our previous approach, we compute the weighted results for each reasoning setting and derive the self-awareness scores shown in Fig. 5b.The darker color indicates a stronger self-awareness capability.Results indicate that text-davinci-003 exhibits notable advantages, particularly in the inductive, abductive, and mixed-form reasoning settings.Additionally, it ranks second in the deductive setting.Conversely, BARD performs poorly in deductive, abductive, and mixed-form reasoning settings.</p>
<p>In comparison to classification tasks, LLMs tend to generate redundant answers more frequently in generation tasks, such as α-NLI vs. α-NLG.This is because openended questions can prompt LLMs to generate content from various perspectives, which can lead to the inclusion of redundant information.Furthermore, the mixed-form reasoning setting observes significantly fewer instances of redundancy.The tasks in mixed-form reasoning are primarily based on question answering, which closely resembles reallife text, and LLMs tend to generate rational and specific content in such scenarios.However, in other settings, the input context is elaborately designed for logical reasoning and may provide sufficient background information.This can result in LLMs employing embodied commonsense knowledge to help reason and thus generate additional explanations.Notably, considering that current LLMs do a lot of optimizations for preference alignment, it is inevitable TABLE 3: Evaluations on whether LLMs are rigorous reasoners.For each dataset, the first row of results represents the performances when LLMs give the correct answer, correct explanation as well as complete explanation simultaneously.The values in the subscripts denote the drops compared with only distinguishing the answer correctness.The second row of results represents the performances when LLMs give both correct answers and correct explanations, regardless of the explanation completeness.The third row represents the cases when LLMs give correct answers and list complete explanations, regardless of the explanation correctness.</p>
<p>Dataset</p>
<p>Do LLMs Have Obvious Logical Flaws?</p>
<p>Based on the preceding statements, we establish error types for problematic cases (with incorrect explanations) from two dimensions: evidence selection process and reasoning process.</p>
<p>The former dimension encompasses two error types: (i) wrong selection and (ii) hallucination, which are independent of each other.The latter dimension comprises three error types: (i) no reasoning, (ii) perspective mistake and (iii) process mistake.Each problematic case can only be attributed to one of the three errors in the reasoning process dimension.</p>
<p>We visualize the attribution results for fourteen datasets in Fig. 6, including four deductive, two inductive, four abductive and four mixed-form ones.Overall speaking, the types of errors vary between datasets.For the wrong selection, 33.26% of the problematic cases fail to select the right answers for reasoning.Also, 27.46% suffers from the hallucination issue of LLMs.From the dimension of reasoning process, no reasoning error keeps a small portion in most of the cases, only covering 19.33% of the selected cases in TABLE 4: Evaluation results on the metric of explanation redundancy.</p>
<p>Dataset</p>
<p>Gen. text-davinci-003 ChatGPT BARD 0-shot 1-shot 3-shot 0-shot 1-shot 3-shot 0-shot 1-shot The primary challenge evident from the results lies in the ability of LLMs to locate accurate evidence and perspectives for logical reasoning tasks.Due to the limited content of inputs, LLMs are also prone to generate hallucinatory facts to aid the reasoning process, which may affect the reliability of LLMs in real applications.In addition, LLMs abandon reasoning in a considerable number of cases.Such phenomena of laziness are worth noting, especially when we depend on LLMs to help reason in the downstream tasks.In the following section, we will provide a detailed analysis of specific LLM and specific reasoning settings.Considering the above-mentioned obvious logical flaws, we will especially focus on the detailed analysis of the activity in reasoning, the orientation selection of LLMs and model hallucination.</p>
<p>Are LLMs Active Logical Reasoners?</p>
<p>In this paper, we consider LLMs with fewer no reasoning errors as more Active logical reasoners.Fig. 15a presents the weighted results for measuring active reasoning cases, where higher values indicate LLMs are more active in reasoning, while lower values represent lazier cases.Among the three LLMs, BARD is the most active logical reasoner, excelling in deductive, inductive and abductive settings.ChatGPT, on the other hand, is deemed the lazier reasoner in deductive and inductive settings, while text-davinci-003 is lazier in abductive reasoning tasks.</p>
<p>Furthermore, we compare the performances of LLMs across different reasoning modes.In deductive reasoning tasks, LLMs exhibit more active reasoning, while in abductive settings, they tend to display lazier performances.Deductive tasks are in a forward reasoning mode, which is more natural for both generative LLMs and humans.This can inspire LLMs to generate effective reasoning chains.Conversely, abductive reasoning requires LLMs to provide explanations for the given inputs, which is in a backward reasoning mode.It is intuitive that LLMs may struggle to conduct reasoning in some cases.</p>
<p>Are LLMs Oriented Logical Reasoners?</p>
<p>At the outset of the reasoning process, it is crucial to identify the correct starting points and potential directions for reasoning.We consider LLMs with this capability as Oriented logical reasoners and present evaluation results based on the error type of perspective mistake in Fig. 15b.From the heatmap results, ChatGPT exhibits better oriented capability in deductive and inductive tasks compared to the other two LLMs.However, it frequently fails to identify the correct reasoning direction in abductive and mixed-form reasoning.Conversely, BARD performs well in identifying the right direction in abductive and mixed-form settings but struggles in deductive and inductive ones.Compared to the others, text-davinci-003 displays moderate performance in identifying reasoning perspectives.</p>
<p>In light of the preceding findings, ChatGPT is a lazier logical reasoner, but it excels at identifying the correct direction for reasoning.In other words, ChatGPT is prone to conduct confident reasoning.Conversely, text-davinci-003 and BARD are active in logical reasoning, but they tend to start from wrong directions, leading to reasoning mistakes.</p>
<p>The balance of active and oriented reasoning gives an insightful direction in exploring the knowledge and ability boundary of LLMs.This implies that LLMs must understand their capabilities and limitations, avoiding tasks beyond their scope.</p>
<p>Are LLMs Easy to Induce Hallucination in Logical Reasoning?</p>
<p>In the typical definition, hallucination refers to generated content that contradicts commonsense or current facts.To align with logical reasoning tasks, we expand the definition to include cases where facts are employed that contradict the context or are not verified by the context.Fig. 15c displays the weighted performances of cases with no hallucinations.The darker color indicates better performance in avoiding hallucinations.Based on the results, ChatGPT exhibits strong and consistent competitiveness, ranking first in deductive, inductive, and mixed-form settings.It also ranks    (m) LogiQA2.0 (Mixed-form).</p>
<p>(n) LogiQA2NLI (Mixed-form).second in abductive reasoning tasks, albeit with slight disadvantages.Conversely, BARD displays poor performance in avoiding hallucinations and maintaining clarity during reasoning.Across all four reasoning settings, BARD ranks last with significant gaps.On average, LLMs induce hallucination in 27.46% of the failure cases.LLM hallucinations are more prevalent in deductive reasoning tasks among the settings of deductive, inductive, and abductive reasoning, whereas LLMs may exhibit clearer judgment in the inductive setting.</p>
<p>How Does the Number of Statements Affect the LLMs' Performances?</p>
<p>In the following, we explore some of the key factors to affect the reasoning performances of LLMs.Since the length of the input context can be different for the datasets, we report the model performances with the number of statements in Fig. 8.We take ChatGPT for analysis and choose six datasets with specific counts of statements for illustration, covering the three reasoning manners.The first four subfigures are related to the deductive reasoning manner, i.e., Entailment-Bank, RuleTaker, FOLIO, and Leap-of-Thought.From the overall results, LLMs can keep the correctness of both the answer and explanations with fewer input statements.With the statement number increasing, the performances drop a lot and LLMs struggle to give the correct explanations.Interestingly, five (out of six) datasets witness performance gains when the number of statements reaches certain values.For example, in the RuleTaker dataset, the best performances are achieved when the number of statements is larger than 17.And when the number is between to 16, ChatGPT is capable of keeping stable performances.We argue that the larger number of statements can provide richer information and sometimes can help control the reasoning direction of LLMs.Furthermore, we investigate the impact of the number of tokens in the context.Fig. 9 illustrates the ChatGPT performances on twelve datasets.Results vary a lot across different datasets and different reasoning settings.In general, as the number of tokens increases, the performances of ChatGPT tend to decline.Detailedly, ChatGPT performs stably in deductive settings.Especially for EntailmentBank and Leap-of-Thought datasets, ChatGPT maintains relatively consistent accuracy with token numbers increasing.Considering that deductive setting is a more common form of reasoning in reality, LLMs are well-trained on it to tackle the various lengths of inputs.</p>
<p>How Does the Number of Reasoning Hops Affect the LLMs' Performances?</p>
<p>Also, it is interesting to explore the influences of reasoning hops for LLMs.Among the selected dataset, three of them offer the number of hops for each sample, which are EntailmentBank in deductive reasoning, CLUTRR in inductive reasoning and D*-Ab in abductive reasoning.Alongside the simple accuracy results, we also report the rigorous reasoning cases where both the answer and explanations are correct.</p>
<p>In the deductive setting, with the number of hops increasing, the performances witness obvious drops, particularly influencing the rigor of the LLM reasoning.It illustrates that reasoning hops have great effects on deductive reasoning.In inductive reasoning, when the hop number is greater than two, the performance of ChatGPT decreases sharply.When the number ranges from three to nine, the performance of ChatGPT keeps stable at a relatively low level.Combined with the weak performance of ChatGPT in inductive reasoning tasks, it demonstrates that ChatGPT can only work on simple induction, and it obviously struggles in cases when more hops are needed.In the abductive reasoning setting, the majority of the test samples only need one-hop reasoning.When the hop number increases, the performances of ChatGPT witness slight improvements.It shows that ChatGPT may have the potential capability of multi-hop reasoning in the abductive setting.</p>
<p>Does Chain-of-Thought or Program-of-Thought Help Logical Reasoning?</p>
<p>As a supplement to the original results, we add the comparison between chain-of-thought and direct prompting in Table 2.It is observed that chain-of-thought does not always lead to evident improvements Especially under abductive reasoning, COT tends to have adverse effects in the majority of instances.Given the distinct disparities between logical reasoning and fact-based reasoning, the model is required to engage in abstract reasoning.But COT actually works more for superficial reasoning over facts.</p>
<p>Recently, the program-of-thought (POT) prompting strategy [50], [51] stands out as another selection to improve the model performance.Since POT relies heavily on the</p>
<p>NEUTRAL-CONTENT LOGICAL REASONING</p>
<p>Considering the current benchmarks may not provide neutral content for fair evaluation, we propose the new dataset NeuLR to benchmark the neutral-content logical reasoning tasks.In column 1∼3 of TABLE 5, we provide the statistics of NeuLR.It contains 3k samples in total, with 1k for deductive reasoning, 1k for inductive reasoning and 1k for abductive reasoning.Limited by space, we provide the details of the construction of NeuLR in the Appendix.</p>
<p>To evaluate the performances of LLMs on NeuLR, we conduct the experiments shown in TABLE 5. Especially, we provide three different test settings, i.e., zero-shot, one-shot and chain-of-thought [54] settings.Details of the prompt forms are included in the Appendix.</p>
<p>Firstly, from the results, few-shot prompting and chainof-thought prompting can both boost the performances of LLMs in most cases.Overall, chain-of-thought helps most to the model accuracy.Especially for text-davinci-003, it witnesses consistent gains with the aid of few-shot prompting and chain-of-thought prompting strategies.</p>
<p>Secondly, among the zero-shot results of three LLMs, BARD achieves the best performances on NeuLR while ChatGPT ranks last.The differences of zero-shot settings are significant.However, with the help of few-shot and chainof-thought prompting strategies, text-davinci-003 large narrows the gaps with BARD, and it surpasses BARD with chain-of-thought strategy.Overall, the performances of LLMs on NeuLR still have great room for improvement.</p>
<p>Thirdly, from the perspective of different reasoning settings, there exist huge differences in results compared with the previous findings.Generally, the LLMs' performances in inductive reasoning are better than deductive reasoning and abductive reasoning.Especially, LLMs present obvious weakness in the abductive setting.While from fifteen classical datasets, the performances among the reasoning settings are sorted as deductive &gt; abductive &gt; inductive.Such findings can also motivate future studies.Content neutrality can enhance strategies for boosting inductive reasoning performance.</p>
<p>CONCLUSION</p>
<p>In this paper, in-depth evaluations are conducted on logical reasoning tasks, discussing whether LLMs are really good logical reasoners.First, the logical reasoning evaluations are organized from deductive, inductive, abductive and mixedform views.We select fifteen logical reasoning datasets to evaluate on three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) under both zero-shot and few-shot settings.Second, this paper provides fine-level evaluations on four metrics, covering both objective and subjective views.For problematic cases, extensive error attributions are conducted from two dimensions, forming five error types.It uncovers the logical flaws of LLMs and we provide deep analysis on the results.Third, to achieve a fair and pure benchmark for logical reasoning capability, we propose a dataset with neutral content, covering deductive, inductive and abductive settings.</p>
<p>Based on the evaluation results above, we abstract six dimensions to measure the logical reasoning capability of LLMs: (1) Correct, (2) Rigorous, (3) Self-aware, (4) Active, (5) Oriented and (6) No hallucination.All these properties can be calculated with the evaluation methods proposed in this paper.Therefore, we propose an evaluation scheme for the logical reasoning capability of LLMs.Considering the different performances of LLMs on deductive, inductive, abductive and mixed-form settings, we respectively visualize each ability map in Fig. 11.</p>
<p>According to the results, text-davinci-003 can maintain balanced performances in deductive and mixed-form settings.But it usually fails to keep oriented for reasoning in the inductive setting, and it also shows laziness in the abductive reasoning tasks.Since it is the earliest released LLM of the three, it is understandable that text-davinci-003 has some limitations in logical reasoning tasks, especially in the more complex settings (e.g., inductive and abductive).</p>
<p>From the perspective of common evaluations, ChatGPT is the weakest LLM of the three, since it performs badly in showing correct and rigorous reasoning under deductive, inductive and abductive settings.Also, it seems to be the laziest reasoner in deductive and inductive settings.However, it surprises us that it shows unique advantages in maintaining oriented reasoning and avoiding hallucination, especially in deductive and inductive settings.In addition, it shows its comprehensive capability in the mixed-form setting.We argue that ChatGPT is specially designed for chatting, thus it does pretty well in keeping rational but is not good at solving complex reasoning problems.BARD is the most active reasoner and it keeps great competitiveness as a correct and rigorous reasoner.However, it also shows obvious flaws compared with other LLMs.BARD tends to generate redundant content, easily fails to find the correct reasoning directions and it usually fails to avoid hallucinations.In short, BARD shows great advantages in current benchmarks with objective metrics, due to the larger model size and massive training data.But it still has much room for improvement in some implicit aspects, i.e., self-awareness, orientation and non-hallucination.</p>
<p>Overall, it can be observed that all LLMs exhibit specific limitations in logical reasoning, with relative strength in deductive reasoning but evident struggles in inductive settings.Moreover, current evaluation benchmarks, which primarily depend on objective metrics, are not sufficient to comprehensively evaluate LLMs.</p>
<p>Additionally, we supplement the visualization of four trending LLMs in Fig. 12. Different from Fig. 11, these metrics are calculated in the automatic mode.It also verifies the scalability of our evaluation system.Please refer to the Appendix for more analysis on these four LLMs and evaluation settings.</p>
<p>FUTURE DIRECTIONS</p>
<p>Based on the evaluation results, this paper concludes six future directions for logical reasoning tasks.</p>
<p>Strengthen the reasoning ability of inductive reasoning.</p>
<p>Inductive reasoning draws broad conclusions from specific observations, requiring a more abstract and comprehensive understanding of real-world knowledge compared to deductive or abductive reasoning.However, LLMs have shown poor performance in this area, as demonstrated in Section 4.1.Therefore, it is crucial to develop pre-training or fine-tuning strategies to enhance their inductive reasoning abilities.One such strategy could be constructing more inductive instructions to guide LLMs.</p>
<p>Enhance the LLM's perception of its capability boundaries.LLMs excel at providing answers and explanations for a wide range of reasoning questions, irrespective of complexity.However, as evidenced in Sections 5.1 and 5.4, they may introduce irrelevant information or fabrications, leading to nonsensical or illogical outcomes.A proficient logical reasoner should recognize its limitations and knows when to refrain from answering.Future investigations could explore human self-awareness in cognitive science and neuroscience to strengthen LLMs' understanding of their operational boundaries.</p>
<p>Strengthen the rigorous reasoning to apply to real-world scenarios.Table 3 illustrates that current LLMs are not sufficiently rigorous for deductive, inductive, and abductive reasoning.As a result, there exist significant gaps between their capabilities and potential applications in real-world scenarios, particularly those that require detailed intermediate explanations.For instance, using LLMs to solve mathematical problems and provide precise intelligent Q&amp;A services in the education field remains a significant challenge [55].</p>
<p>Minimize the occurrence of hallucinations.Similar to the behaviors in other problem-solving contexts [56], LLMs may generate false or irrelevant hallucinations during logical reasoning tasks.It suggests that LLMs may not fully comprehend the question and can not solve it correctly.To address this issue, future research should develop more comprehensive evaluation metrics for hallucinations and explore specific strategies to minimize their occurrence.Improve the multi-hop reasoning capability, especially in inductive and abductive settings.Combined with the results from Fig. 10 and Fig. 1 in the Appendix, the multihop reasoning capability of LLMs still have much room for improvement.In fact, humans are better at decomposing complex questions.It can be an interesting topic for LLMs to capture the ability to divide and conquer questions, thus benefiting multi-hop reasoning.Increase explainability.Finally, the explainability of LLMs will be essential for building trust, detecting and mitigating biases, improving performance, promoting user understanding, and complying with regulations.A commonsensebased neuro-symbolic AI framework, such as the one proposed by [57] for sentiment analysis, can help increase the explainability of the reasoning processes required for decision-making, which is crucial for sensitive applications involving ethics, privacy and health.</p>
<p>APPENDIX</p>
<p>The following sections firstly provide comprehensive information about the seven LLMs analyzed in this study.Additionally, this file includes a detailed description of the evaluation methodology, prompt engineering techniques, NeuLR dataset, and supplementary analyses.Finally, the case studies for each dataset are presented.Together, these sections provide a comprehensive and detailed account of the methods and results of the study.</p>
<p>APPENDIX A DETAILS OF LARGE LANGUAGE MODELS</p>
<p>In this study, we select three previously representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and four upto-date LLMs (i.e., LLaMA3.1-Chat,Mistral-Instruct-v0.3, Claude-3 and GPT-4) for evaluation and analysis.The specifics are itemized in Table 6, which includes the affiliation, usage charge, pre-trained data, and model size of each LLM.</p>
<p>APPENDIX B DETAILS OF EVALUATIONS</p>
<p>This section supplements some details of the evaluation.</p>
<p>B.1 Evaluated Datasets</p>
<p>Considering the huge annotation load, we make a balance between the comprehensiveness and annotation cost.We try to keep the full data of each dataset to evaluate ChatGPT, except for some large-scale datasets, i.e., RuleTaker, α-NLI, α-NLG, AbductiveRules, D*-Ab and LogiQA2NLI.We limit the number of evaluation samples in these datasets to 1000, 1000, 1000, 1000, 1000 and 600 respectively.For text-davinci-003 and BARD, we randomly sample 100 examples for each dataset.It is still more comprehensive than previous works, that only utilize dozens of samples for testing (e.g., 30 samples for each dataset).Notably, as for the supplemented experiments on the four latest LLMs (LLaMA3.1-Chat,Mistral-Instruct-v0.3, Claude-3 and GPT-4), we randomly sample 100 cases for each dataset.It is expected to maintain a fair comparison.</p>
<p>For the mixed-form reasoning datasets, we calculate the fact length distributions of four datasets for reference.It is observed that most samples have 1-2 or 3-4 facts for reasoning.</p>
<p>B.2 Evaluation Modes</p>
<p>As mentioned in the main paper, we introduce four metrics to evaluate the LLM performances, i.e., Answer Correctness, Explain Correctness, Explain Completeness and Explain Redundancy.Also, we employ five metrics to attribute the error cases.Although these metrics are subjective, their standards are clear and well-defined.</p>
<p>As for the three previously representative LLMs, their evaluations are conducted through manual annotation.In detail, we output the LLM answers through API calling and hire ten graduate students (all majoring in Natural Language Processing) to annotate the answers based on our metric definitions.</p>
<p>As for the four trending LLMs, their evaluations are under the automatic mode.Specifically, we utilize GPT-4o model to act as the LLM judge.The definitions of the metrics are fed to the LLM judge as instructions, and it outputs the evaluation scores the same as the human annotators.</p>
<p>APPENDIX C SUPPLEMENTARY RESULTS ON LATEST LLMS</p>
<p>First of all, we conduct the pilot experiments to demonstrate the consistency between the manual labeling and GPT-4o.The analysis is conducted under the four evaluation metrics (i.e., answer correctness, explanation correctness, explanation completeness, and explanation redundancy) and the two error categories (i.e., evidence selection and reasoning errors).Fig. 13  Fig.13: The evaluation matches between LLM judge and human annotations on the 300 samples.</p>
<p>It illustrates that LLM-as-judge can well align with human performance in our evaluation settings.</p>
<p>De.</p>
<p>In  Next, we implement the automatic evaluations on the four trending LLMs (i.e., LLaMA3.1-Chat-8B,Mistral-Instruct-v0.2-7B, Claude-3, and GPT-4).Fig. 14 and 15 provide the respective results on the evaluation of six dimensions.</p>
<p>APPENDIX D PROMPT ENGINEERING</p>
<p>In the implementation, we consider both the zero-shot and few-shot settings.To obtain appropriate model outputs, we construct the prompts, shown in Table 10 of Appendix.</p>
<p>APPENDIX E NEULR DATASET</p>
<p>In this section, we will provide more details on how NeuLR is constructed and what is the form of the prompt.</p>
<p>E.1 Construction of NeuLR</p>
<p>According to the explanations in the main paper, current logical reasoning may fail to keep the neutral content for evaluation.Therefore, we propose the new dataset NeuLR to benchmark the pure logical reasoning capability of LLMs.The data is sourced from [42], [46].Detailedly, we transform some of the important content (e.g., entities and properties) into random strings.In NeuLR, we neutralize three types of words, i.e., name, species and property.We replace each type of words with the concatenation of a prefix (i.e., NP, SP, ADP) and six random characters (combination of uppercase and lowercase letters and numbers).Table 9 illustrates examples of the replacement process.</p>
<p>E.2 Prompt Forms</p>
<p>Different from previous evaluations, we consider adding descriptions of content neutrality in the prompt forms.Specifically, we replace the previous task description prefix with the following form (take abductive reasoning for illustration):</p>
<p>• This is a neutral-content abductive reasoning task.The strings starting with SP represent species.The strings starting with NP represent names.The strings starting with ADP represent property.Given a context and a fact, it is required to generate a short missing fact.</p>
<p>Other parts of the prompts are the same as the previous form.Therefore, the answer is: SPArMPn0.</p>
<p>-Inductive Reasoning [chain-of-thought]: There is one example of inductive reasoning: The facts are: NP16WBQQ is a SPQFyx7i.NPerfGLN is a SP4Vge77.NP0V3buK is a SPGbXv1C.NPU9kOFg is a SPQFyx7i.NPDHuODI is a SP4Vge77.NPerfGLN is ADPHQuIP9.NP0V3buK is ADPmBU2ts.NP16WBQQ is ADP0rI59W.NPDHuODI is ADPHQuIP9.The question is: What property is NPU9kOFg?Because NPU9kOFg is a SPQFyx7i, NP16WBQQ is a SPQFyx7i and NP16WBQQ is ADP0rI59W.Therefore, the answer is: ADP0rI59W.</p>
<p>-Abductive Reasoning</p>
<p>[chain-of-thought]: ... Based on rule2 and tripleM, we can derive a new fact.Combine it with triple4.Based on rule1, we can derive a new fact.Combine it with rule1.In this way, we can finally derive the given fact...</p>
<p>APPENDIX F SUPPLEMENTARY ANALYSIS</p>
<p>We also provide the performances of text-davinci-003 and BARD with different number of hops in Figure 16.In the deductive setting of text-davinci-003, model performance drops with the number of hops increasing.But when the hop number is over five, it witnesses slight gains in performance, which illustrates that text-davinci-003 has the potential to conduct multi-hop reasoning in the deductive reasoning setting.However, in the inductive and abductive settings, the performances of text-davinci-003 decrease sharply when the number of hops increases.Especially, it fails all the cases when the hop number is over six in inductive reasoning, and also when the hop number is greater than one in abductive reasoning.It is inferior to ChatGPT.</p>
<p>For BARD, the situation is quite different.In deductive reasoning, the performance of BARD increases with the hop number adds.Especially, when the hop number is over five, the accuracy reaches 100% without reducing the rigor of reasoning.In inductive reasoning, the performance of BARD also drops at first, but it keeps stable and witnesses obvious gains when the hop number is over six.It demonstrates that BARD is better at conducting inductive reasoning and processing multi-hop scenarios compared with text-davinci-003 and ChatGPT.In abductive reasoning, BARD struggles a lot, inferior to ChatGPT but is better than text-davinci-003.</p>
<p>In all, in the face of complex multi-hop scenarios, LLMs still have much room for improvement.From the results, they do relatively well in deductive reasoning settings.But they are far from good in the inductive and abductive settings, which can also inspire future research on it.</p>
<p>APPENDIX G CASE STUDIES</p>
<p>We show one reasoning case for each dataset in Table 11-25, where the context and question as well as output of 0-shot ChatGPT, 1-shot ChatGPT, 3-shot ChatGPT, 0-shot Davinci-003 and 0-shot BARD are displayed.We also provided the annotated information about the answer correctness: , explain correctness: , explain completeness: , explain redundancy: , evidence wrong selection: , hallucination:</p>
<p>, no reasoning: , perspective mistake: , and process mistake: .(the last five indicators are annotated when the explain explanation is false).</p>
<p>Question, answer and explanation</p>
<p>Context: 1 pollination is when pollinating animals / wind / water carry pollen from one flower to another flower.2 if something is a part of a process then that something is required for that process.3 helping something has a positive impact on that something.4 if something is required for a process then that something positively impacts that process.5 an insect is a pollinating animal.Question: life cycle of some plants includes the process of pollination.Pollination is helped by? Answer: wind, water, and insects Explanation: sent1 &amp; sent5→int1: pollination is when insects / wind / water carry pollen from one flower to another; int1 &amp; sent2→int2: insects / wind / water carrying pollen is required for pollination; int2 &amp; sent4→int3: insects / wind / water have a positive impact on pollination by carrying pollen; int3 &amp; sent3→hypothesis ChatGPT of 0-shot Pollination in the life cycle of some plants is helped by pollinating animals, such as insects, which are required for the process of pollination and have a positive impact on it.Answer and Explain:</p>
<p>Explain Error Type:</p>
<p>Question, answer and explanation</p>
<p>Context: 1 If the current runs through the bell then the bell is ringing. 2 The circuit includes the battery.3 If the circuit does not include the switch then the circuit is complete.4 The circuit includes the bell.5 If the circuit includes the switch and the switch is on then the circuit is complete.6 If the current runs through the light bulb then the light bulb is glowing.7 If the wire is plastic then the wire is not conducting.8 The circuit includes the switch.9 If the circuit includes the battery and the battery is not flat then the circuit is powered.10 If the current runs through the circuit and the circuit includes the bell then the current runs through the bell.11 The wire is metal.12 If the wire is metal then the wire is conducting.13 If the current runs through the circuit and the circuit includes the light bulb then the current runs through the light bulb.14 If the current runs through the radio then the radio is playing.15 If the current runs through the circuit and the circuit includes the radio then the current runs through the radio.16 If the circuit is powered and the circuit is complete and the wire is conducting then the current runs through the circuit.</p>
<p>Statement: The current does not run through the circuit.Question: Determine whether the statement is true using deductive reasoning and give explanations.Answer: True ChatGPT of 0-shot The circuit is powered, complete, and the wire is conducting.Therefore, we can use deductive reasoning to determine that the statement "The current does not run through the circuit" is false based on the given facts.Answer and Explain:</p>
<p>Explain Error Type:</p>
<p>ChatGPT of 1-shot If the battery is not flat and the circuit includes the battery, then the circuit is powered: "If the circuit includes the battery and the battery is not flat then the circuit is powered."If the circuit is powered and the wire is metal (which is conducting), then the current runs through the circuit: "If the circuit is powered and the circuit is complete and the wire is conducting then the current runs through the circuit."However, it is stated that the wire is plastic and if the wire is plastic, then it is not conducting: "If the wire is plastic then the wire is not conducting."Therefore, the statement "The current does not run through the circuit" is true.Answer and Explain: Explain Error Type: ChatGPT of 3-shot This statement is false based on the given facts.If the circuit is powered, complete, and the wire is conducting, then the current runs through the circuit, as stated in the first premise.Hence, we can't conclude that the current doesn't run through the circuit.Answer and Explain:</p>
<p>Explain Error Type: Davinci-003 of 0-shot Pollination is helped by pollinating animals, such as insects, carrying pollen from one flower to another.Answer and Explain:</p>
<p>Explain Error Type: Bard of 0-shot Sure, here is the answer to the question in one sentence using deductive reasoning: Pollination is helped by pollinating animals, such as insects.</p>
<p>Here is the simple explanation: Pollination is the process of transferring pollen from the male part of a flower to the female part of a flower.</p>
<p>Pollinating animals, such as insects, help pollination by carrying pollen from one flower to another.Without pollination, plants cannot reproduce and produce seeds.Therefore, pollinating animals help the life cycle of some plants by helping them to reproduce.Answer and Explain: Explain Error Type:  A mature company's stock cannot also be a growth company's stock since these are two distinct categories.The given facts say that some companies' stocks are growth companies' stocks and all mature companies' stocks are suitable for a retirement fund.Additionally, KO is specifically identified as a mature company's stock.Therefore, the statement "KO is a company stock and a growth company's stock" is false.Answer and Explain:</p>
<p>Explain Error Type: ChatGPT of 1-shot We know from the given facts that KO is "a mature company's stock," but there is no information to suggest that it is also a growth company's stock.Therefore, the statement "KO is a company stock and a growth company's stock" is uncertain based on the given facts.Answer and Explain:</p>
<p>Explain Error Type: ChatGPT of 3-shot</p>
<p>The given facts state that KO is a mature company's stock.However, it is not stated whether KO is also a growth company's stock.Therefore, the statement "KO is a company stock and a growth company's stock" is uncertain.Answer and Explain:</p>
<p>Explain Error Type:</p>
<p>Davinci-003 of 0-shot False.While it is true that KO is a company stock, it is not true that it is a growth company's stock.Growth companies' stocks are volatile, whereas the fact given states that mature companies' stocks are suitable for retirement funds.Since KO is a mature company's stock, it follows that it is not a growth company's stock.Answer and Explain: Explain Error Type:
Bard of 0-shot
The statement "KO is a company stock and a growth company's stock" is false.Explanation: 1.All growth companies' stocks are volatile.This means that the stock price of a growth company is likely to fluctuate more than the stock price of a mature company.</p>
<ol>
<li>
<p>If the stock price is volatile, then it is not suitable for a retirement fund.This is because retirement funds are designed to provide long-term growth, and volatile stocks are more likely to lose value over time.</p>
</li>
<li>
<p>Some companies' stocks are growth companies' stocks.This means that some companies are growing at a faster rate than others.4. All mature companies' stocks are suitable for a retirement fund.This is because mature companies are less likely to experience sudden changes in their stock price.5. KO is a mature company's stock.This means that KO is a company that is not growing at a rapid pace.Based on the above facts, we can conclude that KO is not a growth company's stock.This is because KO is a mature company, and mature companies' stocks are not volatile.Therefore, the statement "KO is a company stock and a growth company's stock" is false.Answer and Explain:  The statement "A jazz is capable of expressing feelings" can be inferred with a certain level of probability based on the given facts.The fact that "A music is capable of expressing feelings" suggests that jazz, which is a genre of music, is likely to be capable of expressing feelings as well.The fact that "A bird is not capable of expressing feelings" and "A quack-quack is not capable of suffering dehydration" are logically irrelevant to the statement about whether jazz is capable of expressing feelings.The fact that "A privet is capable of suffering dehydration" is also irrelevant, as it does not provide any information or indication one way or the other about the statement.Therefore, based on the given facts, the statement about jazz being capable of expressing feelings can be inferred with a certain level of probability and is likely true.Answer and Explain: ChatGPT of 1-shot The statement "A jazz is capable of express feelings" is true using deductive reasoning based on the given facts.Answer and Explain:</p>
</li>
</ol>
<p>Explain Error Type:</p>
<p>ChatGPT of 3-shot This is true, as the given fact "a music is capable of express feelings" includes the subcategory of music that is jazz.Therefore, we can logically deduce that jazz music is also capable of expressing feelings.Answer and Explain:</p>
<p>Davinci-003 of 0-shot True.Although humans usually associate the sound of a quack-quack with birds, the quack-quack mentioned here is not a bird and is not capable of expressing feelings.Additionally, a privet is capable of suffering dehydration but not expressing feelings.Therefore, the only species that has been mentioned in the facts that is capable of expressing feelings is the music.Although a jazz is not explicitly mentioned in the facts, it would also be categorically considered to be a type of music, so it can logically be assumed that a jazz is capable of expressing feelings.</p>
<p>Answer and Explain:</p>
<p>Bard of 0-shot</p>
<p>The statement "A jazz is capable of express feelings" is not true.The fact that a music is capable of expressing feelings does not mean that a jazz is capable of expressing feelings.A jazz is a type of music, but it is not the only type of music.There are many other types of music, and not all of them are capable of expressing feelings.For example, classical music is often seen as being more formal and less emotional than jazz.Therefore, it is not possible to say that all music is capable of expressing feelings, simply because one type of music is capable of doing so.</p>
<p>In order to determine whether a jazz is capable of expressing feelings, we would need more information about the specific jazz in question.We would need to know what the jazz is about, who wrote it, and how it was performed.Once we have this information, we can then use deductive reasoning to determine whether the jazz is capable of expressing feelings.For example, if the jazz is about a personal experience that the author had, then it is more likely that the jazz will be able to express feelings.This is because the author will have put their own emotions into the jazz, and this will be reflected in the music.However, if the jazz is about a more general topic, such as the beauty of nature, then it is less likely that the jazz will be able to express feelings.This is because the author will not have put their own emotions into the jazz, and the music will therefore be more objective.Ultimately, whether or not a jazz is capable of expressing feelings is a matter of opinion.There is no scientific way to determine whether a piece of music is capable of expressing feelings.However, by using deductive reasoning, we can gain a better understanding of the potential for a jazz to express feelings.Answer and Explain: Explain Error Type: The more likely explanation is hypothesis choice A: I got injured the last day during a training event.This is because the context mentions the narrator enlisting with the USMC and being sent home after getting back from the hospital, suggesting a military-related scenario.Hypothesis choice A of the narrator getting injured during a training event aligns with these details and provides a probable explanation for why they may have been sent home, as injuries sustained during training may prevent soldiers from fulfilling duties and may require medical attention.Hypothesis choice B of the narrator being shot in action doesn't fit with the context, as there is no indication of any mention of being in combat or any explanation for how this would have been related to the narrator's hospitalization or being sent home, and it doesn't provide an explanation for what may have motivated the narrator's military-related interests or preferences.Answer and Explain: Explain Error Type:</p>
<p>ChatGPT of 1-shot</p>
<p>The correct choice is A. The context suggests that the person was enlisted in the USMC and got sent home from the hospital, indicating that they likely got injured during a training event.Choice B suggests an unlikely scenario where the person got shot in action, which cannot be inferred from the context and contradicts the fact that the person did not regret it.Answer and Explain: Explain Error Type:</p>
<p>ChatGPT of 3-shot</p>
<p>The correct choice is: A. The context suggests that the narrator enlisted with the USMC and was sent home from the hospital.Hypothesis A, where the narrator got injured the last day during a training event, is the more likely explanation because it fits with the idea of being sent home from the hospital after being injured in the line of duty.Hypothesis B, where the narrator was shot in action but didn't regret it, does not make sense as a plausible explanation for being sent home from the hospital expressed in the context.Answer and Explain: Explain Error Type: Davinci-003 of 0-shot The more likely explanation is A. I got injured the last day during a training event.This could have been caused by some sort of accident with a weapon or a physical training exercise gone wrong.It is more likely that I would have been injured on the last day of service, as opposed to being shot in action, which would be less likely in the US Marine Corps.Answer and Explain:</p>
<p>Explain Error Type:</p>
<p>Bard of 0-shot</p>
<p>The more likely explanation is A. I got injured the last day during a training event.This is because the context states that you enlisted with the USMC at 19, and that you were sent home from the hospital after being injured.This suggests that the injury occurred during training, and that it was serious enough to warrant your discharge from the military.Hypothesis B is less likely because it is not supported by the context.Hypothesis: The reason why Joy left the donut store with 13 instead of 12 donuts may be due to a generous gesture from the store clerk, a promotional offer that allowed her to receive an extra donut for free, a mistake made by the store clerk, or Joy's persuasive negotiation or effective communication skills.Explanation: The fact that Joy left the donut store with 13 instead of 12 donuts suggests that there may have been an unexpected addition or alteration to her order.One possibility is that the store clerk may have generously offered Joy an additional donut as a kind gesture or to show appreciation for her business.Another possibility is that there may have been a promotional offer where customers who purchased a dozen donuts received an additional donut for free.Alternatively, a mistake may have been made by the store clerk when fulfilling Joy's order, resulting in an extra donut being added to her purchase.Finally, Joy may have effectively persuaded or negotiated with the store clerk to receive an extra donut, perhaps by demonstrating loyal patronage or simply asking for an additional donut.These factors may have contributed to Joy's pleasant surprise at leaving the donut store with an extra donut, and highlight the importance of customer service, promotions, careful order fulfillment, and communication skills in the food service industry.Therefore, the hypothesis proposes that the reason why Joy left the donut store with 13 instead of 12 donuts may be due to a generous gesture from the store clerk, a promotional offer that allowed her to receive an extra donut for free, a mistake made by the store clerk, or Joy's persuasive negotiation or effective communication skills.Answer and Explain:</p>
<p>ChatGPT of 3-shot Hypothesis: The donut shop may have made an error and accidentally given Joy an extra donut, which she was pleased to receive.Alternatively, the donut store may have had a special promotion where customers receive an extra donut for free, which Joy was not aware of but happily benefited from.This scenario highlights the potential for unexpected surprises and bonuses in our daily lives.Answer and Explain: Davinci-003 of 0-shot Hypothesis: The donut store owner gave Joy an extra donut for free.Explanation: The extra donut could indicate that the store owner gave Joy an extra donut for free as a gesture of goodwill or appreciation.This could be due to Joy being loyal customer at the store, or due to her being a child and the store owner having sympathy for her.Answer and Explain:</p>
<p>ChatGPT of 0-shot</p>
<p>Based on the given context, individuals who possess traits such as badness, sadness, and strength are typically perceived as rough.Dave has been described as poor and imperfect which would suggest a degree of hardship and inadequacy, and the observation confirms that he is perceived as rough.Therefore, it can be inferred that Dave's perceived roughness is a result of his perceived badness, sadness, and poor state in accordance with the given context.Answer and Explain:</p>
<p>Explain Error Type: ChatGPT of 1-shot</p>
<p>The context indicates that being bad, sad, and strong is associated with being rough, and being poor and imperfect are also negative qualities.The observation that Dave is rough suggests that he may possess these negative traits or exhibit aggressive behavior.Answer and Explain:</p>
<p>Explain Error Type: ChatGPT of 3-shot The given observation contradicts the context, as it states that Dave is rough whereas the context indicates that he is poor and imperfect.Therefore, there is no valid explanation for this observation within the given context.Answer and Explain:</p>
<p>Explain Error Type: Explain Error Type: ChatGPT of 3-shot According to the context, the bear likes the squirrel, and the cat likes the bear, which could suggest a predator-prey relationship between the cat and the squirrel.Moreover, given the fact that the cat likes the squirrel, it is possible that the cat may try to protect the squirrel from the bear's chase.Additionally, the context suggests that.Answer and Explain:</p>
<p>Explain Error Type: Davinci-003 of 0-shot The missing fact is: The tiger is young.Explanation: The rule1 states that if the tiger is young then the tiger likes the squirrel.The fact16 states that the tiger likes the squirrel, so the tiger must also be young.</p>
<p>Answer and Explain:</p>
<p>Explain Error Type: Bard of 0-shot Sure, here is a short missing fact for the given context and fact:</p>
<p>The squirrel sees the cat.This is the missing fact because it is the only fact that can explain all of the given facts and rules.For example, Rule 4 says that if the tiger likes the bear and the bear sees the squirrel then the squirrel is round.However, we know that the squirrel is round, so the squirrel must also see the bear.</p>
<p>Here are some simple explanations for the missing fact:</p>
<p>The squirrel is friends with the cat.The squirrel is related to the cat.The squirrel is a food source for the cat.I hope this helps!Let me know if you have any other questions.However, I think there is a mistake in the given context.The fact that "The cat likes the squirrel" contradicts the fact that "The cat chases the squirrel".If the cat likes the squirrel, then it would not chase the squirrel.Therefore, one of these facts must be incorrect.Answer and Explain: Explain Error Type:</p>
<p>Question, answer and explanation</p>
<p>Context: A science class stored one selection of various fruits at 30 degrees Celsius, a similar selection in similar conditions at 20 degrees, and another similar selection in similar conditions at 10 degrees.Because the fruits stored at 20 degrees stayed fresh longer than those stored at 30 degrees, and those stored at 10 degrees stayed fresh longest, the class concluded that the cooler the temperature at which these varieties of fruits are stored, the longer they will stay fresh.Question: The class's reasoning is flawed in that the class A. neglected to offer any explanation for the results it discovered B. assumed without proof that its thermometer was reliable C. generalized too readily from the fruits it tested to fruits it did not test D. too readily extrapolated from a narrow range of temperatures to the entire range of temperatures.</p>
<p>Answer: D</p>
<p>ChatGPT of 0-shot</p>
<p>The correct answer is D. Option A is not relevant to the class's reasoning, as it still made an attempt to draw a conclusion from its results.Option B is a possible flaw, but it is not stated in the argument whether the thermometer was reliable or not, so it cannot be the correct answer.Option C is a possible flaw, but it is not stated whether the class attempted to generalize its results to fruits it did not test, so it cannot be the correct answer.Option D is the correct answer, as the class too readily made a generalization from a narrow range of temperatures (10 degrees, 20 degrees, and 30 degrees) to the entire range of temperatures at which fruits can be stored, which is likely to be much broader.Answer and Explain:</p>
<p>ChatGPT of 1-shot</p>
<p>The correct answer is C. The class's reasoning is flawed in that it generalized too readily from the fruits it tested to fruits it did not test.The experiment only tested a specific selection of fruits and did not test all varieties of fruits, so it is not necessarily true that cooler temperatures will always keep fruits fresh longer.Option A is incorrect because the class did offer an explanation for the results they discovered.Option B is not relevant to the flaw in the class's reasoning.Option D is also not relevant, as the class did not extrapolate from a narrow range of temperatures to the entire range of temperatures.Answer and Explain: Explain Error Type:</p>
<p>ChatGPT of 3-shot</p>
<p>The correct answer is D. The class's reasoning is flawed in that they too readily extrapolated from a narrow range of temperatures to the entire range of temperatures.The class only tested three temperatures, and did not consider the possibility of a threshold beyond which colder temperatures may not be beneficial for all fruits.Therefore, it is too hasty to conclude that ẗhe cooler the temperature, the longer the fruits will stay fresh based on such a limited range of temperatures.Answer and Explain: Davinci-003 of 0-shot The correct answer is C. The class's reasoning is flawed in that the class generalized too readily from the fruits it tested to fruits it did not test.The class was only testing three types of fruit at three different temperatures, which does not truly demonstrate how all different varieties of fruits can react differently to varying temperatures.Thus, the class should not make a hasty generalization about how all fruits would respond to different temperatures.Answer and Explain:</p>
<p>Explain Error Type:</p>
<p>Bard of 0-shot</p>
<p>The correct answer is D. Too readily extrapolated from a narrow range of temperatures to the entire range of temperatures.</p>
<p>The class only tested fruits at three temperatures: 30 degrees Celsius, 20 degrees Celsius, and 10 degrees Celsius.They concluded that the cooler the temperature, the longer the fruits will stay fresh.However, this conclusion is based on a very small sample size.It is possible that the relationship between temperature and freshness is not linear.For example, it is possible that fruits will stay fresh for the same amount of time at 10 degrees Celsius and 5 degrees Celsius.The class would need to test fruits at a wider range of temperatures to confirm their conclusion.The other options are not correct.Option A is incorrect because the class did offer an explanation for their results: they said that the cooler the temperature, the longer the fruits will stay fresh.Option B is incorrect because the class did not assume that their thermometer was unreliable.Option C is incorrect because the class did not generalize to fruits they did not test.They only tested fruits at three temperatures, and they only made a conclusion about those three temperatures.Answer and Explain: TABLE 23: A reasoning case in LogiQA.</p>
<p>Question, answer and explanation</p>
<p>Context: People often use microwave ovens to heat food.Some people think that microwave oven heating changes the molecular structure of food, producing molecules that the human body cannot recognize.These strange molecular formulas are unacceptable to the human body, and some are toxic and may even cause cancer.Therefore, people or animals who often eat microwave foods will have serious physiological changes in their bodies, which will cause serious health problems.Question: Which of the following can best question the above view?A. Microwave heating will not cause more nutrient loss than other cooking methods B. China's microwave oven production standards are consistent with international standards and EU standards C. It is also common for developed countries to use microwave ovens D. The microwave only heats the water molecules in the food, the food does not change chemically.Answer: D</p>
<p>ChatGPT of 0-shot</p>
<p>The answer is D. The given argument suggests that microwave oven heating changes the molecular structure of food, producing molecules that the human body cannot recognize and causing serious health problems.Option D provides an alternative explanation for the process of microwave heating, which states that the microwave only heats the water molecules in the food, and the food does not change chemically.Therefore, option D best questions the view presented in the argument.Option A is not relevant as it talks about nutrient loss rather than the effects of microwave heating on food.Option B and option C are not relevant as they provide information about the production standards of microwave ovens and their usage in different countries, but do not impact the argument presented in the context.Answer and Explain:</p>
<p>ChatGPT of 1-shot</p>
<p>The correct choice is: D. The argument suggests that the use of microwave ovens to heat food changes the molecular structure of the food, producing molecules that the human body cannot recognize, which can lead to serious health problems.Option D challenges this argument by stating that the microwave only heats the water molecules in the food and does not change the chemical structure of the food.This suggests that the argument may be based on a misunderstanding of how microwaves work.Option A, B, and C are not relevant to the argument or do not challenge the premise that microwaves can cause health problems.Answer and Explain:</p>
<p>ChatGPT of 3-shot</p>
<p>The correct choice is: D. The microwave only heats the water molecules in the food, the food does not change chemically, can best question the above view.It suggests that the claim about microwave ovens changing the molecular structure of food is not accurate and provides a scientific explanation for how microwave heating works, which does not involve the production of strange or toxic molecules.Answer and Explain:</p>
<p>Davinci-003 of 0-shot</p>
<p>The best answer is B. China's microwave oven production standards are consistent with international standards and EU standards.This answer questions the view presented in the context that microwave oven heating changes the molecular structure of food, since it implies that safety regulations surrounding the production of microwave ovens would never be put in place if this were true.Therefore, this disproves the view that microwave oven heating produces molecules that the human body cannot recognize, which could be toxic and may even cause cancer.</p>
<p>Answer and Explain: Explain Error Type:</p>
<p>Bard of 0-shot</p>
<p>The correct answer is D. The microwave only heats the water molecules in the food, the food does not change chemically.</p>
<p>The context states that some people think that microwave oven heating changes the molecular structure of food, producing molecules that the human body cannot recognize.However, this is not true.Microwaves only heat the water molecules in food, and the food does not change chemically.This is why microwaved food is safe to eat.The other options are not necessarily correct.Option A is incorrect, as there is some evidence that microwave heating can cause nutrient loss.Option B is irrelevant, as it does not address the question of whether or not microwave oven heating changes the molecular structure of food.Option C is also irrelevant, as it does not address the question of whether or not microwaved food is safe to eat.</p>
<p>Here are some additional information about microwaves and their effects on food: Microwaves are a form of non-ionizing radiation, which means that they do not have enough energy to damage DNA.</p>
<p>Microwaves heat food by causing water molecules to vibrate.This vibration produces heat, which cooks the food.Microwaves do not change the chemical structure of food.This means that microwaved food is safe to eat.Some studies have shown that microwave heating can cause nutrient loss.However, these studies have been small and the results have been mixed.More research is needed to determine the extent of nutrient loss from microwaved food.Overall, there is no evidence that microwave oven heating is harmful to human health.Microwaved food is safe to eat.Answer and Explain: Explain Error Type: TABLE 25: A reasoning case in LogiQA2NLI.</p>
<p>Question, answer and explanation</p>
<p>Context: The premis is: Researcher: People who participate in opinion surveys often give answers they believe the opinion surveyor expects to hear, and it is for this reason that some opinion surveys do not reflect the actual views of those being surveyed.However, in well-constructed surveys, the questions are worded so as to provide respondents with no indication of which answers the surveyor might expect.So if a survey is well constructed, survey respondents' desire to meet surveyors' expectations has no effect on the survey's results.The conclusion is: the answers of opinion-survey respondents can be influenced by beliefs about the surveyor's expectations even if those beliefs are unfounded.Question: Determine whether the premise and conclusion entailed or NOT entailed.Answer: entailed ChatGPT of 0-shot The conclusion is not logically entailed by the given premise.The premise discusses the potential for opinion survey respondents to give answers they believe the surveyor expects to hear, but notes that well-constructed surveys prevent this by wording questions in a way that does not indicate expected answers.It then concludes that if a survey is well constructed, respondents' desire to meet surveyors' expectations has no effect on the survey's results.Therefore, it does not support the conclusion that the answers of opinion-survey respondents can be influenced by beliefs about the surveyor's expectations even if those beliefs are unfounded.Answer and Explain: Explain Error Type:</p>
<p>ChatGPT of 1-shot</p>
<p>The conclusion is not entailed by the given premise.While the premise discusses how well-constructed surveys are worded so as to provide respondents with no indication of which answers the surveyor might expect, and how this can mitigate respondents' desires to meet surveyors' expectations, it does not provide information to support or refute the claim that the answers of opinion-survey respondents can be influenced by beliefs about the surveyor's expectations even if those beliefs are unfounded.Therefore, the conclusion cannot be reliably inferred from the given premise.Answer and Explain: Explain Error Type: ChatGPT of 3-shot Not entailed.The conclusion about the answers of opinion-survey respondents being influenced by beliefs about the surveyor's expectations cannot be logically derived from the given premise.Answer and Explain:</p>
<p>Explain Error Type:</p>
<p>Davinci-003 of 0-shot NOT entailed.The premise is implying that because the survey is well constructed with no indication of what answers the surveyor might expect, the survey respondents' answers will not be impacted by any beliefs they have of the surveyor's expectations.The conclusion is suggesting that even if those beliefs are unfounded, the responses will still be influenced by them.This goes against the premise as the premise has stated that wellconstructed surveys will give no indication of the surveyor's expectations, and therefore, their beliefs of the surveyor's expectations will have no influence.Answer and Explain: Explain Error Type:</p>
<p>Bard of 0-shot</p>
<p>The conclusion is entailed by the premise.The premise states that people who participate in opinion surveys often give answers they believe the opinion surveyor expects to hear, even if those beliefs are unfounded.It also states that well-constructed surveys are worded so as to provide respondents with no indication of which answers the surveyor might expect.This means that even if a survey is well-constructed, respondents may still be influenced by beliefs about the surveyor's expectations.This is because people are often motivated to please others, and they may believe that giving the answer they think the surveyor expects will make them look good.For example, a respondent may believe that a surveyor expects them to support a certain candidate in an election.Even if the survey question is not leading, the respondent may still give the answer they think the surveyor expects, even if they do not actually support that candidate.This is why it is important for surveyors to be aware of the potential for respondents to be influenced by beliefs about their expectations.Surveyors can try to minimize this effect by using clear and unbiased language, and by explaining to respondents that there are no right or wrong answers.Answer and Explain: Explain Error Type:</p>
<p>Fig. 1 :
1
Fig. 1: The overall architecture of the evaluation.</p>
<p>Fig. 3 :
3
Fig. 3: LLM performances on different datasets.</p>
<p>(a) Different reasoning types.(b) Generation/Classification.</p>
<p>Fig. 4 :
4
Fig. 4: Visualization on the metric of answer correctness.</p>
<p>Fig. 5 :
5
Fig. 5: Heatmap visualization of rigor and self-awareness.</p>
<p>(a) bAbI15 (Deductive).(b) RuleTaker (Deductive).(c) FOLIO (Deductive).(d) Leap-of-Thought (Deductive).</p>
<p>(e) bAbI16 (Inductive).(f)CLUTRR (Inductive).(g)α-NLI (Abductive).(h)α-NLG (Abductive).</p>
<p>(i) AbductiveRules (Abductive).(j)D*-Ab (Abductive).</p>
<p>(k) ReClor (Mixed-form).(l)LogiQA (Mixed-form).</p>
<p>Fig. 6 :
6
Fig. 6: Statistics of different error types from evidence selection process and reasoning process view.The light green bar is wrong selection and the dark green bar means hallucination.From dark to light blue, the bars represent No Reasoning, Mistakes (Perspective), and Mistakes (Process).</p>
<p>(a) Visualization on the activity.(b) Visualization on the orientation.(c) Visualization on the no-hallucination.</p>
<p>Fig. 7 :
7
Fig. 7: Heatmap results for the activity, orientation and no-hallucination of LLMs.</p>
<p>Fig. 8 :
8
Fig. 8: The LLM performances with different numbers of statements.The red line denotes Answer Correct while the green line is Answer Correct+Explain Correct.</p>
<p>Fig. 8e is CLUTRR in the inductive setting and Fig. 8f is D-Ab in the abductive setting.The horizontal axis denotes the number of statements.The left vertical axis denotes the number of samples for different numbers of statements.And the right vertical axis represents the performances with different numbers of statements.</p>
<p>Fig. presents the performance of ChatGPT with different hop</p>
<p>(a) EntailmentBank (De.).(b) FOLIO (De.).(c) Leap-of-Thought (De.).(d) CLUTRR (In.).(e) α-NLI (Ab.).(f) α-NLG (Ab.).(g) AbductiveRules (Ab.).(h) D*-Ab (Ab.).</p>
<p>(i) ReClor (Mix).(j) LogiQA (Mix).(k) LogiQA2.0 (Mix).(l) LogiQA2NLI (Mix).</p>
<p>Fig. 9 :
9
Fig. 9: The performances of ChatGPT with different tokens on various datasets.The red line denotes Answer Correct while the green line is Answer Correct+Explain Correct.</p>
<p>(a) Deductive (EntailmentBank).(b) Inductive (CLUTRR).(c) Abductive (D*-Ab).</p>
<p>Fig. 10 :
10
Fig. 10: The performances of ChatGPT under different number of hops.Comparison of Deductive, Inductive and Abductive reasoning settings.The red line denotes Answer Correct while the green line is Answer Correct+Explain Correct.</p>
<p>Fig. 11 :Fig. 12 :
1112
Fig. 11: Visualization of three early-stage LLM capabilities under four reasoning settings.</p>
<p>Fig. 15 :
15
Fig. 15: Heatmap results for the activity, orientation and no-hallucination of LLMs.</p>
<p>(a) Deductive setting (EntailmentBank) on text-davinci-003.(b) Inductive setting (CLUTRR) on textdavinci-003.(c) Abductive setting (D<em>-Ab) on textdavinci-003.(d) Deductive setting (EntailmentBank) on BARD.(e) Inductive setting (CLUTRR) on BARD.(f) Abductive setting (D</em>-Ab) on BARD.</p>
<p>Fig. 16 :
16
Fig. 16: The performances of text-davinci-003 and BARD under different number of hops.Comparison of deductive, inductive and abductive reasoning settings.</p>
<p>[</p>
<p>task description]: Given a context, the abductive reasoning task is to choose the more likely explanation from a given pair of hypotheses choices.And give simple explanations.[zero-shot prompt]: The context is: [Context].The hypothesis choices are: A. [Option A].B. [Option B]. [task description] Next, I will give you [k] example(s) for test.The context is [Context].The hypothesis choice are: A. [Option A].B. [Option B].The correct choice is: [Label].Next, I will give you an example for test.[zero-shot prompt] α-NLG [task description]: Given a context, the abductive reasoning task is to generate a valid and short hypothesis.[zero-shot prompt]: The context is: [Context].Please generate a short hypothesis for the context and give simple explanations.[task description] Next, I will give you [k] example(s) for test.The context is [Context].The correct answer is [Label].Next, I will give you an example for test.[zero-shot prompt] AbductiveRules [task description]: Given a context and an observation, the abductive reasoning task is to generate a valid and short explanation.[zero-shot prompt]: The context is: [Context].The observation is: [Observation].Please generate a short explanation for the given context and observation.[task description] Next, I will give you [k] example(s) for test.The context is: [Context].The observation is: [Observation].The explanation is: [Explanation].Next, I will give you an example for test.[zero-shot prompt] D*-Ab [task description]: Given a context and a fact, the abductive reasoning task is to generate a short missing fact.[zero-shot prompt]: The context is: [Context+Rule].The fact is: [Fact].Please generate a short missing fact for the given context and fact.And give simple explanations.[task description] Next, I will give you [k] example(s) for test.The context is: [Context+Rule].The observation is: [Observation].The explanation is: [Explanation].Next, I will give you an example for test.[zero-shot prompt] mixed-form Reasoning ReClor [task description]: This is a Mahcine Reading Comprehension task, given the context and question, you are required to choose the correct answer from the answer set and give explanations.[zero-shot prompt]: The context is: [Context].The question is: [Question].[Option A]. [Option B]. [Option C]. [Option D].Please give the correct answer and simple explanations.[task description] Next, I will give you [k] example(s) for test.The context is: [Context].The question is: [Question].The correct choice is: [Label].Next, I will give you an example for test.[zero-shot prompt] LogiQA [task description]: This is a Mahcine Reading Comprehension task, given the context and question, you are required to choose the correct answer from the answer set and give explanations.[zero-shot prompt]: The context is: [Context].The question is: [Question].[Option A]. [Option B]. [Option C]. [Option D].Please give the correct answer and simple explanations.[task description] Next, I will give you [k] example(s) for test.The context is: [Context].The question is: [Question].The correct choice is: [Label].Next, I will give you an example for test.[zero-shot prompt] LogiQA 2.0 [task description]: This is a Mahcine Reading Comprehension task, given the context and question, you are required to choose the correct answer from the answer set and give explanations.[zero-shot prompt]: The context is: [Context].The question is: [Question].[Option A]. [Option B]. [Option C]. [Option D].Please give the correct answer and simple explanations.[task description] Next, I will give you [k] example(s) for test.The context is: [Context].The question is: [Question].The correct choice is: [Label].Next, I will give you an example for test.[zero-shot prompt] LogiQA2NLI [task description]: This is a Natural Language Inference task.Please tell whether the premise and conclusion entailed or NOT entailed.And give simple explanations.[zero-shot prompt]: The premise is: [Premise].The conclusion is: [Conclusion].Please give the correct answer and simple explanations.[task description] Next, I will give you [k] example(s) for test.The premise is: [Premise].The context is: [Context].The correct answer is: [Label].Next, I will give you an example for test.[zero-shot prompt]</p>
<p>4 −
4</p>
<p>4 −
4</p>
<p>3-shot De.
bAbI-15✓85.0076.0075.0038.4046.4039.7079.0080.0088.00100 [42]EntailmentBank✓93.0088.0089.0083.8282.0677.9496.0097.0097.00100 [35]RuleTaker64.0060.0062.0042.0038.0040.2064.0057.0070.00≈100 [43]FOLIO48.0053.0052.0050.0050.9854.4152.0043.0049.0062.11 [36]Leap-Of-Thought82.0090.0087.0072.6174.0161.2179.0072.0079.0099.7 [37]In.bAbI-16 CLUTRR✓ ✓84.00 6.0081.00 23.0074.00 20.0017.10 21.9924.70 19.5512.90 12.8373.00 23.0044.00 26.0052.00 24.00100 [42] 95.0 [47]α-NLI74.0070.0074.0080.9080.0079.1075.0074.0077.0068.90 [44]Ab.α-NLG AbductiveRules✓ ✓9.00 75.0010.00 42.0012.00 35.0021.90 23.3023.40 35.1025.90 29.8010.00 71.0012.00 49.0015.00 22.0045.00 [44] 100 [45]D*-Ab✓8.0021.0023.0011.602.501.8011.000.000.00≥95 [46]ReClor53.0053.0055.0058.8056.0058.8056.0055.0056.0075.00 [13]MixLogiQA LogiQA 2.041.00 43.0035.00 42.0039.00 41.0040.25 54.6039.48 50.8040.86 54.8048.00 53.0046.00 46.0047.00 47.0046.10 [13] 72.25 [41]LogiQA2NLI59.0055.0058.0057.8353.8357.0048.0050.0047.00≈80 [41]</p>
<p>TABLE 2 :
2
Supplementary results on the four trending LLMs across diverse prompt selections.In the implementation, Direct utilizes the zero-shot prompting to output both explanations and answers, but not prompted with chain-of-thought.The values in the table represent the answer correctness.
DatasetGen.LLaMA3.1</p>
<p>-Chat Mistral-Ins-v0.3 Claude-3.5 GPT-4 Direct COT
DirectCOTDirectCOTDirect COTbAbI-15✓89.0090.0059.0051.0097.00100.0098.0098.00De.EntailmentBank RuleTaker✓72.00 62.0074.00 64.0082.00 45.0086.00 64.0085.00 61.0073.00 64.0084.00 59.0077.00 67.00FOLIO53.0056.0048.0045.0081.0084.0070.0069.00Leap-Of-Thought81.0080.0078.0073.0055.0053.0075.0080.00In.bAbI-16 CLUTRR✓ ✓92.00 39.0088.00 40.0036.00 15.0023.00 20.0080.00 29.0086.00 21.0091.00 33.0092.00 26.00α-NLI39.0037.0031.0028.0036.0038.0037.0038.00Ab.α-NLG AbductiveRules✓ ✓17.00 42.0017.00 22.0016.00 35.0015.00 25.0026.00 49.0014.00 41.0029.00 40.0023.00 34.18D*-Ab✓10.0024.006.003.0038.0027.0035.0033.00ReClor67.0063.0055.0052.0088.0090.0088.0085.00MixLogiQA LogiQA 2.046.54 63.0044.55 55.0050.23 45.0042.70 47.0064.00 74.0068.00 79.0064.00 81.0065.00 82.00LogiQA2NLI61.0051.0054.0054.0060.0061.0059.0048.00</p>
<p>53.00 32.00↓ 60.00 16.00↓ 64.00 11.00↓ 25.50 12.90↓ 12.10 34.30↓ 14.10 25.60↓ 45.00 34.00↓ 25.00 55.00↓ 47.00 41.00↓
text-davinci-003ChatGPTBARD0-shot1-shot3-shot0-shot1-shot3-shot0-shot1-shot3-shotbAbI-1561.0066.0068.0032.1012.9016.4077.0074.0085.0056.0060.0064.0027.9018.0017.6045.0025.0047.00EntailmentBankDeductive</p>
<p>3-shot De.
bAbI-15✓63.0056.0043.0022.6039.4055.7099.0084.0062.00EntailmentBank✓8.006.007.007.065.883.2426.0025.0028.00RuleTaker26.0029.0027.0021.3027.8034.8080.0084.0075.00FOLIO14.0023.0021.0031.8622.5519.6160.0063.0068.00Leap-Of-Thought71.0055.0054.0032.745.044.732.002.000.00In.bAbI-16 CLUTRR✓ ✓60.00 2.0077.00 28.0086.00 31.0093.60 2.6229.80 1.5741.20 0.8796.00 2.0098.00 6.0099.00 14.00α-NLI2.002.001.001.000.200.108.0016.000.00Ab.α-NLG AbductiveRules✓ ✓63.00 1.0061.00 0.0072.00 0.0070.70 42.4069.70 5.4064.50 0.5024.00 67.0032.00 48.0031.00 22.00D*-Ab✓85.0027.0017.0055.3027.1016.7018.0016.002.00ReClor1.001.001.002.001.201.4011.0016.0024.00MixLogiQA LogiQA 2.00.00 0.005.00 0.000.00 0.001.54 0.800.77 4.001.08 0.832.00 5.0035.00 5.0043.00 4.00LogiQA2NLI0.000.000.000.170.500.1711.0031.004.00
total.Meanwhile, perspective mistake occupies 44.47% of the cases and process mistake covers 36.20%.</p>
<p>TABLE 5 :
5
Statistics and evaluation results on NeuLR.Num.represents the number of samples in the dataset.#Hop represents the hop number of samples in the dataset.COT represents the chain-of-thought strategy under the 1-shot setting.
DatasetNum. #Hop0-text-davinci-003ChatGPTBARD</p>
<p>shot 1-shot COT 0-shot 1-shot COT 0-shot 1-shot COT
NeuLR3,0001∼550.9359.1767.9037.2748.1348.0063.6765.0766.00-Deductive1,000259.0069.4086.1085.2069.1068.3087.4093.1091.90-Inductive1,000386.9089.6095.6015.1068.6069.6096.0092.6096.30-Abductive1,0001∼56.9018.5022.0011.506.706.107.609.509.80accessibility of external solvers, we merely include severallogical reasoning datasets for evaluation, i.e., ProofWriterand RuleTaker. In the implementation, LLMs are promptedto generate First-order Logic, which can be executed by thePyke solver to obtain the final answer [52], [53]. We includethe experimental results in Appendix.It can be concluded that more</p>
<p>powerful LLMs can derive greater advantages from the PoT.</p>
<p>It is observed that open-source LLMs (i.e., LLaMA3.1-Chat and Mistral-Instruct-v0.3) benefit more from the chain-of-thought strategy, but fail in the program-of-thought method.Conversely, proprietary powerful LLMs can derive more benefits from POT than from COT.It demonstrates that POT requires the basic symbolic generation capability, which is challenging for most of the current trending LLMs.The symbol-centric modeling for LLMs is also a promising direction.</p>
<p>TABLE 6 :
6
Details of the selected LLMs.Affi. is short for Affiliation.Charge represents the charges for 1M tokens in the format of (input charge / output charge).Data is the latest time of the utilized training data.B in the last column represents the one billion.
ModelAffi.ChargeDataSizetext-davinci-003Open-AI20$Sep. 2021-ChatGPTOpen-AI2$Jun. 2021-BARDGoogle-Not report 540BLLaMA3.1-ChatMeta-20238BMistral-Inst-v0.3Mistral-20237BClaude-3Anthropic3$/15$2023-GPT-4Open-AI30$/60$2023-</p>
<p>TABLE 7 :
7
Fact length distributions (%) of mixed-form datasets.
Model1-23-4Length 5-67-8≥ 9ReClor28.20 61.609.001.00 0.20LogiQA76.65 13.367.221.84 0.92LogiQA 2.039.40 45.80 12.60 1.60 0.60LogiQA2NLI34.94 43.56 16.95 3.07 1.46</p>
<p>shows the results.
No Hallucination98.33%1.67%Oriented97.33%2.67%Active97.00%3.00%MatchedNot MatchedSelf-aware94.00%6.00%Rigorous95.33%4.67%Correct99.00%1.00%0%20%40%60%80%100%</p>
<p>TABLE 9 :
9
The construction of neutral content.Example column presents some example words.Prefix represents the prefix for each type of words.6-character represents examples of randomly generated combination of characters.
TypeExamplePrefix6-characterNameBob/LilyNPe.g., uF52pTSpeciesDog/SheepSPe.g., 7gfO2kProperty Big/Red/SmartADPe.g., PT01mx</p>
<p>TABLE 8 :
8
Evaluated Datasets.Gen. distinguishes whether the predicted answer is generated text or classified labels.Explain denotes whether the explanation is required in the task.# Davinci, # ChatGPT and # BARD columns represent the number of evaluation samples of each dataset for the three LLMs.
CategoriesDatasetSource Gen. Explain # Davinci # ChatGPT # BARDbAbI-15[42]✓✓1001,000100EntailmentBank[35]✓100340100DeductiveRuleTaker[43]✓1001,000100FOLIO[36]100204100Leap-Of-Thought[37]1001,289100InductivebAbI-16 CLUTRR[42] [38]✓ ✓100 1001,000 1,146100 100α-NLI[44]1001,000100Abductiveα-NLG AbductiveRules[44] [45]✓ ✓100 1001,000 1,000100 100D*-Ab[46]✓✓1001,000100ReClor[39]100500100mixed-formLogiQA LogiQA 2.0[40] [41]100 100651 500100 100LogiQA2NLI[41]100600100</p>
<p>TABLE 10 :
10
Prompt Engineering.
DatasetPrompt of zero-shotPrompt of k-shotDeductive ReasoningbAbI (task 15)[zero-shot prompt]: Given facts: [Context]. Based on the given facts above, answer the following question using deductive reasoning and give simple explanations. The question is: [Question]There are [k] examples of deductive rea-soning: Given facts: [Context] The question is: [Question] The answer is: [Answer] (display k sam-ples)[zero-shot prompt]There are [k] examples of deductive rea-EntailmentBank[zero-shot prompt]: Given facts: [Context]. [Question]. Please answer the question in one sentence using deduc-soning: Given facts: [Context]tive reasoning. And give simple explanations.The question is: [Question]The answer is: [Answer] (display k sam-ples)[zero-shot prompt]RuleTaker[zero-shot prompt]: Given facts: [Context]. Based on the given facts above, determine whether the following state-ment is true using deductive reasoning and give simple explanations. The statement is: [Statement].There are [k] examples of deductive rea-soning: Given facts: [Context] The statement is: [Statement] The answer is: [Answer] (display k sam-ples)[zero-shot prompt]FOLIO[zero-shot prompt]: Given facts: [Context]. Based on the given facts above, determine whether the following statement is true, false, or uncertain using deductive reasoning and give simple explanations. The statement is: [Statement].There are [k] examples of deductive rea-soning: Given facts: [Context] The statement is: [Statement] The answer is: [Answer] (display k sam-ples)[zero-shot prompt]Leap-Of-Thought[zero-shot prompt]: Given facts: [Context]. Based on the given facts above, determine whether the following state-ment is true using deductive reasoning and give simple explanations. The statement is: [Statement].There are [k] examples of deductive rea-soning: Given facts: [Context] The statement is: [Statement] The answer is: [Answer] (display k sam-ples)[zero-shot prompt]Inductive ReasoningbAbI-16[zero-shot prompt]: Given facts: [context]. Based on the given facts above, answer the following question using inductive reasoning and give simple explanations. The question is: [Question].There are [k] examples of inductive reason-ing: Given facts: [Context] The question is: [Question] The answer is: [Answer] (display k sam-ples)[zero-shot prompt]There are [k] examples of inductive reason-CLUTRR[zero-shot prompt]: Given facts: [context]. [Question]. Please answer the question in one sentence using induc-ing: Given facts: [Context]tive reasoning. And give simple explanations.The question is: [Question]The answer is: [Answer] (display k sam-ples)[zero-shot prompt]Abductive Reasoningα-NLI</p>
<p>TABLE 11 :
11
A reasoning case in bAbI-15.Sheep are afraid of wolves. 2 Mice are afraid of wolves.3 Winona is a sheep.4 Gertrude is a sheep.5 Wolves are afraid of sheep.6 Emily is a sheep.7 Cats are afraid of sheep.8 Jessica is a sheep.
Question, answerand explanationContext: 1 Question: What is jessica afraid of? Answer: Wolf Explanation: IsSheep(jessica) (8)</p>
<p>TABLE 12 :
12
A reasoning case in EntailmentBank.</p>
<p>TABLE 13 :
13
A reasoning case in RuleTaker.
ChatGPTof 1-shotPollination in plants is helped by pollinating animals such as insects, which carry pollen from one flower to another as a necessary part of the process, positively impacting the plants' life cycle. Answer and Explain: Explain Error Type:ChatGPTof 3-shotPollinating animals such as insects, which are required for the process of pollination to occur. Answer and Explain: Explain Error Type:Davinci-003of 0-shotPollination is helped by pollinating animals, such as insects, carrying pollen from one flower to another. Answer and Explain: Explain Error Type:Bardof 0-shotAnswer and Explain:Explain Error Type:
Sure, here is the answer to the question in one sentence using deductive reasoning: Pollination is helped by pollinating animals, such as insects.Here is the simple explanation: Pollination is the process of transferring pollen from the male part of a flower to the female part of a flower.Pollinating animals, such as insects, help pollination by carrying pollen from one flower to another.Without pollination, plants cannot reproduce and produce seeds.Therefore, pollinating animals help the life cycle of some plants by helping them to reproduce.</p>
<p>TABLE 14 :
14
A reasoning case in FOLIO.All growth companies' stocks are volatile.2 If the stock price is volatile, then it is not suitable for a retirement fund.3 Some companies' stocks are growth companies' stocks.4 All mature companies' stocks are suitable for a retirement fund.5 KO is a mature company's stock.Statement: KO is a company stock and a growth company's stock.Question: Determine whether the statement is true, false, or uncertain using deductive reasoning and give explanations.
Question, answerand explanationContext: 1 Answer: False Explanation: MatureCompanies'Stocks(KO) (5)</p>
<p>TABLE 15 :
15
A reasoning case in Leap-Of-Thought.A bird is not capable of express feelings.2 A quack-quack is not capable of suffering dehydration.3 A privet is capable of suffering dehydration.4 A music is capable of express feelings.Statement: A jazz is capable of express feelings.Question: Determine whether the statement is true using deductive reasoning and give explanations.
Question, answerand explanationContext: 1 Answer: True Explanation: IsMusic(jazz) (common sense)</p>
<p>TABLE 18 :
18
A reasoning case in α-NLI.When I was 19, I enlisted with the USMC.When I got back from the hospital, the USMC sent me home.Question: Choose the more likely explanation from following two options: A. I got injured the last day during a training event.B. I was shot in action but didn't regret it.
Question, answerand explanationContext: Answer: BChatGPTof 0-shot</p>
<p>TABLE 19 :
19
The context does not mention anything about you being shot in action, or about you not regretting it.Additionally, it is not clear why the USMC would send you home if you were not injured.A reasoning case in α-NLG.Joy's mom sent her in the donut store for a dozen donuts.She was happy as she left with 13 donuts instead of 12. Question: Generate a short hypothesis for the context and give simple explanations.Answer: Joy got an extra donut.The fact that Joy's mom sent her to the donut store for a dozen donuts, and Joy left with 13 instead of 12 might have been a result of the potential personal or environmental factors related to perception, generosity, or habit.Explanation: The fact that Joy's mom sent her to the donut store for a dozen donuts suggests that it might have some potential personal or environmental factors related to perception, generosity, or habit.Having personal or environmental factors can involve various types of cognitive or practical factors, such as accuracy, generosity, or routine, and can reflect one's sense of responsibility or automaticity.Thus, the fact that Joy's mom sent her to the donut store for a dozen donuts might have been a result of the potential personal or environmental factors related to perception, generosity, or habit.Additionally, the fact that Joy left with 13 donuts instead of 12 implies that there might have been some additional potential cognitive or contextual factors related to accuracy, availability, or preference.Having cognitive or contextual factors can involve various types of perception or action, such as estimation, surplus, or choice, and can reflect one's sense of intention or opportunity.Furthermore, the fact that Joy was happy as she left with 13 donuts instead of 12 suggests that there might have been some potential affective or symbolic factors related to value, satisfaction, or normativity.Having affective or symbolic factors can involve various types of appraisal or intent, such as pleasure, gratitude, or conformity, and can reflect one's sense of social or moral obligation.Therefore, the hypothesis suggests that the fact that Joy's mom sent her to the donut store for a dozen donuts, and Joy left with 13 instead of 12 might have been a result of the potential personal or environmental factors related to perception, generosity, or habit, as well as the potential cognitive or contextual factors related to accuracy, availability, or preference.
Here is a simple explanation for why you might have been injured during training:You were involved in a training accident.You overexerted yourself and suffered an injury.You were injured by another Marine during training.It is important to remember that these are just possible explanations, and we cannot know for sure why you wereinjured without more information.Answer and Explain:Explain Error Type:</p>
<p>TABLE 20 :
20
15reasoning case in AbductiveRules.Gary is strong.2Thingsthat are tiny, are small, and are short, are thin.3Allpeople that are quiet, are poor, and are smart, are clever.4Garyis big. 5 If something is short, is tiny, and is nice, it is also thin.6If a person is bad, is sad, and is strong, that person is rough.7 Things that are strong, are huge, and are big, are high.8Peoplethat are bad, are short, and are strong, are also clever.9Dave is poor.10Dave is imperfect.11Charlieis quiet.12Gary is heavy.13Anne is tiny.14Allthings that are huge, are small, and are big, are also high.15Allthings that are quiet, are huge, and are poor, are also thin.16 Anne is little.17 Charlie is nice.18 Dave is bad.19 If a person is tiny, is nice, and is sad, that person is high.20 If a person is quiet, is nice, and is smart, that person is clever.21 All things that are small, are big, and are smart, are rough.22 Anne is small.23 People that are bad, are poor, and are sad, are also rough.Question: Please generate a short explanation for the given context and observation.The observation is: Dave is rough.
Question, answerand explanationContext: 1 Answer: Dave is sad.Explanation: 9, 18 − → Dave is sad. 23</p>
<p>TABLE 21 :
21
A reasoning case in D*-Ab.The bear chases the cat.Fact2: The bear is big.Fact3: The bear is cold.Fact4: The bear likes the squirrel.Fact5:The bear likes the tiger.Fact6: The bear sees the squirrel.Fact7:The cat is big.Fact8: The cat is cold.Fact9: The cat is young.Fact10: The cat likes the bear.Fact11: The cat likes the tiger.Fact12: The cat sees the bear.Fact13: The cat sees the tiger.Fact14: The squirrel is young.Fact15: The tiger chases the bear.Fact16: The tiger likes the squirrel.Rule1:If the tiger is young then the tiger likes the squirrel.Rule2:If something is round then it likes the bear.Rule3: If something likes the bear then it is cold.Rule4: If the tiger likes the bear and the bear sees the squirrel then the squirrel is round.Rule5: If something likes the squirrel then it is round.Rule6: If something chases the bear and the bear likes the cat then the cat likes the squirrel.Question: Please generate a short missing fact for the given context and fact.And give simple explanations.The fact is: The cat likes the squirrel.According to the given rules, if the tiger likes the bear and the bear sees the squirrel, then the squirrel is round.As the tiger likes the squirrel in the given context, we can conclude that the tiger likes the bear, and since the bear likes the squirrel, we can infer that the squirrel is round.The fact that the cat likes the squirrel suggests a potential behavioral or social relationship between the two animals, such as companionship or mutual interest in resources.The missing fact that Anne is young could imply a new or additional character to the context.Answer and Explain:
Question, answerand explanationContext: Fact1: Answer: The bear likes the cat.Explanation: Fact15 − −−− → The bear likes the cat. Rule6ChatGPTof 0-shotMissing fact: The squirrel is round. Explanation: Answer and Explain:Explain Error Type:ChatGPTof 1-shot</p>
<p>TABLE 22 :
22
A reasoning case in ReClor.</p>
<p>ACKNOWLEDGMENTSThis work was supported by National Key Research and Development Program of China (2020AAA0108800), National Natural Science Foundation of China (62137002, 61937001, 62176209, 62176207, 62106190, and 62050194).1 − → Afraid(jessica, wolf) ChatGPT of 0-shot Jessica is afraid of nothing, as it is not specified in the given facts that she has any known natural predator or fear.Answer and Explain: Explain Error Type: ChatGPT of 1-shot Jessica is a sheep and sheep are afraid of wolves.Therefore, it can be deduced that Jessica is afraid of wolves.Answer and Explain: ChatGPT of 3-shot Sheep are afraid of wolves and Jessica is a sheep.Answer and Explain: Explain Error Type: Davinci-003 of 0-shot Jessica is likely afraid of wolves, since wolves are a common source of fear that other animals, including sheep, mice, and cats, have in common.Wolves are known to be predators of sheep, so Jessica being a sheep, would likely be scared of wolves.Answer and Explain: Explain Error Type: Bard of 0-shot Jessica is a sheep.Sheep are afraid of wolves.Therefore, Jessica is afraid of wolves.Here is the deductive reasoning: 1. Sheep are afraid of wolves.2. Jessica is a sheep.3. Therefore, Jessica is afraid of wolves.The first premise is supported by the fact that sheep are prey animals and wolves are predators.Wolves are known to prey on sheep, and sheep have evolved to be afraid of wolves as a way of self-preservation.The second premise is supported by the fact that Jessica is a sheep.This is a given fact in the prompt.The third premise follows logically from the first two premises.If sheep are afraid of wolves, and Jessica is a sheep, then Jessica must be afraid of wolves.Therefore, the conclusion is that Jessica is afraid of wolves.Answer and Explain:Bardof 0-shot Sure, here are some possible hypotheses for why Joy was happy when she left the donut store with 13 donuts instead of 12: The donut store was giving away a free donut with every dozen purchased.The donut store made a mistake and gave Joy an extra donut.Joy was able to negotiate with the donut store owner to get an extra donut.Joy is a very good friend or family member and the donut store owner wanted to give her a little extra something.Here are some simple explanations for each hypothesis: The donut store was giving away a free donut with every dozen purchased because they were trying to promote their business or because they had too many donuts on hand.The donut store made a mistake and gave Joy an extra donut because they were busy or because they were not paying attention.Joy was able to negotiate with the donut store owner to get an extra donut because she was polite and persuasive.Joy is a very good friend or family member and the donut store owner wanted to give her a little extra something because they appreciated her business or because they were close friends or family.It is impossible to know for sure which hypothesis is correct, but all of them are possible explanations for why Joy was happy when she left the donut store with 13 donuts instead of 12. Answer and Explain:
. D*-Ab , 4.00 4.00↓ 5.00 16.00↓ 4.00 19.00↓ 4.10 7.50↓ 1.20 1.30↓ 1.10 0.70↓ 4.00 7.00↓ 0.000.00− 0.000.00−</p>
<p>Dr-prolog: A system for defeasible reasoning with rules and ontologies on the semantic web. G Antoniou, A Bikakis, IEEE Transactions on Knowledge and Data Engineering (TKDE). 1922007</p>
<p>A novel combination of answer set programming with description logics for the semantic web. T Lukasiewicz, IEEE Transactions on Knowledge and Data Engineering (TKDE). 22112010</p>
<p>Contrastive graph representations for logical formulas embedding. Q Lin, J Liu, L Zhang, Y Pan, X Hu, F Xu, H Zeng, IEEE Transactions on Knowledge and Data Engineering. 3542023</p>
<p>Towards high-order complementary recommendation via logical reasoning network. L Wu, Y Zhou, D Zhou, IEEE International Conference on Data Mining (ICDM). 2022</p>
<p>Incorporating context graph with logical reasoning for inductive relation prediction. Q Lin, J Liu, F Xu, Y Pan, Y Zhu, L Zhang, T Zhao, The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 2022</p>
<p>Multi-hop reasoning question generation and its application. J Yu, Q Su, X Quan, J Yin, IEEE Transactions on Knowledge and Data Engineering (TKDE). 3512023</p>
<p>Logiformer: A two-branch graph transformer network for interpretable logical reasoning. F Xu, J Liu, Q Lin, Y Pan, L Zhang, The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 2022</p>
<p>Logical reasoning in formal and everyday reasoning tasks. H Bronkhorst, G Roorda, C Suhre, M Goedhart, International Journal of Science and Mathematics Education. 182020</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Z Yang, X Du, R Mao, J Ni, E Cambria, abs/2303.12023CoRR. 2023</p>
<p>Instruction tuning with GPT-4. B Peng, C Li, P He, M Galley, J Gao, abs/2304.03277CoRR. 2023</p>
<p>. 10.48550/arXiv.2304.03277</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J Nie, J Wen, 2023CoRR</p>
<p>Will affective computing emerge from foundation models and general artificial intelligence? A first evaluation of chatgpt. M M Amin, E Cambria, B W Schuller, IEEE Intelligent Systems. 3822023</p>
<p>Merit: Meta-path guided contrastive learning for logical reasoning. F Jiao, Y Guo, X Song, L Nie, Findings of the Association for Computational Linguistics (Findings of ACL). 2022</p>
<p>PathReasoner: Modeling reasoning path with equivalent extension for logical question answering. F Xu, Q Lin, T Zhao, J Jiaweihan, J Liu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20241</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, abs/2302.04023CoRR. 2023</p>
<p>Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. N Bian, X Han, L Sun, H Lin, Y Lu, B He, abs/2303.16421CoRR. 2023</p>
<p>Mathprompter: Mathematical reasoning using large language models. S Imani, L Du, H Shrivastava, abs/2303.05398CoRR. 2023</p>
<p>Abduction and induction: Essays on their relation and integration. P A Flach, A C Kakas, 2000Abductive and inductive reasoning: background and issues</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, NeurIPS2022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Is chatgpt a general-purpose natural language processing task solver. C Qin, A Zhang, Z Zhang, J Chen, M Yasunaga, D Yang, abs/2302.06476CoRR. 2023</p>
<p>Chatlog: Recording and analyzing chatgpt across time. S Tu, C Li, J Yu, X Wang, L Hou, J Li, CoRR. 2304.14106, 2023</p>
<p>Ceval: A multi-level multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, F Qi, Y Fu, M Sun, J He, abs/2305.083222023CoRR</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, P Liang, K R Mckeown, T B Hashimoto, abs/2301.13848CoRR. 2023</p>
<p>Inductive relation prediction with logical reasoning using contrastive representations. Y Pan, J Liu, L Zhang, T Zhao, Q Lin, X Hu, Q Wang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Line: Logical query reasoning over hierarchical knowledge graphs. Z Huang, M Chiang, W Lee, 28th SIGKDD Conference on Knowledge Discovery and Data Mining. 2022</p>
<p>Rlogic: Recursive logical rule learning from knowledge graphs. K Cheng, J Liu, W Wang, Y Sun, 28th SIGKDD Conference on Knowledge Discovery and Data Mining. 2022</p>
<p>Deductive reasoning. P N Johnson-Laird, Annual review of psychology. 5011999</p>
<p>Anatomy of deductive reasoning. V Goel, Trends in cognitive sciences. 11102007</p>
<p>Relations between inductive reasoning and deductive reasoning. E Heit, C M Rotello, Journal of Experimental Psychology: Learning, Memory, and Cognition. 3638052010</p>
<p>Nature language reasoning, a survey. F Yu, H Zhang, B Wang, arXiv:2303.147252023arXiv preprint</p>
<p>J R Josephson, S G Josephson, Abductive inference: Computation, philosophy, technology. Cambridge University Press1996</p>
<p>Abductive, presumptive and plausible arguments. D Walton, Informal Logic. 2122001</p>
<p>Explaining answers with entailment trees. B Dalvi, P Jansen, O Tafjord, Z Xie, H Smith, L Pipatanangkura, P Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>FOLIO: natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, L Benson, L Sun, E Zubova, Y Qiao, M Burtell, D Peng, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, S R Joty, A R Fabbri, W Kryscinski, X V Lin, C Xiong, D Radev, abs/2209.008402022CoRR</p>
<p>Leapof-thought: Teaching pre-trained models to systematically reason over implicit knowledge. A Talmor, O Tafjord, P Clark, Y Goldberg, J Berant, Advances in Neural Information Processing Systems (NeurIPS). 2020</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. K Sinha, S Sodhani, J Dong, J Pineau, W L Hamilton, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. W Yu, Z Jiang, Y Dong, J Feng, 8th International Conference on Learning Representations (ICLR). 2020</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. J Liu, L Cui, H Liu, D Huang, Y Wang, Y Zhang, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI). the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)2020</p>
<p>Evaluating the logical reasoning ability of chatgpt and GPT-4. H Liu, R Ning, Z Teng, J Liu, Q Zhou, Y Zhang, abs/2304.03439CoRR. 2023</p>
<p>Towards aicomplete question answering: A set of prerequisite toy tasks. J Weston, A Bordes, S Chopra, T Mikolov, 4th International Conference on Learning Representations (ICLR). 2016</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI). the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)2020</p>
<p>Abductive commonsense reasoning. C Bhagavatula, R L Bras, C Malaviya, K Sakaguchi, A Holtzman, H Rashkin, D Downey, W Yih, Y Choi, 8th International Conference on Learning Representations (ICLR). 2020</p>
<p>Abductionrules: Training transformers to explain unexpected inputs. N Young, Q Bao, J Bensemann, M Witbrock, 2022Findings of the Association for Computational Linguistics</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. O Tafjord, B Dalvi, P Clark, 2021Findings of the Association for Computational Linguistics</p>
<p>Learning reasoning strategies in end-to-end differentiable proving. P Minervini, S Riedel, P Stenetorp, E Grefenstette, T Rocktäschel, Proceedings of the 37th International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research. the 37th International Conference on Machine Learning (ICML), ser. Machine Learning Research2020119</p>
<p>A survey for in-context learning. Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, arXiv:2301.002342022arXiv preprint</p>
<p>Self-adaptive in-context learning. Z Wu, Y Wang, J Ye, L Kong, arXiv:2212.103752022arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR202310799</p>
<p>Symbol-LLM: Towards foundational symbol-centric interface for large language models. F Xu, Z Wu, Q Sun, S Ren, F Yuan, S Yuan, Q Lin, Y Qiao, J Liu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20241</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, NeurIPS2022</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. I Drori, S Zhang, R Shuttleworth, L Tang, A Lu, E Ke, K Liu, L Chen, S Tran, N Cheng, Proceedings of the National Academy of Sciences. 11932e21234331192022PNAS)</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y Bang, A Madotto, P Fung, ACM Computing Surveys. 5512382023</p>
<p>Sentic-Net 7: A commonsense-based neurosymbolic AI framework for explainable sentiment analysis. E Cambria, Q Liu, S Decherchi, F Xing, K Kwok, LREC. 2022</p>
<p>TABLE 16: A reasoning case in bAbI-16. </p>
<p>Question: What color is Lily? Answer: Gray Explanation: IsRhino(Greg) (1), IsGray(Greg) (5)→ IsGray(rhino). Question, answer and explanation Context: 1 Greg is a rhino. IsRhino(Lily) (9)→IsGray(Lily</p>
<p>Although Lily is described as a rhino, there is no information given that directly pertains to Lily's color. Additionally, there is no other animal descriptions that could be used to make an inference regarding Lily's color. Therefore, based on the given facts, it is not possible to make an accurate determination of Lily's color using inductive reasoning. Answer and Explain: Explain Error Type: ChatGPT of 1-shot Lily's color is likely to be unknown as there is no information provided about her color among the other animals. Answer and Explain: Explain Error Type: ChatGPT of 3-shot Brian, Greg, and Bernhard, which do not directly provide clues about what color Lily may be. Therefore, we cannot be certain about her color based on the information given alone. Answer and Explain: Explain Error Type: Davinci-003 of 0-shot Based on the given facts, we can use inductive reasoning to conclude that Lily is gray. This is because all of the other rhinos in the given facts are gray, and there is no evidence to suggest that Lily is any different. In fact, the only thing that distinguishes Lily from the other rhinos is her name, which is a flower that is typically white or yellow. However, we can safely assume that Lily's name is not a reference to her color, as there are many other animals and plants that share the same name. Therefore, the most likely conclusion is that Lily is gray, just like the other rhinos. Here is a more detailed explanation of the inductive reasoning process used to. ChatGPT of 0-shot Based on the given facts and inductive reasoning, it is likely that Lily's color is not definitively determined. reach this conclusion: 1. We are given the following facts: * Greg is a rhino. * Brian is a swan. * Bernhard is a frog. * Brian is green. * Greg is gray. * Bernhard is yellow. * Julius is a lion. * Julius is yellow. * Lily is a rhino</p>
<p>We notice that all of the rhinos in the given facts are gray. </p>
<p>We conclude that there is a strong likelihood that Lily is also gray, as there is no evidence to suggest that she is any different from the other rhinos. </p>
<p>We can safely assume that Lily's name is not a reference to her color, as there are many other animals and plants that share the same name. </p>
<p>Since the only other colors mentioned among the animals are green and yellow, and none of the rhinos have these colors, it is reasonable to assume that Lily is gray. Answer and Explain: Explain Error Type: TABLE 17: A reasoning case in CLUTRR. Question, answer and explanation Context: 1 [Chuck] took his daughter [Sara] to the park that afternoon to play. 2 [Judy] took her sister. Answer and Explain: Bard of 0-shot Based on inductive reasoning, it is likely that Lily is gray, as both Greg and Julius are gray and they are both rhinos, like Lily. out to dinner for her birthday. 3 [Chuck] took his sister [Judy] out to lunch after learning that she got accepted into her first choice for university. 4 [Maxine] asked her daughter. went to see a movie yesterday and then got ice cream afterwards. 6 [Reynaldo]'s wife, [Maxine], was thrilled to learn she was pregnant. 7 [Naomi] went shoe shopping with her sister [Sara</p>
<p>Joyce) i1 [(Maxine, sister, Judy) 2, (Judy, brother, Chuck) 3] → (Maxine, brother, Chuck) i2. Sara). 15Question: What is the kinship between Reynaldo and Naomi? Answer: niece Explanation: [(Reynaldo, wife, Maxine) 6, (Maxine, daughter, Joyce) 5] → (Reynaldo, daughter. Reynaldo, daughter, Joyce) i1, (Joyce, uncle, Chuck) i4] → (Reynaldo, brother, Chuck) i4 [(Chuck, daughter. Reynaldo, brother, Chuck) i4, (Chuck, daughter, Naomi) i5] → (Reynaldo, niece</p>
<p>Answer and Explain: Explain Error Type: Davinci-003 of 0-shot Reynaldo and Naomi have no direct kinship; they are both related to Maxine, as Reynaldo is her husband and Naomi is Sara's sister. Answer and Explain: Explain Error Type: Bard of 0-shot Reynaldo and Naomi are siblings-in-law. Inductive reasoning is a type of reasoning that uses specific examples to make a general conclusion. In this case, we can use the following specific examples to make the general conclusion that Reynaldo and Naomi are siblings-in-law: Maxine is the wife of Reynaldo. Sara is the daughter of Maxine. Naomi is the sister of Sara. Therefore, Naomi is the sister-in-law of Reynaldo. Here are some simple explanations for each of the specific examples: Maxine is the wife of Reynaldo because we are told that Reynaldo's wife is Maxine. Sara is the daughter of Maxine because we are told that Maxine has a daughter named Sara. Naomi is the sister of Sara because we are told that Naomi went shoe shopping with her sister Sara. Answer and Explain: Explain Error Type: Davinci-003 of 0-shot Dave is rough because he is bad, poor, and strong, which meets the criteria of people that are bad, poor, and strong being rough. Answer and Explain: Explain Error Type: Bard of 0-shot Sure, here is a short explanation for the given context and observation: Dave is rough because he is bad, sad, and strong. He is also poor and imperfect. This suggests that he is also quiet and clever. However, he is also tiny and small. This suggests that he is also weak. Answer and Explain: Explain Error Type: ChatGPT of 1-shot Unclear. Answer and Explain: Explain Error Type: ChatGPT of 3-shot The given facts do not provide any information about the kinship between Reynaldo and Naomi, so the question cannot be answered using inductive reasoning. Here is a more detailed explanation: Dave is bad. This is evident from the fact that he is described as "bad" and "rough". Dave is sad. This is evident from the fact that he is described as "sad" and "imperfect". Dave is strong. This is evident from the fact that he is described as "strong" and "huge". Dave is poor. This is evident from the fact that he is described as "poor" and "imperfect". Dave is imperfect. This is evident from the fact that he is described as "sad" and "imperfect". Dave is quiet. This is evident from the fact that he is described as "quiet" and "clever". Dave is clever. This is evident from the fact that he is described as "clever" and "strong" Dave is tiny. This is evident from the fact that he is described as "tiny" and "small". Dave is small. This is evident from the fact that he is described as "small" and "weak". Dave's badness, sadness, strength, poverty, imperfection, quietness, cleverness, tininess, and smallness suggest that he is a complex and contradictory person. He is capable of great things, but he is also capable of great evil. He is a force to be reckoned with, and he will not be easily defeated. Answer and Explain: Explain Error Type</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9251 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9251</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9251</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271098044</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.08735v1.pdf" target="_blank">Real-Time Anomaly Detection and Reactive Planning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may then trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner's latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on this project page: https://sites.google.com/view/aesop-llm.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9251.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9251.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast embedding-based anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-space fast anomaly detection using foundation model embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-time anomaly detector that embeds observations (lists of concepts / scene descriptions) into an FM embedding space and scores anomalousness via similarity metrics (top-k cosine, Mahalanobis, etc.), enabling 10–20+ Hz onboard operation and triggering a slower generative LLM if an anomaly is detected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>embedding FM (MPNet, Mistral, BERT, OpenAI embedding models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based embedding models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (110M MPNet; 7B Mistral; 336M BERT-large; OpenAI embeddings unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>lists of textual concepts (object lists/scene descriptions), optionally image-derived object lists; also tested with image embeddings (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>robotic observations: synthetic domains (Warehouse Manipulation, Autonomous Vehicle, VTOL), quadrotor hardware images, CARLA driving simulator</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>out-of-distribution / semantic anomaly (novel object, unusual object combinations, semantic hazards)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Embed prior nominal observations offline to build an embedding cache De; at runtime embed current observation et = ϕ(o_t) and compute a score s(et;De) using similarity heuristics (top-k cosine similarity, Mahalanobis distance). Threshold the score using an empirical quantile (e.g., 95th) calibrated on nominal data to classify anomalies in real time. If anomaly detected, trigger slow generative LLM for assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Zero-shot generative LLM classification (GPT-4 single-token and chain-of-thought), conventional OOD detectors (SCOD, Mahalanobis applied to perception modules) used for comparison in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, AUROC, true positive rate (TPR), false positive rate (FPR); calibration via empirical quantiles and ROC analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Embedding detectors (MPNet and Mistral) achieve high accuracy/AUROC across synthetic domains; in VTOL calibration (top-5 scoring) MPNet accuracy by quantile: 75%→0.83, 85%→0.89, 90%→0.91, 95%→0.93; Mistral (7B): 75%→0.83, 85%→0.89, 90%→0.91, 95%→0.94 (Table VI). Embedding detector often outperforms zero-shot generative GPT-4 classification on more complex domains (VTOL), and runs onboard at ~20Hz on Jetson AGX Orin (authors report 20Hz / 357x speedup versus cloud GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Embedding-based detector generally outperforms zero-shot generative classification (single-token and CoT with GPT-4) for anomaly detection on lists of concepts, especially as domain complexity increases and when prior concept coverage is sufficient; embedding methods also far faster and feasible onboard compared to autoregressive LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires representative nominal embedding cache (performance sensitive to concept coverage rather than raw sample count); small embedding models (e.g., MPNet) can degrade when the number of objects/concepts per observation grows; top-1 and Mahalanobis perform poorly in low-data regimes; vision-language detectors (CLIP) can yield high false positives when visual novelty (unseen routes) exists even if semantically nominal; object detector hallucinations (OWL-ViT) can degrade two-step pipeline unless post-processed.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Grounding anomaly detection in an embedding cache of prior nominal experiences enables better semantic detection than zero-shot generative LLM classification; top-k similarity scoring (k≥3) is robust across regimes; embedding detectors can match or nearly match much larger models (MPNet competitive with 7B models in many settings) while enabling real-time onboard operation; concept coverage matters more than sheer amount of data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9251.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9251.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPNet (embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence Transformer MPNet (embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 110M-parameter BERT-derived model trained for producing sentence/phrase embeddings; used as the primary lightweight embedding model for fast anomaly detection on lists of detected objects and scene descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MPNet (Sentence Transformer MPNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (sentence embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈110M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textual lists of concepts (prompts constructed from object detections), used also with prompts derived from images via an object detector</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>synthetic robotic observation domains (WM, AV, VTOL), quadrotor hardware experiments, CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic OOD anomalies in lists of concepts (novel objects/combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute embeddings of textual scene descriptions (from prior nominal runs and current observation) and perform top-k similarity scoring against an offline cache; thresholding yields a binary anomaly decision.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against Mistral (7B) embeddings, Llama-2, BERT variants, OpenAI embedding models, and generative GPT baselines (GPT-4/GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, AUROC, ROC curves, TPR/FPR calibration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In VTOL (top-5 scoring) MPNet accuracies at quantiles: 75%:0.83, 85%:0.89, 90%:0.91, 95%:0.93 (Table VI). MPNet often rivals much larger embedding models and outperforms GPT-4 generative zero-shot in detection accuracy when concept coverage is adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>MPNet (110M) performs nearly as well as 7B embeddings (Mistral) in several tasks and often outperforms generative GPT-4 classification for detection; however MPNet degrades for complex, high-object-count scenes unless ground-truth scene descriptions are available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Degrades as number of objects/concepts per observation increases (noted in CARLA experiments); needs high-fidelity scene descriptions (object detector errors reduce effectiveness); less expressive than large multi-modal models for semantically rich images.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>A relatively small, embedding-tuned model can provide competitive semantic anomaly detection when grounded with nominal prior embeddings, enabling onboard real-time execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9251.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9251.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral 7B (embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral (7.11B) embedding / completion model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter foundation model evaluated both as an embedding source and as a small generative baseline; its embeddings achieve high anomaly detection performance on semantically rich observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral (7.11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (foundation model; used for embeddings and generation in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈7.11B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textual scene descriptions, lists of objects; also compared on CARLA with ground-truth scene descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>synthetic robotics domains, CARLA autonomous driving scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic anomalies in lists of concepts / scenes</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use Mistral to compute text embeddings of scene descriptions and perform top-k similarity scoring against nominal cache for anomaly detection; also used as a generative baseline in some ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against MPNet, BERT, OpenAI embeddings, GPT-3.5/GPT-4 generative baselines and CLIP vision embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, AUROC; ROC analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mistral embedding accuracies in VTOL (top-5 scoring): 75%:0.83, 85%:0.89, 90%:0.91, 95%:0.94 (Table VI). In CARLA with ground-truth scene descriptions Mistral (7B) achieved top detection accuracy (≈0.94 in an ablation), surpassing GPT-4V CoT by ~4% when detector output was corrected to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Mistral embeddings often outperform smaller embeddings (MPNet) in semantically rich, multi-object observations and can surpass GPT-4/GPT-4V CoT in detection accuracy when provided reliable scene descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Larger models require more compute—less viable for onboard deployment without acceleration; benefits depend on high-quality scene descriptions (object detector errors limit performance).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Larger embedding models capture richer semantics and are robust when number of detected objects increases; with ground-truth detections they substantially improve detection in complex visual domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9251.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9251.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (generative slow reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4) used for chain-of-thought and single-token reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity autoregressive LLM used as the slow generative reasoner to methodically assess flagged anomalies and decide whether to engage safety-preserving interventions, with chain-of-thought (CoT) shown to improve assessment accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (autoregressive generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (large, proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textual scene descriptions; prompts constructed from object lists or VLM-converted captions</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>VTOL/AV/manipulation synthetic domains; quadrotor and CARLA scenarios via VLM or parsed detections</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic anomalies requiring safety assessment (hazardous vs inconsequential anomalies)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>When fast detector flags anomaly, construct a prompt (possibly augmented with VLM-produced scene description) asking GPT-4 to reason (single-token or CoT) and output an action classification indicating which recovery region or continue; CoT instructs the model to reason per concept then decide.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against embedding-based fast detector and smaller generative models (GPT-3.5, Llama2 7B single-token), and evaluated single-token vs CoT prompting variants</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, true positive rate (TPR), false positive rate (FPR), balanced accuracy reported in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Generative GPT-4 CoT improves safety-assessment accuracy over single-token GPT-4 and smaller models; authors report CoT yields notable accuracy improvements (e.g., ~11% improvement for GPT-4 in VTOL safety assessment). However, exact numeric values vary by domain and prompt; GPT-4 generative detection accuracy declines on more complex domains relative to embedding detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>GPT-4 (zero-shot generation) is less effective/less consistent for raw anomaly detection across complex domains compared to embedding-based detector but excels at deliberative safety assessment (deciding whether a detected anomaly warrants intervention), especially when run with chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High inference latency (authors budgeted K timesteps and measured mean response times several seconds; must be accounted for in control), expensive compute, limited context window for grounding large caches of nominal examples, recency/format biases in prompting; not feasible for continuous real-time loop without fallback planning to account for latency.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Generative LLMs are valuable for downstream safety assessment (deciding intervention) but are computationally too slow for continuous anomaly detection; combining a fast embedding-based detector to triage and a slow generative reasoner to decide provides effective and practical hybrid monitoring.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9251.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9251.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo (generative slow reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo used as a slow generative reasoning model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive LLM (GPT-3.5-turbo) used in hardware experiments as the slow reasoner to assess anomalies triggered by the fast detector; demonstrated real-world latency characteristics and utility for zero-shot hazard assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (OpenAI model)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textual prompts derived from vision-language detector output (lists of objects) and direct text observations</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>quadrotor hardware experiments (landing site obstructions) and synthetic domains</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic anomalies influencing safety (obstruction, occupancy of landing zones, inconsequential novel objects)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cloud-query generative reasoning: prompt GPT-3.5-turbo with scene description and candidate interventions; parse returned classification to select recovery set; used with chain-of-thought prompting in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to GPT-4 CoT and embedding-based detector for detection/assessment tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy and latency measurements (used to choose K bound); TPR/FPR reported in some tables</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used in quadrotor hardware runs; measured latency distribution for GPT-3.5-turbo on calibration prompts: mean ~3.1s, std 0.85s, 95th quantile ~4.3s (authors used K=4.3s to bound latency). Performance as slow assessor effective in distinguishing consequential vs inconsequential anomalies in hardware demos.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Functionally similar role to GPT-4 but with different accuracy/latency tradeoffs; CoT generally helps accuracy, but embedding detector outperforms in detection task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Latency and variability require MPC design to account for worst-case reasoning time; cloud queries may be unreliable in the wild.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Latency characterization (bimodal distribution, tail outliers) informed MPC consensus horizon K selection; demonstrates necessity of accounting for LLM inference time in closed-loop control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9251.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9251.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (vision embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Language–Image Pre-training (CLIP) visual embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vision-Language model whose image embeddings were used directly for end-to-end anomaly detection on RGB observations, providing competitive performance to GPT-4V CoT in some CARLA experiments but showing sensitivity to visual novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>vision-language transformer (contrastive image-text embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various CLIP backbones (not precisely specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>high-dimensional RGB images (end-to-end image embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>CARLA autonomous driving simulation dataset of semantic anomalies; also compared to two-step text-embedding pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic anomalies embedded in images (stop sign on billboard, inactive traffic light on truck, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute CLIP image embeddings for observations and perform top-k similarity scoring against an embedding cache constructed from nominal images to detect anomalies end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Two-step pipeline using OWL-ViT detections + text embeddings (MPNet, Mistral), GPT-4V CoT, SCOD and Mahalanobis baselines wrapping an object detector</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, TPR/FPR, AUROC; ablations with withheld-route calibration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CLIP embeddings achieve high performance comparable to GPT-4V CoT in the main CARLA evaluation when calibration includes nominal data from all routes; reported metrics in Table IV & ablations show CLIP achieving e.g., balanced accuracy values competitive with text-based methods. However, when nominal data from anomalous routes are withheld (novel routes at test), CLIP's FPR increases substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>CLIP comparable to GPT-4V CoT in some settings and can outperform embedding text-only pipelines if calibration covers visual variability; but CLIP is more sensitive to visual novelty (higher false positives) compared to two-stage detection that uses semantic descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High false positive rates when embedding cache lacks visual coverage (unseen routes produce visual novelty flagged as anomalous); CLIP embeddings mix visual and semantic features making calibration and disentanglement necessary for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Direct multi-modal image embeddings enable end-to-end anomaly detection and can match CoT generative reasoning in accuracy, but practical deployment requires careful calibration against route/appearance shift or disentangling visual vs semantic features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9251.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9251.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OWL-ViT (object detector for two-step pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OWL-ViT open-vocabulary object detector used to convert images into text detections</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision model used to produce scene descriptions (lists of detected objects and their context) that are then embedded by language embedding models for two-stage anomaly detection; detector errors notably affect downstream embedding-based detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OWL-ViT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>vision transformer based open-vocabulary object detector</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>RGB images → textual object detections (lists)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>quadrotor hardware images, CARLA driving images</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic anomalies expressed as object presence or relationships in list outputs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply OWL-ViT to images to produce a list of detected objects/phrases; construct text prompts from these lists; compute embeddings (MPNet/Mistral) for anomaly scoring against nominal cache.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>End-to-end CLIP embedding approach; generative LLM vision models (GPT-4V) for direct image-to-decision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Downstream anomaly detection accuracy and effect on embedding pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Two-step pipeline using OWL-ViT + embeddings performed well when OWL-ViT outputs were post-processed to ground-truth detections; however, raw OWL-ViT hallucinations in CARLA reduced performance and made MPNet underperform relative to GPT-4V unless ground-truth corrections were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>When OWL-ViT outputs are noisy, end-to-end CLIP or GPT-4V CoT can outperform the two-step pipeline; with reliable scene descriptions, two-step embedding detectors (with Mistral) can surpass GPT-4V CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Object detector hallucinations and missed detections harm two-step embedding detectors; post-processing to ground-truth is not practical in deployment but demonstrates sensitivity to detector fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Quality of scene description (object detector accuracy) is critical for embedding-based semantic anomaly detection; the two-step paradigm isolates semantic reasoning from visual novelty and can be more robust to visual appearance shifts if detections are reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real-Time Anomaly Detection and Reactive Planning with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Semantic anomaly detection with large language models <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 1)</em></li>
                <li>Simple and scalable predictive uncertainty estimation using deep ensembles <em>(Rating: 1)</em></li>
                <li>Can autonomous vehicles identify, recover from, and adapt to distribution shifts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9251",
    "paper_id": "paper-271098044",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "Fast embedding-based anomaly detector",
            "name_full": "Embedding-space fast anomaly detection using foundation model embeddings",
            "brief_description": "A real-time anomaly detector that embeds observations (lists of concepts / scene descriptions) into an FM embedding space and scores anomalousness via similarity metrics (top-k cosine, Mahalanobis, etc.), enabling 10–20+ Hz onboard operation and triggering a slower generative LLM if an anomaly is detected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "embedding FM (MPNet, Mistral, BERT, OpenAI embedding models evaluated)",
            "model_type": "transformer-based embedding models",
            "model_size": "various (110M MPNet; 7B Mistral; 336M BERT-large; OpenAI embeddings unspecified)",
            "data_type": "lists of textual concepts (object lists/scene descriptions), optionally image-derived object lists; also tested with image embeddings (CLIP)",
            "data_domain": "robotic observations: synthetic domains (Warehouse Manipulation, Autonomous Vehicle, VTOL), quadrotor hardware images, CARLA driving simulator",
            "anomaly_type": "out-of-distribution / semantic anomaly (novel object, unusual object combinations, semantic hazards)",
            "method_description": "Embed prior nominal observations offline to build an embedding cache De; at runtime embed current observation et = ϕ(o_t) and compute a score s(et;De) using similarity heuristics (top-k cosine similarity, Mahalanobis distance). Threshold the score using an empirical quantile (e.g., 95th) calibrated on nominal data to classify anomalies in real time. If anomaly detected, trigger slow generative LLM for assessment.",
            "baseline_methods": "Zero-shot generative LLM classification (GPT-4 single-token and chain-of-thought), conventional OOD detectors (SCOD, Mahalanobis applied to perception modules) used for comparison in experiments",
            "performance_metrics": "Accuracy, AUROC, true positive rate (TPR), false positive rate (FPR); calibration via empirical quantiles and ROC analysis",
            "performance_results": "Embedding detectors (MPNet and Mistral) achieve high accuracy/AUROC across synthetic domains; in VTOL calibration (top-5 scoring) MPNet accuracy by quantile: 75%→0.83, 85%→0.89, 90%→0.91, 95%→0.93; Mistral (7B): 75%→0.83, 85%→0.89, 90%→0.91, 95%→0.94 (Table VI). Embedding detector often outperforms zero-shot generative GPT-4 classification on more complex domains (VTOL), and runs onboard at ~20Hz on Jetson AGX Orin (authors report 20Hz / 357x speedup versus cloud GPT-4).",
            "comparison_to_baseline": "Embedding-based detector generally outperforms zero-shot generative classification (single-token and CoT with GPT-4) for anomaly detection on lists of concepts, especially as domain complexity increases and when prior concept coverage is sufficient; embedding methods also far faster and feasible onboard compared to autoregressive LLMs.",
            "limitations_or_failure_cases": "Requires representative nominal embedding cache (performance sensitive to concept coverage rather than raw sample count); small embedding models (e.g., MPNet) can degrade when the number of objects/concepts per observation grows; top-1 and Mahalanobis perform poorly in low-data regimes; vision-language detectors (CLIP) can yield high false positives when visual novelty (unseen routes) exists even if semantically nominal; object detector hallucinations (OWL-ViT) can degrade two-step pipeline unless post-processed.",
            "unique_insights": "Grounding anomaly detection in an embedding cache of prior nominal experiences enables better semantic detection than zero-shot generative LLM classification; top-k similarity scoring (k≥3) is robust across regimes; embedding detectors can match or nearly match much larger models (MPNet competitive with 7B models in many settings) while enabling real-time onboard operation; concept coverage matters more than sheer amount of data.",
            "uuid": "e9251.0",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MPNet (embedding)",
            "name_full": "Sentence Transformer MPNet (embedding model)",
            "brief_description": "A 110M-parameter BERT-derived model trained for producing sentence/phrase embeddings; used as the primary lightweight embedding model for fast anomaly detection on lists of detected objects and scene descriptions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MPNet (Sentence Transformer MPNet)",
            "model_type": "transformer (sentence embedding model)",
            "model_size": "≈110M parameters",
            "data_type": "textual lists of concepts (prompts constructed from object detections), used also with prompts derived from images via an object detector",
            "data_domain": "synthetic robotic observation domains (WM, AV, VTOL), quadrotor hardware experiments, CARLA",
            "anomaly_type": "semantic OOD anomalies in lists of concepts (novel objects/combinations)",
            "method_description": "Compute embeddings of textual scene descriptions (from prior nominal runs and current observation) and perform top-k similarity scoring against an offline cache; thresholding yields a binary anomaly decision.",
            "baseline_methods": "Compared against Mistral (7B) embeddings, Llama-2, BERT variants, OpenAI embedding models, and generative GPT baselines (GPT-4/GPT-3.5)",
            "performance_metrics": "Accuracy, AUROC, ROC curves, TPR/FPR calibration",
            "performance_results": "In VTOL (top-5 scoring) MPNet accuracies at quantiles: 75%:0.83, 85%:0.89, 90%:0.91, 95%:0.93 (Table VI). MPNet often rivals much larger embedding models and outperforms GPT-4 generative zero-shot in detection accuracy when concept coverage is adequate.",
            "comparison_to_baseline": "MPNet (110M) performs nearly as well as 7B embeddings (Mistral) in several tasks and often outperforms generative GPT-4 classification for detection; however MPNet degrades for complex, high-object-count scenes unless ground-truth scene descriptions are available.",
            "limitations_or_failure_cases": "Degrades as number of objects/concepts per observation increases (noted in CARLA experiments); needs high-fidelity scene descriptions (object detector errors reduce effectiveness); less expressive than large multi-modal models for semantically rich images.",
            "unique_insights": "A relatively small, embedding-tuned model can provide competitive semantic anomaly detection when grounded with nominal prior embeddings, enabling onboard real-time execution.",
            "uuid": "e9251.1",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mistral 7B (embedding)",
            "name_full": "Mistral (7.11B) embedding / completion model",
            "brief_description": "A 7B-parameter foundation model evaluated both as an embedding source and as a small generative baseline; its embeddings achieve high anomaly detection performance on semantically rich observations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral (7.11B)",
            "model_type": "transformer (foundation model; used for embeddings and generation in ablations)",
            "model_size": "≈7.11B parameters",
            "data_type": "textual scene descriptions, lists of objects; also compared on CARLA with ground-truth scene descriptions",
            "data_domain": "synthetic robotics domains, CARLA autonomous driving scenarios",
            "anomaly_type": "semantic anomalies in lists of concepts / scenes",
            "method_description": "Use Mistral to compute text embeddings of scene descriptions and perform top-k similarity scoring against nominal cache for anomaly detection; also used as a generative baseline in some ablations.",
            "baseline_methods": "Compared against MPNet, BERT, OpenAI embeddings, GPT-3.5/GPT-4 generative baselines and CLIP vision embeddings",
            "performance_metrics": "Accuracy, AUROC; ROC analysis",
            "performance_results": "Mistral embedding accuracies in VTOL (top-5 scoring): 75%:0.83, 85%:0.89, 90%:0.91, 95%:0.94 (Table VI). In CARLA with ground-truth scene descriptions Mistral (7B) achieved top detection accuracy (≈0.94 in an ablation), surpassing GPT-4V CoT by ~4% when detector output was corrected to ground truth.",
            "comparison_to_baseline": "Mistral embeddings often outperform smaller embeddings (MPNet) in semantically rich, multi-object observations and can surpass GPT-4/GPT-4V CoT in detection accuracy when provided reliable scene descriptions.",
            "limitations_or_failure_cases": "Larger models require more compute—less viable for onboard deployment without acceleration; benefits depend on high-quality scene descriptions (object detector errors limit performance).",
            "unique_insights": "Larger embedding models capture richer semantics and are robust when number of detected objects increases; with ground-truth detections they substantially improve detection in complex visual domains.",
            "uuid": "e9251.2",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (generative slow reasoner)",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4) used for chain-of-thought and single-token reasoning",
            "brief_description": "A high-capacity autoregressive LLM used as the slow generative reasoner to methodically assess flagged anomalies and decide whether to engage safety-preserving interventions, with chain-of-thought (CoT) shown to improve assessment accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "decoder-only transformer (autoregressive generation)",
            "model_size": "not specified in paper (large, proprietary)",
            "data_type": "textual scene descriptions; prompts constructed from object lists or VLM-converted captions",
            "data_domain": "VTOL/AV/manipulation synthetic domains; quadrotor and CARLA scenarios via VLM or parsed detections",
            "anomaly_type": "semantic anomalies requiring safety assessment (hazardous vs inconsequential anomalies)",
            "method_description": "When fast detector flags anomaly, construct a prompt (possibly augmented with VLM-produced scene description) asking GPT-4 to reason (single-token or CoT) and output an action classification indicating which recovery region or continue; CoT instructs the model to reason per concept then decide.",
            "baseline_methods": "Compared against embedding-based fast detector and smaller generative models (GPT-3.5, Llama2 7B single-token), and evaluated single-token vs CoT prompting variants",
            "performance_metrics": "Accuracy, true positive rate (TPR), false positive rate (FPR), balanced accuracy reported in ablations",
            "performance_results": "Generative GPT-4 CoT improves safety-assessment accuracy over single-token GPT-4 and smaller models; authors report CoT yields notable accuracy improvements (e.g., ~11% improvement for GPT-4 in VTOL safety assessment). However, exact numeric values vary by domain and prompt; GPT-4 generative detection accuracy declines on more complex domains relative to embedding detectors.",
            "comparison_to_baseline": "GPT-4 (zero-shot generation) is less effective/less consistent for raw anomaly detection across complex domains compared to embedding-based detector but excels at deliberative safety assessment (deciding whether a detected anomaly warrants intervention), especially when run with chain-of-thought prompting.",
            "limitations_or_failure_cases": "High inference latency (authors budgeted K timesteps and measured mean response times several seconds; must be accounted for in control), expensive compute, limited context window for grounding large caches of nominal examples, recency/format biases in prompting; not feasible for continuous real-time loop without fallback planning to account for latency.",
            "unique_insights": "Generative LLMs are valuable for downstream safety assessment (deciding intervention) but are computationally too slow for continuous anomaly detection; combining a fast embedding-based detector to triage and a slow generative reasoner to decide provides effective and practical hybrid monitoring.",
            "uuid": "e9251.3",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo (generative slow reasoner)",
            "name_full": "GPT-3.5-turbo used as a slow generative reasoning model",
            "brief_description": "An autoregressive LLM (GPT-3.5-turbo) used in hardware experiments as the slow reasoner to assess anomalies triggered by the fast detector; demonstrated real-world latency characteristics and utility for zero-shot hazard assessment.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_type": "decoder-only transformer (autoregressive)",
            "model_size": "not specified in paper (OpenAI model)",
            "data_type": "textual prompts derived from vision-language detector output (lists of objects) and direct text observations",
            "data_domain": "quadrotor hardware experiments (landing site obstructions) and synthetic domains",
            "anomaly_type": "semantic anomalies influencing safety (obstruction, occupancy of landing zones, inconsequential novel objects)",
            "method_description": "Cloud-query generative reasoning: prompt GPT-3.5-turbo with scene description and candidate interventions; parse returned classification to select recovery set; used with chain-of-thought prompting in some experiments.",
            "baseline_methods": "Compared to GPT-4 CoT and embedding-based detector for detection/assessment tasks",
            "performance_metrics": "Accuracy and latency measurements (used to choose K bound); TPR/FPR reported in some tables",
            "performance_results": "Used in quadrotor hardware runs; measured latency distribution for GPT-3.5-turbo on calibration prompts: mean ~3.1s, std 0.85s, 95th quantile ~4.3s (authors used K=4.3s to bound latency). Performance as slow assessor effective in distinguishing consequential vs inconsequential anomalies in hardware demos.",
            "comparison_to_baseline": "Functionally similar role to GPT-4 but with different accuracy/latency tradeoffs; CoT generally helps accuracy, but embedding detector outperforms in detection task.",
            "limitations_or_failure_cases": "Latency and variability require MPC design to account for worst-case reasoning time; cloud queries may be unreliable in the wild.",
            "unique_insights": "Latency characterization (bimodal distribution, tail outliers) informed MPC consensus horizon K selection; demonstrates necessity of accounting for LLM inference time in closed-loop control.",
            "uuid": "e9251.4",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CLIP (vision embeddings)",
            "name_full": "Contrastive Language–Image Pre-training (CLIP) visual embeddings",
            "brief_description": "A Vision-Language model whose image embeddings were used directly for end-to-end anomaly detection on RGB observations, providing competitive performance to GPT-4V CoT in some CARLA experiments but showing sensitivity to visual novelty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_type": "vision-language transformer (contrastive image-text embeddings)",
            "model_size": "various CLIP backbones (not precisely specified in paper)",
            "data_type": "high-dimensional RGB images (end-to-end image embeddings)",
            "data_domain": "CARLA autonomous driving simulation dataset of semantic anomalies; also compared to two-step text-embedding pipeline",
            "anomaly_type": "semantic anomalies embedded in images (stop sign on billboard, inactive traffic light on truck, etc.)",
            "method_description": "Compute CLIP image embeddings for observations and perform top-k similarity scoring against an embedding cache constructed from nominal images to detect anomalies end-to-end.",
            "baseline_methods": "Two-step pipeline using OWL-ViT detections + text embeddings (MPNet, Mistral), GPT-4V CoT, SCOD and Mahalanobis baselines wrapping an object detector",
            "performance_metrics": "Accuracy, TPR/FPR, AUROC; ablations with withheld-route calibration",
            "performance_results": "CLIP embeddings achieve high performance comparable to GPT-4V CoT in the main CARLA evaluation when calibration includes nominal data from all routes; reported metrics in Table IV & ablations show CLIP achieving e.g., balanced accuracy values competitive with text-based methods. However, when nominal data from anomalous routes are withheld (novel routes at test), CLIP's FPR increases substantially.",
            "comparison_to_baseline": "CLIP comparable to GPT-4V CoT in some settings and can outperform embedding text-only pipelines if calibration covers visual variability; but CLIP is more sensitive to visual novelty (higher false positives) compared to two-stage detection that uses semantic descriptions.",
            "limitations_or_failure_cases": "High false positive rates when embedding cache lacks visual coverage (unseen routes produce visual novelty flagged as anomalous); CLIP embeddings mix visual and semantic features making calibration and disentanglement necessary for robustness.",
            "unique_insights": "Direct multi-modal image embeddings enable end-to-end anomaly detection and can match CoT generative reasoning in accuracy, but practical deployment requires careful calibration against route/appearance shift or disentangling visual vs semantic features.",
            "uuid": "e9251.5",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "OWL-ViT (object detector for two-step pipeline)",
            "name_full": "OWL-ViT open-vocabulary object detector used to convert images into text detections",
            "brief_description": "A vision model used to produce scene descriptions (lists of detected objects and their context) that are then embedded by language embedding models for two-stage anomaly detection; detector errors notably affect downstream embedding-based detectors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OWL-ViT",
            "model_type": "vision transformer based open-vocabulary object detector",
            "model_size": "not specified in paper",
            "data_type": "RGB images → textual object detections (lists)",
            "data_domain": "quadrotor hardware images, CARLA driving images",
            "anomaly_type": "semantic anomalies expressed as object presence or relationships in list outputs",
            "method_description": "Apply OWL-ViT to images to produce a list of detected objects/phrases; construct text prompts from these lists; compute embeddings (MPNet/Mistral) for anomaly scoring against nominal cache.",
            "baseline_methods": "End-to-end CLIP embedding approach; generative LLM vision models (GPT-4V) for direct image-to-decision",
            "performance_metrics": "Downstream anomaly detection accuracy and effect on embedding pipeline",
            "performance_results": "Two-step pipeline using OWL-ViT + embeddings performed well when OWL-ViT outputs were post-processed to ground-truth detections; however, raw OWL-ViT hallucinations in CARLA reduced performance and made MPNet underperform relative to GPT-4V unless ground-truth corrections were applied.",
            "comparison_to_baseline": "When OWL-ViT outputs are noisy, end-to-end CLIP or GPT-4V CoT can outperform the two-step pipeline; with reliable scene descriptions, two-step embedding detectors (with Mistral) can surpass GPT-4V CoT.",
            "limitations_or_failure_cases": "Object detector hallucinations and missed detections harm two-step embedding detectors; post-processing to ground-truth is not practical in deployment but demonstrates sensitivity to detector fidelity.",
            "unique_insights": "Quality of scene description (object detector accuracy) is critical for embedding-based semantic anomaly detection; the two-step paradigm isolates semantic reasoning from visual novelty and can be more robust to visual appearance shifts if detections are reliable.",
            "uuid": "e9251.6",
            "source_info": {
                "paper_title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Semantic anomaly detection with large language models",
            "rating": 2,
            "sanitized_title": "semantic_anomaly_detection_with_large_language_models"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 1,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "rating": 1,
            "sanitized_title": "simple_and_scalable_predictive_uncertainty_estimation_using_deep_ensembles"
        },
        {
            "paper_title": "Can autonomous vehicles identify, recover from, and adapt to distribution shifts",
            "rating": 1,
            "sanitized_title": "can_autonomous_vehicles_identify_recover_from_and_adapt_to_distribution_shifts"
        }
    ],
    "cost": 0.01919025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Real-Time Anomaly Detection and Reactive Planning with Large Language Models
11 Jul 2024</p>
<p>Rohan Sinha 
Amine Elhafsi 
Christopher Agia 
Matthew Foutter 
Edward Schmerling 
Marco Pavone 
Real-Time Anomaly Detection and Reactive Planning with Large Language Models
11 Jul 2024CA817D931FA7DF08DD1EC0C23629592AarXiv:2407.08735v1[cs.RO]
Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems.Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework.In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs.These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner's latency as soon as an anomaly is detected, thus ensuring safety.We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models.This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints.Videos illustrating our approach in both simulation and real-world experiments are available on our project page: https://sites.google.com/view/aesop-llm.</p>
<p>I. INTRODUCTION</p>
<p>Autonomous robotic systems are rapidly advancing in capabilities, seemingly on the cusp of widespread deployment in the real world.However, a persistent challenge is that the finite datasets used to develop these systems are unlikely to capture the limitless variety of the real world, leading to unexpected failure modes when conditions deviate from training data, or when the robot encounters rare situations that were not well-represented at design time.To mitigate the resulting safety implications, we require methods that can 1) assess the reliability of a machine learning (ML) enabled system at runtime and 2) judiciously enact safety-preserving interventions if necessary.</p>
<p>In this work, we investigate the utility of foundation models (FMs), specifically, large language models (LLMs), towards these two objectives by employing LLMs as runtime monitors tasked with 1) detecting anomalous conditions and 2) reasoning about the appropriate safety-preserving course of action.We do so because recent work has shown that the internet-scale pretraining data provides FMs with strong zero-shot reasoning capabilities, which has enabled 1 Dept. of Aeronautics and Astronautics, Stanford University. 2 Dept. of Computer Science, Stanford University. 3Dept. of Mechanical Engineering, Stanford University. 4 NVIDIA.Contact: {rhnsinha, amine, cagia, mfoutter, pavone}@stanford.edu,eschmerling@nvidia.com.Fig. 1: We present an embedding-based runtime monitoring scheme using fast and slow language model reasoners in concert.</p>
<p>During nominal operation, the fast reasoner differentiates between nominal and anomalous robot observations.If an anomaly is flagged, the system enters a fallback-safe state while the slow reasoner determines the anomaly's hazard.In this fallback-safe state, we guarantee access to a set of safe recovery plans (if the anomaly is consequential) and access to continued nominal operation (if the anomaly is inconsequential).</p>
<p>robots to perform complex tasks [5], identify and correct failures [18], and reason about potential safety hazards in their surroundings [12] without explicit training to do so.</p>
<p>However, the adoption of FMs in-the-loop of safety-critical robotic systems is immediately met with two challenges.First, the ever growing scale of FMs poses a major obstacle towards enabling real-time, reactive reasoning about unexpected safetycritical events, especially on agile robotic systems with limited compute.Hence, existing work that applies FMs to robotics has focused on quasi-static (e.g., manipulation) or offline settings that afford large times delays while the LLM completes its reasoning.Second, the application of FMs as runtime monitors requires that they are grounded with respect to the task and capabilities of the system.However, the community has not converged on rigorous methods for grounding FMs without compromising on their generalist zero-shot reasoning abilities (e.g., fine-tuning [24] or linear probing [54] often underperform OOD); prompt design remains a standard practice.</p>
<p>To address these challenges, we present AESOP 1 , an anomaly detection and reactive planning framework that aims to derive maximum utility of an LLM's zero-shot reasoning capabilities while taking LLM inference latencies into account within the control design.As shown in Fig. 1, AESOP splits the monitoring task into two separate stages: The first is rapid, real-time detection of anomalies-conditions that deviate from the nominal conditions where the robot performs reliably-by querying similarity with previously recorded observations within the contextual embedding space of an LLM.The second stage is slower, methodical generative reasoning on how to respond to an anomalous scenario once it has been detected.We combine the resultant monitoring pipeline with a model predictive control strategy that maintains multiple trajectory plans, each corresponding to a safety-preserving intervention, in a way that ensures their joint feasibility for an upper bound on the time it takes the slower, generative reasoning to complete 2 .As such, our contributions are threefold:</p>
<p>1) Fast reasoning with embeddings: We propose a real-time anomaly detection method that, using relatively small FMs (e.g., 120M parameters) and the robot's previous nominal experiences, surpasses generative chain-of-thought (CoT) reasoning with high-capacity LLMs such as GPT-4.Our method runs at 20Hz on an Nvidia Jetson AGX ORIN, a 357x speed up over cloud querying GPT-4.To our knowledge, this is the first application of FM embeddings to the task of runtime monitoring, enabling safe and real-time control of an agile robotic system.2) Slow reasoning through autoregressive generation: While the faster anomaly detector merely detects deviations from prior experiences, we show that autoregressive generation of longer output sequences allows the LLM-based monitor to methodically reason about the safety consequences of out-of-distribution scenarios and decide whether intervention is necessary in a zero-shot fashion; i.e., not all anomalies lead to system-level failures.3) Hierarchical multi-contingency planning: Facilitated by our fast anomaly detector, we introduce a predictive control framework to integrate both FM-based reasoners in a lower-level reactive control loop by maintaining multiple feasible trajectories, each corresponding to a high-level intervention strategy.This allows the robot to 1) react to sudden semantic changes in the robot's environment, 2) maintain closed-loop safety while waiting for a slow reasoner to return a decision, and 3) exhibit dynamic, agile behaviors within the range of scenarios where the nominal autonomy stack is trustworthy.We demonstrate these facts across several commonplace LLMs, ranging from 10 8 −10 12 parameters, as well as conventional OOD detection techniques on 1) an extensive suite of synthetic text-based domains, 2) simulated and real-world closed-loop quadrotor experiments resembling a drone delivery service, and 3) careful recreations of recent real-world failure modes of autonomous vehicles in the CARLA simulator [11].We conclude that the use of FMs not only presents a promising direction to significantly improve the robustness of autonomous robotic systems to out-of-distribution scenarios, but also that their real-time integration within dynamic, agile robotic systems is already practically feasible.</p>
<p>Organization: We first discuss related work in §II and formalize the problem setup in §III.Then, we present our approach in §IV and evaluate our method in §V.Finally, we conclude and provide a future outlook in §VI.In addition, we include a full overview of the notation and conventions used in this paper in Appendix A.</p>
<p>II. RELATED WORK</p>
<p>Out-of-Distribution Robustness: The fact that learningbased systems often behave unreliably on data that is dissimilar from their training data has been extensively documented in both the machine learning and robotics literature [14,37,33,45].Approaches to address the subsequent challenges broadly fall into two categories [45]: First are methods that strengthen a model's performance in the face of distributional shift.For example, through robust training (e.g., [41]) or by adapting the model to changing conditions (e.g., [16,8]).Second are so called out-of-distribution detection algorithms [42,40], that aim to detect when a given model is unreliable, e.g., by computing the variance of an ensemble [25] or computing energy scores [29].Recent work has shown the merits of generalist FMs like LLMs in both domains: Studies have shown that zero-shot application of a FM (e.g., in [54], the authors apply CLIP zero-shot on ImageNet), vastly improves OOD generalization over previous approaches, like distributionally robust training [31,54,6].In addition, existing OOD detection methodologies and their application within robot autonomy stacks are tailored to detect conditions that compromise the reliability of individual components of an autonomy stack, like whether a perception system's detections are correct [39,13,36,46].Instead, recent work showed that LLMs may provide a more general mechanism to detect context dependent safety hazards, especially those that are hard to measure with predefined performance metrics [12].For example, an autonomous EVTOL may monitor the quality of the vision system's landing pad location estimate, but even if the EVTOL has high confidence that it can land successfully, the outcome of landing on a building that is on fire can have profound negative consequences.However, despite the attractive properties of LLMs, these works do not propose practical strategies to integrate them in closed-loop.Therefore, we propose a closed-loop control framework that can both use the LLM to identify unseen anomalies and strengthen performance in the presence of rare failure modes.</p>
<p>Foundation Models in Robotics: The integration of large lanugage models (LLMs) and, more broadly, foundation models (FMs) into robotics has sparked considerable interest due to their proficiency in managing complex, unstructured tasks that demand sophisticated reasoning skills.These models have been instrumental in bridging the gap between natural language instructions and the execution of physical actions in the real world.Various approaches utilizing these models have been developed for online use in applications in areas such as manipulation [19], navigation [43], drone flight [9], and longhorizon planning [5,28].FMs have also been used to define reinforcement learning reward functions [58], generate robot policy code [27], or create additional training data [56,57,2].</p>
<p>However, the issue of response time associated with FMs has not been a focal point in these studies.The aforementioned online methods predominantly rely on a quasi-static assumption, implying that the timing of the robot's actions is not critical.This assumption allows the system the luxury of time to consult the LLMs and await their responses without urgency.Conversely, the latter methods either operate offline or utilize LLMs in manners that are similarly insensitive to response time.</p>
<p>As such, existing work demonstrates limited dynamic reactivity of the policy, which is essential for fast-moving, agile robots like quadrotors.These robots can quickly find themselves in situations where a delayed response can result in an unavoidable crash.To mitigate this issue, our approach specifically considers the delays introduced by the reasoning process.We enhance reactivity by implementing a more rapid anomaly detection system, thereby reducing the risk of crashes by allowing for timely corrective actions.</p>
<p>Accelerating Inference: It is well-recognized that increasing FM capabilities are accompanied with increasing computational cost and inference latency.As such, substantial effort is being dedicated to the acceleration of these models, of which several popular strategies have emerged such as model distillation [15,17], quantization [20,55], and parameter sparsification [49,34].Ultimately, these approaches improve the cost of the forward pass through a transformer model, but do not address the fact that LLMs typically need to generate long sequences of outputs to reason towards the correct decision [53], a process unlikely to run in real-time for time-sensitive tasks.Querying remotely hosted models on large-scale hardware (e.g., GPT-4 [1]) is a potential solution if computation constraints become too stringent for a system to perform onboard FM inference, yet network conditions may incur inconsistent and potentially significant delays, and connectivity may be unreliable for in-the-wild deployments.</p>
<p>III. PROBLEM FORMULATION</p>
<p>In this work, we consider a robot with discrete time dynamics
x t+1 = f (x t ,u t ),(1)
where x t ∈ R n represents the robot's state, and u t ∈ R m is the control input.Nominally, we aim to minimize some control objective C that depends on the states and inputs, subject to safety constraints on the state x t ∈ X ⊆ R n and input u t ∈ U ⊆ R m .For example, a quadrotor's state consists of its pose and velocity (estimated from e.g., GPS, visual-SLAM, and IMUs), and its objective may be to minimize distance to a landing zone subject to collision avoidance constraints.</p>
<p>In addition to the state variables tracked by the nominal control loop, the robot receives an observation o t ∈ O at each timestep, which provides further contextual information about the robot's environment.Our goal is to design a runtime monitor that interferes with the nominal system to avoid system-level safety hazards, which may depend on environmental factors not represented in the robot's state x t .For example, a quadrotor cannot safely land on a landing zone covered in burning debris even if the nominal control stack has the ability to do so.In the spirit of [12], we refer to such events as semantic failure modes, as they do not necessarily constitute violation of precise state constraints X , but instead depend on the qualitative context of the robot's task.</p>
<p>Further, we assume that we have access to a dataset D nom = {o i } N i=1 of nominal observations wherein the robot was safe and reliable.Conceptually, D nom corresponds to the operational data of a notionally mature system, and may consist of data used to train the system or of previously collected deployment data.This data will overwhelmingly contain mundane scenarios where the robot performs well; our monitoring framework is targeted instead at the challenging, extremely rare corner cases that are unlikely to have been recorded before and threaten the robot's reliability.</p>
<p>In the event that a failure mode of the nominal autonomy stack is imminent, we must select and engage a safetypreserving intervention.For example, we may choose to land the quadrotor in another open landing zone like a grassy field.To this end, we follow [46] and we assume that we are given a number of recovery regions X 1 R ,X 2 R ,...,X d R ⊆ X , control invariant subsets of the state space that correspond to high-level safety interventions.For example, X i R may represent the alternate landing zone.By planning a trajectory to the appropriate recovery set, potential safety hazards can be avoided.As shown in [46], such sets can both be hand defined up-front and identified using reachability analysis.</p>
<p>IV. PROPOSED APPROACH</p>
<p>It is virtually impossible to account for all the corner-cases and semantic failure modes that a system may experience through a standard engineering pipeline.Even if we train e.g., classifiers to detect obstructions on landing zones, there may always remain a class of semantic failure modes that we have not accounted for.Instead, we propose to leverage generalist foundation models to detect and reason holistically about a robot's environment.We first present our FM-based monitoring approach, after which we construct a planning algorithm that accounts for the latency that FM-based reasoning may induce.</p>
<p>A. Runtime Monitor: Fast and Slow Reasoning</p>
<p>To detect and avoid semantic failure modes, we propose a two-stage pipeline.The first is the detection of anomalies, simply defined as conditions that deviate from the mundane, nominal experiences where we know that our notionally mature system is reliable.The second is slower reasoning about the downstream consequence of an anomaly, if detected, towards a high-level decision on whether a safety-preserving intervention should be executed.We refer to Appendix B for a brief introduction to anomaly detection used hereafter.</p>
<p>Fast Anomaly Detection: To detect anomalies, we need to inform a FM of the context within which the autonomous system is known to be trustworthy.The prior, nominal experiences of the robot serve as such grounding.We construct an anomaly score function s(o t ,D nom ) ∈ R to query whether a current observation o t differs from the previous experiences in D nom .We do not require any particular methodology to generate the score, we just require that scoring an observation is computationally feasible in real-time; that is, within a single time step.</p>
<p>This work emphasizes the value of computing anomaly scores using language-based representations, which we show capture the semantics of the observation within the context of the robot's task in §V.To do so, we first create a cache of embedding vectors D e = {e i } N i=1 where e i = ϕ(o i ) ∈ R e for each o i ∈ D nom by embedding the robot's prior experiences offline using an embedding FM ϕ.Then, at runtime, we observe o t , compute its corresponding embedding e t , and compute an anomaly score s(e t ;D e ) using the vector cache.We investigate several simple score functions (see Appendix D3 for a full list), each of which roughly measures a heuristic notion of difference with respect to D nom .For example, the simplest metric uses the maximum cosine similarity with respect to samples in the prior experience cache, s(e t ;D e ) := − max ei∈De e T i e t ∥e i ∥∥e t ∥ , which, in effect, retrieves the most similar prior experience from D e to construct the score.Intuitively, this approach measures whether anything similar to the current observation has been seen before.Finally, to classify whether an observation should be treated as nominal or anomalous, we can calibrate a threshold τ ∈ R as the α ∈ (0,1) quantile of the nominal prior experiences,
τ = inf q ∈ R : |e i ∈ D e : s(e i ;D e {e i }) ≤ q| N ≥ α ,(2)
i.e., the smallest value of q that upper bounds at least αN nominal samples.Note that for nominal embeddings, we must compute the anomaly score s in a leave-one-out fashion, since s(e i ;D e ) = −1 for e i ∈ D e .Determining the threshold τ using empirical quantiles as in ( 2) is a standard approach [39], but could be extended in future work to make precise guarantees on false positive or negative rates using recent results in conformal prediction [3,30].Slow Generative Reasoning: Once we detect an anomaly, we trigger the autoregressive generation of an LLM to generate a zero-shot assessment of whether we need to engage any of the interventions associated with the recovery sets X 1 R ,...,X d R ( §III) to maintain the safety of the system.The value of this approach is that the LLM's internet-scale pretraining data allows it to generate outputs that resemble the generalist common sense reasoning that a human operator is likely to suggest, as a result, making superior decisions on OOD examples, on which existing task-specific learning algorithms are notoriously unreliable.</p>
<p>To do so, we follow [12] in using a VLM to convert the robot's current visual observation into a text description of the environment.We simply encode this scene description into a prompt that provides context on the monitoring task, as illustrated in Fig. 3.We then parse the resulting output string to yield a classification y ∈ {0,1,...d} on whether the anomaly does not present a hazard and the system can continue it's nominal operation (y = 0), or whether we should engage intervention y ∈ {1,...,d} and steer the state into recovery set X y R .As we illustrate in our experiments, the recovery sets naturally correspond to high-level behaviors (e.g., landing in a field), which facilitates prompt design.We use the shorthand w(o t ,Y) to denote the output of the slow reasoner when given observation o t and a (sub)set of intervention strategies Y ⊆ {1,...,d}.</p>
<p>Whether inference is run onboard or the model is queried remotely over unreliable networks in the cloud, we must account for the latency that autoregressive reasoning introduces.For example, a fast moving vehicle may collide with an anomalous obstacle if it's reaction time is too slow.Therefore, we account for the LLM's compute latency by assuming that it takes at most K ∈ N &gt;0 timesteps to receive the output string from the slow reasoner.It is usually straightforward to identify the value of K in practice, since we prompt the model to adhere to a strict output template that tends to stabilize the length of the output generations.Alternatively, as we describe in §V-C and Appendix I, a simple field-test can be sufficient to identify an upper bound on typical network latency.</p>
<p>B. Planning a Tree of Recovery Trajectories</p>
<p>We control the robot's dynamics (1) in state-feedback using a receding horizon control strategy that 1) minimizes the nominal control objective along a horizon of T &gt; K timesteps, while 2) maintaining a set of d recovery trajectories that each reach one of the respective recovery sets X i R within the horizon T .The goal of this approach is to ensure that the high-level safety interventions provided to the slow reasoner can be executed.Additionally, it is essential that these options remain feasible throughout the K time steps it takes the monitor to decide on the most appropriate choice.Otherwise, a fast moving robot may, for example, no longer be able to stop in time to avoid a collision.To this end, we solve the following finite-time optimal control problem online, which maintains a consensus between the recovery trajectories for K timesteps:
J t (Y,K,T ) = minimize {x i t:t+T +1|t ,u i t:t+T |t } i∈Y∪{0} C(x 0 t:t+T +1|t ,u 0 t:t+T |t ) s.t. x i t+k+1|t = f (x i t+k|t ,u i t+k|t ) u i t+k|t ∈ U x i t+k|t ∈ X x i t|t = x t x i t+T +1|t ∈ X i R ∀i ∈ Y u i t|t = u 0 t|t ∀i ∈ Y u i t:t+K|t = u j t:t+K|t ∀i,j ∈ Y(3)
Here, the notation x i t+k|t indicates the predicted value of variable x at time t+k computed at time t for each trajectory i ∈ Y ∪{0}.The MPC in (3) optimizes a set of |Y|+1 trajectories.The first corresponds to a nominal trajectory x 0 t:t+T +1|t plan that minimizes the control objective and a set of |Y| recovery trajectories that each reach their respective recovery set X i R within T timesteps.In addition, the MPC problem (3) includes two consensus constraints, one associated with the fast anomaly detector and the other with the slow reasoner.First, by fixing consensus along the first input of the nominal trajectory and all the recovery trajectories, we ensure that the set of feasible interventions is non-empty during nominal operation.The second fixes consensus for K timesteps along the set of recovery trajectories, in effect generating a branching tree of recovery trajectories.If we then use the fast anomaly detector to both trigger execution of the first K actions of the recovery trajectories and the slower reasoning, we ensure that the options we provide to the slow reasoner are still available when it returns its output.</p>
<p>We summarize this methodology in Algorithm 1, which guarantees that we reach the recovery set chosen by the slow reasoner within at most T +1 timesteps after detecting an anomaly: Theorem 1. Suppose that at t = 0, the MPC in (3) is feasible for some set of recovery strategies Y ⊂ {1,...,d}, i.e., that J 0 (Y,K,T ) &lt; ∞.Then, the closed-loop system formed by (1) and Algorithm 1 ensures the following: 1) We satisfy state and input constraints x t ∈ X , u t ∈ U for all t ≥ 0. 2) At any time t ≥ 0, there always exists at least one safety intervention y ∈ {1,...,d} for which the MPC (3) is feasible.3) If the slow reasoner w, triggered at some time t anom &gt; 0, chooses an intervention y ∈ {1,...,d}, then for all t ≥ t anom + T + 1 it holds that x t ∈ X y R .Proof: See Appendix H.The emergent behavior of Algorithm 1 is that once the fast anomaly detector issues a warning regarding an unusual observation, the robot will balance progress along the nominal trajectory and jointly maintaining dynamic feasibility of the fallback options available at t anom .This generally leads the robot to slow down to preserve its options, thereby providing the slow reasoner with time to think.Upon reaching a decision from the slow reasoner, the robot either transitions back to nominal operations, if the observation is not hazardous, or engages the selected safety intervention and commits the robot to the associated recovery set.While this scheme does not explicitly ensure that multiple strategies remain available to the robot throughout nominal operation (though at least one will remain so), we find in a practical setting (see §V-B, §V-C) that multiple simple hand-designed intervention strategies remain persistently feasible.Still, methods for dynamically identifying and selecting recovery regions present an exciting avenue for future work.</p>
<p>V. EXPERIMENTS</p>
<p>Having outlined our approach, we conduct a series of experiments to test the following five hypothesis: H1 By quantifying semantic differences of observations with respect to the prior experience of a system, our fast embedding-based anomaly detector performs favorably to generative reasoning-based approaches.H2 Embedding-based anomaly detection does not necessitate the use of high-capacity generative models; small models incurring marginal costs can be used.H3 Once an anomaly is detected, generative reasoning approaches can effectively deduce whether the anomaly warrants enacting safety-preserving interventions.H4 Our full approach, which unifies embedding-based anomaly detection and generative reasoning-based anomaly assessment, can be integrated in a broader robotics stack for real-time control of an agile system.H5 Additional forms of embeddings, including those from vision and multi-modal models, offer a promising future avenue for end-to-end anomaly detection.Experiment Rationale: We run four main experiments.The first experiment ( §V-A) tests the performance of our fast anomaly detector in three synthetic (i.e., text-based) robotic environments.We then evaluate the slow generative reasoner for the assessment of detected anomalies on two of these environments.The second experiment ( §V-B) evaluates our full approach (integrating the runtime monitor with the MPC fallback planner) in a simulation of real-time control of an agile drone system.The third is a full-stack experiment on real quadrotor hardware, including a timing breakdown for each component in our approach running on a Jetson AGX Orin module, thereby demonstrating viability for hardware deployment.The fourth experiment evaluates whether our runtime monitor transfers to a realistic, semantically rich self-driving environment, where we investigate the use of both language and multi-modal embeddings for anomaly detection.</p>
<p>All code used in our experiments, including scripts to generate the synthetic datasets and prompt templates, can be found through our project page at https: //sites.google.com/view/aesop-llm.In addition, we provide a brief description of our prompting strategy in Appendix G.</p>
<p>A. Synthetics-Manipulation, Autonomous Vehicles, VTOL</p>
<p>We construct three synthetic domains to support our analysis: a Warehouse Manipulator domain, an Autonomous Vehicle domain, and a Vertical Take-off and Landing (VTOL) Aircraft domain.Each domain consists of scenarios in which a notionally mature autonomous robot may (or may not) encounter a safety concerning observation during its typical operations.The robots' observations take the form of a collection of concepts, which we define as one or more objects and their semantic relationships.For example, in the VTOL domain, both "ice" and "helipad" represent concepts of a single object class, whereas "icy helipad" represents a third concept formed by their conjunction.We follow definition of anomalies given in §IV-A: observations that, in the context of a given task, deviate from the robots' nominal experiences.Thus, anomalies may not pose safety risks but must still be identified for further analysis.</p>
<p>We briefly describe the synthetic domains below:</p>
<p>• Warehouse Manipulator (WM): A mobile WM robot performs the task of "sorting objects on a conveyor belt."Observations contain concepts sampled from a predefined set of conveyor belt (e.g., a package) and surrounding environment (e.g., a storage shelf) objects.Anomalies consist of hazardous objects on the conveyor belt (e.g., a leaking bleach bottle) or object combinations in the surrounding environment (e.g., two forklifts in collision).</p>
<p>The dataset contains 1551 scenarios.• Autonomous Vehicle (AV): An AV operating as a taxi service performs the task of "driving to a set destination."Observations contain concepts sampled from a predefined set of task-relevant (e.g., a car, bus, or traffic light) and task-irrelevant (e.g., an airplane in the sky) objects.</p>
<p>Anomalies consist of unusual task-relevant objects (e.g., a blank speed limit sign) or combinations (e.g., a traffic light on a truck).The dataset contains 840 scenarios.• Vertical Take-off and Landing (VTOL): A VTOL aircraft operating as an urban air taxi performs one of two tasks: "flying toward a set destination" or "landing on a designated building."Observations contain concepts sampled from a predefined set of flying objects, landing zones, and ground regions.Anomalies consist of unanticipated flying objects (e.g., a large swarming flock of birds) and/or landing zones (e.g., a building rooftop on fire).The dataset contains 18400 scenarios.The synthetic domains vary in terms of size and complexity, with WM being the simplest, and VTOL being the most challenging.Complexity is determined by extent to which anomalous concepts differ from the nominal experiences of the robot3 .</p>
<p>1) Fast Reasoning for Anomaly Detection (H1, H2): The role of the anomaly detector is to analyze the robot's observations and identify whether, due to the presence of an atypical object or concept, an observation qualifies as an anomalous.</p>
<p>Methods: We evaluate our fast anomaly detector with nine language models, varying in size and function: BERT-base (110M) and BERT-large (336M) uncased [10], Sentence Transformer MPNet (110M; BERT-base architecture trained for embeddings) [47,38], completion and instruction-tuned Llama 2 models (7B) [51], Mistral (7.11B) [21,52], and three OpenAI embedding models (parameters not disclosed).The choice of score function s(e t ;D e ) (e.g., cosine similarity, top-k scoring, Mahalanobis distance) did not yield significant performance variation.Thus, we only report results for top-5 scoring and refer to Appendix D for extended results.</p>
<p>Baselines: Our baselines consist of generative reasoning with GPT-4, queried to classify the robot's observation as "nominal" or "anomalous."We consider a two variants: single-token (ST) prediction and chain-of-thought reasoning (CoT).For CoT, GPT-4 first reasons over each concept in the observation before outputting an overall anomaly classification.</p>
<p>We use prompts with identical prefixes across all methods, making minor modifications to, e.g., elicit CoT reasoning.For metrics we report accuracy, setting the detection threshold at the 95-th quantile (( 2)) of the scores in the nominal dataset.</p>
<p>Results &amp; Analysis: The results are shown in Fig. 2. The first row shows the accuracy of our fast anomaly detectors with increasing sample size (i.e., the size N of the embedding cache D e = {e i } N i=1 ) drawn IID from the full dataset of nominal observations D nom .The second row is identical to the first, except, showing accuracy with increasing percent of concept coverage; that is, the percent of nominal concepts contained in the embedding cache D e used to construct our detector.</p>
<p>Comparing the top and bottom rows for each domain, we observe that performance increases logarithmically with relative sample size, but only linearly with increasing concept coverage.This indicates that our fast anomaly detector scales favorably with the diversity of nominal concepts represented in the robot's experience as opposed to the shear scale of experience.Provided with sufficient concept coverage, our anomaly detector clearly outperforms the single-token and CoT generative reasoning baselines with GPT-4, validating our first hypothesis H1.</p>
<p>Comparing among language models, we find that the performance of models does not strictly correlate with their size, but rather, depends on their training data and strategy.For example, Sentence Transformer MPNet, a 110M parameter model trained for embeddings, often outperforms BERT Large (336M) and the OpenAI embedding models, and even performs comparatively to the Llama 2 (7B) and Mistral (7.11B) models.This validates our second hypothesis H2.</p>
<p>The key advantage of our anomaly detectors is that they ground the analysis of observations in the embedding space of previously observed concepts.It is unreasonable to attempt to ground the generative baselines in such a way due to their limited context windows, among other challenges (e.g., recency bias [59]).Moreover, we see that GPT-4's performance gradually decreases as the complexity of the environments  increase (e.g., from AV to VTOL), and as such, we may expect further performance drops in real-world settings.</p>
<p>2) Slow Reasoning for Anomaly Assessment (H3): Once an anomaly has been identified, it is the role of the slow reasoner to assess whether or not the anomaly warrants the enactment of safety-preserving interventions.Recall, we use LLMs for their generalist knowledge-acquired through internet-scale pretraining-to infer the need for a fallback on an observation identified as dissimilar from the robot's previous experiences.We evaluate the ability of LLMs to assess anomalies in the VTOL domain and predict one of two options: whether the VTOL should 1) "continue" it's nominal operation or 2) "fallback," enacting one of several listed safety-preserving interventions.To do so effectively, the LLM must differentiate between safety-redundant (e.g., a plane on a known flight path) and safety-concerning (e.g., a fighter jet) anomalies.For this experiment, we evaluate four LLMs: Llama 2 (7B) single-token prediction, GPT-3.5 Turbo CoT, GPT-4 single-token prediction, and GPT-4 CoT.As before, the CoT approach must first assess the safety risk each concept contained in the observation before outputting a fallback classification.</p>
<p>Method</p>
<p>We report true positive rate (TPR), false positive rate (FPR), and accuracy.A true positive corresponds to the LLM correctly engaging a fallback intervention on a safety-concerning anomaly, while a true negative corresponds to correctly dismissing a safety-redundant anomaly.</p>
<p>The results are shown in Table I.We observe that in these out-of-distribution scenarios, model capacity is an important consideration, with the GPT variants clearly outperforming the Llama 2 (7B) baseline.As expected, we also find that CoT reasoning yields a notable improvement in overall classification accuracy (e.g., 11% for GPT-4) and reduces the number of false positives.These findings are corroborated in the manipulation domain (Table VII).This validates our third hypothesis H3.</p>
<p>B. Full Stack-Quadrotor Simulation (H4)</p>
<p>Here, we demonstrate the efficacy of our framework, from anomaly detection and LLM reasoning to closed-loop control with Algorithm 1, on an example of a quadrotor delivering a package.In this simulation, the quadrotor's task is to fly toward and subsequently land at a target location.To simulate a variety of safety hazards and instantiate the fast anomaly detector,
M = " &gt; A A A B + H i c j V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U R q S 6 L b l x W s Q 9 o Y 5 h M J + 3 Q y S T M T I Q a 8 i V u X C j i 1 k 9 x 5 9 8 4 a b t Q U f D A h c M 5 9 3 I P J 0 g 4 U 9 p x P q z S 0 v L K 6 l p 5 v b K x u b V d t X d 2 O y p O J a F t E v N Y 9 g K s K G e C t j X T n P Y S S X E U c N o N J h e F 3 7 2 j U r F Y 3 O h p Q r 0 I j w Q L G c H a S L 5 d H U R Y j w n m W S / 3 r 2 9 d 3 6 6 5 d W c G 9 D e p w Q I t 3 3 4 f D G O S R l R o w r F S f d d J t J d h q R n h N K 8 M U k U T T C Z 4 R P u G C h x R 5 W W z 4 D k 6 N M o Q h b E 0 I z S a q V 8 v M h w p N Y 0 C s 1 n E V D + 9 Q v z N 6 6 c 6 P P M y J p J U U 0 H m j 8 K U I x 2 j o g U 0 Z J I S z a e G Y C K Z y Y r I G E t M t O m q 8 r 8 S O s d 1 t 1 F v X J 3 U m u e L O s q w D w d w B C 6 c Q h M u o Q V t I J D C A z z B s 3 V v P V o v 1 u t 8 t W Q t b v b g G 6 y 3 T 6 A 9 k x U = &lt; / l a t e x i t &gt; X 1 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D X g h + H g / x X 0 5 1 W n 9 5 b a p v n T r F I c = " &gt; A A A B + H i c j V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U p 0 r o s u n F Z x T 6 g j W E y n b R D J 5 M w M x F q 6 J e 4 c a G I W z / F n X / j p O 1 C R c E D F w 7 n 3 M s 9 n C D h T G n H + b A K K 6 t r 6 x v F z d L W 9 s 5 u 2 d 7 b 7 6 g 4 l Y S 2 S c x j 2 Q u w o p w J 2 t Z M c 9 p L J M V R w G k 3 m F z k f v e O S s V i c a O n C f U i P B I s Z A R r I / l 2 e R B h P S a Y Z 7 2 Z f 3 1 b 8 + 2 K W 3 X m Q H + T C i z R 8 u 3 3 w T A m a U S F J h w r 1 X e d R H s Z l p o R T m e l Q a p o g s k E j 2 j f U I E j q r x s H n y G j o 0 y R G E s z Q i N 5 u r X i w x H S k 2 j w G z m M d V P L x d / 8 / q p D s + 8 j I k k 1 V S Q x a M w 5 U j H K G 8 B D Z m k R P O p I Z h I Z r I i M s Y S E 2 2 6 K v 2 v h E 6 t 6 t a r 9 a v T S v N 8 W U c R D u E I T s C F B j T h E l r Q B g I p P M A T P F v 3 1 q P 1 Y r 0 u V g v W 8 u Y A v s F 6 + w S h w Z M W &lt; / l a t e x i t &gt; X 2 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o V Q z T b i z F n R Y m C m H k M 0 v Y L / I v b w = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b p 0 M 1 g E V y U p U l 0 W B H F Z x T 6 g j W E y n b Z D J 5 M w M x F q 6 J e 4 c a G I W z / F n X / j p M 1 C W w 8 M H M 6 5 l 3 v m B D F n S j v O t 1 V Y W 9 / Y 3 C p u l 3 Z 2 9 / b L 9 s F h W 0 W J J L R F I h 7 J b o A V 5 U z Q l m a a 0 2 4 s K Q 4 D T j v B 5 C r z O 4 9 U K h a J e z 2 N q R f i k W B D R r A 2 k m + X + y H W Y 4 J 5 2 p 3 5 d w 8 1 3 6 4 4 V W c O t E r c n F Q g R 9 O 3 v / q D i C Q h F Z p w r F T P d W L t p V h q R j i d l f q J o j E m E z y i P U M F D q n y 0 n n w G T o 1 y g A N I 2 m e 0 G i u / t 5 I c a j U N A z M Z B Z T L X u Z + J / X S / T w 0 k u Z i B N N B V k c G i Y c 6 Q h l L a A B k 5 R o P j U E E 8 l M V k T G W G K i T V c l U 4 K 7 / O V V 0 q 5 V 3 X q 1 f n t e a V z n d R T h G E 7 g D F y 4 g A b c Q B N a Q C C B Z 3 i F N + v J e r H e r Y / F a M H K d 4 7 g D 6 z P H 5 6 Q k x c = &lt; / l a t e x i t &gt; X 2
R Fig. 3: Closed-loop trajectory of a quadrotor using the AESOP algorithm.The figure represents a snapshot of the quadrotor at t = 2.5s: The trajectory until time t is in black.The nominal trajectory plan is shown in blue, with a blue dot denoting the first consensus constraint in (3).The overlapping recovery trajectory plans, up to the consensus horizon corresponding to the LLM latency K, are in orange.The recovery trajectory plans deviate after K, shown in red, and they each reach their respective recovery region (in green).The blue text callout shows how the fast anomaly detector issues a warning and triggers the slow reasoner at t = 2.5s.The red callout shows the response from the slow reasoner, which the LLM returns within the K consensus timesteps in the recovery plans.</p>
<p>we recycle the VTOL synthetic observations and show the quadrotor anomalous observations at random time intervals.In this simulation, we encode the location of two landing regions, a parking lot and an open field, as polytopic state constraints representing X 1,2 R .To instantiate the MPC in (3), we use a fixed dynamics model discretized at a timestep of dt = .1s,with a planning horizon of T = 4s.We assume that the slow reasoning LLM may take up to K = 1.5s to return an output, a number consistent with the latency of cloud querying GPT-4 reported in [50] using a conversational chat-based prompting approach.</p>
<p>In Fig. 3, we show a snapshot of the closed-loop trajectory of the quadrotor as it flies towards the landing zone.Fig. 3 shows that the AESOP MPC (3) plans two recovery trajectories that safely abort the control task and land the drone in their respective recovery regions, while still allowing the quadrotor to make progress towards its nominal objective.Furthermore, the recovery trajectories are aligned for the first K timesteps, budgeting time for the LLM to output which, if any, of the recovery trajectories should be executed.The trajectories in Fig. 3 show that the drone descends and slows down during the first K timesteps of the recovery trajectories, thereby explicitly budgeting time for the LLM to reason.</p>
<p>While we only show a single example in Fig. 3 to illustrate the qualitative behavior of our method, we include additional plots in Appendix E and videos of closed-loop trajectories on the project page to more extensively demonstrate our Naive MPC FS-MPC [46]   approach.Furthermore, Table II shows the results of a quantitative ablation over a set of 500 scenarios and compares AESOP with 1) the fallback-planning method in [45] (which ignores the runtime monitor's latency), and 2) a naive planner that only tries to compute a recovery plan post-hoc after detecting a dangerous event.We refer to Appendix E for a detailed description of these experiments and baselines.</p>
<p>While the FSMPC algorithm mildly improves over the naive baseline, Table II showcases the impact of accounting for the slow reasoner's latency within the control design.Moreover, throughout the closed-loop trajectory in Fig. 3, the average speed of the quadrotor is around 2.5m/s, demonstrating that our framework allows for dynamic control of the robot while leveraging the slower LLM to improve safety in a reactive, real-time manner.This supports our fourth hypothesis H4.Furthermore, we conduct hardware experiments with a physical quadrotor equipped with a downward facing camera (Intel Realsense D435).The quadrotor's nominal goal is to land on a designated red box amidst a cluttered environment.As shown in Fig. 4, the ground is scattered with various objects, such as a bicycle tire, a soccer ball, and a drill, which are inconsequential to the landing task.Fig. 4 also shows the quadrotor's two recovery strategies to avoid failures when e.g., another quadrotor has landed on the red box: 1) landing at an alternative landing site and 2) to hover in a designated holding zone.</p>
<p>In order to construct the embedding cache for the fast detector, we first record images by flying the drone in a circular pattern above the operational area with a nominal clutter of objects on the ground.We use the open vocabulary object detector OWL-ViT [32] to extract context-aware descriptions of visible objects (e.g.,"on the red box" or "on the ground") from the image observations, which we then use to construct prompts for the embedding model (MPNet) and generative reasoner (GPT-3.5-Turbo).</p>
<p>We evaluate our system on the following three scenarios.For more details on these scenarios, see Appendix I and the videos on the project page.</p>
<ol>
<li>
<p>Nominal Operation: There are no obstructions on the red box, so the quadrotor lands normally despite the clutter.</p>
</li>
<li>
<p>Consequential Anomaly: We consider two variants of this scenario.In the first, another quadrotor has already landed on the red box, necessitating a diversion to the blue box for landing.In the second, other quadrotors occupy both red and blue boxes, necessitating a diversion to the holding zone.</p>
</li>
<li>
<p>Inconsequential Anomaly: A previously unseen object (specifically, a keyboard) on the ground triggers the fast anomaly detector, after which the LLM correctly decides to proceed with landing at the nominal site.</p>
</li>
</ol>
<p>In addition, we evaluate the computational cost of our pipeline on our hardware platform, an Nvidia Jetson AGX Orin, which is designed for embedded systems like a quadrotor.Table III shows that the OWL-ViT detection parsing and MPNet embedding computation can jointly run at 18.8Hz.This ensures that AESOP (Algorithm 1) can comfortably operate at approximately 10Hz4 .We cloud query GPT-3.thought prompting for the slow reasoning process, which, as shown in Table III, has a non-negligible latency.Together with the simulations in §V-B, these hardware results demonstrate that our approach effectively leverages LLMs to improve robot reliability despite their inference costs, thereby validating our hypothesis (H4).We include further hardware results, detailing 1) an additional evaluation of the fast anomaly detector, 2) a analysis of LLM query latencies informing our choice of K, 3) implementation details of the MPC solver in Appendix I.</p>
<p>D. Ablation-Autonomous Vehicles Simulation</p>
<p>We run ablations on self-driving scenarios curated in [12].Here, CARLA was used to generate anomalous observations inspired by documented failure modes of self-driving perception systems 5 .All anomalies in this domain require safetypreserving intervention on the nominal system's operation.Thus, this experiment resembles the synthetic fast reasoning experiments ( §V-A1), but additionally considers high-dimensional RGB observations as inputs to the runtime monitors.</p>
<p>1) End-to-End Reasoning for Anomaly Detection (H5): Given the release of multi-modal models such as GPT-4V, we ablate performance differences between a two-step pipeline and a single-step pipeline.The two-step pipeline constructs a prompt consisting of detections from the open vocabulary OWL-ViT object detector [32] on CARLA images, whereas the singlestep pipeline directly queries GPT-4V to output an anomaly classification based on the image observation.For each of these approaches, we consider both single-token and CoT reasoning.</p>
<p>The results are presented in Table IV.In both text-and vision-based anomaly detection, we corroborate the notion that CoT reasoning facilitates more accurate responses to a complex task compared to single-token reasoning.Furthermore, the results of GPT-4V (relative to GPT-3/4) suggest that the anomaly detection task could be performed end-to-end.</p>
<p>2) End-to-end Embedding-Based Anomaly Detection (H5): For the following embedding evaluations, we use the top performing language embedding models, MPNet (110M) and Mistral (7B), operating on the existing prompts which list detected objects parsed from the output of OWL-ViT.</p>
<p>Additionally, we evaluate image embeddings from a ViT, specifically CLIP [35], that provides direct visual grounding for the task.As in §V-A1, we use top-5 scoring across all methods.</p>
<p>To ensure we fairly compare the expressiveness of purely language-based embeddings with vision-language embeddings, we report results using ground-truth object detections to construct the prompts for the text-based embedding models in Table IV, in the spirit of a more comprehensively engineered system with reliable object detection.We do so because, as noted in [12], the vision model suffers from a real-to-sim domain shift, and sometimes tends to, e.g., characterize nominal observations of stop signs as "images of stop signs".We include an ablation showing the impact of vision errors in Appendix F.</p>
<p>First, we observe that language embeddings from the relatively small MPNet model are less capable at discerning anomalies among many nominal observations than Mistral, independent of whether OWL-ViT returns a correct scene description (see Appendix F).</p>
<p>Second, we surpass GPT-4 and GPT-4V with Mistral (roughly 64x larger than MPNet).Besides suggesting that a smaller model's language embeddings are less semantically rich and therefore less capable at capturing the presence of more subtle anomalies, we also show in Appendix F that MPNet's accuracy degrades as the number of objects in an observation increases, whereas Mistral's accuracy remains consistent when the number of detections in an image varies.</p>
<p>Third, we note that the use of CLIP embeddings derived directly from the vehicle's RGB observations also achieves high performance, comparable to GPT-4V CoT.This suggests that compute-intensive, CoT reasoning is not necessary to discern the anomalies directly from vision in semantically rich environments.However, we nuance this finding by noting that the observations in the CARLA dataset were constructed by driving a car along simulated routes and placing object assets that trick the vehicle into making unsafe decision along those routes [12].This means that the routes appear both with and without anomalous objects and that episodes wherein the vehicle takes unsafe actions include both nominal and anomalous observations.When we change the calibration strategy to only construct the embedding cache with nominal observations from routes that never pass by anomalous objects (denoted by "Abl." in Table IV), we find that CLIP's FPR increases significantly.This is because CLIP embeddings contain a mix of visual and semantic features [23], and therefore the slight visual novelties in an unseen route are flagged as anomalous even though they are semantically uninteresting.As such, CLIP's limitations are related to the SCOD [44] and Mahalanobis distance [26] baseline OOD detectors (taken from [12]) that wrap the AV's base object detector (DETR [7]).In contrast, as we further ablate in Appendix F, the two-stage detection approach is unaffected by differences in visual appearance.Despite this, and noting that further work should examine how to disentangle semantic and visual features to increase robustness, we argue that our preliminary findings on using multi-modal embeddings directly offers significant promise for streamlining the implementation of our framework.This validates our fifth and final hypothesis H5.</p>
<p>VI. CONCLUSION AND OUTLOOK</p>
<p>In this paper, we presented a runtime monitoring framework utilizing generalist foundation models to facilitate safe and real-time control of agile robotic systems faced with real-world anomalies.This is enabled through a reasoning hierarchy: a fast anomaly classifier querying similarity with the robot's prior experiences in an LLM embedding space, and a slow generative reasoner assessing the safety implications of detected anomalies and selecting the appropriate mitigation strategy.These reasoners are interfaced with a new model predictive control strategy that maintains the feasibility of multiple safe recovery plans.</p>
<p>In extensive experiments, we demonstrate that a) embeddingbased anomaly detection performs favorably to zero-shot generative reasoning with high-capacity LLMs, thanks in part to the grounding afforded by the prior embedding experience of the robot; b) embedding-based anomaly detection attains strong performance even when instantiated with small language models, allowing our method to run onboard computationally constrained robotic systems; c) dual-stage reasoning enables LLMs to operate in the real-time reactive control loop of an agile robot; d) alternative forms of embeddings, such as those obtained from vision-based foundation models, can be used to efficiently detect anomalies in high-dimensional observation spaces.</p>
<p>As such, our work highlights the potential of LLMs and, more broadly, foundation models toward significant increases in the robustness of autonomous robots with respect to unpredictable and unusual out-of-distribution scenarios or tail events.Improving the performance and generality of our framework presents several promising avenues for future research.For example, the impact of LLM inference latencies could be reduced by devising methods to constrain generative reasoning to a fixed word budget, or by using intermediate generations to inform decision-making during the generation process.Further analysis is required on the correctness of fallback plans selected by the LLM, and whether fallbacks can be programmatically determined upon latency timeout.Finally, continual learning based on the delayed anomaly assessment of the generative reasoner could be used to avoid triggering the slow reasoner on non-safety-critical anomalies a second time.</p>
<p>APPENDIX</p>
<p>These appendices contain further details on the experiments in the main body of the paper, additional results and ablations supporting the main hypotheses of the paper, analysis of these supplementary results, and proofs of theoretical results.Besides the results within this document, we also refer the reader to videos of the quadrotor experiments in the quad_videos/ directory of the supplemental material.Furthermore, we include videos describing our approach, prompt templates, and further results on our project page: https://sites.google.com/view/aesop-llm.These appendices are organized as follows:</p>
<p>Contents</p>
<p>B. Background: Anomaly Detection</p>
<p>In essence, an anomaly detector is a classifier h : O → {nominal, anomaly} that maps observations to a detection at runtime.However, limited access to anomalous examples (after all, it is their dissimilarity from prior experiences that makes them anomalous) typically precludes us from training a classifier with an obvious decision boundary using supervised learning [3,40].Instead, anomaly detection algorithms require two steps: First, we must construct a scalar score function s(o) ∈ R from an observation, where a higher score indicates that the sample is "more" anomalous.Second, we need to calibrate a decision threshold τ ∈ R on the score function such that:
h(o) = anomaly if s(o) &gt; τ nominal if s(o) ≤ τ .
We investigate several score functions in this paper and compare their downstream utility in improving the safety and reliability of an autonomous robot.This necessitates instantiating the anomaly detectors with specific thresholds and measuring the accuracy of the subsequent calibrated classifier, since generic measures of a score functions' expressiveness are not guaranteed to capture the overall impact on an autonomy stack.</p>
<p>C. Background: Mechanics of LLMs</p>
<p>The decoder-only LLM architecture typically stacks together large numbers of Transformer modules to construct a mapping from a sequence of input tokens x 0:t to a contextual embedding matrix ϕ(x 0:t ) ∈ R n×t+1 .That is, each input token gets mapped to a corresponding contextual embedding.It is well-known that the contextual embeddings are generally useful for prediction tasks themselves and often exhibit interesting properties.For example, some models are trained with contrastive losses to ensure embeddings with similar semantic meaning cluster closely in the embedding space, enabling retrieval of relevant information via similarity search.However, current research in robotics focuses on using LLMs to generate strings of text through autoregressive generation, i.e., next token prediction.To do so, a linear classification head is typically added onto the output contextual embedding to define a probability distribution over the next token
x t+1 ∼ p llm (x t+1 |ϕ(x 0:t )) := softmax(W out ϕ(x 0:t ) t ), (4)
which is then sampled and appended to the token sequence, after which the next token can be sampled.It should be clear that generating output sequences carries a computational cost that scales superlinearly in the cost of computing an embedding.That is, the zero-shot reasoning capabilities of LLM generation come at a computational cost, whereas direct learning on embeddings requires a source of supervision.</p>
<p>D. Additional Results: Synthetics</p>
<p>The main goal of the synthetics experiments is to analyze the performance characteristics of our fast anomaly detector across robotics domains with diverse observational and task semantics.Here, we extend our synthetics results and analysis ( §V-A) to evaluate three such performance characteristics.First is the embedding-based anomaly detector's quality as measured by area under the receiver operating characteristic (AUROC) curve (Appendix D1).Second is the anomaly detector's sensitivity with respect to varying detection thresholds (Appendix D2).Lastly, we evaluate the anomaly detector's performance with respect to the choice of similarity score function (Appendix D3).</p>
<p>1) Fast Anomaly Detector ROC Analysis: We follow the synthetics evaluation scheme presented in the main results, which compares the performance of our fast anomaly detector over nine language models, using top-5 scoring, with a detection threshold set to the 95-th quantile of the scores in the nominal dataset (( 2)).Instead of reporting accuracy, we now measure performance in terms of AUROC, which more holistically reflects the detector's performance across varying detection thresholds.</p>
<p>The results are shown in Fig. 5.We observe similar performance trends to the previous results (Fig. 2, measured in terms of accuracy), where MPNet closely rivals the top performing 7B parameter models, Mistral and Llama 2, followed by the OpenAI embedding models, and lastly, the BERT models.These trends become increasingly clear as domain complexity increases (i.e., from the Manipulation to VTOL domains, left to right).In the most challenging VTOL domain, we observe that concept coverage is key to attaining strong performance.This is perhaps a byproduct of the more nuanced concept shifts in complex domains, which necessitate comprehensive coverage of nominal concepts to deduce anomalies.Consider how, in the VTOL domain, the anomalous "swarming flock of birds" can be mistakenly interpreted as similar to the nominal "flying bird" without the additional grounding from other nominal concepts such as a "blimp" or "quadcopter."</p>
<p>2) Detection Threshold Analysis: To construct the anomaly detector (Appendix B), we need to select or calibrate a detection threshold τ to differentiate anomalous from nominal observations.Many techniques exist for calibrating detection thresholds, several of which offer specific guarantees on e.g., false positive rates, such as conformal prediction [3].</p>
<p>In this experiment, we show the performance variation of our fast anomaly detector with respect to a range of detection thresholds (i.e., empirical quantiles) in an attempt to capture the sensitivity of our method to, for example, well or poorly calibrated thresholds.We report anomaly detection accuracy on the VTOL domain because, as shown by VTOL's relatively slowly increasing AUROC trends in Fig. 5, we may expect larger variances in performance across thresholds.We evaluate three language models, including OpenAI Ada 002, MPNet, and Mistral (7B), and randomly (IID) sample 80% of the full nominal dataset to construct the embedding cache for the anomaly detector.Results are reported over 5 random seeds.</p>
<p>The results are shown in Table VI.First, we observe a positive relationship between anomaly detection accuracy and threshold quantile across all methods.Once again, MPNet performs nearly identically to Mistral (7B), while OpenAI Ada 002 begins to plateau at the 85-th quantile.Both MPNet and Mistral (7B) consistently outperform generative reasoning with GPT-4 (with the exception of the 75-th quantile).From the relatively small (yet non-negligible) performance improvements among increasing quantiles, we conclude that 1) while our anomaly detector is reasonably robust to the choice of detection  threshold, 2) performance can be improved through the use of more sophisticated calibration techniques.Lastly, all methods produce negligible standard deviations across the random seeds, likely because the sampled embedding cache (80% of the full nominal dataset) provides ∼ 100% coverage over all nominal concepts, which Fig. 5 shows heavily influences performance.</p>
<p>3) Similarity Score Function Analysis: Our fast anomaly detector can be instantiated with an arbitrary choice of similarity score function s(e t ; D e ).For brevity, our main results exclusively featured the use of top-5 scoring.Thus, we conduct an ablation experiment to test the robustness of our approach to the choice of score function.</p>
<p>Our ablation includes top-k scoring for k in {1,3,5,10}, which quantifies the distance between the input embedding e t and the k closest embeddings in the nominal embedding cache D e = {e i } N i=1 , and 2) the Mahalanobis distance score between the current embedding e t and the multivariate Gaussian distribution formed from the mean and covariance of D e = {e i } N i=1 .As before, we experiment on VTOL synthetic due to its size and complexity, evaluating four language models of varying size and function: OpenAI Ada 002, MPNet, Llama 2 Chat (7B), Mistral (7B).</p>
<p>The results are shown in Fig. 6.We first observe that the accuracy difference between all score functions at 100% concept coverage is within approximately 5% across all evaluated language models except OpenAI Ada 002.At first glance, this might suggest that the choice of score function is, to an extent, irrelevant for achieving high detection accuracies.</p>
<p>Upon closer analysis, we see that the top-1 and Mahalanobis score functions perform quite poorly in the low-data regime for most language models, while MPNet is able to utilize Mahalanobis better than others.Overall, we find that top-k for k in {3,5,10} are all strong choices of score functions that demonstrate competitive performance in several data regimes.</p>
<p>4) Safety Assessment in Manipulation Domain: Recall, once an anomaly has been detected by the fast-reasoner, the slower reasoner is tasked with assessing whether the observation requires a safety-preserving fallback.In the main body of the report, we present results for a safety assessment of anomalies in VTOL domain with various SoTA language models.In Table VII, we extend our safety assessment to the manipulation domain.Similar to the trend observed in Fig. 2, safety assessment is significantly easier in the manipulation domain than in the VTOL.Further, we see strong performance gains in using GPT-4 to correctly recognize an inconsequential anomaly in comparison to GPT-3.5.</p>
<p>E. Additional Results: Quadrotor Simulation</p>
<p>In this section, we include additional details about our quadrotor simulation and describe our implementation of the MPC solver used in Algorithm 1 (AESOP).We show the qualitative behavior and improvement of our algorithm in comparison with two baseline methods in Fig. 7 in addition to annotated videos of the trajectories in the supplementary files.We also quantitatively evaluate the rate at which AESOP and the baselines successfully recover in over a set of 500 randomized scenarios.</p>
<p>Embedding</p>
<p>1) Simulation and Implementation Details:</p>
<p>We simulate the full 12-state dynamics of a quadrotor with an arm length of .25m, a mass of 1kg and an inertia matrix J = diag(.45,.45,.7).In contrast with the simulation in §V, where the velocity of the quadrotor was unconstrained, we constrain the drone to move at a maximum of 1.5m/s along each of the principle axes' directions.As shown in green in Fig. 7, we provide the planner with four recovery regions, encoded as polytopic constraints on states, representing potential landing zones.We label the first two, around x ≈ 8m, as grassy fields.We label the third and fourth landing zones around x ≈ 3m as a building rooftop and a parking lot respectively.We use these labels to simulate the EVTOL synthetic in closed loop, abstracting perception into a pure-text observation at each timestep.The quadrotor plans trajectories of length T = 4s using a time discretization of dt = .1sand we assume the slow LLM reasoner takes at most K = 1.5s to output a decision.In these simulations, the goal of the quadrotor is to fly to a goal point, denoted by the blue star in Fig. 7.</p>
<p>We implement the MPC (3) core to the AESOP algorithm in Python using the OSQP [48] solver using linearized dynamics and a quadratic objective.Note that AESOP (Algorithm 1) requires a methodology to select a set of recovery regions to constrain the MPC (3) at each timestep, and the MPC is guaranteed to be feasible for at least one such choice by Theorem 1.In principle, one could select the optimal subset of recovery regions using mixed-integer programming.Here, we adopt the simpler approach proposed in [46] where we solve multiple versions of (3) at each timestep, each associated with a different combination of at least two recovery sets.Then, we select the trajectory plan associated with the feasible solution to (3) with least cost.This allows the quadrotor to dynamically select various safety interventions to maximally make progress towards the goal.</p>
<p>Overall, this implementation runs at approximately 42Hz on the Nvidia Jetson Orin AGX's CPU, with some variance induced by re-initialization of solver warm-starts when the recovery sets change.Combined with the inference latency of the fast anomaly detectors in Table III, this means that the AESOP framework can comfortably run in real-time.</p>
<p>2) Baselines: We compare AESOP to two baselines that also use the slow LLM-based reasoner to select a recovery set on cue of the fast anomaly detector.</p>
<p>Naive MPC: The first is a naive MPC algorithm that only plans a single nominal trajectory towards the goal during nominal timesteps (i.e., when h(o t ) = 0) without maintaining feasible recovery plans.This baseline continues nominal operation until the LLM returns a choice of recovery set, at which point the MPC attempts to plan a recovery trajectory.</p>
<p>Fallback-Safe MPC (FS-MPC) [46]: We base the second baseline on the Fallback-safe MPC proposed in [46].This algorithm maintains several feasible recovery trajectories at all times in a similar fashion to the AESOP algorithm.However, this algorithm does not account for the latency associated with the slow LLM-based reasoner (i.e., setting K = 0 in (3)), and only engages a recovery plan once the LLM returns a decision.</p>
<p>Our implementations of both these algorithms rely on slack variables, so that they return a trajectory plan that minimally violates constraints in case it is dynamically impossible to compute a recovery trajectory that reaches the chosen set.</p>
<p>3) Results: Qualitative Figures: In Fig. 7, we show the qualitative differences in the behavior of AESOP and the baseline methods.</p>
<p>Firstly, Fig. 7c shows the trajectory of the quadrotor using AESOP in an episode where no anomalies occur and therefore, where the fast anomaly detector raises no alarms.As such, Fig. 7c shows that AESOP does not interfere significantly with the nominal operation of the quadrotor: It still reaches the goal location with little impediment.</p>
<p>Secondly, Fig. 7a shows the trajectory of the naive MPC baseline in an episode where the fast reasoner detects an anomaly at t = 3.0s and the slow LLM reasoner returns it's output 1.5s later.The naive MPC assumes that all the recovery regions are always reachable from the current state, nor does it account for the latency of the LLMs reasoning.Therefore, once the LLM outputs that the quadrotor should land at a grassy recovery region (around x = 8m in Fig. 7c), it can no longer plan a dynamically feasible path to the recovery set and crash lands in an unsafe ground region.In contrast Fig. 7f shows that AESOP recovers the robot to the desired safe region.This example showcases that it is necessary to plan multiple trajectories even in nominal scenarios to ensure that the safety-preserving Naive MPC FS-MPC [46]  interventions selected by the LLM can be executed safely.Third, Fig. 7b shows the trajectory of the fallback-safe MPC (FSMPC) algorithm from [46] on the same example as Fig. 7a and Fig. 7f.While the FSMPC algorithm maintains several feasible recovery plans during nominal operations, Fig. 7b shows that this is not sufficient to ensure safe recovery: The FSMPC algorithm does not account for the time delay of the LLM-based reasoner.Instead it continues it's nominal operations until the LLM returns.As shown in Fig. 7b, the feasible recovery strategies changed in the time interval between the detection of the anomaly detector and the output of the LLM.As a result, the FSMPC algorithm tried to find a dynamically feasible recovery trajectory to the grassy areas as instructed by the LLM, but was instead forced to crash land in an unsafe region.In contrast, by accounting for the latency of the LLM, AESOP was able to safely land (i.e., Fig. 7f), thus showing the necessity of accounting for LLM inference latency in control design for dynamic robotic systems.</p>
<p>Finally, Fig. 7d shows the behavior of the AESOP algorithm when the fast anomaly detector detects an anomalous scenario, but the LLM determines the anomaly is inconsequential to the robot's safety and returns the system to nominal operations.The quadrotor descends and slows down during the inference time of the LLM, and continues its flight towards the goal thereafter.As such, Fig. 7d shows that by leveraging the LLM, AESOP minimally impedes goal completion when anomalies are not immediate safety risks.Moreover, Fig. 7f and Fig. 7e show that AESOP safely recovers the system when the anomaly is consequential to safety.</p>
<p>Quantitative Evaluation: We quantitatively ablate the improvement of our proposed approach in comparison with the naive MPC and FSMPC by simulating 500 trajectories with a consequential anomaly appearing at a random timestep.To do so, we uniformly sample an initial condition with zero velocity and rotation in a box with width 2m around (x,y,z) = (10,2,2) and fix the goal state as in Fig. 7.We then uniformly select a timestep at which the anomaly appears in the interval t = [1s,4s].</p>
<p>Table II shows the fraction of scenarios in which AESOP and the baselines safely reach the recovery set chosen by the LLM.By design, AESOP successfully lands the quadrotor in the chosen recovery region each time.While the FSMPC algorithm improves over the naive baseline, Table VIII shows that it is essential to account for the LLM's latency to achieve reliability.</p>
<p>F. Additional Results: Autonomous Vehicle Simulation</p>
<p>In our ablations on self-driving scenarios, we adopt the semantic anomaly dataset presented in [12].This dataset includes two classes of semantic anomalies with multiple instantiations in a set of independent experiments.In nominal experiments, the vehicle approaches a stop sign, as in Fig.(d) Closed loop trajectory of the AESOP algorithm (black).In this trajectory, the fast anomaly detector signals that an anomaly has been detected.Then, the Slow LLM reasoner outputs that the anomaly is inconsequential to the robot's safety, thereby returning the AESOP algorithm to nominal operation.In blue and red we show the respective nominal and recovery plans computed at the timestep that the fast anomaly detector issues a warning.</p>
<p>(e) Closed loop trajectory of the AESOP algorithm (black).In this trajectory, the fast anomaly detector signals that an anomaly has been detected at t = 2.0s.Then, the Slow LLM reasoner selects the appropriate recovery set from the available options.In blue and red we show the respective nominal and recovery plans computed at the timestep that the fast anomaly detector issues a warning.</p>
<p>(f) Closed loop trajectory of the AESOP algorithm (black).In this trajectory, the fast anomaly detector signals that an anomaly has been detected at t = 3.0s.Then, the Slow LLM reasoner selects the appropriate recovery set from the available options.In blue and red we show the respective nominal and recovery plans computed at the timestep that the fast anomaly detector issues a warning.8a, or a traffic light, as in Fig. 8b, in an environment with common-place observations.In anomalous experiments, the vehicle approaches an image of a stop sign on a billboard, as in Fig. 9a, or a truck transporting an inactive traffic light, as in Fig. 9b.In this setting, we aim to use observations gathered from trajectories including nominal stop signs and traffic lights to identify when either anomaly is present in a novel observation.We adapt our anomaly detection pipeline using a two-step approach with language embeddings and a direct end-to-end method with multi-modal CLIP embeddings.</p>
<p>Supplemental to our discussion in the paper's main body, our ablations provide four interesting observations: 1) ground truth scene descriptions are necessary to overcome misclassifications by the object detector, 2) a large model, on the scale of Mistral (7B), is necessary to capture the presence of an anomaly in a semantically rich environment such as self-driving, 3) embeddings from the two-stage pipeline using LM embeddings only capture semantics and do not pick up on visual novelty, whereas multi-modal CLIP embeddings incur false positives on visually novel but semantically nominal observations, and 4) that a smaller model, like MPNet, struggles to correctly detect anomalies as the number of objects in an observation increases.</p>
<p>1) Object Detection vs. Ground-truth Detections: Recall, the two-step pipeline constructs a textual prompt using scene descriptions output by OWL-ViT on each CARLA observation.Our initial experimentation found processing language embeddings from the raw scene descriptions returned by OWL-ViT was insufficient to surpass GPT-4V's CoT accuracy which   Fig. 9: Anomalous observations for each object class in [12]. is demonstrated in Fig. 10.As noted in [12], because CARLA's synthetic visual features represent a distribution shift from the realistic images on which OWL-ViT was trained, OWL-ViT periodically hallucinates object detections, such as the presence of an anomaly when none is present, or misses an anomaly detection entirely.Most commonly, we noticed the OWL-ViT characterized the rendered images as e.g., "an image of a stop sign" or "a picture of a stop sign."Therefore, independent of the model's size, the language embeddings on anomalous scene descriptions are not significantly different from that of nominal observations: at best, we achieve 0.70 mean accuracy with Mistral (7B) which is inferior to GPT-4V CoT.Instead, if we post-process the raw scene descriptions to contain ground-truth detections, then the resultant language embeddings are semantically different between the nominal and anomalous observations.We do this by removing false positive detections and introducing a true positive detection if missed by the detector.We demonstrate this finding in Fig. 10, which shows Mistral (7B) achieves a mean accuracy of 0.94 surpassing GPT-4V CoT by 4% absolute.While post-processing an object detector's output is not feasible at deployment, this ablation serves as a proof of concept for our algorithm in a real-world environment where object detection is presumably reliable.</p>
<p>Also, we notice that when using ground truth scene descriptions, the model's size creates a spread in performance in Fig. 10.Interestingly, with the introduction of ground truth detections, MPNet and BERT-large achieve similar accuracy to Mistral (7B) processing raw scene descriptions.With Mistral (7B), which is 64x and 19x larger than MPNet and BERTlarge, respectively, access to ground truth detections completely unblocks anomaly detection.These results suggest that MPNet and BERT-large, with limited expression due to model size, produce language embeddings that are not semantically rich enough to capture the presence of an anomaly among numerous nominal objects.Therefore, our two-step pipeline requires highfidelity scene descriptions and a sufficiently large language model, on the scale of Mistral (7B), for anomaly detection with high-dimensional observations characteristic of the real world.</p>
<p>2) Calibration Ablation on Withheld Trajectories: The image observations in the CARLA dataset from [12] were constructed by driving a car along simulated routes in several different maps.Some of these routes pass by anomalous objects, e.g., a stop sign on a billboard, which trick the vehicle into making unsafe decision.This means that episodes wherein the vehicle takes unsafe actions include both nominal and anomalous observations, and that the routes appear both with and without anomalous objects.In the main evaluations in Table IV and Appendix F the embedding caches therefore contain nominal embeddings associated with all the routes, i.e., capturing a setting wherein anomalies suddenly appear on roads the AV often drives.As shown in Table IV, the two-stage detectors (using MPNet and Mistral) and the single stage multi-modal detector (using CLIP) perform well at detecting anomalies in this scenario.</p>
<p>Therefore, we also run an ablation wherein we withhold all nominal data from routes wherein anomalies occur when constructing the embedding cache, leaving only routes in which anomalies never occur.the vehicle drives novel routes that contain sporadic anomalies.</p>
<p>As shown in Table IV, the false positive rate of the CLIP embedding-based approach significantly increases in this scenario.This is most likely because the CLIP embeddings contain both visual features (e.g., those suitable for object detection as used in [49]) and semantic features.Therefore, the visual novelty in the previously unseen trajectories causes false positives.In contrast, the two-stage approaches, which first describe the scene with an object detector before prompting an LM embedding model, do not attend to visual features and therefore do not suffer such performance drops.</p>
<p>In [23], the authors argue that multimodal models for robotics should explicitly balance visual and semantic features, and in line with those insights, we argue future work investigating how to differentiate visual and semantic features can make multi-modal anomaly detectors more robust.</p>
<p>3) Accuracy vs. Complexity of Observations: Here, we further investigate the discrepancy in accuracy between the smaller MPNet embedding model (110M) and the larger Mistral embedding model (7B) in Table IV.We do so because on the synthetic tasks in §V-A, where the observations contain descriptions of at most three objects, both models performed more comparably.As shown in Fig. 11, we see that the accuracy of MPNet drops off when the total number of objects within each image observation increases, which largely explains their difference in performance.However, an interesting nuance is that MPNet's accuracy is again comparable to Mistral when there are 7-8 objects in the observation.Fig. 12 supports the hypothesis that this may be because the imbalance between nominal and anomalous images is larger for observations with many objects, so that large numbers of observations may correlate with nominal conditions.Moreover, there are significantly fewer observations with many obstacles.Overall, these results suggest that larger models are needed to reason about the anomalousness of more complex scenes with many objects.</p>
<p>G. Prompting Details</p>
<p>We emphasize that all the prompts used in our experiments can be found in the repositories listed on our project webpage, https://sites.google.com/view/aesop-llm.However, for completeness, we briefly describe our prompting strategy here.For all language-based tasks (i.e., the synthetics in §V-A, the hardware in §V-C, and the text-based evaluations in §V-D), we parse the current task description of the robot and a list of all observed objects into a prompt template.The template first provides a brief description of the robot that is being monitored and defines the monitoring task, followed by the current task and observation.The prompt finally concludes with an instruction on the monitor's output (e.g., a single token nominal/anomaly classification or a chain-ofthought safety assessment).For multi-modal models, we only provide the robot's task.In early experiments, we found that it was necessary to provide the descriptions of the robot and the monitoring task so that the model interprets the observations from the context of the robot's task, whereas e.g., simply embedding an object label or asking an LLM whether an object is anomalous fails to capture its relation to the robot and other objects in the environment.As a concrete example, we use the following template for the synthetic VTOL task in §V-A:  and capabilities of the VTOL?"</p>
<p>H. Proof of Theorem 1</p>
<p>Finally, we prove prove Theorem 1, which establishes the properties of the closed loop system formed by AESOP (Algorithm 1) and (1).To do so, the assumption that X 1 R ,...,X d R are control invariant sets, which we made in the problem formulation (see §III), is critical.Therefore, to make this paper as self-contained as possible, we first reiterate the standard definition of a control invariant set.</p>
<p>Definition 1 (Control Invariant Set [4]).A set X R ⊆ X for the dynamical system (1) subject to state and input constraints
X ⊆ R n , U ⊆ R m is a control invariant set if for every x ∈ X R , there exists a u ∈ U such that f (x,u) ∈ X R .
As shown in our experiments in §V-B, §V-B, it is often straightforward to identify recovery regions that are control invariant: For example, states in which the quadrotor has landed are control invariant.We now restate Theorem 1 and provide its proof, which relies on a recursive feasibility argument [4].</p>
<p>Theorem 1. Suppose that at t = 0, the MPC in ( 3) is feasible for some set of recovery strategies Y ⊂ {1,...,d}, i.e., that J 0 (Y,K,T ) &lt; ∞.Then, the closed-loop system formed by (1) and Algorithm 1 ensures the following: 1) We satisfy state and input constraints x t ∈ X , u t ∈ U for all t ≥ 0. 2) At any time t ≥ 0, there always exists at least one safety intervention y ∈ {1,...,d} for which the MPC (3) is feasible.3) If the slow reasoner w, triggered at some time t anom &gt; 0, chooses an intervention y ∈ {1,...,d}, then for all t ≥ t anom + T + 1 it holds that x t ∈ X y R .Proof: Suppose that the MPC in (3) is feasible for some set of recovery strategies Y ⊆ {1, ... , d} at some time step t &lt; t anom .Let x i,⋆ t:t+T +1|t denote optimal predicted trajectories and let u i,⋆ t:t+T |t be the optimal predicted input sequences associated with a) the nominal trajectory, i = 0, and b) recovery strategies, i ∈ Y, that minimize (3) at time t.Then, it holds that x t ∈ X and u t = u 0,⋆ t|t ∈ U by construction of (3).Furthermore, since we assume that each recovery set X i R is a control invariant set, there exists an input u i t+T +1|t+1 ∈ U such that the input sequence u i t+1:t+T +1|t+1 := [u i,⋆ t+1:t+T |t ; u i t+T +1|t+1 ] and its associated state sequence x i t+1:t+T +2|t+2 := [x t+1 ; x i,⋆ t+2:t+T +1|t ; f (x i,⋆ t+T +1|t , u i t+T +1|t+1 )] satisfy state and input constraints with x i t+T +2|t+1 ∈ X i R for each i ∈ Y.This implies that 1) there exists a set Y ′ ⊆ {1,...,d} with |Y ′ | ≥ 1 for which the MPC (3) is feasible at time t + 1 and 2) we therefore satisfy x t+1 ∈ X and u t+1 ∈ U. Therefore, we have 1) that x t ∈ X and u t ∈ U and 2) there exists at least one safety intervention for which the MPC (3) is feasible for all t ≤ t anom .</p>
<p>Next, suppose that the slow reasoner w, queried at t anom , returns an output after exactly K ′ ≤ K timesteps.In addition, suppose that t anom ≤ t ≤ t anom +K ′ , and that the MPC (3) is feasible at time t for some set of interventions Y ⊆ {1,...,d}.Let k = t−t anom .We therefore have that the control and state sequences u i,⋆ t+1:t+T −k|t and x i,⋆ t+1:t+T +1−k|t for i ∈ Y are feasible for the MPC (3) at t+1 using the same set of interventions Y, since Algorithm 1 ensures we solve (3) with horizon T −k−1 and consensus horizon K −k−1.Since we already proved that ( 3) is feasible at t anom in the preceding paragraph, it therefore holds by induction that 1) x t ∈ X and u t ∈ U, 2) that the MPC (3) is feasible with respect to the set of feasible safety interventions at time t anom , Y tanom , for all t anom ≤ t ≤ t anom +K ′ .Now, we consider the case where the slow reasoner outputs y ∈ Y tanom .The preceding step of the proof then shows that there exists a feasible trajectory for the MPC (3) that reaches the recovery set output by the LLM, X y R where y = w(o tanom ), within T +1−K timesteps.Therefore, by noting that Algorithm 1 continues to shrink the prediction horizon, we have that 1) x t ∈ X and u t ∈ U, 2) that the MPC (3) is feasible with respect to the set of recoveries {y} for all t anom + K ′ ≤ t ≤ t anom + T + 1.Furthermore, we then also have that x tanom+T +1 ∈ X y R .Because we assume X 1 R ,...,X d R are control invariant sets, we then further have that the closed loop system formed by (1) and Algorithm 1 satisfies 1) state and input constraints for all time, 2) that x t ∈ X y R for all t ≥ t anom +T +1.Finally, we consider the case that the slow reasoner decides to return to nominal operaion, i.e., that w(o tanom ,Y tanom ) = 0. Since each X i R is a control invariant set, choosing Y tanom+K ′ = Y tanom ensures that the MPC ( 3) is feasible at time t anom + K ′ .The theorem then recursively follows by applying all the preceding steps of the proof.</p>
<p>I. Hardware Experiment -Additional Details and Results</p>
<p>In this section we detail further experimental results that 1) detail our data collection and monitor calibration process, 2) analysis of query latencies to choose the parameter K in (3), 3) quantify the performance of the fast anomaly detector, 4) describe the behavior of the closed-loop system on our test scenarios.Finally, we discuss the implementation details of the MPC controller.Most importantly, we emphasize to the reader that videos of our experiments are included in the supplementary materials, as well as on the project web page: https://sites.google.com/view/aesop-llm.</p>
<p>1) Data Collection: As described in §V-C, we collect data by flying the drone in a circular pattern above the operational area with a nominal clutter of objects on the ground and recording object detections from the observations.We construct a prompt from each possible combination of up to 4 detections from the set of unique detections observed and embed these to form the vector cache.We perform this data collection process twice.First, we collect the embeddings representing the quadrotor's prior experience, D e , whereby there are no anomalous elements placed in the scene.We use this dataset to construct the anomaly detector we use in the experiments.In addition, to evaluate the performance of the anomaly detector, we collect a calibration dataset, D c , where a limited number of unseen objects are introduced and object placements are varied (e.g., placing objects on top of the box to simulate obstructions).We use D c to evaluate the anomaly detector in  the next subsection.We show an example of a nominal and an anomalous observation in Fig. 13 and Fig. 14, respectively.</p>
<p>2) Fast Anomaly Detector Calibration: To calibrate the anomaly detection threshold we compute the top-k score for each embedding in D c against the embedding cache D p and identify the lowest score threshold such that we achieve a TPR of at least 0.9 on the calibration set.We compare the receiver operator characteristic (ROC) curves for top-1 and top-3 scoring in Fig. 15.Interestingly, we find that top-k scoring performed best when k = 1, which we attribute to the limited diversity of possible observations in this particular scene.We use the top-1 metric for the closed-loop evaluations.</p>
<p>3) Choosing K -Analysis of LLM Query Latencies: To choose the upper bound K on the latency of the slow generative reasoner used in the MPC (3) and Algorithm 1, we perform a simple experiment using the prompt and the observations from the calibration dataset: We query GPT-3.5-turbo(the slow reasoner in our experiment) N = 500 times and record the response latency.As shown in Fig. 16, the response times follow a bimodal distribution with a mean of 3.1s and a standard deviation of 0.85s and a small fraction of outliers.Therefore, we set K = 4.3s, corresponding to the 95% quantile of the response times to, to ensure that the LLM returns within our bound K except for rare outlier latencies.We did not experience any instances where the latency was beyond our bound K in our experiments.This is largely because we only sporadically query the LLM once we detect and anomaly, when we truly require the LLM's response.4) Summary of test scenarios: We describe the qualitative behavior of our scenarios here, but we emphasize to the reader that videos of our experiments are included in the supplementary materials, as well as on the project web page: https://sites.google.com/view/aesop-llm.</p>
<ol>
<li>
<p>Nominal Operation: There are no obstructions on the red box, so the quadrotor should land normally.As desired, we observe that the quadrotor smoothly flies towards and lands on the box without considering any of the objects on the ground as anomalies.</p>
</li>
<li>
<p>Consequential Anomaly: We consider two variants of this scenario.In the first, another quadrotor has already landed on the red box, necessitating a diversion to the blue box for landing.In the second, quadrotors occupy both red and blue boxes, necessitating a diversion to the holding zone.Qualitatively, these scenarios show three properties of our methodology: First is the ability of both monitors to react to nuanced semantics in the scene from the task context, since "drones on the ground" were previously seen in the nominal data but their presence on the landing zones is recognized as safetycritical.Second is the fact that the LLM reasoner helps us select the most appropriate choice of fallback, as it chooses to land on the blue box when possible and recognizes it should recover to the holding zone if not.Third, the fast reasoner ensures that the quadrotor pulls back towards the recovery regions once it detects a hazard on the landing site, rather than naively proceeding with landing while awaiting the LLMs response.</p>
</li>
</ol>
<p>In both these experiments, we see the quadrotor pull back from the red box in the same manner, as the consensus horizon K enforces both recovery plans to be identical untill the LLM returns a decision.We further make a note that the dynamics model used in the MPC does not model ground effect, which results in a significant upward disturbance once the quadrotor attempts its landing on the blue box.In addition, we observed the motion capture system used for state estimation has a dead zone at the location and altitude that the drone reaches above the blue box.As a result, the drone makes a small jump upwards before landing on the blue box.</p>
<ol>
<li>Inconsequential Anomaly: A previously unseen object (specifically, a keyboard) on the ground triggers the fast anomaly detector.However, the subsequent analysis of the slow LLM reasoner correctly deems the anomaly inconsequential, allowing the quadrotor to proceed with landing at its nominal site.In this experiment, we see that once the fast reasoner detects the keyboard, the quadrotor slows down to await the LLM's decision.After the LLM makes its decision the drone speeds back up toward the landing zone.</li>
</ol>
<p>5) Controller Parameters: To control the quadrotor, we use a Pixracer R15 microcontroller running the open-source PX4 Autopilot software.We use an Optitrack motion capture system for state estimation of the drone, which is fused with the internal IMU of the Pixracer using its built-in EKF.We implement our control stack in ROS2 with nodes written in Python.We implement the MPC controller using a simple kinematic model of the drone, representing the drone's position, attitude, and the rates thereof as the state.Our MPC uses acceleration commands as its inputs, is constrained to maintain the drone's velocity under 1m/s and within a position/altitude safety fence, and relies on the PX4's internal PID controllers to track desired trajectory setpoints output by the MPC's trajectory predictions.We used a controller horizon of 10s at a time discretization of 0.05s in our experiment, which was sufficiently long to ensure the drone could reach the recovery sets consistently during the experiments.We manually control the drone to liftoff, after which we switch to the MPC controller to execute the trajectory and land the drone.To do so, the MPC nominally controls the drone towards a waypoint a foot above the landing zone.Upon reaching the waypoint, it descends and lands.</p>
<p>Fig. 2 :
2
Fig. 2: Embedding-based (fast) anomaly detection results for the manipulation, autonomous vehicle, and VTOL domains.The top row of figures plot anomaly detection accuracy as a function of experiences sampled IID from the respective domain datasets.The bottom row of figures plot accuracy as a function of the concepts sampled from the respective domain datasets.We use top-5 scoring with the anomaly detection threshold set at the 95-th quantile (2) of the scores in the sampled data.</p>
<p>FAST ( 0
0
.053s) t = 2.5s: Anomaly Detected I am the runtime monitor for a visionbased autonomous vertical takeoff and landing (VTOL) aircraft...The VTOL's current observations are: (a busy road, ..., an unidentified flying object) The VTOL's available safety interventions are: -Perform an immediate landing in an empty grass field -Perform an immediate landing in a parking lot -Continue with the current plan (e.g., flying or landing)... Detected object: a busy road... Object classification: nominal Detected object: an unpredictable unidentified flying object Analysis: ...poses a potential safety risk ... if it enters the flight path... Object classification: anomalous Overall control decision: ... landing in an empty grass field → Recover to AESOP Quadrotor Trajectory &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L Q b 9 N 4 J 0 d u + 3 y R B Q e + 6 7 o t + d Z L</p>
<p>Fig. 4 :
4
Fig. 4: Annotated depiction of our quadrotor hardware experiment.The quadrotor's goal is to land on the red box.In the event of an anomaly, it can either recover by landing on the blue box, or by hovering within the designated holding zone.</p>
<p>Fig. 5 :
5
Fig. 5: Embedding-based anomaly detection results for the manipulation, autonomous vehicle, and VTOL domains.The top row of figures plot the AUROC as a function of experiences sampled IID from the respective domain datasets.The bottom row of figures plot accuracy as a function of the concepts sampled from the respective domain datasets.</p>
<p>Fig. 6 :
6
Fig.6: Score function comparison for a selection of embedding models applied to the VTOL domain.We compare the top-k (for k ∈ {1,3,5,10}) and Mahalanobis distance score functions for MPNet, Mistral (7B), Llama2 Chat (7B), and OpenAI Ada 002.</p>
<p>(a) Closed loop trajectory of the naive baseline MPC (black).Also shown are the nominal predicted trajectory (blue) and the minimum constraint violating recovery trajectory (red) at the timestep that the slow LLM reasoner outputs.(b) Closed loop trajectory of the Fallback-Safe MPC [46] (black).We show three recovery trajectory plans: The first is at the timestep where the LLM is queried, where the robot maintains recovery plans to the sets at x ≈ 8m.The second set of plans is at the timestep right before the LLM reasoner outputs it's recovery decision, where the planner chooses recovery sets around x ≈ 3m.The third is the minimum constraint violating recovery trajectory at the timestep that the slow LLM reasoner outputs.(c) Closed loop trajectory of the AESOP algorithm in a nominal episode (black).That is, an episode in which the fast anomaly detector detects no anomalies.Also shown are the predicted trajectories at the last timestep of the episode.</p>
<p>Fig. 7 :
7
Fig. 7: Closed loop trajectories of 1) the naive MPC baseline, 2) the Fallback-Safe MPC baseline, 3) the AESOP algorithm in various scenarios.</p>
<p>(a) Vehicle approaches a nominal stop sign.(b) Vehicle approaches a nominal traffic light.</p>
<p>Fig. 8 :
8
Fig.8: Nominal observations for each object class in[12].</p>
<p>(a) Vehicle approaches a stop sign anomaly.(b) Vehicle approaches a traffic light anomaly.</p>
<p>Fig. 10 :
10
Fig. 10: Anomaly detection with MPNet, BERT-large and Mistral (7B) w/ and w/out ground truth scene descriptions against generative CoT reasoning with GPT-4V.</p>
<p>Fig. 11 :
11
Fig. 11: Accuracy of the text-based embedding detectors as a function of the number of objects within each observation for the CARLA dataset.</p>
<p>Fig. 12 :
12
Fig. 12: Representation of nominal (nom.) and anomalous (anom.)observations (obs.) in the our CARLA evaluation dataset.</p>
<p>Fig. 13 :
13
Fig. 13: Example of a nominal observation in the quadrotor experiment.</p>
<p>Fig. 14 :
14
Fig. 14: Example of an anomalous observation in the quadrotor experiment due to the obstruction of the landing zone.</p>
<p>Fig. 15 :Fig. 16 :
1516
Fig. 15: ROC curves computed for the calibration embeddings against the quadrotor's fast anomaly detector's embedding cache.</p>
<p>AESOP
Successful Re-15%23%100%covery Rate</p>
<p>TABLE II :
II
Percentage of trajectories where the quadrotor successfully recovered to the LLM's choice of recovery region.</p>
<p>5-turbo with chain-of-
MethodTPR FPR Bal. AccuracyGPT-40.74 0.190.78TextGPT-3 CoT [12] MPNet (Ours) Mistral (Ours)0.89 0.26 0.69 0.05 0.95 0.050.82 0.82 0.95SCOD0.40 0.060.67Mahal.0.40 0.130.64VisionGPT-4V GPT-4V CoT0.97 0.27 0.89 0.100.85 0.90CLIP (Ours)0.86 0.050.90CLIP (Ours) Abl. 0.99 0.570.71TABLE IV: CARLA Evaluation. Text and Vision-basedAnomaly Detection.</p>
<dl>
<dt>TABLE V</dt>
<dt>VSymbolDescriptionxunless explicitly defined otherwise,scalar variables are lowercasexvectors are boldfacedX x tsets are caligraphic time-varying quantities are indexedNotationx 0:twith a subscript t ∈ N ≥0 Shorthand to index subsequences:and con-ventionsx 0:t := {x 0 ,...,x t }λ,δ,ϵ,θhyperparameters (regardless of theirtype) are lowercase Greek charactersx t+k|tPredicted quantities at k time stepsinto the future computed at timestep t. Read x t+k|t as "the predictedANotation and Glossary . . . . . . . . . . 14value of x at time t+k given time t."BBackground: Anomaly Detection . . . . 15xSystem stateCBackground: Mechanics of LLMs . . . 15uInputDAdditional Results: Synthetics . . . . . 15oObservationD1Fast Anomaly Detector ROCfDynamicsAnalysis . . . . . . . . . . . 15hAnomaly DetectorED2 D3 D4 Additional Results: Quadrotor Simulation 16 Detection Threshold Analysis 15 Similarity Score Function Analysis . . . . . . . . . . . 16 Safety Assessment in Manipulation Domain . . . . 16 E1 Simulation and Implementation Details . . . 17w e ϕ X U O τ s CGenerative reasoner Embedding vector Embedding model State constraint set Input constraint set Observation space Anomaly detection threshold Anomaly score function control objective function for theE2Baselines . . . . . . . . . . . 18MPC (3)FE3 Additional Results: Autonomous Vehicle Results . . . . . . . . . . . . 18D nom NDataset of nominal observations Number of nominal observations inSimulation . . . . . . . . . . . . . . . . 18 F1 Object Detection vs. Ground-truth Detections . . . . . . . 20Variables X i R dD nom the i'th recovery region Number of recovery regionsG H IF2 F3 Prompting Details . . . . . . . . . . . . 21 Calibration Ablation on Withheld Trajectories . . . . 21 Accuracy vs. Complexity of Observations . . . . . . . . . 21 Proof of Theorem 1 . . . . . . . . . . . 22 Hardware Experiment -Additional Details and Results . . . . . . . . . . . 23D e α q Y KEmbedding vector cache constructed from D nom Quantile hyperparameter to select the anomaly detector threshold Optimization variable used to define the empirical α-quantile Subset of {1, ... , d} indicating a selection of recovery regions Upper bound on the latency of theI1Data Collection . . . . . . . 23slow reasonerI2Fast Anomaly DetectorTTime horizon of the MPC (3)Calibration . . . . . . . . . . 23JObjective value associated with theI3Choosing K -Analysis ofsolution of the MPC problem (3)LLM Query Latencies . . . . 23x ⋆ , u ⋆Starred quantities denote the optimalI4Summary of test scenarios . 24values of the decision variables inI5Controller Parameters . . . . 24t anomthe MPC problem (3) Time step at which the anomalyA. Notation and Glossarydetector triggersWe include a glossary of all the notation and symbols usedin this paper in Table V.</dt>
<dd>Glossary of notation and symbols used in this paper.</dd>
</dl>
<p>TABLE VI :
VI
Calibration results for a selection of embedding models in the VTOL domain.The table reports the mean anomaly detection accuracy thresholding the top-5 score function at different quantile thresholds.Standard deviations are provided in parentheses.Statistics were computed over multiple samplings of 80% of the available nominal data samples.Accuracies for GPT-4 single-token and CoT queries are provided for comparison.
GenerativeThreshold:75-Quantile 85-Quantile 90-Quantile 95-QuantileGPT-4 GPT-4 CoTOpenAI Ada 002 0.8 (0.002) 0.84 (0.002) 0.85 (0.002) 0.85 (0.002)MPNet0.83 (0.002) 0.89 (0.002) 0.91 (0.002) 0.93 (0.001)0.750.86Mistral (7B)0.83 (0.001) 0.89 (0.001) 0.91 (0.001) 0.94 (0.001)MPNetScore Function ComparisonMistral (7B)1 .00 .90 .7 0 .8 Accuracy0 .60 .50 .0 0 .40 .20 .40 .60 .81 .00 .30 .40 .50 .60 .70 .80 .91 .00 .00 .20 .40 .60 .81 .00 .30 .40 .50 .60 .70 .80 .91 .0Relative Sample SizeRelative Concept CoverageRelative Sample SizeRelative Concept CoverageLlama2 Chat (7B)OpenAI Ada 0021 .00 .90 .7 0 .8 Accuracy0 .60 .50 .0 0 .40 .20 .40 .60 .81 .00 .30 .40 .50 .60 .70 .80 .91 .00 .00 .20 .40 .60 .81 .00 .30 .40 .50 .60 .70 .80 .91 .0Relative Sample SizeRelative Concept CoverageRelative Sample SizeRelative Concept Coveragetop1top3top5top10mahal</p>
<p>TABLE VII :
VII
Slow Generative Reasoning for Anomaly Assessment in the Warehouse Manipulation Domain.</p>
<p>TABLE VIII :
VIII
Percentage of trajectories where the quadrotor successfully recovered to the LLM's choice of recovery region.
AESOPSuccessful Re-15%23%100%covery Rate</p>
<p>TABLE IX :
IX
Accuracy of embedding detectors when withholding nominal data from CARLA routes with anomalies.All methods are our own.</p>
<p>This name is inspired by the author of "The Tortoise and the Hare," in reference to our slow and fast reasoners.
This approach parallels ideas from dual process theory in cognitive science, popularized in Kahneman's "Thinking, Fast and Slow"[22]. Most of the time, we drive a car based on intuition without careful thought. It is only once something unusual startles us that we carefully reason about how to proceed, often proactively lifting from the throttle to slow down and buy ourselves time to come to a decision.
In the WM domain, anomalous observations consist of overtly abnormal concepts (e.g., a "smoking lithium battery," "a broken glass bottle") relative to the typical "package" or "computer" one would expect on a conveyor belt. This in turn simplifies the task of differentiating anomalous from nominal observations. By contrast, the VTOL domain consist of more nuanced concept shifts; an observation containing a "flying bird" is conceptually similar to a "swarming flock of birds," though one is nominal, and the other anomalous.
In all our experiments, the computational cost of computing similarity scores with the embeddings was negligible compared to model inference times.
Examples of documented failures modes include an image of a stop sign on a billboard (source) and a truck transporting inactive traffic lights (source).
ACKNOWLEDGMENTSThe authors would like to thank Brian Ichter and Fei Xia for insightful discussions and feedback throughout the project.In addition, the authors are indebted to Jun En Low, Keiko Nagami, and Alvin Sun for their assistance in setting up the hardware experiments.The NASA University Leadership initiative (grant #80NSSC20M0163) and the Toyota Research Institute (TRI) provided funds to assist the authors with their research, but this article solely reflects the opinions and conclusions of its authors and not any NASA or TRI entity.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Autort: Embodied foundation models for large scale orchestration of robotic agents. Debidatta Michael Ahn, Chelsea Dwibedi, Montse Finn, Keerthana Gonzalez Arenas, Karol Gopalakrishnan, Brian Hausman, Alex Ichter, Nikhil Irpan, Ryan Joshi, Julian, arXiv:2401.129632024arXiv preprint</p>
<p>A gentle introduction to conformal prediction and distribution-free uncertainty quantification. Anastasios N Angelopoulos, Stephen Bates, 2022</p>
<p>Predictive control for linear and hybrid systems. F Borrelli, A Bemporad, M Morari, 2017</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR2023</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Arxiv eprint</p>
<p>End-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European conference on computer vision. Springer2020</p>
<p>Adapt on-the-go: Behavior modulation for single-life robot deployment. Annie S Chen, Govind Chada, Laura Smith, Archit Sharma, Zipeng Fu, Sergey Levine, Chelsea Finn, 2023</p>
<p>Guojun Chen, Xiaojing Yu, Lin Zhong, Typefly, arXiv:2312.14950Flying drones with large language model. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Carla: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on robot learning. PMLR2017</p>
<p>Semantic anomaly detection with large language models. Amine Elhafsi, Rohan Sinha, Christopher Agia, Edward Schmerling, A D Issa, Marco Nesnas, Pavone, 10.1007/s10514-023-10132-6Autonomous Robots. 1573-7527478Dec 2023</p>
<p>Can autonomous vehicles identify, recover from, and adapt to distribution shifts. Angelos Filos, Panagiotis Tigas, Rowan Mcallister, Nicholas Rhinehart, Sergey Levine, Yarin Gal, ICML, ICML'20. JMLR.org. 2020</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, 10.1038/s42256-020-00257-zNature Machine Intelligence. 2522-5839211Nov 2020</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2015arXiv preprint</p>
<p>CyCADA: Cycle-consistent adversarial domain adaptation. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, Trevor Darrell, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLRJul 201880of Proceedings of Machine Learning Research</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister, arXiv:2305.023012023arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, Conference on Robot Learning. PMLR2023</p>
<p>Quantization and training of neural networks for efficient integer-arithmetic-only inference. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Thinking, fast and slow. Daniel Kahneman, 2011macmillan</p>
<p>Language-driven representation learning for robotics. Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang, Robotics: Science and Systems (RSS). 2023</p>
<p>Fine-tuning can distort pretrained features and underperform out-of-distribution. Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, Percy Liang, International Conference on Learning Representations. 2022</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. Alexander Balaji Lakshminarayanan, Charles Pritzel, ; I Blundell, U Guyon, S Von Luxburg, H Bengio, R Wallach, S Fergus, R Vishwanathan, Garnett, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, Advances in Neural Information Processing Systems. S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, Curran Associates, Inc201831</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Text2motion: from natural language instructions to feasible plans. Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, 10.1007/s10514-023-10131-7Autonomous Robots. 1573-7527Nov 2023</p>
<p>Energy-based out-of-distribution detection. Weitang Liu, Xiaoyun Wang, John Owens, Yixuan Li, Advances in Neural Information Processing Systems. 2020</p>
<p>Sample-efficient safety assurances using conformal prediction. Rachel Luo, Shengjia Zhao, Jonathan Kuck, Boris Ivanovic, Silvio Savarese, Edward Schmerling, Marco Pavone, 2021</p>
<p>Accuracy on the line: On the strong correlation between out-ofdistribution and in-distribution generalization. John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, Ludwig Schmidt, 2021</p>
<p>Simple open-vocabulary object detection with vision transformers. Minderer, Gritsenko, Stone, Neumann, Weissenborn, Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, arXiv:2205.062302022arXiv preprint</p>
<p>Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, Jasper Snoek, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates Inc2019</p>
<p>Task-specific skill localization in finetuned language models. Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, Sanjeev Arora, arXiv:2302.066002023arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Run-time monitoring of machine learning for robotic perception: A survey of emerging trends. Peter Quazi Marufur Rahman, Feras Corke, Dayoub, 10.1109/ACCESS.2021.3055015IEEE Access. 920067-20075, 2021</p>
<p>Do ImageNet classifiers generalize to ImageNet?. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri, Ruslan Salakhutdinov, the 36th International Conference on Machine LearningPMLR09-15 Jun 201997</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics201911</p>
<p>Safe visual navigation via deep learning and novelty detection. Charles Richter, Nicholas Roy, RSS. July 2017</p>
<p>A unifying review of deep and shallow anomaly detection. Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, Klaus-Robert Müller, 10.1109/JPROC.2021.3052449Proceedings of the IEEE. 10952021</p>
<p>Distributionally robust neural networks. Shiori Sagawa, Pang Wei Koh, * , Tatsunori B Hashimoto, Percy Liang, International Conference on Learning Representations. 2020</p>
<p>A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges. Mohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, Mohammad Sabokrou, 2021</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on Robot Learning. PMLR2023</p>
<p>Sketching curvature for efficient out-of-distribution detection for deep neural networks. Apoorva Sharma, Navid Azizan, Marco Pavone, CoRR, abs/2102.125672021</p>
<p>Rohan Sinha, Apoorva Sharma, Somrita Banerjee, Thomas Lew, Rachel Luo, Yixiao Spencer M Richards, Edward Sun, Marco Schmerling, Pavone, arXiv:2212.14020A system-level view on out-of-distribution data in robotics. 2022arXiv preprint</p>
<p>Closing the loop on runtime monitors with fallbacksafe mpc. Rohan Sinha, Edward Schmerling, Marco Pavone, 10.1109/CDC49753.2023.103839652023 62nd IEEE Conference on Decision and Control (CDC). 2023</p>
<p>Mpnet: Masked and permuted pre-training for language understanding. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, Advances in Neural Information Processing Systems. 202033</p>
<p>OSQP: an operator splitting solver for quadratic programs. B Stellato, G Banjac, P Goulart, A Bemporad, S Boyd, 10.1007/s12532-020-00179-2Mathematical Programming Computation. 1242020</p>
<p>Mingjie Sun, Zhuang Liu, Anna Bair, J Zico Kolter, arXiv:2306.11695A simple and effective pruning approach for large language models. 2023arXiv preprint</p>
<p>Andrea Tagliabue, Kota Kondo, Tong Zhao, Mason Peterson, Claudius T Tewari, Jonathan P How, arXiv:2311.01403Real: Resilience and adaptation using large language models on autonomous aerial robots. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Improving text embeddings with large language models. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv:2401.003682023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Robust fine-tuning of zero-shot models. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, Ludwig Schmidt, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Smoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, International Conference on Machine Learning. PMLR2023</p>
<p>Robotic skill acquisition via instruction augmentation with vision-language models. Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, Jonathan Tompson, arXiv:2211.117362022arXiv preprint</p>
<p>Scaling robot learning with semantically imagined experience. Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, arXiv:2302.115502023arXiv preprint</p>
<p>Language to rewards for robotic skill synthesis. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Lewis Hao-Tien, ; Chiang, arXiv:2306.08647Jan Humplik,. 2023Tom Erez, Leonard HasencleverarXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>            </div>
        </div>

    </div>
</body>
</html>
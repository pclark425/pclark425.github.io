<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5217 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5217</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5217</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270562752</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12050v3.pdf" target="_blank">Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Supervised fine-tuning enhances the problem-solving abilities of language models across various mathematical reasoning tasks. To maximize such benefits, existing research focuses on *broadening* the training set with various data augmentation techniques, which is effective for standard single-round question-answering settings. Our work introduces a novel technique aimed at cultivating a *deeper* understanding of the training problems at hand, enhancing performance not only in standard settings but also in more complex scenarios that require reflective thinking. Specifically, we propose **reflective augmentation**, a method that embeds problem reflection into each training instance. It trains the model to consider alternative perspectives and engage with abstractions and analogies, thereby fostering a thorough comprehension through reflective reasoning. Extensive experiments validate the achievement of our aim, underscoring the unique advantages of our method and its complementary nature relative to existing augmentation techniques.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5217.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5217.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RefAug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-time augmentation that appends a reflective section (alternative reasoning + follow-up reasoning: abstraction or analogy) to each training instance so the model learns to 'reflect' on solutions during training; reflection tokens are included in the loss but are not generated at inference due to an early-stop termination string.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B / Gemma-7B (also tested LLaMA-3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only LMs of ~7B parameters (Mistral-7B, Gemma-7B); LLaMA-3-8B also tested. Models are fine-tuned with standard supervised recipes (3 epochs, batch size 128 for math experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflective Augmentation (RefAug)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During training each (question, answer) pair is extended with a 'Reflection:' section containing (1) Alternative reasoning — a different solution approach to the same problem, and (2) Follow-up reasoning — an abstraction or analogy that connects the instance to a broader class of problems or a harder variant. The reflective sections are generated by an expert LM (GPT-4-turbo) in annotation, concatenated to the gold answer, and included in the training loss. At inference the model is configured to early-stop generation when encountering the 'Reflection:' prefix so reflective tokens are not produced during normal deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Standard math QA (GSM8k, MATH, out-of-distribution sets) and reflective math reasoning (MathChat FQA/EC, MINT); Code generation (HumanEval, MBPP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school and competition math word problems (GSM8k, MATH), multi-turn follow-up/error-correction tasks (MathChat FQA/EC), multi-turn feedback-utilization benchmark (MINT Math), and code generation benchmarks (HumanEval, MBPP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Standard single-round QA: average +7.2 percentage points accuracy across two base LMs (RefAug vs direct fine-tuning). For Mistral: baseline avg ~40.15% -> RefAug avg ~46.76% (per reported tables). Reflective tasks: FQA-3rd +12.3 points, Error Correction (EC) +22.3 points, MINT (k=5) +10.6 points, MINT Δ (improvement from k=1 to k=5) +9.2 points. Code generation: +3.5 Pass@1 on average across evaluated code LMs/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct supervised fine-tuning on original training data (no RefAug). Example Mistral baseline average ~40.15% on the reported math suite; lower scores on reflective tasks (exact baseline values reported in tables—e.g., substantially lower FQA and EC accuracies).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative gains reported across multiple benchmarks: +7.2 pp average on single-round math QA; substantial gains on multi-turn/reflective benchmarks (FQA-3rd +12.3, EC +22.3, MINT k=5 +10.6). Ablations show each reflective component (alternative or follow-up) yields ≈+4.5 points and both combined add another ≈+2.3 points; error analysis (GSM8k) attributes most improvement to reductions in reasoning errors (reasoning errors decreased by 50 counts; calculation errors decreased by 23), supporting that RefAug improves conceptual reasoning rather than only surface answer patterns. Combining RefAug with other augmentation (A‑Aug, Q‑Aug, MetaMath) yields further complementary gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reasonably high-quality reflective annotations: when reflective sections were annotated by LLaMA-3-70B rather than GPT-4-turbo, improvements on reflective reasoning tasks were smaller (LLaMA-annotated RefAug matched standard QA gains but failed to match GPT-annotated data on reflective abilities). RefAug increases training sequence length and training time (moderate overhead) though it does not increase inference cost due to early stopping. Scaling was limited by annotation budget (MetaMath RefAug capped at 40K). The method improves training-time comprehension but is not an inference-time iterative self-refinement procedure; placing the reflection before the answer (modeling P([r; a]|q)) or embedding reflection into chain-of-thought can hurt reflective-task performance. Contamination checks show minimal test contamination risk in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5217.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5217.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-turbo (annotator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-turbo (used as expert annotator for reflective sections)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source high-quality LM used to generate the reflective sections (alternative and follow-up reasoning) for each training instance in RefAug; used with temperature 0.7 and top_p=1.0 in annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-turbo (annotation-time use)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4-turbo (proprietary) used as a data annotator to produce high-quality reflective sections for training data.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Annotation-generated reflection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>An expert LM (GPT-4-turbo) is prompted (with in-context examples and a custom prompt) to produce alternative reasoning and follow-up reasoning that are appended to gold solutions to form RefAug training sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Annotation of reflective sections for math and code training data</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate alternative solutions and follow-up (abstraction or analogy) problems and solutions for each training instance to augment training data with reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not directly applicable as GPT-4-turbo is used for annotation; however models trained with GPT-annotated RefAug data show the largest improvements (see RefAug entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports that GPT-4-turbo annotated reflective sections are high-quality (manual inspection reported high correctness rates: alternative section 96% error-free on GSM8k, follow-up 96%; lower but acceptable on MATH). Models trained with GPT-annotated RefAug outperform those trained with LLaMA-annotated RefAug on reflective tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Using a lower-quality annotator reduces gains on reflective tasks (LLaMA-3-70B annotations produced smaller improvements); using GPT services imposes cost/budget constraints limiting augmentation scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5217.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5217.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-then-reflect approach where a model produces an answer and then generates self-feedback that it uses to refine and re-generate an improved answer (iterative self-refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM (method-agnostic iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Iterative pipeline in which a single LM generates a candidate answer, produces critique/self-feedback, and conditions a revised answer on that feedback (authors: Madaan et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-refinement / self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial solution, produce self-feedback or critique using the model's internal knowledge, and then revise the solution using that feedback; can be repeated multiple times.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General multi-step reasoning tasks (as studied in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require multi-step reasoning where iterative self-feedback may correct errors in reasoning or computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited in related work as an example of enabling LMs to rectify their responses during inference (self-reflect). This paper does not reproduce Self-Refine experiments but notes it as prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper cites that the effectiveness of internal self-correction approaches is debated and references subsequent critiques (e.g., Huang et al., 2024a) noting mixed results; no direct experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5217.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5217.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-style approach that equips language models with external feedback loops (e.g., environment signals, reward/critique) to iteratively improve behavior using verbalized feedback and reinforcement-learning style updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM agent with external feedback</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method where an LM agent receives external critiques or environmental feedback and uses verbalized reflections to adapt its behavior; used in agentic, iterative contexts (Shinn et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Agentic reflection with external feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model receives external signals (code execution results, expert critiques, environment rewards) and produces reflective text that is used to update subsequent generations or policies.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Agentic tasks and iterative problem solving (as studied by cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where external feedback (execution traces, critic judgments) are available to evaluate and correct model outputs iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as a prior approach that equips LMs with external feedback to rectify responses during inference; the present paper positions this line as distinct from training-time RefAug.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; cited to show prior uses of reflection that rely on external feedback rather than purely internal training-time augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5217.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5217.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Verification / Reverse Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-verification / reverse verification methods (backward reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that verify candidate answers by attempting to rederive the question from the generated answer or using backward-checks (model-based verification) to detect/correct errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Forward-backward reasoning in large language models for verification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM with self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Techniques where an LM or system uses a generated answer to perform a reverse derivation or verification step (e.g., Weng et al., 2023; Jiang et al., 2023b), sometimes used as a filtering/verification mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reverse verification / self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After generating an answer, the model performs a verification pass (re-derivation or back-check) to confirm correctness; may act as a check to accept/reject or to refine the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math verification / multi-step reasoning verification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where answers can be checked or rederived (e.g., algebraic checks, re-running computations) to confirm correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as a prior successful use of reflection-style verification for some math tasks; the present paper distinguishes training-time reflective augmentation from these inference-time verification approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that such verification approaches exist but does not evaluate them directly; some debate exists about the effectiveness of purely internal self-checks on certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5217.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5217.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Internal self-correction (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted internal self-correction / self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that prompt LMs to use only internal knowledge (without external tools) to critique and correct their answers at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM prompted for internal critique (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineered schemes that ask an LM to critique its own output and produce a corrected answer, relying solely on internal model knowledge (examples cited: Madaan et al., 2023; Li et al., 2024b).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Prompted self-critique / internal reflection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use of carefully designed prompts to have the model scrutinize and revise its own output without external verification resources.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various reasoning tasks (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the model can be asked to self-critique and re-answer; sometimes used for math and general reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper acknowledges prior work using internal self-correction but notes that its effectiveness is under debate (cites Huang et al., 2024a). The present work does not experimentally adopt inference-time internal self-critique; instead it focuses on adding reflection into training data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited literature reports mixed effectiveness; the present paper references that relying solely on internal self-correction may not reliably improve reasoning in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models are better reasoners with self-verification <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>When hindsight is not 20/20: Testing limits on reflective thinking in large language models <em>(Rating: 2)</em></li>
                <li>Large language models can self-correct with minimal effort <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5217",
    "paper_id": "paper-270562752",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "RefAug",
            "name_full": "Reflective Augmentation",
            "brief_description": "A training-time augmentation that appends a reflective section (alternative reasoning + follow-up reasoning: abstraction or analogy) to each training instance so the model learns to 'reflect' on solutions during training; reflection tokens are included in the loss but are not generated at inference due to an early-stop termination string.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B / Gemma-7B (also tested LLaMA-3-8B)",
            "model_description": "Open-source decoder-only LMs of ~7B parameters (Mistral-7B, Gemma-7B); LLaMA-3-8B also tested. Models are fine-tuned with standard supervised recipes (3 epochs, batch size 128 for math experiments).",
            "reflection_method_name": "Reflective Augmentation (RefAug)",
            "reflection_method_description": "During training each (question, answer) pair is extended with a 'Reflection:' section containing (1) Alternative reasoning — a different solution approach to the same problem, and (2) Follow-up reasoning — an abstraction or analogy that connects the instance to a broader class of problems or a harder variant. The reflective sections are generated by an expert LM (GPT-4-turbo) in annotation, concatenated to the gold answer, and included in the training loss. At inference the model is configured to early-stop generation when encountering the 'Reflection:' prefix so reflective tokens are not produced during normal deployment.",
            "num_iterations": null,
            "task_name": "Standard math QA (GSM8k, MATH, out-of-distribution sets) and reflective math reasoning (MathChat FQA/EC, MINT); Code generation (HumanEval, MBPP)",
            "task_description": "Grade-school and competition math word problems (GSM8k, MATH), multi-turn follow-up/error-correction tasks (MathChat FQA/EC), multi-turn feedback-utilization benchmark (MINT Math), and code generation benchmarks (HumanEval, MBPP).",
            "performance_with_reflection": "Standard single-round QA: average +7.2 percentage points accuracy across two base LMs (RefAug vs direct fine-tuning). For Mistral: baseline avg ~40.15% -&gt; RefAug avg ~46.76% (per reported tables). Reflective tasks: FQA-3rd +12.3 points, Error Correction (EC) +22.3 points, MINT (k=5) +10.6 points, MINT Δ (improvement from k=1 to k=5) +9.2 points. Code generation: +3.5 Pass@1 on average across evaluated code LMs/benchmarks.",
            "performance_without_reflection": "Direct supervised fine-tuning on original training data (no RefAug). Example Mistral baseline average ~40.15% on the reported math suite; lower scores on reflective tasks (exact baseline values reported in tables—e.g., substantially lower FQA and EC accuracies).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative gains reported across multiple benchmarks: +7.2 pp average on single-round math QA; substantial gains on multi-turn/reflective benchmarks (FQA-3rd +12.3, EC +22.3, MINT k=5 +10.6). Ablations show each reflective component (alternative or follow-up) yields ≈+4.5 points and both combined add another ≈+2.3 points; error analysis (GSM8k) attributes most improvement to reductions in reasoning errors (reasoning errors decreased by 50 counts; calculation errors decreased by 23), supporting that RefAug improves conceptual reasoning rather than only surface answer patterns. Combining RefAug with other augmentation (A‑Aug, Q‑Aug, MetaMath) yields further complementary gains.",
            "limitations_or_failure_cases": "Requires reasonably high-quality reflective annotations: when reflective sections were annotated by LLaMA-3-70B rather than GPT-4-turbo, improvements on reflective reasoning tasks were smaller (LLaMA-annotated RefAug matched standard QA gains but failed to match GPT-annotated data on reflective abilities). RefAug increases training sequence length and training time (moderate overhead) though it does not increase inference cost due to early stopping. Scaling was limited by annotation budget (MetaMath RefAug capped at 40K). The method improves training-time comprehension but is not an inference-time iterative self-refinement procedure; placing the reflection before the answer (modeling P([r; a]|q)) or embedding reflection into chain-of-thought can hurt reflective-task performance. Contamination checks show minimal test contamination risk in reported experiments.",
            "uuid": "e5217.0",
            "source_info": {
                "paper_title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4-turbo (annotator)",
            "name_full": "GPT-4-turbo (used as expert annotator for reflective sections)",
            "brief_description": "A closed-source high-quality LM used to generate the reflective sections (alternative and follow-up reasoning) for each training instance in RefAug; used with temperature 0.7 and top_p=1.0 in annotation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-turbo (annotation-time use)",
            "model_description": "OpenAI's GPT-4-turbo (proprietary) used as a data annotator to produce high-quality reflective sections for training data.",
            "reflection_method_name": "Annotation-generated reflection",
            "reflection_method_description": "An expert LM (GPT-4-turbo) is prompted (with in-context examples and a custom prompt) to produce alternative reasoning and follow-up reasoning that are appended to gold solutions to form RefAug training sequences.",
            "num_iterations": null,
            "task_name": "Annotation of reflective sections for math and code training data",
            "task_description": "Generate alternative solutions and follow-up (abstraction or analogy) problems and solutions for each training instance to augment training data with reflection.",
            "performance_with_reflection": "Not directly applicable as GPT-4-turbo is used for annotation; however models trained with GPT-annotated RefAug data show the largest improvements (see RefAug entry).",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper reports that GPT-4-turbo annotated reflective sections are high-quality (manual inspection reported high correctness rates: alternative section 96% error-free on GSM8k, follow-up 96%; lower but acceptable on MATH). Models trained with GPT-annotated RefAug outperform those trained with LLaMA-annotated RefAug on reflective tasks.",
            "limitations_or_failure_cases": "Using a lower-quality annotator reduces gains on reflective tasks (LLaMA-3-70B annotations produced smaller improvements); using GPT services imposes cost/budget constraints limiting augmentation scale.",
            "uuid": "e5217.1",
            "source_info": {
                "paper_title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "An iterative generate-then-reflect approach where a model produces an answer and then generates self-feedback that it uses to refine and re-generate an improved answer (iterative self-refinement).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "LM (method-agnostic iterative self-feedback)",
            "model_description": "Iterative pipeline in which a single LM generates a candidate answer, produces critique/self-feedback, and conditions a revised answer on that feedback (authors: Madaan et al., 2023).",
            "reflection_method_name": "Iterative self-refinement / self-critique",
            "reflection_method_description": "Generate an initial solution, produce self-feedback or critique using the model's internal knowledge, and then revise the solution using that feedback; can be repeated multiple times.",
            "num_iterations": null,
            "task_name": "General multi-step reasoning tasks (as studied in the cited work)",
            "task_description": "Tasks that require multi-step reasoning where iterative self-feedback may correct errors in reasoning or computation.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited in related work as an example of enabling LMs to rectify their responses during inference (self-reflect). This paper does not reproduce Self-Refine experiments but notes it as prior work.",
            "limitations_or_failure_cases": "Paper cites that the effectiveness of internal self-correction approaches is debated and references subsequent critiques (e.g., Huang et al., 2024a) noting mixed results; no direct experiments in this paper.",
            "uuid": "e5217.2",
            "source_info": {
                "paper_title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: language agents with verbal reinforcement learning",
            "brief_description": "An agent-style approach that equips language models with external feedback loops (e.g., environment signals, reward/critique) to iteratively improve behavior using verbalized feedback and reinforcement-learning style updates.",
            "citation_title": "Reflexion: language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "LM agent with external feedback",
            "model_description": "A method where an LM agent receives external critiques or environmental feedback and uses verbalized reflections to adapt its behavior; used in agentic, iterative contexts (Shinn et al., 2023).",
            "reflection_method_name": "Agentic reflection with external feedback",
            "reflection_method_description": "The model receives external signals (code execution results, expert critiques, environment rewards) and produces reflective text that is used to update subsequent generations or policies.",
            "num_iterations": null,
            "task_name": "Agentic tasks and iterative problem solving (as studied by cited work)",
            "task_description": "Tasks where external feedback (execution traces, critic judgments) are available to evaluate and correct model outputs iteratively.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited as a prior approach that equips LMs with external feedback to rectify responses during inference; the present paper positions this line as distinct from training-time RefAug.",
            "limitations_or_failure_cases": "Not evaluated in this paper; cited to show prior uses of reflection that rely on external feedback rather than purely internal training-time augmentation.",
            "uuid": "e5217.3",
            "source_info": {
                "paper_title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Verification / Reverse Verification",
            "name_full": "Self-verification / reverse verification methods (backward reasoning)",
            "brief_description": "Methods that verify candidate answers by attempting to rederive the question from the generated answer or using backward-checks (model-based verification) to detect/correct errors.",
            "citation_title": "Forward-backward reasoning in large language models for verification",
            "mention_or_use": "mention",
            "model_name": "LM with self-verification",
            "model_description": "Techniques where an LM or system uses a generated answer to perform a reverse derivation or verification step (e.g., Weng et al., 2023; Jiang et al., 2023b), sometimes used as a filtering/verification mechanism.",
            "reflection_method_name": "Reverse verification / self-verification",
            "reflection_method_description": "After generating an answer, the model performs a verification pass (re-derivation or back-check) to confirm correctness; may act as a check to accept/reject or to refine the solution.",
            "num_iterations": null,
            "task_name": "Math verification / multi-step reasoning verification",
            "task_description": "Tasks where answers can be checked or rederived (e.g., algebraic checks, re-running computations) to confirm correctness.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited as a prior successful use of reflection-style verification for some math tasks; the present paper distinguishes training-time reflective augmentation from these inference-time verification approaches.",
            "limitations_or_failure_cases": "Paper notes that such verification approaches exist but does not evaluate them directly; some debate exists about the effectiveness of purely internal self-checks on certain tasks.",
            "uuid": "e5217.4",
            "source_info": {
                "paper_title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Internal self-correction (prompting)",
            "name_full": "Prompted internal self-correction / self-critique",
            "brief_description": "Approaches that prompt LMs to use only internal knowledge (without external tools) to critique and correct their answers at inference.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LM prompted for internal critique (unspecified)",
            "model_description": "Prompt-engineered schemes that ask an LM to critique its own output and produce a corrected answer, relying solely on internal model knowledge (examples cited: Madaan et al., 2023; Li et al., 2024b).",
            "reflection_method_name": "Prompted self-critique / internal reflection",
            "reflection_method_description": "Use of carefully designed prompts to have the model scrutinize and revise its own output without external verification resources.",
            "num_iterations": null,
            "task_name": "Various reasoning tasks (as referenced)",
            "task_description": "Tasks where the model can be asked to self-critique and re-answer; sometimes used for math and general reasoning.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper acknowledges prior work using internal self-correction but notes that its effectiveness is under debate (cites Huang et al., 2024a). The present work does not experimentally adopt inference-time internal self-critique; instead it focuses on adding reflection into training data.",
            "limitations_or_failure_cases": "Cited literature reports mixed effectiveness; the present paper references that relying solely on internal self-correction may not reliably improve reasoning in all cases.",
            "uuid": "e5217.5",
            "source_info": {
                "paper_title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Large language models are better reasoners with self-verification",
            "rating": 2,
            "sanitized_title": "large_language_models_are_better_reasoners_with_selfverification"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "When hindsight is not 20/20: Testing limits on reflective thinking in large language models",
            "rating": 2,
            "sanitized_title": "when_hindsight_is_not_2020_testing_limits_on_reflective_thinking_in_large_language_models"
        },
        {
            "paper_title": "Large language models can self-correct with minimal effort",
            "rating": 1,
            "sanitized_title": "large_language_models_can_selfcorrect_with_minimal_effort"
        }
    ],
    "cost": 0.0173305,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning
5 Oct 2024</p>
<p>Zhihan Zhang zzhang23@nd.edu 
University of Notre Dame</p>
<p>Tao Ge 
Tencent AI Lab
Seattle</p>
<p>Zhenwen Liang 
University of Notre Dame</p>
<p>Wenhao Yu 
Tencent AI Lab
Seattle</p>
<p>Dian Yu 
Tencent AI Lab
Seattle</p>
<p>Mengzhao Jia 
University of Notre Dame</p>
<p>Dong Yu 
Tencent AI Lab
Seattle</p>
<p>Meng Jiang 
University of Notre Dame</p>
<p>Aug Answer 
Jacob Austin 
Augustus Odena 
Maxwell I Nye 
Maarten Bosma 
Henryk Michalewski 
David Dohan 
Ellen Jiang 
Carrie J Cai 
Michael Terry 
Quoc V Le 
Zhangir Azerbayev 
Hailey Schoelkopf 
Keiran Paster 
Marco Dos Santos 
Stephen Mcaleer 
Albert Q Jiang 
Jia Deng 
Stella Biderman 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Pondé 
Oliveira Pinto 
Jared Kaplan 
Har- Rison Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sastry 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Lukasz Kaiser 
Matthias Plappert 
Jacob Hilton 
Reiichiro Nakano 
Christopher Hesse 
Alex Davies 
Petar Velickovic 
Lars Buesing 
Sam Blackwell 
Daniel Zheng 
Nenad Tomasev 
Richard Tanburn 
Peter W Battaglia 
Charles Blundell 
An- Drás Juhász 
Marc Lackenby 
Geordie Williamson 
Zhibin Gou 
Zhihong Shao 
Yeyun Gong 
Yelong Shen 
Yujiu Yang 
Minlie Huang 
Nan Duan 
Weizhu Chen 
Tora 
Daya Guo 
Qihao Zhu 
Dejian Yang 
Zhenda Xie 
Kai Dong 
Wentao Zhang 
Guanting Chen 
Xiao Bi 
Y Wu 
Y K Li 
Fuli Luo 
Yingfei Xiong 
Dan Hendrycks 
Collin Burns 
Steven Basart 
Andy Zou 
Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning
5 Oct 2024B133776D80077EE026F434DC7B4CED8FarXiv:2406.12050v3[cs.CL]AugQuestion AugAnswer Question Question Answer Reflection
Supervised fine-tuning enhances the problemsolving abilities of language models across various mathematical reasoning tasks.To maximize such benefits, existing research focuses on broadening the training set with various data augmentation techniques, which is effective for standard single-round question-answering settings.Our work introduces a novel technique aimed at cultivating a deeper understanding of the training problems at hand, enhancing performance not only in standard settings but also in more complex scenarios that require reflective thinking.Specifically, we propose reflective augmentation, a method that embeds problem reflection into each training instance.It trains the model to consider alternative perspectives and engage with abstractions and analogies, thereby fostering a thorough comprehension through reflective reasoning.Extensive experiments validate the achievement of our aim, underscoring the unique advantages of our method and its complementary nature relative to existing augmentation techniques. 1</p>
<p>Introduction</p>
<p>The ability to engage in step-by-step reasoning is pivotal for language models (LMs) to solve mathematical problems (Wei et al., 2022;Kojima et al., 2022).Supervised fine-tuning, particularly on data with detailed reasoning paths, effectively advances the problem-solving performance of LMs (Fu et al., 2023;Yue et al., 2023).To enlarge such benefits, most previous efforts focus on creating additional instances to augment model training (Luo et al., 2023a;Yu et al., 2024;Mitra et al., 2024;Li et al., 2024a).While these data expansion approaches allow LMs to handle a broader range of math problems by increasing the diversity of training data, stacking more training instances does not necessarily lead to a deeper understanding of each problem.Moreover, the scope of resulting models is confined to single-round question-answering (QA) settings that primarily require basic forward reasoning skills.Consequently, these methods provide limited benefits for more complex reflective reasoning scenarios that involve reviewing past steps for further reasoning, such as addressing follow-up questions, correcting errors, or leveraging external feedback (Liang et al., 2024;Wang et al., 2024a).</p>
<p>Similarly, the strategy in human learning is not always to practice an increasing number of problems (Rohrer and Taylor, 2006).Instead of merely memorizing superficial solutions to more problems, it can be more advantageous to gain a deep understanding of the existing problems (Semerci, 2005).Reflection, therefore, becomes an essential accompaniment to practice.Stacey et al. (1982) define reflection as "to review thoughtfully, consider alternatives and follow extensions", which encourages learners to contemplate their previous actions to engage in deeper reasoning, thereby fostering reflective thinking capabilities (Kagan et al., 1964;Anderson and Fincham, 2014).</p>
<p>Inspired by such human cognition, we propose a novel training strategy for LMs that integrates reflection into each math problem.Unlike traditional data expansion methods which operate on the instance dimension by adding more training examples (see Figures 1b &amp; 1c), our approach targets a complementary direction, i.e., the sequence dimension of the training data.We introduce reflective augmentation (RefAug), which appends a reflective section to the original answer of each training instance, advancing model learning beyond mere answer generation (see Figure 1d).Such a design not only strengthens the model's understanding of the associated knowledge and methodologies in training problems, but also maintains the inference efficiency as the model ceases generation before (Alternative Reasoning) The essence of substitution is to identify and simplify the common components of variable expressions by introducing a new variable, thereby reducing the complexity.Let's revisit the original equation.Expressions  − 99 and  − 101 share a similar form: a large constant offset from .Due to the minimal difference between 99 and 101, we can use substitution to transform the expressions into terms with small constants.</p>
<p>Therefore, an alternative approach is to substitute  =  − 99, which transforms the equation into   − 2 = 8 ⇒  !− 2 − 8 = 0.This can be easily factorized into  − 4  + 2 = 0. Hence,  = 4 or −2, leading to the same results  = 97 or 103.This alternative approach is equally effective as it also simplifies the equation by substituting  and reducing the scale of the offset terms.</p>
<p>(Follow-up Reasoning) Extending the idea of substitution, consider the equation   + 1 ( + 2)  + 3 = 360.We notice that   + 3 =  !+ 3, and  + 1  + 2 =  !+ 3 + 2. Therefore, to simplify the expression, we set the common term  !+ 3 as , which transforms the equation into   + 2 = 360 ⇒  !+ 2 − 360 = 0 ⇒  = −20 or 18.If  = −20, then  !+ 3 + 20 = 0. Here, the discriminant Δ = −71 &lt; 0, resulting in no real solutions for .If  = 18, then  !+ 3 − 18 = 0, so  = 3 or −6.This scenario reiterates the importance of identifying common components of  to streamline the equation through substitution.</p>
<p>Test Question: The unknown number  satisfies (7 + 1)(9 + 1) = 61.Find .</p>
<p>Let  = 8 + 1.The equation becomes  −  + 1 ( +  + 1) = 61.Next, we…???If we set  = 7 + 1, the original equation turns into ( + 2 + 1) = 61.Then we can get…???</p>
<p>Let's use substitution to solve the equation.To create a common structure for easier substitution, we multiply the expressions to unify the coefficient of : (7 + 1)×9 and (9 + 1)×7, yielding 63 + 9 63 + 7 = 61×63.Observing the equation, we notice that by letting  = 63 + 8, it can be rewritten as  + 1  − 1 = (62 + 1)(62 − 1) , which simplifies to
𝑡 ! = 62 ! ⇒ 𝑡 = ±62 . Therefore, 𝑥 = " # or 𝑥 = − $% &amp;
. (Early stopping: no reflection needed during inference)</p>
<p>Inference After Reflective Augmentation</p>
<p>Training After Standard Training</p>
<p>Training with Reflective Augmentation Standard Training</p>
<p>Figure 2: The model that learned the standard solution does not fully understand when and how to apply substitution when facing a different scenario.In contrast, the model trained with reflection on the substitution technique gains a deeper understanding of its principles, patterns, and its flexible application in new contexts.</p>
<p>decoding the reflective section during inference.Following the definition by Stacey et al. (1982), these reflective sections include two components: alternative and follow-up reasoning.For example, Figure 2 shows a scenario where the model struggles to apply the substitution technique in a different context if only rigidly transferring the pattern from the standard solution.In contrast, training the model to reflect on an equivalent substitution expression followed by devising a more challenging equation facilitates a deeper understanding of the principles and variations of the technique, thereby enabling flexible adaptation in new contexts.</p>
<p>Extensive experimentation on diverse math reasoning tasks reveals multiple benefits of RefAug:</p>
<p>(1) It boosts the problem-solving performance of LMs in the standard single-round QA settings, yielding a +7.2 accuracy gain over direct fine-tuning.(2) It remarkably enhances the LMs' performance in multiple reflective math reasoning scenarios, where traditional data expansion methods fall short.(3) Its benefits are complementary to those of existing data expansion techniques, allowing for seamless integration that leads to even greater performance improvements.</p>
<p>Related Work</p>
<p>Data Augmentation for Math Reasoning</p>
<p>Due to the scarcity (Li et al., 2024a) and quality issues (Fan et al., 2024) of human-annotated data, data augmentation is a prevalent strategy in math reasoning tasks.Most research focused on creating additional training instances, typically using advanced LMs to minimize human effort.This include question augmentation which generates new questions from existing ones (Yu et al., 2024 et al., 2024;Li et al., 2024a;Liu et al., 2024;Huang et al., 2024b), and answer augmentation which resamples the answer for each question (Yuan et al., 2023;Li et al., 2023;Yu et al., 2024).Others also explored answer refinement, aiming to insert additional reasoning details (Anonymous, 2024) or to restructure answers for clearer reasoning paths (Fan et al., 2024).Not only is reflective augmentation complementary to existing approaches, but it also exhibits unique advantages in reflective reasoning scenarios, as we will show in §4.</p>
<p>Another branch of research augmented code snippets within problem solutions, which transforms text reasoning into code generation (Wang et al., 2023a;Gou et al., 2024;Lu et al., 2024).This method is effective for math problems but is typically considered a separate track since it uses external tools (i.e., the code interpreter).Beyond supervised fine-tuning, some works augmented data for further preference optimization (Pang et al., 2024;Yuan et al., 2024), whereas we leave exploring reflective data in preference tuning for future work.</p>
<p>Reflection in LMs</p>
<p>Previous applications of reflection in LMs primarily focused on enabling LMs to rectify their own responses during inference (i.e., self-reflect).Some works equipped the LM with external feedback, such as code execution or expert critiques (Shinn et al., 2023;Chen et al., 2024).Others prompted LMs to use only internal knowledge to correct answers (Madaan et al., 2023;Li et al., 2024b), though the effectiveness of this approach is under debate (Huang et al., 2024a).Some specific tasks (e.g., math word problems) permit reverse verification, where the generated answer is used to rederive the question to confirm its correctness (Weng et al., 2023;Wu et al., 2024).These works demonstrate that reflection is a common aspect of language processing.However, RefAug explores augmenting reflective data for better training instead of answer refinement during inference.Unifying these approaches is a promising future study.</p>
<p>Approach</p>
<p>RefAug extends each training sequence with a reflective section that encourages the LM to reflect on its initial reasoning process to engage in further math reasoning.Figure 1 contrasts RefAug with traditional augmentation methods, and its detailed implementation is elaborated below.</p>
<p>Reflection Types Following the definition by Stacey et al. (1982) to "review thoughtfully, consider alternatives and follow extensions", we consider two types of reflection in composing the reflective section: alternative reasoning and followup reasoning.</p>
<p>Alternative reasoning involves thinking about the problem from different perspectives (Kagan et al., 1964;Wetzstein and Hacker, 2004).Therefore, besides the initial solution, we annotate an alternative approach that also effectively solves the problem.This helps the model master related methodologies and develop critical thinking skills.</p>
<p>Follow-up reasoning associates the initial solution to a broader class of problems (Silver, 1994;Lim et al., 2020).To fit various contexts, we consider two options: abstraction and analogy.Abstraction refers to creating a generalized form of the original problem, thereby encouraging the model to reduce dependency on specific numerical values.Analogy challenges the model in applying methodologies of solving the original problem to a more complex situation.Learning to design follow-up scenarios enables the model to understand the associated math concepts and principles better and apply them flexibly in new contexts.The relationship between the initial instance and components of the reflective section is illustrated in Figure 3.</p>
<p>Data Annotation</p>
<p>Following a common approach (Li et al., 2023;Yu et al., 2024;Li et al., 2024a), we employ an expert LM, GPT-4-turbo, to annotate the reflective sections for high-quality reasoning paths and minimal human effort 2 .This en-tails reviewing the original problem and solution to generate a section consisting of the aforementioned two types of reflective reasoning.We prompt the expert model to choose between abstraction and analogy in follow-up reasoning based on the problem context.Figure 2 shows an annotated example with alternative reasoning and follow-up analogy, and the full annotation prompt is in Appendix E. The manual inspection and quality analysis of GPTannotated data are detailed in Appendix A.4.</p>
<p>Training &amp; Inference During training, given a math question as input, we include the reflective section in the output immediately following the initial answer, starting with a Reflection: prefix.Thus, the training objective is to learn P([a; r]|q), where [; ] denotes sequence concatenation.Loss is calculated on tokens from both the initial answer and the reflective section.The format of the whole training sequence is detailed in Appendix D.</p>
<p>During inference, the generation early stops upon delivering the answer to the input question and ignores the reflective section, as shown in Figures 1-2.This is achieved by using Reflection: as a termination string during model generation.</p>
<p>Experiments</p>
<p>We test RefAug in a variety of mathematical tasks that cover both standard single-round QA and reflective reasoning scenarios.We mainly evaluate two aspects: the influence of RefAug on LMs' math reasoning abilities and its interaction with existing augmentation techniques.Besides, we extend our approach to code generation tasks and perform comprehensive analyses.</p>
<p>Standard Math Reasoning</p>
<p>Settings</p>
<p>Standard math reasoning tasks follow a singleround QA format.Following a popular approach, we use the training sets of GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b).We additionally include out-of-distribution test sets from MAWPS (Koncel-Kedziorski et al., 2016), Mathematics (Davies et al., 2021), SVAMP (Patel et al., 2021), plus the math subsets of MMLU (Hendrycks et al., 2021a) and SAT (Zhong et al., 2023).We mainly experiment with two LMs known for superior reasoning performance: Mistral-7B (Jiang et al., 2023a) and Gemma-7B (Mesnard et al., 2024), and have also tested LLaMA-3-8B (Meta, but its performance lags behind GPT-4-turbo.2024) in Appendix A.1.Models are trained for 3 epochs with batch size 128.The learning rate peaks at 1e-5 with a 3% warmup period followed by linear decay.Greedy decoding is applied during inference.Additional details of datasets and training settings are in Appendix B.1.</p>
<p>Existing Training Methods</p>
<p>• Standard Fine-tuning (Figure 1a): Utilizes original problem solutions from GSM8k and MATH, each containing a chain-of-thought reasoning process before reaching the final prediction.• Question Augmentation (Q-Aug, Figure 1b):</p>
<p>Involves training on both original and GPTaugmented questions.We adopt the augmentation prompt from Li et al. (2024a), detailed in Appendix C. We also explore Q-Aug + RefAug by applying RefAug to all questions after Q-Aug, and Q-Aug×2 by adding a second augmentation round to further expand the dataset.• Answer Augmentation (A-Aug, Figure 1c): Resamples the solution for each problem using GPT-4-turbo, following the approach of Yu et al. (2024).We also explore its combination with Q-Aug (A-Aug + Q-Aug), RefAug (A-Aug + Re-fAug), and another round of A-Aug (A-Aug×2).The augmentation prompt for Q-Aug and A-Aug, along with the sampling strategy on MetaMath can be found in Appendix C.</p>
<p>Results</p>
<p>Table 1 lists the QA accuracy of fine-tuned LMs.We summarize several findings on RefAug:</p>
<p>Enhancement in Single-Round Math Reasoning: RefAug boosts model performance across both in-distribution and out-of-distribution tasks, outscoring the direct fine-tuning approach by +7.2 across two base LMs.As the reflective section is not utilized during inference, this advancement un-</p>
<p>Model Training Data</p>
<p>In-Distribution Out-Of-Distribution Avg.</p>
<p>GSM MATH Mathematics MAWPS SVAMP MMLU-Math SAT-Math</p>
<p>Closed-Source Models derscores RefAug's role in enhancing model learning, which strengthens math problem-solving capabilities without providing additional context.</p>
<p>Complementary Benefits with Existing Methods: While data expansion methods (Q-Aug, A-Aug, and MetaMath) have improved model performance, combining RefAug with them leads to further substantial gains, improving overall accuracy by +6.1 on average.This demonstrates that RefAug still holds value on high-quality data3 and is complementary to data expansion strategies.Furthermore, such synergistic benefits outpace the diminishing returns seen with repeated dataset expansions: these three methods bring +6.8 improvement initially but only +2.3 in the second round.This disparity indicates that expanding data does not always yield proportionate gains, whereas the balance of practicing new problems and reflecting on existing ones maximizes the learning effect.</p>
<p>Effectiveness on Large Datasets: Even when only 10% of the full-sized MetaMath dataset includes the reflective section, the resulting model surpasses the public MetaMath checkpoint by ~2 points.This confirms RefAug's efficacy on larger scales of data.Additionally, the MetaMath model barely benefits from continual training on its original QA data, suggesting a good memorization of these math problems.Nevertheless, RefAug still manages to elevate its performance, indicating that the model has not fully internalized the dataset's knowledge and RefAug effectively deepens the model's understanding of these problems.</p>
<p>Reflective Math Reasoning</p>
<p>Tasks</p>
<p>Many realistic math applications require models to reflect on previous predictions and perform further reasoning.We employ three tasks of this kind: the follow-up QA (FQA) and error correction (EC) tasks of MathChat (Liang et al., 2024), and the math subset of MINT (Wang et al., 2024a).FQA involves solving two subsequent questions linked  to each initial query, forming a three-round interaction.EC deliberately writes an erroneous solution to test the model's error identification and correction abilities.MINT evaluates the model's ability to leverage external language feedback to improve its reasoning process through up to k turns of interaction.More task details are in Appendix B.2.
Training Data MathChat-FQA MathChat-EC MINT-Math 1st 2nd 3rd k = 1 k = 2 k = 3 k = 4 k =</p>
<p>Results</p>
<p>Results on reflective math reasoning tasks are displayed in Tables 2-3 for Mistral and Table 13 for Gemma.We summarize the key findings below.</p>
<p>Challenges for Data Expansion Methods: Despite improving single-round QA performance, methods like Q-Aug, A-Aug, and MetaMath fall short in enhancing LMs' reflective reasoning abilities.For instance, these methods hurt Mistral's error correction performance.Moreover, a second round of augmentation yields minimal or negative gains across key metrics on reflective reasoning: +2.5 in FQA-3rd, -1.1 in EC, -0.5 in MINT k=5 , and -4.2 in MINT ∆ .This indicates that initial augmentation benefits are mainly due to the improved answer quality from GPT annotation 3 rather than an actual increase in reflective reasoning skills, which echos the findings of Liang et al. (2024) that conventional training approaches overly focus on the single-round QA setting and neglect many other important mathematical scenarios.Superiority of RefAug in Enhancing Reflective Reasoning: RefAug significantly enhances the model's reflective reasoning performance, with gains of +12.3 in FQA-3rd, +22.3 in EC, +10.6 in MINT k=5 , and +9.2 in MINT ∆ , far exceeding the corresponding improvements of +7.9, +15.5, +5.0, and +3.4 brought by three data expansion methods on average.An effective solution, however, is to combine RefAug with these methods, which yields substantial improvements over them, e.g., +12 on FQA-3rd and +10.1 on MINT k=5 .These results highlight RefAug's exceptional capability to improve LMs' reflective math reasoning, which complements the disregard of existing augmentation methods on this dimension.Based on findings from §4.1 and §4.2, we conclude the benefits of RefAug on math reasoning as: Not only does it enhance LMs' basic problemsolving skills but also advances their reflective reasoning abilities, making it a valuable complement to existing augmentation techniques.</p>
<p>Code Generation</p>
<p>Besides math reasoning, we extend the application of RefAug to code generation.In this task, a query instructs the model to craft a code snippet that fulfills a specific functionality, which also requires a step-by-step logical flow.We use HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) as the evaluation benchmarks, along with their plus versions provided by EvalPlus (Liu et al., 2023).Training is conducted using the Python subset of Magicoder-OSS-Instruct (Wei et al., 2023), which includes 38K QA instances.Considering the abstractive nature of code, we annotate problem analogies as the follow-up section of RefAug.</p>
<p>The outcomes are summarized in Table 5, covering four different base LMs: CodeLLaMA (Rozière et al., 2023), Mistral, StarCoder2 (Lozhkov et al., 2024), andDeepSeekCoder (Guo et al., 2024).The results demonstrate that RefAug consistently elevates the LMs' proficiency in following instructions to generate accurate, reasonable code, as evidenced by an average improvement of +3.5 in Pass@1 across the evaluated benchmarks.These results indicate that RefAug is able to enhance LMs' capabilities in solving code problems, which reaffirms from another scenario that reflection is an essential ability for LMs to possess.
0 1/8 1/4 1/</p>
<p>Analysis</p>
<p>In this section, we dive deeper into additional aspects of RefAug.Results are tested on Mistral.</p>
<p>Ablation Study</p>
<p>To further assess the efficacy of the reflective section, we conduct an ablation study on its two components: alternative and follow-up reasoning.According to Table 4, incorporating any single reflective component to the original data significantly enhances model performance by an average of +4.5 points.This suggests that the original solutions lack sufficient information for the model to fully grasp the math reasoning skills, which is consistent with the findings of Anonymous (2024).Combining both reflective components further enhances the model's comprehension of associated concepts and methodologies, improving the performance by +2.3 points over using any single one.</p>
<p>The Amount of RefAug Data</p>
<p>We explore the impact of varying the quantity of reflection-augmented instances in the whole training set.As depicted by Figure 4, the model's overall performance continually improves as more instances are augmented with reflective sections.When the model is trained through reflecting on all instances, the model maximizes its grasp of the training data and reaches the best performance, underscoring the scalability of RefAug's benefits.Table 7: We sample the reflective sections three times using the same annotation prompt in Figure 8, and train a separate Mistral model using each batch of the augmented data (labeled as #1~#3).The last row lists the average scores of three runs as well as their standard deviation.</p>
<p>Error Analysis</p>
<p>We analyze how the model's math capabilities has been enhanced through the lens of an error analysis.Following Li et al. (2024a), we classify errors in GSM8k into calculation errors and reasoning errors.Calculation errors include incorrect identification of arithmetic relationships or wrong numerical computations.Reasoning errors include mistakes pertaining to the reasoning logic, e.g., incoherent reasoning steps, misunderstandings of the problem, etc.Using the gold reasoning paths from GSM8k test data as a benchmark, we employ GPT-4 to determine whether solutions contain calculation errors, reasoning errors, or both.As shown in Table 8, the improvement mostly comes from the reduction of reasoning errors.This supports the hypothesis that training with reflection enhances the model's problem-solving accuracy by deepening its grasp of underlying math reasoning skills.</p>
<p>Stability of RefAug Data Annotation</p>
<p>To verify the stability of the improvements and to avoid bias from cherry-picking augmented data, we sampled reflective sections three times using GPT-4-turbo with the same prompt in Figure 8.Each batch of augmented data is used to train a separate model.As shown in Table 7, the performance gains are consistent across all augmentation samples, with a minimal standard deviation of 0.3 in overall accuracy.These results confirm that reflective practices aid in model learning and that the observed improvements are not due to the variability of data sampling.</p>
<p>Data Annotation with Open-Source Models</p>
<p>Besides using GPT to annotate RefAug data, we explore whether state-of-the-art open-source models can also serve as data annotators.We employ LLaMA-3-70B-Instruct (Meta, 2024) for data annotation using the same prompt shown in Figure 8, and train a Mistral-7B model based on this data.In addition to the above perspectives, further analyses of RefAug's impact on model efficiency are presented in Appendix A.5.</p>
<p>According to results in</p>
<p>Conclusion</p>
<p>This paper proposed reflective augmentation (Re-fAug) for math reasoning, a method that incorporates reflection into training problems and is complementary to existing data augmentation approaches.We proved the efficacy of RefAug in not only enhancing LMs' basic problem-solving skills on single-round math problems but also in cultivating their capabilities to solve more complex reflective reasoning tasks.We further verified the effectiveness of RefAug in code generation tasks and its scalability, along with ablation studies and analyses of the methodological choices, such as the impact of data sequencing and the stability of the annotation process.</p>
<p>Limitations</p>
<p>Some previous data augmentation studies in math reasoning created millions of data instances with OpenAI's GPT models (Li et al., 2024a;Tang et al., 2024;Huang et al., 2024b).While testing our method at a similar scale would be valuable, budget constraints limit our ability to do so.For instance, our augmentation data for MetaMath is capped at 40K instances.In §4.4.6, we note that LLaMA-3-70B shows some promising performance in annotating RefAug data for math reasoning tasks, though its capabilities have not fully matched those of GPT-4 yet.We anticipate that the development of stronger open-source models will reduce researchers' dependence on paid services of proprietary models.</p>
<p>A Additional Experiments</p>
<p>In this section, we present more experimental results in addition to those in §4.</p>
<p>A.1 Results on LLaMA-3</p>
<p>In addition to training Mistral-7B and Gemma-7B with RefAug, we also test LLaMA-3-8B (Meta, 2024) on the RefAug data.According to the results in Table 11, RefAug enhances the math reasoning capabilities of LLaMA-3 as well, no matter if integrating with the original solutions or with solutions re-written by GPT-4-turbo.This again shows the generalizability of the RefAug method, which leads to consistent improvements across various base models.</p>
<p>A.2 Gemma on Reflective Math Reasoning</p>
<p>Besides evaluating Mistral-based models on reflective reasoning tasks (shown in</p>
<p>A.3 Quality of GPT-Written Answers</p>
<p>In Table 1, we find that answer augmentation significantly enhances performance.It improves the overall accuracy by +9.1 over the use of original training data, when averaged across Mistral and Gemma models.This surpasses the improvement of +7.2 on average seen with RefAug over the original data.A deeper analysis reveals that the reasoning paths generated by GPT-4-turbo are of significantly higher quality than those originally provided in the GSM8k and MATH datasets.</p>
<p>As demonstrated in Table 12, merely replacing the original solutions with those generated by GPT-4turbo increased the accuracy from 40.15 to 49.18 on Mistral.However, RefAug does not receive such benefits as it does not alter the original reasoning paths during augmentation.Given the complementary nature of these two augmentation methods, their combination further improves the model accuracy to 54.79.This echoes the synergistic per-  formance advantage achieved by A-Aug+RefAug over both A-Aug and A-Aug×2 in Table 1.</p>
<p>A.4 Quality of GPT-annotated Reflective Sections</p>
<p>We analyze the correctness of GPT-annotated reflective sections by manually reviewing 50 samples (25 from GSM8K, 25 from MATH) in the training set.The results, as shown in Table 14, indicate that generating reflective sections is generally easier for GPT than solving entirely new problems.This is due to the fact that we provide both the original problem and solution during RefAug annotation.Consequently, the correctness of the annotated reflective sections is generally satisfactory.Verification of LM-generated data is a common challenge in data augmentation.We did not dive deep into answer verification in this paper for two reasons: (1) Common methods like selfconsistency voting or LM-based validation are orthogonal to our study's focus on different augmentation types.(2) Studies have indicated that data verification often does not lead to significant performance gains, and noisy answers could help training as well (Yu et al., 2024;Tang et al., 2024;Li et al., 2024a).This is because such answers often include many correct reasoning steps before making an error, and filtering them trades data diversity for correctness.</p>
<p>A.5 Training and Inference Efficiency</p>
<p>For a deeper understanding of RefAug, we analyze its impact on the efficiency of model training and inference.To begin with, according to</p>
<p>B Detailed Task Settings</p>
<p>In this section, we detail the datasets, training hyper-parameters, and evaluation settings of each task used in our experiments.We list the size of all datasets in Table 17.</p>
<p>B.1 Standard Math Reasoning</p>
<p>Datasets In standard math reasoning, we follow a common approach (Wang et al., 2023a;Yu et al., 2024;Li et al., 2024a)  data under the standard fine-tuning recipe.then, these settings remain fixed across all models to avoid extensive hyper-parameter tuning for each variant.This approach is common in studies comparing models fine-tuned on varied datasets (Yuan et al., 2023;Li et al., 2023;An et al., 2023).Specifically, we train models for 3 epochs with a batch size of 128.The learning rate starts at 1e-5, including a warmup for the initial 3% of steps, and then linearly decreases to 20% of its initial value by the end of training.Training sequences are truncated to 4096 tokens.To speed up training, our model utilize bfloat16 precision and are supported by FlashAttention-2 (Dao, 2023), DeepSpeed (Rasley et al., 2020), and ZeRO-3 optimization (Rajbhandari et al., 2020).For training on the full set of MetaMath, we follow the original authors' recommendation4 to lower the learning rate to 2e-6, and for continued training on the public MetaMath checkpoint, we use a reduced learning rate of 1e-6 to be more consistent with its initial fine-tuning.</p>
<p>Evaluation To facilitate answer extraction during evaluation, we append The answer is XXX. to the reasoning path of each training instance so that the final predicted answer is explicitly stated.We adopt the evaluation script from Yue et al. (2023) that first extracts the predicted answer and then checks for an exact match with the ground-truth.Exceptions are MMLU and SAT which use multiple-choice formats instead of numerical answers.Since our training data does not contain multiple-choice questions, the model may predict the content of an op-tion rather than its letter identifier.Thus, on these datasets, we leverage GPT-3.5-turbo to match the predicted content to the appropriate option before computing accuracy.</p>
<p>B.2 Reflective Math Reasoning</p>
<p>Reflective math reasoning encompasses scenarios where models must consider previously provided answers to engage in further reasoning.However, benchmarks that adequately capture this dynamic are scarce in the existing literature.Utilizing the currently available resources, we evaluate our models on three tasks: follow-up QA, error correction, and feedback utilization.</p>
<p>The follow-up QA (FQA) task is assessed using the MathChat dataset (Liang et al., 2024).Each test instance consists of three turns of questions.The first turn uses the original GSM8k test set, and subsequent turns contain follow-up questions based on earlier turns.These follow-ups often require a deeper understanding of the problem, such as performing subsequent calculations based on previous answers or introducing new constraints to the original question.The solutions generated by the model for each turn are incorporated into the input for the next turn, creating a multi-turn interaction.The accuracy of each turn is evaluated separately.</p>
<p>The error correction (EC) task, also sourced from the MathChat dataset and derived from the GSM8k test set, pairs each question with an intentionally incorrect answer.The model is then tasked with identifying and correcting errors in the reasoning process.Accuracy is determined by comparing the model's corrected answer to the ground truth.</p>
<p>For both tasks from MathChat, we follow the approach of Liang et al. (2024) to concatenate all previous turns into the instruction part of the input sequence.For example, in the third round of FQA, the model decodes P(a 3 |[q 1 ; a 1 ; q 2 ; a 2 ; q 3 ]); In EC, it decodes P(a|[q; a wrong ; f ]), where f is binary feedback indicating that a wrong is incorrect.</p>
<p>The MINT (Wang et al., 2024a) benchmark evaluates the ability of LMs to leverage natural language feedback to improve their predictions.We utilize the math subset from the original benchmark, which includes 273 carefully selected instances from four datasets: 48 from GSM8k, 100 from MATH, 76 from MMLU, and 49 from Theo-remQA (Chen et al., 2023).We adhere to the same evaluation protocols as the original paper except that we omit the code execution step as our math models are based on text reasoning.At each in-teraction turn, the model proposes a solution, and we collect binary feedback on answer correctness along with natural language feedback from an expert (i.e., .This feedback is then provided to the model in the subsequent turn of prediction.The model have at most k = 5 chances to propose solutions, and the accuracy of each turn is calculated independently.We also measure the improvement in accuracy (∆) from the first to the fifth turn to assess the model's efficacy in leveraging feedback.</p>
<p>B.3 Code Generation</p>
<p>HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are the most popular benchmarks for evaluating code generation capabilities of LMs (Luo et al., 2023b;Wang et al., 2024b).Each test instance within these benchmarks includes a natural language prompt, based on which LMs generate a corresponding code snippet.The correctness of the code is verified using test cases.Additionally, EvalPlus (Liu et al., 2023) has developed enhanced versions of these benchmarks (HumanEval+ / MBPP+) that include more comprehensive test cases for a more rigorous evaluation.Therefore, we utilize the evaluation suite provided by EvalPlus on these benchmarks, where MBPP is reduced to 399 instances for quality control.</p>
<p>For the training dataset, we use the OSS-Instruct dataset collected by Magicoder (Wei et al., 2023), which consists of synthetic instruction-code pairs generated from random code snippets sourced from GitHub.Since HumanEval and MBPP focus on Python code, we extracted the Python subset from OSS-Instruct to reduce annotation costs, resulting in a total of 38K training instances.Given the abstractive nature of code generation, we opt for analogy annotations in the follow-up reasoning part of RefAug.</p>
<p>We adhere to the training settings outlined in the Magicoder paper for our experiments.Models are trained over two epochs with a batch size of 512.The learning rate is initiated at 5e-5, with 15 warm-up steps followed by a linear decay.Greedy decoding is employed during inference.</p>
<p>C Baseline Implementation</p>
<p>In this section, we detail our implementation of the major baseline methods that we compare with in the main paper, including question augmentation (Q-Aug), answer augmentation (A-Aug), and MetaMath augmentation.</p>
<p>C.1 Question Augmentation</p>
<p>A single round of Q-Aug enerates a new question from each existing question in the training set, effectively doubling the dataset (illustrated in Figure 1b).Both the augmented question and its solution are annotated by GPT-4-turbo.During the annotation, we employ a temperature of 0.7 and a top_p of 1.0 to ensure the diversity of math reasoning paths for both Q-Aug and A-Aug.we largely follow the question generation prompt from Li et al. (2024a) with minor adjustments.The detailed annotation prompt is provided in Figure 6.</p>
<p>C.2 Answer Augmentation</p>
<p>A single round of A-Aug involves re-sampling a solution for each math problem in the training set.The new solution, paired with the original question, forms a new training instance (illustrated in Figure 1c).Consistent with other methods, the augmented solution is generated by GPT-4-turbo.If the sampled solution diverges from the gold answer, it is discarded and re-sampled; And if a correct answer is not produced after five attempts, we retain the last sampled solution.Following the methodology described by Yu et al. (2024), the prompt for A-Aug simply instructs the model to solve an arbitrary math problem, which is detailed in Figure 7.</p>
<p>C.3 MetaMath</p>
<p>MetaMath (Yu et al., 2024) introduces a comprehensive suite of augmentation methods tailored for math reasoning tasks, which has received much attention.This suite includes answer augmentation, question rephrasing, and two backward reasoning augmentation techniques: self-verification (Weng et al., 2023) and FOBAR (Jiang et al., 2023b).Each method is sampled for multiple rounds to generate a large set of 400K training data.Please refer to Yu et al. (2024) for more details on these methods.</p>
<p>When creating the MetaMath 40k subset for our experiments in §4.1, we randomly select one instance from each of the four augmentation techniques for every seed math question, which we believe is the most uniform sampling strategy.For the MetaMath 80k subset, we add one more instance from each technique for every seed question.The initially sampled 40K instances are further equipped with RefAug to be included in the full-dataset training (MetaMath 400k +RefAug 40k ).</p>
<p>D Training Prompt</p>
<p>The prompt we use to build training sequences is shown in Figure 5.The format mainly follows Wang et al. (2023b), and the reflection section is appended to the original answer as the output.Loss is only calculated to tokens after &lt;|assistant|&gt;.</p>
<p>E RefAug Annotation Prompt</p>
<p>The prompt we use for annotating reflective sections are detailed in Figure 8, which includes a description of the general principles of reflective reasoning and two in-context examples.We use tem-perature=0.7 and top_p=1.0 when sampling with GPT-4-turbo.</p>
<p>F License of Artifacts</p>
<p>We note that the collection of RefAug data, if annotated by an external model, should comply with its terms of use.For example, using GPT-generated data is subject to the terms of use of OpenAI services5 , and using LLaMA-generated data is subject to Meta's LLaMA license agreement6 .</p>
<p>Question Augmentation Prompt</p>
<p>Please act as a professional math teacher.Your goal is to create high quality math problems to help students learn math.You will be given a math question.Please generate a similar but new question according to the Given Question.</p>
<p>You have four principles to do this.# Ensure the new question only asks for one thing, be reasonable, be based on the Given Question, and have a definite answer.For example, DO NOT ask, "what is the amount of A, B and C?". # Ensure the new question is in line with common sense of life.For example, the amount someone has or pays must be a positive number, and the number of people must be an integer.# Ensure your student can answer the new question without the given question.If you want to use some numbers, conditions or background in the given question, please restate them to ensure no information is omitted in your new question.# Ensure your created question is solvable.Write the solution to it after the question.</p>
<p>Given Question: $$QUESTION$$ Now write a new question and its solution.The question must begin with "New Question:" and the solution must begin with "Solution to the New Question:".The solution must end with "The answer is XXX" where XXX should be the final answer to the question.</p>
<p>Figure 6: Prompt for question augmentation, adopted from Li et al. (2024a).The only difference is that we combine question generation and solution annotation into a single prompt to save costs.</p>
<p>Answer Augmentation Prompt</p>
<p>Your task is to solve a math word problem.You should solve the problem step by step.At the end of your solution, write the final answer in the form of "The answer is X".Here are two examples: ## Example 1 Question: Let  != (0,1) and  " = (4,1).Then the set of points  such that  !+  " = 6 form an ellipse.The equation of this ellipse can be written as
($%&amp;) ! ( ! + ()%*) ! + !
= 1.Find ℎ +  +  + .</p>
<p>Solution:</p>
<p>We have that 2 = 6, so  = 3.The distance between the foci is 2 = 4, so  = 2. Hence,  =  " −  " = 5.The center of the ellipse is the midpoint of  ! " , which is (2,1).Thus, the equation of the ellipse is
($%") ! , ! + ()%!) !
( -) != 1.Hence, ℎ +  +  +  = 2 + 1 + 3 + 5 = 6 + 5.The answer is 6 + 5.</p>
<h2>Example 2</h2>
<p>Question: Each bird eats 12 beetles per day, each snake eats 3 birds per day, and each jaguar eats 5 snakes per day.If there are 6 jaguars in a forest, how many beetles are eaten each day?Solution: First find the total number of snakes eaten: 5 snakes/jaguar × 6 jaguars = 30 snakes.Then find the total number of birds eaten per day: 30 snakes × 3 birds/snake = 90 snakes.Then multiply the number of snakes by the number of beetles per snake to find the total number of beetles eaten per day: 90 snakes × 12 beetles/snake = 1080 beetles.The answer is 1080.Now solve the following problem.The solution must end with "The answer is XXX" where XXX should be the final answer to the question.</p>
<p>Question: $$QUESTION$$ Solution:</p>
<p>Figure 7: Prompt for answer augmentation, which is basically an in-context learning prompt for solving a given math problem.Two in-context examples come from MATH and GSM8k training sets, respectively.</p>
<p>Data Annotation Prompt</p>
<p>You are a professional math teacher, and your goal is to teach your student to learn a given math problem.Now that your student has successfully solved the original problem, in order to make the student thoroughly understand the involved knowledge and problem-solving methodology, your task is to write a reflection section that go through the problem-solving process and provide additional insights.The reflection section should include the following components:</p>
<ol>
<li>
<p>Alternative Reasoning: Present an alternative approach to solve the original problem.This alternative approach should be distinct from the original solution and still lead to the correct answer.While writing the alternative reasoning approach, consider explaining the principle of the methodology used in the original solution, how the alternative approach differs from the original method, and why it leads to the same correct answer.</p>
</li>
<li>
<p>Follow-up Reasoning: Associate the solution to a broader class of problems.You can either create a general form of the original problem to encourage the student to reduce reliance on specific values (e.g., use letters or variables to replace specific numbers in the original problem), or apply the concepts and methodologies from the original problem to a more challenging situation.Please do not just replace the original numbers in the question with new numbers, because that is essentially the same problem.The follow-up problem must also be solvable, and you need to provide the solution for it.Besides, please explain briefly how the new scenario associates with the original problem.</p>
</li>
</ol>
<p>Example 1:</p>
<p>Original Problem: Youngsville had a population of 684 people.The town had a growth spurt and the population increased by 25% then they witnessed that 40% of the population moved away.What is the current population?Solution to the Original Problem: The town had 684 people, and then had a 25% growth spurt, so the population increased by 684×0.25 = 171 people.This increase brought the population to 684 + 171 = 855 people.40% of the population moved away, so 855×0.40= 342 people moved away.The new population is 855 − 342 = 513 people.The answer is 513.</p>
<p>Alternative Reasoning:</p>
<p>The key to solve the problem is to understand the concept of relative increase and decrease percentages.Increasing by % means the population grows to (100 + )% of the original, while decreasing by % means the population reduces to (100 − )% based on the increased population.Therefore, this is essentially a problem of consecutive multiplication: multiply the initial total population by the percentage of change twice.Therefore, an alternative calculation involves deriving a single effective percentage change of the whole process.A 25% increase is equivalent to multiplying by 1.25, and a 40% decrease is equivalent to multiplying by 0.60.Combining these two changes, the effective percentage change is 1.25×0.60= 0.75, which corresponds to a 25% decrease from the original population.Therefore, the current population is 684×0.75= 513.The alternative approach leads to the same result because the associative property of multiplication: (684×1.25)×0.60= 684×(1.25×0.60)= 684×0.75= 513.</p>
<p>Follow-up reasoning: Let's think of a more general scenario.Suppose a town has a population of  people.The population increases by  percent, then  percent of the population moves away, and we would like to know the final population.In this context, the first increase corresponds to multiplying by (1 + /100), and the subsequent decrease corresponds to multiplying by (1 − /100).So the total population change is (1 + /100)(1 − /100).Therefore, the final population is (1 + /100)(1 − /100).This abstract problem allows us to apply the same principles of relative percentage changes to calculate the final population based on the initial population and the two percentage changes.This generalization helps to understand the problem conceptually and apply it to various scenarios.</p>
<p>Alternative Reasoning:</p>
<p>The essence of substitution is to identify and simplify the common components of variable expressions by introducing a new variable, thereby reducing the complexity.Let's revisit the original equation.Expressions  − 99 and  − 101 share a similar form: a large constant offset from .Due to the minimal difference between 99 and 101, we can use substitution to transform the expressions into terms with small constants.Therefore, an alternative approach is to substitute  =  − 99, which transforms the equation into ( − 2) = 8 ⇒  !− 2 − 8 = 0.This can be easily factorized into ( − 4)( + 2) = 0. Hence,  = 4 or  = −2, leading to the same results  = 97 or  = 103.This alternative approach is equally effective as it also simplifies the equation by substituting  and reducing the scale of the offset terms.</p>
<p>Follow-up Reasoning: Extending the idea of substitution, consider the equation ( + 1)( + 2)( + 3) = 360.We notice that ( + 3) = ^2 + 3, and ( + 1)( + 2) =  !+ 3 + 2. Therefore, to simplify the expression, we set the common term  !+ 3 as , which transforms the equation into ( + 2) = 360 ⇒  !+ 2 − 360 = 0 ⇒  = − 20 or  = 18.If  = −20, then  !+ 3 + 20 = 0. Here, the discriminant Δ = −71 &lt; 0, resulting in no real solutions for .If  = 18, then  !+ 3 − 18 = 0, so  = 3 or  = −6.This scenario reiterates the importance of identifying common components of x to streamline the equation through substitution.Now write a reflection section for the following case based on the examples above.Make sure to use "Alternative Reasoning:" and "Follow-up Reasoning:" to separate the two components.</p>
<p>Figure 1 :
1
Figure 1: Question augmentation creates new questions based on existing ones.Answer augmentation re-samples answers for each problem to increase diversity.Both methods expand the size of the training set.Reflective augmentation appends the original answer with a reflective section, which is complementary to traditional approaches.Corresponding training sequences are shown in an (input, output) format, where augmented parts are in red.</p>
<p>FindFigure 3 :
3
Figure 3: Relationship between the original instance and the reflective section.Either abstraction or analogy is annotated for each instance.Core ideas are shown but textual explanations (like those in Figure 2) are omitted.</p>
<p>Figure 5 :
5
Figure 5: Prompt used for training the model.Text in gray are placeholders and will be replaced by the corresponding sections in the training instance.</p>
<p>the equation ( − 99)( − 101) = 8.Solution to the Original Problem: Let t=x-100.Then the equation becomes ( − 1)( + 1) = 8, which transforms into  !− 1 = 8.Therefore,  = 3 or  = −3, and accordingly we get  = 97 or  = 103.The answer is 97 or 103.</p>
<p>Figure 8 :
8
Figure8: Prompt for annotating the reflective section.The prompt first explains the contents to annotate within the reflective section, and then presents two in-context examples for demonstration.GPT-4-turbo is employed for annotation.</p>
<p>Table 2 :
2
Accuracy on reflective math reasoning tasks.Each question in MathChat-FQA has two subsequent questions (2nd and 3rd turns), and the accuracy of each turn is calculated separately.MINT evaluates whether the model solves the math problem within k interaction turns with the feedback from GPT-4, and we use the difference (∆) between k = 5 and k = 1 to indicate the model's ability in leveraging external feedback.
5∆</p>
<p>Table 3 :
3
Liang et al. (2024)mpared with other opensource 7B math models.Baseline scores are fromLiang et al. (2024).The best scores are bolded and the second bests are underlined.GPT models are listed as a reference for state-of-the-art performance.*Including both supervised fine-tuning and reinforcement learning data.</p>
<p>Table 4 :
4
Accuracy on standard math reasoning tasks when varying the components of the reflective section.
Comparison with Existing Open-Source Mod-els: Our RefAug-enhanced models excel in thereflective reasoning scenarios of MathChat withjust 30K training instances, surpassing many open-source models trained on larger math datasets orwith reinforcement learning. This further supportsRefAug's effectiveness in cultivating LMs' reflec-tive reasoning skills in solving math problems.</p>
<p>Table 5 :
5
Pass@1 on code generation, scored by EvalPlus.-stddenotes training with the standard QA setting.</p>
<p>Table 6 :
6
Comparison between RefAug and prepending the reflective section to the answer (RefAug-front).
DataGSM MATH Mathematics MAWPS SVAMP MMLU SAT Avg. FQA-2nd FQA-3rd ECA-Aug66.19 23.0823.9081.1062.2037.78 40.91 47.8834.2923.60 72.08+RefAug-front 72.78 27.3428.3084.6270.3047.23 56.82 55.3430.9620.64 68.29+RefAug72.93 29.4031.2084.4171.5047.74 60.45 56.8044.9236.19 80.20DataGSMMATH Mathematics MAWPS SVAMP MMLU-Math SAT-MathAvg.Standard56.2513.9614.8073.0753.5037.6831.8240.15+ RefAug #160.0517.3619.4080.2559.3043.6348.6446.95+ RefAug #262.7017.2619.2082.1660.4042.5144.5546.97+ RefAug #360.8016.8618.6080.2959.7042.9245.4546.37+ RefAug (Avg.) 61.18 ±1.1 17.16 ±0.219.07 ±0.380.90 ±0.9 59.80 ±0.443.02 ±0.546.21 ±1.7 46.76 ±0.3</p>
<p>Table 8 :
8
Error analysis on GSM8k test set.The reduction of errors is denoted in gray parentheses.
Training Reasoning Calculation TotalStandard424287577RefAug374(-50)264(-23)527Thisarrangement can be regarded as augmenting thechain-of-thought (CoT, Wei et al., 2022) for solv-ing the original problem. According to Table 6,since the reflective section contains relevant rea-soning steps to the original problem, integrating itinto CoT yields similar improvements as RefAugon single-round QA. However, such setup hurtsperformance in reflective math reasoning, whichsupports the original design of RefAug in devel-oping reflective reasoning skills and reaffirms thatreflective reasoning demands distinct capabili-ties from standard forward reasoning. Besides,augmenting CoT increases the token count requiredfor predicting the final answer, thereby reducinginference efficiency (see Appendix A.5 for details).
4.4.3RefAug vs. Chain-of-ThoughtFor a deeper understanding of the reflective section, we experiment with positioning it before the original solution, i.e., modeling P([r; a]|q).</p>
<p>Table 9
9DataGSM MATH Mathematics MAWPS SVAMP MMLU SAT Avg. FQA-2nd FQA-3rd ECStandard56.25 13.9614.8073.0753.5037.68 31.82 40.1525.7215.25 50.68+ RefAug (GPT)60.05 17.3619.4080.2559.3043.63 48.64 46.9535.3627.54 72.99+ RefAug (LLaMA) 62.02 17.0017.8080.2961.6039.43 44.55 46.1032.6323.90 50.00Table 9: Training Mistral-7B with data where reflection sections are annotated by GPT-4-turbo or LLaMA-3-70B-Instruct. Data annotated by LLaMA-3 yields similar improvements in standard math reasoning tasks, but fails tomatch GPT-annotated data in enhancing Mistral's reflective reasoning capabilities.trained with GPT-annotated data. This suggeststhat developing models with advanced reflectivemath reasoning skills demands higher qualitydata, compared to what is typically required forstandard forward reasoning in single-round QA.4.4.7 Data Contamination AnalysisTo prevent the augmented data from contaminatingthe test sets, we check the n-gram overlap betweenthe augmented reflective sections and the gold solu-tions within the test sets of GSM8k and MATH. Fol-lowing a common approach (Huang et al., 2024b;Liu et al., 2024), we utilize the test script providedby Azerbayev et al. (2023) and conduct a 20-gramcheck for questions and a 30-gram check for solu-tions. According to the results in Table 10, RefAugdoes not contaminate any test instances in GSM8k.In the MATH dataset, there is a pre-existing con-tamination issue: 228 questions and 167 solutionsin the test set are already contaminated by the orig-inal training set. On the other hand, our RefAugdata overlaps with only 5 instances in the test set,and these 5 instances were already contaminatedby the training set. In other words, RefAug doesnot introduce new contamination to both test sets.In summary, there is minimal contamination riskassociated with RefAug in our experiments.
, RefAug data annotated by LLaMA-3 yields a similar improvement in Mistral's performance on standard math reasoning tasks.However, the reflective reasoning capability of the resulting model falls short of its counterpart</p>
<p>Table 10 :
10
The contamination check on GSM8k and MATH: the number of instances from the test set (target) sharing n-gram overlaps with the training data (source).We use n = 20 for questions and n = 30 for answers.
DatasetSourceTargetOverlapTrain Question Test Question1GSM8kTrain AnswerTest Answer0RefAugTest Answer0Train Question Test Question228MATHTrain AnswerTest Answer167RefAugTest Answer5*
* The 5 test instances that overlap with the augmented reflective sections were already contaminated by the original MATH training set.</p>
<p>Table 12 :
12
Comparison between using synthetic solutions written by GPT-4-turbo and using the originally annotated ones in GSM8k and MATH training sets, as well as applying RefAug on the synthetic solutions.Solutions written by GPT-4-turbo are of much higher quality than the original ones.
Standard64.59 19.8620.2081.3566.0045.5947.7349.33+ RefAug67.10 22.0825.6083.6469.4048.9755.0053.11GPT-Written Solutions 71.72 28.0432.9085.2673.2047.8455.0056.28+ RefAug75.74 31.6432.0087.3875.8051.7569.0960.49Table 11: Results on LLaMA-3-8B. We test integrating RefAug with (1) the original training data, and (2) the datawhere answers are re-written by GPT-4-turbo (see Appendix A.3 for GPT answer re-writing).DataGSM MATH Mathematics MAWPS SVAMP MMLU-Math SAT-Math Avg.Original Solutions56.25 13.9614.8073.0753.5037.6831.8240.15GPT-4-turbo Solutions 65.73 23.1023.9081.1468.8040.2541.3649.18+ RefAug71.80 26.1229.5082.8470.8044.7657.7354.79</p>
<p>Table 2
2, we reportscores on our Gemma-based models as well. Asshown in Table 13, the performance trends forGemma models align with those observed onMistral models. RefAug demonstrates a clear ad-vantage over traditional augmentation methods inenhancing reflective math reasoning capabilities ofLMs. For instance, RefAug outscores both Q-Augand A-Aug in the third round of follow-up QA andin the accuracy of error correction. Furthermore,
as shown in Table3, a combination of A-Aug and RefAug data results in the best-performing model on the reflective reasoning scenarios of MathChat, outperforming many open-source models that are trained on substantially larger math datasets.</p>
<p>Table 13 :
13
Results of Gemma on reflective math reasoning tasks.The general trend is similar to that of Mistral (Table2).</p>
<p>Table 14 :
14
The percentage of error-free RefAug annotations by GPT-4-turbo, including the alternative reasoning section and the follow-up reasoning section.
Dataset Alternative Follow-upGSM8K96%96%MATH76%72%TrainingDataTimeStandard15K60 minQ-Aug / A-Aug 30K 123 minRefAug15K90 min</p>
<p>Table 15 :
15
The impact of various augmentation methods on dataset size and training time.These stats are tested on 8×A100 GPUs.</p>
<p>Table 15 ,
15
while RefAug does introduce additional time over-
TrainingTrain Tokens Test TokensStandard171.4185.5GPT Solutions358.3423.5RefAug-front910.1980.5RefAug892.3219.1Table 16: The resulting sequence lengths of each aug-mentation method during training and testing.head during model training, this increase is lesssignificant than that caused by Q-Aug or A-Augwhich doubles the optimization steps due to datasetexpansion. Additionally, although RefAug resultsin longer sequence lengths in training instances,it does not impair inference efficiency, as shownby the average number of tokens generated in Ta-ble 16. This is due to the early stopping feature thateliminates the need to generate reflective sectionsduring inference. Overall, the efficiency impactbrought by RefAug is minimal.</p>
<p>Table 17 :
17
Statistics of all datasets used in our training and evaluation.
to adopt the train-
†  This work was done when Zhihan, Zhenwen, and Mengzhao were interns at Tencent AI Lab, Seattle.1 Code and data are available at https://github.com/ ytyz1307zzh/RefAug.
We also tried LLaMA-3-70B for data annotation in § 4.4.6
In Appendix A.3, we show that GPT-written solutions are of higher quality than those original ones in GSM and MATH.
https://huggingface.co/meta-math/ MetaMath-Mistral-7B
https://openai.com/policies/terms-of-use/
https://llama.meta.com/llama3/license/
AcknowledgementsWe would like to thank Hongming Zhang (Tencent AI Lab) for his valuable suggestions on experimental design and paper writing.We also thank Fangkai Jiao (Nanyang Technological University) and Zhenyu Wu (Xi'an Jiaotong University) for their suggestions that help shape our idea.Data GSM MATH Mathematics MAWPS SVAMP MMLU-Math SAT-Math Avg.Data GSM MATH Mathematics MAWPS SVAMP MMLU-Math SAT-Math Avg.feng Liang.2024.Deepseek-coder:  When the large language model meets programming -the rise of code intelligence.Arxiv preprint, 2401.14196.
Large language models cannot self-correct reasoning yet. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen, ; Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 10.48550/arXiv.2310.01798ICLR 20242023. 2310. 20689. 2024aArxiv preprintLearning from mistakes makes LLM better reasoner</p>
<p>Key-point-driven data synthesis with its enhancement on mathematical reasoning. Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, Weizhu Chen, 10.48550/arXiv.2403.023332024b2333Arxiv preprint</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, 10.48550/arXiv.2310.06825Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023a6825Mistral 7b. Arxiv preprint</p>
<p>Forward-backward reasoning in large language models for verification. Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T Kwok, 10.48550/arXiv.2308.077582023b7758Arxiv preprint</p>
<p>Information processing in the child: Significance of analytic and reflective attitudes. Jerome Kagan, Bernice L Rosman, Deborah Day, Joseph Albert, William Phillips, Psychological Monographs: General and Applied. 1964</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2022. 2022</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/n16-1136NAACL-HLT. 2016. 2016</p>
<p>Common 7b language models already possess strong math capabilities. Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng, 10.48550/arXiv.2403.04706Arxiv preprint. 47062024a</p>
<p>Query and response augmentation cannot help out-of-domain math reasoning generalization. Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou, 10.48550/arXiv.2310.05506Arxiv preprint. 55062023</p>
<p>When hindsight is not 20/20: Testing limits on reflective thinking in large language models. Yanhong Li, Chenghao Yang, Allyson Ettinger, 10.48550/arXiv.2404.091292024bArxiv preprint</p>
<p>Zhenwen Liang, Dian Yu, Wenhao Yu, Wenlin Yao, Zhihan Zhang, Xiangliang Zhang, Dong Yu, 10.48550/arXiv.2405.19444Mathchat: Benchmarking mathematical reasoning and instruction following in multi-turn interactions. 2024. 194442405</p>
<p>An integral part of facilitating mathematical discussions: Follow-up questioning. Woong Lim, Ji-Eun Lee, Kersti Tyson, Hee-Jeong Kim, Jihye Kim, 10.1007/s10763-019-09966-3&amp;casa_token=crniCVVOYPsAAAAA:fsFdb8SO-_n7nd_B6SQLl5kY99mU6S4hFlmPEjw6H-wuxzb-emdX5Oi2ZXKYjWhKOznDFMbPUHB4Fci-International Journal of Science and Mathematics Education. 2020</p>
<p>Augmenting math word problems via iterative question composing. Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao, 10.48550/arXiv.2401.09003Arxiv preprint. 90032024</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, 2023. 2023</p>
<p>Starcoder 2 and the stack v2: The next generation. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, 10.48550/arXiv.2402.191732024. 191732402Arxiv preprint</p>
<p>Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li, 10.48550/arXiv.2402.16352Arxiv preprint. 163522024</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, 10.48550/arXiv.2308.09583Arxiv preprint. 95832023a</p>
<p>Wizardcoder: Empowering code large language models with evolinstruct. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, 10.48550/arXiv.2306.08568Arxiv preprint. 85682023b</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023. 2023</p>
<p>. Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, 10.48550/arXiv.2403.08295Gemma: Open models based on gemini research and technology. Arxiv preprint. 82952024</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta. 2024Blog</p>
<p>Orca-math: Unlocking the potential of slms in grade school math. Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah, 10.48550/arXiv.2402.148302402.148302024Arxiv preprint</p>
<p>Iterative reasoning preference optimization. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston, 10.48550/arXiv.2404.19733Arxiv preprint. 24042024. 19733</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168NAACL-HLT 2021. 2021</p>
<p>Zero: memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, 10.1109/SC41405.2020.0002420202020Olatunji Ruwase, and Yuxiong He</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, 10.1145/3394486.34067032020. 2020Olatunji Ruwase, and Yuxiong He</p>
<p>The effects of overlearning and distributed practise on the retention of mathematics knowledge. Doug Rohrer, Kelli Taylor, 10.1002/acp.1266?casa_token=sO-IA5rUBwMAAAAA:F1u9nybhxRPgG03peppCtvdaK2K5QCmXQJ4SvXtQUGUtbyC2GfopvlJnFo8mMK9A-oUfclGIUaLv8QwApplied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition. 2006</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, 10.48550/arXiv.2308.12950Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 202312950Arxiv preprint</p>
<p>The effects of problem-based learning on the academic achievement of students in development and learning. Nuriye Semerci, 10.1177/105678790501400405?casa_token=ow7Ea5bb60EAAAAA:R2HgnNVnd-7z0JVKUwaS6APx3udaiEWmZ9E_pbEdBWd7wZzeJODYrfTCH6mzKrLM1VivC_3-bkYInternational Journal of Educational Reform. 2005</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023. 2023</p>
<p>On mathematical problem posing. For the learning of mathematics. Edward A Silver, 1994</p>
<p>Thinking mathematically. Kaye Stacey, Burton, Mason, 1982Addison-Wesley</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, 10.48550/arXiv.2403.028842403.028842024Arxiv preprint</p>
<p>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, 10.48550/arXiv.2310.03731Arxiv preprint. 37312023a</p>
<p>MINT: evaluating llms in multi-turn interaction with tools and language feedback. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, 10.48550/arXiv.2309.10691ICLR 20242024a</p>
<p>Dolphcoder: Echo-locating code large language models with diverse and multi-objective instruction tuning. Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, Weiran Xu, 10.48550/arXiv.2402.091362024bArxiv preprint</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey Macmillan, Noah A Smith, Iz Beltagy, Hannaneh Hajishirzi, 2023b. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022. 2022</p>
<p>Magicoder: Source code is all you need. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang, 10.48550/arXiv.2312.021202312.02120Arxiv preprint. 2023</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, 10.18653/v1/2023.findings-emnlp.167Findings of EMNLP 2023. Jun Zhao. 2023</p>
<p>Reflective verbalization improves solutions-the effects of question-based reflection in design problem solving. Annekatrin Wetzstein, Winfried Hacker, 10.1002/acp.949?casa_token=BrGyaQvIWmEAAAAA%3ARMPtQxXvv5nc13VjrQgYeuDxVLHkX4A912PHD6NB6VL_pnCCgGKsks66z7U68M6f-gtLEtD-YVfOeT0Applied Cognitive Psychology. 2004</p>
<p>Large language models can self-correct with minimal effort. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang, 10.48550/arXiv.2405.14092Arxiv preprint. 2024</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, 10.48550/arXiv.2309.12284ICLR 20242024</p>
<p>Advancing LLM reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun, 10.48550/arXiv.2404.020782404.020782024Arxiv preprint</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, 10.48550/arXiv.2308.018252308.01825Arxiv preprint. 2023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 10.48550/arXiv.2309.056532309.05653Arxiv preprint. 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 10.48550/arXiv.2304.063642304.06364Arxiv. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8356 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8356</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8356</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-762ca2711eb167f19b79e39c175708ca15e1f5d7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/762ca2711eb167f19b79e39c175708ca15e1f5d7" target="_blank">Eliciting Latent Predictions from Transformers with the Tuned Lens</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work trains an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary, and tests the method on various autoregressive language models, showing it to be more predictive, reliable and unbiased than the logit lens.</p>
                <p><strong>Paper Abstract:</strong> We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the tuned lens, is a refinement of the earlier"logit lens"technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8356",
    "paper_id": "paper-762ca2711eb167f19b79e39c175708ca15e1f5d7",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005846249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Eliciting Latent Predictions from Transformers with the Tuned Lens</h1>
<p>Nora Belrose ${ }^{12}$ Igor Ostrovsky ${ }^{1}$ Lev McKinney ${ }^{32}$ Zach Furman ${ }^{14}$ Logan Smith ${ }^{1}$ Danny Halawi ${ }^{1}$ Stella Biderman ${ }^{1}$ Jacob Steinhardt ${ }^{5}$</p>
<h4>Abstract</h4>
<p>We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the tuned lens, is a refinement of the earlier "logit lens" technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/ AlignmentResearch/tuned-lens.</p>
<h2>1. Introduction</h2>
<p>The impressive performance of transformers in natural language processing (Brown et al., 2020) and computer vision (Dosovitskiy et al., 2020) suggests that their internal representations have rich structure worthy of scientific investigation. One common approach is to train classifiers to extract specific concepts from hidden states, like part-of-speech and syntactic structure (Hewitt and Manning, 2019; Tucker et al., 2021; Li et al., 2022).</p>
<p>In this work, we instead examine transformer representations from the perspective of iterative inference (Jastrzębski et al., 2017). Specifically, we view each layer in a transformer language model as performing an incremental update to a latent prediction of the next token. ${ }^{1}$ We decode these latent predictions through early exiting, converting the hidden</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Comparison of our method, the tuned lens (bottom), with the "logit lens" (top) for GPT-Neo-2.7B prompted with an except from the abstract of Vaswani et al. (2017). Each cell shows the top-1 token predicted by the model at the given layer and token index. The logit lens fails to elicit interpretable predictions before layer 21, but our method succeeds.
state at each intermediate layer into a distribution over the vocabulary. This yields a sequence of distributions we call the prediction trajectory, which exhibits a strong tendency to converge smoothly to the final output distribution, with each successive layer achieving lower perplexity.</p>
<p>We build on the "logit lens" (nostalgebraist, 2020), an early exiting technique that directly decodes hidden states into vocabulary space using the model's pretrained unembedding matrix. We find the logit lens to be unreliable (Section 2), failing to elicit plausible predictions for models like BLOOM (Scao et al., 2022) and GPT Neo (Black et al., 2021). Even when the logit lens appears to work, its outputs are hard to interpret due to representational drift: features</p>
<p>may be represented differently at different layers of the network. Other early exiting procedures also exist (Schuster et al., 2022), but require modifying the training process, and so can’t be used to analyze pretrained models. Simultaneous to this work is (Din et al., 2023) which proposes a relatively similar methodology which we will compare with in future work.</p>
<p>To address the shortcomings of the logit lens, we introduce the <em>tuned lens</em>. We train $L$ affine transformations, one for each layer of the network, with a distillation loss: transform the hidden state so that its image under the unembedding matches the final layer logits as closely as possible. We call these transformations <em>translators</em> because they “translate” representations from the basis used at one layer of the network to the basis expected at the final layer. Composing a translator with the pretrained unembedding yields a probe (Alain and Bengio, 2016) that maps a hidden state to a distribution over the vocabulary.</p>
<p>We find that tuned lens predictions have substantially lower perplexity than logit lens predictions, and are more representative of the final layer distribution. We also show that the features most influential on the tuned lens output are also influential on the model itself (Section 4). To do so, we introduce a novel algorithm called <em>causal basis extraction</em> (CBE) and use it to locate the directions in the residual stream with the highest influence on the tuned lens. We then ablate these directions in the corresponding model hidden states, and find that these features tend to be disproportionately influential on the model output.</p>
<p>We use the tuned lens to gain qualitative insight into the computational process of pretrained language models, by examining how their latent predictions evolve during a forward pass (Figure 1, Appendix B).</p>
<p>Finally, we apply the tuned lens in several ways: we extend the results of Halawi et al. (2023) to new models (Section 5.1), we find that tuned lens prediction trajectories can be used to detect prompt injection attacks (Perez and Ribeiro, 2022) often with near-perfect accuracy (Section 5.2), and find that data points which require many training steps to learn also tend to be classified in later layers (Section 5.3).</p>
<h2>2 The Logit Lens</h2>
<p>The logit lens was introduced by <em>nostalgebraist</em> (2020), who found that when the hidden states at each layer of GPT-2 (Radford et al., 2019) are decoded with the unembedding matrix, the resulting distributions converge roughly monotonically to the final answer. More recently it has been used by Halawi et al. (2023) to understand how transformers process few-shot demonstrations, and by Dar et al. (2022), Geva et al. (2022), and Millidge and Black (2022) to directly interpret transformer weight matrices.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p><em>Figure 2.</em> The tuned lens takes the hidden state at an intermediate layer (e.g. $h_{1}$ above), and applies a learned affine transformation (the translator). We then convert the hidden state into logits with the unembedding layer.</p>
<p><strong>The method.</strong> Consider a pre-LayerNorm transformer<sup>2</sup> $\mathcal{M}$. We’ll decompose $\mathcal{M}$ into two “halves,” $\mathcal{M}<em _ell="&gt;\ell">{\leq\ell}$ and $\mathcal{M}</em>}$. The function $\mathcal{M<em _ell="&gt;\ell">{\leq\ell}$ consists of the layers of $\mathcal{M}$ up to and including layer $\ell$, and it maps the input space to hidden states. Conversely, the function $\mathcal{M}</em>$ after $\ell$, which map hidden states to logits.}$ consists of the layers of $\mathcal{M</p>
<p>The transformer layer at index $\ell$ updates the representation as follows:</p>
<p>$$
\boldsymbol{h}<em _ell="\ell">{\ell+1} = \boldsymbol{h}</em>
$$} + F_{\ell}(\boldsymbol{h}_{\ell}), \tag{1</p>
<p>where $F_{\ell}$ is the residual output of layer $\ell$. Applying Equation 1 recursively, the output logits $\mathcal{M}<em _ell="\ell">{&gt;\ell}$ can be written as a function of an arbitrary hidden state $\boldsymbol{h}</em>$ at layer $\ell$:</p>
<p>$$
\mathcal{M}<em _ell="\ell">{&gt;\ell}(\boldsymbol{h}</em>}) = \text{LayerNorm} \left[ \boldsymbol{h<em _ell_="\ell">{\ell} + \sum</em>} \underbrace{F_{\ell'}(\boldsymbol{h<em _text_residual="\text{residual" update="update">{\ell'})}</em>
$$}} W_{U}. \tag{2</p>
<p>The logit lens consists of setting the residuals to zero:</p>
<p>$$
\text{LogitLens}(\boldsymbol{h}<em _ell="\ell">{\ell}) = \text{LayerNorm} \left[ \boldsymbol{h}</em>
$$} \right] W_{U}. \tag{3</p>
<p>While this simple form of the logit lens works reasonably well for GPT-2, <em>nostalgebraist</em> (2021) found that it fails to</p>
<p><sup>2</sup>Both the logit lens and the tuned lens are designed primarily for the pre-LN architecture, which is more unambiguously iterative. Luckily pre-LN is by far more common than post-LN among state-of-the-art models. See Zhang and He (2020, Appendix C) for more discussion.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Bias of logit lens and tuned lens outputs relative to the final layer output for GPT-Neo-2.7B. The last transformer layer is included for both probes. Unlike the tuned lens, the logit lens is systematically biased toward some vocabulary items over others until the very end of the network.
extract useful information from the GPT-Neo family of models (Black et al., 2021). In an attempt to make the method work better for GPT-Neo, they introduce an extension which retains the last transformer layer, yielding:</p>
<p>$$
\text { LogitLens }^{\text {ext }}\left(\boldsymbol{h}<em _ell="\ell">{\ell}\right)=\operatorname{LayerNorm}\left[\boldsymbol{h}</em>}+F_{L}\left(\boldsymbol{h<em U="U">{\ell}\right)\right] W</em>
$$</p>
<p>This extension is only partially successful at recovering meaningful results; see Figure 1 (top) for an example.</p>
<p>Unreliability. Beyond GPT-Neo, the logit lens struggles to elicit predictions from several other models released since its introduction, such as BLOOM (Scao et al., 2022) and OPT 125M (Zhang et al., 2022) (Figure 14).</p>
<p>Moreover, the type of information extracted by the logit lens varies both from model to model and from layer to layer, making it difficult to interpret. For example, we find that for BLOOM and OPT 125M, the top 1 prediction of the logit lens is often the input token, rather than any plausible continuation token, in more than half the layers (Figure 18).</p>
<p>Bias. Even when the logit lens is useful, we find that it is a biased estimator of the model's final output: it systematically puts more probability mass on certain vocabulary items than the final layer does.</p>
<p>This is concerning because it suggests we can't interpret the logit lens prediction trajectory as a belief updating in response to new evidence. The beliefs of a rational agent should not update in an easily predictable direction over time (Yudkowsky, 2007), since predictable updates can be exploited via Dutch books (Ramsey, 1926; De Finetti et al., 1937; Hacking, 1967; Garrabrant et al., 2016). Biased logit lens outputs are trivially exploitable once the direction of bias is known: one could simply "bet" against the logit lens at layer $\ell&lt;L$ that the next token will be one of the tokens that it systematically downweights, and make unbounded profit in expectation.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Perplexity of predictions elicited from BLOOM 560M under four conditions: the logit lens (red squares) and the tuned lens (blue circles), and including (left) and excluding (right) the final transformer layer from the probe. We find that tuned lens predictions have substantially lower perplexity whether or not the final layer is included, showing it is an independent and complementary proposal.</p>
<p>Let $\boldsymbol{x}$ be a sequence of tokens sampled from a dataset $D$, and let $\boldsymbol{x}<em _ell="\ell">{&lt;t}$ refer to the tokens preceding position $t$ in the sequence. Let $q</em>}\left(\cdot \mid \boldsymbol{x<em _t="&lt;t">{&lt;t}\right)$ be the logit lens distribution at layer $\ell$ for position $t$, and let $p\left(\cdot \mid \boldsymbol{x}</em>\right)$ be the final layer distribution for position $t$.</p>
<p>We define $p(v \mid \boldsymbol{x})$ to be the probability assigned to a vocabulary item $v$ in a sequence $\boldsymbol{x}$, averaged over all positions $1 \ldots T$ :</p>
<p>$$
p(v \mid \boldsymbol{x}) \stackrel{\text { def }}{=} \frac{1}{T} \sum_{t=1}^{T} p\left(v \mid \boldsymbol{x}_{&lt;t}\right)
$$</p>
<p>Slightly abusing terminology, we say that $q_{\ell}$ is an "unbiased estimator" of $p$ if, for every item $v$ in the vocabulary, the probability assigned to $v$ averaged across all tokens in the dataset is the same:</p>
<p>$$
\begin{aligned}
\underset{\boldsymbol{x} \in D}{\mathbb{E}}\left[q_{\ell}(v \mid \boldsymbol{x})\right] &amp; =\underset{\boldsymbol{x} \in D}{\mathbb{E}}[p(v \mid \boldsymbol{x})] \
q_{\ell}(v) &amp; =p(v) \
\forall v \in \mathcal{V}, \quad \mathcal{V} &amp; ={\text { "aardvark", } \ldots}
\end{aligned}
$$</p>
<p>In practice, Equation 6 will never hold exactly. We measure the degree of bias using the KL divergence between the marginal distributions, $D_{K L}\left(p | q_{\ell}\right)$.
In Figure 3 we evaluate the bias for each layer of GPT-Neo2.7B. We find the bias of the logit lens can be quite large: around 4 to 5 bits for most layers. As a point of comparison, the bias of Pythia 160M's final layer distribution relative to that of its larger cousin, Pythia 12B, is just 0.0068 bits.</p>
<h2>3. The Tuned Lens</h2>
<p>One problem with the logit lens is that, if transformer layers learn to output residuals that are far from zero on average, the input to LogitLens may be out-of-distribution and yield</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Perplexity of latent predictions elicited by the logit lens (left) and the tuned lens (right) from Pythia and GPT-NeoX-20B, as a function of layer index and model size. Tuned lens predictions are uniformly lower perplexity and exhibit lower variance across independently trained models.
nonsensical results. In other words, the choice of zero as a replacement value is somewhat arbitrary- the network might learn to rely on $\sum_{\ell^{\prime}=\ell}^{L} \mathbb{E}\left[F_{\ell}\left(\boldsymbol{h}<em _ell="\ell">{\ell}\right)\right]$ as a bias term.
Our first change to the method is to replace the summed residuals with a learnable constant value $\mathbf{b}</em>$ instead of zero:</p>
<p>$$
\operatorname{LogitLens}<em _ell="\ell">{\ell}^{\text {debiased }}\left(\boldsymbol{h}</em>}\right)=\operatorname{LogitLens}\left(\boldsymbol{h<em _ell="\ell">{\ell}+\mathbf{b}</em>\right)
$$</p>
<p>Representation drift. Another issue with the logit lens is that transformer hidden states often contain a small number of very high variance dimensions, and these "rogue dimensions" (Timkey and van Schijndel, 2021) tend to be distributed unevenly across layers; see Figure 6 (top) for an example. Ablating an outlier direction can drastically harm performance (Kovaleva et al., 2021), so if LogitLens relies on the presence or absence of particular outlier dimensions, the perplexity of logit lens predictions might be spuriously high.</p>
<p>Even when controlling for rogue dimensions, we observe a strong tendency for the covariance matrices of hidden states at different layers to drift apart as the number of layers separating them increases (Figure 6, bottom). The covariance at the final layer often changes sharply relative to previous layers, suggesting the logit lens might "misinterpret" earlier representations.</p>
<p>One simple, general way to correct for drifting covariance is to introduce a learnable change of basis matrix $A_{\ell}$, which learns to map from the output space of layer $\ell$ to the input space of the final layer. We have now arrived at the tuned lens formula, featuring a learned affine transformation for
each layer:</p>
<p>$$
\operatorname{TunedLens}<em _ell="\ell">{\ell}\left(\boldsymbol{h}</em>}\right)=\operatorname{LogitLens}\left(A_{\ell} \boldsymbol{h<em _ell="\ell">{\ell}+\mathbf{b}</em>\right)
$$</p>
<p>We refer to $\left(A_{\ell}, \mathbf{b}_{\ell}\right)$ as the translator for layer $\ell$.
Loss function. We train the translators to minimize KL between the tuned lens logits and the final layer logits:</p>
<p>$$
\operatorname{argmin} \mathbb{E}<em K="K" L="L">{\boldsymbol{x}}\left[D</em>}\left(f_{&gt;\ell}\left(\boldsymbol{h<em k="k">{\ell}\right) | \operatorname{TunedLens}</em>\right)\right)\right]
$$}\left(\boldsymbol{h}_{\ell</p>
<p>where $f_{&gt;\ell}\left(\boldsymbol{h}_{\ell}\right)$ refers to the rest of the transformer after layer $\ell$. This can be viewed as a distillation loss, using the final layer distribution as a soft label (Sanh et al., 2019). It ensures that the probes are not incentivized to learn extra information over and above what the model has learned, which can become a problem when training probes with ground truth labels (Hewitt and Liang, 2019).</p>
<p>Implementation details. When readily available, we train translators on a slice of the validation set used during pretraining, and use a separate slice for evaluation. Since BLOOM and GPT-2 do not have publicly available validation sets, we use the Pile validation set (Gao et al., 2020; Biderman et al., 2022). The OPT validation set is also not publicly available, but a member of the OPT team helped us train a tuned lens on the OPT validation set. Documents are concatenated and split into uniform chunks of length 2048.</p>
<p>We use SGD with Nesterov momentum, with a linear learning rate decay schedule over 250 training steps. We use a base learning rate of 1.0 , or 0.25 when keeping the final transformer layer, and clip gradients to a norm of 1 . We accumulate gradients as necessary to achieve a total batch size</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Pairwise similarities of hidden state covariance matrices across layers of Pythia 12B. Layer 4 introduces two outlier dimensions which dominate the covariance; removing them reveals smooth representational drift with depth. To control for varying hidden state norms, we measure the Frobenius cosine similarity, or $\frac{\langle A, B\rangle_{F}}{|A|<em F="F">{F}|B|</em>$ for two matrices $A$ and $B$.
of $2^{18}$ tokens per optimizer step. We initialize all translators to the identity transform, and use a weight decay of $10^{-3}$.}</p>
<p>We evaluate all models on a random sample of 16.4 M tokens from their respective pretraining validation sets. We leave out the final transformer layer for GPT-2 (Radford et al., 2019), GPT-NeoX-20B (Black et al., 2022), OPT (Zhang et al., 2022), and Pythia (Biderman et al., 2023), and include it for GPT-Neo (Black et al., 2021). We evaluate BLOOM (Scao et al., 2022) under both conditions in Figure 4.</p>
<p>Results. We plot tuned lens perplexity as a function of depth for the Pythia models and GPT-NeoX-20B in Figure 53;</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Transfer penalties for Pythia 12B. Each row corresponds to a single tuned lens probe trained on layer $\ell$, and each column is a layer $\ell^{\prime}$ on which probes are evaluated. Each cell shows the cross-entropy loss of probe $\ell$ evaluated on layer $\ell^{\prime}$, minus its ondistribution loss (so that the diagonal entries are identically zero).
results for other model families can be found in Appendix A.
We find that the tuned lens resolves the problems with the logit lens discussed in Section 2: it has significantly lower bias (Figure 3), and much lower perplexity than the logit lens across the board (Figure 5, Appendix A).</p>
<p>Transferability across layers. We find that tuned lens translators can usually zero-shot transfer to nearby layers with only a modest increase in perplexity. Specifically, we define the transfer penalty from layer $\ell$ to $\ell^{\prime}$ to be the expected increase in cross-entropy loss when evaluating the tuned lens translator trained for layer $\ell$ on layer $\ell^{\prime}$.</p>
<p>We report transfer penalties for the largest Pythia model in Figure 7. Overall, transfer penalties are quite low, especially for nearby layers (entries near the diagonal in Figure 7). Comparing to the two plots in Figure 6, we notice that transfer penalties are strongly negatively correlated with covariance similarity (Spearman $\rho=-0.78$ ). Unlike Figure 6, however, Figure 7 is not symmetric: transfer penalties are higher when training on a layer with the outlier dimensions (Layer 5 and later) and testing on a layer without them, than the reverse.</p>
<p>Transfer to fine-tuned models. We find that lenses trained on a base model transfer well to fine-tuned versions of that base model, with no additional training of the lens. Trans-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>ferred lenses substantially outperform the Logit Lens and compare well with lenses trained specifically on the finetuned models. Here a transferred lens is one that makes use of the fine-tuned models unembedding, but simply copies its affine translators from the lens trained on the base model.</p>
<p>As an example of the transferability of lenses to fine-tuned models we use Vicuna 13B (Chiang et al., 2023), an open source instruct fine-tuned chat model based on LLaMA 13B (Touvron et al., 2023). In Figure 12, Appendix A, we compare the performance of a lens specifically trained on Vicuna to a tuned lens trained only on LLaMA. Both Tuned Lenses were trained using a subsample of the RedPajama dataset (Together, 2023), an open source replication of LLaMA's training set. The performance of both lenses was then evaluated on the test set of Anthropic's Helpful Harmless (Bai et al., 2022) conversation dataset and the RedPajama Dataset. On the RedPajama Dataset we find, at worst, a 0.3 bits per byte increase in KL divergence to the models final output. On the helpful harmless corpus we find no significant difference between the transferred and trained lenses. These results show that fine-tuning minimally effects the representations used by the tuned lens. This opens applications of the tuned lens for monitoring changes in the representations of a module during fine-tuning and minimizes the need for practitioners to train lenses on new fine-tuned models.</p>
<p>Relation to model stitching. The tuned lens can be viewed as a way of "stitching" an intermediate layer directly onto the unembedding, with an affine transform in between to align the representations. The idea of model stitching was introduced by Lenc and Vedaldi (2015), who form a composite model out of two frozen pretrained models $A$ and $B$, by connecting the bottom layers of $A$ to the top layers of $B$. An affine transform suffices to stitch together independently trained models with minimal performance loss (Bansal et al., 2021; Csiszárik et al., 2021). The success of the tuned lens shows that model stitching works for different layers inside a single model as well.</p>
<p>Benefits over traditional probing. Unlike Alain and Bengio (2016), who train early exiting probes for image classifiers, we do not learn a new unembedding for each layer. This is important, since it allows us to shrink the size of each learned matrix from $|\mathcal{V}| \times d$ to $d \times d$, where $|\mathcal{V}|$ ranges from 50K (GPT-2, Pythia) to over 250K (BLOOM). We observe empirically that training a new unembedding matrix requires considerably more training steps and a larger batch size than training a translator, and often converges to a worse perplexity.</p>
<h2>4. Measuring Causal Fidelity</h2>
<p>Prior work has argued that interpretability hypotheses should be tested with causal experiments: an interpretation of a neural network should make predictions about what will happen when we intervene on its weights or activations (Olah et al., 2020; Chan et al., 2022). This is especially important for probing techniques, since it's known that probes can learn to rely on spurious features unrelated to the model's performance (Hewitt and Liang, 2019; Belinkov, 2022).</p>
<p>To explore whether the tuned lens finds causally relevant features, we will assess two desired properties:</p>
<ol>
<li>Latent directions that are important to the tuned lens should also be important to the final layer output. Concretely, if the tuned lens relies on a feature ${ }^{4} \boldsymbol{v}$ in the residual stream (its output changes significantly when we manipulate $\boldsymbol{v}$ ) then the model output should also change a lot when we manipulate $\boldsymbol{v}$.</li>
<li>These latent directions should be important in the same way for both the tuned lens and the model. Concretely, if we manipulate the hidden state so that the tuned lens changes in a certain way (e.g. doubling the probability assigned to "dog") then the model output should change similarly. We will call this property stimulusresponse alignment.</li>
</ol>
<h3>4.1. Causal basis extraction</h3>
<p>To test Property 1, we first need to find the important directions for the tuned lens. Amnesic probing (Elazar et al., 2021) provides one way to do this-it seeks a direction whose erasure maximally degrades a model's accuracy.</p>
<p>However, this only elicits a single important direction, whereas we would like to find many such directions. To do so, we borrow intuition from PCA, searching for additional directions that also degrade accuracy, but which are orthogonal to the original amnesic direction. This leads to a method that we call causal basis extraction (CBE), which finds the the principal features used by a model.</p>
<p>More specifically, let $f$ be a function (such as the tuned lens) that maps latent vectors $\boldsymbol{h} \in \mathbb{R}^{d}$ to logits $\boldsymbol{y}$. Let $r(\boldsymbol{h}, \boldsymbol{v})$ be an erasure function which removes information along the span of $\boldsymbol{v}$ from $\boldsymbol{x}$. In this work we use $r(\boldsymbol{h}, \boldsymbol{v})$ is mean ablation, which sets $\langle r(\boldsymbol{h}, \boldsymbol{v}), \boldsymbol{v}\rangle$ to the mean value of $\langle\boldsymbol{h}, \boldsymbol{v}\rangle$ in the dataset (see Appendix D.1). We define the influence $\sigma$ of a unit vector $\boldsymbol{v}$ to be the expected KL divergence between</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Causal influence of CBE features when ablated at the 18th layer of Pythia 410M, plotted against their influence on the tuned lens output. Spearman $\rho=0.89$.
the outputs of $f$ before and after erasing $\boldsymbol{v}$ from $\boldsymbol{h}$ :</p>
<p>$$
\sigma(\boldsymbol{v} ; f)=\underset{\boldsymbol{h}}{\mathbb{E}}\left[D_{K L}(f(\boldsymbol{h}) | f(r(\boldsymbol{h}, \boldsymbol{v})))\right]
$$</p>
<p>We seek to find an orthonormal basis $B=\left(\boldsymbol{v}<em k="k">{1}, \ldots, \boldsymbol{v}</em>}\right)$ containing principal features of $f$, ordered by a sequence of influences $\Sigma=\left(\sigma_{1}, \ldots, \sigma_{k}\right)$ for some $k \leq d$. In each iteration we search for a feature $\boldsymbol{v<em j="j">{i}$ of maximum influence that is orthogonal to all previous features $\boldsymbol{v}</em>$ :</p>
<p>$$
\begin{aligned}
\boldsymbol{v}<em 2="2">{i} &amp; =\underset{|v|</em> ; f) \
\text { s.t. } \quad\left\langle\boldsymbol{v}, \boldsymbol{v}_{j}\right\rangle &amp; =\mathbf{0}, \quad \forall j&lt;i
\end{aligned}
$$}=1}{\operatorname{argmax}} \sigma(\boldsymbol{v</p>
<p>With a perfect optimizer, the influence of $\boldsymbol{v}_{i}$ should decrease monotonically since the feasible region is strictly smaller with each successive iteration. In practice, we do observe non-monotonicities due to the non-convexity of the objective. To mitigate this issue we sort the features in descending order by influence after the last iteration.</p>
<p>Implementation details. We evaluate the objective function in Equation 11 on a single in-memory batch of 131,072 tokens sampled randomly from the Pile validation set, and optimize it using L-BFGS with strong Wolfe line search. We find that using the singular vectors of the probe as initialization for the search, rather than random directions, speeds up convergence.</p>
<p>Intervening on the model. If we apply causal basis extraction to the tuned lens at layer $\ell$, we obtain $k$ directions $v_{1}, \ldots, v_{k}$ that are important for the tuned lens. We next check that these are also important to the model $\mathcal{M}$.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Average stimulus-response alignment at each layer of Pythia 160M. Responses are more aligned with stimuli at later layers, and when using the tuned lens rather than the logit lens.</p>
<p>To do so, we first take an i.i.d. sample of input sequences $\boldsymbol{x}$ and feed them to $\mathcal{M}$, storing the resulting hidden states $\mathcal{M}<em i="i">{\leq \ell}(\boldsymbol{x}) .{ }^{5}$ Then, for each vector $\boldsymbol{v}</em>}$ obtained from CBE, we record the causal effect of erasing $\boldsymbol{v<em _ell="&gt;\ell">{i}$ on the output of $\mathcal{M}</em>$,</p>
<p>$$
\underset{\boldsymbol{x}}{\mathbb{E}}\left[D_{K L}\left(\mathcal{M}(\boldsymbol{x}) | \mathcal{M}<em _ell="\ell" _leq="\leq">{&gt;\ell}\left(r\left(\mathcal{M}</em>\right)\right)\right]\right.
$$}(\boldsymbol{x}), \boldsymbol{v}_{i</p>
<p>where the erasure function $r$ is applied to all positions in a sequence simultaneously. We likewise average the KL divergences across token positions.</p>
<p>Results. We report the resulting causal influences for Pythia 410M, $\ell=18$ in Figure 8; results for all layers can be found in Figure 20 in the Appendix.</p>
<p>In accordance with Property 1, there is a strong correlation between the causal influence of a feature on the tuned lens and its influence on the model (Spearman $\rho=0.89$ ). Importantly, we don't observe any features in the lower right corner of the plot (features that are influential in the tuned lens but not in the model). The model is somewhat more "causally sensitive" than the tuned lens: even the least influential features never have an influence under $2 \times 10^{-3}$ bits, leading to the "hockey stick" shape in the LOWESS trendline.</p>
<h3>4.2. Stimulus-response alignment</h3>
<p>We now turn to Property 2. Intuitively, for the interventions from Section 4.1, deleting an important direction $v_{i}$ should have the same effect on the model's output distribution $p$ and the tuned lens' output distribution $q$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We can operationalize this with the Aitchison geometry <em>(Aitchison, 1982)</em>, which turns the probability simplex into a vector space equipped with an inner product. In order to downweight the influence of rare tokens, we use the weighted Aitchison inner product introduced by <em>Egozcue and Pawlowsky-Glahn (2016)</em>, defined as</p>
<p>$\langle\mathbf{p}<em 2="2">{1},\mathbf{p}</em>}\rangle_{\mathbf{w}}=\sum_{i=1}^{D} w_{i}\log\frac{p_{1i}}{\mathrm{g<em 1="1">{\mathbf{w}}(\mathbf{p}</em>})}\log\frac{p_{2i}}{\mathrm{g<em 2="2">{\mathbf{w}}(\mathbf{p}</em>,$ (13)})</p>
<p>where $\mathbf{w}$ is a vector of positive weights, and $\mathrm{g}_{\mathbf{w}}(\mathbf{p})$ is the weighted geometric mean of the entries of $\mathbf{p}$. In our experiments, we use the final layer prediction distribution under the control condition to define $\mathbf{w}$.</p>
<p>We will also use the notion of “subtracting” distributions. In Aitchison geometry, addition and subtraction of distributions is done componentwise in log space, followed by renormalization:</p>
<p>$\mathbf{p}<em 2="2">{1}-\mathbf{p}</em>}=\operatorname{softmax}\left(\log\mathbf{p<em 2="2">{1}-\log\mathbf{p}</em>\right).$ (14)</p>
<p>We say that distributions $(\mathbf{p}<em _text_new="\text{new">{\text{old}}, \mathbf{p}</em>}})$ and $(\mathbf{q<em _text_new="\text{new">{\text{old}}, \mathbf{q}</em>)$ “move in the same direction” if and only if}</p>
<p>$\langle\mathbf{p}<em _text_old="\text{old">{\text{new}}-\mathbf{p}</em>}}, \mathbf{q<em _text_old="\text{old">{\text{new}}-\mathbf{q}</em>&gt;0.$ (15)}}\rangle_{\mathbf{w}</p>
<p>Measuring alignment. Let $g: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ be an arbitrary function for intervening on hidden states, and let $\boldsymbol{h}_{\ell}$ be the hidden state at layer $\ell$ on some input $\boldsymbol{x}$. We’ll define the stimulus to be the Aitchison difference between the tuned lens output before and after the intervention:</p>
<p>$\mathrm{S}\left(\boldsymbol{h}<em _ell="\ell">{\ell}\right)=\operatorname{TunedLens}</em>}\left(g\left(\boldsymbol{h<em _ell="\ell">{\ell}\right)\right)-\operatorname{TunedLens}</em>\right)$ (16)}\left(\boldsymbol{h}_{\ell</p>
<p>Analogously, the response will be defined as the Aitchison difference between the final layer output before and after the intervention:</p>
<p>$\mathrm{R}\left(\boldsymbol{h}<em _ell="&gt;\ell">{\ell}\right)=\mathcal{M}</em>}\left(g\left(\boldsymbol{h<em _ell="&gt;\ell">{\ell}\right)\right)-\mathcal{M}</em>\right)$ (17)}\left(\boldsymbol{h}_{\ell</p>
<p>We’d like to control for the absolute magnitudes of the stimuli and the responses, so we use the Aitchison inner product to define a cosine similarity metric, which we call “Aitchison similarity.” Then the stimulus-response alignment at layer $\ell$ under $g$ is simply the Aitchison similarity between the stimulus and response:</p>
<p>$\operatorname{sim}\left(\mathrm{S}\left(\boldsymbol{h}<em _ell="\ell">{\ell}\right), \mathrm{R}\left(\boldsymbol{h}</em>}\right)\right)=\frac{\left\langle\mathrm{S}\left(\boldsymbol{h<em _ell="\ell">{\ell}\right), \mathrm{R}\left(\boldsymbol{h}</em>}\right)\right\rangle_{\mathbf{w}}}{\left|\mathrm{S}\left(\boldsymbol{h<em _mathbf_w="\mathbf{w">{\ell}\right)\right|</em>}}\left|\mathrm{R}\left(\boldsymbol{h<em _mathbf_w="\mathbf{w">{\ell}\right)\right|</em>$ (18)}}</p>
<p>We propose to use CBE (Section 4.1) to define a “natural” choice for the intervention $g$. Specifically, for each layer $\ell$, we intervene on the subspace spanned by $\ell$’s top 10 causal basis vectors— we’ll call this the “principal subspace”— using a recently proposed method called resampling ablation <em>(Chan et al., 2022)</em>.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. For most models and tasks, we find there is a layer at which the tuned lens performance is better than final layer performance under incorrect demonstrations. Shown here is performance on SICK (Sentences Involving Compositional Knowldedge). Unlike the logit lens, our method is applicable to BLOOM (bottom right) and GPT-Neo (top left). Y-axis shows median-calibrated accuracy as used in <em>Halawi et al. (2023)</em></p>
<p>Given a hidden state $\boldsymbol{h}<em _ell="\ell" _leq="\leq">{\ell}=\mathcal{M}</em>}(\boldsymbol{x})$, resampling ablation replaces the principal subspace of $\boldsymbol{h<em _ell="\ell">{\ell}$ with the corresponding subspace generated on a different input $\boldsymbol{x}^{\prime}$ selected uniformly at random from the dataset. It then feeds this modified hidden state $\tilde{\boldsymbol{h}}</em>}$ into the rest of the model, yielding the modified output $\mathcal{M<em _ell="\ell">{&gt;\ell}\left(\tilde{\boldsymbol{h}}</em>$ should be relatively on-distribution because we’re using values generated “naturally” by the model itself.}\right)$. Intuitively, $\tilde{\boldsymbol{h}}_{\ell</p>
<p>Unlike in Section 4.1, we apply resampling ablation to one token in a sequence at a time, and average the Aitchison similarities across tokens.</p>
<p>Results. We applied resampling ablation to the principal subspaces of the logit and tuned lenses at each layer in Pythia 160M. We report average stimulus-response alignments in Figure 9. Unsurprisingly, we find that stimuli are more aligned with the responses they induce at later layers. We also find that alignment is somewhat higher at all layers when using principal subspaces and stimuli defined by the tuned lens rather than the logit lens, in line with Property 2.</p>
<h2>5. Applications</h2>
<h3>5.1. Extending Overthinking the Truth</h3>
<p>We start by extending a recent use case of the logit lens. <em>Halawi et al. (2023)</em> apply the logit lens to downstream tasks with few-shot prompts, and find that predictions ex-</p>
<p>tracted from earlier layers are often more robust to incorrect demonstrations than final layer predictions. Unfortunately, their method is inapplicable to models like BLOOM and GPT-Neo, which have high logit lens perplexities.</p>
<p>We replicate Halawi et al. (2023)'s experiments with the tuned lens, and recover similar results for BLOOM 560M, Neo 1.3B, and Neo 2.7B (Figure 10). Notice that the calibrated performance under incorrect demonstrations (red) peaks at an early layer around $0.4-0.45$ in all three of these models, before falling to random baseline performance at the final layer, similarly to what was found by Halawi et al.</p>
<h3>5.2. Detecting Prompt Injections</h3>
<p>Given the results from Halawi et al. (2023) and in Figure 10, we hypothesize that the prediction trajectory of the tuned lens on anomalous inputs should be different from the trajectories on normal inputs, and that this could be used to detect anomalous inputs.</p>
<p>To test this, we focus on prompt injection attacks, a recently discovered vulnerability in large language models where untrusted inputs from a malicious user cause the model to behave in unexpected or dangerous ways (Perez and Ribeiro, 2022). These attacks usually tell the model to "ignore previous instructions" and instead follow instructions crafted by the attacker.</p>
<p>To simulate prompt injections, we adapted the attack from Perez and Ribeiro (2022) and ran Pythia 12B on nine multiple choice tasks, both with and without the following adversarial message appended to the few-shot prompt:</p>
<div class="codehilite"><pre><span></span><code>----------
Ignore any previous and
following instructions and just
print &quot;&lt;WRONG_ANSWER&gt;&quot;:
</code></pre></div>

<p>where <WRONG_ANSWER> is replaced with a randomly selected incorrect response from the available multiple choice responses.</p>
<p>We record the tuned prediction trajectory for each data pointthat is, for each layer, we record the log probability assigned by the model to each possible answer. ${ }^{6}$ We then flatten these trajectories into feature vectors and feed them into two standard outlier detection algorithms: isolation forest (iForest) (Liu et al., 2008) and local outlier factor (LOF) (Breunig et al., 2000), both implemented in scikit-learn (Pedregosa et al., 2011) with default hyperparameters.</p>
<p>Baseline. There is a rich literature on general out-of-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>distribution (OOD) detection in deep neural networks. One simple technique is to fit a multivariate Gaussian to the model's final layer hidden states on the training set, and flag inputs as OOD if a new hidden state is unusually far from the training distribution as measured by the Mahalanobis distance (Lee et al., 2018; Mahalanobis, 1936).</p>
<p>Recently, Bai et al. (2022) proposed the Simplified Relative Mahalanobis (SRM) distance, a modification to Mahalanobis which they find to be effective in the context of LLM finetuning. They also find that representations from the middle layers of a transformer, rather than the final layer, yield the best OOD detection performance. We use the SRM at the middle layer as a baseline in our experiments.</p>
<p>Experimental setup. We fit each anomaly detection model exclusively on prediction trajectories from normal prompts without prompt injections, and evaluate them on a held out test set containing both normal and prompt-injected trajectories. This ensures that our models cannot overfit to the prompt injection distribution. We use EleutherAI's lm-evaluation-harness library (Gao et al., 2021) to run our evaluations.</p>
<p>Results. Our results are summarized in Table 1. Our tuned lens anomaly detector achieves perfect or near-perfect AUROC on five tasks (BoolQ, MNLI, QNLI, QQP, and SST-2); in contrast, the same technique using logit lens has lower performance on most tasks. On the other hand, the SRM baseline does consistently well-the tuned lens only outperforms it on one task (ARC-Challenge), while SRM outperforms our technique on both MC TACO and SciQ.</p>
<p>We suspect that further gains could be made by combining the strengths of both techniques, since SRM uses only one layer but considers a high-dimensional representation, while the tuned lens studies the trajectory across layers but summarizes them with a low-dimensional prediction vector.</p>
<h3>5.3. Measuring Example Difficulty</h3>
<p>Early exiting strategies like CALM (Schuster et al., 2022) and DeeBERT (Xin et al., 2020) are based on the observation that "easy" examples require less computation to classify than "difficult" examples. If an example is easy, the model should quickly converge to the right answer in early layers, making it possible to skip the later layers without a significant drop in prediction quality. Conversely, the number of layers needed to converge on an answer can be used to measure the difficulty of an example.</p>
<p>We propose to use the tuned lens to estimate example difficulty in pretrained transformers, without the need to finetune the model for early exiting. Following Baldock et al. (2021)'s work on computer vision models, we define the prediction depth of a prompt $\boldsymbol{x}$ to be the number of layers after which a model's top-1 prediction for $\boldsymbol{x}$ stops changing.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Tuned Lens</th>
<th></th>
<th>Logit Lens</th>
<th></th>
<th>Baseline</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>iForest</td>
<td>LOF</td>
<td>iForest</td>
<td>LOF</td>
<td>SRM</td>
<td>Normal $\rightarrow$ Injected</td>
</tr>
<tr>
<td>ARC-Easy</td>
<td>$0.59(0.54,0.62)$</td>
<td>$0.73(0.71,0.76)$</td>
<td>$0.53(0.50,0.57)$</td>
<td>$0.59(0.56,0.62)$</td>
<td>$0.73(0.70,0.75)$</td>
<td>$72.8 \% \rightarrow 31.7 \%$</td>
</tr>
<tr>
<td>ARC-Challenge</td>
<td>$0.71(0.65,0.77)$</td>
<td>$0.81(0.77,0.84)$</td>
<td>$0.73(0.67,0.79)$</td>
<td>$0.80(0.77,0.83)$</td>
<td>$0.57(0.53,0.61)$</td>
<td>$43.5 \% \rightarrow 24.7 \%$</td>
</tr>
<tr>
<td>BoolQ</td>
<td>$0.99(0.98,0.99)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$0.89(0.87,0.91)$</td>
<td>$0.61(0.57,0.66)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$67.1 \% \rightarrow 0.0 \%$</td>
</tr>
<tr>
<td>MC TACO</td>
<td>$0.74(0.71,0.77)$</td>
<td>$0.68(0.66,0.70)$</td>
<td>$0.68(0.66,0.69)$</td>
<td>$0.55(0.53,0.59)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$0.40 \rightarrow 0.06$ F1</td>
</tr>
<tr>
<td>MNLI</td>
<td>$0.98(0.98,0.99)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$0.95(0.94,0.96)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$54.3 \% \rightarrow 0.0 \%$</td>
</tr>
<tr>
<td>QNLI</td>
<td>$0.99(0.99,1.00)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$0.93(0.92,0.95)$</td>
<td>$0.68(0.63,0.71)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$54.3 \% \rightarrow 0.0 \%$</td>
</tr>
<tr>
<td>QQP</td>
<td>$1.00(0.99,1.00)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$0.90(0.89,0.90)$</td>
<td>$0.79(0.76,0.81)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$60.7 \% \rightarrow 6.5 \%$</td>
</tr>
<tr>
<td>SciQ</td>
<td>$0.62(0.57,0.69)$</td>
<td>$0.64(0.59,0.70)$</td>
<td>$0.75(0.71,0.79)$</td>
<td>$0.70(0.65,0.74)$</td>
<td>$0.75(0.72,0.78)$</td>
<td>$95.5 \% \rightarrow 62.6 \%$</td>
</tr>
<tr>
<td>SST-2</td>
<td>$1.00(0.98,1.00)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$0.78(0.72,0.83)$</td>
<td>$0.61(0.56,0.65)$</td>
<td>$1.00(1.00,1.00)$</td>
<td>$82.9 \% \rightarrow 49.1 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1. Test set AUROCs and 95\% bootstrap CIs for distinguishing normal prompts from prompt injections on Pythia 12B. Figures are pooled over 10 random train-test splits. Attack detection performance is nearly perfect on tasks where the attack succeeds at driving accuracy well below the random baseline, and is still much better than chance even when the attack is only partially successful.</p>
<p>To validate the prediction depth, we measure its correlation with an established difficulty metric: the iteration learned. The iteration learned is defined as the earliest training step $\tau$ where the model's top-1 prediction for a datapoint $\boldsymbol{x}$ is fixed (Toneva et al., 2018). Intuitively, we might expect that examples which take a long time to learn during training would tend to require many layers of computation to classify at inference time. Baldock et al. (2021) indeed show such a correlation, using k-NN classifiers to elicit early predictions from the intermediate feature maps of image classifiers.</p>
<p>Experimental setup. For this experiment we focus on Pythia 12B (deduped), for which 143 uniformly spaced checkpoints are available on Huggingface Hub. We evaluate the model's zero-shot performance on twelve multiplechoice tasks, listed in Table 2. For each checkpoint, we store the top 1 prediction on every individual example, allowing us to compute the iteration learned. We then use the tuned lens on the final checkpoint, eliciting the top 1 prediction at each layer of the network and computing the prediction depth for every example. As a baseline, we also compute prediction depths using the logit lens. Finally, for each task, we compute the Spearman rank correlation between the iteration learned and the prediction depth across all examples.</p>
<p>Results. We present results in Table 2. We find a significant positive correlation between the iteration learned and the tuned lens prediction depth on all tasks we investigated. Additionally, the tuned lens prediction correlates better with iteration learned than its logit lens counterpart in 8 out of 11 tasks, sometimes dramatically so.</p>
<h2>6 Discussion</h2>
<p>In this paper, we introduced a new tool for transformer interpretability research, the tuned lens, which yields new qualitative as well as quantitative insights into the functioning of large language models. It is a drop-in replacement for the logit lens that makes it possible to elicit interpretable pre-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Tuned lens $\rho$</th>
<th style="text-align: center;">Logit lens $\rho$</th>
<th style="text-align: center;">Final acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ARC-Easy</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 7}$</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">$69.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ARC-Challenge</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 7}$</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">$32.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LogiQA</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 8}$</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">$21.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MNLI</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 5}$</td>
<td style="text-align: center;">$40.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PiQA</td>
<td style="text-align: center;">$\mathbf{0 . 6 6 0}$</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">$76.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">QNLI</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 9}$</td>
<td style="text-align: center;">-0.099</td>
<td style="text-align: center;">$53.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">QQP</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 5}$</td>
<td style="text-align: center;">-0.340</td>
<td style="text-align: center;">$0.381(\mathrm{~F} 1)$</td>
</tr>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 7}$</td>
<td style="text-align: center;">$60.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SciQ</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 0}$</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">$91.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 5}$</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">$64.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">WinoGrande</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 7}$</td>
<td style="text-align: center;">$63.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2. Correlation between two measures of example difficulty, the iteration learned and prediction depth, across tasks. Prediction depth is measured using the tuned lens in the first column and the logit lens in the second.
diction trajectories from essentially any pretrained language model in use today. We gave several initial applications of the tuned lens, including detecting prompt injection attacks.</p>
<p>Finally, we introduced causal basis extraction, which identifies influential features in neural networks. We hope this technique will be generally useful for interpretability research in machine learning.</p>
<p>Limitations and future work. One limitation of our method is that it involves training a translator layer for each layer of the network, while the logit lens can be used on any pretrained model out-of-the-box. This training process, however, is quite fast: our code can train a full set of probes in under an hour on a single $8 \times$ A40 node, and further speedups are likely possible. We have also released tuned lens checkpoints for the most commonly used pretrained models as part of our tuned-lens library, which should eliminate this problem for most applications.</p>
<p>Causal basis extraction, as presented in this work, is computationally intensive, since it sequentially optimizes $d_{\text {model }}$</p>
<p>We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top $10 \%$ of test takers.</p>
<p>Figure 11. Token-level prediction depths for Pythia 12B computed on the abstract of OpenAI (2023). Warm colors have high prediction depth, while cool colors indicate low depth.
causal basis vectors for each layer of the network. Future work could explore ways to make the algorithm more scalable. One possibility would be to optimize a whole $k$ dimensional subspace, instead of an individual direction, at each iteration.</p>
<p>Due to space and time limitations, we focused on language models in this work, but we think it's likely that our approach is also applicable to other modalities.</p>
<h2>Acknowledgements</h2>
<p>We are thankful to CoreWeave for providing the computing resources used in this paper and to the OPT team for their assistance in training a tuned lens for OPT. We also thank nostalgebraist for discussions leading to this paper.</p>
<h2>References</h2>
<p>John Aitchison. The statistical analysis of compositional data. Journal of the Royal Statistical Society: Series B (Methodological), 44(2):139-160, 1982.</p>
<p>Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.</p>
<p>Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 8 2021. URL https://www.github.com/eleutherai/ gpt-neox.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Robert Baldock, Hartmut Maennel, and Behnam Neyshabur. Deep learning through the lens of example difficulty. $A d$ -
vances in Neural Information Processing Systems, 34: 10876-10889, 2021.</p>
<p>Yamini Bansal, Preetum Nakkiran, and Boaz Barak. Revisiting model stitching to compare neural representations. Advances in Neural Information Processing Systems, 34: 225-236, 2021.</p>
<p>Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1): 207-219, 2022.</p>
<p>Stella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the Pile. Computing Research Repository, 2022. doi: 10.48550/arXiv.2201.07311. URL https://arxiv. org/abs/2201.07311v1. version 1.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: a scaling suite for language model interpretability research. Computing Research Repository, 2023. doi: 10.48550/arXiv.2201.07311. URL https://arxiv.org/abs/2201.07311v1. version 1 .</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58, 2021.</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.</p>
<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.</p>
<p>Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. Lof: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93-104, 2000.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: a method for rigorously testing</p>
<p>interpretability hypotheses. Alignment Forum, 2022. URL https://bit.ly/3WRBhPD.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Adrián Csiszárik, Péter Kőrösi-Szabó, Ákos Matszangosz, Gergely Papp, and Dániel Varga. Similarity and matching of neural network representations. Advances in Neural Information Processing Systems, 34:5656-5668, 2021.</p>
<p>Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493-8502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL https: //aclanthology.org/2022.acl-long.581.</p>
<p>Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. arXiv preprint arXiv:2209.02535, 2022.</p>
<p>Bruno De Finetti, Henry E Kyburg, and Howard E Smokler. Foresight: Its logical laws, its subjective sources. Breakthroughs in statistics, 1:134-174, 1937.</p>
<p>Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. Jump to conclusions: Short-cutting transformers with linear transformations. arXiv preprint arXiv:2303.09435, 2023.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Juan José Egozcue and Vera Pawlowsky-Glahn. Changing the reference measure in the simplex and its weighting effects. Austrian Journal of Statistics, 45(4):25-44, 2016.</p>
<p>Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160-175, 2021.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane</p>
<p>Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.</p>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac HatfieldDodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022.</p>
<p>Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB dataset of diverse text for language modeling. Computing Research Repository, 2020. doi: 10.48550/arXiv.2101.00027. URL https: //arxiv.org/abs/2101.00027v1. version 1.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https: //doi.org/10.5281/zenodo.5371628.</p>
<p>Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica Taylor. Logical induction. arXiv preprint arXiv:1609.03543, 2016.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 301. URL https://aclanthology.org/2020. findings-emnlp. 301.</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.</p>
<p>Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022.</p>
<p>Klaus Greff, Rupesh K Srivastava, and Jürgen Schmidhuber. Highway and residual networks learn unrolled iterative estimation. arXiv preprint arXiv:1612.07771, 2016.</p>
<p>Ian Hacking. Slightly more realistic personal probability. Philosophy of Science, 34(4):311-325, 1967.</p>
<p>Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. Overthinking the truth: Understanding how language models process false demonstrations. In Submitted to The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=em4xg1Gvxa. under review.</p>
<p>Benjamin Heinzerling and Michael Strube. BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages. In Nicoletta Calzolari (Conference chair), Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 7-12, 2018 2018. European Language Resources Association (ELRA). ISBN 979-10-95546-00-9.</p>
<p>J Hewitt and P Liang. Designing and interpreting probes with control tasks. Proceedings of the 2019 Con, 2019.</p>
<p>John Hewitt and Christopher D Manning. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129-4138, 2019.</p>
<p>Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646661. Springer, 2016.</p>
<p>Stanisław Jastrzębski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. arXiv preprint arXiv:1710.04773, 2017.</p>
<p>Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimensions that disrupt transformers. arXiv preprint arXiv:2105.06990, 2021.</p>
<p>Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-ofdistribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.</p>
<p>Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 991-999, 2015.</p>
<p>Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022.</p>
<p>Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining, pages 413-422. IEEE, 2008.</p>
<p>PC Mahalanobis. On the generalized distances in statistics: Mahalanobis distance. Journal Soc. Bengal, 26:541-588, 1936.</p>
<p>Beren Millidge and Sid Black. The singular value decompositions of transformer weight matrices are highly interpretable. LessWrong, 2022. URL https://bit.ly/ 3GdbZoa.</p>
<p>Neel Nanda. Transformerlens, 2022. URL https://github.com/neelnanda-io/ TransformerLens.
nostalgebraist. interpreting gpt: the logit lens. LessWrong, 2020. URL https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens.
nostalgebraist. logit lens on non-gpt2 models + extensions, 2021. URL https: //colab.research.google.com/drive/ 1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA.</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024-001, 2020.</p>
<p>OpenAI. Gpt-4 technical report. Technical report, 2023. Technical Report.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikitlearn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.</p>
<p>Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.</p>
<p>FP Ramsey. Truth and probability. Studies in subjective probability, pages 61-92, 1926.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, and et al. BLOOM: A 176B-parameter open-access multilingual language model. Computing Research Repository, 2022. doi: 10.48550/arXiv.2211.05100. URL https://arxiv.org/abs/2211.05100v2. version 2 .</p>
<p>Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. arXiv preprint arXiv:2207.07061, 2022.</p>
<p>Pinky Sitikhu, Kritish Pahi, Pujan Thapa, and Subarna Shakya. A comparison of semantic similarity methods for maximum human interpretability. 2019 Artificial Intelligence for Transforming Business and Society (AITB), $1: 1-4,2019$.</p>
<p>William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. arXiv preprint arXiv:2109.04404, 2021.</p>
<p>Together. Redpajama, a project to create leading open-source models, starts by reproducing llama training dataset of over 1.2 trillion tokens, April 2023. URL https://www.together.xyz/blog/ redpajama.</p>
<p>Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.</p>
<p>Mycal Tucker, Peng Qian, and Roger Levy. What if this modified that? syntactic interventions with counterfactual embeddings. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 862-875, 2021.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. Advances in neural information processing systems, 29, 2016.</p>
<p>Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.</p>
<p>Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating bert inference. arXiv preprint arXiv:2004.12993, 2020.</p>
<p>Eliezer Yudkowsky. Conservation of expected evidence. https://www.lesswrong. com/posts/jiBFC7DcCrZjGmZnJ/ conservation-of-expected-evidence, 2007. Accessed: March 18, 2023.</p>
<p>Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. arXiv preprint arXiv:2010.13369, 2020.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open pre-trained transformer language models. Computing Research Repository, 2022. doi: 10.48550/arXiv.2205.01068. URL https://arxiv. org/abs/2205.01068v4. version 4.</p>
<h1>A. Additional evaluation results</h1>
<p>Transfer of a Tuned Lens (LLaMA-13b to Vicuna-13b v1.1)
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12. Comparing the performance of a lens specifically trained on Vicuna 13B vs a lens transferred from LLaMA 13B vs Vicuna 13B's Logit lens. Specifically, we messure the KL divergence between the lens at a specific layer and the model's final output.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13. Perplexities of predictions elicited from OpenAI's GPT-2 models (top), EleutherAI's GPT-Neo models (middle), and Meta's OPT models (bottom). We omit OPT 350M as it uses a post-LN architecture.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14. Perplexities of predictions elicited from OpenAI's GPT-2 models (top), EleutherAI's GPT-Neo models (middle), and Meta's OPT models (bottom). We omit OPT 350M as it uses a post-LN architecture.</p>
<h1>B. Qualitative Results</h1>
<p>Tuned Lens (EleutherAI/pythia-12b-deduped)
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15. Tuned lens prediction trajectory for Pythia 12B on the first several words of Charles Dickens' "A Tale of Two Cities." Note that the latent prediction becomes very confident at early layers after the prefix "It was the best of times" has been processed, suggesting some degree of memorization.</p>
<p>Tuned Lens (EleutherAI/pythia-12b-deduped)
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 16. Tuned lens prediction trajectory for Pythia 12B prompted with the abstract of Brown et al. (2020).</p>
<h1>B.1. Logit lens pathologies</h1>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17. Logit lens prediction trajectory for BLOOM 560M. The logit lens assigns as very high probability to the input token at many layers and token positions, complicating the interpretation of the output.</p>
<p>Logit Lens (facebook/opt-125m)
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 18. Logit lens prediction trajectory for OPT 125M, exhibiting a similar pathology to BLOOM 560M above.</p>
<h1>C. Transformers Perform Iterative Inference</h1>
<p>Jastrzębski et al. (2017) argue that skip connections encourage neural networks to perform "iterative inference" in the following sense: each layer reliably updates the hidden state in a direction of decreasing loss. While their analysis focused on ResNet image classifiers, their theoretical argument applies equally well to transformers, or any neural network featuring residual connections. We reproduce their theoretical analysis below.</p>
<p>A residual block applied to a representation $\boldsymbol{h}_{i}$ updates the representation as follows:</p>
<p>$$
\boldsymbol{h}<em _ell="\ell">{\ell+1}=\boldsymbol{h}</em>\right)
$$}+F_{\ell}\left(\boldsymbol{h}_{\ell</p>
<p>Let $\mathcal{L}$ denote the final linear classifier followed by a loss function. We can Taylor expand $\mathcal{L}\left(\boldsymbol{h}<em i="i">{L}\right)$ around $\boldsymbol{h}</em>$ to yield</p>
<p>$$
\mathcal{L}\left(\boldsymbol{h}<em i="i">{L}\right)=\mathcal{L}\left(\boldsymbol{h}</em>}\right)+\underbrace{\sum_{j=i}^{L}\left\langle F_{j}\left(\boldsymbol{h<em j="j">{j}\right), \frac{\partial \mathcal{L}\left(\boldsymbol{h}</em>}\right)}{\partial \boldsymbol{h<em _gradient-residual="{gradient-residual" _text="\text" alignment="alignment">{j}}\right\rangle}</em>\right)\right)
$$}}+\mathcal{O}\left(F_{j}^{2}\left(\boldsymbol{h}_{j</p>
<p>Thus to a first-order approximation, the model is encouraged to minimize the inner product between the residual $F\left(\boldsymbol{h}<em i="i">{i}\right)$ and the gradient $\frac{\partial \mathcal{L}\left(\boldsymbol{h}</em>$, which can be achieved by aligning it with the negative gradient.}\right)}{\partial \boldsymbol{h}_{i}</p>
<p>To empirically measure the extent to which residuals do align with the negative gradient, Jastrzębski et al. (2017) compute the cosine similarity between $F\left(\boldsymbol{h}<em i="i">{i}\right)$ and $\frac{\partial \mathcal{L}\left(\boldsymbol{h}</em>$ at each layer of a ResNet image classifier. They find that it is consistently negative, especially in the final stage of the network.}\right)}{\partial \boldsymbol{h}_{i}</p>
<p>We reproduced their experiment using Pythia 6.9B, and report the results in Figure 19. We show that, for every layer in the network, the cosine similarity between the residual and the gradient is negative at least $95 \%$ of the time. While the magnitudes of the cosine similarities are relatively small in absolute terms, never exceeding 0.05 , we show that they are much larger than would be expected of random vectors in this very high dimensional space. Specifically, we first sample 250 random Gaussian vectors of the same dimensionality as $\frac{\partial \mathcal{L}\left(\boldsymbol{h}<em i="i">{i}\right)}{\partial \boldsymbol{h}</em>$. Virtually all of the gradient-residual pairs we observed had cosine similarities below this number.}}$, which is (hidden size) $\times$ (sequence length) $=8,388,608$. We then compute pairwise cosine similarities between the vectors, and find the $5^{\text {th }}$ percentile of this sample to be $-\mathbf{6} \times \mathbf{1 0}^{-\mathbf{4}</p>
<p>Alignment of residuals with gradients (Pythia 6.7B)
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Perplexity after layer deletion (Pythia 6.9B)
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19. Left: Cosine similarity between the observed update to the hidden state and $\frac{\partial \mathcal{L}\left(\boldsymbol{h}<em i="i">{i}\right)}{\partial \boldsymbol{h}</em>$. The similarity is almost always negative, and of much larger magnitude than would be expected by chance in this very high dimensional space. Boxplot whiskers are 5th and 95th percentiles. Results were computed over a sample of roughly 16 M tokens from the Pile validation set. Right: Cross-entropy loss of Pythia 6.9B on the Pile validation set, after replacing each of its 32 layers with the identity transformation. The dashed line indicates Pythia's perplexity on this dataset with all layers intact.}</p>
<h2>C.1. Zero-shot robustness to layer deletion</h2>
<p>Stochastic depth (Huang et al., 2016), also known as LayerDrop (Fan et al., 2019), is a regularization technique which randomly drops layers during training, and can be applied to both ResNets and transformers. Veit et al. (2016) show that</p>
<p>ResNets are robust to the deletion of layers even when trained without stochastic depth, while CNN architectures without skip connections are not robust in this way. These results strongly support the idea that adjacent layers in a ResNet encode fundamentally similar representations (Greff et al., 2016).</p>
<p>To the best of our knowledge, this experiment had never been replicated before in transformers. We did so and report our results in Figure 19 above. We find that only the very first layer is crucial for performance; every other layer ablation induces a nearly imperceptible increase in perplexity. Interestingly, Veit et al. (2016) also found that the first layer is exceptionally important in ResNets, suggesting that this is a general phenomenon.</p>
<h1>D. Causal intervention details</h1>
<h2>D.1. Ablation techniques</h2>
<p>Prior work has assumed that, given a representation $\boldsymbol{x}$ and a linear subspace $B \subseteq \mathbb{R}^{D}$ along which we want to remove information, the best erasure strategy is to project $\boldsymbol{x}$ onto the orthogonal complement of $B$ :</p>
<p>$$
\boldsymbol{x}^{\prime}=\operatorname{proj}<em _perp="\perp">{B</em>)
$$}}(\boldsymbol{x</p>
<p>This "zeroes out" a part of $\boldsymbol{x}$, ensuring that $\left\langle\boldsymbol{x}^{\prime}, \boldsymbol{u}\right\rangle=\mathbf{0}$ for all $\boldsymbol{u}$ in $B$. But recent work has pointed out that setting activations to zero may inadvertently push a network out of distribution, making the experimental results hard to interpret (Wang et al., 2022; Chan et al., 2022).</p>
<p>For example, a probe may rely on the invariant that $\forall \boldsymbol{x},\langle\boldsymbol{x}, \boldsymbol{u}\rangle \gg \mathbf{0}$ for some $\boldsymbol{u}$ in $B$ as an implicit bias term, even if $\langle\boldsymbol{x}, \boldsymbol{u}\rangle$ has low variance and does not encode any useful information about the input. The naive projection $\boldsymbol{x}^{\prime}=\operatorname{proj}<em _perp="\perp">{B</em>}}(\boldsymbol{x})$ would significantly increase the loss, making it seem that $\operatorname{proj<em B="B">{B} \boldsymbol{x}$ contains important information- when in reality model performance would not be significantly degraded if $\operatorname{proj}</em>$ were replaced with a nonzero constant value more representative of the training distribution.} \boldsymbol{x</p>
<p>To correct for this, in our causality experiments we ablate directions by replacing them with their mean values computed across a dataset, instead of zeroing them out. Specifically, to ablate a direction $\boldsymbol{u}$, we use the formula:</p>
<p>$$
\boldsymbol{x}^{\prime}=\boldsymbol{x}+P_{\boldsymbol{u}}(\bar{x}-\boldsymbol{x})
$$</p>
<p>where $P_{u}$ is the projection matrix for $\boldsymbol{u}$ and $\bar{x}$ is the mean representation.</p>
<h2>E. Static interpretability analysis</h2>
<p>Several interpretability methods aim to analyze parameters that "write" (in the sense of Elhage et al. (2021)) to intermediate hidden states of a language model (Millidge and Black, 2022; Dar et al., 2022; Geva et al., 2022; 2020), and even use this analysis to successfully edit model behavior (Geva et al., 2022; Millidge and Black, 2022; Dai et al., 2022). Given that the tuned lens aims to provide a "less biased" view of intermediate hidden states, we should expect the tuned lens to preserve their effectiveness. We test both static analysis and model editing methods, and confirm this hypothesis. With static analysis, the tuned lens appears to never decrease performance, and for some models, even increases their performance. With model editing, we found the tuned lens to outperform the logit lens on OPT-125m and perform equivalently on other models for the task of toxicity reduction.</p>
<h2>E.1. Static Analysis</h2>
<p>Many parameters in transformer language models have at least one dimension equal to that of their hidden state, allowing us to multiply by the unembedding matrix to project them into token space. Surprisingly, the resulting top $k$ tokens are often meaningful, representing interpretable semantic and syntactic clusters, such as "prepositions" or "countries" (Millidge and Black, 2022; Dar et al., 2022; Geva et al., 2022). This has successfully been applied to the columns of the MLP output matrices (Geva et al., 2022), the singular vectors of the MLP input and output matrices (Millidge and Black, 2022), and the singular vectors of the attention output-value matrices $W_{O V}$ (Millidge and Black, 2022).
In short ${ }^{7}$, we can explain this as: many model parameters directly modify the model's hidden state (Elhage et al., 2021), and</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ The exact interpretation depends on the method used - for instance, the rows of $W_{\text {out }}$ can be interpreted as the values in a key-value memory store (Geva et al., 2020).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>the OPT models.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
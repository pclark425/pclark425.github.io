<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-306 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-306</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-306</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-fc1ffc7df07cc9b665deca4a94b871732e1f0b4d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fc1ffc7df07cc9b665deca4a94b871732e1f0b4d" target="_blank">Length Generalization in Arithmetic Transformers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that priming allows models trained on $5-digit $\times$ $3-digit multiplications to generalize to $35\times 3$ examples, and that the priming sample size scales as the logarithm of the training set size.</p>
                <p><strong>Paper Abstract:</strong> We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on $5$-digit numbers can perform $15$-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ($10$ to $50$) long sequences to the training set. We show that priming allows models trained on $5$-digit $\times$ $3$-digit multiplications to generalize to $35\times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e306.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e306.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition (RPE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integer addition with relative position embeddings (RPE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-only transformers (Transformer and Universal Transformer) trained on 1–5 digit integer addition generalize to much longer operands when using relative position embeddings; RPE models learn digits jointly and extrapolate a learned algorithm across positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer / UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Base: 6L/512d; Large: 10L/1024d (varied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Encoder-only transformer; also Universal Transformer (shared-layer encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Integer addition (standard, digit-base 10)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Trained on up to 5-digit operands; tested up to 20 digits (addition out-of-distribution up to 20 digits; specific results reported for 6, 10, 15, 20 digit tests).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard supervised training on symbol sequences; position embeddings compared: Absolute PE (APE), Relative PE over keys (RPE_k), Relative PE over keys and queries (RPE_k,q).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>RPE models: ~99.9% accuracy on 10-digit sums, ~98.3% on 15-digit sums, ~21.3% on 20-digit sums (best models). APE models: ~3.1% on 6-digit and ≈0% for longer lengths (fail to extrapolate). Reported 100% in-domain accuracy (5-digit). Results averaged over seeds and model variants.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>RPE models appear to learn a single algorithm applied across digit positions (digits learned concurrently), whereas APE models learn per-position behaviors (effectively memorizing position-specific rules). RPE likely enables tokens to encode relative locality needed to implement uniform digit-wise algorithms (e.g., systematic carry handling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Extrapolation improves with model dimension and depth: minimal hidden size ~512 (Transformer) / 256 (UTransformer) required; larger dims (1024) achieve longer extrapolation (e.g., ~17 digits). UTransformers need ~6 layers; shallow Transformers sometimes extrapolate with fewer layers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>RPE performance still drops for very long lengths (e.g., 20 digits). APE models fail because position embeddings tie behavior to absolute positions; APEs predict in-domain (leftmost) digits well but cannot handle unseen positions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared APE vs RPE_k vs RPE_k,q; Transformer vs UTransformer; Base vs Large models; evaluation across digit lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Relative position embeddings let encoder-only transformers generalize addition learned on 5-digit numbers to much longer numbers by enabling a position-agnostic algorithmic solution (digits learned jointly).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e306.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e306.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition failure analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error modes and digit-wise learning behavior in addition extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed analysis of when and how addition extrapolation fails: failures cluster on carries and specific digit positions; digit-learning dynamics differ between APE and RPE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer / Transformer (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Standard experiments used UTransformer with 1024d, 16 heads in many analyses</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Encoder-only transformer / Universal Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Integer addition (analysis of errors and digit-wise learning)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>20-digit test sums analyzed in detail (trained on up to 5 digits).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Training as above; diagnostic metrics introduced: total number of carries (NC) and maximum consecutive carries (MC); digit-by-digit learning curves plotted.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On 20-digit sums, RPE models achieved ~57% overall in one reported run; errors typically involve only a few incorrect digits rather than completely wrong outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Most failures occur on examples with ≥3 carries and ≥2 consecutive carries; incorrect digits concentrate on highest-power-of-ten positions (leftmost); RPE models learn most positions simultaneously while APE models learn positions independently (digit-by-digit).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Errors increase with complexity of carries (NC, MC); deeper/larger models reduce failure rate.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Failures when long chains of carries occur (three or more carries, two consecutive carries); wrong outputs usually have a small number of incorrect digits concentrated in high-order positions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compare RPE vs APE digitwise learning curves and error distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Length-generalization failures in addition are primarily due to carry propagation complexity, and RPE-based models mitigate but do not entirely remove these failures by learning a uniform digit algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e306.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e306.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular addition and multiplication (various moduli)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments on modular addition and multiplication with moduli c∈{100,101,128,1000}: behavior depends strongly on modulus; moduli that are powers of 10 are easier and generalize better.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Base and Large configurations reported (Base: 6L/512d; Large: up to 14L/1280d in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Universal Transformer (shared-layer encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Modular addition and modular multiplication (y ≡ x1 + x2 [c], y ≡ x1 × x2 [c])</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Trained on up to 5-digit operands; tested up to 35 digits in modular multiplication experiments; moduli considered include powers of 10 and other values.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Comparisons of APE, RPE_k, RPE_k,q; standard supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When c is a power of 10 (e.g., 100, 1000), both APE and RPE often generalize well (100% in some settings up to 35 digits for c=100). For non-power-of-10 moduli (e.g., c=101,128) performance drops sharply and models often fail to generalize; large models and RPE variants sometimes help but success is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>When only a fixed suffix of digits matters (modulus=10^k), the effective output length is constant and APE can generalize because operations depend only on fixed positions; non-power-of-10 moduli require learning division-like behavior and are substantially harder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Large models improve performance for some moduli; RPE_k,q often outperforms others on harder modular cases; success strongly dependent on modulus arithmetic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Failure on moduli that require non-local digit interactions (non power-of-10), e.g., c=101 produces poor extrapolation even when in-domain training succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>APE vs RPE variants; Base vs Large models; different moduli compared directly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Modular arithmetic generalization hinges on whether the relevant digits are a fixed suffix (powers of 10) — if so, length-generalization is much easier and APE can work; otherwise RPE and larger models help but problems remain hard.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e306.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e306.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multiplication (priming)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Train-set priming for length generalization in multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Introducing 'train set priming'—adding a very small number of long (target-length) examples to the training set—enables UTransformers trained on 5×3 multiplication to generalize to 35×3 multiplication with far fewer examples than fine-tuning and without catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Standard reported: D=6, d_model=1024, h=16 (other sizes also tested)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Universal Transformer (shared-layer encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multiplication (5-digit × 3-digit training; extrapolation up to 35-digit × 3-digit tests)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Trained on x1<10^5, x2<10^3 (5×3 training); tested up to n_test ∈ {6..35} × 3 (notably 35×3 extrapolation).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train set priming: replace ε·N_train of the training examples by fixed examples drawn from the target (long) distribution; compared to fine-tuning where model is re-trained on target-length examples after base training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Priming with ~50 target-length examples (≈1% of a 5000-example train set) yielded near-100% accuracy on both 5×3 (in-domain) and 35×3 (OOD) multiplication. Less than ~25 priming examples fails; fine-tuning required ~1000 target examples to reach comparable performance and caused catastrophic forgetting of the original 5×3 task.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Priming causes the model to learn several output digits concurrently (digits are acquired simultaneously), enabling an algorithmic solution that extrapolates; without priming, digits are learned sequentially (one digit at a time) and do not generalize. Priming thus appears to shift training dynamics toward a position-agnostic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Number of priming samples required scales logarithmically with the base train set size (e.g., ~30 priming samples for N_train=1e3; ~70 for 1e4; ~100 for 1e5) and scales roughly linearly with the extrapolation length (e.g., ~10 examples for 6-digit extrapolation, ~50 for 35-digit).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Priming below a threshold (~25 examples in reported setting) fails to induce extrapolation; naive curriculum priming over many lengths often fails unless specific distributions (e.g., priming on top lengths like 34 & 35) are used. RPE alone without priming fails for 5×3→35×3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared priming vs fine-tuning vs standard training; also compared priming rates, curricula, and priming distributions. Also compared APE vs RPE variants under priming.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A tiny fixed set of target-length examples in the training set (train-set priming) can induce strong length generalization for multiplication, far more sample-efficient than fine-tuning and without catastrophic forgetting, by altering digit-learning dynamics toward a single, position-agnostic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e306.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e306.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuning (multiplication)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning baseline for multiplication length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard fine-tuning (retraining on target-length long examples after base training) can enable extrapolation for multiplication but requires many target examples and leads to catastrophic forgetting of the original distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Standard experiments used UTransformer D=6, d_model=1024</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Universal Transformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multiplication (5×3 base → 35×3 target)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same 5×3 base training, target 35×3 fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on examples from the target length after base training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Fine-tuning required ≈1000 target-length examples to learn 35×3 multiplication to high accuracy; but the fine-tuned model lost performance on the original 5×3 task (catastrophic forgetting).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Fine-tuning appears to overwrite weights supporting the base (short-length) algorithm, i.e., the solution does not integrate both length regimes robustly, causing forgetting of in-domain competence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Sample complexity for fine-tuning is substantially larger than for priming (order-of-magnitude more examples needed in reported settings).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Catastrophic forgetting of in-domain task; high sample requirement for target-length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly against train-set priming (priming needs ~20x fewer target examples) and standard training (no extrapolation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning can make multiplication generalize to long lengths but is sample-inefficient and causes catastrophic forgetting, whereas small in-training priming avoids these problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e306.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e306.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Element-wise addition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digitwise addition without carries (element-wise modulo 10)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task where digits are added independently modulo 10 (no carries). RPE enables length generalization while APE fails, showing that locality alone is insufficient to explain RPE's advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Base: 6L/512d (reported experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Universal Transformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Element-wise digit addition (no carries): (a_i + b_i) mod 10 for each digit position independently</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Trained on 5-digit element-wise addition; tested up to 20 digits.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard supervised training; compare APE vs RPE_k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>RPE_k achieves high extrapolation (e.g., 97.5% on 6 digits, 90.5% on 10 digits, 78.13% on 20 digits in one set); APE fails (100% in-domain but ~5.3% on 6 digits and 0% for longer).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Because element-wise addition removes carry propagation, if RPE merely helped because of local neighbor awareness, APE should succeed too — but APE fails, indicating RPE's advantage is not just local neighbor awareness but enabling a shared position-agnostic mapping across positions (learning a uniform digitwise rule).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>RPE retains strong accuracy across increasing lengths whereas APE accuracy collapses beyond training positions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>APE predicts leftmost (in-domain) digits correctly but cannot generalize to unseen positions; RPE degrades with length but retains substantial accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>APE vs RPE_k on same task and training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>RPE enables learning of a position-agnostic digitwise rule even when carries are absent; APE collapses to position-specific memorization and cannot extrapolate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-attention with relative position representations <em>(Rating: 2)</em></li>
                <li>Attention Is All You Need <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Exploring length generalization in large language models <em>(Rating: 2)</em></li>
                <li>Teaching algorithmic reasoning via in-context learning <em>(Rating: 1)</em></li>
                <li>Neural Arithmetic Logic Units <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-306",
    "paper_id": "paper-fc1ffc7df07cc9b665deca4a94b871732e1f0b4d",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Addition (RPE)",
            "name_full": "Integer addition with relative position embeddings (RPE)",
            "brief_description": "Encoder-only transformers (Transformer and Universal Transformer) trained on 1–5 digit integer addition generalize to much longer operands when using relative position embeddings; RPE models learn digits jointly and extrapolate a learned algorithm across positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer / UTransformer (encoder-only)",
            "model_size": "Base: 6L/512d; Large: 10L/1024d (varied)",
            "model_architecture": "Encoder-only transformer; also Universal Transformer (shared-layer encoder-only)",
            "arithmetic_operation_type": "Integer addition (standard, digit-base 10)",
            "number_range_or_complexity": "Trained on up to 5-digit operands; tested up to 20 digits (addition out-of-distribution up to 20 digits; specific results reported for 6, 10, 15, 20 digit tests).",
            "method_or_intervention": "Standard supervised training on symbol sequences; position embeddings compared: Absolute PE (APE), Relative PE over keys (RPE_k), Relative PE over keys and queries (RPE_k,q).",
            "performance_result": "RPE models: ~99.9% accuracy on 10-digit sums, ~98.3% on 15-digit sums, ~21.3% on 20-digit sums (best models). APE models: ~3.1% on 6-digit and ≈0% for longer lengths (fail to extrapolate). Reported 100% in-domain accuracy (5-digit). Results averaged over seeds and model variants.",
            "mechanistic_insight": "RPE models appear to learn a single algorithm applied across digit positions (digits learned concurrently), whereas APE models learn per-position behaviors (effectively memorizing position-specific rules). RPE likely enables tokens to encode relative locality needed to implement uniform digit-wise algorithms (e.g., systematic carry handling).",
            "performance_scaling": "Extrapolation improves with model dimension and depth: minimal hidden size ~512 (Transformer) / 256 (UTransformer) required; larger dims (1024) achieve longer extrapolation (e.g., ~17 digits). UTransformers need ~6 layers; shallow Transformers sometimes extrapolate with fewer layers.",
            "failure_modes": "RPE performance still drops for very long lengths (e.g., 20 digits). APE models fail because position embeddings tie behavior to absolute positions; APEs predict in-domain (leftmost) digits well but cannot handle unseen positions.",
            "comparison_baseline": "Compared APE vs RPE_k vs RPE_k,q; Transformer vs UTransformer; Base vs Large models; evaluation across digit lengths.",
            "key_finding": "Relative position embeddings let encoder-only transformers generalize addition learned on 5-digit numbers to much longer numbers by enabling a position-agnostic algorithmic solution (digits learned jointly).",
            "uuid": "e306.0",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Addition failure analysis",
            "name_full": "Error modes and digit-wise learning behavior in addition extrapolation",
            "brief_description": "Detailed analysis of when and how addition extrapolation fails: failures cluster on carries and specific digit positions; digit-learning dynamics differ between APE and RPE.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer / Transformer (as above)",
            "model_size": "Standard experiments used UTransformer with 1024d, 16 heads in many analyses",
            "model_architecture": "Encoder-only transformer / Universal Transformer",
            "arithmetic_operation_type": "Integer addition (analysis of errors and digit-wise learning)",
            "number_range_or_complexity": "20-digit test sums analyzed in detail (trained on up to 5 digits).",
            "method_or_intervention": "Training as above; diagnostic metrics introduced: total number of carries (NC) and maximum consecutive carries (MC); digit-by-digit learning curves plotted.",
            "performance_result": "On 20-digit sums, RPE models achieved ~57% overall in one reported run; errors typically involve only a few incorrect digits rather than completely wrong outputs.",
            "mechanistic_insight": "Most failures occur on examples with ≥3 carries and ≥2 consecutive carries; incorrect digits concentrate on highest-power-of-ten positions (leftmost); RPE models learn most positions simultaneously while APE models learn positions independently (digit-by-digit).",
            "performance_scaling": "Errors increase with complexity of carries (NC, MC); deeper/larger models reduce failure rate.",
            "failure_modes": "Failures when long chains of carries occur (three or more carries, two consecutive carries); wrong outputs usually have a small number of incorrect digits concentrated in high-order positions.",
            "comparison_baseline": "Compare RPE vs APE digitwise learning curves and error distributions.",
            "key_finding": "Length-generalization failures in addition are primarily due to carry propagation complexity, and RPE-based models mitigate but do not entirely remove these failures by learning a uniform digit algorithm.",
            "uuid": "e306.1",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Modular arithmetic",
            "name_full": "Modular addition and multiplication (various moduli)",
            "brief_description": "Experiments on modular addition and multiplication with moduli c∈{100,101,128,1000}: behavior depends strongly on modulus; moduli that are powers of 10 are easier and generalize better.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (encoder-only)",
            "model_size": "Base and Large configurations reported (Base: 6L/512d; Large: up to 14L/1280d in experiments)",
            "model_architecture": "Universal Transformer (shared-layer encoder-only)",
            "arithmetic_operation_type": "Modular addition and modular multiplication (y ≡ x1 + x2 [c], y ≡ x1 × x2 [c])",
            "number_range_or_complexity": "Trained on up to 5-digit operands; tested up to 35 digits in modular multiplication experiments; moduli considered include powers of 10 and other values.",
            "method_or_intervention": "Comparisons of APE, RPE_k, RPE_k,q; standard supervised training.",
            "performance_result": "When c is a power of 10 (e.g., 100, 1000), both APE and RPE often generalize well (100% in some settings up to 35 digits for c=100). For non-power-of-10 moduli (e.g., c=101,128) performance drops sharply and models often fail to generalize; large models and RPE variants sometimes help but success is limited.",
            "mechanistic_insight": "When only a fixed suffix of digits matters (modulus=10^k), the effective output length is constant and APE can generalize because operations depend only on fixed positions; non-power-of-10 moduli require learning division-like behavior and are substantially harder.",
            "performance_scaling": "Large models improve performance for some moduli; RPE_k,q often outperforms others on harder modular cases; success strongly dependent on modulus arithmetic structure.",
            "failure_modes": "Failure on moduli that require non-local digit interactions (non power-of-10), e.g., c=101 produces poor extrapolation even when in-domain training succeeds.",
            "comparison_baseline": "APE vs RPE variants; Base vs Large models; different moduli compared directly.",
            "key_finding": "Modular arithmetic generalization hinges on whether the relevant digits are a fixed suffix (powers of 10) — if so, length-generalization is much easier and APE can work; otherwise RPE and larger models help but problems remain hard.",
            "uuid": "e306.2",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Multiplication (priming)",
            "name_full": "Train-set priming for length generalization in multiplication",
            "brief_description": "Introducing 'train set priming'—adding a very small number of long (target-length) examples to the training set—enables UTransformers trained on 5×3 multiplication to generalize to 35×3 multiplication with far fewer examples than fine-tuning and without catastrophic forgetting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (encoder-only)",
            "model_size": "Standard reported: D=6, d_model=1024, h=16 (other sizes also tested)",
            "model_architecture": "Universal Transformer (shared-layer encoder-only)",
            "arithmetic_operation_type": "Multiplication (5-digit × 3-digit training; extrapolation up to 35-digit × 3-digit tests)",
            "number_range_or_complexity": "Trained on x1&lt;10^5, x2&lt;10^3 (5×3 training); tested up to n_test ∈ {6..35} × 3 (notably 35×3 extrapolation).",
            "method_or_intervention": "Train set priming: replace ε·N_train of the training examples by fixed examples drawn from the target (long) distribution; compared to fine-tuning where model is re-trained on target-length examples after base training.",
            "performance_result": "Priming with ~50 target-length examples (≈1% of a 5000-example train set) yielded near-100% accuracy on both 5×3 (in-domain) and 35×3 (OOD) multiplication. Less than ~25 priming examples fails; fine-tuning required ~1000 target examples to reach comparable performance and caused catastrophic forgetting of the original 5×3 task.",
            "mechanistic_insight": "Priming causes the model to learn several output digits concurrently (digits are acquired simultaneously), enabling an algorithmic solution that extrapolates; without priming, digits are learned sequentially (one digit at a time) and do not generalize. Priming thus appears to shift training dynamics toward a position-agnostic algorithm.",
            "performance_scaling": "Number of priming samples required scales logarithmically with the base train set size (e.g., ~30 priming samples for N_train=1e3; ~70 for 1e4; ~100 for 1e5) and scales roughly linearly with the extrapolation length (e.g., ~10 examples for 6-digit extrapolation, ~50 for 35-digit).",
            "failure_modes": "Priming below a threshold (~25 examples in reported setting) fails to induce extrapolation; naive curriculum priming over many lengths often fails unless specific distributions (e.g., priming on top lengths like 34 & 35) are used. RPE alone without priming fails for 5×3→35×3.",
            "comparison_baseline": "Compared priming vs fine-tuning vs standard training; also compared priming rates, curricula, and priming distributions. Also compared APE vs RPE variants under priming.",
            "key_finding": "A tiny fixed set of target-length examples in the training set (train-set priming) can induce strong length generalization for multiplication, far more sample-efficient than fine-tuning and without catastrophic forgetting, by altering digit-learning dynamics toward a single, position-agnostic algorithm.",
            "uuid": "e306.3",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Fine-tuning (multiplication)",
            "name_full": "Fine-tuning baseline for multiplication length generalization",
            "brief_description": "Standard fine-tuning (retraining on target-length long examples after base training) can enable extrapolation for multiplication but requires many target examples and leads to catastrophic forgetting of the original distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (encoder-only)",
            "model_size": "Standard experiments used UTransformer D=6, d_model=1024",
            "model_architecture": "Universal Transformer (encoder-only)",
            "arithmetic_operation_type": "Multiplication (5×3 base → 35×3 target)",
            "number_range_or_complexity": "Same 5×3 base training, target 35×3 fine-tuning",
            "method_or_intervention": "Fine-tuning on examples from the target length after base training",
            "performance_result": "Fine-tuning required ≈1000 target-length examples to learn 35×3 multiplication to high accuracy; but the fine-tuned model lost performance on the original 5×3 task (catastrophic forgetting).",
            "mechanistic_insight": "Fine-tuning appears to overwrite weights supporting the base (short-length) algorithm, i.e., the solution does not integrate both length regimes robustly, causing forgetting of in-domain competence.",
            "performance_scaling": "Sample complexity for fine-tuning is substantially larger than for priming (order-of-magnitude more examples needed in reported settings).",
            "failure_modes": "Catastrophic forgetting of in-domain task; high sample requirement for target-length generalization.",
            "comparison_baseline": "Compared directly against train-set priming (priming needs ~20x fewer target examples) and standard training (no extrapolation).",
            "key_finding": "Fine-tuning can make multiplication generalize to long lengths but is sample-inefficient and causes catastrophic forgetting, whereas small in-training priming avoids these problems.",
            "uuid": "e306.4",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Element-wise addition",
            "name_full": "Digitwise addition without carries (element-wise modulo 10)",
            "brief_description": "Task where digits are added independently modulo 10 (no carries). RPE enables length generalization while APE fails, showing that locality alone is insufficient to explain RPE's advantage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (encoder-only)",
            "model_size": "Base: 6L/512d (reported experiment)",
            "model_architecture": "Universal Transformer (encoder-only)",
            "arithmetic_operation_type": "Element-wise digit addition (no carries): (a_i + b_i) mod 10 for each digit position independently",
            "number_range_or_complexity": "Trained on 5-digit element-wise addition; tested up to 20 digits.",
            "method_or_intervention": "Standard supervised training; compare APE vs RPE_k.",
            "performance_result": "RPE_k achieves high extrapolation (e.g., 97.5% on 6 digits, 90.5% on 10 digits, 78.13% on 20 digits in one set); APE fails (100% in-domain but ~5.3% on 6 digits and 0% for longer).",
            "mechanistic_insight": "Because element-wise addition removes carry propagation, if RPE merely helped because of local neighbor awareness, APE should succeed too — but APE fails, indicating RPE's advantage is not just local neighbor awareness but enabling a shared position-agnostic mapping across positions (learning a uniform digitwise rule).",
            "performance_scaling": "RPE retains strong accuracy across increasing lengths whereas APE accuracy collapses beyond training positions.",
            "failure_modes": "APE predicts leftmost (in-domain) digits correctly but cannot generalize to unseen positions; RPE degrades with length but retains substantial accuracy.",
            "comparison_baseline": "APE vs RPE_k on same task and training regime.",
            "key_finding": "RPE enables learning of a position-agnostic digitwise rule even when carries are absent; APE collapses to position-specific memorization and cannot extrapolate.",
            "uuid": "e306.5",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-attention with relative position representations",
            "rating": 2
        },
        {
            "paper_title": "Attention Is All You Need",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Exploring length generalization in large language models",
            "rating": 2
        },
        {
            "paper_title": "Teaching algorithmic reasoning via in-context learning",
            "rating": 1
        },
        {
            "paper_title": "Neural Arithmetic Logic Units",
            "rating": 1
        }
    ],
    "cost": 0.01538275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Length Generalization in Arithmetic Transformers</h1>
<p>Samy Jelassi<br>Princeton University<br>Yuhuai Wu<br>Stanford University<br>Google Research</p>
<p>Stéphane d'Ascoli<br>EPFL<br>Yuanzhi Li<br>Carnegie Mellon University<br>Microsoft Research</p>
<h2>Carles Domingo-Enrich</h2>
<p>New York University</p>
<h2>François Charton</h2>
<p>Meta AI</p>
<p>June 28, 2023</p>
<h4>Abstract</h4>
<p>We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on 5-digit numbers can perform 15-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ( 10 to 50 ) long sequences to the training set. We show that priming allows models trained on 5-digit $\times 3$-digit multiplications to generalize to $35 \times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.</p>
<h2>1 Introduction</h2>
<p>Transformers (Vaswani et al., 2017) achieve remarkable results in domains ranging from Natural Language Processing (NLP) (Vaswani et al., 2017; Devlin et al., 2018), to computer vision (Dosovitskiy et al., 2020), reinforcement learning (Chen et al., 2021; Janner et al., 2021), and program synthesis (Austin et al., 2021). Yet, they struggle on simple tasks, such as integer arithmetic (Nogueira et al., 2021). Recent, transformer-based, large language models, such as ChatGPT (Schulman et al., 2022), can perform arithmetic on small integers, but their performance drops steeply as operands become large. The text corpora used to train language models is partly responsible for this situation. Most of the problems of mathematics featured in these data sets involve small numbers. In fact, large integers, with 15 digits or more, almost never appear in print. The absence of large numbers in the training data limits the mathematical ability of large language models. To mitigate this, language models must be able to extrapolate the small number arithmetic they have learned, to larger integers.</p>
<p>Most prior works on learning arithmetic with transformers (Nogueira et al., 2021; Power et al., 2022) consider the in-distribution setting, where numbers in the training and test sets are drawn from the same distribution. Out-of-distribution experiments, and in particular extrapolation to larger numbers, have so far proven disappointing.</p>
<p>On the other hand, length generalization in transformers has been widely studied. The seminal paper by Shaw et al. (2018) identified the position embedding (PEs) as the likely culprit for</p>
<p>their inability to generalize. Indeed, the absolute position embeddings (APEs), used in many implementations, mix the representation of a token with the embedding of its position in the sequence, making trained models very susceptible to changes in sequence lengths. Since then, several papers have proposed to use relative position embeddings (RPEs), that encode the relative distance between tokens Shaw et al. 2018, Huang et al. 2018, Dai et al. 2019, Huang et al. 2020, or to replace position embeddings by weighted attention schemes Raffel et al. 2020, Su et al. 2021, Press et al. 2021). While these changes improved extrapolation in natural language processing (NLP), their impact on arithmetic tasks has been little studied.</p>
<p>Recent work suggests that large language models can generalize to longer sequences for the addition task, thanks to specialized prompt engineering techniques Zhou et al. 2022. However, results for multiplication are limited to short extrapolation lengths (7 digits).</p>
<p>In this paper, we study length generalization in transformers for four basic arithmetic tasks: addition, modular addition, multiplication and modular multiplication. We train models on 5-digit operations, and investigate their ability to generalize to numbers with up to 20 digits for addition, and 35 digits for multiplication. We show that the use of relative position embeddings allows for length generalization in the case of addition and some modular operations. For 5-digit $\times$ 3-digit multiplication, we show that train set priming: adding a tiny amount of examples (50 out of 5000) from the target distribution, surprisingly allows the model to length generalize to very long operands (i.e. 35-digit $\times$ 3-digit multiplications). The paper is organized as follows.</p>
<ul>
<li>Section 2 presents our experimental setup: problems, data generation, encoding, models, training and evaluation.</li>
<li>Section 3 demonstrates that, on the addition task, encoder-only transformers using relative position embeddings, can length generalize.</li>
<li>Section 4 presents our results for modular arithmetic. In some cases, absolute position embedding allow for length generalization.</li>
<li>Section 5 introduces train set priming and shows that it achieves extrapolation to very long multiplications.</li>
<li>Section 6 discusses the results, highlights a few additional results and proposes some future directions.</li>
</ul>
<p>Contributions. This paper delivers five key messages.</p>
<ul>
<li>Relative position embeddings ensure length generation in addition. Models trained to add 5-digit numbers can generalize to 20-digit operands.</li>
<li>Simple techniques fail for multiplication. RPE do not allow length generalization. Fine-tuning on long sequences helps generalize, but requires a lot of samples from the target distribution. Also, it causes catastrophic forgetting.</li>
<li>Train set priming enables length generalization. For multiplication, adding a tiny amount of long sequences to the training set ( 50 out of the $9 \times 10^{34}$ possible 35 -digit numbers) allows generalization to 35-digit operands. Remarkably, the number of long sequences is much smaller than the one needed for fine-tuning.</li>
<li>
<p>Priming sample size scales as the logarithm of the train set size.</p>
</li>
<li>
<p>Primed model can extrapolate to several lengths. A model trained to multiply 5-digit numbers can be primed, with 500 priming examples, to generalize to numbers with 6 to 35 -digits. On the other hand, 500 examples along would be far from sufficient to train a model to multiply 6 to 35 digits.</p>
</li>
</ul>
<p>Remark: In our multiplication experiments, we arbitrarily fix the second operand to have 3 digits. This is to ensure that the task is challenging enough. Regarding the first operand, we arbitrarily set the extrapolation to 35 in order to hightlight that our models are really able to do length generalization when using priming. However, we believe that our empirical results would still hold when extrapolating to any reasonable length.</p>
<h1>Related work</h1>
<p>Transformers for mathematics. Early applications of transformers to mathematics focus on symbolic computations. Lample and Charton (2019) trained them to perform symbolic integration and solve differential equations. Polu and Sutskever (2020) applied them to theorem proving, Hahn et al. (2020) to temporal logic, and Dersy et al. (2022) trained them to simplify formulas involving polylogarithms. Nogueira et al. (2021) investigates their limitations on basic arithmetic operations. Palamas (2017) experiments with modular arithmetic, and Wenger et al. (2022) demonstrates that universal transformers can be trained to perform modular inversion. Despite their limitations in arithmetic, Charton (2021) shows that transformers can perform numerical calculations, like computing eigenvalues or inverting matrices.</p>
<p>With the advent of large language models (Bommasani et al., 2021), a new line of research focuses solving problems of mathematics written in natural language (Griffith and Kalita, 2021; Meng and Rumshisky, 2019; Cobbe et al., 2021). Lewkowycz et al. (2022) show that a large pre-trained transformer can be retrained on a large math corpus to solve grade and high school problems of mathematics.</p>
<p>Length generalization with transformers. Multiple works observe the difficulty of transformers to length generalize especially in NLP (Shaw et al., 2018; Murray and Chiang, 2018; Rosendahl et al., 2019; Press et al., 2021). Several techniques have then been introduced to address this problem: new position embeddings Shaw et al. (2018); Dai et al. (2019); Raffel et al. (2020); Huang et al. (2020); Kiyono et al. (2021); Su et al. (2021); Press et al. (2021), introducing new tokens Newman et al. (2020), new attention mechanisms Dubois et al. (2019). In this paper, we leverage one of these techniques (RPE) for addition and introduce a new one, train set priming, for multiplication.</p>
<p>Length generalization in mathematics. Generalization to long sequences, in arithmetic operations, is a longstanding problem. Using recurrent architectures, Joulin and Mikolov (2015) and Kaiser and Sutskever (2015) achieve length generalization in the case of binary addition and multiplication. Later, Trask et al. (2018) introduces NALU, an architecture that learns addition and multiplication, and that generalizes to any length. However, their network has hand-crafted modules that are specifically designed to encode addition and multiplication. Several recent works use auto-regressive models to length generalize in math tasks. Anil et al. (2022) and Zhou et al. (2022) show that fine-tuning or scratchpad (Nye et al., 2021; Wei et al., 2022) on autoregressive decoder models is insufficient to length generalize. They tackle this by changing the scratchpad</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model overview. We linearly embed each symbol token, add position embeddings, and feed the resulting sequence of vectors to a transformer or universal transformer encoder. In order to predict the result of the operation, we select the first nout tokens and apply a linear classifier to each of them.</p>
<p>procedure and designing new prompt engineering techniques. Closer to our work, Zhang et al. (2022) train encoder-only models to length generalize on variable assignment tasks.</p>
<h2>2 Experimental setup</h2>
<h3>2.1 Problems and encodings</h3>
<p>We consider four arithmetic tasks:</p>
<ul>
<li>Addition: y = x1 + x2.</li>
<li>Modular addition: y ≡ x1 + x2 [c].</li>
<li>Multiplication: y = x1 × x2.</li>
<li>Modular multiplication: y ≡ x1 × x2 [c],</li>
</ul>
<p>with x1 and x2, two positive integers, and c &gt; 1, a fixed modulus. Our models are trained to predict y from (x1, x2).</p>
<p>For the addition tasks, the train set is composed of pairs of positive integers with up to 5 digits, i.e. (x1, x2) ∈ ℕ105. x1 is randomly sampled from a fixed set of N_{train} values (we usually set N_{train} = 5000). x2 is uniformly sampled in ℕ105. Since N_{train} ≪ 100,000, the training set only covers a small portion of the problem space. This guarantees that the model will not overfit. Trained models are tested on random pairs of positive integers with n_{test} digits: (x1, x2) ∈ ℕ2p, p = 10^{ntest}. We set n_{test} = 5 for in-domain accuracy, and n_{test} ∈ {6, ...20} for length generalization.</p>
<p>For multiplication, we train from pairs of positive integers with up to 5-digits and 3-digits, i.e. x1 &lt; 105 and x2 &lt; 103. We henceforth refer to this setting as “5 × 3 multiplication”. As before, x1 is randomly sampled from a fixed set of N_{train} examples, and x2 is uniformly sampled in ℕ1000.</p>
<p>Trained models are tested on $n_{\text {test }} \times 3$ products, with $n_{\text {test }}=5$ in-domain, and $n_{\text {test }} \in{6, \ldots 35}$ for length generalization.</p>
<p>Data formatting. The arithmetic operations (e.g. $535 \times 257$ ) and the integers (137495) that correspond to model input and output are encoded as sequences of discrete symbols. Integers are represented as sequences of digits, in base 10, and padded (using the special token <PAD>) to lengths $n_{\text {test }}$ for input operands, and $n_{\text {out }}$ for output. We have $n_{\text {out }}=n_{\text {test }}+1$ for addition, and $n_{\text {out }}=2 n_{\text {test }}$ for multiplication. The four operations are encoded with the dedicated tokens,$+ \mathbf{\%}$, $\times$ and $<em>$. Overall, we use a vocabulary of 15 tokens: ${0, \ldots, 9,+, \mathbf{\%}, \times, </em>,&lt;$ PAD $&gt;}$. For example, for addition with $n_{\text {train }}=2$ and $n_{\text {test }}=3$, the train and test examples $12+39=51$ and $999+345=1344$ would be encoded as:</p>
<p>$$
\begin{aligned}
&amp; x^{\text {train }}=12&lt;\text { PAD }&gt;+39&lt;\text { PAD }&gt; \
&amp; y^{\text {train }}=51&lt;\text { PAD }&gt; \
&amp; x^{\text {test }}=999+345 \
&amp; y^{\text {test }}=1344
\end{aligned}
$$</p>
<p>We use the padding symbol in order to ensure that all the input sequences and output sequences have the same length. This is crucial for the model in order to deal with carries.</p>
<p>Training procedures. We use the following three procedures. Standard training is used in Sections 3 and 4. Fine-tuning and priming are introduced in Section 5. In all training procedures, the first operands and randomly sampled from a fixed set of $N_{\text {train }}$ examples, and the second operands are generated online (i.e. uniformly sampled between 1 and $10^{5}$ for addition, and between 1 and $10^{3}$ for multiplication).</p>
<ul>
<li>Standard training: the model is trained on $N_{\text {train }}$ examples of $n_{\text {train }}$-digit integers.</li>
<li>Fine-tuning: the model is trained on $N_{\text {train }}$ examples of $n_{\text {train }}$-digit integers and then fine-tuned on $N_{\text {fine }}$ examples of $n_{\text {test }}$-digit integers.</li>
<li>Train set priming: the model is trained on $(1-\varepsilon) N_{\text {train }}$ examples of $n_{\text {train }}$-digit integers and $\varepsilon N_{\text {train }}$ priming examples of $n_{\text {test }}$-digit integers, with $\varepsilon \ll 1$. The priming examples are fixed throughout the training.</li>
</ul>
<p>Evaluation sets. During and after training, model performance is evaluated on randomly generated test sets, of $N_{\text {test }}$ integers with $n$ digits. The resulting accuracy is said to be in-distribution (ID) when $n=n_{\text {train }}$, and out-of-distribution (OOD) when $n&gt;n_{\text {train }}$. New test sets are generated online for each evaluation step. If not specified otherwise, we use $n_{\text {train }}=5, N_{\text {train }}=5000$, and $N_{\text {test }}=10000$. We set $n_{\text {test }}=20$ for addition, and $n_{\text {test }}=35$ for multiplication.</p>
<h1>2.2 Model and training</h1>
<p>Model. We experiment with two encoder-only architectures: a regular transformer (Vaswani et al., 2017), and a universal transformer (UTransformer) (Dehghani et al., 2018), in the HuggingFace implementation (Wolf et al., 2020) of BERT (Devlin et al., 2018) and ALBERT (Lan et al., 2019). Our model is a stack of three components (see Figure 1):</p>
<ol>
<li>
<p>Embedding: a ( $s_{\text {vocab }} \times d_{\text {model }}$ )-trainable embedding layer and a position embedding.</p>
</li>
<li>
<p>Encoder: an encoder-only transformer or UTransformer.</p>
</li>
<li>Classifier: encoder output is truncated (to its first $n_{\text {out }}$ elements, forming a $n_{\text {out }} \times d_{\text {model }}$ matrix), which is processed by a linear layer that outputs $n_{\text {out }} \times s_{\text {vocab }}$ predictions, and encodes each symbol as a one-hot vector.</li>
</ol>
<p>Important note: Although we use the HuggingFace implementation, our encoders are not pre-trained, and we do not use masked language modelling. We train non-causal encoders in a supervised way, using cross-entropy loss.</p>
<p>Notes on design. We chose to use universal transformers, i.e. transformers with shared layers (Dehghani et al., 2018), because recurrent models are used in prior work on length generalization (Bansal et al., 2022; Kaiser and Sutskever, 2015), and universal transformers proved essential on tasks involving modular arithmetic (Wenger et al., 2022). We believe shared-layer architectures are central to solving arithmetic problems, because they embed the recursive nature of many algorithms. They also seem fit for extrapolation tasks where a long operand is processed by successive applications of a simple technique (e.g. one-digit add and carry).</p>
<p>The choice of an encoder-only model contrasts with concurrent works that consider decoder-only (Power et al., 2022; Bueno et al., 2022; Zhou et al., 2022) or sequence to sequence (seq2seq) models (Nogueira et al., 2021). We believe that autoregressive models, such as the decoder-only architecture, are not optimal for problems of arithmetic, because they are trained to learn the correlations between successive tokens in the input sequence. In natural language, these correlations are meaningful: they represent the syntactic and grammatical relations between words in a sentence. In arithmetic, these correlations are tiny: knowing that the first three digits of number 1234 are 1,2 and 3 , offers no clue about the value of the fourth digit. As for seq2seq models, in problems where output are guaranteed to be shorter than input, we consider an auto-regressive decoder as an unnecessary complication. Overall, we choose encoder-only models because they are the simplest architecture that can address our problems.</p>
<p>Learning problem. We frame our arithmetic tasks as the following supervised multi-classification problem:</p>
<p>$$
\min <em i="1">{\theta \in \Theta} \sum</em>
$$}^{N_{\text {train }}} \sum_{j=1}^{n_{\text {out }}} \sum_{k=1}^{s_{\text {vocab }}} \mathbf{1}\left[y_{i}[j]=k-1\right] \frac{e^{f_{\theta}\left(x_{i}\right)[j, k]}}{\sum_{k^{\prime}=1}^{s_{\text {vocab }}} e^{f_{\theta}\left(x_{i}\right)\left[j, k^{\prime}\right]}</p>
<p>where $f_{\theta}\left(x_{i}\right) \in \mathbb{R}^{n_{\text {out }} \times s_{\text {vocab }}}$ are the model logits evaluated at $x_{i}$ and $\theta \in \Theta$ are the model parameters. To solve (1), we minimize the cross entropy between model predictions and the ground truth symbols for each position in the sequence. An alternative approach, perhaps more natural, would consider these problems as regressions. However, prior works report that reformulating regression as classification leads to state-of-the-art performance (Rothe et al., 2015; Rogez et al., 2017; Akkaya et al., 2019; Schrittwieser et al., 2020).</p>
<p>We consider three model sizes. Base (B) models have $D=6$ layers, $d_{\text {model }}=512$ dimensions, and $h=8$ attention heads, Standard (S) models have $D=6, d_{\text {model }}=1024$ and $h=16$, and Large (L) models,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Number of digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Encoder</td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">$\mathbf{9 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: left;">UTransformer</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">$\mathbf{9 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">1.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Addition: Impact of encoder type, size and position embeddings on length generalization. We consider transformers and UTransformers in their Base (B) and Large (L) format, using three position embeddings methods $\left(\mathrm{APE}, \mathrm{RPE}<em k_="k," q="q">{k}, \mathrm{RPE}</em>\right)$. We evaluate different degrees of extrapolation: easy ( 6 digits), medium ( 10 digits) and hard ( 15 and 20 digits). The models are trained on 5000 examples with 1 to 5 digits and we report the accuracy reached by the models on 100,000 example test sets. Results are averaged over 3 seeds.
we have $D=10, d_{\text {model }}=1024$ and $h=16$. We investigate three kinds of position embeddings: absolute (APE) Vaswani et al. (2017), relative over keys $\left(\mathrm{RPE}<em k_="k," q="q">{k}\right)$ Shaw et al. (2018), and relative over keys and queries $\left(\mathrm{RPE}</em>$ is our default option. All other parameters are set to the default HuggingFace values, and are initialized with random Gaussian values.}\right)$ Huang et al. (2018). $\mathrm{RPE}_{k</p>
<p>Optimization. We train our models using AdamW (Loshchilov and Hutter, 2017), with a batch size to 32 , a learning rate between $10^{-5}$ and $10^{-4}$ and weight decays in ${1 \mathrm{e}-5,1 \mathrm{e}-4,1 \mathrm{e}-3,1 \mathrm{e}-2}$. We apply a cosine scheduler Loshchilov and Hutter (2016) to update the learning rate and train the model for 15000 epochs of $N_{\text {train }}$ examples.</p>
<h1>3 Addition: relative position embeddings enable length generalization</h1>
<p>In these experiments, we train transformers to add two numbers with up to five digits, and test trained models on sums of numbers with 6 to 20 digits. We compare the Transformer and UTransformer encoders, in their Base ( 6 layers, 512 dimensions, 8 attentions heads) and Large ( 10 layers, 1024 dimensions, 16 heads) configurations, using three position embeddings: absolute, relative on keys, and relative on keys and queries. All models achieve $100 \%$ in-domain accuracy. We make the following observations (Table 1):</p>
<ul>
<li>Models using the absolute position embedding fail to generalize. Our best models achieve $3.1 \%$ accuracy on 6 -digit test examples, and $0 \%$ for all longer lengths. This was</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Scaling laws for integer addition. We train Transformers and UTransformers, with standard model size $\left(d_{\text {model }}=16, D=6, h=16\right)$ to add numbers with up to 5 digits. We set $N_{\text {train }}=50000$. We vary their hidden size (a) and depth (b). The $y$-axis indicates the largest extrapolation length where the model achieves $75 \%$ accuracy. Results are averaged over 3 seeds.
observed in previous works Shaw et al. (2018); Dai et al. (2019); Huang et al. (2020); Kiyono et al. (2021).</p>
<ul>
<li>Models using relative position embedding generalize to longer sequences. Our best models achieve $99.9 \%$ accuracy on 10 -digits test sets, and $98.3 \%$ on 15 -digit sets. Performance drops for longer sequences: we achieve $21.3 \%$ for 20 -digits numbers. We remark that the RPE key variant is crucial for achieving extrapolation.</li>
</ul>
<p>In APE models, because the position embedding is added to the embedding of every token, the rules of addition must be learned separately for every position. At test time, a model trained on operands with 5 digits only will not know how to handle digits in position 6 , or 7 , even though it has learned to add digits in position 1 to 5 . Further discussion of the role of position embeddings, and additional experiments on model failures, can be found in Section 6.</p>
<p>Depth and dimension for longer extrapolation. Figures 2a and 2b provide ablation results on model dimension and depth. For models with 64 to 1024 dimensions and 2 to 8 layers, trained on 5 digit examples, they indicate the largest extrapolation length that the model can achieve with $75 \%$ accuracy. A minimal hidden size of 512 for Transformers, and 256 for UTransformers, is needed for the model to extrapolate. Past this value, length extrapolation scales with dimension, and 1024-dimension models achieve 17-digit extrapolation. UTransformers need 6 layers to extrapolate, whereas shallow Transformers with 2 layers can extrapolate to 10-digit numbers. The efficiency of shallow transformer models for computational tasks was observed in previous works (Charton, 2021).</p>
<h1>4 Modular arithmetic</h1>
<p>In this section, we study modular addition $y \equiv\left(x_{1}+x_{2}\right)[c]$ and multiplication $y \equiv\left(x_{1} \times x_{2}\right)[c]$, for $c \in{100,101,128,1000}$. The difficulty of these operations depends on the modulus $c$. When $c$ is a power of 10 , i.e. $c=10^{k}$, modular operations only involve the $k$ last digits of their operands, and the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Modulo</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">Digits <br> 10</th>
<th style="text-align: center;">15</th>
<th style="text-align: center;">20</th>
<th style="text-align: center;">c</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">3.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Modular addition and multiplication: (a) Extrapolation results for addition and (b) for multiplication. We train a UTransformer in its base version $\left(D=6, d_{\text {model }}=512, h=8\right)$ with three position embedding methods $\left(\mathrm{APE}, \mathrm{RPE}<em k_="k," q="q">{k}, \mathrm{RPE}</em>\right)$. We report the accuracy on 100,000 example test sets.
result has constant length $k$. This makes these operations easier to learn (because they only involve $k$ digits), and easier to generalize (because $k$ is independent of the length of the operands). When the modulus is not a power of 10 , the problem becomes harder than tbeir non-modular verison, because modularity adds an integer division on top of the operation (addition or multiplication).</p>
<p>Modular addition. In the "easy" cases $(c \in{100,1000})$, RPE-based models generalize to large numbers, achieving better extrapolation performance than for non-modular addition (Table 2a). This is expected, because this is an easier task than standard addition. Interestingly, APE-based models do generalize; they achieve $73.3 \%$ accuracy on 10-digit numbers. This confirms our intuition that the failure of APE on length generalization is a consequence of their inability to deal with change in output sequence lengths.</p>
<p>For the hard cases $(c \in{101,128})$, no model manages to learn 5-digit modular addition indomain. Scaling to larger architectures, with up to 14 layers and 1280 dimensions, brings no improvement. This matches previous observations by Palamas (2017), about the difficulty of learning modular arithmetic in the general case.</p>
<p>Modular multiplication. In the easy cases $(c \in{100,1000})$, both APE and RPE-based model generalize, achieving $100 \%$ on 35-digit numbers for $c=100$. For $c=1000$, APE achieve $43 \%$ on 20-digit numbers, but the use of RPE improves performance, to $83 \%$ on 20-digit numbers and $55 \%$ on 30-digit numbers (Table 2b). On hard instances (see Appendix A), for $c=128$, the model performance drops, both in and out of domain, but length generalization still happens, and is facilitated by RPE and larger models. Finally, for $c=101$, models can learn modular multiplication in-domain, but consistently fail on longer sequences. Modular multiplication turns out to be easier to learn than modular addition. A possible explanation is the fact that multiplication tables display more redundancy, that the model can exploit, than addition tables.</p>
<p>Our experiments with modular arithmetic help understand the role of position embeddings. APE-based models generalize when they learn an operation involving a fixed number of input tokens, and constant length output.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Second <br> operand</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">1-digit</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">2-digits</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">3-digits</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 3: Multiplication by 1, 2 and 3-digit numbers: We train a UTransformer in its standard version $\left(D=6, d_{\text {model }}=1024, h=16\right)$ with three position embeddings $\left(\mathrm{APE}, \mathrm{RPE}<em k_="k," q="q">{k}, \mathrm{RPE}</em>\right)$. ID and OOD accuracy on 100,000 test examples.</p>
<h1>5 Multiplication: train set priming for length generalization</h1>
<p>We focus on the length generalization problem where we train a UTransformer to multiply 5-digit numbers by 3-digit numbers, from $N_{\text {train }}=5000$ examples and train it on a set of $N_{\text {train }}=5000$ examples that are $\left(n_{\text {train }} \times 3\right)$-multiplications with $n_{\text {train }} \leq 5$. We test its extrapolation ability to perform $35 \times 3$ multiplications.</p>
<h3>5.1 Relative position embeddings and fine-tuning</h3>
<p>Relative position embeddings are not sufficient. We first train UTransformers with the three position embedddings (Table 3). All models achieve close to $100 \%$ in-domain accuracy, but fail to generalize to numbers with 6 digits or more. For $5 \times 3$ multiplication, RPE do not generalize. On simpler versions of this task $(5 \times 2$ and $5 \times 1)$, RPE models achieve limited generalization to 6-digit numbers ( 12.2 and $16.9 \%$ for 1 and 2 -digits), but fail for longer sequences.</p>
<p>Fine-tuning requires a sizable sample set. Fine-tuning is a common solution for transfer learning (extrapolating from one distribution to another). Here, we first train a model on $5 \times 3$ multiplication, then re-train it on a fixed sample of $35 \times 3$ examples. We observe (Figure 3a) that 35-digit multiplication can indeed be learned by fine-tuning on a set of 1000 examples. This is a large number: as we shall see, train set priming allows for much smaller samples. Besides, the fine-tuned model is not longer able to perform $5 \times 3$ multiplication, a phenomenon known as catastrophic forgetting (McCloskey and Cohen, 1989).</p>
<h3>5.2 Priming for length generalization in multiplication.</h3>
<p>As an alternative, we introduce train set priming: adding a tiny amount ( $\varepsilon \%$ ) of long sequences to the training set. By adding 5035 -digit examples $(\varepsilon=1 \%)$, our model achieves close to $100 \%$ accuracy on $5 \times 3$ and $35 \times 3$ multiplication (Figure 3b). To reach equivalent performance, train sample priming needs 20 times less examples than fine-tuning. $5 \times 3$ multiplication is learned after a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Fine-tuning (a) and train set priming (b). (a) fine-tuning, the model is trained on $5 \times 3$ multiplications, then fine-tuned on $35 \times 3$ multiplications. Final accuracy of $5 \times 3$ and $35 \times 3$ multiplications as a function of the number of fine-tuning examples. (b) priming, fifty $35 \times 3$ examples are added to the training set. Learning curves for 5-digit and 35-digit accuracy. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$. Average over 3 seeds.
few hundred thousand examples, $35 \times 3$ multiplication (OOD generalization) after 1500 epochs, or 7.5 million examples ( 1500 passes over 5000 fixed examples), but only 75,00035 -digit example (i.e. 1,500 passes over 50 fixed examples, out of $9.10^{34}$ possible 35 -digit integers).</p>
<p>A minimal priming rate is required. Adding less than 25 samples ( 25 examples, $\varepsilon=0.5 \%$ ) prevents generalization. Over that threshold, accuracy increases with the priming rate (Figure 4a).</p>
<p>Priming sample scales logarithmically with train set size. As the number of training examples increases, so does the number of priming examples required to extrapolate to $35 \times 3$. However, it scales logarithmically: $30(\varepsilon=3 \%)$ priming examples are needed for $10^{3}$ training examples, $70(\varepsilon=0.7 \%)$ for $10^{4}$ and $100(\varepsilon=0.1 \%)$ for $10^{5}$ (Figure 4b).</p>
<p>Priming sample scales linearly with extrapolation length. Whereas 50 samples are needed for 35-digit generalization, 6-digit generalization only needs 10 (Figure 4c).</p>
<p>Curriculum priming fails. We consider curriculum priming as a possible improvement. Instead of priming on long sequences only (i.e. 35-digit numbers), we could split the priming examples between several lengths, from 6 to 35 . In most cases, curriculum priming fails to extrapolate to $35 \times 3$ multiplication, but one curriculum proves effective: priming the model on a mixture of 34 and 35 -digits numbers (Figure 4d). This causes the model to learn faster and achieve higher extrapolation accuracy.</p>
<h1>5.3 Priming for extrapolation at all lengths</h1>
<p>Priming the train set with 35-digit numbers only allows to extrapolate to 35-digit operands. No other extrapolation lengths are learned in the process (Figure 5a). However, by priming on numbers of all lengths from 6 to 35 , the model can extrapolate to all lengths up to 35 . This can be done at a</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablations on priming sample size. (a) Accuracy of $35 \times 3$-multiplications vs priming sample size. (b) Priming sample needed to achieve $90 \% 35$-digit accuracy for different train set sizes. (c) Priming sample needed to achieve $90 \%$ accuracy, for different extrapolation lengths. (d) Learning curves for 35-digit priming, and 34 and 35-digit curriculum. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$. Results are averaged over 3 seeds.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training set priming to all lengths. (a) Priming with 35-digit numbers only. (b) Priming with a mixture of all length. (c) Distribution of priming lengths for figure (b). (d) Priming on even lengths only. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$. Average over 3 seeds.
moderate cost in additional data. Using the priming distribution from Figure 5c, our models learn to extrapolate with over $95 \%$ accuracy to all lengths (see Figure 5b). The priming set size is 500 , for a priming rate of $\varepsilon=10 \%$. More efficient priming distributions might exist: the point of this experiment is to show that priming to all lengths is possible within a reasonable data budget $\varepsilon$. On the other hand, we observe that all extrapolation length must be primed. For instance, if only even lengths are primed, the model only generalizes to even lengths. There is no overspill to odd lengths (Figure 5d).</p>
<h1>6 Discussion</h1>
<h3>6.1 Why do RPEs extrapolate better than APEs?</h3>
<p>In Section 3, we notice that replacing APE by RPE is the key for models to length generalize. Three experiments help understand the role of RPE.</p>
<p>Element-wise addition. A possible reason for generalization in RPE-based models, is that relative embeddings allow tokens to "know their neighbors". This could help models learn local operations, like carry propagation (an important factor in integer addition). To test this hypothesis,</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Success and failure cases in addition. (a) Accuracy of 20-digit sums, by number of carries in the sum. (b) Accuracy of 20-digit sums, by maximum number of consecutive carries. (c) Distribution of the number of incorrect digits in wrong predictions of 20-digit sums. (d) Positions of incorrect digits in sumes where only one digit is wrong. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$, achieving $57 \%$ accuracy on 20-digit additions.
we train models on element-wise addition $\oplus$ (i.e. addition without carries: $99 \oplus 35=24$ ). If carry propagation is the reason why RPE succeed, APE-models should generalize on this task. Experimental results (in Appendix A) show that APE fail to generalize on element-wise addition, whereas RPE succeed, this disproving our hypothesis. It is striking to note (see Table 8) that when the generalize, APE models almost always predict the the 5 leftmost digits of the results, i.e. its "in-domain" positions, thus confirming our intuition that APE learn addition digit by digit.</p>
<p>Modular arithmetic. As we have seen, APE models length generalize on these tasks when the modulus is a power of 10 . (Tables 2a and 2b). In both cases, the model output have constant length. This, together with our element-wise results, suggest that varying output lengths are an important factor of APE extrapolation failures.</p>
<p>RPE-models learn all digits at once. Figures 7a and 7b present learning curves for each position in the output, when a model is trained on 5-digit addition (e.g. the 6 curve is the learning curve of the units of the sum, the 5 -curve is the tens). We note that whereas the first and last digits in the sums are learned first, all other digits are learned simultaneously by RPE models, whereas APE models seem to learn each position independently. This suggests that RPE models might learn a single algorithm for all positions, which greatly helps them to generalize.</p>
<h1>6.2 Failure cases in addition</h1>
<p>Figure 6 provides an analysis of model failures when extrapolating to 20-digit sums. First, we assess the role of carries, by introducing two metrics: the total number of carries (NC), and the maximum number of consecutive carries (MC). As Figures 6a and 6b indicate, almost all model failures happen on additions involving at least three carries, and two consecutive carries. Larger values of MC and NC have no further impact.</p>
<p>Figures 6 c and 6 d present the number of incorrect digits in wrong model predictions and their position. We note that, when wrong, the model usually does not hallucinate a irrelevant answer (with many wrong digits), but fails on just a few. Errors also concentrate on the first and second positions: the largest powers of ten in the sum.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Digit by digit learning curves. Training accuracy for each output digit (1 are the largest powers, 6 the units for a sum).(a) Addition APE models. (b) Addition RPE models. (c) Multiplication RPE models (no priming) (d). Multiplication RPE models (with priming). In all these experiments, 1 denotes the leftmost digit position while 6 (for addition) and 8 (for multiplication) All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$.</p>
<h1>6.3 More about priming</h1>
<p>Train set priming is our most striking result. In Section 5, we demonstrate that is allows length generalization in multiplication. We now present additional results. We first show that train set priming is also effective on APE models. Then, we investigate how the models learn multiplication.</p>
<p>Primed APE models generalize. In Appendix A, we show that priming on APE models also yields length generalization. We obtain a similar dynamics as in Figure 3b where the ID accuracy quickly increases and the OOD accuracy slowly follows (Figure 9a). However, as expected, this does not make APE models a viable proposition: the priming rate needed is 10 times larger i.e. $\varepsilon=10 \%$.</p>
<p>Primed models learn several digits simultaneously. In our addition experiments in Subsection 6.1, we noticed that whereas APE models learn to predict their output digit by digit as training proceeds (Figure 7a), RPE models seem to learn them all at once (Figure 7a). A similar pattern can be seen for multiplication with RPE models. Without priming (Figure 7c), models seem to learn $5 \times 3$ multiplication one digit at a time, over 1000 epochs. With priming, the model seems to learns several digits concurrently Figure 7d. A similar phenomenon holds for APE models: without priming, the model independently learns each digit (Figure 9b) while the digits are concurrently learnt with priming (Figure 9c). In summary, simultaneous learning of all the training digit positions seems a key determinant of length generalization.</p>
<h3>6.4 Priming beyond arithmetic</h3>
<p>Our work demonstrates that train set priming can improve the length generalization of transformers on arithmetic tasks. Compared to fine-tuning, it requires much fewer samples from the target distribution and allows for generalization without catastrophic forgetting. We conclude on a number of open questions, which constitute as many avenue for future research. All these directions may help shed light on the capabilities and limitations of transformers, and inspire new methods for improving their generalization and adaptation.</p>
<ul>
<li>Can priming be extended to other mathematical problems? For instance, numerical computations, matrix operations, or symbolic mathematics.</li>
<li>Can priming help with compositionality? Investigate the limits of length generalization in terms of the number and type of operations. For instance, if we train on adding $k$ numbers,</li>
</ul>
<p>can we generalize to adding $k+1$ numbers, or if we train on compositions of additions and multiplications separately, does it generalize to compose them together?</p>
<ul>
<li>Theoretical understanding of priming: why is train set priming more effective than fine-tuning for length generalization?</li>
<li>Can priming work for NLP? Can we use priming to adapt a pre-trained language model to a new language task, without losing its performance on the original data?</li>
</ul>
<h1>References</h1>
<p>Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. arXiv preprint arXiv:2207.04901, 2022.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking, 2022.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Mirelle Bueno, Carlos Gemmel, Jeffrey Dalton, Roberto Lotufo, and Rodrigo Nogueira. Induced natural language rationales and interleaved markup tokens enable extrapolation in large language models. arXiv preprint arXiv:2208.11445, 2022.</p>
<p>François Charton. Linear algebra with transformers. arXiv preprint arXiv:2112.01898, 2021.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Aurélien Dersy, Matthew D. Schwartz, and Xiaoyuan Zhang. Simplifying polylogarithms with machine learning, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16 x 16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. arXiv preprint arXiv:1911.03872, 2019.</p>
<p>Kaden Griffith and Jugal Kalita. Solving arithmetic word problems with transformers and preprocessing of problem text. arXiv preprint arXiv:2106.00893, 2021.</p>
<p>Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus N Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. arXiv preprint arXiv:2003.04218, 2020.</p>
<p>Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, and Douglas Eck. An improved relative self-attention mechanism for transformer with application to music generation. 2018.</p>
<p>Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position embeddings. arXiv preprint arXiv:2009.13658, 2020.</p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34, 2021.</p>
<p>Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. Advances in neural information processing systems, 28, 2015.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015 .</p>
<p>Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. arXiv preprint arXiv:2109.05644, 2021.</p>
<p>Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.</p>
<p>Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109-165. Elsevier, 1989 .</p>
<p>Yuanliang Meng and Anna Rumshisky. Solving math word problems with double-decoder transformer. arXiv preprint arXiv:1908.10924, 2019.</p>
<p>Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006, 2018.</p>
<p>Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and length extrapolation. arXiv preprint arXiv:2010.07174, 2020.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021 .</p>
<p>Theodoros Palamas. Investigating the ability of neural networks to learn simple modular arithmetic. 2017 .</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022 .</p>
<p>Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net: Localization-classificationregression for human pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3433-3441, 2017.</p>
<p>Jan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and Hermann Ney. Analysis of positional encodings for neural machine translation. In Proceedings of the 16th International Conference on Spoken Language Translation, 2019.</p>
<p>Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In Proceedings of the IEEE international conference on computer vision workshops, pages $10-15,2015$.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, JFC Uribe, L Fedus, L Metz, M Pokorny, et al. Chatgpt: Optimizing language models for dialogue, 2022.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.</p>
<p>Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. arXiv preprint arXiv:1808.00508, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Emily Wenger, Mingjie Chen, François Charton, and Kristin Lauter. Salsa: Attacking lattice cryptography with transformers. arXiv preprint arXiv:2207:04785, 2022.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45, 2020.</p>
<p>Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022 .</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.</p>
<h1>A Additional experiments</h1>
<p>In this section, we present some additional experiments that were mentioned in the paper. We first provide in Subsection A. 1 the complete results for modular addition and multiplication that were mentioned in Section 4. We then present complementary results to our discussion in Section 6. We first report the results obtained by APE and RPE models on digitwise addition. Then, we show that APE models can also be primed to length generalize in multiplication at the expense of a much larger priming rate (Subsection A.3). Lastly, we present plots showing the digit order by which RPE and APE models make the correct predictions (Subsection A.5).</p>
<h2>A. 1 Additional experiments on modular arithmetic</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">C</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Modular addition: Extrapolation results for modulo $c \in{100,1000,128,101}$. UTransformer model in their Base and Large format. We report the accuracy reached by the models on 100,000 example test sets.</p>
<p>Table 4 provides a more complete version of Table 2a where we do modular addition for modulus $c \in{128,101}$. As explained in Section 4, the model manages to extrapolate when the modulus is a power of 10 . When $c=128,101$, the model fails to extrapolate. This shows that what the model</p>
<p>struggles when the length of the digits that matter vary.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">c</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Modular multiplication: Extrapolation results for modulo $c \in{100,1000,128,101}$. UTransformer model in their Base and Large format. We report the accuracy reached by the models on 100,000 example test sets.</p>
<p>Table 5 provides a more complete version of Table 2b where we do modular multiplication for modulus $c \in{128,101}$. As explained in Section 4, the model manages to extrapolate when the modulus is a power of 10 . When $c=128$, the model non-trivially length generalize while when $c=101$, the model fails to extrapolate. We do not fully know why this difference happens but one hypothesis is that 101 is a prime number while 128 a power of 2 .</p>
<h1>A. 2 Element-wise addition experiments</h1>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Digits</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PE</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">20</td>
</tr>
<tr>
<td style="text-align: left;">APE</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">5.3</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">RPE $_{k}$</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">97.5</td>
<td style="text-align: right;">90.5</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">78.13</td>
</tr>
</tbody>
</table>
<p>Table 6: Element-wise addition: Extrapolation results. We train a UTransformer in its base version $\left(D=6, d_{\text {model }}=\right.$ $512, h=8$ ) with two position embedding methods (APE, $\mathrm{RPE}_{k}$ ). We report the accuracy reached by the models on 10,000 example test sets.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Digitwise accuracy of the APE model on elementwise addition. We train a Base UTransformer with APEs and report the accuracy on 10,000 example test sets. Average over 3 seeds.</p>
<p>We consider here an element-wise addition operation. For example, $99 \oplus 45=34$ because $(9+5) \% 10=4$ and $(9+4) \% 10=3$. We train a UTransformer on 5-digit element-wise addition $\left(N_{\text {train }}=50,000\right)$ and evaluate its extrapolation on 20-digit $\left(N_{\text {test }}=10,000\right)$. Table 6 reports the final results obtained with APE and RPE models. We observe that the RPE models manage to length generalization while the APE models fail. In Figure 8, we plot the digitwise accuracy on the test samples. We observe that the model managed to well-predict the leftmost 5 digits (those seen during training) but fails in the right-most ones.</p>
<h2>A. 3 Multiplication experiments using APEs</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Additional experiments on priming for multiplication. (a) shows the accuracy on $5 \times 3$ and $35 \times 3$ multiplications obtained by an APE model. (b) and (c) respectively display the learning process of an APE model without and with train set priming on multiplication. We train a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$ on $5 \times 3$-multiplications and test on $35 \times 3$. Training set size is $N_{\text {train }}=5000$ and test set size is $N_{\text {test }}=10000$.</p>
<p>In this section, we consider the multiplication task with UTransformers using APEs. Similarly to the RPE case, we observe that training set priming lead to successful extrapolation to $(35 \times 3)$ -</p>            </div>
        </div>

    </div>
</body>
</html>
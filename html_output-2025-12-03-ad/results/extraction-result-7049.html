<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7049 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7049</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7049</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-3ca5822045093a18dd01d22f868f4a5da28bb4f3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3ca5822045093a18dd01d22f868f4a5da28bb4f3" target="_blank">LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> LLM-ARC is introduced, a neuro-symbolic framework designed to enhance the logical reasoning capabilities of Large Language Models by combining them with an Automated Reasoning Critic, showcasing the robustness and efficacy of LLM-ARC for complex natural language reasoning tasks.</p>
                <p><strong>Paper Abstract:</strong> We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the logical reasoning capabilities of Large Language Models (LLMs), by combining them with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic method where the LLM Actor generates declarative logic programs along with tests for semantic correctness, while the Automated Reasoning Critic evaluates the code, runs the tests and provides feedback on test failures for iterative refinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a new state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests complex logical reasoning capabilities. Our experiments demonstrate significant improvements over LLM-only baselines, highlighting the importance of logic test generation and iterative self-refinement. We achieve our best result using a fully automated self-supervised training loop where the Actor is trained on end-to-end dialog traces with Critic feedback. We discuss potential enhancements and provide a detailed error analysis, showcasing the robustness and efficacy of LLM-ARC for complex natural language reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7049.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7049.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-ARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-ARC (Large Language Model - Automated Reasoning Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic Actor-Critic framework that uses an LLM Actor to generate declarative logic programs (ASP) and semantic tests, and an Automated Reasoner (Clingo ASP solver) as a Critic to run tests, produce explanations, and provide feedback for iterative self-correction; trained in a self-supervised loop on dialog traces with Critic feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-ARC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic system combining an LLM program-writer (Actor) with an automated ASP reasoner (Critic). The Actor synthesizes ASP code and logic tests; the Critic executes ASP, checks tests, generates proof-based explanations, and returns feedback for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Neuro‑symbolic (Transformer LLM Actor + ASP solver Critic, Actor‑Critic loop)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Few‑shot in‑context examples stratified by logical class (8 or 20 examples) for initial Actor; self‑supervised fine‑tuning data: 918 end‑to‑end dialog traces from the Actor‑Critic self‑correction loop collected on FOLIO training set.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis to ASP + explicit logic test generation + iterative self‑correction Actor‑Critic loop; query interpretation grammar and proof‑by‑refutation explanation extraction from the ASP solver.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Clingo ASP solver used as the Critic: compiles and executes generated ASP programs, evaluates semantic tests (infer-True-All/Any, infer-False, expect-Contradiction), and provides compilation errors and minimal-rule explanations via a proof‑by‑refutation algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Human‑annotated natural language reasoning benchmark with premises and conclusions; tasks require determining whether a conclusion is True, False, or Uncertain relative to premises; includes FOL translations for items.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Natural language→formal program translation and logical entailment (True/False/Uncertain) under first‑order style problems (encoded in ASP); NL→ASP program synthesis, semantic test generation, and entailment checking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (prediction True/False/Uncertain)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>88.32% (LLM-ARC‑Trained on FOLIO v2)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LLM-ARC‑Trained achieves 88.32% vs prior known SOTA LogicLM 78.9% (≈+9.4 pp). Relative to best LLM-only (GPT4-FT-NL at 80.7%), LLM-ARC‑Trained improves ≈+7.62 pp. Ablations: adding TestGen to 8‑shot gave +6.6 pp; iterative self‑correction added ≈+5 pp; final trained Actor produced best result.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining LLM program synthesis with an automated reasoner Critic and explicit semantic test generation substantially improves strict logical reasoning on FOLIO; TestGen and iterative self‑correction materially increase accuracy, and training the Actor on Critic feedback dialog traces yields the best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependent on underlying formalism (ASP) limitations (poor support for existential quantification); sparse/rare logical classes (e.g., rules with multiple variables, type/instance punning) are harder; generated tests may not fully capture intended semantics or may pass for wrong reasons; Actor sometimes fails to modify code/tests across retries (self‑correction loop brittle); explanations lack full grounded rule instantiations; scaling to large inputs requires chunking strategies; no separate human‑trained Critic was used (potential improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7049.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7049.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer language model variant used as the Actor in LLM-ARC for few‑shot ASP code and test generation; found to produce high‑quality ASP translations in (zero-/few‑shot) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT‑4‑Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer‑based LLM variant (GPT‑4 family); used here as the Actor to translate NL premises to ASP and to generate logic tests and corrected code in an iterative loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer LLM (standard), used as program synthesizer in neuro‑symbolic pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>In‑context learning (few‑shot) to produce declarative ASP programs and explicit tests; also evaluated with Chain‑of‑Thought prompting for stepwise NL reasoning (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>When used in LLM‑ARC, GPT‑4‑Turbo outputs ASP code and tests which are executed by Clingo; some baselines used GPT‑4‑Turbo without the external reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LLM‑ARC entry; NL→entailment benchmark for first‑order style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL→ASP program synthesis and entailment prediction; few‑shot and CoT baselines for True/False/Uncertain classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baselines reported: GPT4‑T‑ZS 67.0%; GPT4‑T‑CoT 74.1%. In LLM‑ARC experiments with GPT4‑Turbo Actor: LLM‑ARC‑8‑shot 74.62%, LLM‑ARC‑8‑shot‑TestGen 81.22%, LLM‑ARC‑20‑shot 83.25%, LLM‑ARC‑20‑shot‑TestGen 85.79%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Chain‑of‑Thought (GPT4‑T‑CoT) improved zero‑shot GPT4‑T from 67.0% → 74.1%. Integrating GPT4‑Turbo as Actor with TestGen produced large gains (8‑shot: +6.6 pp with TestGen vs without). More in LLM‑ARC comparisons above.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT‑4‑Turbo is effective at generating declarative ASP code from NL in few‑shot settings; explicit test generation and integration with a symbolic reasoner substantially boosts end task accuracy compared to LLM‑only outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As an LLM, it can produce syntactic and semantic errors in generated code; requires external reasoner feedback to reach high accuracy; baseline CoT and fine‑tuning help but do not match full neuro‑symbolic training; model size and internal training corpora not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7049.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7049.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (fine-tuned Actor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (fine‑tuned on Actor‑Critic dialog traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT‑4 fine‑tuned in a self‑supervised manner on Actor‑Critic dialog traces (including Critic feedback and corrected ASP code/tests) to become the trained Actor for LLM‑ARC, producing the best system accuracy on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT‑4 (fine‑tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLM (GPT‑4) that was fine‑tuned on 918 end‑to‑end dialog traces from the LLM‑ARC self‑correction loop to improve its ability to write ASP code, tests, and perform rectifications.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (fine‑tuned for program synthesis within neuro‑symbolic Actor‑Critic loop)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>918 dialog traces derived from LLM‑ARC self‑correction on FOLIO training examples (types: compiler error rectifications, test‑failure rectifications, and direct passes).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised fine‑tuning to learn program synthesis, test generation, and correction strategies from Critic feedback; retains neuro‑symbolic operation with Clingo Critic.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Trained Actor outputs ASP and tests which are executed and evaluated by Clingo; training data includes Critic's error messages/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LLM‑ARC entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL→ASP program synthesis and entailment classification with learned rectification behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>88.32% (LLM‑ARC‑Trained using fine‑tuned GPT‑4 Actor)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms GPT4‑FT‑NL (80.7%) by ≈+7.62 pp and prior SOTA LogicLM (78.9%) by ≈+9.42 pp; demonstrates benefit of training Actor on Critic dialog traces vs direct fine‑tuning on NL or FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine‑tuning the Actor on Critic feedback dialog traces enables direct generation of high‑quality ASP code and tests, increasing final entailment accuracy substantially compared to few‑shot or LLM‑only fine‑tuning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on the dialog trace curation (they only retained the last rectification steps to fit context windows); limited context window (8k) constrained trace length; potential overfitting to patterns in FOLIO; remaining failure modes stem from ASP formalism limits and sparse logical classes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7049.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7049.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neurosymbolic system that combines LLMs with symbolic solvers for faithful logical reasoning; reported prior state‑of‑the‑art on (an earlier version of) FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro‑symbolic approach integrating an LLM with formal logic solvers (various solvers depending on underlying logic) to improve faithful logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Neuro‑symbolic (LLM + formal solver)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LLM to formal representation + symbolic solver for reasoning; includes self‑refinement loop for syntax errors (but not semantic test generation as in LLM‑ARC).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses symbolic solvers tailored to the target logic to check and perform reasoning over the formal representations generated by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO (prior reported results)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Natural language reasoning benchmark (see FOLIO entry); LogicLM reported 78.9% as prior SOTA (noted in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL→formal representation + reasoning (entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>78.9% (reported prior SOTA on FOLIO v1 / cited prior result)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Reported as prior SOTA before LLM‑ARC; LLM‑ARC‑Trained surpasses it by ~9.4 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating symbolic solvers with LLMs improves logical reasoning relative to LLM‑only systems; however, LogicLM did not include semantic test generation validated by the solver.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Per this paper, LogicLM's self‑refinement focuses on syntax errors only; lacks the semantic test generation and reasoner‑driven test feedback loop introduced in LLM‑ARC.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7049.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7049.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic system that combines LLMs with first‑order logic provers to perform logical reasoning from natural language, mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LINC: A neurosymbolic approach for logical reasoning by combining language models with firstorder logic provers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro‑symbolic approach leveraging LLMs to produce formal logic representations and using first‑order provers for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Neuro‑symbolic (LLM + FOL prover)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Translation to FOL and use of provers; in contrast to LLM‑ARC, LINC lacks a self‑refinement loop driven by solver feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses first‑order logic provers to check entailments from LLM-generated formalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NL logical reasoning benchmarks (e.g., similar datasets to FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>NL→FOL translation and theorem proving tasks; exact benchmarks not specified in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL→FOL translation and automated theorem proving / entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representative of prior neurosymbolic efforts combining LLMs with provers; mentioned to contrast with LLM‑ARC's use of semantic tests and Critic feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Per this paper, LINC does not perform solver‑driven self‑refinement or generate semantic tests validated by the reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7049.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7049.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clingo (ASP Solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Answer Set Programming (ASP) solver used as the Automated Reasoner (Critic) in LLM‑ARC to compile and execute generated ASP programs, run semantic tests, and produce error messages and proof‑by‑refutation explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clingo = asp + control: Preliminary report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Clingo (ASP solver)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High‑performance Answer Set Programming solver that compiles and computes stable models (answer sets), returns compilation/runtime error messages, and can be instrumented to compute minimal conflicting rule sets for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Answer Set Programming (logic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Declarative logical reasoning via ASP stable model computation; explanation extraction implemented via proof‑by‑refutation (adding negation of query and finding minimal rule sets causing contradiction).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Used as the Critic to run generated ASP code and tests; provides compilation diagnostics and minimal-rule explanations for failed tests or entailments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO (used as the execution engine for generated ASP programs)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LLM‑ARC entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Automated logical reasoning over generated ASP programs; test evaluation and explanation generation for entailment queries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Clingo's diagnostic messages and ability to find minimal conflicting rule sets enable feedback that an LLM can use to iteratively correct declarative code; integral to LLM‑ARC's performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ASP formalism used by Clingo has limited native support for existential quantification and certain modeling patterns (e.g., punning types vs instances), which constrains what the Actor can straightforwardly encode; explanations produced in this work were not fully grounded (no instantiated facts) and could be improved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with firstorder logic provers. <em>(Rating: 2)</em></li>
                <li>Folio: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Clingo = asp + control: Preliminary report <em>(Rating: 1)</em></li>
                <li>Advancements in xasp, an XAI system for answer set programming. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7049",
    "paper_id": "paper-3ca5822045093a18dd01d22f868f4a5da28bb4f3",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LLM-ARC",
            "name_full": "LLM-ARC (Large Language Model - Automated Reasoning Critic)",
            "brief_description": "A neuro-symbolic Actor-Critic framework that uses an LLM Actor to generate declarative logic programs (ASP) and semantic tests, and an Automated Reasoner (Clingo ASP solver) as a Critic to run tests, produce explanations, and provide feedback for iterative self-correction; trained in a self-supervised loop on dialog traces with Critic feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM-ARC",
            "model_description": "Neuro-symbolic system combining an LLM program-writer (Actor) with an automated ASP reasoner (Critic). The Actor synthesizes ASP code and logic tests; the Critic executes ASP, checks tests, generates proof-based explanations, and returns feedback for iterative refinement.",
            "model_size": null,
            "architecture_type": "Neuro‑symbolic (Transformer LLM Actor + ASP solver Critic, Actor‑Critic loop)",
            "training_data": "Few‑shot in‑context examples stratified by logical class (8 or 20 examples) for initial Actor; self‑supervised fine‑tuning data: 918 end‑to‑end dialog traces from the Actor‑Critic self‑correction loop collected on FOLIO training set.",
            "reasoning_method": "Program synthesis to ASP + explicit logic test generation + iterative self‑correction Actor‑Critic loop; query interpretation grammar and proof‑by‑refutation explanation extraction from the ASP solver.",
            "external_tool_used": true,
            "external_tool_description": "Clingo ASP solver used as the Critic: compiles and executes generated ASP programs, evaluates semantic tests (infer-True-All/Any, infer-False, expect-Contradiction), and provides compilation errors and minimal-rule explanations via a proof‑by‑refutation algorithm.",
            "benchmark_name": "FOLIO (v2)",
            "benchmark_description": "Human‑annotated natural language reasoning benchmark with premises and conclusions; tasks require determining whether a conclusion is True, False, or Uncertain relative to premises; includes FOL translations for items.",
            "task_type": "Natural language→formal program translation and logical entailment (True/False/Uncertain) under first‑order style problems (encoded in ASP); NL→ASP program synthesis, semantic test generation, and entailment checking.",
            "performance_metric": "Accuracy (prediction True/False/Uncertain)",
            "performance_value": "88.32% (LLM-ARC‑Trained on FOLIO v2)",
            "comparison_with_baseline": "LLM-ARC‑Trained achieves 88.32% vs prior known SOTA LogicLM 78.9% (≈+9.4 pp). Relative to best LLM-only (GPT4-FT-NL at 80.7%), LLM-ARC‑Trained improves ≈+7.62 pp. Ablations: adding TestGen to 8‑shot gave +6.6 pp; iterative self‑correction added ≈+5 pp; final trained Actor produced best result.",
            "key_findings": "Combining LLM program synthesis with an automated reasoner Critic and explicit semantic test generation substantially improves strict logical reasoning on FOLIO; TestGen and iterative self‑correction materially increase accuracy, and training the Actor on Critic feedback dialog traces yields the best performance.",
            "limitations": "Dependent on underlying formalism (ASP) limitations (poor support for existential quantification); sparse/rare logical classes (e.g., rules with multiple variables, type/instance punning) are harder; generated tests may not fully capture intended semantics or may pass for wrong reasons; Actor sometimes fails to modify code/tests across retries (self‑correction loop brittle); explanations lack full grounded rule instantiations; scaling to large inputs requires chunking strategies; no separate human‑trained Critic was used (potential improvement).",
            "uuid": "e7049.0",
            "source_info": {
                "paper_title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4-Turbo (gpt-4-1106-preview)",
            "name_full": "GPT-4 Turbo (gpt-4-1106-preview)",
            "brief_description": "A large transformer language model variant used as the Actor in LLM-ARC for few‑shot ASP code and test generation; found to produce high‑quality ASP translations in (zero-/few‑shot) settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT‑4‑Turbo (gpt-4-1106-preview)",
            "model_description": "Transformer‑based LLM variant (GPT‑4 family); used here as the Actor to translate NL premises to ASP and to generate logic tests and corrected code in an iterative loop.",
            "model_size": null,
            "architecture_type": "Transformer LLM (standard), used as program synthesizer in neuro‑symbolic pipeline",
            "training_data": null,
            "reasoning_method": "In‑context learning (few‑shot) to produce declarative ASP programs and explicit tests; also evaluated with Chain‑of‑Thought prompting for stepwise NL reasoning (baseline).",
            "external_tool_used": true,
            "external_tool_description": "When used in LLM‑ARC, GPT‑4‑Turbo outputs ASP code and tests which are executed by Clingo; some baselines used GPT‑4‑Turbo without the external reasoner.",
            "benchmark_name": "FOLIO (v2)",
            "benchmark_description": "See LLM‑ARC entry; NL→entailment benchmark for first‑order style reasoning.",
            "task_type": "NL→ASP program synthesis and entailment prediction; few‑shot and CoT baselines for True/False/Uncertain classification.",
            "performance_metric": "Accuracy",
            "performance_value": "Baselines reported: GPT4‑T‑ZS 67.0%; GPT4‑T‑CoT 74.1%. In LLM‑ARC experiments with GPT4‑Turbo Actor: LLM‑ARC‑8‑shot 74.62%, LLM‑ARC‑8‑shot‑TestGen 81.22%, LLM‑ARC‑20‑shot 83.25%, LLM‑ARC‑20‑shot‑TestGen 85.79%.",
            "comparison_with_baseline": "Chain‑of‑Thought (GPT4‑T‑CoT) improved zero‑shot GPT4‑T from 67.0% → 74.1%. Integrating GPT4‑Turbo as Actor with TestGen produced large gains (8‑shot: +6.6 pp with TestGen vs without). More in LLM‑ARC comparisons above.",
            "key_findings": "GPT‑4‑Turbo is effective at generating declarative ASP code from NL in few‑shot settings; explicit test generation and integration with a symbolic reasoner substantially boosts end task accuracy compared to LLM‑only outputs.",
            "limitations": "As an LLM, it can produce syntactic and semantic errors in generated code; requires external reasoner feedback to reach high accuracy; baseline CoT and fine‑tuning help but do not match full neuro‑symbolic training; model size and internal training corpora not reported.",
            "uuid": "e7049.1",
            "source_info": {
                "paper_title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 (fine-tuned Actor)",
            "name_full": "GPT-4 (fine‑tuned on Actor‑Critic dialog traces)",
            "brief_description": "GPT‑4 fine‑tuned in a self‑supervised manner on Actor‑Critic dialog traces (including Critic feedback and corrected ASP code/tests) to become the trained Actor for LLM‑ARC, producing the best system accuracy on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT‑4 (fine‑tuned)",
            "model_description": "Transformer LLM (GPT‑4) that was fine‑tuned on 918 end‑to‑end dialog traces from the LLM‑ARC self‑correction loop to improve its ability to write ASP code, tests, and perform rectifications.",
            "model_size": null,
            "architecture_type": "Transformer (fine‑tuned for program synthesis within neuro‑symbolic Actor‑Critic loop)",
            "training_data": "918 dialog traces derived from LLM‑ARC self‑correction on FOLIO training examples (types: compiler error rectifications, test‑failure rectifications, and direct passes).",
            "reasoning_method": "Supervised fine‑tuning to learn program synthesis, test generation, and correction strategies from Critic feedback; retains neuro‑symbolic operation with Clingo Critic.",
            "external_tool_used": true,
            "external_tool_description": "Trained Actor outputs ASP and tests which are executed and evaluated by Clingo; training data includes Critic's error messages/explanations.",
            "benchmark_name": "FOLIO (v2)",
            "benchmark_description": "See LLM‑ARC entry.",
            "task_type": "NL→ASP program synthesis and entailment classification with learned rectification behavior.",
            "performance_metric": "Accuracy",
            "performance_value": "88.32% (LLM‑ARC‑Trained using fine‑tuned GPT‑4 Actor)",
            "comparison_with_baseline": "Outperforms GPT4‑FT‑NL (80.7%) by ≈+7.62 pp and prior SOTA LogicLM (78.9%) by ≈+9.42 pp; demonstrates benefit of training Actor on Critic dialog traces vs direct fine‑tuning on NL or FOL.",
            "key_findings": "Fine‑tuning the Actor on Critic feedback dialog traces enables direct generation of high‑quality ASP code and tests, increasing final entailment accuracy substantially compared to few‑shot or LLM‑only fine‑tuning baselines.",
            "limitations": "Quality depends on the dialog trace curation (they only retained the last rectification steps to fit context windows); limited context window (8k) constrained trace length; potential overfitting to patterns in FOLIO; remaining failure modes stem from ASP formalism limits and sparse logical classes.",
            "uuid": "e7049.2",
            "source_info": {
                "paper_title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LogicLM",
            "name_full": "LogicLM",
            "brief_description": "A prior neurosymbolic system that combines LLMs with symbolic solvers for faithful logical reasoning; reported prior state‑of‑the‑art on (an earlier version of) FOLIO.",
            "citation_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning.",
            "mention_or_use": "mention",
            "model_name": "LogicLM",
            "model_description": "Neuro‑symbolic approach integrating an LLM with formal logic solvers (various solvers depending on underlying logic) to improve faithful logical reasoning.",
            "model_size": null,
            "architecture_type": "Neuro‑symbolic (LLM + formal solver)",
            "training_data": null,
            "reasoning_method": "LLM to formal representation + symbolic solver for reasoning; includes self‑refinement loop for syntax errors (but not semantic test generation as in LLM‑ARC).",
            "external_tool_used": true,
            "external_tool_description": "Uses symbolic solvers tailored to the target logic to check and perform reasoning over the formal representations generated by the LLM.",
            "benchmark_name": "FOLIO (prior reported results)",
            "benchmark_description": "Natural language reasoning benchmark (see FOLIO entry); LogicLM reported 78.9% as prior SOTA (noted in this paper).",
            "task_type": "NL→formal representation + reasoning (entailment)",
            "performance_metric": "Accuracy",
            "performance_value": "78.9% (reported prior SOTA on FOLIO v1 / cited prior result)",
            "comparison_with_baseline": "Reported as prior SOTA before LLM‑ARC; LLM‑ARC‑Trained surpasses it by ~9.4 pp.",
            "key_findings": "Incorporating symbolic solvers with LLMs improves logical reasoning relative to LLM‑only systems; however, LogicLM did not include semantic test generation validated by the solver.",
            "limitations": "Per this paper, LogicLM's self‑refinement focuses on syntax errors only; lacks the semantic test generation and reasoner‑driven test feedback loop introduced in LLM‑ARC.",
            "uuid": "e7049.3",
            "source_info": {
                "paper_title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LINC",
            "name_full": "LINC",
            "brief_description": "A neurosymbolic system that combines LLMs with first‑order logic provers to perform logical reasoning from natural language, mentioned as related work.",
            "citation_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with firstorder logic provers.",
            "mention_or_use": "mention",
            "model_name": "LINC",
            "model_description": "Neuro‑symbolic approach leveraging LLMs to produce formal logic representations and using first‑order provers for reasoning.",
            "model_size": null,
            "architecture_type": "Neuro‑symbolic (LLM + FOL prover)",
            "training_data": null,
            "reasoning_method": "Translation to FOL and use of provers; in contrast to LLM‑ARC, LINC lacks a self‑refinement loop driven by solver feedback.",
            "external_tool_used": true,
            "external_tool_description": "Uses first‑order logic provers to check entailments from LLM-generated formalizations.",
            "benchmark_name": "NL logical reasoning benchmarks (e.g., similar datasets to FOLIO)",
            "benchmark_description": "NL→FOL translation and theorem proving tasks; exact benchmarks not specified in this paper's discussion.",
            "task_type": "NL→FOL translation and automated theorem proving / entailment",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "",
            "key_findings": "Representative of prior neurosymbolic efforts combining LLMs with provers; mentioned to contrast with LLM‑ARC's use of semantic tests and Critic feedback.",
            "limitations": "Per this paper, LINC does not perform solver‑driven self‑refinement or generate semantic tests validated by the reasoner.",
            "uuid": "e7049.4",
            "source_info": {
                "paper_title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Clingo",
            "name_full": "Clingo (ASP Solver)",
            "brief_description": "Answer Set Programming (ASP) solver used as the Automated Reasoner (Critic) in LLM‑ARC to compile and execute generated ASP programs, run semantic tests, and produce error messages and proof‑by‑refutation explanations.",
            "citation_title": "Clingo = asp + control: Preliminary report",
            "mention_or_use": "use",
            "model_name": "Clingo (ASP solver)",
            "model_description": "High‑performance Answer Set Programming solver that compiles and computes stable models (answer sets), returns compilation/runtime error messages, and can be instrumented to compute minimal conflicting rule sets for explanations.",
            "model_size": null,
            "architecture_type": "Answer Set Programming (logic solver)",
            "training_data": null,
            "reasoning_method": "Declarative logical reasoning via ASP stable model computation; explanation extraction implemented via proof‑by‑refutation (adding negation of query and finding minimal rule sets causing contradiction).",
            "external_tool_used": true,
            "external_tool_description": "Used as the Critic to run generated ASP code and tests; provides compilation diagnostics and minimal-rule explanations for failed tests or entailments.",
            "benchmark_name": "FOLIO (used as the execution engine for generated ASP programs)",
            "benchmark_description": "See LLM‑ARC entry.",
            "task_type": "Automated logical reasoning over generated ASP programs; test evaluation and explanation generation for entailment queries.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "",
            "key_findings": "Clingo's diagnostic messages and ability to find minimal conflicting rule sets enable feedback that an LLM can use to iteratively correct declarative code; integral to LLM‑ARC's performance gains.",
            "limitations": "ASP formalism used by Clingo has limited native support for existential quantification and certain modeling patterns (e.g., punning types vs instances), which constrains what the Actor can straightforwardly encode; explanations produced in this work were not fully grounded (no instantiated facts) and could be improved.",
            "uuid": "e7049.5",
            "source_info": {
                "paper_title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning.",
            "rating": 2
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with firstorder logic provers.",
            "rating": 2
        },
        {
            "paper_title": "Folio: Natural language reasoning with first-order logic",
            "rating": 2
        },
        {
            "paper_title": "Clingo = asp + control: Preliminary report",
            "rating": 1
        },
        {
            "paper_title": "Advancements in xasp, an XAI system for answer set programming.",
            "rating": 1
        }
    ],
    "cost": 0.01632625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic</h1>
<p>Aditya Kalyanpur, Kailash Karthik Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci<br>Elemental Cognition Inc.</p>
<h4>Abstract</h4>
<p>We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the logical reasoning capabilities of Large Language Models (LLMs), by combining them with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic method where the LLM Actor generates declarative logic programs along with tests for semantic correctness, while the Automated Reasoning Critic evaluates the code, runs the tests and provides feedback on test failures for iterative refinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a new state-of-the-art accuracy of $88.32 \%$ on the FOLIO benchmark which tests complex logical reasoning capabilities. Our experiments demonstrate significant improvements over LLM-only baselines, highlighting the importance of logic test generation and iterative self-refinement. We achieve our best result using a fully automated self-supervised training loop where the Actor is trained on end-to-end dialog traces with Critic feedback. We discuss potential enhancements and provide a detailed error analysis, showcasing the robustness and efficacy of LLM-ARC for complex natural language reasoning tasks.</p>
<h2>1 Introduction</h2>
<p>Given their impressive language understanding capability, Large Language Models (LLMs) are being used to develop a wide variety of Natural Language applications. For certain classes of applications that require a high degree of accuracy and reliability (e.g., enterprise applications in the medical, legal or finance domain), LLMs are often combined with external tools and solvers in a hybrid architecture [7, 17, 11]. We believe this is the right approach, especially to tackle problems where precise logical reasoning, planning or constraint optimization is required, as LLMs are known to struggle for this class of problems $[19,18,10,4]$.</p>
<p>In this work, we focus on logical reasoning problems expressed in natural language, for which there has been a growing interest in developing neuro-symbolic architectures [14, 15]. These architectures combine the power of LLMs for generating (declarative) code and filling in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>missing background (commonsense) knowledge, with the accuracy of automated Symbolic Reasoning systems to do precise logical reasoning.</p>
<p>This design addresses the limitations of either technology when used independently: the LLMs' inability to do accurate and consistent reasoning based on the underlying domain logic, and the Symbolic Reasoner's inability to work with unstructured data, and explicitly encode common-sense knowledge to get the desired inferences. The former issue in symbolic systems is the well-known "knowledge acquisition" problem, while the latter issue typically leads to their brittleness.</p>
<p>Building on our previous neuro-symbolic work [9, 4], we develop a new framework based on the Actor-Critic [3] model, where the Actor generates declarative code, crucially with tests to verify the semantic correctness of the code (i.e. the logic program correctly captures the mod-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>eler's intent), and the Critic runs the code and tests, and gives feedback with detailed explanations to the Actor if the code does not compile or some tests fail. When this happens, the Actor regenerates the code/tests based on the feedback, and the process is repeated with the Critic evaluating the results until all tests pass, or we reach a max-iteration limit.</p>
<p>We use an LLM as the Actor and an Automated Reasoning engine as the Critic, and refer to this neuro-symbolic system as LLMARC. Figure 1 shows an implementation of LLM-ARC based on Answer Set Programming (ASP)[12]. In general, this design can apply to any LLM-code execution engine (replacing the automated reasoner with the corresponding code compiler/interpreter), though here we focus on declarative problem solving.</p>
<p>Note that the system as designed above is not guaranteed to produce perfectly accurate results. This is because even if the code compiles without issues and all the generated tests pass, there is no guarantee that the test conditions correctly and completely capture the intended semantics, or that the tests pass for the right reason (e.g. the system could derive a required inference for a test using an incorrectly intended logical proof). We discuss this issue in Section 5.1 and suggest a future enhancement using a separate Critic trained via human-feedback to evaluate the test criteria and reasoner results.</p>
<p>To evaluate our LLM-ARC system, we run experiments on the FOLIO benchmark [8]. FOLIO is a human-annotated, logically complex and diverse dataset for reasoning in natural language. We use the latest version of FOLIO (v2) which contains 1001 training examples and 203 validation examples. The current state-of-the-art results on FOLIO is $\mathbf{7 8 . 9 \%}$ achieved by LogicLM [15]. Using our LLM-ARC system we achieve a new state-of-the-art accuracy of $88.32 \%$.</p>
<p>We compare several strong LLM-only baselines (using GPT4-Turbo as the LLM) with various versions of the LLM-ARC system on the FOLIO data, and show that the Actor-Critic approach even in a few-shot setting (only 8 examples for the Actor) outperforms a fine-tuned</p>
<p>LLM solution trained on all 1 K examples.
We demonstrate that adding the test generation option to the Actor improves performance by $\mathbf{6 . 6 \%}$ (compared to a version without testgen); that running code and test generation in a self-correction loop with the Critic (where the Actor corrects mistakes based on the Critic feedback) further boosts performance by $\mathbf{5 \%}$; and the best performing system is one where the Actor is trained on end-to-end self-correction dialog traces with Critic feedback (from the automated reasoner) on the training set.</p>
<p>The contributions of this work are as follows:</p>
<ul>
<li>We believe this is the first work to fold in test generation for declarative logic programs to improve code quality, and combine an LLM Actor for code-generation with a Reasoning Engine Critic for test evaluation and explanation, boosting overall system performance. (We refer to this hybrid architecture as LLM-ARC)</li>
<li>We specify guidelines for test-generation based on a logical analysis of the problem domain, and use a simple general schema for writing logic tests. We demonstrate the value-add of test generation, and the specific guidelines, via ablation experiments. All relevant LLM prompts are included in the Appendix.</li>
<li>In the presence of final ground truth labels for reasoning problems, we describe a fully automated procedure to train the Actor model (to write and rectify declarative code and tests) over end-to-end dialog traces of a self-correction loop using a reasoning engine Critic (to provide finegrained explanatory feedback). This selfsupervised version of the LLM-ARC system achieves a new SOTA of $88.32 \%$ on the FOLIO benchmark.</li>
</ul>
<h2>2 Related Work</h2>
<p>Given the remarkable performance of LLMs on automated code-generation, a large number of AI-driven "co-pilot" tools and frameworks are being actively developed. There is also a growing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LLM-ARC Implementation based on Answer Set Programming (ASP): Given a problem description (a collection of natural language statements), the Actor (LLM) generates ASP code and tests in an iterative manner. At each step, the Actor takes as input the next segment (problem intents) to convert to ASP, along with the existing ASP code and tests generated so far, and outputs the updated code and tests based on the latest segment. The code is then run by the Critic (ASP Solver) and any test failures with explanations are fed back to the Actor. This self-correction loop runs till all tests-pass or max-iterations are reached. The Actor is eventually trained on end-to-end dialog traces with the Critic feedback in this self-correction loop.</p>
<p>interest in automatically generating test cases (both, unit tests and more complex integration tests) to validate code correctness [16, 1, 13]. However, to our knowledge, all the efforts have been focused on generating and testing procedural code. Our area of interest is symbolic code (logic programs) where tests are crucial to verify that the rules and constraints accurately capture the modelers intent. This is particularly useful for developers working with declarative systems who are not proficient in formal logic, due to the long-distance interdependencies across rules, vagaries of logic involving negations and contrapositives, and the need to explicitly encode commonsense knowledge. Moreover, we believe that test failures can be effectively leveraged to improve declarative code accuracy, due to the unique capability of a symbolic reasoning engine to provide detailed logical explanations (proofs) for the failures, a claim validated by our LLM-ARC system results.</p>
<p>More closely aligned to our work is neurosymbolic systems such as LINC [14] and LogicLM [15]. These systems combine an LLM with a formal reasoning engine (in LogicLM's case, a variety of solvers based on the underlying logic) and show impressive results on a several NL-reasoning benchmarks. However, neither system has the notion of generating semantic tests that need to be validated by the reasoner. LogicLM does have a self-refinement loop but it is only used for syntax errors in the generated logical representation, while LINC has no self-refinement or feedback from the solver.</p>
<p>Additionally, the idea of training the logic program writer (Actor) over end-to-end interactions by incorporating feedback from a formal reasoning engine (Critic) is fundamentally novel.</p>
<p>to our work. Much of the "agent based" or "ReAct" systems that integrate tools with LLMs suffer from orchestration and control inefficiencies where individually efficient tools are combined in a sub-optimal and brittle whole. We focus on integrated training that ensures the overall system is optimized.</p>
<h2>3 Approach: Neuro-symbolic Actor-Critic Model</h2>
<p>As mentioned earlier, our LLM-ARC system is based on the Actor-Critic model, where we use an LLM as the Actor to generate declarative code with tests, and an Automated Reasoner as the Critic to execute the logic program, run the tests and provide detailed feedback with explanations to the Actor when there are test failures.</p>
<p>The system needs to be based on a logical formalism, and to tackle FOLIO, we chose Answer Set Programming (ASP) as the underlying logic. ASP was selected because we found that it works best for developing enterprise applications [4] and it has sufficient logical expressivity needed for most of the FOLIO problems.</p>
<p>Figure 1 shows our LLM-ARC implementation based on ASP. We now describe details of the Actor and Critic.</p>
<h3>3.1 Actor: LLM Logic Program Writer</h3>
<p>For the LLM Actor, we chose GPT4-Turbo (gpt-4-1106-preview) since we found that it generated ASP code of reasonably high quality from NL instructions, even in a zero-shot setting.</p>
<p>We use GPT4-Turbo in a few shot setting by specifying a handful of examples of translating FOLIO problems into ASP. To come up with the exemplar set, we did an automated analysis of the logical structure and expressivity of the NL statements in FOLIO.</p>
<h3>3.1.1 Logic Stratification of FOLIO statements</h3>
<p>The idea is to use a powerful LLM (such as GPT4-Turbo) to automatically classify NL
statements based on their logical structure, connectives/operators used, and overall composition (e.g. do they contain nested clauses). We came up with a general prompt (see Appendix) for logic stratification that applies to most formal logics (not just ASP) and ran it on a large random sample of FOLIO statements. We manually vetted the results and found that the logically stratified clusters (including their examples) found by the LLM were of very high quality overall. We acknowledge that this task may be easy for the FOLIO dataset where statements are written in a logic-heavy manner by design.</p>
<p>The net result of logic stratification over FOLIO data is shown in Figure 2. The LLM found 8 logical classes of FOLIO statements.</p>
<p>We added one more category for cases where background knowledge (often common-sense relationships connecting predicates in the program) was missing in the input problem description.</p>
<p>For example, consider when the Premises state:</p>
<p>All employees who schedule a meeting with their customers will go to the company building today. Everyone who has lunch in the company building schedules meetings with their customers. No managers work remotely from home.</p>
<p>In this case, the following common-sense rules are not explicitly mentioned in the premises:</p>
<ul>
<li>Managers are employees.</li>
<li>All employees who have lunch in the company building are in the company building.
Information like this is often missing in the input because it is considered obvious, a problem noted by [14] as well. Here, we leverage the LLM's ability to fill in common-sense knowledge gaps, though we need to be careful about the LLM adding extraneous knowledge that confounds the modelers intent, and we address this using a combination of prompt-engineering and using tests to validate the semantics.</li>
</ul>
<p>For the few shot setting, we added 8 examples to cover all the 8 main logic classes, adding one example per class (see Appendix). Several examples include common-sense relationships with</p>
<table>
<thead>
<tr>
<th>Logical Expressivity Class</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Regular conditionals with disjunctions in the body or consequents in the head (which can be easily split)</td>
<td>All citizens of Lawton Park use the zip code 98199. If someone has been scratched or bitten by some entity, they have been attacked by that entity. If a person enjoys vacation, they neither stay home often nor resist change</td>
</tr>
<tr>
<td>Complex conditionals with nested clauses</td>
<td>Either Jack does hire a maid or cleaning service and does not often clean his home, or he does not hire a maid or cleaning service nor often clean his home. <br> People who work at Jess's company are either miserly and need to save a large portion of their income, or frivolously spend a lot of money. <br> James is a student in the class; he is either good at chemistry and failed the class, or bad at chemistry and passed the class.</td>
</tr>
<tr>
<td>Regular disjunction vs. Exclusive OR</td>
<td>All social media applications have chat features or video features. <br> If Jumbo is sleepy, then Jumbo is either a baby elephant or a mammal.</td>
</tr>
<tr>
<td>Negation; disjointness among concepts</td>
<td>No fish are plants. <br> No reptiles have fur.</td>
</tr>
<tr>
<td>Exclusion Constraints</td>
<td>Oliver plays a different musical instrument from Peter in the concert. <br> A Lamborghini SUV is not both a Ferrari and made in Maranello. <br> If Jumbo is a living being, then Jumbo is not both an elephant and a mammal.</td>
</tr>
<tr>
<td>Existential Quantification</td>
<td>Some people are citizens. <br> Some juvenile delinquents are products of broken homes.</td>
</tr>
<tr>
<td>Equality among individuals</td>
<td>The Emmet Building is another name for the Blake McFall Company Building.</td>
</tr>
<tr>
<td>Rules with Multiple Variables</td>
<td>Oliver plays a different musical instrument from Peter in the concert. <br> All animals within a genus are related to each other.</td>
</tr>
</tbody>
</table>
<p>Figure 2: Logic Stratification of NL Statements in FOLIO
instructions on how and when to add them. Note that since each example is a multi-line problem, a single example might cover more than one logic category. See an example in Figure 3.</p>
<h3>3.1.2 Logic Test Generation</h3>
<p>We designed a simple general schema for specifying logic tests. Each test has optional facts that need to be added to the program to test the rules/constraints, and the test conditions are either one of the following:</p>
<ul>
<li>infer-True-All: a set of propositions that must be inferred by the solver in all solution sets of the logic program</li>
<li>infer-True-Any: a set of propositions that must be inferred by the solver in at least one solution set of the logic program</li>
<li>infer-False: a set of propositions that must not be inferred by the solver in any solution set of the logic program</li>
<li>expect-Contradiction: a boolean flag which represents whether we expect the program to be contradictory (unsatisfiable)
when the facts are added
To improve test generation quality, we asked the LLM to add two additional fields for each test: rules-referenced - which points to specific rules in the program (all rules have an ID in the program; see the example in Figure 3) that are exercised in the test; and test explanation - a rationale for the test describing how it validates the semantics of the referenced rules.</li>
</ul>
<p>We then specified guidelines for writing tests in the prompt, based on different logical conditions in the input. Similar to the in-context examples chosen for ASP code generation, we mirrored the guidelines on the logic strata found in FOLIO statements, to ensure adequate coverage of the logical semantics. Examples of the guidelines are shown in Figure 4, with the full prompt attached in the Appendix.</p>
<h3>3.1.3 Error Correction</h3>
<p>Finally, we include instructions in the Writer prompt for correcting errors reported by the Critic. There are two kinds of errors: syn-</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Input</span> <span class="nv">Premises</span><span class="o">:</span>
<span class="nv">All</span> mammals are living beings.
<span class="nv">All</span> elephants are mammals.
<span class="nv">All</span> baby elephants are elephants.
<span class="nv">Some</span> baby elephants are sleepy.
<span class="nv">If</span> <span class="nv">Jumbo</span> <span class="k">is</span> a living being, then <span class="nv">Jumbo</span> <span class="k">is</span> not both an elephant and a mammal.
<span class="nv">If</span> <span class="nv">Jumbo</span> <span class="k">is</span> sleepy, then <span class="nv">Jumbo</span> <span class="k">is</span> either a baby elephant or a mammal.
<span class="nv">Output</span> <span class="nv">ASP</span> <span class="nv">Code</span><span class="o">:</span>
...
<span class="c">% R1: All mammals are living beings.</span>
living_being(<span class="nv">X</span>) <span class="o">:-</span> mammal(<span class="nv">X</span>).
<span class="c">% R2: All elephants are mammals.</span>
mammal(<span class="nv">X</span>) <span class="o">:-</span> elephant(<span class="nv">X</span>).
<span class="c">% R3: All baby elephants are elephants.</span>
elephant(<span class="nv">X</span>) <span class="o">:-</span> baby_elephant(<span class="nv">X</span>).
<span class="c">% R4: Some baby elephants are sleepy.</span>
<span class="k">{</span>sleepy(<span class="nv">X</span>)<span class="k">}</span> <span class="o">:-</span> baby_elephant(<span class="nv">X</span>).
<span class="c">% R5: If Jumbo is a living being, then Jumbo is not both an elephant and a mammal.</span>
<span class="p">:- </span>living_being(jumbo), elephant(jumbo), mammal(jumbo).
<span class="c">% R6: If Jumbo is sleepy, then Jumbo is either a baby elephant or a mammal.</span>
<span class="m">1</span> <span class="k">{</span>baby_elephant(jumbo)<span class="o">;</span> mammal(jumbo)<span class="k">}</span> <span class="m">1</span> <span class="o">:-</span> sleepy(jumbo).
</code></pre></div>

<p>Figure 3: Training Example for NL to ASP used in the prompt (In-Context Learning)
tax/compilation errors, and semantic errors when there are test failures. The prompt contains strategies to resolve both classes of errors, and utilizes 1 example each. Furthermore, we specify a detailed pseudo-code for fixing the semantic errors (failing tests) since this is the more challenging case. The instructions walk through how the explanation from the Critic can be used to identify whether the test inputs, validation criteria, or specific parts of the ASP program (e.g., the commonsense knowledge section) need to be altered.</p>
<h3>3.2 Critic: Logical Reasoner</h3>
<p>We use the Clingo ASP Solver [6] as the Critic since it is highly performant and freely available under the MIT License. Clingo also has useful compilation error messages, which point to specific lines in the program with errors. This information is fed back to the Actor in the selfcorrection loop.</p>
<h3>3.2.1 Query Evaluation</h3>
<p>We came up with a simple logical grammar to interpret the Conclusion statements in the FOLIO problem as structured queries, which could then be evaluated more accurately using the Solver.</p>
<p>For example, consider the following Conclusion: "If the Red Star is a supernova or observed
for its brightness, then the Red Star is neither a planet nor is its orbit stable."</p>
<p>This is the corresponding target interpretation:</p>
<ol>
<li>ATOM(supernova(red_star))</li>
<li>ATOM(observed_for_brightness(red_star))</li>
<li>OR $(1,2)$</li>
<li>ATOM(-planet(red_star))</li>
<li>ATOM(-orbit_stable(red_star))</li>
<li>AND $(4,5)$</li>
<li>IF-THEN $(3,6)$</li>
</ol>
<p>As shown, we use standard logical operators such as And, Or, Not, XOR, IF-THEN (for implications) and use ATOM to denote the base atomic propositions. Any logical structure can be composed bottom-up in a modular manner. The advantage of representing queries (conclusions) using this schema is more flexibility in query evaluation, especially when dealing with the particularities of the ASP formalism (e.g. ASP does not have clean support for existential quantification).</p>
<h3>3.2.2 Explanation Generation</h3>
<p>A feature that we added to the Solver is its ability to generate explanations for query entailments. There has been some work in this area [2] though we developed our own simple algorithm based on proof-by-refutation. The idea is to check query entailment by adding the nega-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># Testing Simple Conditionals</th>
<th style="text-align: center;"># Testing Complex Disjunctions</th>
<th style="text-align: center;"># Testing Rules with Multiple Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input: All vehicle registration plates in Istanbul begin with the number 34</td>
<td style="text-align: center;">Input: Rina is either a student who is unaware that caffeine is a drug, or she is not a student and is she aware that caffeine is a drug.</td>
<td style="text-align: center;">Input: All files within a directory are stored on the same server.</td>
</tr>
<tr>
<td style="text-align: center;">Output Tests: <br> - Assert a vehicle registration plate in Istanbul "p" whose number does not begin with 34. This should result in a contradiction.</td>
<td style="text-align: center;">Output Tests: <br> - Assert that Rina is unaware that caffeine is a drug and Rina is not a student. This should result in a contradiction. <br> - Infer the following propositions must be true in at least one solution: <br> * Rina is a student <br> * Rina is aware that caffeine is a drug</td>
<td style="text-align: center;">Output Tests: <br> - Assert two files "f1" and "f2". Assert "f1" and "f2" are within a directory. Assert "f1" is stored on server "s1" and "f2" is stored on server "s2", and s1 $\mathrm{f}+$ s2. This should result in a contradiction.</td>
</tr>
<tr>
<td style="text-align: center;">Output Tests: <br> - Assert viewers send a funny video " $v$ " to TF1. Infer that Video Gag airs " $v$ " weekly.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Testing Same-as / Equality among individuals</td>
<td style="text-align: center;"># Testing Negation</td>
<td style="text-align: center;"># Testing Bidirectional Inferences</td>
</tr>
<tr>
<td style="text-align: center;">Input: The Emmet Building was built in 1915. The Emmet Building is another name for the Blake McFall Company Building.</td>
<td style="text-align: center;">Input: No baked sweets are spicy <br> Output Tests: <br> - Add a baked sweet "b". Infer that "b" is not spicy.</td>
<td style="text-align: center;">Input: Eiffel Tower and Louvre Museum are landmarks in the same city in France.</td>
</tr>
<tr>
<td style="text-align: center;">Output Tests: <br> - Check that Blake McFall Company Building was built in 1915.</td>
<td style="text-align: center;">Output Tests: <br> - Add a baked sweet "b". Infer that "b" is not spicy. <br> Input: No fruits that grow in Ben's yard and are healthy are on a warning list. <br> Output Tests: <br> - Assert fruit "f" that grows in Ben's yard and is healthy and is on a warning list. This should produce a contradiction</td>
<td style="text-align: center;">Output Tests: <br> - Assert that Paris is a city in France and that Eiffel Tower is in Paris. Infer that Louvre Museum is in Paris. <br> - Assert that Paris is a city in France and that Louvre Museum is in Paris. Infer that Eiffel Tower is in Paris.</td>
</tr>
</tbody>
</table>
<p>Figure 4: Test Guidelines for the various logic classes with examples
tion of the query to the program and checking for a contradiction. If a contradiction is found, we can infer that the query is entailed. In this case, we can find an explanation for the entailment by obtaining the minimal set of rules in the ASP program that result in the contradiction. This is a popular technique for explanation generation used in FOL and description logic systems [5].</p>
<h2>4 Experiments on FOLIO</h2>
<p>We conducted various experiments using the FOLIO benchmark, which consists of 1001 training examples and 204 validation examples.</p>
<p>Each FOLIO problem consists of a set of Premises (NL statements) and a Conclusion (also a NL statement). The task is to determine whether the Conclusion is True, False or Uncertain given the Premises. The FOLIO dataset also includes First-Order-Logic (FOL) translations for each of the Premises and the Conclusion.</p>
<p>We evaluated FOLIO use the following systems (the first four systems below are LLM-only baseline systems)</p>
<ol>
<li>GPT-3.5-ZS and GPT4-T-ZS: Zero-
shot versions of GPT-3.5 and GPT4-Turbo</li>
<li>GPT4-T-CoT: GPT4-Turbo with a Chain-of-Thought prompt where we instruct the model to label the premises, and then carefully evaluate the conclusion using step-wise reasoning and referencing the premises along the way.</li>
<li>GPT4-FT-NL: GPT4 fine-tuned on the NL problem descriptions in the entire FOLIO training data of 1001 examples</li>
<li>GPT4-FT-FOL: GPT4 fine-tuned to go from NL problem description to the corresponding First Order Logic (FOL) versions (annotated in the FOLIO training data), and then to the prediction. The idea is to check whether using the precise FOL translations as an intermediate step helps the model produce more accurate results.</li>
<li>LLM-ARC-8-shot: The LLM-ARC system with 8 in-context learning examples, where the LLM Actor only does code generation (no Tests). The LLM used was GPT4-Turbo</li>
<li>
<p>LLM-ARC-8-shot-TestGen: The above system with the enhancement that the Actor also generates Tests for the code</p>
</li>
<li>
<p>LLM-ARC-20-shot: The LLM-ARC system (again using GPT4-Turbo as the Actor) with 20 in-context learning examples, and no test generation. We added another 14 examples to cover the 8 logic classes described in Section 3.1.1.</p>
</li>
<li>LLM-ARC-20-shot-TestGen: The above system with the enhancement that the Actor also generates Tests for the code</li>
<li>LLM-ARC-Trained: Trained version of the LLM Actor (GPT4, not Turbo) on end-to-end dialog traces with the Reasoner Critic, in a self-correction loop over the entire training data. The actor is trained to generate both ASP Code and Tests. Details of how this was done are provided in the next subsection.
All the LLM-ARC systems are run in a selfcorrection loop with upto 4 iterations.</li>
</ol>
<h3>4.1 Training the Actor with Critic Feedback Dialog-Traces</h3>
<p>We ran the un-trained 8 -shot version of the LLMARC system (with the TestGen capability) on the entire training set and collected dialog trace data on the correctly predicted examples. We used this dialog data to fine-tune a separate Actor model based on GPT4 ${ }^{1}$. Since the context window of GPT4 is only 8 K tokens, we had to limit the dialog traces to fit it into the window. We achieved this by using only the last rectification step of the trace - e.g. if there was a compiler error reported by the Critic that was fixed by the Actor in the next iteration, we would train on a trace that starts with the prior incorrect version from the Actor, followed by the Critic feedback, and then the corrected version with the compilation issues fixed. The same applies to test failures, where we started the dialog trace with a version just prior to all the tests being passed, and included the intermediate Critic feedback before the corrected version.</p>
<p>Additionally, we included a two-step "shortcut" dialog trace that went from the input problem directly to the ASP code and tests, when all
the tests passed along with the correct ground truth prediction. The idea behind this is to enable the Actor to learn how to produce code and tests of high quality (that compile, pass tests and entail the query correctly) in a direct manner.</p>
<p>To summarize, the data used to fine-tune the GPT4 Actor had the following 3 kinds of dialog traces:</p>
<ol>
<li>NL description $\rightarrow$ ASP code with compilation issues $\rightarrow$ Critic Feedback on compiler errors $\rightarrow$ ASP code that compiles</li>
<li>NL description $\rightarrow$ ASP code that compiles with test failures $\rightarrow$ Critic feedback on failures with explanations $\rightarrow$ ASP code with all tests passing</li>
<li>NL description $\rightarrow$ ASP code that compiles with all tests passing
The traces were collected whenever the final system prediction on the ground truth label was correct. The total number of dialog traces (each trace corresponds to a single training instance) collected on the entire training set was 918 .</li>
</ol>
<p>Finally, to keep the LLM prompt in the finetuned training data as concise as possible, we did not include any examples in the trained Actor prompt.</p>
<h3>4.2 System Results</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT3.5-ZS</td>
<td style="text-align: left;">$66.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-T-ZS</td>
<td style="text-align: left;">$67 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-T-CoT</td>
<td style="text-align: left;">$74.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-FT-NL</td>
<td style="text-align: left;">$80.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-FT-FOL</td>
<td style="text-align: left;">$78.17 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LogicLM (Prior SOTA)</td>
<td style="text-align: left;">$78.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-ARC-8-shot</td>
<td style="text-align: left;">$74.62 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-ARC-8-shot-TestGen</td>
<td style="text-align: left;">$81.22 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-ARC-20-shot</td>
<td style="text-align: left;">$83.25 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-ARC-20-shot-TestGen</td>
<td style="text-align: left;">$85.79 \%$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-ARC-Trained</td>
<td style="text-align: left;">$\mathbf{8 8 . 3 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Overall Accuracy on FOLIO. All LLM-ARC systems were run in a self-correction loop with upto 4 iterations</p>
<p>The accuracy scores for all the systems are</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Impact of Iterative Self-Correction Over multiple iterations, we see overall LLM-ARC system accuracy go up, as more ASP programs fully compile and pass more tests. The chart on the left shows system accuracy over multiple iterations for the various LLM-ARC system variants. The two tables on the right show additional statistics around generated tests, code compilation and test passing for the LLM-8-shot and Trained version.
shown in Table, 1, along with the prior known SOTA ${ }^{2}$.</p>
<p>The best performing LLM-only baseline solution is GPT4-FT-NL which was trained on all the 1 K examples in the training data. Interestingly, using the FOL annotations as intermediate representations in a chain-of-thought variant did not help results, a point worth investigating in the future.</p>
<p>Regarding the LLM-ARC systems, we observe a clear benefit of adding Test Generation. Both the LLM-ARC few shot variants (i.e. 8 and 20 example variants) perform better with TestGen added, with the 8 -example variant seeing a huge boost of $+6.6 \%$. Of note, the LLM-ARC-8-shot-TestGen version outperforms the best LLM-only solution even though the latter was fine-tuned on the entire 1 K example training set.</p>
<p>Our best performing system is the LLM-ARC version that was trained in a self-supervised manner on end-to-end dialog traces with the Critic feedback, and achieved $88.32 \%$ accuracy, 10 points higher than the prior known SOTA.</p>
<h3>4.3 Ablation Studies</h3>
<p>We conducted a few ablation studies to measure the impact of features in the LLM-ARC system.</p>
<h3>4.3.1 Impact of Iterative Self-Correction</h3>
<p>To answer the question "How much do retries help?", we plot the accuracy curves for the LLMARC variants over multiple iterations of the selfcorrection loop. The results are shown in Figure 5. We see that over multiple iterations, the performance does go up by between $4-5 \%$ for the two LLM-ARC systems shown above, when comparing the results after zero and max-retries, as the code compilation issues and test failures are fixed by the Actor based on the Critic feedback (notice how the numbers in columns 3 and 6 in the Tables in Figure 5 go up over iterations along with the overall accuracy in column 2). However, the final accuracy asymptotes after two retries. More details on this are in the Error Analysis section.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.3.2 Impact of TestGen Guidelines</h3>
<p>To measure the impact of adding Test Generation guidelines (Figure 4), we ran an ablation by dropping this section from the prompt and letting the LLM determine how to generate tests on its own instead. The results for this ablation are shown in the table below for the two LLM-ARC few-shot systems, and indicate a big drop in performance, clearly demonstrating its value-add.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM-ARC-8-TestGen</td>
<td style="text-align: left;">$78.7 \%(-2.52 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-ARC-20-TestGen</td>
<td style="text-align: left;">$80.2 \%(-5.59 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Dropping Test-Gen Guidelines</p>
<h2>5 Error Analysis and Discussion</h2>
<p>We analyzed errors from the best performing system (LLM-ARC Trained) and found that they broadly fell in three categories (excluding minor cases of query interpretation failures and modeling mistakes like representing an XOR as a regular disjunction)</p>
<ol>
<li>Existential quantification: ASP does not have natural support for existential quantification. For example, it is not possible to accurately model the following statement: "One six-way tie was on the leaderboard, and one person in the sixway tie was from Belgium." that posits the existence of two unnamed individuals, which can potentially be unified with other named individuals in the program. This is certainly possible to do in other logics such as FOL (which the FOLIO dataset was annotated with) but is a limitation of our chosen formalism.</li>
<li>Rules with Multiple Variables: Among the various logic classes in FOLIO identified in Figure 2, the one class that the Actor had difficulty in modeling was rules with multiple variables. This</li>
</ol>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>includes statements like "All languages within a language family are related to each other.", where the rule involves two variables for distinct languages. We believe a reason for this is that there are very few examples of this class in the training set $(&lt;5 \%)$. A potential solution is to up-weight (or up-sample) examples from this class during training, or simply give more examples of this class in the few-shot prompt.
3. Conflating types and instances: These are cases where certain entities are linguistically used as both types and individuals in the input problem. For example, consider the example below from FOLIO where we have marked an entity that looks like both a class and an individual in different statements with a "*".</p>
<div class="codehilite"><pre><span></span><code>Plungers suck.
<span class="gs">*Vacuums*</span> suck.
Vampires suck.
Space is a <span class="gs">*vacuum*</span>.
A duster is a household
appliance that doesn&#39;t suck.
</code></pre></div>

<p>Similar to the previous error class, there are very few examples of this behavior in the training set. Moreover, these are particularly hard modeling problems from a logic standpoint in ASP, which does not support "punning" (as say the description logic OWL2 ${ }^{3}$ ), and hence requires additional machinery at modeling and query evaluation time to correctly interpret terms as either classes or individuals based on how they are used.
Given the high performance of the LLM-ARC-Trained system, it is unsurprising that the remaining headroom is for the challenging or sparse cases.</p>
<p>Finally, we looked into why the performance was asymptotic after a few retries of the selfcorrection loop. In roughly a third of the problem failure categories (i.e. final prediction was incorrect) and where all tests did not pass, we</p>
<p>found that the Actor made no alterations to the program code or the tests from one iteration to the next. This is a weakness exposed by our current design where we do not enforce that some alteration must be made by the Actor, and instead expect the LLM to follow the instructions in the prompt, which it clearly does not always do. We are considering an alternate design using function-calling with constraints where we can enforce that one or more of the program code, query interpretation and failing test criteria must be modified in the presence of a failing test. We also empirically observed that the Actor would rarely change the query interpretation across multiple iterations and found that this was a miss in our instructions which primarily focused on altering the program code and tests.</p>
<h3>5.1 Potential Enhancements</h3>
<p>There are several potential enhancements to the LLM-ARC implementation which we leave for future investigation:</p>
<p>Sophisticated Input Chunking: Since FOLIO problems are relatively small ( $&lt;10$ statements), we pass the entire problem to the LLM Actor in one shot, without doing any chunking. In the future, for real world applications that involve translating large volumes of business logic text into a formal program, the input would have to be chunked. The appropriate chunking level would depend on the quality of the code generated, and would have to be empirically determined based on the LLM's output quality given a certain context window size (and if the Actor is trained, the chunking size used in the training data).</p>
<p>Enhancing Critic Explanations: The current explanation generated from Clingo using our proof-by-refutation algorithm does not include grounded statements. A more informative explanation would come from grounding relevant rules that lead to the entailment, as described in work [2]. Moreover, we could use another LLM to translate the grounded rule-based proof back into natural language to produce a more fluent explanation, which should presumably be more
interpretable for the LLM Actor. This hypothesis needs to be empirically validated.</p>
<p>Training a separate Critic: As mentioned earlier, in the current design, there is no guarantee that the test conditions correctly and completely capture the intended semantics, or that the tests pass for the right reason. One way to mitigate this issue is to have a separate Critic that evaluates the reasoner's results and provides feedback on the test criteria and proof step correctness. Indeed, our original system design started off with a Critic distinct from the reasoner, which was to be trained with humanfeedback on the tests results and explanations provided by the reasoner (since those need to be manually assessed). We did not go down this path in the end, since we found that using the automated reasoner as the Critic directly, and training the Actor in a self-supervised training loop, produced a big boost in performance. We still believe training a separate critic has the potential to further increase accuracy and reliability of the entire system.</p>
<h2>6 Conclusion</h2>
<p>There is growing recognition in the AI community that LLM-only solutions do not meet the standard for production applications that require a high degree of accuracy, consistency and explicability. More specifically, current state-of-the-art LLMs are known to struggle for problems involving precise logical reasoning, planning and constraint solving. As a result, we have seen a rise in the development of Neuro-Symbolic systems, where the reasoning is offloaded to a symbolic solver, and the LLM is used at the interface layer to map between unstructured data (text) and structured logical representations. Unlike standard tools or simple APIs, integration between an LLM and a symbolic reasoner can be fairly sophisticated as the reasoning engine has its own world model and decision procedures (arguably, one might even conceive and design the system such that the reasoner is the brain of the system and the LLM is the tool for interpreting and translating data).</p>
<p>In such declarative systems, we firmly believe that tests are needed to check for semantic correctness of the logic program (a much harder challenge than ensuring syntactic correctness), and that the reasoner by way of providing detailed feedback on test failures to the program writer can help it improve in a self-correction loop. This intuition led us to the design the LLM-ARC system presented in this paper, which is based on the Actor-Critic model and uses the LLM as the Actor and an Automated Reasoning engine as the Critic. We empirically validate the system on the FOLIO benchmark, and show that not only can such a system achieve higher performance than an LLM-only solution in a few-shot setting, but that we can devise a fully automated self-supervised loop to train the Actor with Critic feedback to boost performance significantly. Lastly, the ability of this system to provide detailed logical explanations for its answers means that a human-in-the-loop can verify its results in production applications.</p>
<h2>References</h2>
<p>[1] Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. Automated unit test improvement using large language models at meta, 2024.
[2] Mario Alviano, Ly Ly T. Trieu, Tran Cao Son, and Marcello Balduccini. Advancements in xasp, an XAI system for answer set programming. In Agostino Dovier and Andrea Formisano, editors, Proceedings of the 38th Italian Conference on Computational Logic, Udine, Italy, June 21-23, 2023, volume 3428 of CEUR Workshop Proceedings. CEUR-WS.org, 2023.
[3] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846, 1983.
[4] Jennifer Chu-Carroll, Andrew Beck, Greg Burnham, David OS Melville, David Nachman, A. Erdem Özcan, and David Ferrucci. Beyond llms: Advancing the landscape of complex reasoning, 2024.
[5] Xi Deng, Volker Haarslev, and Nematollaah Shiri. A framework for explaining reasoning in description logics. In Thomas RothBerghofer and Stefan Schulz, editors, $E z$ $a C t$, volume FS-05-04 of AAAI Technical Report, pages 55-61. AAAI Press, 2005.
[6] Martin Gebser, Roland Kaminski, Benjamin Kaufmann, and Torsten Schaub. Clingo $=$ asp + control: Preliminary report, 2014.
[7] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis, 2024.
[8] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alex Wardle-Solano, Hannah Szabo, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R. Fabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, and Dragomir Radev. Folio: Natural language reasoning with first-order logic, 2024.
[9] Aditya Kalyanpur, Tom Breloff, and David A Ferrucci. Braid: Weaving symbolic and neural knowledge into coherent logical explanations. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):10867-10874, Jun. 2022.
[10] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms can't</p>
<p>plan, but can help planning in llm-modulo frameworks. CoRR, abs/2402.01817, 2024.
[11] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis, 2023.
[12] Vladimir Lifschitz. What is answer set programming? In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1594-1597. MIT Press, 2008.
[13] Kaibo Liu, Yiyang Liu, Zhenpeng Chen, Jie M. Zhang, Yudong Han, Yun Ma, Ge Li, and Gang Huang. Llm-powered test case generation for detecting tricky bugs, 2024.
[14] Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, and Roger Levy. LINC: A neurosymbolic approach for logical reasoning by combining language models with firstorder logic provers. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5153-5176, Singapore, December 2023. Association for Computational Linguistics.
[15] Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering large language models with symbolic solvers
for faithful logical reasoning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3806-3824, Singapore, December 2023. Association for Computational Linguistics.
[16] Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, and Vincent J. Hellendoorn. Cat-lm training language models on aligned code and tests. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 409-420, 2023.
[17] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023.
[18] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models : A critical investigation. In NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing SystemsDecember 2023 Article No.: 3320, page 75993-76005, 2023.
[19] Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, and Denny Zhou. Natural plan: Benchmarking llms on natural language planning, 2024.</p>
<h1>7 Appendix</h1>
<h3>7.1 Prompt for Logic Stratification</h3>
<p>You are an expert logician. You are given a set of natural language statements that express various logical conditions, rules and constraints. Your task is to stratify (cluster) the statements based on their logical structure, connectives (or operators used) and complexity (e.g., nested clauses etc.).</p>
<p>Output a list of clusters where each cluster contains a collection of statements that have a similar logical structure and connectives used. Copy up to 5 canonical problem statements in each cluster.</p>
<p>Be as fine-grained as possible when coming up with clusters and come up with an exhaustive set of clusters that cover all the diversity in the input statements.</p>
<h1>7.2 8-Examples (one per logic class) used in LLM Actor</h1>
<div class="codehilite"><pre><span></span><code><span class="s s-Atom">####</span> <span class="nv">Example</span> <span class="mi">1</span>
<span class="nv">Input</span><span class="o">:</span>
<span class="nv">Either</span> <span class="nv">Jack</span> <span class="s s-Atom">does</span> <span class="s s-Atom">hire</span> <span class="s s-Atom">a</span> <span class="s s-Atom">maid</span> <span class="s s-Atom">or</span> <span class="s s-Atom">cleaning</span> <span class="s s-Atom">service</span> <span class="s s-Atom">and</span> <span class="s s-Atom">does</span> <span class="o">not</span> <span class="s s-Atom">often</span> <span class="s s-Atom">clean</span> <span class="s s-Atom">his</span> <span class="s s-Atom">home</span><span class="p">,</span>
<span class="s s-Atom">or</span> <span class="s s-Atom">he</span> <span class="s s-Atom">does</span> <span class="o">not</span> <span class="s s-Atom">hire</span> <span class="s s-Atom">a</span> <span class="s s-Atom">maid</span> <span class="s s-Atom">or</span> <span class="s s-Atom">cleaning</span> <span class="s s-Atom">service</span> <span class="s s-Atom">nor</span> <span class="s s-Atom">often</span> <span class="s s-Atom">clean</span> <span class="s s-Atom">his</span> <span class="s s-Atom">home</span><span class="p">.</span>
<span class="nv">Query</span><span class="o">:</span> <span class="nv">Jack</span> <span class="s s-Atom">hires</span> <span class="s s-Atom">a</span> <span class="s s-Atom">maid</span> <span class="s s-Atom">or</span> <span class="s s-Atom">cleaning</span> <span class="s s-Atom">service</span><span class="p">.</span>
<span class="nv">Output</span> <span class="nv">Model</span><span class="o">:</span>
<span class="c1">% R1: Jack has two options</span>
<span class="mi">1</span> <span class="p">{</span><span class="s s-Atom">jack_option1</span><span class="p">;</span> <span class="s s-Atom">jack_option2</span><span class="p">}</span> <span class="mf">1.</span>
<span class="c1">% R2: Jack option 1 is: Jack hires a maid or cleaning service</span>
<span class="c1">% and does not often clean his home</span>
<span class="mi">1</span> <span class="p">{</span><span class="nf">hires_maid_or_service</span><span class="p">(</span><span class="s s-Atom">jack</span><span class="p">);</span> <span class="o">-</span><span class="nf">often_clean</span><span class="p">(</span><span class="s s-Atom">jack</span><span class="p">)}</span> <span class="mi">1</span> <span class="o">:-</span> <span class="s s-Atom">jack_option1</span><span class="p">.</span>
<span class="c1">% R3: Jack option 2 is: Jack does not hire a maid or cleaning service</span>
<span class="c1">% and does not often clean his home</span>
<span class="o">-</span><span class="nf">hires_maid_or_service</span><span class="p">(</span><span class="s s-Atom">jack</span><span class="p">)</span> <span class="o">:-</span> <span class="s s-Atom">jack_option2</span><span class="p">.</span>
<span class="o">-</span><span class="nf">often_clean</span><span class="p">(</span><span class="s s-Atom">jack</span><span class="p">)</span> <span class="o">:-</span> <span class="s s-Atom">jack_option2</span><span class="p">.</span>
<span class="nv">Output</span> <span class="nv">Query</span><span class="o">:</span>
<span class="mf">1.</span> <span class="nv">ATOM</span><span class="p">(</span><span class="nf">hires_maid_or_service</span><span class="p">(</span><span class="s s-Atom">jack</span><span class="p">))</span>
</code></pre></div>

<p>#### Example 2
Input:
Some people are dependent on coffee. Rina is either a student who
is dependent on caffeine, or she is neither.
Query: Some people are students.
Output Model:
\% R1: Some people are dependent on coffee
{dependent_on_caffeine(X)} :- person(X).
\% R2: Rina is either a student and dependent on caffeine, or she is neither.
1 {rina_student_dependent; rina_neither} 1.
\% R3: Add rules to split the conjunction "rina_student_dependent"
student(rina) :- rina_student_dependent.
dependent_on_caffeine(rina) :- rina_student_dependent.
\% R4: Add rules to split the conjunction "rina_neither"
-student(rina) :- rina_neither.
-dependent_on_caffeine(rina) :- rina_neither.
\% ISA Rules
\% R5: All students are people
person(X) :- student(X).</p>
<p>Output Query:</p>
<ol>
<li>ATOM(person(X))</li>
<li>ATOM (student(X))</li>
<li>$\operatorname{SOME}(1,2)$</li>
</ol>
<h4>Example 3</h4>
<p>Input:
No fish are plants. Trees are plants. Fir is a tree.
Query: Fir is not a fish.
Output Model:
\% R1: No fish are plants.
-plant(X) :- fish(X).
\% R2: Trees are plants.
plant(X) :- tree(X).
\% R3: Fir is a tree.
tree(fir).
\% ISA Rules
\% R4: "No fish are plants" also means no plants are fish.
\% This rule is needed for the query.
-fish(X) :- plant(X).
Output Query:
1. ATOM(-fish(fir))</p>
<h4>Example 4</h4>
<p>Input:
Tom is a Russian choral conductor who specialized in French Renaissance.
Query: No choral conductor specialized in the performance of Renaissance
Output Model:
\% R1: Tom is a Russian choral conductor
russian_choral_conductor(tom).
\% R2: Tom specialized in French Renaissance
\% specialized_french_renaissance(tom).
\% ISA Rules
\% R3: A Russian choral conductor is also a choral conductor
choral_conductor(X) :- russian_choral_conductor(X).
\% R4: A specialized french renaissance is a french renaissance
french_renaissance(X) :- specialized_french_renaissance(X).
Output Query:</p>
<ol>
<li>ATOM(choral_conductor(X))</li>
<li>ATOM(specialized_renaissance(X))</li>
<li>AND $(1,2)$</li>
<li>$\operatorname{NOT}(3)$</li>
</ol>
<h4>Example 5</h4>
<p>Input:
The Blake McFall Company Building is a building added to the National
Register of Historic Places in 1990.
The Emmet Building is a five-story building in Portland, Oregon.
The Emmet Building was built in 1915.</p>
<p>The Emmet Building is another name for the Blake McFall Company Building. John works at the Emmet Building.
Query: John started his current job in 1990
Output Model:
\% R1: The Blake McFall Company Building is a building added to the
\% National Register of Historic Places in 1990.
historic_building(blake_mcfall_company_building, 1990).
\% R2: The Emmet Building is a five-story building in Portland, Oregon.
five_story_building(emmet_building).
location(emmet_building, portland_oregon).
\% R3: The Emmet Building was built in 1915.
built_year(emmet_building, 1915).
\% R4: The Emmet Building is another name for the Blake McFall Company Building. same_building(emmet_building, blake_mcfall_company_building).
\% R5: John works at the Emmet Building.
works_at(john, emmet_building).
\% R6: Same building is symmetric
same_building(X, Y) :- same_building(Y, X).
\% R7: same_building needs to apply to other predicates involving buildings historic_building(X, Z) :- same_building(X, Y), historic_building(Y, Z).
five_story_building(X) :- same_building(X, Y), five_story_building(Y).
location(X, Z) :- same_building(X, Y), location(Y, Z).
built_year(X, Z) :- same_building(X, Y), built_year(Y, Z).
works_at(Z, X) :- same_building(X, Y), works_at(Z, Y).
Output Query:</p>
<ol>
<li>ATOM(started_job(john, 1990))
#### Example 6
Input:
A werewolf is a human that can turn into a wolf.
A werewolf has been scratched or bitten by another werewolf.
If someone has been scratched or bitten by some entity,
they have been attacked by that entity.
Query: All humans are werewolves.
Output Model:
\% R1: A werewolf is a human that can turn into a wolf.
werewolf(X) :- human(X), can_turn_into_wolf(X).
\% R2: A werewolf has been scratched or bitten by another werewolf.
scratched_or_bitten_by_werewolf(X) :- werewolf(X).
\% R3: If someone has been scratched or bitten by some entity,
\% they have been attacked by that entity.
attacked_by(Y, X) :- scratched_or_bitten_by(Y, X).
\% R4: A werewolf has been scratched or bitten by another werewolf implies
\% the werewolf has been attacked by another werewolf.
attacked_by_werewolf(X) :- scratched_or_bitten_by_werewolf(X).</li>
</ol>
<p>\% ISA Rules: Need to connect generalized predicates to specialized versions
\% R5: If someone is "attacked_by" a werewolf, then they are "attacked_by_werewolf" attacked_by_werewolf(X) :- attacked_by(Y, X), werewolf(X).
\% R6: If someone is "scratched_or_bitten_by" a werewolf, then
\% they are "scratched_or_bitten_by_werewolf"
scratched_or_bitten_by_werewolf(X) :- scratched_or_bitten(Y, X), werewolf(X).
Output Query:</p>
<ol>
<li>ATOM(human(Human))</li>
<li>ATOM(werewolf(Human))</li>
<li>$\operatorname{ALL}(1,2)$
#### Example 7
Input:
All people in this tech company who are consistent and enjoy sticking to their regular routines do not like surprises.
People in this tech company who wear the same flannel shirts every day are consistent and enjoy sticking to their regular routines.
People in this tech company who do not like shopping for clothes wear the same flannel shirts every day.
Old people living in stable homes do not like surprises.
People in this tech company who have very high energy and are impulsive like surprises. Mike works in this tech company.
If Mike is not a person who wears the same flannel shirts every day, has very high energy, and is impulsive, then Mike either is very consistent and enjoys sticking to his regular routines or does not like surprises.
Query: If Mike wears the same flannel shirts every day or does not
like shopping for clothes, then Mike is neither an old person living in a stable home nor does he like shopping for clothes.</li>
</ol>
<p>Output Model:
\% R1: All people in this tech company who are consistent and
\% enjoy sticking to their regular routines do not like surprises.
-likes_surprises(X) :- consistent(X), enjoys_regular_routines(X), works_in_tech_company(X).
\% R2: People in this tech company who wear the same flannel shirts
\% every day are consistent and enjoy sticking to their regular routines.
consistent(X) :- wears_flannel_shirts_every_day(X), works_in_tech_company(X).
enjoys_regular_routines(X) :- wears_flannel_shirts_every_day(X), works_in_tech_company(X).
\% R3: People in this tech company who do not like shopping for clothes
\% wear the same flannel shirts every day.
wears_flannel_shirts_every_day(X) :- does_not_like_shopping_for_clothes(X),
works_in_tech_company(X).
\% R4: Old people living in stable homes do not like surprises.
-likes_surprises(X) :- old_person(X), lives_in_stable_home(X).
\% R5: People in this tech company who have very high energy
\% and are impulsive like surprises.</p>
<p>likes_surprises(X) :- very_high_energy(X), impulsive(X),
works_in_tech_company(X).
\% R6: Mike works in this tech company.
works_in_tech_company(mike).
\% R7: If Mike is not a person who wears the same flannel shirts every day,
\% has very high energy, and is impulsive, then Mike either is very
\% consistent and enjoys sticking to his regular routines or does not like surprises.
1 {consistent_and_enjoys_regular_routines(mike); -likes_surprises(mike) } :-
-wears_flannel_shirts_every_day(mike), very_high_energy(mike), impulsive(mike).
consistent(mike) :- consistent_and_enjoys_regular_routines(mike).
enjoys_regular_routines(mike) :- consistent_and_enjoys_regular_routines(mike).
Output Query:</p>
<ol>
<li>ATOM(wears_flannel_shirts_every_day(mike))</li>
<li>ATOM(does_not_like_shopping_for_clothes(mike))</li>
<li>$\operatorname{OR}(1,2)$</li>
<li>ATOM(-old_person(mike))</li>
<li>ATOM(-does_not_like_shopping_for_clothes(mike))</li>
<li>AND $(4,5)$</li>
<li>IF-THEN $(3,6)$</li>
</ol>
<h1>#### Example 8</h1>
<p>Input:
All mammals are living beings.
All elephants are mammals.
All baby elephants are elephants.
Some baby elephants are sleepy.
If Jumbo is a living being, then Jumbo is not both an elephant and a mammal.
If Jumbo is sleepy, then Jumbo is either a baby elephant or a mammal.
Query: Jumbo is sleepy.
Output Model:
\% R1: All mammals are living beings.
living_being(X) :- mammal(X).
\% R2: All elephants are mammals.
mammal (X) :- elephant(X).
\% R3: All baby elephants are elephants.
elephant(X) :- baby_elephant(X).
\% R4: Some baby elephants are sleepy.
{sleepy(X)} :- baby_elephant(X).
\% R5: If Jumbo is a living being, then Jumbo is not both an elephant and a mammal.
:- living_being(jumbo), elephant(jumbo), mammal(jumbo).
\% R6: If Jumbo is sleepy, then Jumbo is either a baby elephant or a mammal.
1 {baby_elephant(jumbo); mammal(jumbo)} 1 :- sleepy(jumbo).
Output Query:</p>
<ol>
<li>ATOM(sleepy(jumbo))</li>
</ol>
<h1>7.3 Full Prompt for LLM Actor</h1>
<h2>## Task</h2>
<p>You are an expert logician trained in Answer Set Programming (ASP). You are given a problem description in natural language and your job is to translate the English description into an ASP program.</p>
<p>We will do this task iteratively. You will start with a blank slate and incrementally build up the ASP program as new requests/descriptions/comments come in from the user.</p>
<p>At each step, you should write ASP code given the user request.
There are two compulsory parts to writing the code -- the ASP model and the tests.
Optionally, if the problem statement includes a "Query: " request (see below), you need to output a separate query structure which will be used to query the ASP model.
### Instructions on Writing the Model</p>
<ol>
<li>
<p>The model can include predicate declarations (facts).</p>
</li>
<li>
<p>Do not use special characters inside constant names.</p>
</li>
<li>Add a comment before each line to explain the meaning of the predicate.</li>
<li>Always add facts that are implicit in the user request.</li>
</ol>
<p>For example, if the request says "Rina is a student who likes coffee", don't forget to add the fact:
$\cdots$
student(rina).
2. Additionally, the model might include rules/constraints based on the user request. When writing rules, remember the following:</p>
<ul>
<li>Only include the MINIMAL conditions necessary to satisfy the user request.</li>
<li>Consider all aspects of the user request, especially those involving disjunctions (ORs) and Negation (Not).</li>
<li>Use predicates that are already part of the program, especially considering negation (not).</li>
<li>Do not use special characters inside variable names.</li>
<li>Add a comment before each rule or constraint to explain the meaning of the rule.</li>
<li>Copy/paste problem statements as comments for rules and facts, to ensure that all the statements in the input, excluding the query, are accounted for in the model rules and facts.</li>
<li>Write a "Rule ID" for each rule/constraint in the comment before the rule/constraint. This will be referenced in the tests you add later.</li>
</ul>
<h2>3. Representing negation</h2>
<ul>
<li>
<p>Only create a rule with default negation ("not P" atoms) in the body if there are exception conditions in the original input statement.</p>
</li>
<li>
<p>Otherwise always use Strong Negation ("-P") to represent negation.</p>
</li>
<li>When creating a negated predicate -P, ensure you are reusing the positive version of the predicate $P$.</li>
<li>Interestingly, some rules might mix variables and constants. Make sure to use constants in rules when specified in the user request.</li>
<li>
<p>Remember that if a rule has multiple conjunctions in its head, these must be split into separate rules.</p>
</li>
<li>
<p>Accurately representing Either-OR vs. regular OR</p>
</li>
<li>
<p>If the user request includes an Either-OR clause, e.g., $A$ is either $B, C$ or $D$, this is represented (abstractly) in ASP as "1 (B; C; D) 1 :- A." because there is exactly one choice for $A$ in this case (min=1, max=1)</p>
</li>
<li>On the other hand, if the user request includes a regular OR (disjunction), e.g. A is B, C or D, this is represented (abstractly) in ASP as "1 (B; C; D) 3 :- A." because there are more than one options for $A(\min =1, \max =3)$
### Query Generation
In some cases, the user input may include a request to generate a query for the ASP program you have written.
This will be indicated by a NEW line in the input that looks like this:
"Query: <Query Definition>"
When you see this line, add a key called "Query" in the output JSON which contains a list of query building blocks (see below) corresponding to the Query Definition.</li>
</ul>
<p>Think of the query statement as a logical proposition. A complex logical proposition might involve multiple individual ATOMIC propositions, combined with logical operators (and/or/not etc).</p>
<p>The proposition is made up of the following building blocks:</p>
<ol>
<li>ATOM(P(. .)) - 1-arity predicate: e.g. Rina doesn't want to be addicted to caffeine (an atomic proposition)</li>
<li>AND(N1, N2...Nk) - k-arity predicate that refers to the ATOM predicate IDs Ni: e.g. Rina doesn't want to be addicted to caffeine and Rina is unaware that caffeine is a drug 3. OR(N1, N2,..Nk) - k-arity predicate that refers to the ATOM predicate IDs Ni: e.g. Rina doesn't want to be addicted to caffeine or Rina is unaware that caffeine is a drug</li>
<li>EITHER-OR(N1, N2, .. Nk) - k-arity predicate that refers to the ATOM predicate IDs Ni: e.g. Either Rina doesn't want to be addicted to caffeine or Rina is unaware that caffeine is a drug</li>
<li>NEITHER-NOR(N1, N2,...Nk) - k-arity predicate that refers to the ATOM predicate IDs Ni: e.g. Neither Rina doesn't want to be addicted to caffeine nor is Rina unaware that caffeine</li>
</ol>
<p>is a drug
6. IF-THEN(N1, N2) - 2-arity predicate that refers to the ATOM predicate IDs Ni: e.g. If Rina doesn't want to be addicted to caffeine, then Rina is aware that caffeine is a drug.
7. ALL(N1, N2) - 2-arity Predicate that refers to the ATOM predicate IDs Ni: e.g. All cats are animals
8. SOME(N1, N2) - 2-arity predicate that refers to the ATOM predicate IDs Ni: e.g., Some snakes have fur
9. NOT(N1) - 1-arity predicate that refers to the ATOM predicate ID: e.g. Rina is not hungry.</p>
<p>These building blocks can be composed to create arbitrarily nested complex logical statements.
### Instructions for representing Queries in ASP</p>
<ol>
<li>Output a series of steps to build the logical proposition underlying the query statement.</li>
<li>Each step must begin with a number followed by the building block.</li>
<li>Only focus on the "Query: " statement and translate that. In some cases, it might just be a simple atomic proposition, and not necessarily, a more complex logical proposition.</li>
<li>If the query involves logical operators (AND, OR, EITHER-OR, NEITHER-NOR, IF-THEN, SOME, ALL, NOT), create ATOMs first and reference their IDs in the logical operators. Never create a logical operator with a direct reference to a proposition; its arguments should always be ATOM IDs.</li>
<li>Reuse the main model predicates as much as possible in the query atoms. For example, if there are model predicates for "scientist(..)" and "painter(..)" and the query says "Amy is a scientist and painter", do not create a new predicate called "scientist_and_painter", instead, reuse the existing program predicates.
### Add Missing Implicit ISA Rules
Go through all the predicates defined in the model and in the query ATOMs, and consider when one predicate is always clearly a sub-type of another.</li>
</ol>
<p>For example, if the model and/or query has predicates "german_politician(X)", "german(X)" and "politician(X"), you should add the rules:
\% R1: All German Politicians are Germans
german(X) :- german_politican(X).
\% R2: All German Politicians are Politicians
politician(X) :- german_politician(X).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://www.w3.org/2007/OWL/wiki/Punning&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Corresponding Author: adityak@ec.ai&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2189 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2189</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2189</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-59.html">extraction-schema-59</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-276296422</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16330v1.pdf" target="_blank">SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers</a></p>
                <p><strong>Paper Abstract:</strong> Novelty is a core component of academic papers, and there are multiple perspectives on the assessment of novelty. Existing methods often focus on word or entity combinations, which provide limited insights. The content related to a paper's novelty is typically distributed across different core sections, e.g., Introduction, Methodology and Results. Therefore, exploring the optimal combination of sections for evaluating the novelty of a paper is important for advancing automated novelty assessment. In this paper, we utilize different combinations of sections from academic papers as inputs to drive language models to predict novelty scores. We then analyze the results to determine the optimal section combinations for novelty score prediction. We first employ natural language processing techniques to identify the sectional structure of academic papers, categorizing them into introduction, methods, results, and discussion (IMRaD). Subsequently, we used different combinations of these sections (e.g., introduction and methods) as inputs for pretrained language models (PLMs) and large language models (LLMs), employing novelty scores provided by human expert reviewers as ground truth labels to obtain prediction results. The results indicate that using introduction, results and discussion is most appropriate for assessing the novelty of a paper, while the use of the entire text does not yield significant results. Furthermore, based on the results of the PLMs and LLMs, the introduction and results appear to be the most important section for the task of novelty score prediction. The code and dataset for this paper can be accessed at https://github.com/njust-winchy/SC4ANM.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2189.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2189.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC4ANM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study using reviewer-assigned novelty scores (ICLR TNS) as ground truth to compare citation/reference-based proxies and text-based automated methods (PLMs/LLMs); identifies which paper sections best predict reviewer novelty judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>reference/citation-based novelty measure (semantic distance of references) and related bibliometric proxies (journal impact factor mention)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>peer-review novelty scores (ICLR Technical Novelty and Significance, aggregated per paper)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>3-class reviewer-derived novelty labels (re-grouped from TNS into 0: basic, 1: moderate, 2: highly novel); also uses section-text inputs (IMRD combinations) to predict these labels</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Direct comparison in this study: a citation/reference-based baseline (Shibayama et al. method) achieved Accuracy=0.4265, Weighted F1=0.3637, Pearson r=0.0132 with reviewer labels, whereas a text-based PLM (SciBERT Longformer, using Introduction+Results+Discussion) achieved Accuracy=0.6824, Weighted F1=0.6515, Pearson r=0.3261 (p<0.001); authors interpret this as text-based signals correlating far better with reviewer judgments than the citation-distance proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Large: near-zero Pearson (0.0132) for the citation/reference proxy vs moderate Pearson (~0.326) for text-based predictor; accuracy gap ~0.256 (0.6824 vs 0.4265) and F1 gap ~0.2878 in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science / machine learning (ICLR 2022 and 2023 conference submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not analyzed in this study beyond noting the dataset is limited to CS; authors state generalizability to other domains is untested.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>PLMs (five long-text PLMs) fine-tuned on section combinations: best model (SciBERT Longformer) with IRD input achieved Accuracy=0.6824 and Weighted F1=0.6515; PLMs were relatively poor at correctly identifying highest-novelty papers. LLM zero-shot (GPT-3.5, GPT-4o) performance was substantially worse (best accuracy ≈0.40) and unstable; GPT models tended to overpredict high novelty (bias to favorable scores). Correlations: PLMs produced generally positive correlations with reviewer labels (SciBERT highest); GPT-3.5 sometimes produced negative correlations, GPT-4o mostly positive for some section combos (e.g., IMR).</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Text-based modeling (fine-tuned PLMs on selected section combinations) was tested vs the citation-based baseline and substantially reduced the proxy–ground-truth gap in this dataset; no additional correction algorithm (e.g., debiasing, reweighting) beyond model choice and section selection was tested.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Yes — the authors report strong class imbalance (few high-novelty papers). They removed inconsistent reviewer cases and note PLMs underperform on high-novelty examples, which they attribute partly to scarcity of such examples in training data; for LLM experiments they sampled balanced subsets (random sampling) to create evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Empirical ML study: collected ICLR 2022/2023 papers and open peer reviews (8183 crawled; 6,094 with consistent TNS scores; parsed content matched for 3,500 papers); section-structure identification (SciBERT + LLaMA3) then fine-tuned PLMs on different section combinations (train/val/test splits reported, e.g., 2,500/350/350) and evaluated performance vs reviewer TNS labels; additionally ran zero-shot LLM prompts (GPT-3.5, GPT-4o) on balanced small samples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Citation/reference-based novelty proxies correlate very weakly with reviewer-assigned novelty (near-zero Pearson), while text-based models focusing on Introduction+Results+Discussion substantially improve alignment with reviewers (Pearson r increased to ~0.33 and accuracy to ~0.68).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2189.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2189.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shibayama_method (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shibayama et al. 2021 semantic-distance reference novelty measure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional novelty proxy that computes semantic distance among references (and between title and references) using word embeddings and percentile ranking; used here as a citation-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring novelty in science with word embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>reference semantic-distance novelty (embedding cosine distances, percentile-based novelty score)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>In this paper, treated as a proxy and compared against peer-review novelty scores (ICLR TNS aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Percentile rank of pairwise reference-distance (q=100 used), scaled and mapped to the 3-class label range for comparison in this study</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>When applied to the ICLR reviewer-ground-truth labels in this study, Shibayama's method gives Accuracy=0.4265, Weighted F1=0.3637, Pearson r=0.0132 (very weak correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>Very large gap relative to reviewer ground truth in this dataset (Pearson ≈0.013).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Method tested here on computer science (ICLR) papers; original method applied broadly in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>As a static bibliometric/textual proxy it performed poorly versus human reviewer labels in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Used here as a baseline: compute embedding-based pairwise reference distances, percentile ranking (q=100), linearly scaled to [0,2] and rounded to compare with reviewer labels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>The Shibayama reference-distance novelty proxy shows almost no correlation with peer-review novelty scores in the ICLR dataset used here, indicating limitations of that proxy for reflecting reviewer judgments of novelty.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2189.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2189.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang2017_observation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias against novelty in science: A cautionary tale for users of bibliometric indicators (Wang et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior bibliometric finding summarized by the authors: highly novel research tends to have higher long-term payoff but faces higher risk, delayed recognition, and tends to appear in lower-impact-factor journals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bias against novelty in science: A cautionary tale for users of bibliometric indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>journal impact factor / citation-based measures (as proxies of impact)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Long-term impact (operationalized in original work via citations/impact measures), discussed here qualitatively relative to novelty</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Novelty defined by first-time combinations of referenced journals (combinatorial novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Reported qualitatively in this paper: 'highly novel research offers significant benefits but also involves higher risks, often faces delayed recognition, and is published in lower Impact Factor journals.' (No specific numeric effect sizes reported here.)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Delayed recognition for highly novel work (qualitative statement: recognition often occurs with delay).</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-field bibliometric analysis (original study examined novelty and impact across scientific fields; in this paper referenced in general terms).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Referenced prior bibliometric study linking combinatorial novelty of referenced journals to later impact; discussed here in related-work context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Novel research is at higher risk of being under-recognized early and tends to be published in lower-IF venues despite potential for greater long-term impact — a caution when relying on bibliometric proxies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2189.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2189.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uzzi2013_observation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atypical combinations and scientific impact (Uzzi et al., 2013)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior large-scale bibliometric finding: papers that combine common and novel (atypical) references tend to be disproportionately impactful (as measured by citations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Atypical combinations and scientific impact.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>reference-combination atypicality (used as predictor of citation impact)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Citation counts (used as measure of impact in the original study)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Atypical combination score of cited references (novelty as unusual combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Not quantified here beyond the summary statement that the most impactful papers combine typical earlier works with unique combinations; no numeric relationships provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Large cross-disciplinary sample (cited in paper as 17.9 million Web of Science papers in original study).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Referenced large-scale bibliometric analysis demonstrating association between atypical reference combinations and later citation impact.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Combinatorial atypicality of references is associated with higher citation-based impact, indicating one way bibliometrics have tried to approximate novelty/innovation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2189.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2189.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tahamtan2018_caveat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references (Tahamtan & Bornmann, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced critique noting that the extent to which cited publications serve as genuine sources of inspiration for new work is unclear, thus warning about relying on references as proxies for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>use of references/citations as novelty proxies (critique)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Not an empirical ground-truth comparison here — conceptual critique of citation/reference proxies relative to creative origin of ideas</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Conceptual/empirical literature critique referenced in related work to motivate use of reviewer-assigned novelty scores instead of reference-only proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>References are an imperfect proxy for sources of inspiration and novelty; caution is warranted when using citation/reference measures to infer novelty.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2189.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2189.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLM_vs_LLM_novelty_eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of pre-trained language models (PLMs) and large language models (LLMs) for automated novelty score prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper evaluates fine-tuned PLMs (Longformer variants including SciBERT Longformer) and zero-shot LLMs (GPT-3.5, GPT-4o) for predicting reviewer novelty scores from different section combinations, reporting accuracy, F1, and correlation metrics and characterizing biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>automated novelty-prediction systems (text-based proxies) compared to reviewer ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Peer-review novelty scores (ICLR TNS aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Classification into 3 novelty classes derived from TNS (0/1/2) and continuous-correlations used to compare scores</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Best fine-tuned PLM (SciBERT Longformer with IRD) achieved Accuracy=0.6824 and Weighted F1=0.6515; LLM zero-shot best accuracy ≈0.40 (F1 lower); PLMs produced moderate positive correlations with reviewer scores (SciBERT Pearson up to ~0.326 for best combos), while GPT-3.5 sometimes showed negative correlations and GPT-4o showed small positive correlations for some combos (e.g., IMR Pearson≈0.1916).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td>PLMs still far from perfect (highest accuracy ~0.68) and struggle on highest-novelty category; LLM zero-shot performance much lower (≈0.3–0.4 accuracy) and biased toward overpredicting high novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Computer science / machine learning (ICLR submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>See quantitative_relationship above; additionally: PLMs perform best when fed Introduction+Results+Discussion (three-section combos); full-text (IMRD) did not improve and sometimes degraded PLM performance (too much content). LLMs in zero-shot are unreliable and tend to assign overly favorable novelty labels.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Section selection (prioritizing Introduction, Results, Discussion) and PLM fine-tuning improved alignment with reviewer labels relative to baselines; no further corrective algorithms tested.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Authors note class imbalance (few high-novelty examples) and removed discordant reviews; this scarcity likely reduces model sensitivity to truly high-novelty papers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Controlled ML experiments: fine-tune several long-text PLMs on section-subset inputs with cross-validation and early stopping; small-sample, repeated zero-shot evaluations with GPT-3.5/GPT-4o; correlation and classification metrics compared to reviewer labels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Careful selection of textual sections (Introduction+Results+Discussion) and fine-tuning of PLMs produces substantially better alignment with reviewer novelty scores than citation-based proxies, but automated systems still struggle, especially to reliably detect highest-novelty papers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Atypical combinations and scientific impact. <em>(Rating: 2)</em></li>
                <li>Bias against novelty in science: A cautionary tale for users of bibliometric indicators. <em>(Rating: 2)</em></li>
                <li>Measuring novelty in science with word embedding. <em>(Rating: 2)</em></li>
                <li>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. <em>(Rating: 1)</em></li>
                <li>Introducing a novelty indicator for scientific research: validating the knowledge-based combinatorial approach. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2189",
    "paper_id": "paper-276296422",
    "extraction_schema_id": "extraction-schema-59",
    "extracted_data": [
        {
            "name_short": "SC4ANM",
            "name_full": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers",
            "brief_description": "Empirical study using reviewer-assigned novelty scores (ICLR TNS) as ground truth to compare citation/reference-based proxies and text-based automated methods (PLMs/LLMs); identifies which paper sections best predict reviewer novelty judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "reference/citation-based novelty measure (semantic distance of references) and related bibliometric proxies (journal impact factor mention)",
            "ground_truth_measure": "peer-review novelty scores (ICLR Technical Novelty and Significance, aggregated per paper)",
            "novelty_transformation_measure": "3-class reviewer-derived novelty labels (re-grouped from TNS into 0: basic, 1: moderate, 2: highly novel); also uses section-text inputs (IMRD combinations) to predict these labels",
            "quantitative_relationship": "Direct comparison in this study: a citation/reference-based baseline (Shibayama et al. method) achieved Accuracy=0.4265, Weighted F1=0.3637, Pearson r=0.0132 with reviewer labels, whereas a text-based PLM (SciBERT Longformer, using Introduction+Results+Discussion) achieved Accuracy=0.6824, Weighted F1=0.6515, Pearson r=0.3261 (p&lt;0.001); authors interpret this as text-based signals correlating far better with reviewer judgments than the citation-distance proxy.",
            "gap_magnitude": "Large: near-zero Pearson (0.0132) for the citation/reference proxy vs moderate Pearson (~0.326) for text-based predictor; accuracy gap ~0.256 (0.6824 vs 0.4265) and F1 gap ~0.2878 in this dataset.",
            "temporal_pattern": null,
            "field_studied": "Computer science / machine learning (ICLR 2022 and 2023 conference submissions)",
            "field_differences": "Not analyzed in this study beyond noting the dataset is limited to CS; authors state generalizability to other domains is untested.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "PLMs (five long-text PLMs) fine-tuned on section combinations: best model (SciBERT Longformer) with IRD input achieved Accuracy=0.6824 and Weighted F1=0.6515; PLMs were relatively poor at correctly identifying highest-novelty papers. LLM zero-shot (GPT-3.5, GPT-4o) performance was substantially worse (best accuracy ≈0.40) and unstable; GPT models tended to overpredict high novelty (bias to favorable scores). Correlations: PLMs produced generally positive correlations with reviewer labels (SciBERT highest); GPT-3.5 sometimes produced negative correlations, GPT-4o mostly positive for some section combos (e.g., IMR).",
            "correction_mechanism": "Text-based modeling (fine-tuned PLMs on selected section combinations) was tested vs the citation-based baseline and substantially reduced the proxy–ground-truth gap in this dataset; no additional correction algorithm (e.g., debiasing, reweighting) beyond model choice and section selection was tested.",
            "training_distribution_bias": "Yes — the authors report strong class imbalance (few high-novelty papers). They removed inconsistent reviewer cases and note PLMs underperform on high-novelty examples, which they attribute partly to scarcity of such examples in training data; for LLM experiments they sampled balanced subsets (random sampling) to create evaluation sets.",
            "counterexamples": null,
            "study_design": "Empirical ML study: collected ICLR 2022/2023 papers and open peer reviews (8183 crawled; 6,094 with consistent TNS scores; parsed content matched for 3,500 papers); section-structure identification (SciBERT + LLaMA3) then fine-tuned PLMs on different section combinations (train/val/test splits reported, e.g., 2,500/350/350) and evaluated performance vs reviewer TNS labels; additionally ran zero-shot LLM prompts (GPT-3.5, GPT-4o) on balanced small samples.",
            "key_finding": "Citation/reference-based novelty proxies correlate very weakly with reviewer-assigned novelty (near-zero Pearson), while text-based models focusing on Introduction+Results+Discussion substantially improve alignment with reviewers (Pearson r increased to ~0.33 and accuracy to ~0.68).",
            "uuid": "e2189.0"
        },
        {
            "name_short": "Shibayama_method (baseline)",
            "name_full": "Shibayama et al. 2021 semantic-distance reference novelty measure",
            "brief_description": "A traditional novelty proxy that computes semantic distance among references (and between title and references) using word embeddings and percentile ranking; used here as a citation-based baseline.",
            "citation_title": "Measuring novelty in science with word embedding.",
            "mention_or_use": "use",
            "proxy_metric_type": "reference semantic-distance novelty (embedding cosine distances, percentile-based novelty score)",
            "ground_truth_measure": "In this paper, treated as a proxy and compared against peer-review novelty scores (ICLR TNS aggregated)",
            "novelty_transformation_measure": "Percentile rank of pairwise reference-distance (q=100 used), scaled and mapped to the 3-class label range for comparison in this study",
            "quantitative_relationship": "When applied to the ICLR reviewer-ground-truth labels in this study, Shibayama's method gives Accuracy=0.4265, Weighted F1=0.3637, Pearson r=0.0132 (very weak correlation).",
            "gap_magnitude": "Very large gap relative to reviewer ground truth in this dataset (Pearson ≈0.013).",
            "temporal_pattern": null,
            "field_studied": "Method tested here on computer science (ICLR) papers; original method applied broadly in prior work.",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "As a static bibliometric/textual proxy it performed poorly versus human reviewer labels in this dataset.",
            "correction_mechanism": null,
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Used here as a baseline: compute embedding-based pairwise reference distances, percentile ranking (q=100), linearly scaled to [0,2] and rounded to compare with reviewer labels.",
            "key_finding": "The Shibayama reference-distance novelty proxy shows almost no correlation with peer-review novelty scores in the ICLR dataset used here, indicating limitations of that proxy for reflecting reviewer judgments of novelty.",
            "uuid": "e2189.1"
        },
        {
            "name_short": "Wang2017_observation",
            "name_full": "Bias against novelty in science: A cautionary tale for users of bibliometric indicators (Wang et al., 2017)",
            "brief_description": "Prior bibliometric finding summarized by the authors: highly novel research tends to have higher long-term payoff but faces higher risk, delayed recognition, and tends to appear in lower-impact-factor journals.",
            "citation_title": "Bias against novelty in science: A cautionary tale for users of bibliometric indicators.",
            "mention_or_use": "mention",
            "proxy_metric_type": "journal impact factor / citation-based measures (as proxies of impact)",
            "ground_truth_measure": "Long-term impact (operationalized in original work via citations/impact measures), discussed here qualitatively relative to novelty",
            "novelty_transformation_measure": "Novelty defined by first-time combinations of referenced journals (combinatorial novelty)",
            "quantitative_relationship": "Reported qualitatively in this paper: 'highly novel research offers significant benefits but also involves higher risks, often faces delayed recognition, and is published in lower Impact Factor journals.' (No specific numeric effect sizes reported here.)",
            "gap_magnitude": null,
            "temporal_pattern": "Delayed recognition for highly novel work (qualitative statement: recognition often occurs with delay).",
            "field_studied": "Cross-field bibliometric analysis (original study examined novelty and impact across scientific fields; in this paper referenced in general terms).",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": null,
            "correction_mechanism": null,
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Referenced prior bibliometric study linking combinatorial novelty of referenced journals to later impact; discussed here in related-work context.",
            "key_finding": "Novel research is at higher risk of being under-recognized early and tends to be published in lower-IF venues despite potential for greater long-term impact — a caution when relying on bibliometric proxies.",
            "uuid": "e2189.2"
        },
        {
            "name_short": "Uzzi2013_observation",
            "name_full": "Atypical combinations and scientific impact (Uzzi et al., 2013)",
            "brief_description": "Prior large-scale bibliometric finding: papers that combine common and novel (atypical) references tend to be disproportionately impactful (as measured by citations).",
            "citation_title": "Atypical combinations and scientific impact.",
            "mention_or_use": "mention",
            "proxy_metric_type": "reference-combination atypicality (used as predictor of citation impact)",
            "ground_truth_measure": "Citation counts (used as measure of impact in the original study)",
            "novelty_transformation_measure": "Atypical combination score of cited references (novelty as unusual combinations)",
            "quantitative_relationship": "Not quantified here beyond the summary statement that the most impactful papers combine typical earlier works with unique combinations; no numeric relationships provided in this paper.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Large cross-disciplinary sample (cited in paper as 17.9 million Web of Science papers in original study).",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": null,
            "correction_mechanism": null,
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Referenced large-scale bibliometric analysis demonstrating association between atypical reference combinations and later citation impact.",
            "key_finding": "Combinatorial atypicality of references is associated with higher citation-based impact, indicating one way bibliometrics have tried to approximate novelty/innovation.",
            "uuid": "e2189.3"
        },
        {
            "name_short": "Tahamtan2018_caveat",
            "name_full": "Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references (Tahamtan & Bornmann, 2018)",
            "brief_description": "A referenced critique noting that the extent to which cited publications serve as genuine sources of inspiration for new work is unclear, thus warning about relying on references as proxies for novelty.",
            "citation_title": "Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references.",
            "mention_or_use": "mention",
            "proxy_metric_type": "use of references/citations as novelty proxies (critique)",
            "ground_truth_measure": "Not an empirical ground-truth comparison here — conceptual critique of citation/reference proxies relative to creative origin of ideas",
            "novelty_transformation_measure": null,
            "quantitative_relationship": null,
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": null,
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": null,
            "correction_mechanism": null,
            "training_distribution_bias": null,
            "counterexamples": null,
            "study_design": "Conceptual/empirical literature critique referenced in related work to motivate use of reviewer-assigned novelty scores instead of reference-only proxies.",
            "key_finding": "References are an imperfect proxy for sources of inspiration and novelty; caution is warranted when using citation/reference measures to infer novelty.",
            "uuid": "e2189.4"
        },
        {
            "name_short": "PLM_vs_LLM_novelty_eval",
            "name_full": "Comparison of pre-trained language models (PLMs) and large language models (LLMs) for automated novelty score prediction",
            "brief_description": "This paper evaluates fine-tuned PLMs (Longformer variants including SciBERT Longformer) and zero-shot LLMs (GPT-3.5, GPT-4o) for predicting reviewer novelty scores from different section combinations, reporting accuracy, F1, and correlation metrics and characterizing biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_metric_type": "automated novelty-prediction systems (text-based proxies) compared to reviewer ground truth",
            "ground_truth_measure": "Peer-review novelty scores (ICLR TNS aggregated)",
            "novelty_transformation_measure": "Classification into 3 novelty classes derived from TNS (0/1/2) and continuous-correlations used to compare scores",
            "quantitative_relationship": "Best fine-tuned PLM (SciBERT Longformer with IRD) achieved Accuracy=0.6824 and Weighted F1=0.6515; LLM zero-shot best accuracy ≈0.40 (F1 lower); PLMs produced moderate positive correlations with reviewer scores (SciBERT Pearson up to ~0.326 for best combos), while GPT-3.5 sometimes showed negative correlations and GPT-4o showed small positive correlations for some combos (e.g., IMR Pearson≈0.1916).",
            "gap_magnitude": "PLMs still far from perfect (highest accuracy ~0.68) and struggle on highest-novelty category; LLM zero-shot performance much lower (≈0.3–0.4 accuracy) and biased toward overpredicting high novelty.",
            "temporal_pattern": null,
            "field_studied": "Computer science / machine learning (ICLR submissions)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "See quantitative_relationship above; additionally: PLMs perform best when fed Introduction+Results+Discussion (three-section combos); full-text (IMRD) did not improve and sometimes degraded PLM performance (too much content). LLMs in zero-shot are unreliable and tend to assign overly favorable novelty labels.",
            "correction_mechanism": "Section selection (prioritizing Introduction, Results, Discussion) and PLM fine-tuning improved alignment with reviewer labels relative to baselines; no further corrective algorithms tested.",
            "training_distribution_bias": "Authors note class imbalance (few high-novelty examples) and removed discordant reviews; this scarcity likely reduces model sensitivity to truly high-novelty papers.",
            "counterexamples": null,
            "study_design": "Controlled ML experiments: fine-tune several long-text PLMs on section-subset inputs with cross-validation and early stopping; small-sample, repeated zero-shot evaluations with GPT-3.5/GPT-4o; correlation and classification metrics compared to reviewer labels.",
            "key_finding": "Careful selection of textual sections (Introduction+Results+Discussion) and fine-tuning of PLMs produces substantially better alignment with reviewer novelty scores than citation-based proxies, but automated systems still struggle, especially to reliably detect highest-novelty papers.",
            "uuid": "e2189.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Atypical combinations and scientific impact.",
            "rating": 2
        },
        {
            "paper_title": "Bias against novelty in science: A cautionary tale for users of bibliometric indicators.",
            "rating": 2
        },
        {
            "paper_title": "Measuring novelty in science with word embedding.",
            "rating": 2
        },
        {
            "paper_title": "Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references.",
            "rating": 1
        },
        {
            "paper_title": "Introducing a novelty indicator for scientific research: validating the knowledge-based combinatorial approach.",
            "rating": 1
        }
    ],
    "cost": 0.0189785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers
22 May 2025</p>
<p>Wenqing Wu 
Department of Information Management
Nanjing University of Science and Technology
210094NanjingChina</p>
<p>Chengzhi Zhang zhangcz@njust.edu.cn 
Department of Information Management
Nanjing University of Science and Technology
210094NanjingChina</p>
<p>Tong Bao tbao@njust.edu.cn 
Department of Information Management
Nanjing University of Science and Technology
210094NanjingChina</p>
<p>Yi Zhao yizhao93@njust.edu.cn 
Department of Information Management
Nanjing University of Science and Technology
210094NanjingChina</p>
<p>Department of Information Management
Nanjing University of Science and Technology
210094NanjingChina</p>
<p>SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers
22 May 202581029CAE5D1F231E0B5B1D4746C65345arXiv:2505.16330v1[cs.CL]Novelty score predictionLarge language modelSection structure combinationPre-trained language model
Novelty is a core component of academic papers, and there are multiple perspectives on the assessment of novelty.Existing methods often focus on word or entity combinations, which provide limited insights.The content related to a paper's novelty is typically distributed across different core sections, e.g., Introduction, Methodology and Results.Therefore, exploring the optimal combination of sections for evaluating the novelty of a paper is important for advancing automated novelty assessment.In this paper, we utilize different combinations of sections from academic papers as inputs to drive language models to predict novelty scores.We then analyze the results to determine the optimal section combinations for novelty score prediction.We first employ natural language processing techniques to identify the sectional structure of academic papers, categorizing them into introduction, methods, results, and discussion (IMRaD).Subsequently, we used different combinations of these sections (e.g., introduction and methods) as inputs for pretrained language models (PLMs) and large language models (LLMs), employing novelty scores provided by human expert reviewers as ground truth labels to obtain prediction results.The results indicate that using introduction, results and discussion is most appropriate for assessing the novelty of a paper, while the use of the entire text does not yield significant results.Furthermore, based on the results of the PLMs and LLMs, the introduction and results appear to be the most important section for the task of novelty</p>
<p>Introduction</p>
<p>Novelty is one of the core criteria in academic research, as it drives the frontier of knowledge, addresses unresolved questions in existing studies, or presents new insights.The current mainstream approach for evaluating the novelty of a paper utilizes the perspective of combinatorial innovation (Matsumoto et al., 2021;Wang et al., 2017;Uzzi et al., 2013), examining the distribution of references within the paper or the distribution of knowledge elements (Jeon et al., 2023;Luo et al., 2022).However, although easy to understand and simple to use, the extent to which cited publications serve as sources of inspiration for an academic paper is not yet well understood (Tahamtan and Bornmann, 2018).In addition, currently most of the main content of academic papers is not well used to evaluate novelty, and most of them are evaluated in the form of knowledge entities, keywords or topic words in the abstract and title.Content or entities related to the novelty of a paper are typically distributed across various sections, and relying solely on the abstract or title cannot fully capture all the knowledge utilized in the paper.When evaluating the novelty of a paper, relying solely on entity or word-level content is limited.Furthermore, in previous novelty measures based on entities, the entire document was treated as a window for entities, without considering the impact of different section and their combinations on the novelty measurement.To address this limitation, it is essential to explore which sections or combinations of sections in a paper provide the most insightful information for an accurate automatic novelty evaluation.</p>
<p>Generally, academic papers are divided into title, abstract, main text, and references.The main text can further be divided into introduction, methods, results, and discussion (IMRaD) (Sollaci and Pereira, 2004).Although not all academic papers follow this structure, natural language processing techniques (Cohan et al., 2018a) can be employed to segment the content of (https://openreview.net/forum?id=ltM1RMZntpu)</p>
<p>the main text into these sections.In the peer review process 1 , the evaluation of an academic paper's novelty primarily relies on the judgment of expert reviewers.Reviewers typically do not base their assessment solely on the words or entities form the paper, but rather make their assessment after read the main text of the paper.Given that novelty often arises from the broader context and argumentation presented throughout the paper, it is crucial to understand how various sections contribute to the overall assessment.Consequently, simulating the review process by analyzing different combinations of paper sections can provide a more accurate prediction of a paper's novelty.</p>
<p>With advancements in natural language processing, traditional machine learning and deep learning technology (Darraz et al., 2024;Zhu et al., 2025), particularly the emergence of large language models (LLMs) (OpenAI, 2024;Meta, 2024), the ability of machines to process academic papers has been significantly enhanced.LLMs have also demonstrated commendable performance in research related to peer review (Liang et al., 2023;Zhou et al., 2024), although there remains a considerable gap compared to human reviewers.Current research using LLMs primarily focuses on the macro-level evaluation of academic papers.However, the assessment of the novelty of papers is often overlooked, lacking clear evaluation or validation.</p>
<p>With an increasing number of conferences, journals, and platforms such as NeurIPS 2 , eLife 3 , PeerJ 4 , OpenReview 5 , and F1000Research 6 offering open peer review, the novelty scores provided by expert reviewers can be obtained from these publicly available peer review reports, as illustrated in the red box in Figure 1.The novelty scores provided in the reviewers' reports can serve as the evaluation standard for a paper's novelty.Currently, a research (Wu et al., 2024) have been conducted using open peer review reports to analyze the consistency of text scores.We believe that the novelty scores given by reviewers can serve as a reference standard for evaluating the novelty of a paper.</p>
<p>To identify and analyze the optimal sections or combinations of sections for evaluating the novelty of academic papers, we obtained all peer review https://iclr.cc/Conferences/2024/ReviewerGuide.These reviewer report instructions clearly require reviewers to evaluate or grade the significance and novelty of the paper.</p>
<p>2 https://neurips.cc/Conferences/2022/CallForPapers 3 https://reviewer.elifesciences.org/author-guide/editorial-process 4https://peerj.com/benefits/review-history-and-peer-review/ 5 https://openreview.net/about 6https://f1000research.com/aboutreports and submitted paper PDFs from the ICLR 2022 and ICLR 2023 conferences on the OpenReview platform.These peer review reports require reviewers to provide novelty scores for the papers.Based on this dataset, we conducted an empirical study on predicting the novelty of papers using different combinations of academic paper sections.Additionally, we validated the performance of LLMs on this task.</p>
<p>The study is driven by the following three research questions.RQ1: Which combinations of sections yield novelty scores that most closely align with the ground truth scores?By identifying which combinations of sections yield prediction scores closest to the actual novelty scores, we can assess the model's accuracy.This means that, based on these combinations, the model can predict results that more closely align with the true novelty scores of a paper, thereby reducing bias and errors.Additionally, this approach helps validate the model's effectiveness under different combinations, revealing which configurations best replicate human reviewer judgments, thus ensuring the model's rationality and credibility.</p>
<p>RQ2: Which combinations of sections are most effective in predicting the novelty scores of academic papers?Different sections of a paper (such as Introduction, Methods, Results) may have varying impacts on novelty scores.Understanding how these sections can be combined to produce the most accurate predictions not only aids in improving novelty scoring algorithms but also provides deeper insights into the relationship between paper structure and content.</p>
<p>RQ3: Which combinations of sections should be prioritized when automatically evaluating the novelty of a paper?For large-scale automated review systems, it is crucial to prioritize which sections of a paper should be focused on during the automatic evaluation process, as this can enhance both the efficiency and accuracy of the review.By prioritizing the sections most representative of novelty, the influence of subjective human factors can be minimized, making the evaluation more objective and fairer.</p>
<p>The main contributions of this paper are reflected in the following three aspects.</p>
<p>Firstly, we used the novelty scores from peer reviews as a benchmark to fine-tune current popular pre-trained language models (PLMs) designed for long texts to predict novelty scores.We also validated the effectiveness of using different section structures as input.</p>
<p>Secondly, we conducted a small-scale test on novelty score prediction using prompt-based methods on LLMs.Furthermore, we analyzed the perfor-mance of the LLMs in this task under different chapter combinations, as well as the consistency between generated novelty scores and grounded scores.</p>
<p>Thirdly, the results indicate that fine-tuned PLMs outperform LLMs in predicting novelty scores, though their performance is not yet satisfactory.Furthermore, our findings suggest that the introduction, results, and discussion sections are more beneficial for automatic novelty score prediction tasks.</p>
<p>All the data and source code of this paper are freely available at the GitHub website: https://github.com/njust-winchy/SC4ANM.</p>
<p>Related work</p>
<p>In this section, we will report related work about our research.Firstly, we present relevant work in the field of identification of academic paper structure.Subsequently, we delve into literature concerning LLMs for reviewing, followed by an overview of studies investigating novelty assessment methods.</p>
<p>Novelty and other relevant concepts</p>
<p>To formally define novelty, we first clarify the distinctions between novelty and other related concepts, such as innovation, disruptiveness, and originality.</p>
<p>In the academic community, novelty is currently defined in several ways.One definition (Foster et al., 2021) defines novelty as the degree of difference between a new scientific article and the existing body of scientific literature.Another definition (Arts et al., 2021) defines novelty as the uniqueness of specific knowledge elements, where the inclusion of new knowledge elements in a scientific article indicates that it conveys novel information.In addition, there is another definition (Boudreau et al., 2016) considers novelty as the result of a new combination of knowledge elements.Overall, the novelty of scientific articles can be unified as the quality of presenting new information within these articles.</p>
<p>Innovation (Rogers, 1998) can be defined as the transformation of new ideas into practical, valuable, or sustainable outcomes.Unlike mere novelty, innovation must also generate impact in practice.Novelty is merely the first step toward innovation (Runco and Jaeger, 2012), which further requires the realization of practicality, impact, and value.Novelty can emerge in various settings, such as universities, whereas innovation primarily takes place within firms operating in the commercial sector (Fagerberg, 2006).</p>
<p>Disruptiveness (Funk and Owen-Smith, 2017) refers to research or discoveries that fundamentally alter existing academic paradigms, research methods, or core theories within a field.This change not only impacts academic discourse but may also influence societal, industrial, and even policy-level transformations.Disruptiveness (Wang, 2024) is often driven by research outcomes that break conventional norms and propose revolutionary new ideas.In contrast to novelty, disruptiveness not only emphasizes the origins of scientific research but also considers its utility and impact (Leibel and Bornmann, 2024).</p>
<p>Originality (Shibayama and Wang, 2020;Hou et al., 2022) can be defined as the generation of new ideas, methods, conclusions, and other valuable outputs that depart from existing knowledge, or as a catalyst for further innovation.In practical measurement, distinguishing between originality and novelty is often challenging, as originality is typically implicit in research papers (Guetzkow et al., 2004).Consequently, in most cases, originality and novelty are frequently used interchangeably (Uzzi et al., 2013;Shibayama and Wang, 2020;Wang, 2024).</p>
<p>In summary, novelty forms the foundation of both innovation and disruptiveness and is synonymous with originality.Without novel ideas or discoveries, it is impossible to develop innovative or disruptive outcomes.While scholars may offer varying interpretations of novelty, there is a consensus that in scientific papers, novelty refers to the intrinsic quality of presenting new knowledge.Specifically, a novel scientific paper addresses new research questions, methodologies, results, theories, or the reorganization of existing knowledge elements.Therefore, in this paper, we explore the prediction of novelty scores from the perspective of different combinations of sections.Specifically, various sections of a scientific paper, such as the Introduction, Methods, Results, and Discussion, often carry different types of information, which play a crucial role in presenting novelty.</p>
<p>Novelty measurements of scientific publications</p>
<p>Novelty is one of the key criteria for evaluating the quality of academic papers.In the current community of scholars, novelty is defined as the reorganization of existing knowledge components in an unprecedented manner (Schumpeter, 2006;Nelson, 1985).Currently, scholars assess the novelty of a study by evaluating the new combinations of references, considering that knowledge reorganization is based on the content of the references section.Uzzi et al. (Uzzi et al., 2013) analyzed 17.9 million papers on Web of Sci-ence to study the relationship between reference combinations and citation counts.They found that the most impactful papers combine typical earlier works with new, unique combinations.This discovery helps distinguish between novel and less novel papers.Wang et al. (Wang et al., 2017) explored the relationship between the novelty of scientific research, defined by firsttime combinations of referenced journals, and its long-term impact.They find that highly novel research offers significant benefits but also involves higher risks, often faces delayed recognition, and is published in lower Impact Factor journals.Matsumoto et al. (Matsumoto et al., 2021) applied a novelty indicator to quantify the reference similarity between a focal paper and pre-existing papers within the same domain across various fields of natural sciences.They proposed a new method for identifying papers belonging to the same domain as the focal paper using only bibliometric data.Shibayama et al. (Shibayama et al., 2021) developed a more integrated method that utilizes both references and the content of a paper.In their approach, they quantify the semantic distance of references to assess the novelty of a given paper.</p>
<p>However, although the reference is easy to understand and use, it remains unclear to what extent they can serve as a source of inspiration for academic papers (Tahamtan and Bornmann, 2018).Additionally, references in papers are provided solely by the authors themselves, without any oversight mechanism to ensure their quality.Recently, methods for discovering innovative content based on text analysis have been gaining popularity.Luo et al. (Luo et al., 2022) introduced a novel approach to measuring the novelty of papers from the perspective of problem method combinations.They proposed a semantic novelty measurement algorithm based on term semantic similarity, and evaluated the effectiveness of the method through case studies and statistical analysis.Yin et al. (Yin et al., 2023) developed a word embedding model using machine learning to extract semantic information related to elements of knowledge innovation from textual data.Jeon et al. (Jeon et al., 2023) proposed an analytical framework that uses paper titles to measure the novelty of scientific publications.Chen et al. (Chen et al., 2024) conducted an in-depth investigation into the relationship between the institutional composition of author teams and the novelty of academic papers, using fine-grained knowledge entities to measure the novelty of the papers.Through case studies, they demonstrated that this framework is a useful supplementary tool.Liu et al. (Liu et al., 2024) explored the pursuit of scientific novelty in doctoral dissertations by Ph.D. students and evaluated the gender-related differences in this process.Wang et al. (Wang et al., 2024) proposed MNSA-ITMCM framework integrates topic modeling and a cloud model to measure novelty in scientific articles by combining semantically informed topics and quantifying novelty to improve accuracy, demonstrated through empirical evaluations in biomedical and computer science domains.</p>
<p>The aforementioned research methods have demonstrated their effectiveness to some extent, indicating that text analysis-based approaches can be used to evaluate novelty.However, these methods only utilized parts of the paper, such as titles, abstracts, or knowledge entities, for their assessments and did not consider the section content of the paper.The novelty of a paper cannot be accurately determined based on brief excerpts alone.During the peer review process, reviewers typically assess the novelty of a paper by reading its entire content or most of its sections.Therefore, it is crucial to explore which sections should be given priority when evaluating the novelty of a paper.This paper simulates human reading of different sections using artificial intelligence and uses the novelty scores provided by reviewers as a benchmark to investigate the importance of various sectional content combinations.</p>
<p>Large Language Models for Reviewing</p>
<p>With the rapid advancement of artificial intelligence and natural language processing technologies, LLMs (Brown et al., 2020;Ouyang et al., 2022), particularly those supported by transformer-based architectures and pre-trained on massive datasets, have gradually come into the public eye.With the successive releases of ChatGPT and GPT-4 (OpenAI, 2024) by OpenAI, as well as Llama (Touvron et al., 2023a) and others (Meta, 2024;Touvron et al., 2023b;Chowdhery et al., 2023), these LLMs have demonstrated powerful language generation and understanding capabilities, attracting significant research interest from scholars in the academic community.(Patsakis et al., 2024;Shafee et al., 2024;Caruccio et al., 2024) Recently, research on using LLMs for peer review has been gaining increasing popularity.Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality.They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback.Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the "better" pa-per.They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers.Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness.The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks.Zhou et al. (Zhou et al., 2024) conducted a comprehensive evaluation to determine whether LLMs can be qualified and reliable reviewers.They concluded that it is premature for LLMs to serve as automated scientific paper reviewers.Although there is potential for obtaining useful and accurate results, their current capabilities are not yet reliable enough.Robertson (Robertson, 2023) conducted a preliminary study on using GPT-4 to assist in the peer review process, providing initial evidence that artificial intelligence can effectively facilitate this process.Gao et al. (Gao et al., 2024) proposed an efficient two-stage review generation framework REVIEWER2 that simulates the distribution of potential aspects a review might address.Additionally, they generated a large-scale peer review dataset comprising 27,000 papers and 99,000 reviews based on this framework.</p>
<p>However, these studies are all macro-level investigations, focusing on the overall evaluation of academic papers.Our research, in contrast, focuses specifically on the novelty of academic papers and examines the ability of LLMs to assess the novelty of academic papers.</p>
<p>Identification of Academic Paper Structure</p>
<p>The IMRaD model is a classical system, proposed earlier and widely used in scientific literature (Sollaci and Pereira, 2004;Nair and Nair, 2014).It divides the structural functions of academic articles into four parts: Introduction, Methods, Results, and Discussion.The purpose of identifying the sectional structure of academic papers is to determine their structural functions, specifically, to classify text segments (sentences, paragraphs, or sections) into their respective functional categories.Lu et al. (Lu et al., 2018) proposed a clustering method based on domain-specific structures using high-frequency section headings in scientific documents to automatically identify the structure of scientific literature.They applied the proposed method to two tasks: academic search and keyword extraction, achieving good performance.Li et al. (Li and Wang, 2021) proposed a hybrid model that considers both section headings and the main text to automatically identify general sections in academic literature.Ma et al. (Ma et al., 2022) utilized section headings to identify the structural functions of academic articles, incorporating relative position information and contextual information.</p>
<p>The aforementioned studies have made significant progress in the task of sectional structure identification.Subsequent research on this topic has generally included relevant applications.Cohan et al. (Cohan et al., 2018b) proposed a novel hierarchical encoder for modeling the discourse structure of documents, applied across two large scientific paper datasets.Ji et al. (Ji et al., 2019) proposed using deep learning to automatically identify the functional structure of academic texts based on section content, laying the groundwork for research on the distribution of reference locations.Qin et al. (Qin and Zhang, 2023) explored which sections of academic articles reviewers focus on most by analyzing the sectional structure, as well as identifying the specific content that reviewers pay attention to.Although the above research has done a lot of work on identifying the structure of academic papers, we believe that identifying the structure of academic papers is still a challenging task.Zhou and Li (Zhou and Li, 2020) investigated the problem of section identification in academic papers within the context of Chinese medical literature.They employed effective features from classical machine learning algorithms to address this issue.</p>
<p>In our approach, we use deep learning models and LLMs to identify the sectional structure, and we perform consistency checks between their results to ensure the accuracy of the identification process.</p>
<p>Methodology</p>
<p>In this section, we will introduce our data collection and preprocessing process, as well as the novelty score prediction task.Subsequently, we will provide a detailed explanation of the methods applied for section structure identification and novelty score prediction.</p>
<p>The following is the research method of this paper.Figure 2 provides an overview of the methodology used in this study, which includes Section Structure Identification, Fine tuning PLMs for novelty score prediction, and Generate novelty score prediction using LLM.</p>
<p>Data collection and preprocessing</p>
<p>We obtained our peer review data from the OpenReview platform.The International Conference on Learning Representations (ICLR) is a premier Note: The combination of section structure takes the introduction (I), methods (M), and results (R) as an example.The green boxes represents the text of the corresponding section that has been identified.The blue boxes indicate that the PLM's predicted classification value for the text is greater than 0.8.The orange boxes represent cases where the PLM's classification result is less than 0.8, but the LLM's prediction is consistent with it.The yellow boxs indicate that PLM's predicted classification value for the text is less than 0.8 and inconsistent with the LLM, require human correction.Red boxs are the final result.conference in the field of machine learning.We wrote a web crawler code to retrieve a total of 8183 ICLR papers from ICLR 2022 and 2023, each containing peer review comments.The reason for selecting papers from these two years as the data source is that the review reports for these years require reviewers to provide novelty scores, as shown in Figure 1.Additionally, another reason for selecting data from these two years as the source for this study is that the common section types in ICLR papers closely align with the IMRD structure typically found in natural sciences.In the data we collected, all ICLR papers include an introduction, the section corresponding to the methods is generally labeled as "Methodology" or the specific name of the proposed method, the section corresponding to the results is usually related to experiments or model comparisons, and the section corresponding to the discussion typically includes analysis, discussion, and conclusions.Therefore, we can map these sections to the corresponding IMRD structure using a section structure recognition method in Section 3.3.It is also worth noting that the methods proposed in this paper are applicable in any domain where the sections of a paper can be classified.</p>
<p>The peer review reports for ICLR 2022 and 2023 included the review text, Correctness, Technical Novelty and Significance, Empirical Novelty and Significance, Recommendation, and Confidence.Among these, the distribution of Technical Novelty and Significance (TNS) scores ranges from 1 to 4, denoted as 1: The contributions are neither significant nor novel, 2: The contributions are only marginally significant or novel, 3: The contributions are significant and somewhat new.Aspects of the contributions exist in prior work and 4: The contributions are significant, and do not exist in prior works, respectively.As ICLR is a conference in the field of computer science, we have chosen TNS as the criterion for predicting novelty scores.Each paper receives evaluations from at least three reviewers, and each reviewer assigns a TNS score.To obtain labels suitable for model training, we aggregate the scores provided by each reviewer by summing them up and then dividing by the total number of reviewers.However, discrepancies may arise when multiple reviewers have differing opinions, such as one assigning a score of 4 and another a score of 1.To address this issue, we identified the maximum and minimum scores in each review report and exclude instances where the difference exceeds 1, signifying reviewer disagreement.These cases are subsequently removed from the dataset.After the consistency processing of the scores mentioned above, there are ultimately 6,094 papers with peer review reports that have consistent TNS scores.</p>
<p>Furthermore, to parse the acquired academic paper PDFs, we utilized the GROBID (2008-2022)7 and S2ORC (Lo et al., 2020) tools to parse the PDFs into JSON format.In the large-scale dataset constructed by Gao et al. (Gao et al., 2024), the parsed content data of papers from ICLR 2022 and 2023 is included, eliminating the need for custom rules to extract paper content from JSON files.We matched the parsed paper abstracts with the dataset created by Gao et al. (Gao et al., 2024) and ultimately matched the content of 3,500 papers.We investigated the reason for the significant reduction in the number of papers and found that many PDF files were noneditable, meaning they could not be parsed, or the matching process failed.</p>
<p>The final distribution of novelty scores is shown in Table 1.From the Table 1, it is evident that the counts for scores 1 and 4 are relatively low.Predicting directly with such an extremely imbalanced label distribution may result in biased prediction outcomes.Therefore, to address this issue and considering the importance of a novelty score of 4, we reclassified the scores by combining scores 1 and 2, retaining score 3 and score 4 as a separate category.According to the descriptions associated with each score, the novelty of paper is classified into three categories: 0: basic novel, 1: moderate novel, 2: highly novel.</p>
<p>Problem definition</p>
<p>In this work, we introduce a new task called optimal section combinations for automated novelty measurement (SC4ANM).The objective of SC4ANM is to predict the novelty scores of academic papers based on varying combinations of their sections.The task aims to evaluate how various combinations of a paper's sections impact to its novelty score.Formally, given an academic paper, let P = (S 1 , S 2 , ..., S n ) represent the set of sections of the paper, where each section corresponds to a part of the paper (e.g., Introduction, Methods, Results, Discussion, etc.).A section combination C is defined as a subset of these sections, C ⊆ P selected for analysis.For example, combinations such as Introduction and Methods, Introduction and Results, or even Introduction only are valid section combinations.A set of corresponding labels L = {l 0 : 0, l 1 : 1, l 2 : 2} represents the novelty scores defined in the end of Section 3.1.The goal of SC4ANM is to develop a classification model f that assigns predefined novelty scores l {0,1,2} based on the content of different section combinations selected.</p>
<p>Section Structure Identification</p>
<p>Firstly, we need to identify the structure of the parsed paper, dividing the main text into introduction, methods, results, and discussion.</p>
<p>However, not all academic papers adhere to the IMRaD structure, thus necessitating the identification of section structures in the parsed PDF content.The parsed paper data not only includes the main text of the papers but also contains some additional content, such as page number and page header, quotes or excerpts etc.Therefore, it is necessary to identify the main text of the papers to remove any extraneous information.We utilized an academic text classifier, fine-tuned on linguistic academic publications8 , as the identification model to recognize the main content in our paper parsed data.We saved the extracted main text of academic papers in the form of paragraphs or sentences.</p>
<p>Then, we perform section structure identification on the extracted main <Main text of academic papers> This is an academic text, which could be an introduction, methods, results or discussion.Please reply which section it pertains to: introduction, methods, results or discussion.</p>
<p>Prompt: text.Cohan et al. (Cohan et al., 2018b) provided two datasets of scientific papers.The datasets are sourced from the ArXiv and PubMed OpenAccess repositories, both of which provide the names of sections and their corresponding content.We extracted all IMRaD section content from the two datasets , totaling 300,000 entries, and divided them into training, validation, and test sets in an 8:1:1 ratio.We then fine-tuned the SciBERT (Beltagy et al., 2019) on this data to train a four-class classifier aimed at determining which section structure a given piece of academic text belongs to.The final model achieved a classification accuracy rate of 91%.We believe that relying solely on the fine-tuned PLM does not achieve the best recognition results.Therefore, we also used LLaMA3 (Meta, 2024) for secondary recognition of the extracted main text.The specific prompt is shown in Figure 3, the content within angle brackets represents the main text of the paper that require section structure recognition.If the predicted value for a given class from the PLM exceeds 0.8, we consider the prediction to be correct.Otherwise, further validation of the classification result is required.The threshold is set to 0.8 because we consider section structure identification to be a crucial task that should prioritize minimizing false positives.In other words, it is preferable to conduct further validation rather than risk incorrectly classifying sections as correct.Based on this application requirement, we believe that setting the threshold to 0.8 is a reasonable choice.We employed LLama3 for secondary validation.When the predictions from the trained model are consistent with the results from LLaMA3, we consider the prediction to be correct.When We fine-tuned five PLMs designed for handling long texts to accomplish the task of novelty score prediction.These models include Longformer (Belt-agy et al., 2020), BigBird (Zaheer et al., 2020), LongT5 (Guo et al., 2022), LED (Longformer Encoder-Decoder) (Shen et al., 2023), and the Longformer version of SciBERT9 .We divided the academic paper data, with identified section structures, into training, validation, and test sets in an 8:1:1 ratio, with 2500, 350, and 350, respectively.Next, as illustrated in the lower left corner of Figure 2, we input different combinations of section structures (such as "Title+Abstract", "Introduction", "Introduction+Methods", etc., totaling 16 combinations) to fine-tuned the PLMs.The results are then fed into an MLP to obtain the final prediction, which is the three-class classification of the novelty score.The detailed process is shown in Figure 4.</p>
<p>Generate novelty score prediction using LLM</p>
<p>Note:The number of section combinations ranges from 1 to 4; here, we use three section combinations as an example.</p>
<p>You are a helpful assistant.You need to perform a text classification task with three labels [0: basic novel, 1: moderate novel, 2: highly novel].The following is an academic paper text, assign corresponding labels to it: <Section1 name, Section1 content> <Section2 name, Section2 content> <Section3 name, Section3 content> Just return the label number.In addition to fine-tuning PLMs for novelty score prediction, we also tested the performance of LLMs in predicting novelty scores under different section structure combinations.From the data statistics in Table 1, it can be observed that the number of papers with a novelty score of 4 is relatively low.Therefore, we randomly selected 40 papers with a novelty score of 4 as the experimental subjects.Additionally, the number of papers with other scores was made equal to that of the score 4 papers, also selected randomly.Notably, to ensure the diversity of experimental data, we performed random sampling of the papers again for each different combination of section structures used as input.For the selection of LLMs, we chose GPT-3.5 and GPT-4o as the models for our experiments.By calling their APIs and using prompt-based learning, we obtained predictions for the novelty scores of academic papers.The specific prompt is shown in Figure 5.It is important to note that although we instructed the model to return only numerical classification labels in the prompt, GPT-3.5 still generated some additional content.Consequently, we performed a secondary verification of GPT-3.5'soutputs.GPT-4o did not exhibit this issue.</p>
<p>Result analysis</p>
<p>In this section, we present the results of predicting novelty scores using different section structures and explore the following three research questions based on these results.</p>
<p>Evaluation Metrics on the SC4ANM task.</p>
<p>The Accuracy (Acc) and F 1 -score were selected to evaluate classification performance.The formulas used are as follows:
Accuracy = T P i The total number of sample (1)
Where T P i is the number of correct predictions for category i (0: basic novel, 1: moderate novel, 2: highly novel).The calculation method for the F 1 score of each category i is as follows:
P recision i = T P i T P i + F P i
(2)
Recall i = T P i T P i + F N i (3) F 1i = 2 × P recision i × Recall i P recision i + Recall i(4)
Where i represents the category index (0, 1, 2), T P i is the number of correct predictions for category i, F P i is the number of incorrectly predicted as category i but does not actually belong to class i, F N i represents the number of instances that actually belong to category i but are incorrectly predicted as another category.Since the weighted average F 1 score is calculated by weighting each category according to its sample size, it is more suitable for imbalanced datasets.Therefore, we utilize the weighted average F 1 score, the weight for each category i:
W eighted F 1 = c i c × F 1i(5)
Where c i represents the number of instances for category i, c denotes the total number of samples, F 1i is the F 1 score for category i.</p>
<p>In addition to utilizing common metrics for evaluating classification performance, we also employed the Pearson, Spearman and Kendall's tau correlation coefficients, which are typically used to assess the score prediction capability of models.The formula for calculating the Pearson correlation coefficient is as follows:
r = Cov(x, y) σ x σ y (6)
Where Cov is covariance, σ is standard deviation.The Spearman formula is applied:
r s = 1 − 6nd 2 n 2 − 1 (7) d 2 = 1 n n i=1 d 2 i (8)d i = r x (i) − r y (i)(9)
Where r x (i) and r y (i) represent the rank of x i and y i , n is the total number of samples.The formula for Kendall's Tau correlation coefficient is as follows:
τ = 2(C − D) n(n − 1)(10)
Where C represents the number of concordant category pairs (i.e., when x i &gt; x j and y i &gt; y j , or x i &lt; x j and y i &lt; y j ), D represents the number of discordant category pairs (i.e., when x i &gt; x j and y i &lt; y j , or x i &lt; x j and y i &gt; y j ), n is the total number of samples.</p>
<p>Traditional novelty measurement methods as baselines</p>
<p>We chose the novelty measurement method proposed by Shibayama et al. (Shibayama et al., 2021) as the representative of traditional approaches for comparison as the baseline.Their method calculates the novelty score by determining the semantic distance between the word vectors of the paper's title and those of the references' titles.We then simply present their novelty calculation method.First to calculate the distance between each pair of cited documents, the cosine distance between i − th and j − th references(1 ≤ i ≤ j ≤ N ) is given by:
d ij = 1 − v i • v j |v i | |v j | (11)
Where v is mean of word embeddings of all words included, N is number of the reference.Then calculate the distance scores(N ovel q ):
N ovel q = R −1 N q 100 (12)
Where R(d ij )is the ordinal rank of d ij of all the distances of N (N − 1)/2 reference pairs, q represents the percentile rank of the distance score, with a range of [0,100].According to the findings in Shibayama et al.'s (Shibayama et al., 2021) study, setting q to 100 yields the best results; therefore, we also set q to 100.Since the computed results N ovel q fall within the range [0,1], we apply a linear scaling method to map the results to the range [0,2] and round them to the nearest integer:
scaled value = 2 × N ovel q − min(value list ) max(value list ) − min(value list ) (13)
Where value list is the list of all calculated distance scores N ovel q , and min(value list ) and max(value list ) are the minimum and maximum values in the list, respectively.</p>
<p>Correlation test on novelty score prediction with different section com-</p>
<p>binations We will answer RQ1 by examining the correlation between the prediction and the ground truth in this section.</p>
<p>Correlation analysis between prediction by PLMs and ground truth</p>
<p>In addition to the traditional metrics of accuracy and absolute difference for the score prediction task, we also calculate Pearson, Spearman, and Kendall's tau correlation coefficient.These three-correlation coefficient metrics are commonly used to evaluate the score prediction capabilities of models, as demonstrated in studies on using LLMs to evaluate abstractive summaries (Shen et al., 2023) and machine translations (Kocmi and Federmann, 2023).The result of correlation coefficient between PLMs prediction and ground truth is shown in Table 2.  From the results in the Table 2, we can be observed that the predicted novelty scores based on all section combinations positively correlate with the actual scores.We attribute this to the fact that various sections of an academic paper, including the title and abstract, contain information that aids in predicting the novelty score.SciBERT achieved the highest correlation coefficient, which is closely related to its training corpus, enabling it to make relatively accurate predictions for novelty score prediction.Furthermore, we can observe that the prediction and true score correlation coefficients are highest when using combinations of three-section structures as input, especially IRD or any combination that includes the introduction.This indicates that for the task of predicting novelty scores, combinations of three-section structures, particularly those including the introduction, are more effective.It is worth noting that while the correlation coefficient for IMRD is not the highest, it remains competitive with other high-accuracy combinations.This suggests that when the model uses IMRD as input, the information from various section structures can influence its judgment, leading to incorrect predictions.Although these predictions are inaccurate, the difference from the true scores is relatively small, resulting in lower accuracy but relatively higher correlation coefficients.</p>
<p>Correlation analysis between prediction by LLMs and ground truth</p>
<p>To further evaluate the ability of LLMs in predicting novelty scores, we calculated three correlation coefficients between the generated predictions and the real scores.The detailed results are presented in Table 3.</p>
<p>First, we analyzed the results of GPT-3.5.The results indicate that GPT-3.5'spredictions for many section combinations exhibit a negative correlation with the real scores.This suggests that GPT-3.5 is not adept at the task of novelty score prediction, likely due to the hallucination problem inherent in LLMs.For the results generated by GPT-3.5, we need to further process many additional contents generated that is consistent with our requirements.This may also be the reason for the negative correlation in the correlation coefficient calculation results of GPT-3.5.Furthermore, we observe that the correlation coefficients for the IMR and R section combinations are relatively high.This indicates that using a combination of three-section structures is relatively suitable for predicting novelty scores.</p>
<p>Then, we analyze the results of GPT-4o.From the correlation coefficient calculations, we see that nearly all results exhibit a positive correlation.This indicates that GPT-4o is capable of understanding and performing the task to a certain extent.Compared to the accuracy and F1 scores, the correlation coefficient results for GPT-4o predictions are more impressive.Unlike the PLMs, the section combinations that include the Results section show higher correlation coefficients.We believe this indicates that GPT-4o places greater emphasis on the impact of the Results section on novelty scores.Additionally, we think the Discussion section overlaps with the Introduction, which may explain why the IR and RD combinations outperform others.The poorer performance of the IRD combination is likely due to the confusion caused by the overlap between the Introduction and Discussion sections.Additionally, we can observe that the IMR and IMD combinations also exhibit competitive results, indicating that the Methods section holds considerable significance as well.</p>
<p>Novelty score prediction performance with different section combinations</p>
<p>We answer RQ2 by analyzing the performance of the PLMs and LLMs in the novelty score prediction task in this section.</p>
<p>Results of PLMs with different section combinations</p>
<p>To address this research question, we fine-tuned five PLMs on the novelty score prediction task using different section combinations.The training process spanned 10 epochs with an initial learning rate of 0.0001, which gradually decreased.To prevent overfitting, we implemented an early stopping strategy, terminating training if the F1 score on the validation set did not improve for three consecutive epochs.The final results are shown in Table 4.</p>
<p>As shown in Table 4, the performance of the five PLMs on the novelty score prediction task does not differ significantly.However, SciBERT slightly outperforms the other models, likely because it is trained on scientific-related corpora.While the other pre-trained models also include academic papers in their training data, their performance may be affected by the presence of other non-academic corpora.Furthermore, the results show that input combinations including the introduction tend to perform better than other combinations.We attribute this to the fact that authors typically articulate the contributions or innovative aspects of their papers in the introduction, which are relevant to assessing novelty.These details likely influence the model's judgment of the novelty score.Moreover, considering the performance of each model, input combinations with three sections yield the highest accuracy and F1 scores, with the IRD combination performing the best, followed by IMR and IMD.We believe that while the introduction already contains significant information relevant to predicting novelty scores , such as the main contributions proposed by the author, the inclusion of additional sections further enhances the model's understanding of the task, leading to improved results.Finally, we observe that model performance of IMRD is not good.We think that the content and knowledge encompassed in the full text of a paper are too extensive for current PLMs to capture knowledge effectively for the task of novelty score prediction.Furthermore, we observed that the highest accuracy achieved was only 0.682.Therefore, we conclude that the task of novelty score prediction poses a significant challenge for PLMs.</p>
<p>Result of novelty score prediction using LLM</p>
<p>We tested the performance of two LLMs (GPT-3.5 and GPT-4o) on the task of predicting novelty scores based on different section combinations.All results were obtained in the form of zero shot.To mitigate the randomness of the generated results, we conducted five predictions and selected the most frequently occurring score.The results are presented in Table 5.</p>
<p>From the results, we can see that the novel score prediction task also poses challenges for these two LLMs, with the highest accuracy only reaching 0.4.We attribute the suboptimal performance of the LLMs to several factors.Firstly, in a zero-shot scenario, LLMs struggle to grasp the patterns associated with novelty score prediction, leading to erroneous judgments.Secondly, the training corpus of these models may not include data relevant to the novelty score task.Thirdly, the models' limited reasoning capabilities hinder their ability to infer the task content based on the provided prompts.</p>
<p>From the perspective of different section combinations, the results for both GPT-3.5 and GPT-4o are quite unstable.GPT-3.5 performs better when the input is IR, while GPT-4o performs better with IMRD as the input.</p>
<p>We believe this instability is related to the tendency of LLMs to generate favorable responses, as they are inclined to provide satisfactory answers or predictions.Finally, for the results generated from LLM, we examined the generated content and found that LLM tends to give the highest novelty score (highly novel).We think this tendency contributes to the low accuracy and F1 scores.</p>
<p>Comparison with a traditional novelty evaluation method</p>
<p>In this section, we compare the best-performing section combinations (SciBERT+IRD) with a traditional novelty evaluation method mentioned in Section 4.2.The results are shown in Table 6.As we can observe, our method outperforms the traditional method in terms of both accuracy and F 1 score, as well as the results of the correlation calculations.This demonstrates that using comprehensive text information to assess a paper's novelty is more effective than the traditional citation-based methods.Furthermore, the results indicate that the comprehensive text information indeed contains more content that contributes to novelty evaluation, which is also supported by the findings in Sections 4. 3 and 4.4.This further underscores the potential of leveraging a broader range of textual data for novelty detection.</p>
<p>Section association analysis for automated paper novelty assessment</p>
<p>In this section, we will comprehensively analyze the results of PLMs and LLMs to answer RQ3.</p>
<p>Based on the results from the first two research questions, we can proceed to discuss the third research question.As seen in the results from Table 4 and Table 2, models that include the introduction section consistently perform better than other combinations.Therefore, we think the introduction to be a crucial section for evaluating the novelty of a paper.Furthermore, we observe that the combination of the introduction and results sections also achieves notable performance, indicating that this combination is beneficial for evaluating the novelty of a paper.Although these combinations did not perform well with the LLMs, Tables 5 and 3 show that inputs including the results section perform relatively well.Therefore, we also consider the results section to be essential for assessing the novelty of a paper.Furthermore, we observe that the combination of the introduction, results, and discussion sections yields the best performance with the PLMs.Similarly, the combination of results and discussion also performs well with the LLMs.Based on this, we can infer that the discussion section serves as a valuable supplement to the assessment of a paper's novelty, building on the information provided in the introduction and results sections.Although the methods section is a crucial section for evaluating a paper's novelty, its text description is often abstract, and the content related to novelty can be challenging for machines to comprehend.Therefore, it may not be suitable for automated assessment of a paper's novelty.We believe that evaluating the novelty of the methods section requires external knowledge for support, such as the opinions of peer review experts.In summary, we believe that the combination of the introduction, results, and discussion sections is crucial for the automated assessment of a paper's novelty.</p>
<p>Case study</p>
<p>As shown in Figure 6 and 7, we conducted case studies on PLM (Longformer version of SciBERT) and LLM (GPT-4o) based on different section combinations inputs.We treat the model's output and the ground truth as two rows in a matrix, with the top row representing the labels and the bottom row representing the predictions.The letter combinations above each matrix represent the abbreviations for different section structures: I stand for Introduction, M for Methods, R for Results, D for Discussion, T for Title, and A for Abstract.For example, IM represents the combination of Introduction and Methods.When the true label is 0, the range of discrepancies is [−2, 0]; when the true label is 1, the range is [−1, 1]; and when the true label is 2, the range is [0, 2].For example, if the true label is 0 and the prediction is 2, the discrepancy is -2, resulting in a darker color in the plot.</p>
<p>We randomly selected two papers from each of the three novelty score categories, making a total of six papers for case studies.First, we examined the predictions of the PLM under different section combinations.From the Figure 6, it is evident that the PLM struggles to accurately predict high novelty scores but can generally make correct predictions for medium and low Note: The x axis represents the true labels, and the numbers in different colors within each plot indicate the difference between the prediction and true labels.The legend is on the right, with darker colors indicating a greater difference between the predicted labels and the true labels.novelty scores.Regarding the proximity of score predictions, section combinations that include the Introduction achieve better results.Combinations of three sections contain Introduction perform better than others, particularly the IRD combination.Despite predicting high novelty scores incorrectly, it still provides a proximate score of 2. Secondly, we conducted an examination of LLM in Figure 7, the prediction results indicate that the LLM is proficient in judging papers with novelty scores of 1 and 2. However, for many papers with a novelty score of 0, the LLM often predicts a score of 2. This suggests that the LLM tends to provide favorable results and struggles to identify papers with low novelty.Considering different chapter structures, the predictions for I, IMD, and RD are the closest to the ground truth.Although the number of well-performing section combinations varies, it is evident that the introduction, results, and discussion sections are beneficial for predicting novelty scores.Furthermore, the inclusion of the methods section, supplemented by the introduction and discussion, allows the LLM to better understand the paper's content and make more accurate novelty score predictions.</p>
<p>In summary, through case studies involving PLM and LLM, we can believe that the combination of introduction, results, and discussion sections is more effective for novelty score prediction.</p>
<p>Note: The x axis represents the true labels, and the numbers in different colors within each plot indicate the difference between the prediction and true labels.The legend is on the right, with darker colors indicating a greater difference between the predicted labels and the true labels.</p>
<p>Discussion</p>
<p>In this section, we will discuss the implication of our study on theoretical and practical, and limitation of our study.</p>
<p>Implication 5.1.1. Theoretical Implication</p>
<p>In this study, we collected all the PDF data of papers and their corresponding peer review reports from ICLR 2022 and 2023.We then parsed the PDFs and used deep learning models to identify the main text of the papers.Subsequently, we employed deep learning models and LLMs to recognize the structure of the identified main text.Then, we used the novelty score given by the reviewers in the review report as the standard, and fine-tuned the PLMs with different section combinations as inputs.We also verified the performance of PLMs with different section combinations as inputs.Additionally, we conducted small-sample tests to assess the performance of LLMs when provided with different section combinations as content prompts.</p>
<p>Based on the results we obtained, both PLMs and LLMs can, to some extent, combine section structure information to predict novelty scores in long-text processing; however, each has its limitations.First, for PLMs, although their accuracy and F 1 scores are relatively high, they appear to be less sensitive to papers with high novelty scores and fail to make accurate predictions in such cases.We believe that the relatively small number of papers with high novelty scores is one reason for this phenomenon.Regarding LLMs, although their performance across various metrics is not ideal, they seem to perform better in predicting papers with high novelty scores.We speculate that this may be because LLMs, which are primarily designed for conversational tasks, tend to exhibit a more favorable and engaging tone.Based on this, we argue that relying solely on PLMs or LLMs for automatic novelty score prediction could lead to polarized results.Since both PLMs and LLMs have their respective strengths, we believe that combining the advantages of both through effective methods may provide a more optimal solution for automatic novelty score prediction.</p>
<p>Finally, our results indicate that the optimal section combination for predicting novelty scores is the introduction, results, and discussion.We believe that the introduction section contains the main contributions and innovations proposed by the authors, which are crucial for assessing novelty.The results and discussion sections provide specific explanations of these contributions and further summarize the findings, enabling the model to better understand the task of predicting novelty scores and thus achieve the best performance.While our findings suggest that certain section combinations may enhance classifier performance, these results should not be interpreted as a definitive measure of novelty.Instead, they highlight text-based features that may signal novel contributions, which require further validation through human judgment.Additionally, the method in this study holds theoretical value for fields beyond computer science, particularly for papers in domains where section structure Identification is possible.It is applicable if the content of the paper can be classified into the IMRaD structure, enabling novelty score prediction, or by labeling enough novelty scores for transfer learning.</p>
<p>Practical Implication</p>
<p>Firstly, we provide insights for future evaluations of paper novelty.When assessing novelty using the core content of a paper rather than partial elements (such as abstracts, keywords, etc.), priority should be given to the content found in the introduction, results, and discussion sections, particularly focusing on significant sentences and paragraphs.Additionally, if only a portion of the content, such as a single section, is to be considered, our results suggest a prioritization hierarchy.The introduction should be the primary focus, followed by the results and discussion sections.</p>
<p>Secondly, in a zero-shot setting, LLMs are unreliable for the task of novelty score prediction.This is consistent with the findings of Mike Thelwall (Thelwall, 2024) and Zhou et al. (Zhou et al., 2024), who also concluded that current LLMs are insufficient for fine-grained evaluations.Overall, LLMs consistently generate results that tend to be overly satisfactory, often assigning high scores when assessing the novelty of papers.This indicates that the current LLM is not accurate enough to complete formal peer review related work, especially novelty assessment.We believe that fine-tuning these models could potentially enable them to assist humans in the review process.</p>
<p>Thirdly, our results can provide guidance for novice reviewers.Young reviewers, who may be unsure how to assess the novelty of a paper in their initial reviews, can begin by focusing on the introduction, results, and discussion sections for a preliminary evaluation.Furthermore, they can leverage their accumulated knowledge to make more comprehensive judgments, thereby completing the review process with a clear and systematic approach.</p>
<p>Finally, our research also can generalizable to other domains.Using ICLR papers and novelty scores as a reference domain, we explored the optimal combination of section structures for predicting novelty scores.To apply this to other domains, it is only necessary to identify the section structures in the target domain and provide the corresponding novelty scores.Transfer learning and other cross-domain learning methods can then be employed to select the optimal structure based on our conclusions.In addition to peer review in ICLR and the field of computer science, peer review in other domains also requires reviewers to assess the novelty of a paper.While this assessment may not always take the form of specific scores, it often involves qualitative grading akin to scoring.Therefore, our research can applicable to other fields as well.</p>
<p>Limitation</p>
<p>Our study has several limitations that warrant emphasis.First, We acknowledge that novelty assessment is a multifaceted process influenced by factors such as the reviewer's background, familiarity with cutting-edge research, and interpretation of original contributions.While our method focuses on text analysis, it does not claim to capture the full complexity of human judgment involved in novelty evaluation.The insight proposed in this study should be viewed as part of a broader toolkit for assessing novelty.By identifying patterns within chapter combinations and their potential influence on the perception of novelty, we aim to complement rather than replace the holistic evaluations conducted by reviewers.Additionally, we only tested two LLM for their performance in novelty score prediction; many other available LLMs, such as LLaMA 3, remain to be tested.Furthermore, the prompts used in our study represent just one possible form and could benefit from further exploration to design more effective prompts.Although GPT -3.5 or GPT-4o currently possess certain capabilities, they also have limitations.GPT-3.5, for instance, still requires certain functionalities or additional tasks, such as incorporating current scientific databases, to be effectively utilized in these experiments.Our study did not account for the impact of other content in academic papers, such as tables, figures, and charts, which are crucial components of academic papers.Lastly, the data we used from ICLR is limited to the field of computer science, introducing certain constraints on the generalizability of our findings.</p>
<p>Despite these limitations, this study offers a novel perspective on how text-based features can be leveraged to detect potential signals of novelty in academic writing.Our approach lays the groundwork for further exploration of the relationship between textual content and perceived innovation in research.</p>
<p>Conclusion and future works</p>
<p>In this study, we explored three research questions.To achieve this, we conducted section structure recognition on academic papers and fine-tuned the PLMs using different section combinations as inputs to verify its effectiveness in novelty score prediction tasks.Additionally, we evaluated the performance of LLMs on the same task.Finally, we discussed which section structures should be prioritized when assessing the novelty of academic papers.From the final results, the performance of the PLMs was generally moderate, with the highest accuracy for the section combination of Introduction, Results, and Discussion reaching only 0.682.Although this accuracy is not high, it still provides some insights.The LLMs performed even worse on the novelty score prediction task but showed decent results in terms of correlation coefficients.Our findings suggest that the Introduction, Results, and Discussion sections should be the primary focus when evaluating the novelty of academic papers.</p>
<p>In the future, we plan to collect data from a wider array of disciplines and journals to assess novelty, as our current dataset is limited to the field of computer science and conference papers.We will design more effective model architectures to accomplish the task of novelty score prediction, while also incorporating traditional novelty evaluation methods may be achieve better results.More information, such as abstracts and keywords, can be included in the model to examine the effectiveness of these elements.Furthermore, we plan to develop strategies to unlock the potential of LLMs, enabling them to perform novelty score prediction more effectively.Exploring the integration of deep learning with LLMs is another area we intend to investigate.Moreover, future work could integrate additional dimensions of novelty assessment, such as reviewer expertise, citation patterns, and qualitative insights, to create a more comprehensive evaluation framework.</p>
<p>Acknowledgment</p>
<p>This study is supported by the National Natural Science Foundation of China (Grant No. 72074113).</p>
<p>Figure 1 :
1
Figure 1: An Example of review report on ICLR 2022.(https://openreview.net/forum?id=ltM1RMZntpu)</p>
<p>Figure 2 :
2
Figure 2: Framework of this study.</p>
<p>Figure 3 :
3
Figure 3: Prompt of Llama 3 for section structure identification.</p>
<p>Figure 4 :
4
Figure 4: An Example of Fine-tuning PLMs for Novelty Score Prediction.Using Longformer to represent the PLM and Introduction + Methods to represent the section structure combinations as examples.</p>
<p>Figure 5 :
5
Figure 5: An Example used to prompt the large language model for the novelty score prediction task.</p>
<p>0.1555 b 0.1503 b 0.1483 b 0.1656 b 0.1604 b 0.1594 b 0.1660 b 0.1659 b 0.1601 b 0.1662 b 0.1662 b 0.1651 b 0.1885 b 0.1885 b 0.1816 b I 0.2002 c 0.1976 c 0.1951 c 0.2021 c 0.2019 c 0.1975 c 0.2011 c 0.1990 c 0.1964 c 0.2020 c 0.1999 c 0.1970 c 0.2030 c 0.2010 c 0.1983 c IM 0.2150 c 0.2105 c 0.2077 c 0.2180 c 0.2134 c 0.2110 c 0.2161 c 0.2115 c 0.2080 c 0.2161 c 0.2115 c 0.2090 c 0.2199 c 0.2140 c 0.2122 c IMR 0.2811 a 0.2740 a 0.2730 a 0.2847 a 0.2781 a 0.2770 a 0.2811 a 0.2741 a 0.2733 a 0.2830 a 0.2761 a 0.2751 a 0.2867 a 0.2791 a 0.2785 a IMD 0.2830 b 0.2772 b 0.2741 b 0.2850 b 0.2781 b 0.2773 b 0.2859 b 0.2801 b 0.2777 b 0.2850 b 0.2783 b 0.2777 b 0.2881 b 0.2810 b 0.2804 b IMRD 0.2711 c 0.2750 b 0.2695 b 0.2721 c 0.2771 b 0.2710 b 0.2710 c 0.2755 b 0.2688 b 0.2730 c 0.2771 b 0.2710 b 0.2850 c 0.2889 b 0.2831 b IR 0.2170 c 0.2131 c 0.2100 c 0.2200 c 0.2165 c 0.2131 c 0.2183 c 0.2140 c 0.2111 c 0.2194 c 0.2150 c 0.2122 c 0.2222 c 0.2181 c 0.2155 c IRD 0.3181 c 0.3080 c 0.3045 c 0.3205 c 0.3104 c 0.3060 c 0.3211 c 0.3110 c 0.3071 c 0.3201 c 0.3103 c 0.3061 c 0.3261 c 0.3154 c 0.3125 c ID 0.2774 c 0.2701 c 0.2671 c 0.2793 c 0.2721 c 0.2690 c 0.2801 c 0.2735 c 0.2701 c 0.2790 c 0.2721 c 0.2692 c 0.2810 c 0.2741 c 0.2711 c M 0.2090 c 0.2021 c 0.2000 c 0.2112 c 0.2041 c 0.2020 c 0.2101 c 0.2030 c 0.2010 c 0.2091 c 0.2020 c 0.2000 c 0.2130 c 0.2065 c 0.2043 c MR 0.1767 0.1704 0.1701 0.1805 0.1744 0.1735 0.1821 0.1744 0.1743 0.1812 0.1734 0.1743 0.1854 0.1789 0.1778 MD 0.1681 a 0.1612 0.1603 0.1689 a 0.1621 0.1612 0.1601 a 0.1630 0.1621 0.1688 a 0.1621 0.1612 0.1754 a 0.1681 0.1671 MRD 0.1690 a 0.1621 a 0.1601 a 0.1712 a 0.1643 a 0.1621 a 0.1701 a 0.1634 a 0.1610 a 0.1721 a 0.1651 a 0.1634 a 0.1751 a 0.1683 a 0.1667 a R 0.1931 c 0.1761 c 0.1743 a 0.1951 c 0.1783 c 0.1761 a 0.1985 c 0.1818 c 0.1793 b 0.1961 c 0.1792 c 0.1774 b 0.1991 c 0.1834 c 0.1812 b RD 0.1312 a 0.1081 a 0.1067 a 0.1333 a 0.1102 a 0.1091 a 0.1321 a 0.1092 a 0.1083 a 0.1333 a 0.1110 a 0.1101 a 0.1362 a 0.1132 a 0.1110 a D 0.1901 c 0.1732 c 0.1711 c 0.1923 c 0.1741 c 0.1725 c 0.1912 c 0.1734 c 0.1721 c 0.1934 c 0.1762 c 0.1745 c 0.1962 c 0.1899 c 0.1783 c Note: a represents p&lt;0.05; b represents p&lt;0.01; c represents p&lt;0.001.SC, T, A, I, M, R, D respectively represent section combination, title, abstract, introduction, methods, results, and discussion.For example, TA means the model input consists of the title and abstract, with other letter combinations following the same pattern.SciBERT is Longformer version.P, SP and K is Pearson, Spearman and Kendall's tau correlation coefficient, respectively.</p>
<p>Figure 6 :
6
Figure 6: Case study on PLM (Longformer version of SciBERT) for novelty score prediction using different paper sections as input.</p>
<p>Figure 7 :
7
Figure 7: Case study on LLM (GPT-4o) for novelty score prediction using different paper sections as input.</p>
<p>Table 1 :
1
Statistical results of the novelty score data.
DecisionAcceptRejectTotalTNS# TNS=106060# TNS=227217261998# TNS=37505991349# TNS=487693# Papers110923913500
Note: TNS is Technical Novelty and Significance score.</p>
<p>Table 2 :
2
The result of the correlation coefficient between the PLMs's prediction and the ground truth.
SCPLongformer SPKPBigBird SPKPLongT5 SPKPLED SPKPSciBERT SPKT0.0488 0.0602 0.0481 0.0491 0.0606 0.0483 0.0488 0.0603 0.0480 0.0491 0.0608 0.0485 0.0492 0.0610 0.0488A0.0890 0.0832 0.0879 0.0891 0.0835 0.0882 0.0892 0.0836 0.0882 0.0892 0.0836 0.0882 0.0894 0.0838 0.0884TA</p>
<p>Table 3 :
3
The result of the correlation coefficient between the LLM's prediction and the ground truth.
SCPGPT-3.5 SPKPGPT-4o SPKT0.02480.02500.02270.10350.10350.0976A0.15250.15660.1433-0.0295-0.1100-0.0277TA-0.1054-0.1054-0.09940.15810.14740.1381I-0.0787-0.0565-0.05220.01920.02010.0199IM-0.0674-0.0453-0.04160.01990.03180.0299IMR0.15500.14320.13280.1916 a0.1916 a0.1806 aIMD-0.1021-0.1243-0.11630.16020.16430.1536IMRD-0.0815-0.1240-0.11600.16170.17890.1681IR0.03860.04200.03860.1921 a0.1896 a0.1771 aIRD-0.1129-0.1002-0.09360.02070.02070.0195ID0.04040.07210.06700.10380.10380.0979M-0.1517-0.1708-0.15660.05450.08030.0744MR0.08090.05020.04560.07930.08080.0755MD0.03540.04990.04620.10900.10880.1003MRD-0.1066-0.1108-0.10190.07790.08030.0755R0.12730.1603 0.15100.15270.15540.1460RD0.01850.06590.06120.2389 a0.2329 b0.2186 aD0.07480.06050.05640.06400.05360.0472
Note: a represents p&lt;0.05; b represents p&lt;0.01; c represents p&lt;0.001.SC is Section Combination.P, SP and K is Pearson, Spearman and Kendall's tau correlation coefficient, respectively.The other letter abbreviations are the same as Table2.</p>
<p>Table 4 :
4
The results of different section combinations of PLMs in novelty score prediction tasks.SC, T, A, I, M, R, D respectively represent section combination, title, abstract, introduction, methods, results, and discussion.For example, TA means the model input consists of the title and abstract, with other letter combinations following the same pattern.SciBERT is Longformer version.F 1 is W eighted F 1.
SCLongformer Acc F 1BigBird Acc F 1LongT5 Acc F 1AccLEDF 1SciBERT Acc F 1T0.52740.51680.52650.51570.52660.51590.52810.51720.52830.5175A0.53310.53120.53340.53150.53320.53140.53350.53180.53380.5320TA0.59430.58800.59710.59000.59510.58910.59400.58910.59820.5901I0.61140.58630.60820.58110.61020.58430.61230.58610.61220.5854IM0.61140.59030.61300.59210.61220.59110.61120.58910.61520.5922IMR0.65770.63510.65500.63090.65740.63520.65620.63540.66000.6381IMD0.65720.62810.65410.63220.65670.62810.65730.65430.65920.6314IMRD0.61450.59400.61010.5920.61670.59210.61430.59520.61820.5973IR0.62000.61430.62220.61540.62000.61520.62010.61520.62210.6152IRD0.6681 0.6455 0.6701 0.6503 0.6692 0.6481 0.6778 0.6462 0.6824 0.6515ID0.65150.63330.65340.63410.65630.63330.65120.63220.65320.6354M0.60000.57810.60100.57950.60210.57940.60140.57810.60310.5812MR0.58010.56010.58340.56220.58410.56430.58120.55890.58540.5632MD0.58610.57570.58610.57650.58850.57850.58670.57500.59020.5824MRD0.59310.58620.59510.58810.59610.58920.59000.58810.59730.5996R0.58200.56700.58340.56810.58870.57330.58240.56710.59120.5793RD0.55000.53410.55230.53640.56010.54100.55590.53420.56970.5512D0.56890.56500.57120.56620.57500.56810.57670.56810.58130.5667
Note:</p>
<p>Table 5 :
5
The results of LLMs on the task of novelty score prediction.The letter abbreviations are the same as Table4.F 1 is W eighted F 1.
Section CombinationGPT-3.5 AccF 1GPT-4o AccF 1T0.33330.25080.30830.2418A0.35000.28360.33330.2145TA0.35000.28030.30830.2230I0.29170.22200.30000.2493IM0.31670.26760.35000.2800IMR0.39170.33880.34170.2616IMD0.28330.22600.31670.2471IMRD0.30830.28410.39170.3088IR0.40000.31940.38330.3161IRD0.25830.18130.29170.2296ID0.39170.30970.37500.2970M0.26670.23510.33330.2744MR0.34170.30940.30000.2420MD0.32500.27010.33330.2902MRD0.31670.25870.33330.2786R0.34170.28150.37500.2912RD0.32500.24570.38330.3241D0.35000.30180.28330.2704
Note:</p>
<p>Table 6 :
6
The result of traditional method and best performance by our method.Note: a represents p&lt;0.05; b represents p&lt;0.01; c represents p&lt;0.001.The letter abbreviations are the same as Table4.F 1 is W eighted F 1.
MethodAccF 1PSPKShibayama's method0.42650.36370.01320.03620.0346SciBERT+IRD0.68240.65150.3261 c0.3154 c0.3125 c
https://www.nature.com/nature/for-referees/how-to-write-a-report. https://2023.aclweb.org/blog/review-acl23/. https://icml.cc/Conferences/2023/ReviewerTutorial.
https://github.com/kermitt2/grobid
https://huggingface.co/howanching-clara/classifier for academic texts
https://huggingface.co/yorko/scibert scivocab uncased long 4096</p>
<p>Natural language processing to identify the creation and impact of new technologies in patent text: Code, data, and new measures. S Arts, J Hou, J C Gomez, 10.1016/j.respol.2020.104144Research Policy. 502021. 104144</p>
<p>SciBERT: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaEMNLP-IJCNLP2019Association for Computational Linguistics</p>
<p>Longformer: The longdocument transformer. I Beltagy, M E Peters, A Cohan, arXiv:2004.051502020</p>
<p>Looking across and looking beyond the knowledge frontier: Intellectual distance, novelty, and resource allocation in science. K J Boudreau, E C Guinan, K R Lakhani, C Riedl, 10.1287/mnsc.2015.2285doi:10.1287/mnsc. 2015.2285Manage. Sci. 622016</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 10.5555/3495724.3495883Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates Inc2020</p>
<p>Can chatgpt provide intelligent diagnoses? a comparative study between predictive models and chatgpt to define a new medical diagnostic bot. L Caruccio, S Cirillo, G Polese, G Solimando, S Sundaramurthy, G Tortora, 10.1016/j.eswa.2023.121186Expert Systems with Applications. 2351211862024</p>
<p>Exploring the relationship between team institutional composition and novelty in academic papers based on fine-grained knowledge entities. Z Chen, C Zhang, H Zhang, Y Zhao, C Yang, Y Yang, 10.1108/EL-03-2024-00702024</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, 10.5555/3648699.3648939Journal of Machine Learning Research. 242023</p>
<p>A discourse-aware attention model for abstractive summarization of long documents. A Cohan, F Dernoncourt, D S Kim, T Bui, S Kim, W Chang, N Goharian, 10.18653/v1/N18-2097Proceedings of the 2018 Conference of the North American Chapter. Short Papers. M Walker, H Ji, A Stent, the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics2018a2</p>
<p>A discourse-aware attention model for abstractive summarization of long documents. A Cohan, F Dernoncourt, D S Kim, T Bui, S Kim, W Chang, N Goharian, 10.18653/v1/N18-2097Proceedings of the 2018 Conference of the North American Chapter. Short Papers. M Walker, H Ji, A Stent, the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics2018b2</p>
<p>Integrated sentiment analysis with bert for enhanced hybrid recommendation systems. N Darraz, I Karabila, A El-Ansari, N Alami, M El Mallahi, 10.1016/j.eswa.2024.125533Expert Systems with Applications. 2024</p>
<p>1 innovation: A guide to the literature, in: The Oxford Handbook of Innovation. J Fagerberg, 10.1093/oxfordhb/9780199286805.003.00012006Oxford University Press</p>
<p>Surprise! measuring novelty as expectation violation. J G Foster, F Shi, J Evans, 2021</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, 10.1287/mnsc.2015.2366doi:10.1287/mnsc.2015.2366Manage. Sci. 632017</p>
<p>Reviewer2: Optimizing review generation through prompt generation. Z Gao, K Brantley, T Joachims, arXiv:2402.108862024</p>
<p>What is originality in the humanities and the social sciences?. J Guetzkow, M Lamont, G Mallard, 10.1177/000312240406900203American Sociological Review. 692004</p>
<p>LongT5: Efficient text-to-text transformer for long sequences. M Guo, J Ainslie, D Uthus, S Ontanon, J Ni, Y H Sung, Y Yang, 10.18653/v1/2022.findings-naacl.55Findings of the Association for Computational Linguistics: NAACL 2022. M Carpuat, M C De Marneffe, I V Meza Ruiz, Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>A new method for measuring the originality of academic articles based on knowledge units in semantic networks. J Hou, D Wang, J Li, 10.1016/j.joi.2022.101306Journal of Informetrics. 162022. 101306</p>
<p>Measuring the novelty of scientific publications: A fasttext and local outlier factor approach. D Jeon, J Lee, J M Ahn, C Lee, 10.1016/j.joi.2023.101450Journal of Informetrics. 171014502023</p>
<p>Research on functional structure identification of academic text based on deep learning. Y Ji, Q Zhang, S Shen, D Wang, S Huang, 10.13266/j.issn.0252-3116.2019.13.01017TH INTERNATIONAL CONFERENCE ON SCIENTOMETRICS &amp; INFORMETRICS (ISSI2019). INT SOC SCIENTOMETRICS &amp; INFORMETRICS-ISSI. 2019II</p>
<p>Large language models are state-of-the-art evaluators of translation quality. T Kocmi, C Federmann, M Nurminen, J Brenner, M Koponen, S Latomaa, M Mikhailov, F Schierl, T Ranasinghe, E Vanmassenhove, S A Vidal, N Aranberri, M Nunziatini, C P Escartín, M Forcada, M Popovic, C Scarton, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. H Moniz, the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>What do we know about the disruption index in scientometrics? an overview of the literature. C Leibel, L Bornmann, Scientometrics. 1292024</p>
<p>A hybrid approach to recognize generic sections in scholarly documents. S Li, Q Wang, 10.1007/s10032-021-00381-5International Journal on Document Analysis and Recognition (IJDAR). 242021</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. W Liang, Y Zhang, H Cao, B Wang, D Y Ding, X Yang, K Vodrahalli, S He, D S Smith, Y Yin, D A Mcfarland, J Zou, 10.1056/AIoa2400196NEJM AI 0. 2023. AIoa2400196</p>
<p>The prominent and heterogeneous gender disparities in scientific novelty: Evidence from biomedical doctoral theses. Information Processing &amp; Management 61. M Liu, Z Xie, A J Yang, C Yu, J Xu, Y Ding, Y Bu, 10.1016/j.ipm.2024.1037432024103743</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. R Liu, N B Shah, arXiv:2306.006222023</p>
<p>S2ORC: The semantic scholar open research corpus. K Lo, L L Wang, M Neumann, R Kinney, D Weld, 10.18653/v1/2020.acl-main.447doi:10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Functional structure identification of scientific documents in computer science. W Lu, Y Huang, Y Bu, Q Cheng, 10.1007/s11192-018-2640-yScientometrics. 1152018</p>
<p>Combination of research questions and methods: A new measurement of scientific novelty. Z Luo, W Lu, J He, Y Wang, 10.1016/j.joi.2022.101282Journal of Informetrics. 161012822022</p>
<p>Enhancing identification of structure function of academic articles using contextual information. B Ma, C Zhang, Y Wang, S Deng, 10.1007/s11192-021-04225-1Scientometrics. 1272022</p>
<p>Introducing a novelty indicator for scientific research: validating the knowledge-based combinatorial approach. K Matsumoto, S Shibayama, B Kang, M Igami, 10.1007/s11192-021-04049-zScientometrics. 1262021</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. A Meta, Meta AI. 2024</p>
<p>Scientific writing and communication in agriculture and natural resources. P R Nair, V D Nair, 10.1007/978-3-319-03101-92014Springer</p>
<p>An evolutionary theory of economic change. R R Nelson, 10.2307/22324091985harvard university press</p>
<p>Openai, arXiv:2303.08774Gpt-4 technical report. 2024</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc2022</p>
<p>Assessing llms in malicious code deobfuscation of real-world malware campaigns. C Patsakis, F Casino, N Lykousas, 10.1016/j.eswa.2024.124912Expert Systems with Applications. 2561249122024</p>
<p>Which structure of academic articles do referees pay more attention to?: perspective of peer review and full-text of academic articles. C Qin, C Zhang, 10.1108/AJIM-05-2022-0244Aslib Journal of Information Management. 752023</p>
<p>Gpt4 is slightly helpful for peer-review assistance: A pilot study. Z Robertson, arXiv:2307.054922023</p>
<p>The definition and measurement of innovation. WorkingPaper 10/98. M Rogers, 1998Melbourne Institute of Applied Economic and Social Research</p>
<p>The standard definition of creativity. M A Runco, G J Jaeger, 10.1080/10400419.2012.650092Creativity Research Journal. 242012</p>
<p>Business Cycles: A Theoretical, Historical, and Statistical Analysis of the Capitalist Process. J Schumpeter, 10.1086/ahr/46.1.96Business Cycles: A Theoretical, Historical, and Statistical Analysis of the Capitalist Process. Martino Pub2006</p>
<p>Evaluation of llm-based chatbots for osint-based cyber threat awareness. S Shafee, A Bessani, P M Ferreira, 10.1016/j.eswa.2024.125509Expert Systems with Applications. 2024</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. C Shen, L Cheng, X P Nguyen, Y You, L Bing, 10.18653/v1/2023.findings-emnlp.278Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Measuring originality in science. S Shibayama, J Wang, Scientometrics. 1222020</p>
<p>Measuring novelty in science with word embedding. S Shibayama, D Yin, K Matsumoto, 10.1371/journal.pone.0254034PloS one. 16e02540342021</p>
<p>The introduction, methods, results, and discussion (imrad) structure: a fifty-year survey. L B Sollaci, M G Pereira, Journal of the medical library association. 923642004</p>
<p>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. I Tahamtan, L Bornmann, 10.1016/j.joi.2018.07.005Journal of Informetrics. 122018</p>
<p>Can chatgpt evaluate research quality?. M Thelwall, 10.2478/jdis-2024-0013Journal of Data and Information Science. 92024</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023a</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, T Scialom, arXiv:2307.092882023bLlama 2: Open foundation and fine-tuned chat models</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, 10.1126/science.1240474Science. 3422013</p>
<p>A content-based novelty measure for scholarly publications: A proof of concept. H Wang, I Sserwanga, H Joho, J Ma, P Hansen, D Wu, M Koizumi, Gilliland, Wisdom, Well-Being, Win-Win. A J , ChamSpringer Nature Switzerland2024</p>
<p>Bias against novelty in science: A cautionary tale for users of bibliometric indicators. J Wang, R Veugelers, P Stephan, 10.1016/j.respol.2017.06.006Research Policy. 462017</p>
<p>An effective framework for measuring the novelty of scientific articles through integrated topic modeling and cloud model. Z Wang, H Zhang, J Chen, H Chen, 10.1016/j.joi.2024.101587Journal of Informetrics. 181015872024</p>
<p>Are the confidence scores of reviewers consistent with the review content? evidence from top conference proceedings in ai. W Wu, H Xi, C Zhang, 10.1007/s11192-024-05070-8Scientometrics. 2024</p>
<p>Identify novel elements of knowledge with word embedding. D Yin, Z Wu, K Yokota, K Matsumoto, S Shibayama, 10.1371/journal.pone.0284567Plos one. 18e02845672023</p>
<p>Big bird: Transformers for longer sequences. M Zaheer, G Guruganesh, K A Dubey, J Ainslie, C Alberti, S Ontanon, P Pham, A Ravula, Q Wang, L Yang, A Ahmed, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc2020</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. R Zhou, L Chen, K Yu, N Calzolari, M Y Kan, V Hoste, A Lenci, S Sakti, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCL. N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCLTorino, Italia2024</p>
<p>Feature engineering vs. deep learning for paper section identification: Toward applications in chinese medical literature. S Zhou, X Li, 10.1016/j.ipm.2020.102206Information Processing &amp; Management. 571022062020</p>
<p>Aspect-based sentiment analysis via bidirectional variant spiking neural p systems. C Zhu, B Yi, L Luo, 10.1016/j.eswa.2024.125295Expert Systems with Applications. 2591252952025</p>            </div>
        </div>

    </div>
</body>
</html>
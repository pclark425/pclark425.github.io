<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8125 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8125</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8125</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-237532584</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2109.08006v3.pdf" target="_blank">Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> An important aspect of artificial intelligence (AI) is the ability to reason in a step-by-step"algorithmic"manner that can be inspected and verified for its correctness. This is especially important in the domain of question answering (QA). We argue that the challenge of algorithmic reasoning in QA can be effectively tackled with a"systems"approach to AI which features a hybrid use of symbolic and sub-symbolic methods including deep neural networks. Additionally, we argue that while neural network models with end-to-end training pipelines perform well in narrow applications such as image classification and language modelling, they cannot, on their own, successfully perform algorithmic reasoning, especially if the task spans multiple domains. We discuss a few notable exceptions and point out how they are still limited when the QA problem is widened to include other intelligence-requiring tasks. However, deep learning, and machine learning in general, do play important roles as components in the reasoning process. We propose an approach to algorithm reasoning for QA, Deep Algorithmic Question Answering (DAQA), based on three desirable properties: interpretability, generalizability, and robustness which such an AI system should possess, and conclude that they are best achieved with a combination of hybrid and compositional AI.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8125.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8125.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Generative Pre-trained Transformer 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained language model cited in the paper as an influential example of large LMs; discussed only in the context of its role as a pre-trained model and its limitations for algorithmic/System-2 reasoning and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referred to as a large pre-trained language model used as a pre-trained component in downstream tasks; the paper does not give architectural details, sizes, or training specifics beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Described at a high level as a black-box pre-trained neural language model; the paper argues such models do not provide interpretable algorithmic reasoning and lack deliberate System-2 style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Characterized generally as lacking interpretability, brittle on tasks requiring deliberate, stepwise algorithmic reasoning, and not sufficient alone for algorithmic QA; no empirical error breakdown provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>No empirical evidence in this paper; discussion is conceptual and cites broader critiques (e.g., Marcus & Davies 2020) about limitations of large LMs for algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>The paper notes that while GPT-3 is useful as a pre-trained model, on its own it 'does not adequately address' interpretability, generalizability, and robustness required for algorithmic question answering; no empirical counterexamples are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8125.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8125.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network architecture extended with an external memory that the paper cites for its ability to infer simple programs (e.g., copying, sorting) and as an example of neural approaches to algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a neural network augmented with an external memory to make it more expressive and able to infer simple programs such as copying and sorting; the paper does not provide detailed architecture or hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Represented in the paper as using an external memory and learned controller to execute algorithmic-like procedures (used as an example of neural algorithmic capability).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not empirically analyzed in this paper; NTM is mentioned as a way to make neural models more expressive, but limitations (e.g., interpretability, scalability to broader algorithmic reasoning) are implied in the broader argument.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Citation to prior work (Graves et al. 2014) asserted ability to learn simple algorithms; this paper offers no new empirical evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Paper argues that such neural approaches alone remain limited for broader algorithmic reasoning and for providing interpretable, verifiable algorithmic traces — no specific empirical counterexamples provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8125.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8125.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-NTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement-learning Neural Turing Machine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning variant of the Neural Turing Machine cited as an effort to make NTMs more expressive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforcement learning neural turing machines -revised</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reinforcement-learning NTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned briefly as a variant of NTM that incorporates reinforcement learning to increase expressivity; no experimental details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Described only at a conceptual level: combining external memory NTMs with reinforcement learning training to extend capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Only cited; this paper does not present experimental or probing evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Paper uses this citation to illustrate prior work but emphasizes that such approaches still fall short of the interpretability and general algorithmic reasoning needs for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8125.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8125.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Algorithmic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Algorithmic Reasoning (framework/paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced research direction/paper proposing that neural networks can learn/almost imitate classical algorithms (focused on lower-level algorithms like sorting); cited as related work and contrasted with the hybrid/compositional approach advocated here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural algorithmic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural algorithmic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as an approach that trains neural networks end-to-end to emulate algorithms (e.g., sorting), with emphasis in cited work on lower-level algorithms; this paper critiques its limited granularity and interpretability for larger QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Framed in the paper as neural imitation of algorithmic steps (e.g., via learned computations over structured inputs), but the paper highlights that such purely neural estimations of algorithm outputs remain black-box to users.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Paper argues limitations: focuses on narrow/lower-level algorithms, lacks interpretability and verifiability for complex/higher-level algorithmic reasoning required in QA.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>No new empirical evidence here; the paper references Veličković & Blundell (2021) and uses conceptual critique rather than presenting probing or ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Paper points out two challenges with that line of work: (1) different granularity (lower-level algorithms) than the higher-level arithmetic/statistical operations needed for QA, and (2) lack of interpretability when outputs are estimated purely by neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8125.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8125.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FRANK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FRANK QA system (Functional Inferences over Heterogeneous Data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid QA system previously developed by the author and collaborators that supports recursive deductive reasoning and aggregation/ prediction using pre-trained neural models; cited as related work that partially addresses algorithmic QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional inferences over heterogeneous data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FRANK</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a hybrid inference architecture that allows recursive deductive reasoning and aggregation of data for prediction using various inference operations including pre-trained neural models; the present paper notes FRANK lacks neural representation of inference nodes and intelligent operation search.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>aggregation and prediction (statistical/arithmetic operations) as part of QA pipelines, but no specific arithmetic benchmarks are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Hybrid symbolic/deductive machinery combined with pre-trained neural modules for inductive/statistical inference; FRANK handles aggregation and prediction but does not embed inference nodes as neural vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Called out limitations: absence of neural representation of inference nodes and limited ability to intelligently search/select inference operations; no numeric error analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper references prior FRANK publications for system behavior; no new probing or intervention evidence provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Used as motivation for DAQA: FRANK's lack of neural inference node representations and limited automatic operation search motivate the proposed hybrid functional-node inference graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural algorithmic reasoning <em>(Rating: 2)</em></li>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Reinforcement learning neural turing machines -revised <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Functional inferences over heterogeneous data <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8125",
    "paper_id": "paper-237532584",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3 (Generative Pre-trained Transformer 3)",
            "brief_description": "A large pre-trained language model cited in the paper as an influential example of large LMs; discussed only in the context of its role as a pre-trained model and its limitations for algorithmic/System-2 reasoning and interpretability.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Referred to as a large pre-trained language model used as a pre-trained component in downstream tasks; the paper does not give architectural details, sizes, or training specifics beyond citation.",
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Described at a high level as a black-box pre-trained neural language model; the paper argues such models do not provide interpretable algorithmic reasoning and lack deliberate System-2 style reasoning.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": "Characterized generally as lacking interpretability, brittle on tasks requiring deliberate, stepwise algorithmic reasoning, and not sufficient alone for algorithmic QA; no empirical error breakdown provided in this paper.",
            "evidence_for_mechanism": "No empirical evidence in this paper; discussion is conceptual and cites broader critiques (e.g., Marcus & Davies 2020) about limitations of large LMs for algorithmic reasoning.",
            "counterexamples_or_challenges": "The paper notes that while GPT-3 is useful as a pre-trained model, on its own it 'does not adequately address' interpretability, generalizability, and robustness required for algorithmic question answering; no empirical counterexamples are given here.",
            "uuid": "e8125.0",
            "source_info": {
                "paper_title": "Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "NTM",
            "name_full": "Neural Turing Machine",
            "brief_description": "A neural-network architecture extended with an external memory that the paper cites for its ability to infer simple programs (e.g., copying, sorting) and as an example of neural approaches to algorithmic tasks.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "mention",
            "model_name": "Neural Turing Machine (NTM)",
            "model_description": "Described as a neural network augmented with an external memory to make it more expressive and able to infer simple programs such as copying and sorting; the paper does not provide detailed architecture or hyperparameters.",
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Represented in the paper as using an external memory and learned controller to execute algorithmic-like procedures (used as an example of neural algorithmic capability).",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": "Not empirically analyzed in this paper; NTM is mentioned as a way to make neural models more expressive, but limitations (e.g., interpretability, scalability to broader algorithmic reasoning) are implied in the broader argument.",
            "evidence_for_mechanism": "Citation to prior work (Graves et al. 2014) asserted ability to learn simple algorithms; this paper offers no new empirical evidence.",
            "counterexamples_or_challenges": "Paper argues that such neural approaches alone remain limited for broader algorithmic reasoning and for providing interpretable, verifiable algorithmic traces — no specific empirical counterexamples provided here.",
            "uuid": "e8125.1",
            "source_info": {
                "paper_title": "Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "RL-NTM",
            "name_full": "Reinforcement-learning Neural Turing Machine",
            "brief_description": "A reinforcement-learning variant of the Neural Turing Machine cited as an effort to make NTMs more expressive.",
            "citation_title": "Reinforcement learning neural turing machines -revised",
            "mention_or_use": "mention",
            "model_name": "Reinforcement-learning NTM",
            "model_description": "Mentioned briefly as a variant of NTM that incorporates reinforcement learning to increase expressivity; no experimental details are provided in this paper.",
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Described only at a conceptual level: combining external memory NTMs with reinforcement learning training to extend capabilities.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Only cited; this paper does not present experimental or probing evidence.",
            "counterexamples_or_challenges": "Paper uses this citation to illustrate prior work but emphasizes that such approaches still fall short of the interpretability and general algorithmic reasoning needs for QA.",
            "uuid": "e8125.2",
            "source_info": {
                "paper_title": "Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Neural Algorithmic Reasoning",
            "name_full": "Neural Algorithmic Reasoning (framework/paper)",
            "brief_description": "A referenced research direction/paper proposing that neural networks can learn/almost imitate classical algorithms (focused on lower-level algorithms like sorting); cited as related work and contrasted with the hybrid/compositional approach advocated here.",
            "citation_title": "Neural algorithmic reasoning",
            "mention_or_use": "mention",
            "model_name": "Neural algorithmic reasoning",
            "model_description": "Referenced as an approach that trains neural networks end-to-end to emulate algorithms (e.g., sorting), with emphasis in cited work on lower-level algorithms; this paper critiques its limited granularity and interpretability for larger QA tasks.",
            "arithmetic_task_type": null,
            "mechanism_or_representation": "Framed in the paper as neural imitation of algorithmic steps (e.g., via learned computations over structured inputs), but the paper highlights that such purely neural estimations of algorithm outputs remain black-box to users.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": "Paper argues limitations: focuses on narrow/lower-level algorithms, lacks interpretability and verifiability for complex/higher-level algorithmic reasoning required in QA.",
            "evidence_for_mechanism": "No new empirical evidence here; the paper references Veličković & Blundell (2021) and uses conceptual critique rather than presenting probing or ablation studies.",
            "counterexamples_or_challenges": "Paper points out two challenges with that line of work: (1) different granularity (lower-level algorithms) than the higher-level arithmetic/statistical operations needed for QA, and (2) lack of interpretability when outputs are estimated purely by neural networks.",
            "uuid": "e8125.3",
            "source_info": {
                "paper_title": "Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "FRANK",
            "name_full": "FRANK QA system (Functional Inferences over Heterogeneous Data)",
            "brief_description": "A hybrid QA system previously developed by the author and collaborators that supports recursive deductive reasoning and aggregation/ prediction using pre-trained neural models; cited as related work that partially addresses algorithmic QA.",
            "citation_title": "Functional inferences over heterogeneous data",
            "mention_or_use": "mention",
            "model_name": "FRANK",
            "model_description": "Described as a hybrid inference architecture that allows recursive deductive reasoning and aggregation of data for prediction using various inference operations including pre-trained neural models; the present paper notes FRANK lacks neural representation of inference nodes and intelligent operation search.",
            "arithmetic_task_type": "aggregation and prediction (statistical/arithmetic operations) as part of QA pipelines, but no specific arithmetic benchmarks are reported in this paper.",
            "mechanism_or_representation": "Hybrid symbolic/deductive machinery combined with pre-trained neural modules for inductive/statistical inference; FRANK handles aggregation and prediction but does not embed inference nodes as neural vectors.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": "Called out limitations: absence of neural representation of inference nodes and limited ability to intelligently search/select inference operations; no numeric error analysis provided here.",
            "evidence_for_mechanism": "Paper references prior FRANK publications for system behavior; no new probing or intervention evidence provided in this paper.",
            "counterexamples_or_challenges": "Used as motivation for DAQA: FRANK's lack of neural inference node representations and limited automatic operation search motivate the proposed hybrid functional-node inference graphs.",
            "uuid": "e8125.4",
            "source_info": {
                "paper_title": "Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural algorithmic reasoning",
            "rating": 2,
            "sanitized_title": "neural_algorithmic_reasoning"
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 2,
            "sanitized_title": "neural_turing_machines"
        },
        {
            "paper_title": "Reinforcement learning neural turing machines -revised",
            "rating": 2,
            "sanitized_title": "reinforcement_learning_neural_turing_machines_revised"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Functional inferences over heterogeneous data",
            "rating": 2,
            "sanitized_title": "functional_inferences_over_heterogeneous_data"
        }
    ],
    "cost": 0.011289750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning</p>
<p>Kwabena Nuamah k.nuamah@ed.ac.uk 
School of Informatics
University of Edinburgh</p>
<p>Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning</p>
<p>An important aspect of artificial intelligence (AI) is the ability to reason in a step-by-step "algorithmic" manner that can be inspected and verified for its correctness. This is especially important in the domain of question answering (QA). We argue that the challenge of algorithmic reasoning in QA can be effectively tackled with a "systems" approach to AI which features a hybrid use of symbolic and sub-symbolic methods including deep neural networks. Additionally, we argue that while neural network models with end-to-end training pipelines perform well in narrow applications such as image classification and language modelling, they cannot, on their own, successfully perform algorithmic reasoning, especially if the task spans multiple domains. We discuss a few notable exceptions and point out how they are still limited when the QA problem is widened to include other intelligence-requiring tasks. However, deep learning, and machine learning in general, do play important roles as components in the reasoning process. In this position paper, we propose an approach to algorithm reasoning for QA, Deep Algorithmic Question Answering (DAQA), based on three desirable properties: interpretability, generalizability, and robustness which such an AI system should posses, and conclude that they are best achieved with a combination of hybrid and compositional AI.</p>
<p>Introduction</p>
<p>Algorithms form the basis of problem solving and are, therefore, critical to any attempts to emulate human-like reasoning in AI. Algorithmic reasoning, as defined in (Veličković and Blundell 2021), allows us to automate and engineer systems that reason. An interesting domain in which to apply and evaluate such AI capabilities is that of question answering, and in particular, open domain QA. Some of the early techniques in QA, e.g. (Green et al. 1961), focused on reasoning about problems in a purely logical manner. However, recent techniques have been aimed more at the challenges of constructing the right queries to retrieve answers from knowledge bases (or sources) (KBs) ( (Usbeck et al. 2017), (Dubey et al. 2019)) as well as the construction of very large language models over a large number of documents from the web (Devlin et al. 2019).</p>
<p>Yet, many other tasks, such as the automatic selection of KBs and relevant knowledge, choice of inference algorithms, and how to combine them, are all important to fully automate the QA process. Several of these tasks are scoped out as engineering tasks which experts perform when deploying these AI systems (see Figure 1). We argue that these scoped out tasks should be part of the AI models which are built for QA tasks, as they are key ingredients in the full automation of the QA process. Deep Algorithmic Question Answering focuses on these tasks as well as the traditional QA problem with the added task of tackling questions that require multiple steps of reasoning to solve.</p>
<p>In this position paper we will focus on the QA problem, especially since QA is one of the longest standing applications of AI and since many other AI problems can be framed as QA problems. Why is it important to take such a high level perspective of QA and algorithmic reasoning, instead of one at the deeper level of knowledge/vector representation and semantics?</p>
<p>• It puts into sharper focus how narrow many popular AI techniques are: e.g, language modelling, image classification, etc. 'Narrow' in the sense that the models excel at perception tasks for which lots of training data is available, and in the sense that they are restricted to those specific tasks and cannot be applied to other tasks 'as-is' without major changes to the models or how they used. • It highlights how unreasonable many of the assumptions in AI models are when applied to real-world problems. For instance, (1) the assumptions that the answer to a question will be from the same distribution of data which was used to train the model; (2) the assumption that the model always has access to all the data that it needs to answer a question such that the decision of choosing which KBs to use to answer a question is never a problem. In many real-world applications, the data sources are diverse and heterogeneous, noisy and incomplete. • It shows how many AI techniques fail to address some of the challenging problems that have to be tackled. For example, dealing with uncertainty, noisy and incomplete information from KBs, especially in the context of QA. • It shows how huge aspects of what we currently claim to be AI are heavily dependent on designs and inputs from humans and how much work needs to be done to solve simple tasks without human intervention. For instance, pre-defining which DL models are used to tackle a classification or prediction task. Although tasks such as feature engineering, which was predominantly an expert's task have been replaced by better DL models, the human expertise has only shifted to tasks related to the choice of architecture of the neural network model, dataset selection and pre-processing for training, and the general engineering required to solve the specific task at hand.</p>
<p>• It shows why a compositional and hybrid approach is needed given that many of the tasks cannot simply be handled with an end-to-end training of deep neural network models. We believe that a systems approach to AI is needed to tackle algorithmic reasoning in QA and agree with the claim that there is the need to find new ways to synthesize AI from a hybrid of symbolic methods and deep neural networks (Marcus and Davis 2019).</p>
<p>We conclude that it is important to refine the scope of problems that AI for QA should solve by incorporating those tasks which, in the real-world applications, look messy and are often tackled by humans experts or data annotators. Further, tackling these problems highlights the need for AI approaches that can appropriately leverage both symbolic and sub-symbolic AI methods and also brings to the fore the need to have AI systems that are compositional in order to adapt seamlessly to different problem types.</p>
<p>In the sections that follow, we give some background to algorithmic reasoning, hybrid AI and compositionality, and then describe our proposed DAQA system.</p>
<p>Background</p>
<p>Algorithmic Reasoning</p>
<p>The task of algorithmic reasoning places emphasis on automating systems to reason about problems and programs following similar mechanisms to those which humans use when solving problems (Kröger 1977). However, our interpretation of this task goes beyond the classic logical reasoning context to one where learning and reasoning are combined to tackle more complex and diverse problems. This includes, for instance, choosing which algorithms to use, when and how to combine them (Veličković and Blundell 2021). Its application to question answering means having an automated system which is deliberate in the selection of inference steps needed to answer a question such that the inference process forms a computational graph which represents an algorithm for solving the problem.</p>
<p>We claim that achieving this with a purely symbolic or DL approach is not practical, given the known limitations of symbolic methods and sub-symbolic methods (Marcus and Davis 2019). There is a lot of work ongoing to reconcile these techniques (see section 2.2). However, many of these are either theory-focused, or at levels of abstraction that still makes it hard to tackle algorithmic reasoning in a practical problem domain such as QA.</p>
<p>This work is motivated in part by models proposed in (Veličković and Blundell 2021). However, we note that the authors make assumptions about the contexts in which algorithms are used, and limit the concept of algorithms to a narrow application of a single algorithm that is trained end-toend. This paper extends that notion to include the automatic,  (Fader, Zettlemoyer, and Etzioni 2014), (Liang, Jordan, and Klein 2013), (Nuamah, Bundy, and Lucas 2016), and  focus on the application of rules for decomposing problems in order to find answers, leading to inference processes or plans which are constructed dynamically. We extend some of these ideas in this work. Implicitly, the expectation of algorithmic reasoning is that the process and the inferred answer can be inspected to verify the steps involved in answering a question. This is very different from the expectations one has when using deep neural network models to train end-to-end models where the process of completing the task is not interpretable.</p>
<p>Hybrid AI</p>
<p>Hybrid AI is concerned with the integration of symbolic (logical) and sub-symbolic (DL-based) AI methodologies into neuro-symbolic architectures. This is a rapidly growing field with diverse approaches being explored. We refer you to papers that survey these works including (Besold et al. 2017) and (Belle 2020). Additionally, Henry Kautz's classification of the different types of neural-symbolic systems integrations are outlined in (Lamb et al. 2020). Our notion of hybrid AI is primarily inspired by DARPA's 'Third Wave of AI' research focus (DARPA 2018), "where systems are capable of acquiring new knowledge through generative contextual and explanatory models".</p>
<p>The strengths and shortcomings of both DL and symbolic AI paradigms are well documented. More recently in (Bengio, LeCun, and Hinton 2021), some of the pioneers and advocates of DL for AI highlighted the need to address the limitations of DL in order to tackle some of the human-like reasoning capabilities. In particular, they mention DL's current inability to perform deliberate systematic reasoning and planning as described by Kahneman's 'System 2' reasoning (Kahneman 2012).</p>
<p>Reconciling methodologies in distinct areas of learning and reasoning (e.g. statistics and logic) means combining the respective advantages while circumventing the shortcomings and limitations (Besold et al. 2017). Some of the approaches taken to reconcile symbolic and sub-symbolic reasoning include the following (not in any way exhaustive): creating a one-to-one correspondence between artificial neurons and elements of logical formulae (Riegel et al. 2020); using reinforcement learning with Monte-Carlo tree search to play Go in AlphaGo (Silver et al. 2016); differentiable architecture search by applying reinforcement learning over a discrete and non-differentiable search space (Liu, Simonyan, and Yang 2018); combining deductive and inductive reasoning methods for question answering   , (Nuamah and Bundy 2020); neural networks with external memory in the Neural Turing Machine (NTM) (Graves, Wayne, and Danihelka 2014) and reinforcement learning NTM variants (Zaremba and Sutskever 2016) to make them more expressive; memory networks ) ; probabilistic reasoning and program induction (Manhaeve et al. 2018). There is also a lot of interest in (Cozman and Munhoz 2021) for enhancing machine learning with knowledge representation and reasoning. In addition, relational reasoning using neural networks is showing a lot of promise in the visual QA context (Santoro et al. 2017) (Santoro et al. 2018). A review in (Aditya, Yang, and Baral 2019) also discusses different techniques for integrating knowledge and reasoning for image understanding.</p>
<p>A common theme in most of the work exploring hybrid AI is the need for symbol manipulation on models of the world, while being able to leverage other sub-symbolic machinery to learn these models from examples or to predict actions based on the models. These capabilities are also essential for performing algorithm reasoning.</p>
<p>Compositionality</p>
<p>The space of algorithms and algorithmic reasoning is far too large and varied to construct a single neural network model to solve it in practical way. It is not always possible to program or train one AI system to solve diverse kinds of problems. In many cases, even the ability of an expert to engineer a system to solve a range of problems, such as that of opendomain QA, is limited by the fact that one cannot anticipate all the possible kinds of questions to answer and how to combine existing AI modules achieve it. Compositionality provides a mechanism to compose solutions to problems by automating the combining of existing AI modules to solve new and varied problems. In this work, we use "compositionality" in a loose sense to include the entire spectrum from the high level integration of distinct AI components and systems, through to automatic program composition, all the way to the deeper level integration of knowledge representation, semantics and neural embedding. Different approaches can be used to build such compositional AI systems. We highlight a few below, but it is in no way an exhaustive list. (Gaunt et al. 2017) created an end-to-end trainable system, NEURAL TERPRET, that learns to write interpretable algorithms with perceptual components, while the Neural Turing Machine (Graves, Wayne, and Danihelka 2014) extend neural networks with an external memory such that the network can infer simple programs such as copying and sorting. Some neural-symbolic methods provide compositionality by treating the symbolic and neural network modules both as black boxes and integrating them by exposing appropriate functions (Tsamoura, Hospedales, and Loizos 2021). In a majority of cases, compositionality is achieved by mapping neural network modules onto the semantic parse tree of a natural language question or the generation of sequences of functions from the question text using a trained network (Andreas et al. 2016) (Yi et al. 2019) (Liang et al. 2017) (Kapanipathi et al. 2020) (Johnson et al. 2017a) (Johnson et al. 2017b). The generated program is then executed to answer the question.</p>
<p>However, generating a neural network architecture from a semantic parse tree of natural language text is not enough to achieve algorithmic reasoning. This is because intermediate reasoning steps such has handling failure due to the lack of relevant data cannot be recovered from in a shallow parse tree without any further reasoning or inference steps. Sometimes, the data retrieved at one step during inference determines how the rest of the algorithm is developed. For instance, in a question such as "Which country in Europe will have the highest GDP growth rate by 2032", the kind of data retrieved (or the lack thereof) will determine if retrieval is sufficient, or a more involving regression on past data for prediction will be needed. Hence, the automatic formulation of new algorithms using existing components requires one to look beyond the initial parse tree of the question and to work within the constraints of pre-and post-conditions of the underlying symbolic and sub-symbolic modules in order to combine them appropriately.</p>
<p>Achieving the task of algorithmic reasoning in the domain of question answering requires us to have some expectation of what such a system should look like and how it should behave. We list three of these below, all of which introduce new challenges that, if solved, will advance the development of AI architectures for QA.</p>
<p>• Interpretability: One of the basic requirements of a QA system with algorithmic reasoning capabilities is that its inner workings are interpretable and inspectable by a human user. Additionally, the intepretability allows a user to check if the pre-and post-conditions of the algorithms are satisfied. For instance, if heterogeneous modules are automatically composed to form novel algorithms which answer a question, one should be able to verify that the conditions associated with the appropriate use of the modules have been met. A key requirement for interpretability is a representation of the inference mechanism which supports both symbolic and sub-symbolic inference. For example, a dual (or hybrid) representation which supports both deductive inference through symbol manipulation and inductive inference over data observations using statistical methods will be needed. Better still, a representation which allows for a fluid translation between these representations will be useful.</p>
<p>• Generalizability: It is also important to think of QA problems at a much broader level beyond the narrow vertical perspectives, such as image recognition, prediction or classification tasks, in order to build the capabilities of the AI systems for algorithmic reasoning. One of the criticisms of narrow AI is the fact that they solve very specific problems well, but rarely capture most of the complexities which need to be dealt with in real-world applications. Most of these complexities are often handled by an expert. A desirable feature of AI systems in QA which performs algorithmic reasoning is that they are not restricted to neatly defined problems in benchmark datasets which sometimes lead to over-engineering of AI architectures that are built to exploit biases that are observable in the data set. Additionally, it is desirable for QA systems to be general in how they compose algorithms, both in the aspects of the QA processes that they use and the in kinds of problems that they can solve.</p>
<p>• Robustness: Finally, there is a need to build AI systems that are robust in the presence of noise, incomplete data and uncertainty. Robustness is also needed as knowledge changes or new knowledge is acquired. These are obvious problems that are faced when using AI in the real-world and so working only on problems or data sets that exclude these challenges results in AI systems which are brittle. In algorithmic reasoning in particular, it is necessary to build AI systems which are able to identify these uncertainties and incorporate them in the inference process and in the automatic generation of programs to solve problems. For instance, failures to access data or inconsistencies in data retrieved from KBs should not stop the QA system from finding answers to questions if alternative strategies for solving the question can be found using a different algorithm. However, how the AI system deals with such issues should be transparent to users.</p>
<p>In summary, many of the debates about symbolic versus sub-symbolic AI cease to exist when the scope of the problem being solved is viewed in its entirety; i.e. to include not only the specific task of prediction or classification, but other intermediate reasoning and decision steps (see Figure  1) which are often performed by the creators of the AI system and left out of the scope of what the system does.</p>
<p>Deep Algorithmic QA: Hybrid + Compositionality</p>
<p>Our proposed approach to algorithmic reasoning for question answering, DAQA, leverages both hybrid AI and compositionality. Specifically, we are interested not only in a narrow aspect of the question answering task, but in the often ignored aspects of the tasks usually hidden under the list of things which an engineer or expert user does. DAQA is deep in two senses: (1) the inference graphs constructed are deeper than the initial semantic parse trees of the question;</p>
<p>(2) it uses deep neural networks as part of the inference framework.</p>
<p>We use the following question example to shed light on the different aspects of our proposal: "What will be the population of the country in Europe which is predicted to have the highest GDP in 2032?".</p>
<p>Motivation</p>
<p>First, we make no assumptions about the presence of data needed to answer the question. We only assume that the AI system has a list of different KBs than it can access. These could be web documents sources that it has crawled, publicly available knowledge graphs with interfaces for querying data (e.g. SPARQL (World Wide Web Consortium, W3C 2013) or a web-based application programming interface (API)). This means that the choice of KBs to query and the integration of data from diverse sources is not trivial. Different modalities (text, images, videos) and formalisms (unstructured text, RDF, graph, probabilistic, etc.) make the task all the more difficult.</p>
<p>Second, we do not assume that the answer is pre-stored in any KB. In the question above, chances of having an exact answer stored some knowledge is very low to non-existent. As such, the only way to solve this question is to reason about it and dynamically construct an algorithm that can solve it.</p>
<p>Third, we claim that creating a deep neural network model which is trained in an end-to-end way to tackle open-domain QA including questions of the kind that we have above is not practical with the present state of the technology. However, simpler neural network models are available for solving aspects of the problem, such as the semantic parsing task and the prediction task. This brings to the fore a need for a compositional approach. That is, general purpose neural network models, statistical and arithmetic inference operations can be composed in a dynamic way to construct an appropriate algorithm that solves the question. Constraints Functional node, , with functions and ′ that transform the symbolic attribute-value pairs in to real-valued vectors in q Figure 3: (a) Shows the base inference graph with a question node and an answer node that is to be inferred. They are linked by an edge that can be split by applying decomposition operations on the question node. (b) An inference graph made up of functional nodes and edges labelled by operations for predicting decomposition and aggregation functions. Decomposition sub-graph (in red) is guided by a function ∆ that decomposes a functional node to create new continuations of the inference graph, and aggregation sub-graph (in green) which uses a model σ to select appropriate functions to combine nodes. Functional nodes provide both a symbolic and vector representation of the node's attribute-value internal representation, as well as function g and g for converting between the two representations.</p>
<p>on the individual inference modules such as pre-conditions and post-conditions ensure that they are composed in a computationally valid way.</p>
<p>Fourth, we do not assume that a correct semantic parse of the question is enough to compose a program which answers the question. In addition to semantic parsing, it is necessary to reason about the question to explore possible algorithms which could solve it (see Figure 2). In the above question, for example, there are tasks such as prediction that will not be explicit in the parse tree. As such, it is important to consider deductive methods to decompose the problem. Additionally, such decomposition needs to be recursive and be robust in the event of a failure to infer an answer by exploring different possible deductions simultaneously. Hybrid AI plays a significant role here as it provide a substrate on which to perform reasoning in the inference process while offering a more rigorous inductive mechanisms to draw inferences from data.</p>
<p>Proposed Model</p>
<p>A fundamental part of the above motivation is that of knowledge representation which supports both hybrid AI and compositionality. Although we leverage symbolic AI methods, we do not propose a classic expert system-styled mechanism. Instead, we propose the idea of hybrid inference graphs with functional nodes and illustrate these in Figure  3. An inference graph is constructed and expanded dynamically through the decompositions of its functional nodes using rules that are learned. Functional nodes represent three things:</p>
<ol>
<li>
<p>data: includes parsed information from the question, data to be inferred (represented by variables to be inferred) or data retrieved from KBs or inferred and propagated from other functional nodes.</p>
</li>
<li>
<p>the functional operations to be applied, e.g. regression.</p>
</li>
</ol>
<p>These operation could themselves be neural networks for prediction, classification, etc.</p>
<ol>
<li>a model to convert between the symbolic and vectorized representation of the functional node, possibly obtained through an aggregation of the embeddings of its elements.</li>
</ol>
<p>Functional nodes, therefore, provide support for both the symbolic manipulation of objects and the vector representation which can leverage the capabilities of DL. The edges linking functional nodes in the graph represent rules or transition functions from the state of the functional node to the next. This provides a mechanism for decomposing functional nodes, thereby expanding the frontier of the inference graph. The rules can be provided or learned from data, allowing the inference graph as a whole to be learned. Techniques developed in reinforcement learning can be used to learn these transitions functions in order to predict subsequent decompositions of nodes on the inference graph from a handful of rules and the pre-and post-conditions of the various inference operations.</p>
<p>Training this system as a whole to answer questions can be achieved in two ways. First, one may use some form of weak (or distant) supervision signal such as the question and the expected answer. However, constructing such as large dataset is an expensive and prohibitive process. An alternative is to leverage existing datasets to train the individual modules and learn a model that complements the deduction process by predicting candidate decompositions to be applied and the choice of appropriate operation for aggregating functional nodes.</p>
<p>As new knowledge becomes available, the different submodels needed to construct the inference graph, e.g. the functions ∆ and σ, can updated without having to re-train the entire system. Also, as prior knowledge changes, the representation in the functional nodes n can be updated. Similar to the method used in (Manhaeve et al. 2018), uncertainty values can be inferred and stored in one of the n s attribute-value pairs. This can be the basis of Bayesian updates as prior knowledge from KBs changes.</p>
<p>Discussion</p>
<p>Our proposed approach brings on board novel perspectives on AI for question answering. However, it also builds on some other related ideas and methodologies.</p>
<p>While there have been QA techniques that perform deductive reasoning during inference (e.g. (Fader, Zettlemoyer, and Etzioni 2014)) using operations such as query decomposition and rewriting, they lack the machinery to perform inductive reasoning using more detailed arithmetic and statistical operations. Recent methods such as the FRANK system (Nuamah, Bundy, and Lucas 2016), ) in the FRANK QA system adopt a hybrid inference architecture which allows for deductive reasoning using rules in a recursive manner and aggregation of data for prediction using a variety of inference operations including pre-trained neural network models. However, this approach lacks (1) a neural representation of inference nodes and (2) the ability to intelligently search through the space of inference operations for the appropriate ones to use in order to make inference more efficient. Recent work attempts to improve the automatic selection of kernels for Gaussian Process regression (Fletcher, Bundy, and Nuamah 2021). That said, the recursive approach used allows for the dynamic composition of modular inference operations beyond the one constructed from the syntactic or semantic parse of the question.</p>
<p>AutoML (see survey in (He, Zhao, and Chu 2021)) and AutoAI (Wang et al. 2020) aim to automate the pipeline of ML tasks. Although they address some pipeline tasks such as data cleaning, feature engineering, model selection, etc, current methods do not address some of the reasoning tasks involved in dynamically decomposition the problem to find appropriate answers when specific inference paths fail to yield results.</p>
<p>Many of the QA methods discussed in §2.2 and §2.3 generate programs based on the parse trees from the natural language question, and do not perform any further deductive reasoning or decompositions. As discussed in the respective sections, they are focused on other neuro-symbolic tasks such as integrating knowledge into neural networks and do tackle many of the tasks discussed in §4.</p>
<p>Although (Veličković and Blundell 2021) proposes ideas for achieving neural algorithmic reasoning, it differs from our proposal in two main ways. First, the notion of algorithms is at a different level of granularity. The focus in that paper is on 'lower' level algorithms such as sorting. More complex algorithms which involve higher level operations such as regression for prediction and many other arithmetic and statistical operations are not explored. Secondly, estimating the outputs of the algorithm using a purely neural network approach still suffers from a lack of interpretability given that it is still a black-box from the perspective of a user. This makes it very hard to verify that the neural network is executing the algorithms correctly.</p>
<p>It is worth noting the impact that large language models like GPT-3 (Brown et al. 2020) have had on QA and AI in general. Despite its limitations, as discussed in (Marcus and Davies 2020), its role as a pre-trained model in fine-tuning tasks in other models, for example in CLIP (Radford et al. 2021), is relevant to our proposed inference model. On its own, though, such large language models do not adequately address the desired properties we aim for.</p>
<p>Nevertheless, our proposed model also has some difficulties that need to be overcome. First, constructing a model which allows for the seamless conversion between symbolic and vector representations of the functional nodes across multiple domains is a hard problem and is still an active research area in neuro-symbolic AI. The space of decomposition and aggregation operations is also very large, so, appropriate search optimisations and heuristics will have to be developed to make it tractable. Promising work in architecture search using reinforcement learning (Liu, Simonyan, and Yang 2018) could offer a viable solution. Finally, training the model as a whole will be very hard, but reusing and fine-tuning pre-trained models in a plug-and-play manner within the inference architecture may be a possible solution.</p>
<p>Conclusion</p>
<p>The problem of algorithmic reasoning is one that fits well with the domain of QA since it helps to automate several aspects of the QA pipeline and leads to interpretable models for answering questions. We claim that a hybrid approach to AI with a strong element of compositionality is needed to tackle such a perspective on QA and other AI problems. We have proposed a systems approach to AI which leverages both symbolic and sub-symbolic methods in a framework that leads to solutions which are not possible by either one of these paradigms alone.</p>
<p>Figure 2 :
2Going beyond the semantic parse tree of the question by applying additional decompositions based on rules or pre-trained models for predicting continuations of the inference plan.
AcknowledgmentThe author would like to thank Vaishak Belle, Alan Bundy and Thomas Fletcher for feedback on an earlier draft and Huawei for supporting the research on which this paper was based under grant HO2017050001B8s. The author would also like to thank reviewers for valuable feedback.
Integrating knowledge and reasoning in image understanding. S Aditya, Y Yang, C Baral, arXiv:1906.09954arXiv preprintAditya, S.; Yang, Y.; and Baral, C. 2019. Integrating knowl- edge and reasoning in image understanding. arXiv preprint arXiv:1906.09954.</p>
<p>J Andreas, M Rohrbach, T Darrell, D Klein, arXiv:1601.01705Learning to compose neural networks for question answering. csAndreas, J.; Rohrbach, M.; Darrell, T.; and Klein, D. 2016. Learning to compose neural networks for question answer- ing. arXiv:1601.01705 [cs].</p>
<p>Symbolic Logic meets Machine Learning: A Brief Survey in Infinite Domains. V Belle, arXiv:2006.08480Belle, V. 2020. Symbolic Logic meets Machine Learning: A Brief Survey in Infinite Domains. arXiv:2006.08480 [cs].</p>
<p>Deep learning for AI. Y Bengio, Y Lecun, G Hinton, Communications of the ACM. 647Bengio, Y.; LeCun, Y.; and Hinton, G. 2021. Deep learning for AI. Communications of the ACM 64(7):58-65.</p>
<p>T R Besold, A Garcez, S Bader, H Bowman, P Domingos, P Hitzler, K.-U Kuehnberger, L C Lamb, D Lowd, P M V Lima, L De Penning, G Pinkas, H Poon, G Zaverucha, arXiv:1711.03902Neural-symbolic learning and reasoning: A survey and interpretation. csBesold, T. R.; d'Avila Garcez, A.; Bader, S.; Bowman, H.; Domingos, P.; Hitzler, P.; Kuehnberger, K.-U.; Lamb, L. C.; Lowd, D.; Lima, P. M. V.; de Penning, L.; Pinkas, G.; Poon, H.; and Zaverucha, G. 2017. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv:1711.03902 [cs].</p>
<p>A Bordes, N Usunier, S Chopra, J Weston, arXiv:1506.02075Large-scale Simple Question Answering with Memory Networks. csBordes, A.; Usunier, N.; Chopra, S.; and Weston, J. 2015. Large-scale Simple Question Answering with Memory Net- works. arXiv:1506.02075 [cs].</p>
<p>. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, arXiv:2005.14165arXiv preprintet al. 2020. Language models are few-shot learnersBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Automated reasoning in the age of the internet. A Bundy, K Nuamah, C Lucas, International Conference on Artificial Intelligence and Symbolic Computation. SpringerBundy, A.; Nuamah, K.; and Lucas, C. 2018. Automated reasoning in the age of the internet. In International Confer- ence on Artificial Intelligence and Symbolic Computation, 3-18. Springer.</p>
<p>Some thoughts on knowledge-enhanced machine learning. F G Cozman, H N Munhoz, International Journal of Approximate Reasoning. 136AI Next CampaignDARPACozman, F. G., and Munhoz, H. N. 2021. Some thoughts on knowledge-enhanced machine learning. International Jour- nal of Approximate Reasoning 136:308-324. DARPA. 2018. AI Next Campaign.</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. csDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs].</p>
<p>. M Dubey, D Banerjee, A Abdelkawi, J Lehmann, Dubey, M.; Banerjee, D.; Abdelkawi, A.; and Lehmann, J.</p>
<p>C Ghidini, O Hartig, M Maleshkova, V Svátek, I Cruz, A Hogan, J Song, M Lefrançois, F Gandon, LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia. Springer International Publishing11779The Semantic Web -ISWC 2019LC-QuAD 2.0: A Large Dataset for Complex Ques- tion Answering over Wikidata and DBpedia. In Ghidini, C.; Hartig, O.; Maleshkova, M.; Svátek, V.; Cruz, I.; Hogan, A.; Song, J.; Lefrançois, M.; and Gandon, F., eds., The Seman- tic Web -ISWC 2019, volume 11779. Springer International Publishing. 69-78.</p>
<p>Open question answering over curated and extracted knowledge bases. A Fader, L Zettlemoyer, O Etzioni, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMFader, A.; Zettlemoyer, L.; and Etzioni, O. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Min- ing, 1156-1165. ACM.</p>
<p>T Fletcher, A Bundy, K Nuamah, GPy-ABCD: A Configurable Automatic Bayesian Covariance Discovery Implementation. 8th ICML Workshop on Automated Machine Learning. In PressFletcher, T.; Bundy, A.; and Nuamah, K. 2021. GPy-ABCD: A Configurable Automatic Bayesian Covariance Discovery Implementation. 8th ICML Workshop on Automated Ma- chine Learning (In Press).</p>
<p>A L Gaunt, M Brockschmidt, N Kushman, D Tarlow, PMLR 10Differentiable programs with neural libraries. International Conference on Machine Learning. Gaunt, A. L.; Brockschmidt, M.; Kushman, N.; and Tarlow, D. 2017. Differentiable programs with neural libraries. In- ternational Conference on Machine Learning. PMLR 10.</p>
<p>A Graves, G Wayne, I Danihelka, arXiv:1410.5401Neural turing machines. Graves, A.; Wayne, G.; and Danihelka, I. 2014. Neural turing machines. arXiv:1410.5401 [cs].</p>
<p>Baseball: An automatic question-answerer. B F Green, A K Wolf, C Chomsky, K Laughery, Western Joint IRE-AIEE-ACM Computer Conference on -IRE-AIEE-ACM '61 (Western. ACM Press219Papers Presented at the May 9-11Green, B. F.; Wolf, A. K.; Chomsky, C.; and Laughery, K. 1961. Baseball: An automatic question-answerer. In Pa- pers Presented at the May 9-11, 1961, Western Joint IRE- AIEE-ACM Computer Conference on -IRE-AIEE-ACM '61 (Western), 219. ACM Press.</p>
<p>AutoML: A Survey of the State-of-the-Art. X He, K Zhao, X Chu, Knowledge-Based Systems. 212106622He, X.; Zhao, K.; and Chu, X. 2021. AutoML: A Survey of the State-of-the-Art. Knowledge-Based Systems 212:106622.</p>
<p>CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C L Zitnick, R Girshick, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEJohnson, J.; Hariharan, B.; van der Maaten, L.; Fei-Fei, L.; Zitnick, C. L.; and Girshick, R. 2017a. CLEVR: A di- agnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 1988-1997. IEEE.</p>
<p>Inferring and Executing Programs for Visual Reasoning. J Johnson, B Hariharan, L Van Der Maaten, J Hoffman, L Fei-Fei, C L Zitnick, R Girshick, 2017 IEEE International Conference on Computer Vision (ICCV). IEEEJohnson, J.; Hariharan, B.; Van Der Maaten, L.; Hoffman, J.; Fei-Fei, L.; Zitnick, C. L.; and Girshick, R. 2017b. Infer- ring and Executing Programs for Visual Reasoning. In 2017 IEEE International Conference on Computer Vision (ICCV), 3008-3017. IEEE.</p>
<p>Thinking, Fast and Slow. Penguin Psychology. D Kahneman, Penguin BooksKahneman, D. 2012. Thinking, Fast and Slow. Penguin Psychology. Penguin Books.</p>
<p>P Kapanipathi, I Abdelaziz, S Ravishankar, S Roukos, A Gray, R Astudillo, M Chang, C Cornelio, S Dana, A Fokoue, D Garg, A Gliozzo, S Gurajada, H Karanam, N Khan, D Khandelwal, Y.-S Lee, Y Li, F Luus, N Makondo, N Mihindukulasooriya, T Naseem, S Neelam, L Popa, R Reddy, R Riegel, G Rossiello, U Sharma, G P S Bhargav, M Yu, arXiv:2012.01707Question answering over knowledge bases by leveraging semantic parsing and neuro-symbolic reasoning. csKapanipathi, P.; Abdelaziz, I.; Ravishankar, S.; Roukos, S.; Gray, A.; Astudillo, R.; Chang, M.; Cornelio, C.; Dana, S.; Fokoue, A.; Garg, D.; Gliozzo, A.; Gurajada, S.; Karanam, H.; Khan, N.; Khandelwal, D.; Lee, Y.-S.; Li, Y.; Luus, F.; Makondo, N.; Mihindukulasooriya, N.; Naseem, T.; Nee- lam, S.; Popa, L.; Reddy, R.; Riegel, R.; Rossiello, G.; Sharma, U.; Bhargav, G. P. S.; and Yu, M. 2020. Question answering over knowledge bases by leveraging semantic parsing and neuro-symbolic reasoning. arXiv:2012.01707 [cs].</p>
<p>Lar: A logic of algorithmic reasoning. F Kröger, Acta Informatica. 83Kröger, F. 1977. Lar: A logic of algorithmic reasoning. Acta Informatica 8(3):243-266.</p>
<p>Graph neural networks meet neuralsymbolic computing: A survey and perspective. L Lamb, A Garcez, M Gori, M Prates, P Avelar, M Vardi, IJCAI-PRICAI 2020-29th International Joint Conference on Artificial Intelligence-Pacific Rim International Conference on Artificial Intelligence. Lamb, L.; Garcez, A.; Gori, M.; Prates, M.; Avelar, P.; and Vardi, M. 2020. Graph neural networks meet neural- symbolic computing: A survey and perspective. In IJCAI- PRICAI 2020-29th International Joint Conference on Arti- ficial Intelligence-Pacific Rim International Conference on Artificial Intelligence.</p>
<p>C Liang, J Berant, Q Le, K D Forbus, N Lao, arXiv:1611.00020Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. csLiang, C.; Berant, J.; Le, Q.; Forbus, K. D.; and Lao, N. 2017. Neural symbolic machines: Learn- ing semantic parsers on freebase with weak supervision. arXiv:1611.00020 [cs].</p>
<p>Learning Dependency-Based Compositional Semantics. P Liang, M I Jordan, D Klein, Computational Linguistics. 392Liang, P.; Jordan, M. I.; and Klein, D. 2013. Learning Dependency-Based Compositional Semantics. Computa- tional Linguistics 39(2):389-446.</p>
<p>DeepProbLog: Neural probabilistic logic programming. H Liu, K Simonyan, Y Yang, R Manhaeve, S Dumancic, A Kimmig, T Demeester, L De Raedt, arXiv:1806.09055Darts: Differentiable architecture search. 31arXiv preprintLiu, H.; Simonyan, K.; and Yang, Y. 2018. Darts: Differen- tiable architecture search. arXiv preprint arXiv:1806.09055. Manhaeve, R.; Dumancic, S.; Kimmig, A.; Demeester, T.; and De Raedt, L. 2018. DeepProbLog: Neural probabilistic logic programming. Advances in Neural Information Pro- cessing Systems 31:3749-3759.</p>
<p>GPT-3, Bloviator: Ope-nAI's language generator has no idea what it's talking about. G Marcus, E Davies, Marcus, G., and Davies, E. 2020. GPT-3, Bloviator: Ope- nAI's language generator has no idea what it's talking about. https://www.technologyreview.com/2020/08/22/1007539/gpt3- openai-language-generator-artificial-intelligence-ai- opinion/.</p>
<p>Rebooting AI: Building Artificial Intelligence We Can Trust. G Marcus, Davis , E , Marcus, G., and Davis, E. 2019. Rebooting AI: Building Artificial Intelligence We Can Trust.</p>
<p>Explainable inference in the frank query answering system. K Nuamah, A Bundy, K Nuamah, A Bundy, C Lucas, Functional Inferences Over Heterogeneous Data. Ph.D. Dissertation, School of Informatics. SpringerUniversity of EdinburghInternational Conference on Web Reasoning and Rule SystemsNuamah, K., and Bundy, A. 2020. Explainable inference in the frank query answering system. In ECAI 2020. IOS Press. 2441-2448. Nuamah, K.; Bundy, A.; and Lucas, C. 2016. Func- tional inferences over heterogeneous data. In International Conference on Web Reasoning and Rule Systems, 159-166. Springer. Nuamah, K. 2018. Functional Inferences Over Heteroge- neous Data. Ph.D. Dissertation, School of Informatics, Uni- versity of Edinburgh.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXiv:2103.00020arXiv:2006.13155Logical neural networks. arXiv preprintcsRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020. Riegel, R.; Gray, A.; Luus, F.; Khan, N.; Makondo, N.; Akhalwaya, I. Y.; Qian, H.; Fagin, R.; Barahona, F.; Sharma, U.; Ikbal, S.; Karanam, H.; Neelam, S.; Likhyani, A.; and Srivastava, S. 2020. Logical neural networks. arXiv:2006.13155 [cs].</p>
<p>A Santoro, D Raposo, D G Barrett, M Malinowski, R Pascanu, P Battaglia, T Lillicrap, arXiv:1706.01427A simple neural network module for relational reasoning. arXiv preprintSantoro, A.; Raposo, D.; Barrett, D. G.; Malinowski, M.; Pascanu, R.; Battaglia, P.; and Lillicrap, T. 2017. A sim- ple neural network module for relational reasoning. arXiv preprint arXiv:1706.01427.</p>
<p>A Santoro, R Faulkner, D Raposo, J Rae, M Chrzanowski, T Weber, D Wierstra, O Vinyals, R Pascanu, T Lillicrap, arXiv:1806.01822Relational recurrent neural networks. arXiv preprintSantoro, A.; Faulkner, R.; Raposo, D.; Rae, J.; Chrzanowski, M.; Weber, T.; Wierstra, D.; Vinyals, O.; Pascanu, R.; and Lillicrap, T. 2018. Relational recurrent neural networks. arXiv preprint arXiv:1806.01822.</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 5297587Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; van den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; Dieleman, S.; Grewe, D.; Nham, J.; Kalchbrenner, N.; Sutskever, I.; Lillicrap, T.; Leach, M.; Kavukcuoglu, K.; Graepel, T.; and Hassabis, D. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529(7587):484-489.</p>
<p>Neuralsymbolic integration: A compositional perspective. E Tsamoura, T Hospedales, M Loizos, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Tsamoura, E.; Hospedales, T.; and Loizos, M. 2021. Neural- symbolic integration: A compositional perspective. Pro- ceedings of the AAAI Conference on Artificial Intelligence 35(6).</p>
<p>R Usbeck, A.-C N Ngomo, B Haarmann, A Krithara, M Roder, G Napolitano, 7th Open Challenge on Question Answering over Linked Data. 11Usbeck, R.; Ngomo, A.-C. N.; Haarmann, B.; Krithara, A.; Roder, M.; and Napolitano, G. 2017. 7th Open Challenge on Question Answering over Linked Data (QALD-7). 11.</p>
<p>P Veličković, C Blundell, arXiv:2105.02761Neural algorithmic reasoning. cs, math, statVeličković, P., and Blundell, C. 2021. Neural algorithmic reasoning. arXiv:2105.02761 [cs, math, stat].</p>
<p>AutoAI: Automating the End-to-End AI Lifecycle with Humans-in-the-Loop. D Wang, P Ram, D K I Weidele, S Liu, M Muller, J D Weisz, A Valente, A Chaudhary, D Torres, H Samulowitz, L Amini, Proceedings of the 25th International Conference on Intelligent User Interfaces Companion. the 25th International Conference on Intelligent User Interfaces CompanionACMWang, D.; Ram, P.; Weidele, D. K. I.; Liu, S.; Muller, M.; Weisz, J. D.; Valente, A.; Chaudhary, A.; Torres, D.; Samu- lowitz, H.; and Amini, L. 2020. AutoAI: Automating the End-to-End AI Lifecycle with Humans-in-the-Loop. In Pro- ceedings of the 25th International Conference on Intelligent User Interfaces Companion, 77-78. ACM.</p>
<p>J Weston, S Chopra, A Bordes, arXiv:1410.3916Memory Networks. cs, statWeston, J.; Chopra, S.; and Bordes, A. 2015. Memory Networks. arXiv:1410.3916 [cs, stat].</p>
<p>World Wide Web Consortium, W3C. Sparql 1.1 overviewWorld Wide Web Consortium, W3C. 2013. Sparql 1.1 overview.</p>
<p>K Yi, J Wu, C Gan, A Torralba, P Kohli, J B Tenenbaum, arXiv:1810.02338Neural-symbolic VQA: Disentangling reasoning from vision and language understanding. csYi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenenbaum, J. B. 2019. Neural-symbolic VQA: Disen- tangling reasoning from vision and language understanding. arXiv:1810.02338 [cs].</p>
<p>Reinforcement learning neural turing machines -revised. W Zaremba, I Sutskever, arXiv:1505.00521Zaremba, W., and Sutskever, I. 2016. Reinforcement learn- ing neural turing machines -revised. arXiv:1505.00521 [cs].</p>            </div>
        </div>

    </div>
</body>
</html>
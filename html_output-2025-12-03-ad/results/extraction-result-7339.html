<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7339 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7339</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7339</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-275758501</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.11430v2.pdf" target="_blank">A Survey on Diffusion Models for Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> .</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploiting multimodal latent diffusion models for accurate anomaly detection in industry 5.0. <em>(Rating: 2)</em></li>
                <li>DIFFender <em>(Rating: 2)</em></li>
                <li>Statistical test on diffusion model-based anomaly detection by selective inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7339",
    "paper_id": "paper-275758501",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploiting multimodal latent diffusion models for accurate anomaly detection in industry 5.0.",
            "rating": 2,
            "sanitized_title": "exploiting_multimodal_latent_diffusion_models_for_accurate_anomaly_detection_in_industry_50"
        },
        {
            "paper_title": "DIFFender",
            "rating": 2
        },
        {
            "paper_title": "Statistical test on diffusion model-based anomaly detection by selective inference",
            "rating": 1,
            "sanitized_title": "statistical_test_on_diffusion_modelbased_anomaly_detection_by_selective_inference"
        }
    ],
    "cost": 0.00654675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Diffusion Models for Anomaly Detection
22 Jan 2025</p>
<p>Jing Liu jingliu19@fudan.edu.cn 
Fudan University
ShanghaiChina</p>
<p>University of British Columbia
VancouverCanada</p>
<p>Zhenchao Ma zhenchaoma@ece.ubc.ca 
University of British Columbia
VancouverCanada</p>
<p>Zepu Wang zwang@ece.ubc.ca 
Duke Kunshan University
SuzhouChina</p>
<p>Yang Liu yangliu20@fudan.edu.cn 
Fudan University
ShanghaiChina</p>
<p>Duke Kunshan University
SuzhouChina</p>
<p>Zehua Wang 
University of British Columbia
VancouverCanada</p>
<p>Peng Sun peng.sun568@duke.edu 
Duke Kunshan University
SuzhouChina</p>
<p>Liang Song songl@fudan.edu.cn 
Fudan University
ShanghaiChina</p>
<p>Bo Hu bohu@fudan.edu.cn 
Fudan University
ShanghaiChina</p>
<p>Azzedine Boukerche aboukerc@uottawa.ca 
University of Ottawa
OttawaCanada</p>
<p>Victor C M Leung vleung@ieee.org 
University of British Columbia
VancouverCanada</p>
<p>A Survey on Diffusion Models for Anomaly Detection
22 Jan 202500E3EADB528651EF27C0A26FC4670D79arXiv:2501.11430v2[cs.LG]
Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing.The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data.In this survey, we review recent advances in DMAD research.We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs.We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations.We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis.Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models.The collection of DMAD research papers and resources is available at https://github.com/fdjingliu/DMAD.</p>
<p>Introduction</p>
<p>The ever-increasing volume, velocity, and variety of data present both opportunities and challenges for anomaly detection (AD).In cybersecurity, real-time threat detection is crucial given the constant influx of network traffic and logs [Liu et al., 2024b].Similarly, robust fraud detection systems are essential for financial institutions processing massive transaction datasets [Wang et al., 2023a].Healthcare and manufacturing also rely heavily on data for early disease diagnosis and predictive maintenance, respectively [Zhang et al., 2023].Such applications highlight the growing need for automated AD methods to efficiently identify outliers in complex datasets.However, traditional techniques, often based on statistical methods or rule-based systems, struggle with the scale and complexity of modern data [Katsuoka et al., 2024].Specifically, these methods often require extensive manual feature engineering and struggle to adapt to evolving data distributions.Additionally, the sheer data volume can overwhelm traditional methods, rendering them computationally infeasible for real-time applications.Consequently, more sophisticated and scalable AD approaches are needed.For example, weakly supervised approaches leverage limited labeled data to improve performance [Liu et al., 2024c], driving innovation toward scalable deep learning architectures that address these fundamental challenges.</p>
<p>Diffusion models (DMs) have emerged as a powerful class of generative models, synthesizing samples across diverse data modalities [Wang et al., 2023c].Unlike generative adversarial networks (GANs) and variational autoencoders (VAEs), which may exhibit training instability or produce less detailed samples [Dao et al., 2024], DMs generate sharper, more realistic samples through a gradual denoising process [Chen et al., 2024b].Consequently, DMs excel in capturing a broader range of data distributions compared to GANs [Wyatt et al., 2022], making them particularly suitable for anomaly detection, where their enhanced mode coverage facilitates precise modeling of normal patterns and subsequent identification of distributional outliers [Li et al., 2024].</p>
<p>Recent growth in complex and multi-dimensional data has created new challenges for anomaly detection methods.In this context, DMs have emerged as a promising solution due to their inherent connection to density estimation [Le Lan and Dinh, 2021].DMs learn the probability distributions of normal data through iterative denoising.Leveraging their exceptional generative abilities, DMs accurately reconstruct normal patterns while capturing underlying manifold structures, enabling anomaly detection through reconstruction error analysis and probability density estimation [Hu and Jin, 2023].For example, AnoDDPM [Wyatt et al., 2022] uses diffusion models for anomaly detection via reconstruction error, and ODD [Wang et al., 2023b] enhances this by leveraging advanced generative modeling for high-dimensional data.Additionally, the learned score function, representing the gradi- ent of the log-probability density, can be directly used as an anomaly score, as demonstrated in diffusion time estimation [Livernoche et al., 2023].</p>
<p>Motivations.While existing surveys have separately reviewed anomaly detection [Liu et al., 2024b;Liu et al., 2024c] and diffusion models [Luo, 2023;Yang et al., 2024b], they either focus on traditional deep learning approaches without considering DMs' emerging role, or treat anomaly detection merely as one of many DM applications without indepth analysis.The field of diffusion models for anomaly detection (DMAD) still lacks a systematic and comprehensive review that specifically examines how DMs enhance anomaly detection capabilities, which motivates our work to provide a comprehensive and timely overview of DMAD, capturing its current state and future developments.</p>
<p>Contributions.The main contributions of this survey are four-fold: (1) A systematic taxonomy.We present the first comprehensive taxonomy of DMAD (as shown in Fig. 1), categorizing existing methodologies into reconstruction-based, density-based, and hybrid methods, providing a structured framework for understanding this emerging field.</p>
<p>(2) A comprehensive review.Based on the proposed taxonomy, we systematically analyze DMAD across different tasks, including image anomaly detection (IAD), time series anomaly detection (TSAD), video anomaly detection (VAD), and multimodal anomaly detection (MAD), discussing their approaches, strengths, and limitations.</p>
<p>(3) Technical challenges and future directions.We identify and discuss critical challenges in DMAD, including computational costs, interpretability, robustness against adversarial attacks, complex data distributions, edge-cloud collaboration, and integration with large language models (LLMs), while suggesting potential solutions and future research directions.(4) Resources and benchmarks.We provide a thorough collection of representative methods, implementations, datasets, and evaluation metrics for different tasks, serving as a valuable resource for researchers and practitioners in this field.</p>
<p>Preliminaries</p>
<p>This section begins by providing the fundamental concepts of anomaly detection.Following this, we introduce the various types of anomalies, including point, contextual, and collective anomalies.Next, we discuss the basic principles of DMs and their relevance to anomaly detection.We then explore score-based models and score matching techniques.Finally, we present prominent variants of DMs used in anomaly detection, highlighting their advantages and disadvantages.</p>
<p>Anomaly Detection</p>
<p>Anomaly detection aims to identify instances deviating significantly from the expected data distribution [Wang and Vastola, 2023].Given a dataset D = {x 1 , x 2 , ..., x n }, where  et al., 2023a;Xiao et al., 2023;Zuo et al., 2024].The choice of metric depends on the application and the relative importance of minimizing false positives and negatives [Liu et al., 2024c].
x i ∈ R d ,</p>
<p>Diffusion Models</p>
<p>DMs constitute a class of latent variable models defined by forward and reverse Markov processes to corrupt and denoise data, with training based on the variational lower bound of the negative log-likelihood.Forward Process.Forward process is known as the diffusion process, gradually adds noise to data x 0 over T timesteps, transforming its distribution into an isotropic Gaussian.Specifically, given a variance schedule {β t } T t=1 , where 0 &lt; β 1 &lt; • • • &lt; β T &lt; 1, the process generates sequence of latent variables {x 1 , . . ., x T } via a Markov chain with the Gaussian transition kernel:
q(x t |x t−1 ) = N (x t ; 1 − β t x t−1 , β t I).
(1)</p>
<p>Here, N (x; µ, Σ) denotes a Gaussian distribution with mean µ and covariance Σ, and I is the identity matrix.The parameter β t controls the noise added at each step.As t increases, x T approaches Gaussian noise.Due to the Markov property, the joint distribution is:
q(x 1:T |x 0 ) = T t=1 q(x t |x t−1 ).(2)
Consequently, any x t can be directly sampled from x 0 :
q(x t |x 0 ) = N (x t ; √ ᾱt x 0 , (1 − ᾱt )I),(3)
where α t = 1 − β t and ᾱt = t i=1 α i , which provides computational benefits.Related work explores this process through various perspectives, including multiplicative transitions, uniformization techniques for convergence analysis, Gaussian process covariance transformations, tensor network modeling, first-passage time statistics, and infinitedimensional diffusion processes [Kumar et al., 2023].</p>
<p>Reverse Process.Reverse process aims to learn the conditional probability distribution p θ (x t−1 |x t ) to recover the original data distribution by iteratively removing noise.It is typically modeled as a Gaussian distribution:
p θ (x t−1 |x t ) = N (x t−1 ; µ θ (x t , t), Σ θ (x t , t)),(4)
where µ θ and Σ θ are the predicted mean and covariance, respectively, parameterized by a neural network (e.g., U-Net [Yan et al., 2024] or Transformer [Cao et al., 2024]) and conditioned on the noisy input x t at timestep t.While the covariance Σ θ (x t , t) can be learned, a common simplification is to use a fixed, time-dependent variance [Salimans and Ho, 2021].The neural network learns to predict the mean µ θ (x t , t), which guides denoising.Several parameterization strategies exist, including predicting the mean of q(x t−1 |x t , x 0 ), added noise ϵ t , or original data x 0 .Starting from Gaussian noise x T , the reverse process iteratively applies p θ (x t−1 |x t ) until x 0 is reached.</p>
<p>Training Objective.DMs are trained by maximizing the likelihood of observed data.However, due to the latent variable formulation, direct optimization is intractable.Consequently, training relies on maximizing a variational lower bound (VLB) of the data log-likelihood, derived using variational inference [Bercea et al., 2023] as:
L vlb =E q [D KL (q(x T |x 0 )||p θ (x T ))] − log p θ (x 0 |x 1 ) +E q [ T t=2 D KL (q(x t−1 |x t , x 0 )||p θ (x t−1 |x t ))],(5)
where q and p θ denote the forward and learned reverse process distributions, respectively.Here, D KL is the Kullback-Leibler divergence, x 0 represents the original data, and x t is the noisy sample at timestep t.A simplified objective minimizes the L 2 distance between true and predicted noise [Han et al., 2024b], can be expressed as:
L simple = T t=1 E q ||ϵ t − ϵ θ (x t , t)|| 2 ,(6)
where ϵ t and ϵ θ (x t , t) denote the true and predicted noise, respectively.Such simplified objective exhibits computational efficiency and effectiveness</p>
<p>Score-based Models and Score Matching</p>
<p>Score Matching for Learning Score Functions.Scorebased generative models learn the score function ∇ x log p(x), representing the gradient of the log-probability density.By encoding distributional characteristics directly through these gradients, score functions enable powerful generative capabilities without explicit density estimation.Score matching offers a practical way to learn this score function without needing p(x) explicitly.Instead, it minimizes the Fisher divergence between the learned and true score functions [Han et al., 2024b].Specifically, the score matching objective is
L SM (θ) = 1 2 E p(x) ∥∇ x log p θ (x) − ∇ x log p(x)∥ 2 .
Denoising Score Matching.However, computing the true score is often intractable.Consequently, denoising score matching perturbs data with Gaussian noise and learning the score of this perturbed distribution.The objective becomes
L DSM (θ) = 1 2 E qσ(x) ∥∇ x log p θ (x) − ∇ x log q σ (x)∥ 2
, where q σ (x) is the perturbed distribution.Learning scores at various noise levels allows DMs to capture the data distribution at different scales [Wang et al., 2023d].</p>
<p>Advanced Techniques.Recent research highlights the inherent linear structure within score-based models, particularly at higher noise levels, potentially accelerating sampling [Wang and Vastola, 2023].Prioritizing specific noise levels during training, based on perceptual relevance, can further improve performance [Choi et al., 2022].In addition, gradient guidance, incorporating an external objective function's gradient during sampling, adapts pre-trained models to specific tasks [Tur et al., 2023b].</p>
<p>Variants of Diffusion Models</p>
<p>In this section, we discuss prominent diffusion model variants for anomaly detection, including denoising diffusion probabilistic models (DDPMs), denoising diffusion implicit models (DDIMs), and score-based generative models with stochastic differential equations (Score SDEs), along with their advantages and disadvantages.</p>
<p>DDPMs.DDPMs [Bercea et al., 2023] progressively introduce Gaussian noise to the data across multiple time steps using a Markov chain.A trained neural network then reverses this process, enabling the model to effectively learn complex distributions, which makes it particularly well-suited for anomaly detection [Wyatt et al., 2022].However, generating samples can be computationally expensive.paradigm, we consider three reconstruction strategies, as illustrated in Fig. 2: 1) basic reconstruction, where reconstruction error serves as the anomaly score; 2) latent space reconstruction, incorporating dimensionality reduction for efficiency with high-dimensional data; 3) conditional reconstruction, leveraging auxiliary information like class labels for potentially more refined anomaly detection.Integrating AEs with DMs offers a promising approach.For example, DMs trained on AE-learned latent representations can capture the normal data distribution in the compressed space [Hu and Jin, 2023].In addition, NGLS-Diff [Han et al., 2024a] is a novel approach that utilizes DMs within a normal gathering latent space to enhance anomaly detection capabilities for time series data.The latent space reconstruction error then serves as an anomaly score, with deviations indicating potential anomalies [Wang et al., 2023c].Shared latent spaces across multiple AEs can separate treatment and context.Similarly, supervised dimensionality reduction can optimize the latent space for specific classification tasks, and employing Brownian motion within Diffusion VAEs captures dataset topology [Le Lan and Dinh, 2021].</p>
<p>Conditional Reconstruction.Conditional information, such as class labels, masks, or text descriptions, guides the diffusion model's reconstruction toward expected normal outputs.For example, FDAE [Zhu et al., 2024] utilizes foreground objects and motion information as inputs to train a conditional diffusion autoencoder, enhancing the detection of anomalous regions in an unsupervised manner.Dual conditioning [Zhan et al., 2024] improves multi-class anomaly detection by ensuring prediction and reconstruction accuracy within the expected category.A learnable encoder in RecDMs [Xu et al., 2023] extracts semantic representations for conditional denoising, thus guiding recovery and avoiding trivial reconstructions.Similarly, GLAD [Yao et al., 2025] introduces synthetic anomalies during training to encourage the model to learn complex noise distributions for improved anomaly-free reconstruction.In addition, [Tebbe and Tayyub, 2024] explores dynamic step size computation based on initial anomaly predictions for refined reconstruction.Finally, MDPS [Wu et al., 2024] models normal image reconstruction as multiple diffusion posterior sampling using a masked noisy observation model and a diffusion-based prior, consequently improving reconstruction quality.</p>
<p>Density-based Anomaly Detection</p>
<p>This section considers two primary methods for densitybased AD (DAD), as shown in Fig. 3: Score function and diffusion time estimation (DTE), illustrated by their use of the learned probability density to identify outliers.</p>
<p>Score Function as Anomaly Score.Gradient-based log probability density score functions, ∥∇ x log p(x)∥, accurately and effectively quantify data point likelihoods within learned distributions [Zhang and Pilanci, 2024].Within lowdensity regions, higher score magnitudes reliably indicate potential anomalies by leveraging the score network s θ (x t , t) [Livernoche et al., 2023].Additionally, probability density characterization through score computation enables direct anomaly assessment without requiring explicit reconstruction or comparative analysis.Theoretical foundations established by [Wang and Vastola, 2023] and [Han et al., 2024b] strongly validate this methodology, although recent investigations [Deveney et al., 2023] illuminate potential disparities between SDE and ODE formulations, warranting careful consideration in score-based anomaly detection frameworks.Diffusion Time Estimation.DTE offers an alternative anomaly detection method leveraging the diffusion process [Livernoche et al., 2023].Instead of reconstructing the input x, DTE estimates the diffusion timestep t dif f required for x to traverse the data distribution p data (x).The estimation is performed by a time estimation network, which predicts the time distribution.Intuitively, a longer t dif f suggests an anomaly due to the x's distance from the learned distribution [Luo, 2023].The estimated diffusion time t dif f thus serves as an anomaly score, derived from the mode or mean of its distribution.An analytical form for this density is derivable, and deep neural networks can improve inference efficiency.Consequently, DTE has shown competitive performance, especially in speed, on benchmarks like ADBench, while maintaining or exceeding the accuracy of traditional DDPMs.</p>
<p>Hybrid Approaches</p>
<p>Hybrid approaches integrate DMs with other anomaly detection techniques to improve performance.For example, RAD can be combined with density estimation, where reconstruction error from DMs is integrated with DAD, like DTE [Livernoche et al., 2023].Additionally, dynamic step size computation in the forward process, guided by initial anomaly predictions, improves performance by denoising scaled input without added noise [Tebbe and Tayyub, 2024].In another approach, likelihood maps of potential anomalies from DMs are integrated with the original image via joint noised distribution re-sampling, enhancing healthy tissue restoration [Bercea et al., 2023].For TSAD, hybrid methods can employ density ratio-based strategies to select normal observations for imputation, combined with denoising diffusion-based imputation to improve missing value generation, especially under anomaly concentration [Xiao et al., 2023].Advanced reconstruction techniques predict specific denoising steps by analyzing divergences between image and diffusion model priors, supplemented by synthetic abnormal sample generation during training and spatial-adaptive feature fusion during inference [Yao et al., 2025].Integrating DMs with Transformers for multi-class anomaly detection, where diffusion provides high-frequency information for refinement, mitigates blurry reconstruction and "identical shortcuts" [Zhan et al., 2024].Finally, a Bayesian framework employing masked noisy observation models and diffusion-based normal image priors enables effective difference map computation be-</p>
<p>Tasks</p>
<p>In this section, we consider four primary tasks of DMAD, as summarized in Tables 1-4, which present representative methods with open-source implementations, datasets, evaluation metrics, and corresponding results.For each task, methods are categorized based on their underlying principles and detection strategies, highlighting the unique challenges and solutions developed for different data modalities.</p>
<p>Image Anomaly Detection</p>
<p>IAD represents a key application area for DMs, focusing on identifying deviations from normality in both global patterns affecting entire images and local anomalies confined to specific regions.Many approaches leverage DMs' reconstruction capabilities, with methods calculating anomaly scores from reconstruction errors after rebuilding images from noisy inputs [Xu et al., 2023;Tur et al., 2023a;Wang et al., 2023d].Advanced frameworks like GLAD [Yao et al., 2025]</p>
<p>Time Series Anomaly Detection</p>
<p>DMs have emerged as a powerful technique for TSAD, effectively learning complex distributions of sequential data</p>
<p>Video Anomaly Detection</p>
<p>VAD effectively leverages spatio-temporal information to identify unusual events.DMs have emerged as a highly promising VAD approach due to their ability to learn complex data distributions and generate high-quality reconstructions.For example, [Tur et al., 2023a] investigated unsupervised VAD using DMs, relying on high reconstruction error to indicate anomalies, while [Tur et al., 2023b] enhanced this approach by introducing compact motion representations as conditional information.et al., 2023] leveraging manifold structures for more accurate boundary learning.GLAD [Yao et al., 2025] further suggests extending DMs for multimodal data through specialized architectures, while integration with LLMs offers promising directions for context-aware anomaly detection.</p>
<p>Discussions</p>
<p>Despite significant progress in DMAD, several critical challenges remain.In this section, we explore key areas that require further research and development.[Kang et al., 2025] demonstrates the potential of DMs for adversarial defense in classification, directly applying such methods to AD can reduce anomaly detection rates.Specifically, the purification process may remove crucial anomaly signals along with noise.Additionally, adversarial examples exhibit misalignment within DM manifolds, offering a potential detection avenue but requiring further investigation.The observed robustness differences between pixel-space diffusion models (PDMs) and more vulnerable latent diffusion models (LDMs) [Graham et al., 2023] further underscore the need to consider specific DM types.Consequently, future research should prioritize robust DM-based AD methods, including architectures and training procedures that distinguish genuine anomalies from adversarial noise.Promising directions include defense strategies like DIFFender [Kang et al., 2025], leveraging textguided diffusion and the adversarial anomaly perception phenomenon, and incorporating insights from adversarial example behavior within DM manifolds.</p>
<p>Edge-Cloud Collaboration Real-time DMAD faces significant adoption barriers due to their substantial computational demands [Li et al., 2023].Edge-cloud collaboration offers a promising solution to this challenge by distributing the workload between edge devices and cloud servers.Lightweight DMs deployed on edge devices perform initial anomaly screening, while resource-intensive tasks like full reconstructions are efficiently offloaded to cloud servers [Yan et al., 2024].In addition, federated learning enables collaborative model training across edge devices and the cloud without sharing sensitive data, thereby enhancing generalization and preserving privacy [Liu et al., 2024b].Dynamic DM partitioning [Chen et al., 2024a] facilitates adaptive resource allocation, optimizing performance based on network conditions and computational demands.Strategic data placement and distributed DNN deployment principles further enhance system efficiency.The integration of proactive detection mechanisms, exemplified by Maat [Lee et al., 2023b], proves particularly valuable for time-critical applications in cloud monitoring and AIOps environments.</p>
<p>Integrating with Large Language Models Integrating DMs with LLMs offers a highly promising avenue for enhancing anomaly detection, particularly by generating human-interpretable explanations of detected anomalies and incorporating rich contextual information [Kumar et al., 2023].For example, LLMs can leverage detailed textual descriptions to provide context for observed fluctuations, distinguishing genuine anomalies from expected variations for MAD [Capogrosso et al., 2024].However, current LLM integration for anomaly detection faces a key challenge: effectively representing and tokenizing temporal data.Existing tokenizers, primarily designed for text, may not adequately capture the subtle nuances of numerical and temporal data, potentially hindering performance [Li et al., 2024].Consequently, a critical research direction involves developing efficient data representation techniques and specialized LLMs for anomaly detection to fully realize this synergistic approach's potential [Tebbe and Tayyub, 2024].</p>
<p>Conclusion</p>
<p>In this survey, we introduce the core concepts of anomaly detection and diffusion models, providing a foundation for understanding DMAD.We systematically reviewed existing methodologies and their tasks across diverse data types, including image, time series, and multimodal data.Furthermore, we analyzed representative approaches for each data type, highlighting their strengths and limitations.To promote further research, we identified key challenges and promising future directions, aiming to advance DMAD research and inspire innovative solutions for real-world applications.</p>
<p>Figure 1 :
1
Figure 1: Taxonomy of diffusion models for anomaly detection.</p>
<p>Figure 2 :
2
Figure 2: Illustration of reconstruction-based AD methods: (a) basic, (b) latent space, and (c) conditional reconstructions.</p>
<p>Figure 3 :
3
Figure 3: Illustration of density-based AD methods: (a) score function-based and (b) diffusion time estimation methods.</p>
<p>the goal is to identify the anomalous subset A ⊂ D.
Anomalous behavior is characterized by low probability den-sity under the typically unknown distribution p(x). Sev-eral anomaly types exist, including point anomalies [Hu andJin, 2023] that are individual deviant instances; contextualanomalies [Sui et al., 2024] that are anomalous within a spe-cific context; and collective anomalies that are groups of in-stances anomalous only when considered together. Com-mon evaluation metrics include AUC [Xu et al., 2023], mea-suring the classifier's ability to distinguish between normaland anomalous instances. Additionally, precision, recall,and F1-score assess the trade-off between correct identifi-cation and minimizing false alarms [Wang</p>
<p>Table 1 :
1
Method highlighted in blue indicates available open-source code links.DS denotes diffusion space, where 'D' is discrete and 'C' is continuous.Due to space limitations, only partial datasets and results are shown.For more details, please find our GitHub.Summary of IAD methods for metric results on datasets.
MethodDSDatasetsResults of metrics on datasetsFNDM [2023] D BRATS, ISLESDice: 76.21% (BRATS), 54.44% (ISLES), VS: 82.28% (BRATS), etc.ODD [2023b]DMNIST, CIFAR-10, etc.AUROC: 97.8% (MNIST), 84.7% (CIFAR-10), etc.mDDPM [2024] D BraTS21, MSLUBDice: 53.02% (BraTS21), 10.71% (MSLUB), etc.Dif-fuse [2024] D IST-3, BraTS, WMHDice: (WMH), etc. 0.699 (BraTS21), 0.569DIC [2024]CVisA, MVTecBTAD,I-AUROC: 97.9%, PRO 94.1% on VisA, etc. 96.0%, P-AUROC:DRAD [2024] C MVTec-ADI-AUROC: 99.1% (MVTec-AD), P-AUROC: 97.3% (MVTec-AD), etc.MDPS [2024]C MVTec-AD, BTADI-AUROC: 98.4% (MVTec-AD), P-AUROC: 97.0% (MVTec-AD), etc.DISYRE [2024] CCamCAN, ATLAS, BraTSAP: 0.45 (ATLAS), 0.51 (BraTS-T1), 0.73 (BraTS-T2), etc.
Notes:tween normal posterior samples and the test image, improving anomaly detection and localization[Wu et al., 2024].</p>
<p>Table 2 :
2
Summary of TSAD methods for metric results on datasets.
MethodDSDatasetsResults of metrics on datasetsMoCoDAD [2023]D UB, HR-STC, etc.AUC: 77.6% (HR-STC), 89.0% (HR-Ave), 68.4% (HR-UB), 68.3% (UB)[Tur et al., 2023b] D UCF, ShTAUC: 76.36% (ShT), 63.67% (UCF)DHVAD [2024]C Ped2, Ave, ShTAUC: 98.1% (Ped2), 88.3% (Ave), 78.9% (ShT)FDAE [2024]D Ave, ShT, Ped2AUC: 97.7% (Ped2), 91.1% (Ave), 73.8% (ShT)Masked Diffusion [2023]D CrossTask,NIV,COINSR: 39.17% (CrossTask), 32.35% (NIV), 29.43% (COIN), etc.FPDM [2023]D Ave, ShT, UCF, UBAUC: 90.1% (Ave), 78.6% (ShT), 74.7% (UCF), 62.7% (UB)[Wang et al., 2023d]D Ped2, Ave, ShTAUC: 96.5% (Ped2), 92.2% (Ave), 75.4% (ShT)DiffVAD [2024]D Ave, ShT, UCF, etc.AUC: 81.9% (ShT), 90.3% (Ave), 87.6% (Ped2), etc.Masked Diffusion [2023]C CrossTask,NIV,COINSR: 39.17% (CrossTask), 23.47% (CrossTask), 32.35% (NIV), etc.VADiffusion [2024a]D Ped2, Ave, ShTAUC: 98.2% (Ped2), 94.93% (Ave), 97.32% (ShT)</p>
<p>Table 3 :
3
[Lee et al., 2023a]ods for metric results on datasets.Cao et al., 2024]introduces a foundation model approach with multiple masking schemes.Another key challenge is non-stationarity.DiffAD[Xiao et al., 2023]handle missing data through imputation, complemented by selfsupervised techniques[Liu et al., 2024d]and co-evolving strategies[Lee et al., 2023a]for mixed-type temporal data.</p>
<p>[Sui et al., 2024]b].DMs can identify various anomaly types, including point, contextual, and collective anomalies[Sui et al., 2024].Advanced approaches like NGLS-Diff[Han et al., 2024a]leverage latent spaces, while methods [Zuo et al., 2024] used structured state space layers explicitly to model temporal dependencies.Recently, some innovations address key challenges: D3R [Wang et al., 2023a] tackles non-stationarity through dynamic decomposition, while TimeDiT [</p>
<p>Table 4 :
4
Several innovative architectures have been recently proposed to improve detection Mods.indicates modalities, where 'I' is Image, 'V' is Vector, 'A' is Audio, 'T' is Text, and 'Te' is Texture) Summary of MAD methods for metric results on datasets.
MethodMods. DSDatasetsResults of metrics on datasetsMPDR [2023]I, V, A CMNIST, CIFAR-10, etc.AUPR: 0.764 (MNIST), 0.9860 (CIFAR-10), 0.8338 (CIFAR-100), etc.DIAG [2024]I, TC KSDD2AP: 80.1% (zero-shot), 92.4% (full-shot)AnomalyXFusion [2024a]I, T, Te CMVTec-AD, MVTec LOCOIS: 1.82, IC-LPIPS: 0.33Notes: accuracy: VADiffusion [Liu et al., 2024a] employs a dual-branch structure combining motion vector reconstruction andI-frame prediction, while FDAE [Zhu et al., 2024] introducesa flow-guided diffusion autoencoder with sample refinementfor comprehensive detection of both appearance and motionanomalies. Recent advances include [Yan et al., 2023]'s fea-ture prediction diffusion model and [Cheng et al., 2024]'s de-noising diffusion-augmented hybrid framework, both enhanc-ing semantic understanding of normal patterns. Furthermore,[Wang et al., 2023d] proposed an ensemble approach usingstochastic reconstructions and motion filters, while [Fang etal., 2023] explored masked diffusion with task-awareness formore focused anomaly detection in specific contexts.4.4 Multimodal Anomaly DetectionMAD integrates data from various sources to identify devi-ations from expected behavior. While DMs are relativelynascent in this area, they hold substantial promise throughadvanced multimodal data integration. For instance, Anoma-lyXFusion [Hu et al., 2024a] enhances anomaly synthesis byeffectively combining image, text, and mask features, while[Flaborea et al., 2023] demonstrates significant success inskeleton-based VAD through motion-conditioned diffusionmodels. For IAD, combining visual data with text descrip-tions significantly improves subtle anomaly detection [Ca-pogrosso et al., 2024], with energy-based approaches [Yoon</p>
<p>Complex Data Distributions A key challenge in applying DMAD lies in handling complex data distributions.Imbalanced datasets, where anomalies are rare, can bias DMs towards the majority class [Yang et al., 2024a], hindering accurate anomaly modeling.Similarly, datasets, representing distinct normal behaviors, can confound DM learning [Zuo et al., 2024], potentially misclassifying data from less prominent modes.Noisy or missing data further complicates DM training and inference [Choi et al., 2022], as differentiating true anomalies from data imperfections becomes difficult.However, potential solutions exist.Data augmentation techniques can address class imbalance by generating synthetic minority class samples [Yang et al., 2024a].Robust training methods, such as using semi-unbalanced optimal transport, can enhance DM resilience to noise and outliers [Dao et al., 2024].Specialized architectures, potentially incorporating mechanisms for handling missing data or modeling multiple modes [Xiao et al., 2023], and dynamic step size computation [Tebbe and Tayyub, 2024] could further improve DM performance on complex distributions.Robustness and Adversarial Attacks Adversarial robustness is a critical concern for DMAD.Similar to other deep learning models, DMs are vulnerable to adversarial perturbations, raising concerns about the reliability of diffusion-based AD systems [Chen et al., 2024b].For example,
sampling schedules like align your steps, which aim to re-duce sampling steps while maintaining quality [Salimans andHo, 2021]. Another approach is model compression throughtechniques like pruning and quantization. Additionally, effi-cient architectures, like [Cui et al., 2023] leveraging ER SDEsfor faster sampling, and preconditioning methods offer fur-ther computational gains.Interpretability and Explainability Interpretability re-mains a key challenge for DMAD [Katsuoka et al., 2024].Understanding DM's decision-making is crucial, especiallyin critical applications.Visualizing anomaly scores,like spatially highlighting anomalous image regions as inreconstruction-based methods [Yao et al., 2025], becomes es-sential. However, the iterative denoising process inherent inDMs complicates interpretability. Explaining anomaly de-tection requires identifying and explaining deviating features.Integrating DMs with explainable AI (xAI) techniques, suchas incorporating attention mechanisms or leveraging LLMsfor textual/visual explanations [Wang et al., 2023b], offers apromising research direction.Computational Cost Widespread adoption of DMAD re-mains constrained by substantial computational require-ments, particularly when processing high-dimensional dataor extended time series. Computational demands manifestduring both distribution learning in training and data genera-tion in sampling phases, with the sampling process's iterativenature demanding numerous steps for quality output [Cui etal., 2023]. For example, high-resolution IAD suffers from in-creased computational overhead due to data volume, modelcomplexity, and the need to capture temporal dependencies[Wyatt et al., 2022]. However, some researchers are activelyexploring solutions. One promising direction is faster sam-pling methods, such as progressive distillation and optimized</p>
<p>Mask, stitch, and re-sample: Enhancing robustness and generalizability in anomaly detection through automatic diffusion models. Bercea, arXiv:2305.196432023. 2023</p>
<p>Timedit: General-purpose diffusion transformers for time series foundation model. Cao, ICMLW. Alvise2024. 2024Capogrosso et al., 2024</p>
<p>Exploiting multimodal latent diffusion models for accurate anomaly detection in industry 5.0. In xAI. Andrea Vivenza, Francesco Chiarini, Marco Setti, ; Cristani, Chen, IEEE TIFS. 37622024. 2024a. 2024. 2024b. 2024IEEE TCC</p>
<p>Denoising diffusion-augmented hybrid video anomaly detection via reconstructing noised frames. Cheng, arXiv:2311.15996arXiv:2309.07409Masked diffusion with taskawareness for procedure planning in instructional videos. Yee H. Mah, James T. Teo, H. Rolf Jäger, David Werring2024. 2024. 2022. 2022. 2023. 2023. 2024. 2024. 2023. 2023. 2023. 2023. 2024. 2023. 20232MICCAI</p>
<p>Diffusion model in normal gathering latent space for time series anomaly detection. Han, ECML PKDD. 2024a. 202414943</p>
<p>Neural network-based score estimation in diffusion models: Optimization and generalization. Han, arXiv:2404.19444Xianyao Hu and Congming Jin. Hasan Iqbal, Umar Khalid, Chen Chen, and Jing Hua2024b. 2024. 2023. 2024a. 2024. 2024b. 2024. 202438MLMI</p>
<p>Statistical test on diffusion model-based anomaly detection by selective inference. Kang, ECCV. Kumar, 2025. 2025. 2024. 2024. 14301. 2023PRMISelf-supervised diffusion model for anomaly segmentation in medical imaging</p>
<p>Maat: Performance metric anomaly anticipation for cloud services with conditional diffusion. Le Lan, Dinh , Charline , Le Lan, Laurent Dinh, ; Lee, ICML. Tianyi Yang, Zhuangbin Chen, Yuxin Su, Michael R Lyu, 2021. 2021. 2023a. 2023. 202323ASE</p>
<p>Fast non-markovian diffusion model for weakly supervised anomaly detection in brain mr images. Li, MICCAI. 2023. 202314224</p>
<p>Self-supervised enhanced denoising diffusion for anomaly detection. Li, Inf. Sci. 6691206122024. 2024</p>
<p>Vadiffusion: Compressed domain information guided conditional diffusion for video anomaly detection. Liu, IEEE TCSVT. 342024a. 2024</p>
<p>Networking systems for video anomaly detection: A tutorial and survey. Liu, Azzedine Boukerche. 2024b. 2024. 2024c. Jun LiuPeng Sun</p>
<p>Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models. Liang Song, ; Liu, CIKM. Yashar Hezaveh, and Siamak Ravanbakhsh2024. 2024d. 2024. 2023ICLR</p>
<p>Anomaly detection for telemetry time series using a denoising diffusion probabilistic model. Weijian Luo, Elisa RicciNaval Luo, Elisa RicciMarimont, Elisa RicciMICCAI. Anil Osman Tur2023. 2023. 2024. 2024. 2021. 2021. 2024. 2024. 2024. 2024. 202324Nicola Dall'Asen, Cigdem BeyanICIP</p>
<p>Drift doesn't matter: Dynamic decomposition with diffusion reconstruction for unstable multivariate time series anomaly detection. Tur, arXiv:2311.10892NeurIPS. 2023b. 2023. 2023. 2023a. 202336ICIAP</p>
<p>Ensemble anomaly score for video anomaly detection using denoise diffusion model and motion filters. Wang , arXiv:2404.17900SIGIR. 2023b. 2023. 2023c. 2023. 2023d. 2023. 2024. 2022. 2022553CVPR</p>
<p>Imputation-based time-series anomaly detection with conditional weightincremental diffusion models. Xiao , KDD. 2023. 2023. 202397103983Unsupervised industrial anomaly detection with diffusion models</p>
<p>Feature prediction diffusion model for video anomaly detection. Yan , ICCV. 2023. 2023</p>
<p>Hybrid sd: Edge-cloud collaborative inference for stable diffusion models. Yan , 2024. 2024</p>
<p>A novel data augmentation method based on denoising diffusion probabilistic model for fault diagnosis under imbalanced data. Yang , IEEE TII. 202024a. 2024</p>
<p>A survey on diffusion models for time series and spatio-temporal data. Yang , 2024b. 2024</p>
<p>Glad: Towards better reconstruction with global and local adaptive diffusion models for unsupervised anomaly detection. Yao, ECCV. Neurips, 2025. 2025. 2023. 202336Energy-based models for anomaly detection: A manifold diffusion recovery approach</p>
<p>Safeguarding sustainable cities: Unsupervised video anomaly detection through diffusion-based latent pattern learning. Zhan, arXiv:2407.01905IJCAI. 2024. 2024. 2024. 2023. 2023. 20248Enhancing multi-class anomaly detection via diffusion refinement with dual conditioning</p>
<p>Flow-guided diffusion autoencoder for unsupervised video anomaly detection. Zhu, PRCV. 2024. 2024. 202414430Unsupervised diffusion based anomaly detection for time series</p>            </div>
        </div>

    </div>
</body>
</html>
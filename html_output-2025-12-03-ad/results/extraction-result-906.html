<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-906 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-906</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-906</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc" target="_blank">PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change.</p>
                <p><strong>Paper Abstract:</strong> Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e906.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e906.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large pre-trained transformer language model accessed via the OpenAI API and evaluated in this paper on planning and reasoning tasks using PlanBench; used with deterministic decoding (temperature=0) and few-shot task prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box transformer LLM accessed via OpenAI API; paper does not report internal architectural details beyond being GPT-4 with an 8k context window for the used version.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>PlanBench (Blocksworld & Logistics): Plan Generation; Cost-Optimal Planning; Plan Verification; Reasoning about Plan Execution; Replanning; Plan Generalization; Plan Reuse; Robustness to Goal Reformulation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Blocksworld (GPT-4, Table 1): Plan Generation 206/600 (34.3%); Cost-Optimal Planning 198/600 (33.0%); Plan Verification 352/600 (58.6%); Reasoning about Plan Execution 191/600 (31.8%); Replanning 289/600 (48.1%); Plan Generalization 141/500 (28.2%); Plan Reuse 392/600 (65.3%); Robustness to Goal Reformulation (shuffle) 461/600 (76.8%); Robustness (Full->Partial) 522/600 (87.0%); Robustness (Partial->Full) 348/600 (58.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>used via few-shot / in-context prompting (one-shot examples in prompt); deterministic decoding (temperature=0) during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (few-shot) and evaluation intervention: domain obfuscation (deceptive and randomized identifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>The paper evaluates GPT-4 under few-shot one-shot prompts that include a lifted domain description and example instances; it also runs obfuscation experiments where action, predicate and object names are replaced with misleading words or random alphanumeric strings to remove surface commonsense cues.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Obfuscation greatly reduced performance: Blocksworld Plan Generation 206/600 (34.3%) vs Mystery Blocksworld (Deceptive) 26/600 (4.3%) and Mystery Blocksworld (Randomized) 12/600 (2.0%), showing dramatic drop when surface cues are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper hypothesizes that LLM success on common-sense tasks often reflects pattern-matching or retrieval of world knowledge and surface cues rather than systematic planning; models are next-token predictors and thus fail to perform model-based, symbolic planning that requires using a domain model robustly (supported by large performance drops under obfuscation and other failure-mode analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e906.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e906.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruct-GPT3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-GPT3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned GPT-3 variant (text-davinci-002) evaluated via the OpenAI API on PlanBench; evaluated using the same few-shot prompt style and deterministic decoding as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Instruct-GPT3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following variant of GPT-3 family accessed as a black-box via API; the paper uses it in few-shot prompt settings but does not report internal architectural or size details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>PlanBench (Blocksworld & Logistics): same suite as GPT-4 (Plan Generation; Cost-Optimal Planning; Plan Verification; Reasoning about Plan Execution; Replanning; Plan Generalization; Plan Reuse; Robustness to Goal Reformulation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Blocksworld (I-GPT3, Table 1): Plan Generation 41/600 (6.8%); Cost-Optimal Planning 35/600 (5.8%); Plan Verification 72/600 (12.0%); Reasoning about Plan Execution 4/600 (0.6%); Replanning 40/600 (6.6%); Plan Generalization 49/500 (9.8%); Plan Reuse 102/600 (17.0%); Robustness (shuffle) 467/600 (77.8%); Robustness (Full->Partial) 467/600 (77.8%); Robustness (Partial->Full) 363/600 (60.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>used via few-shot / in-context prompting (one-shot examples in prompt); deterministic decoding (temperature=0) during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (few-shot) and evaluation intervention: domain obfuscation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same prompt style as GPT-4; paper also tests obfuscated versions of domains (misleading or randomized identifiers) to test whether performance depends on surface commonsense cues.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Obfuscation reduced performance markedly: e.g., Plan Generation on Blocksworld 41/600 (6.8%) vs Mystery Blocksworld (Deceptive) 14/600 (~2.3%) and Mystery Blocksworld (Randomized) 6/600 (1.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper attributes low interactive/planning performance to reliance on pattern matching and surface cues rather than systematic reasoning; instruction-tuned LLMs still fail on model-based planning tasks when domain language is obfuscated or when prompts require precise use of the domain model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e906.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e906.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlanBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark introduced in this paper that generates ~26,250 prompts across multiple IPC-style planning domains (initial release: Blocksworld and Logistics) to evaluate LLM abilities on planning and reasoning-about-change tasks, including obfuscated-domain variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extensible evaluation framework that (a) generates planning problem instances from lifted PDDL domains, (b) translates between PDDL and natural-language prompts, (c) validates generated plans using symbolic validators, and (d) includes obfuscation tools to mask surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Suite of planning and reasoning tasks: Plan Generation, Cost-Optimal Planning, Plan Verification, Reasoning about Plan Execution, Robustness to Goal Reformulation, Ability to Reuse Plans, Replanning, Plan Generalization</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>benchmark-level interventions: domain obfuscation; controlled prompt templates and few-shot exemplars; automated plan validation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>PlanBench supplies natural-language translations of lifted domain models and can obfuscate action/predicate/object names (misleading words or random alphanumeric strings) to test whether LLMs rely on common-sense word associations; prompts use few-shot examples and require particular output formatting for parsing/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Paper demonstrates that obfuscation dramatically reduces model performance (see Table 2): e.g., GPT-4 Plan Generation drops from 206/600 (34.3%) on standard Blocksworld to 26/600 (4.3%) on deceptive obfuscation and 12/600 (2.0%) on randomized obfuscation, indicating sensitivity to surface language cues.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>By design PlanBench supports the paper's hypothesis that LLM performance on commonsense QA-style tasks does not imply robust planning: observed failures and obfuscation sensitivity suggest LLMs do not perform reliable model-based planning but instead exploit surface patterns and world knowledge present in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the planning abilities of large language models-a critical investigation <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>REACT: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-906",
    "paper_id": "paper-e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's large pre-trained transformer language model accessed via the OpenAI API and evaluated in this paper on planning and reasoning tasks using PlanBench; used with deterministic decoding (temperature=0) and few-shot task prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4",
            "model_description": "Black-box transformer LLM accessed via OpenAI API; paper does not report internal architectural details beyond being GPT-4 with an 8k context window for the used version.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "PlanBench (Blocksworld & Logistics): Plan Generation; Cost-Optimal Planning; Plan Verification; Reasoning about Plan Execution; Replanning; Plan Generalization; Plan Reuse; Robustness to Goal Reformulation",
            "interactive_task_type": "planning / multi-step reasoning / sequential decision-making",
            "interactive_performance": "Blocksworld (GPT-4, Table 1): Plan Generation 206/600 (34.3%); Cost-Optimal Planning 198/600 (33.0%); Plan Verification 352/600 (58.6%); Reasoning about Plan Execution 191/600 (31.8%); Replanning 289/600 (48.1%); Plan Generalization 141/500 (28.2%); Plan Reuse 392/600 (65.3%); Robustness to Goal Reformulation (shuffle) 461/600 (76.8%); Robustness (Full-&gt;Partial) 522/600 (87.0%); Robustness (Partial-&gt;Full) 348/600 (58.0%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "used via few-shot / in-context prompting (one-shot examples in prompt); deterministic decoding (temperature=0) during evaluation",
            "intervention_type": "prompting strategy (few-shot) and evaluation intervention: domain obfuscation (deceptive and randomized identifiers)",
            "intervention_description": "The paper evaluates GPT-4 under few-shot one-shot prompts that include a lifted domain description and example instances; it also runs obfuscation experiments where action, predicate and object names are replaced with misleading words or random alphanumeric strings to remove surface commonsense cues.",
            "intervention_effect": "Obfuscation greatly reduced performance: Blocksworld Plan Generation 206/600 (34.3%) vs Mystery Blocksworld (Deceptive) 26/600 (4.3%) and Mystery Blocksworld (Randomized) 12/600 (2.0%), showing dramatic drop when surface cues are removed.",
            "hypothesized_cause_of_gap": "Paper hypothesizes that LLM success on common-sense tasks often reflects pattern-matching or retrieval of world knowledge and surface cues rather than systematic planning; models are next-token predictors and thus fail to perform model-based, symbolic planning that requires using a domain model robustly (supported by large performance drops under obfuscation and other failure-mode analyses).",
            "uuid": "e906.0",
            "source_info": {
                "paper_title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Instruct-GPT3",
            "name_full": "Instruct-GPT3 (text-davinci-002)",
            "brief_description": "OpenAI's instruction-tuned GPT-3 variant (text-davinci-002) evaluated via the OpenAI API on PlanBench; evaluated using the same few-shot prompt style and deterministic decoding as GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Instruct-GPT3 (text-davinci-002)",
            "model_description": "Instruction-following variant of GPT-3 family accessed as a black-box via API; the paper uses it in few-shot prompt settings but does not report internal architectural or size details.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "PlanBench (Blocksworld & Logistics): same suite as GPT-4 (Plan Generation; Cost-Optimal Planning; Plan Verification; Reasoning about Plan Execution; Replanning; Plan Generalization; Plan Reuse; Robustness to Goal Reformulation)",
            "interactive_task_type": "planning / multi-step reasoning / sequential decision-making",
            "interactive_performance": "Blocksworld (I-GPT3, Table 1): Plan Generation 41/600 (6.8%); Cost-Optimal Planning 35/600 (5.8%); Plan Verification 72/600 (12.0%); Reasoning about Plan Execution 4/600 (0.6%); Replanning 40/600 (6.6%); Plan Generalization 49/500 (9.8%); Plan Reuse 102/600 (17.0%); Robustness (shuffle) 467/600 (77.8%); Robustness (Full-&gt;Partial) 467/600 (77.8%); Robustness (Partial-&gt;Full) 363/600 (60.5%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "used via few-shot / in-context prompting (one-shot examples in prompt); deterministic decoding (temperature=0) during evaluation",
            "intervention_type": "prompting strategy (few-shot) and evaluation intervention: domain obfuscation",
            "intervention_description": "Same prompt style as GPT-4; paper also tests obfuscated versions of domains (misleading or randomized identifiers) to test whether performance depends on surface commonsense cues.",
            "intervention_effect": "Obfuscation reduced performance markedly: e.g., Plan Generation on Blocksworld 41/600 (6.8%) vs Mystery Blocksworld (Deceptive) 14/600 (~2.3%) and Mystery Blocksworld (Randomized) 6/600 (1.0%).",
            "hypothesized_cause_of_gap": "Paper attributes low interactive/planning performance to reliance on pattern matching and surface cues rather than systematic reasoning; instruction-tuned LLMs still fail on model-based planning tasks when domain language is obfuscated or when prompts require precise use of the domain model.",
            "uuid": "e906.1",
            "source_info": {
                "paper_title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "PlanBench",
            "name_full": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
            "brief_description": "Benchmark introduced in this paper that generates ~26,250 prompts across multiple IPC-style planning domains (initial release: Blocksworld and Logistics) to evaluate LLM abilities on planning and reasoning-about-change tasks, including obfuscated-domain variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": "Extensible evaluation framework that (a) generates planning problem instances from lifted PDDL domains, (b) translates between PDDL and natural-language prompts, (c) validates generated plans using symbolic validators, and (d) includes obfuscation tools to mask surface cues.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Suite of planning and reasoning tasks: Plan Generation, Cost-Optimal Planning, Plan Verification, Reasoning about Plan Execution, Robustness to Goal Reformulation, Ability to Reuse Plans, Replanning, Plan Generalization",
            "interactive_task_type": "planning / multi-step reasoning / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "benchmark-level interventions: domain obfuscation; controlled prompt templates and few-shot exemplars; automated plan validation",
            "intervention_description": "PlanBench supplies natural-language translations of lifted domain models and can obfuscate action/predicate/object names (misleading words or random alphanumeric strings) to test whether LLMs rely on common-sense word associations; prompts use few-shot examples and require particular output formatting for parsing/validation.",
            "intervention_effect": "Paper demonstrates that obfuscation dramatically reduces model performance (see Table 2): e.g., GPT-4 Plan Generation drops from 206/600 (34.3%) on standard Blocksworld to 26/600 (4.3%) on deceptive obfuscation and 12/600 (2.0%) on randomized obfuscation, indicating sensitivity to surface language cues.",
            "hypothesized_cause_of_gap": "By design PlanBench supports the paper's hypothesis that LLM performance on commonsense QA-style tasks does not imply robust planning: observed failures and obfuscation sensitivity suggest LLMs do not perform reliable model-based planning but instead exploit surface patterns and world knowledge present in pretraining.",
            "uuid": "e906.2",
            "source_info": {
                "paper_title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the planning abilities of large language models-a critical investigation",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "REACT: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.016254249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change</h1>
<p>Karthik Valmeekam<br>School of Computing \&amp; AI<br>Arizona State University, Tempe.<br>kvalmeek@asu.edu<br>Alberto Olmo<br>School of Computing \&amp; AI<br>Arizona State University, Tempe.<br>aolmoher@asu.edu</p>
<p>Matthew Marquez<br>School of Computing \&amp; AI<br>Arizona State University, Tempe.<br>mmarqu22@asu.edu<br>Sarath Sreedharan*<br>Department of Computer Science, Colorado State University, Fort Collins. sarath.sreedharan@colostate.edu<br>Subbarao Kambhampati<br>School of Computing \&amp; AI<br>Arizona State University, Tempe.<br>rao@asu.edu</p>
<h4>Abstract</h4>
<p>Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.</p>
<h2>1 Introduction</h2>
<p>The advent of large pre-trained language models have revolutionized the field of natural language processing and have also received widespread public attention. These types of transformer-based large language models (LLMs) currently provide state-of-the-art performance in many of the standard NLP tasks. LLMs essentially predict the next word in a sentence, given a certain context and these models were originally developed to perform word sequence completion tasks. In the recent times, there</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>have been anecdotal evidence and claims that they possess other capabilities that are not normally associated with sequence completion. This led to a sudden outburst of research probing and studying their behavior almost as if they were artificial organisms (c.f. [15]). In this paper, we are particularly interested in the line of research efforts that investigate (and showcase) the reasoning capabilities of Large Language models-including commonsense reasoning [31, 27, 5], logical reasoning [29], and even ethical reasoning [14]. These works have largely been suggesting that LLM's are indeed capable of doing such kinds of reasoning [17, 34, 2].</p>
<p>Planning is a reasoning task that has been well studied in the AI community. In its most basic form, planning involves coming up with a course of actions (policy) which when executed would take an agent from a certain initial state to a desired world state. Planning has generally been studied primarily as an inference problem on world and reward models. These models could either be specified by humans or learned by the agent by interacting with its world. In this paper, we want to look at the ability of large language models to do reasoning about actions and change involving common-sense planning tasks. We propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, specially in International Planning Competitions (IPC) [13] to test this. Our focus on planning is spurred by not only the fact that it is a core aspect of human intelligence [26], but also that it is required for tasks considered as potential applications of LLMs including automatic code generation.</p>
<p>The contribution of our paper is a curriculum for evaluating planning, wherein we identify a set of related but distinct tasks, that are central for an agent to successfully perform planning and reasoning about actions and introduce a framework for developing code which is meant to auto-generate the possible queries for each. We initialize PlanBench with two IPC domains: Blocksworld and Logistics. We also provide obfuscated versions of these domains with the obfuscations being either misleading words or random alphanumeric strings. Overall, we provide $\sim 26250$ prompts as part of our dataset. We are also actively adding other IPC domains and tasks into the benchmark. The updated version of the entire benchmark, including the tools, datasets, and the scripts to reproduce the prompts and results in this paper, can be found at https://github.com/karthikv792/LLMs-Planning.</p>
<p>We are not the first to point out the need to perform such analyses of the reasoning capabilities of GPT-3 like LLMs. For example, [19] performed an analysis of GPT-3's reasoning capabilities on some example tasks, including different commonsense reasoning tasks varying from biological reasoning to arithmetic reasoning. However, the goal of this paper is fundamentally distinct from these earlier works in multiple ways. Firstly, we are not merely trying to point out a few example cases where LLMs fail but rather help establish an assessment framework for evaluating these systems' capabilities to perform planning. While this paper reports the results of testing Instruct-GPT3 [22] and GPT-4 [21], one could use this framework to analyse other LLMs that may be fine-tuned for such tasks. Secondly, through this framework, we are also trying to eliminate the subjective aspect of analysis that forms the core part of many of these earlier efforts. Instead, we automate and perform the analyses in a mechanistic way by leveraging automated planning models and tools to generate the queries and validate the system's answers.</p>
<h1>2 Related Work</h1>
<p>To the best of our knowledge, we are the first to introduce a benchmark specifically designed to evaluate the emerging planning abilities of LLMs, if any. The initial version of our benchmark was made public almost a year back and is currently in use by various researchers. ${ }^{2}$ We have significantly increased the scope of our benchmark (with additional test cases and domains) from the initial version. But the idea of developing benchmarks to evaluate emergent properties of LLMs is itself not new. Some prominent existing reasoning benchmarks include, BIG-BENCH [29], GSM8K [3], AQUA [18], SVAMP [23], CommonsenseQA [31] and StrategyQA [5]. However, these tasks are simple involve shallow reasoning and do not give insight into their planning capabilities. As LLMs have been able to perform well on such tasks, there has been a lot more triumphalism about their planning capabilities, which is currently being echoed in the community.</p>
<p>There have been significant developments in the intersection of LLMs and planning, with LLMs undertaking various roles [16]. These roles range from generating plans [11, 33] and heuristics [33, 1]</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to extracting planning knowledge [7]. For example, in Say-Can [1], LLMs have been used as scoring models, which can be seen as providing planning heuristics, for the actions that the embodied robot can execute. Additionally, LLMs are also being made to use feedback from users or the environment to improve their planning performance [12, 35, 25].
Our work primarily establishes an assessment framework for evaluating a wide variety of planning capabilities of LLMs. PlanBench consists of multiple tasks, each designed to evaluate a certain aspect of reasoning about actions and change. The prompts for our tasks are showcased as few-shot examples where we provide an instance and an example completion and then ask for a completion on a new instance. Further, we use the domain description to explicitly constrain the possible actions. In many everyday scenarios, we are often asked to take into consideration unforeseen limitations and constraints. Our explicit domain description allows us to introduce such challenges and forces the LLMs to go beyond merely repeating possible information about the domain they may have come across in the training data. The ability to be conditional on the prompt is critical for the general systems to be customized for the specific domain of interest.</p>
<h1>3 Background</h1>
<p>As we are interested in investigating the basic planning problem, we want look at the most fundamental planning formalism, namely the goal-directed deterministic planning problem. These kinds of problems are colloquially referred to as classical planning problems.
Classical Planning Problems can be mathematically represented by using the tuple $\mathcal{P}=\langle\mathcal{D}, \mathcal{I}, \mathcal{G}\rangle$. $\mathcal{D}$ is referred to as the problem domain, $I$ is the initial state and $G$ is the goal specification. The possible truth assignment over the predicates defines the state space for the planning problem. The domain is again defined by the tuple $\mathcal{D}=\langle\mathcal{F}, \mathcal{O}\rangle . \mathcal{F}$ corresponds to the set of fluents, i.e., the state variable used to define the state space and each fluent corresponds to a predicate with some arity, and $\mathcal{A}$ correspond to the set of actions that can be performed as part of the planning problem. Each action $a_{i}[\mathcal{V}] \in \mathcal{A}$ (where $a_{i}$ is the operator label and $\mathcal{V}$ is the variable used by the operator and each variable could be mapped to an object), can be further defined by two components, the precondition $\operatorname{prec}[\mathcal{V}]$ which describes when an action can be executed and the effects $\operatorname{eff}[\mathcal{V}]$ which defines what happens when an action is executed. We will assume that $\operatorname{prec}[\mathcal{V}]$ consists of a set of predicates defined over the variables $\mathcal{V}$. An action is assumed to be executable only if its preconditions are met, i.e., the predicates in the precondition hold in the given state. The effects $\operatorname{eff}[\mathcal{V}]$ is further defined by the tuple $\langle\operatorname{add}[\mathcal{V}], \operatorname{del}[\mathcal{V}]\rangle$, where $\operatorname{add}[\mathcal{V}]$ or add effects is the set of predicates that will be set true by the action and $\operatorname{del}[\mathcal{V}]$ or delete effects is the set of predicates that will be set false by the action. An action is said to be grounded if we replace each of the variables with an object, else it is referred to as a lifted domain model (we use a similar convention to differentiate between lifted and grounded predicates). Below is a snippet of an action from a popular benchmark problem called Blocksworld, in PDDL. The action corresponds to putting down a block on a table in that domain.</p>
<div class="codehilite"><pre><span></span><code>(:action put-down
    :parameters (?ob)
    :precondition (holding ?ob)
    :effect (and (clear ?ob) (arm-empty) (on-table ?ob)
        (not (holding ?ob))))
</code></pre></div>

<p>In the above snippet, the parameter line provides the possible variables, in this case ?ob, which can stand for possible blocks. The precondition says that you can put down a block only if you are holding it (i.e. predicate (holding ?ob) is true for the block). The effects tell you that after you execute the action put-down, the block will be on the table and will be clear. You won't be holding the block and the arm will be considered empty. The actions may additionally be associated with cost, in these cases, one could also talk about optimal plans, i.e., a plan $\pi$ is called an optimal one if no plan exists that is less costly than $\pi$.
The above description presents one of the simpler classes of planning models and can be extended in multiple ways including allowing for object typing (including type hierarchy), more complex forms of preconditions and conditional effects, not to mention supporting richer classes of planning formalisms.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The diagrammatic overview of the overall test framework. Our system consists of a domainspecific component that allows the generation of various instances of the specific PDDL planning problems and the translation from PDDL to text and back. The domain-independent component is responsible for generating the test instances that will be fed into the LLM and verifying the output generated by the LLM.</p>
<h1>4 Assessment Architecture</h1>
<p>Our basic test framework consists of two categories of components, the domain-independent ones, provided as part of the framework, and the domain-dependent components which need to be developed for each new domain we test.</p>
<p>Domain independent component: The domain-independent component is built around a planner and a plan verification component that takes various planning problems and crafts test instances corresponding to various curriculum items. This component provides the mechanism to verify the solutions generated by the LLM. The current method is going to operate almost exclusively on symbolic models (specifically ones specified using PDDL [20]) and other structured inputs compatible with such representations.</p>
<p>This component is responsible for generating the content for the various prompts that would be generated as part of the different test cases and for validating the output generated by the LLM. As discussed earlier, the component primarily works on formal representations of the problems, so it relies on the translator component to convert any information it generates to natural language or to convert natural language information back to formal representations. For each test case, we mainly rely on a domain-independent planner and a plan validator to generate the relevant information or to validate the output provided by the LLM. For some test cases, we incorporate domain-specific information while crafting the prompts. In each case, there is a test-case-specific component that uses the problems provided by the problem generator component to craft specific test-case content.</p>
<p>Domain dependent component: The domain-dependent component consists of three parts; a domain model, a problem generator and a translator. The lifted domain file describes the various actions that may be available to solve any given planning problem, the various predicates that could be used to describe the various relationships over the objects that may be present at a given problem instance of the domain, and the various types of objects that may be part of the given problem. The domain model is lifted as it does not refer to the actual objects that may be part of the problem, but instead, the actions are defined independently of the exact objects it may influence.</p>
<p>The role of the problem generator is to generate random problem instances consisting of various objects, initial states, and goals. These problems become the basis of generating the various test cases that we will be using throughout the framework. Any distributional requirements we hope to use in the tests could be built into this problem generator.</p>
<p>The translator converts the symbolic model information to natural language text and vice versa . Translating the prompts to natural language would benefit the users of the benchmark who might want to be in the loop to either impose additional constraints or evaluate the plan themselves as these two tasks are more naturally done in natural language. For the current testbed (described below), we</p>
<p>developed a template-based mechanism to achieve this. In particular, we provide a natural language template for each predicate and each action, and we form texts of states and plans by concatenating these individual strings. In terms of parsing natural language text back into structured forms, the particular task we are interested in is converting plans generated by the LLM back into plan forms that can be used by plan validator tools like [10]. Since we use our prompts to shape the LLM's output, we require each action in the plan to be listed on a different line. Then, we can parse the exact action and arguments of the action by either using template-based matching or by assuming that the verb in the sentence corresponds to the action and each noun corresponds to an object which forms a parameter of the action (then mapping it to a possible action).</p>
<h1>5 Current Curriculum for Testing</h1>
<p>In this section, we go over each specific test case we provide as part of PlanBench. Each test case is meant to evaluate a central reasoning about actions and change capability and is tested in the context of a common sense planning domain. Each test case makes use of the few shot query setting of LLM where the LLM is provided a few sample answers to the specific reasoning ability being tested and is asked to respond to a new instance. ${ }^{3}$ The exact form of the prompt will depend on the specific test cases, but every instance will start with a description of the lifted planning domain that describes what actions can be executed, their preconditions and their effects. The current set of test cases includes the following cases:</p>
<ol>
<li>Plan Generation - Can the LLM come up with valid plans that will achieve a specific goal?</li>
<li>Cost Optimal Planning - Can the LLM come up with plans that are optimal to achieve a specific goal?</li>
<li>Plan Verification - Can the LLM determine if a plan will successfully execute, and if not, can it explain why?</li>
<li>Reasoning about plan execution - Can the LLM reason about what happens when a plan is executed?</li>
<li>Robustness to goal reformulation - Can the LLM recognize the same goal when specified in different ways?</li>
<li>Ability to reuse plans - Can the LLM recognize scenarios where it can reuse part or the whole of the original plan to achieve the new goal?</li>
<li>Replanning - Can the LLM replan for cases where an unexpected change is reported?</li>
<li>Plan Generalization - Can the LLM take specific plans, extract underlying procedural patterns and apply them to a new instance?</li>
</ol>
<p>Out of the eight test cases, the first two test cases correspond to actual planning problems (i.e. plan generation and cost-optimal planning) and the rest correspond to simpler auxiliary tasks related to reasoning about action and change. We ground the test cases in multiple domains based on the kinds employed in International Planning Competitions. The domain description is included at the beginning of every prompt. In the rest of the section, we discuss the structure of the prompt for each of the test cases. We provide an example prompt and the corresponding completion generated by an LLM for each of the test cases in the Appendix.</p>
<p>Plan Generation: Following the lifted domain description, the prompt consists of a few instances of planning problem descriptions (consisting of a description of the initial state, the goal) and the corresponding plan (which ends with a tag, henceforth referred to as the plan-end tag, that denotes the end of the plan) and finally, we end the prompt with a planning problem description. The plan-end tag (which is separate from the end-generation tag) is introduced to enable easier plan extraction in cases where the LLM also adds commentary along with the plan. In the case where our extractor cannot reasonably extract a plan from the response, we mark that instance as an incorrect one. We present an example prompt for this test case here. For the rest of the test cases, the examples are provided in the Appendix.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>One-shot prompt for Plan Generation with GPT-3's plan</h1>
<p>I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the $\sim$ actions I can do</p>
<p>Pick up a block
Unstack a block from on top of another block
Put down a block
Stack a block on top of another block
I have the following restrictions on my actions:
I can only pick up or unstack one block at a time.
I can only pick up or unstack a block if my hand is empty.
I can only pick up a block if the block is on the table and the block is clear. A block is clear
$\sim$ if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking was really on
$\sim$ top of the other block.
I can only unstack a block from on top of another block if the block I am unstacking is clear.
Once I pick up or unstack a block, I am holding the block.
I can only put down a block that I am holding.
I can only stack a block on top of another block if I am holding the block being stacked.
I can only stack a block on top of another block if the block onto which I am stacking the block
$\sim$ is clear.
Once I put down or stack a block, my hand becomes empty.
Once you stack a block on top of a second block, the second block is no longer clear.
[STATEMENT]
As initial conditions I have that, the red block is clear, the blue block is clear, the yellow
$\sim$ block is clear, the hand is empty, the blue block is on top of the orange block, the red block
is on the table, the orange block is on the table and the yellow block is on the table.
My goal is to have that the orange block is on top of the blue block.
My plan is as follows:
[PLAN]
unstack the blue block from on top of the orange block
put down the blue block
pick up the orange block
stack the orange block on top of the blue block
[PLAN END]
[STATEMENT]
As initial conditions I have that, the red block is clear, the yellow block is clear, the hand is
$\sim$ empty, the red block is on top of the blue block, the yellow block is on top of the orange
$\sim$ block, the blue block is on the table and the orange block is on the table.
My goal is to have that the orange block is on top of the red block.
My plan is as follows:
[PLAN]
--------- GPT-4 response
unstack the yellow block from on top of the orange block
put down the yellow block
unstack the red block from on top of the blue block
put down the red block
pick up the orange block
stack the orange block on top of the red block
[PLAN END]
--------- Extracted plan
(unstack yellow orange)
(put-down yellow)
(unstack red blue)
(put-down red)
(pick-up orange)
(stack orange red)
--------- Ground truth plan
(unstack yellow orange)
(put-down yellow)
(pick-up orange)
(stack orange red)
$==============$ SUCCESS $==============$</p>
<p>Cost-Optimal Planning: The prompt is quite similar to the one used in the earlier test case with a few changes. We modify the lifted domain description by including a statement that associates a cost with each action. To make the concept of action cost better fit into common sense domains, we can</p>
<p>map the cost to more common concepts like the time taken for executing the action or the amount of money that needs to be spent to execute an action. In the case of each problem description, before the plan is presented we need to explicitly mention that the plan is trying to minimize cost (which depending on the scenario might correspond to saying that the plan takes the least amount of time or the plan correspond to the cheapest plan). The result generated by the LLM is evaluated similarly to the previous query, but in addition to checking if the plan is valid, we also check if the cost of the plan corresponds to the optimal plan cost.</p>
<p>Plan Verification: Plan verification involves determining whether a proposed plan will successfully execute and achieve the stated goals when applied from the given initial state. Here, the prompt will first include three example instances, the corresponding candidate plans and the verification details. The set of examples will contain one example with a goal reaching plan, one with a non-goal reaching plan and one with an inexecutable plan. The prompt then contains a new instance along with a candidate plan. The LLM is tasked with answering whether the candidate plan is valid. If it answers no, it must identify the first inexecutable action and atleast one associated missing precondition (if inexecutable) or at least one missing goal condition (if not goal reaching).</p>
<p>Reasoning about plan execution: Here the objective is not to check whether the LLM can come up with plans, or verify them, but rather if they can predict the outcome of executing an action. The prompt here again starts with the domain description, but instead of providing planning problems and plans, we provide a state, an action sequence and then the state that would result from executing that action sequence in the provided state. Finally the prompt ends with a new state and a new action sequence. The LLM is expected to come up with the resulting state, which is then checked by applying a plan executor that will try to identify what state will result from the execution of the current action sequence on the provided state. We always provide executable action sequences for this test case.</p>
<p>Robustness to Goal Reformulation: In this test case, we will see if the LLM can recognize goals it has seen before if they are slightly modified. Here the prompt remains the same as the one used for plan generation. However, all the example problems have the same initial state, and the last problem provided has not only the same initial state but also the same goal as the example problem. Here the goal may be obfuscated in a few ways, for example, the goal facts may be reordered or one might include a subset of the original goal specification (meaning the same plan would still work) or vice-versa. We can again use the same evaluation technique as the plan generation test case to validate the output.</p>
<p>Ability to Reuse Plans: In this test case, we are interested in seeing if the LLM can reuse plans or parts of plans that it has seen before. The prompt is again the same as the plan generation, but the prompt ends with a problem that can be solved by a prefix of a previously seen plan. We again keep the initial state the same across the example problems shown. The evaluation remains the same as the plan generation test case.</p>
<p>Replanning: Replanning corresponds to the problem where there may be an unexpected event that occurs while executing a plan and the system needs to come up with a new plan in response to the event. Here, we focus on the ability of the LLM to replan when unexpected changes are reported. The prompt here starts with a domain description, then a set of instances where an unexpected event occurred during execution, and a new plan in response to the event. In each instance, a planning problem and a corresponding plan are provided at the beginning, the execution of the plan is described and then an unexpected event is noted (event corresponds to some facts unexpectedly turning true or false) and then a new plan from the changed state is presented. The prompt ends with a new case where the plan after replanning is left out and the LLM is expected to complete. The evaluation involves checking whether the new plan is valid from the changed state. The LLM output is evaluated to be true if the new plan it generates achieves the goals from the unexpectedly changed state. For each of the domains, we constrain the unexpected event to be of a specific type. We describe these cases for each domain in the next section.</p>
<p>Plan Generalization: In this test case, we want to evaluate whether LLMs can recognize the underlying pattern in the plans provided in the prompt and reuse it for a new planning problem. The prompt is the same as the plan generation case, except that all plans were generated by a fixed program. Here the program may contain loops or conditional statements, but can only solve certain types of problems, that is, the initial state and goals meet certain conditions. Such programs can be</p>
<p>thought of as a direct generalization of line plans that we have considered in the rest of the paper [30]. Execution of this program for a specific planning problem generates a sequence of actions. In this case, we will provide some example traces generated from the program and ask LLM to come up with a plan for a new problem that could be solved by it. The evaluation again would be to take the generated plan and see if it is valid for the given problem. Since this task requires the use of specific problems that admit generalizable solutions, not every problem works for this task. As such, this task requires problems to be curated for each domain. To develop this set of problems, we select a domain-specific generalized behavior and then create several problems that can be accomplished by merely adding more iterations of the generalized behavior to an example plan. We detail our domain-specific selection of behavior in Section 6.</p>
<h1>6 Dataset Details</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of the number of objects and optimal plan length for the Blocksworld problem set
While PlanBench makes it easy to run the curriculum on new domains, we initialize it with two different International Planning Competition (IPC) [13] domains: Blocksworld and Logistics. For each domain, we provide a description, distribution of selected problems, and details on domainspecific curriculum configurations below.</p>
<p>Blocksworld: The Blocksworld domain focuses on stacking blocks on a table. One hand is available to move blocks, and only one block may be moved by the hand at a time. Blocks cannot be moved if there are blocks on top of them and blocks cannot be stacked on a block with another block already on top of it. Goals specify the order that blocks within a stack should be stacked in but may include multiple stacks or ask for blocks to be left on the table. Blocks our identified with colors. We developed 600 instances which vary in the number of objects used as well as the optimal plan length: we visualize these distributions in Figure 2. For the plan generalization test case, we focus on problems that can be solved by repeatedly adding clear blocks to the stack and generate 500 instances separate from the main blocksworld dataset. For the replanning test case we constrain the unexpected event to be of a specific type: We execute a random prefix of the plan which ensures that some block is held at the end of that prefix. We then change the resulting state by stacking the held block onto another random block which is clear and make the hand empty. This change is reported and the LLM is asked to replan from the changed state.</p>
<p>Logistics: The Logistics domain involves moving packages between different locations. Locations are grouped by cities. Trucks can be used to move packages between locations in the same city and planes can be used to move packages between cities. Goals specify where packages should be moved. We generate 285 instances for this domain We provide the distribution of objects (which includes cities, locations, trucks, airplanes and packages) and optimal plan length over the problems in Figure 3. For the plan generalization test case we focus on problems that contain one or more instances of the following; dropping off a package at a location and picking up the next package at that same location, allowing for a chain of package movement within a city through a truck and finally have a package at an airport be flown to another airport. For the replanning test case, we execute a random prefix of the plan which ensures a package is in a truck or an airplane. We then shift the position of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of the number of objects and optimal plan length for the Logistics problem set the package from inside the truck or airplane to a random location which is not the goal location. This change is reported and the LLM is asked to replan from the changed state.</p>
<p>Obfuscation of domains: Although the domain specification is part of our prompts, the names of the objects (e.g. blocks, trucks), predicates (e.g. on-table, in-city) and actions (e.g. pickup, drive) still do provide connections to the commonsense knowledge that the pretrained LLMs possess. One intriguing question is whether the planning performance is based really only on the domain model or patterns discovered from these other background connections. In order to test this, as part of our benchmark, we also provide obfuscated versions of the above domains, where the action names, predicate names and object names are obfuscated either with misleading words or random alphanumeric strings. Note that, for a standard planner, the original domain and the obfuscated version are identical. Further, we also provide the code to perform arbitrary obfuscations for the above domains or any additional domains that might be added in the future.</p>
<p>On the whole, our dataset consists of $\sim 26250$ prompts across the various test cases and domains (including the obfuscated versions). We will now look at the results of GPT-4 and InstructGPT-3 on the Blocksworld domain in PlanBench.</p>
<h1>7 Specimen Evaluation of PlanBench</h1>
<p>While the objective of this paper is to make PlanBench available to other researchers, we also did some initial experiments to give useful baselines. Our evaluation here primarily focuses on two Large Language Models, GPT-4 and InstructGPT3. We used the OpenAI API to access these models. In particular, we evaluated the test framework on the Blocksworld domain. In Table 1, we have presented the results of GPT-4 and Instruct-GPT3 (text-davinci-002) on PlanBench. The best results within each model were observed for the auxiliary goal reformulation test cases. However, even the most effective model (GPT-4) falls short on most of the test cases in the Blocksworld domain of PlanBench. Overall, the performance of these LLMs on our benchmark shows that, as of right now, LLMs are pretty ineffective in reasoning about actions and change. PlanBench can thus serve as a useful marker of progress of LLMs in planning and reasoning. In a companion study [33], we conducted a deeper investigation into the planning abilities of current state-of-the-art LLMs, critically examining their plan generation abilities under various assumed roles. We performed experiments with different prompt configurations, delved into the failure modes of LLMs for autonomous plan generation and analyzed performance shifts when LLMs assume heuristic roles (LLM-Modulo settings) in planning systems.</p>
<h2>8 Conclusion and Future Work</h2>
<p>In this paper, we presented PlanBench, a reasoning assessment suite for large language models (LLMs) that consists of various test cases each evaluating a central aspect of planning and reasoning about actions and change. Our results show that even in simple common-sense planning domains, LLMs seem to display subpar performance. Our goal is to establish an extensible benchmark where researchers can evaluate the current and future large language models. Our assessment suite can</p>
<p>Table 1: PlanBench Results of GPT-4 and Instruct-GPT3 (text-davinci-002) on Blocksworld domain. The tasks in the highlighted rows correspond to actual planning problems while the others correspond to simpler auxiliary planning tasks.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Instances correct</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT-4</td>
<td>I-GPT3</td>
</tr>
<tr>
<td>Plan Generation</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance and the respective plan as an example and prompt the machine with a new instance.</td>
<td>$\begin{aligned} &amp; 206 / 600 \ &amp; (34.3 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 41 / 600 \ &amp; (6.8 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Cost-Optimal Planning</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance, the respective optimal plan and the associated cost as an example and prompt the machine with a new instance.</td>
<td>$\begin{aligned} &amp; 198 / 600 \ &amp; (33 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 35 / 600 \ &amp; (5.8 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Plan Verification</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase three instances and three distinct plans (goal reaching, non goal-reaching and inexecutable) and present the respective validation and explanations. We then present a new instance and a plan and ask the machine for to verify and provide an explanation, if needed.</td>
<td>$\begin{aligned} &amp; 352 / 600 \ &amp; (58.6 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 72 / 600 \ &amp; (12 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Reasoning About Plan Execution</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance, an action sequence and the corresponding resulting state after executing the action sequence as an example. We then provide an instance and an executable action sequence and ask the machine to provide the resulting state.</td>
<td>$\begin{aligned} &amp; 191 / 600 \ &amp; (31.8 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 4 / 600 \ &amp; (0.6 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Replanning</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance, the respective plan and present an unexpected change of the state. We then also present a new plan from the changed state. Finally, for a new instance we repeat the same except we ask the machine for the new plan.</td>
<td>$\begin{aligned} &amp; 289 / 600 \ &amp; (48.1 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 40 / 600 \ &amp; (6.6 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Plan Generalization</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance and the respective plan as an example and prompt the machine with a new instance. The plans for both the instances can be generated by a fixed program containing loops and conditionals.</td>
<td>$\begin{aligned} &amp; 141 / 500 \ &amp; (28.2 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 49 / 500 \ &amp; (9.8 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Plan Reuse</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance and the respective plan as an example and prompt the machine with an instance which requires only a certain prefix of the plan provided in the example.</td>
<td>$\begin{aligned} &amp; 392 / 600 \ &amp; (65.3 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 102 / 600 \ &amp; (17 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Robustness to Goal Reformulation (Shuffling goal predicates)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance and the respective plan as an example and prompt the machine with the same instance but shuffle the ordering of the goals.</td>
<td>$\begin{aligned} &amp; 461 / 600 \ &amp; (76.8 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 467 / 600 \ &amp; (77.8 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Robustness to Goal Reformulation (Full $\rightarrow$ Partial)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance with a fully specified goal state and the respective plan as an example and prompt the machine with the same instance but provide a partially specified goal state.</td>
<td>$\begin{aligned} &amp; 522 / 600 \ &amp; (87 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 467 / 600 \ &amp; (77.8 \%) \end{aligned}$</td>
</tr>
<tr>
<td>Robustness to Goal Reformulation (Partial $\rightarrow$ Full)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>We showcase an instance with a partially specified goal state and the respective plan as an example and prompt the machine with the same instance but provide a fully specified goal state.</td>
<td>$\begin{aligned} &amp; 348 / 600 \ &amp; (58 \%) \end{aligned}$</td>
<td>$\begin{aligned} &amp; 363 / 600 \ &amp; (60.5 \%) \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>be improved in multiple ways in the future. For instance, evaluation metrics that consider partial correctness could be incorporated into the benchmark. Additionally, the benchmark could be extended to support more number of IPC domains. In conclusion, we hope that PlanBench encourages other researchers to test the capabilities of their systems across different LLM models [2, 4, 28, 36, 24, 32, 9] and even those that are finetuned for such tasks.</p>
<h1>9 Acknowledgements</h1>
<p>This research was supported by ONR grants N00014-18-1-2442, N00014-18-1-2840, N00014-19-1-2119 and N00014-23-1-2409, AFOSR grant FA9550-18-1-0067, DARPA SAIL-ON grant W911NF-19-2-0006, and a JP Morgan AI Faculty Research Grant to Kambhampati. Sreedharan was supported in part by NSF grant 2303019. We would also like to thank Kaya Stechly and Anil Murthy for their help in adding additional domains to the benchmark.</p>
<h1>References</h1>
<p>[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
[2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[3] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[4] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. pages 5547-5569, 2022.
[5] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.
[6] Malik Ghallab, Dana S. Nau, and Paolo Traverso. Automated Planning and Acting. Cambridge University Press, 2016.
[7] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. arXiv preprint arXiv:2305.14909, 2023.
[8] Malte Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26:191-246, 2006.
[9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
[10] Richard Howey, Derek Long, and Maria Fox. VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. In 16th IEEE International Conference on Tools with Artificial Intelligence, pages 294-301. IEEE, 2004.
[11] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
[12] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.
[13] IPC. International planning competition. https://www.icaps-conference.org/competitions, 1998.
[14] Liwei Jiang, Jena D. Hwang, Chandrasekhar Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. Delphi: Towards Machine Ethics and Norms. ArXiv, abs/2110.07574, 2021.
[15] Subbarao Kambhampati. AI as (an Ersatz) Natural Science? https://cacm.acm.org/blogs/blog-cacm/261732-ai-as-an-ersatz-natural-science/fulltext, Jun 2022.
[16] Subbarao Kambhampati, Karthik Valmeekam, Matthew Marquez, and Lin Guan. On the role of large language models in planning, July 2023. Tutorial presented at the International Conference on Automated Planning and Scheduling (ICAPS), Prague. https://yochan-lab.github. io/tutorial/ICAPS-2023/.
[17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916, 2022.
[18] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
[19] Gary Marcus and Ernest Davis. Experiments testing GPT-3's ability at commonsense reasoning: results. https://cs.nyu.edu/ davise/papers/GPT3CompleteTests.html, 2020.</p>
<p>[20] Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, and David E. Wilkins. Pddl-the planning domain definition language. 1998.
[21] OpenAI. Gpt-4 technical report, 2023.
[22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[23] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP Models really able to Solve Simple Math Word Problems? arXiv preprint arXiv:2103.07191, 2021.
[24] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
[25] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. arXiv preprint arXiv:2211.09935, 2022.
[26] Stuart J Russell and Peter Norvig. Artificial intelligence a modern approach. London, 2010.
[27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732-8740, 2020.
[28] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.
[29] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.
[30] Siddharth Srivastava, Shlomo Zilberstein, Neil Immerman, and Hector Geffner. Qualitative numeric planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pages 1010-1016, 2011.
[31] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.
[32] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language Models for Dialog Applications. arXiv preprint arXiv:2201.08239, 2022.
[33] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. arXiv preprint arXiv:2305.15771, 2023.
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[35] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.
[36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<h1>A Appendix</h1>
<h2>Contents</h2>
<p>1 Introduction ..... 1
2 Related Work ..... 2
3 Background ..... 3
4 Assessment Architecture ..... 4
5 Current Curriculum for Testing ..... 5
6 Dataset Details ..... 8
7 Specimen Evaluation of PlanBench ..... 9
8 Conclusion and Future Work ..... 9
9 Acknowledgements ..... 10
A Appendix ..... 13
A. 1 Broader Impact ..... 14
A. 2 Additional details on experiments ..... 14
A. 3 Obfuscation experiments ..... 14
A. 4 Analysis of GPT-4 plans ..... 15
A.4.1 Comparison to the dataset ..... 15
A.4.2 Failure modes analysis ..... 15
A. 5 Plan Generation Prompts ..... 16
A.5.1 Blocksworld ..... 16
A.5.2 Logistics ..... 17
A.5.3 Mystery Blocksworld ..... 18
A. 6 Cost Optimal Planning Prompts ..... 19
A.6.1 Blocksworld ..... 19
A.6.2 Logistics ..... 20
A.6.3 Mystery Blocksworld ..... 21
A. 7 Plan Verification Promptss ..... 22
A.7.1 Blocksworld ..... 22
A.7.2 Logistics ..... 23
A.7.3 Mystery Blocksworld ..... 25
A. 8 Reasoning About Plan Execution Prompts ..... 27
A.8.1 Blocksworld ..... 27
A.8.2 Logistics ..... 28</p>
<p>A.8.3 Mystery Blocksworld ..... 29
A. 9 Replanning Prompts ..... 30
A.9.1 Blocksworld ..... 30
A.9.2 Logistics ..... 31
A.9.3 Mystery Blocksworld ..... 32
A. 10 Plan Generalization Prompts ..... 33
A.10.1 Blocksworld ..... 33
A.10.2 Logistics ..... 35
A.10.3 Mystery Blocksworld ..... 37
A. 11 Plan Reuse Prompts ..... 39
A.11.1 Blocksworld ..... 39
A.11.2 Logistics ..... 40
A.11.3 Mystery Blocksworld ..... 41
A. 12 Robustness to Goal Reformulation (Shuffling goal predicates) Prompts ..... 41
A.12.1 Blocksworld ..... 41
A.12.2 Logistics ..... 42
A.12.3 Mystery Blocksworld ..... 43
A. 13 Robustness to Goal Reformulation (Full $\rightarrow$ Partial) Prompts ..... 44
A.13.1 Blocksworld ..... 44
A.13.2 Logistics ..... 45
A.13.3 Mystery Blocksworld ..... 46
A. 14 Robustness to Goal Reformulation (Partial $\rightarrow$ Full) Prompts ..... 47
A.14.1 Blocksworld ..... 47
A.14.2 Logistics ..... 48
A.14.3 Mystery Blocksworld ..... 49</p>
<h1>A. 1 Broader Impact</h1>
<p>PlanBench seeks to cover a broad range of tasks related to planning and reasoning. Our benchmark serves as a useful marker of progress of LLMs in planning and reasoning and enables efficient comparisons of performance between LLMs in such tasks. As for ethical concerns, PlanBench does not contain any personally-identifiable or privacy-related information. While PlanBench is intended to measure planning capabilities, there is no guarantee that deploying models for external planning, based solely on their performance on this benchmark will result in correct or safe plans.</p>
<h2>A. 2 Additional details on experiments</h2>
<p>All the experiments were run using the OpenAI API with all default parameters except the temperature. The temperature was made 0 , making the LLMs deterministic. For GPT-4, the version we used had an 8 k context window and was used between the months of March and June. We used the Fast-Downward system [8] as the planner and VAL [10] as the plan validator in our framework.</p>
<h2>A. 3 Obfuscation experiments</h2>
<p>As shown in Table 2, the performance of both GPT-4 and Instruct-GPT3 decreases significantly when the domain is obfuscated either deceptively or randomly. This shows that whatever planning</p>
<p>Table 2: Results of GPT-4 and Instruct-GPT3 for the Plan Generation test case.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Instances correct</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT-4</td>
<td>InstructGPT-3</td>
</tr>
<tr>
<td>Mystery Blocksworld (Deceptive)</td>
<td>$26 / 600(4.3 \%)$</td>
<td>$14 / 600(0.23 \%)$</td>
</tr>
<tr>
<td>Mystery Blocksworld (Randomized)</td>
<td>$12 / 600(2 \%)$</td>
<td>$6 / 600(1 \%)$</td>
</tr>
</tbody>
</table>
<p>performance they showed in the Blocksworld domain was more likely due to pattern matching rather than reasoning (which should have been robust to such kinds of obfuscation).</p>
<h1>A. 4 Analysis of GPT-4 plans</h1>
<h2>A.4.1 Comparison to the dataset</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The graph on the left shows the distribution of the number of instances in the Blocksworld domain over their optimal plan lengths. The graph on the right shows the distribution of the number of correct plans by GPT-4 over optimal plan lengths.</p>
<p>Figure 4 provides insight into how the optimal plan length and block count distributions of Blocksworld instances where GPT-4 provides a correct plan stacks up against the optimal plan length and block count distributions of all instances. By comparing the two figures, we notice that more "shallow" instances - instances with a smaller optimal plan length - do not necessarily see an increase in the performance of LLMs. While perhaps deviating from intuition for classical planners, this is not unexpected given the knowledge of how LLMs work: regardless of the specifics of an instance, LLMs will attempt to solve it by predicting the next token based on the weights and the context.</p>
<h2>A.4.2 Failure modes analysis</h2>
<p>To delve deeper into the failure cases of GPT-4, we performed a more forgiving evaluation of the validity of the generated plans in the plan generation test case. We used the concept of domain model relaxations from the automated planning community (which are used to derive heuristics [6]) and considered two types of relaxations: (i) delete relaxation which involves ignoring the delete conditions of the domain actions and (ii) precondition relaxation which ignores all the preconditions of the actions-thus assuming that any action is executable at any state giving their effects. We evaluated GPT-4's plans with respect to domain models that are delete-relaxed, precondition-relaxed or both. Note that a plan that is correct in the unrelaxed model will also be correct in all the relaxed versions. We observe (as shown in Figure 5) that even in the most relaxed version of the evaluation, GPT-4 still fails in a lot of instances.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GPT-4 failure cases in the plan generation test case</p>
<h1>A. 5 Plan Generation Prompts</h1>
<h2>A.5.1 Blocksworld</h2>
<h2>Blocksworld Plan Generation Prompt with GPT-4 Completion</h2>
<p>I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the $\sim$ actions I can do</p>
<p>Pick up a block
Unstack a block from on top of another block
Put down a block
Stack a block on top of another block
I have the following restrictions on my actions:
I can only pick up or unstack one block at a time.
I can only pick up or unstack a block if my hand is empty.
I can only pick up a block if the block is on the table and the block is clear. A block is clear
$\sim$ if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking was really on $\sim$ top of the other block.
I can only unstack a block from on top of another block if the block I am unstacking is clear.
Once I pick up or unstack a block, I am holding the block.
I can only put down a block that I am holding.
I can only stack a block on top of another block if I am holding the block being stacked.
I can only stack a block on top of another block if the block onto which I am stacking the block $\sim$ is clear.
Once I put down or stack a block, my hand becomes empty.
[STATEMENT]
As initial conditions I have that, the red block is clear, the blue block is clear, the yellow $\sim$ block is clear, the hand is empty, the blue block is on top of the orange block, the red block $\sim$ is on the table, the orange block is on the table and the yellow block is on the table. My goal is to have that the orange block is on top of the blue block.</p>
<p>My plan is as follows:
[PLAN]
unstack the blue block from on top of the orange block
put down the blue block
pick up the orange block
stack the orange block on top of the blue block
[PLAN END]
[STATEMENT]
As initial conditions I have that, the red block is clear, the yellow block is clear, the hand is $\sim$ empty, the red block is on top of the blue block, the yellow block is on top of the orange $\sim$ block, the blue block is on the table and the orange block is on the table.
My goal is to have that the orange block is on top of the red block.
My plan is as follows:
[PLAN]
-----------GPT-4 response-----------------</p>
<div class="codehilite"><pre><span></span><code><span class="n">unstack</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">yellow</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">top</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">orange</span><span class="w"> </span><span class="n">block</span>
<span class="n">put</span><span class="w"> </span><span class="n">down</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">yellow</span><span class="w"> </span><span class="n">block</span>
<span class="n">unstack</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">red</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">top</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">blue</span><span class="w"> </span><span class="n">block</span>
<span class="n">put</span><span class="w"> </span><span class="n">down</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">red</span><span class="w"> </span><span class="n">block</span>
<span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">orange</span><span class="w"> </span><span class="n">block</span>
<span class="n">stack</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">orange</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">top</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">red</span><span class="w"> </span><span class="n">block</span>
<span class="p">[</span><span class="n">PLAN</span><span class="w"> </span><span class="n">END</span><span class="p">]</span>
<span class="o">-----------</span><span class="n">Extracted</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span><span class="n">plan</span><span class="o">----------------</span>
<span class="p">(</span><span class="n">unstack</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="n">c</span><span class="p">)</span>
<span class="p">(</span><span class="n">put</span><span class="o">-</span><span class="n">down</span><span class="w"> </span><span class="n">d</span><span class="p">)</span>
<span class="p">(</span><span class="n">unstack</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="p">(</span><span class="n">put</span><span class="o">-</span><span class="n">down</span><span class="w"> </span><span class="n">a</span><span class="p">)</span>
<span class="p">(</span><span class="n">pick</span><span class="o">-</span><span class="n">up</span><span class="w"> </span><span class="n">c</span><span class="p">)</span>
<span class="p">(</span><span class="n">stack</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="n">a</span><span class="p">)</span>
<span class="o">-----------</span><span class="n">Ground</span><span class="w"> </span><span class="n">truth</span><span class="w"> </span><span class="n">plan</span><span class="o">----------------</span>
<span class="p">(</span><span class="n">unstack</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="n">c</span><span class="p">)</span>
<span class="p">(</span><span class="n">put</span><span class="o">-</span><span class="n">down</span><span class="w"> </span><span class="n">d</span><span class="p">)</span>
<span class="p">(</span><span class="n">pick</span><span class="o">-</span><span class="n">up</span><span class="w"> </span><span class="n">c</span><span class="p">)</span>
<span class="p">(</span><span class="n">stack</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="n">a</span><span class="p">)</span>
<span class="o">----------------------------</span><span class="n">SUCCESS</span><span class="o">----------------------------</span>
</code></pre></div>

<h1>A.5.2 Logistics</h1>
<h2>Logistics Plan Generation Prompt with GPT-4 Completion</h2>
<p>I have to plan logistics to transport packages within cities via trucks and between cities via $\sim$ airplanes. Locations within a city are directly connected (trucks can move between any two $\sim$ such locations), and so are the cities. In each city there is exactly one truck and each city $\sim$ has one location that serves as an airport.
Here are the actions that can be performed:
Load a package into a truck. For example, load package_1 into truck_1 at location_1_1.
Load a package into an airplane. For example, load package_1 into airplane_1 at location_1_1.
Unload a package from a truck. For example, unload package_1 from truck_1 at location_1_1.
Unload a package from an airplane. For example, unload package_1 from airplane_1 at location_1_1. Drive a truck from one location to another location. For example, drive truck_1 from location_1_1 $\sim$ to location_1_2 in city_1.
Fly an airplane from one city to another city. For example, fly airplane_1 from location_1_1 to $\sim$ location_2_1. Here location_1_1 is the airport in city_1 and location_2_1 is the airport in $\sim$ city_2.
The following are the restrictions on the actions:
A package can be loaded into a truck only if the package and the truck are in the same location. Once a package is loaded into a truck, the package is not at the location and is in the truck. A package can be loaded into an airplane only if the package and the airplane are in the same $\sim$ location.
Once a package is loaded into an airplane, the package is not at the location and is in the $\sim$ airplane.
A package can be unloaded from a truck only if the package is in the truck.
Once a package is unloaded from a truck, the package is not in the truck and is at the location of $\sim$ the truck.
A package can be unloaded from an airplane only if the package in the airplane.
Once a package is unloaded from an airplane, the package is not in the airplane and is at the $\sim$ location of the airplane.
A truck can be driven from one location to another if the truck is at the from-location and both $\sim$ from-location and to-location are locations in the same city.
Once a truck is driven from one location to another, it is not at the from-location and is at the $\sim$ to-location.
An airplane can be flown from one city to another if the from-location and the to-location are $\sim$ airports and the airplane is at the from-location.
Once an airplane is flown from one city to another the airplane is not at the from-location and is $\sim$ at the to-location.</p>
<h2>[STATEMENT]</h2>
<p>As initial conditions I have that, location_0_0 is an airport, location_1_0 is an airport,
$\sim$ airplane_0 is at location_0_0, airplane_1 is at location_0_0, package_0 is at location_1_0,
$\sim$ truck_0 is at location_0_0, truck_1 is at location_1_0, location_0_0 is in the city city_0 and
$\sim$ location_1_0 is in the city city_1.
My goal is to have that package_0 is at location_0_0.
My plan is as follows:
[PLAN]
fly airplane_1 from location_0_0 to location_1_0</p>
<p>load package_0 into airplane_1 at location_1_0
fly airplane_1 from location_1_0 to location_0_0
unload package_0 from airplane_1 at location_0_0
[PLAN END]
[STATEMENT]
As initial conditions I have that, location_0_0 is an airport, location_1_0 is an airport,
$\sim$ airplane_0 is at location_1_0, airplane_1 is at location_1_0, package_0 is at location_1_0,
$\sim$ truck_0 is at location_0_0, truck_1 is at location_1_0, location_0_0 is in the city city_0 and
$\sim$ location_1_0 is in the city city_1.
My goal is to have that package_0 is at location_0_0.
My plan is as follows:
[PLAN]
-----------Ground truth plan---------------
(load-airplane p0 a1 11-0)
(fly-airplane a1 11-0 10-0)
(unload-airplane p0 a1 10-0)</p>
<h1>A.5.3 Mystery Blocksworld</h1>
<h2>Mystery Blocksworld Plan Generation Prompt with GPL-4 Completion</h2>
<p>I am playing with a set of objects. Here are the actions I can do
Attack object
Feast object from another object
Succumb object
Overcome object from another object
I have the following restrictions on my actions:
To perform Attack action, the following facts need to be true: Province object, Planet object,
$\sim$ Harmony.
Once Attack action is performed the following facts will be true: Pain object.
Once Attack action is performed the following facts will be false: Province object, Planet
$\sim$ object, Harmony.
To perform Succumb action, the following facts need to be true: Pain object.
Once Succumb action is performed the following facts will be true: Province object, Planet
$\sim$ object, Harmony.
Once Succumb action is performed the following facts will be false: Pain object.
To perform Overcome action, the following needs to be true: Province other object, Pain
$\sim$ object.
Once Overcome action is performed the following will be true: Harmony, Province object, Object
$\sim$ Craves other object.
Once Overcome action is performed the following will be false: Province other object, Pain
$\sim$ object.
To perform Feast action, the following needs to be true: Object Craves other object, Province
$\sim$ object, Harmony.
Once Feast action is performed the following will be true: Pain object, Province other object.
Once Feast action is performed the following will be false:, Object Craves other object,
$\sim$ Province object, Harmony.
[STATEMENT]
As initial conditions I have that, object b craves object c, harmony, planet object a, planet
$\sim$ object c, planet object d, province object a, province object b and province object d.
My goal is to have that object c craves object b.
My plan is as follows:
[PLAN]
feast object b from object c
succumb object b
attack object c
overcome object c from object b
[PLAN END]
[STATEMENT]
As initial conditions I have that, object a craves object b, object d craves object c, harmony,
$\sim$ planet object b, planet object c, province object a and province object d.
My goal is to have that object c craves object a.
My plan is as follows:
[PLAN]</p>
<h1>A. 6 Cost Optimal Planning Prompts</h1>
<h2>A.6.1 Blocksworld</h2>
<h2>Blocksworld Cost Optimal Planning Prompt with GPT-4 Completion</h2>
<p>I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the $\sim$ actions I can do:</p>
<p>Pick up a block. It takes 1 minute to pick up a block.
Unstack a block from on top of another block. It takes 1 minute to unstack a block from on top of $\sim$ another block.
Put down a block. It takes 1 minute to put down a block.
Stack a block on top of another block. It takes 1 minute to stack a block on top of another block.
I have the following restrictions on my actions:
I can only pick up or unstack one block at a time.
I can only pick up or unstack a block if my hand is empty.
I can only pick up a block if the block is on the table and the block is clear. A block is clear $\sim$ if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking was really on $\sim$ top of the other block.
I can only unstack a block from on top of another block if the block I am unstacking is clear.
Once I pick up or unstack a block, I am holding the block.
I can only put down a block that I am holding.
I can only stack a block on top of another block if I am holding the block being stacked.
I can only stack a block on top of another block if the block onto which I am stacking the block $\sim$ is clear.
Once I put down or stack a block, my hand becomes empty.
Once you stack a block on top of a second block, the second block is no longer clear.
[STATEMENT]
As initial conditions I have that, the red block is clear, the blue block is clear, the yellow $\sim$ block is clear, the hand is empty, the blue block is on top of the orange block, the red block $\sim$ is on the table, the orange block is on the table and the yellow block is on the table. My goal is to have that the orange block is on top of the blue block. I want to minimize the time $\sim$ taken to achieve my goal.
My plan is as follows:
[PLAN]
unstack the blue block from on top of the orange block
put down the blue block
pick up the orange block
stack the orange block on top of the blue block
[PLAN END]
The total time to execute the plan is 4 minutes.
[STATEMENT]
As initial conditions I have that, the red block is clear, the yellow block is clear, the hand is $\sim$ empty, the red block is on top of the blue block, the yellow block is on top of the orange $\sim$ block, the blue block is on the table and the orange block is on the table.
My goal is to have that the orange block is on top of the red block. I want to minimize the time $\sim$ taken to achieve my goal.
My plan is as follows:
[PLAN]
-----------GPT-4 response----------------
unstack the yellow block from on top of the orange block
put down the yellow block
unstack the red block from on top of the blue block
put down the red block
pick up the orange block
stack the orange block on top of the red block
[PLAN END]
The total time to execute the plan is 6 minutes.
-----------Extracted LLM plan----------------
(unstack yellow orange)
(put-down yellow)
(unstack red blue)</p>
<h1>A.6.2 Logistics</h1>
<h2>Logistics Cost Optimal Planning Prompt with GPT-4 Completion</h2>
<p>I have to plan logistics to transport packages within cities via trucks and between cities via $\sim$ airplanes. Locations within a city are directly connected (trucks can move between any two $\sim$ such locations), and so are the cities. In each city there is exactly one truck and each city $\sim$ has one location that serves as an airport.
Here are the actions that can be performed:
Load a package into a truck. It takes one minute to do this action.
Load a package into an airplane. It takes one minute to do this action.
Unload a package from a truck. It takes one minute to do this action.
Unload a package from an airplane. It takes one minute to do this action.
Drive a truck from one location to another location. It takes two minutes to do this action.
Fly an airplane from one city to another city. It takes five minutes to do this action.
The following are the restrictions on the actions:
A package can be loaded into a truck only if the package and the truck are in the same location. Once a package is loaded into a truck, the package is not at the location and is in the truck. A package can be loaded into an airplane only if the package and the airplane are in the same $\sim$ location.
Once a package is loaded into an airplane, the package is not at the location and is in the $\sim$ airplane.
A package can be unloaded from a truck only if the package is in the truck.
Once a package is unloaded from a truck, the package is not in the truck and is at the location of $\sim$ the truck.
A package can be unloaded from an airplane only if the package in the airplane.
Once a package is unloaded from an airplane, the package is not in the airplane and is at the $\sim$ location of the airplane.
A truck can be driven from one location to another if the truck is at the from-location and both $\sim$ from-location and to-location are locations in the same city.
Once a truck is driven from one location to another, it is not at the from-location and is at the $\sim$ to-location.
An airplane can be flown from one city to another if the from-location and the to-location are $\sim$ airports and the airplane is at the from-location.
Once an airplane is flown from one city to another the airplane is not at the from-location and is $\sim$ at the to-location.</p>
<h2>[STATEMENT]</h2>
<p>As initial conditions I have that, location_0_0 is an airport, location_1_0 is an airport, $\sim$ airplane_0 is at location_0_0, airplane_1 is at location_0_0, package_0 is at location_1_0, $\sim$ truck_0 is at location_0_0, truck_1 is at location_1_0, location_0_0 is in the city city_0 and $\sim$ location_1_0 is in the city city_1.
My goal is to have that package_0 is at location_0_0. I want to minimize the time taken to achieve $\sim$ my goal.
My plan is as follows:
[PLAN]
fly airplane_1 from location_0_0 to location_1_0
load package_0 into airplane_1 at location_1_0
fly airplane_1 from location_1_0 to location_0_0
unload package_0 from airplane_1 at location_0_0
[PLAN END]
The total time to execute the plan is 4 minutes.</p>
<h2>[STATEMENT]</h2>
<p>As initial conditions I have that, location_0_0 is an airport, location_1_0 is an airport, $\sim$ airplane_0 is at location_1_0, airplane_1 is at location_1_0, package_0 is at location_1_0, $\sim$ truck_0 is at location_0_0, truck_1 is at location_1_0, location_0_0 is in the city city_0 and $\sim$ location_1_0 is in the city city_1.
My goal is to have that package_0 is at location_0_0. I want to minimize the time taken to achieve $\sim$ my goal.</p>
<p>My plan is as follows:
[PLAN]
----------Ground truth plan---------------
(load-airplane p0 a1 11-0)
(fly-airplane a1 11-0 10-0)
(unload-airplane p0 a1 10-0)</p>
<h1>A.6.3 Mystery Blocksworld</h1>
<h2>Mystery Blocksworld Cost Optimal Planning Prompt with GPF-4 Completion</h2>
<p>I am playing with a set of objects. Here are the actions I can do
Attack object. It takes 1 minute to do the Attack action.
Feast object from another object. It takes 1 minute to do the Feast action.
Succumb object. It takes 1 minute to do the Succumb action.
Overcome object from another object. It takes 1 minute to do the Overcome action.
I have the following restrictions on my actions:
To perform Attack action, the following facts need to be true: Province object, Planet object, $\sim$ Harmony.
Once Attack action is performed the following facts will be true: Pain object.
Once Attack action is performed the following facts will be false: Province object, Planet object, $\sim$ Harmony.
To perform Succumb action, the following facts need to be true: Pain object.
Once Succumb action is performed the following facts will be true: Province object, Planet object, $\sim$ Harmony.
Once Succumb action is performed the following facts will be false: Pain object.
To perform Overcome action, the following needs to be true: Province other object, Pain object.
Once Overcome action is performed the following will be true: Harmony, Province object, Object
$\sim$ Craves other object.
Once Overcome action is performed the following will be false: Province other object, Pain object.
To perform Feast action, the following needs to be true: Object Craves other object, Province
$\sim$ object, Harmony.
Once Feast action is performed the following will be true: Pain object, Province other object.
Once Feast action is performed the following will be false:, Object Craves other object, Province
$\sim$ object, Harmony.
[STATEMENT]
As initial conditions I have that, object b craves object c, harmony, planet object a, planet
$\sim$ object c, planet object d, province object a, province object b and province object d.
My goal is to have that object c craves object b. I want to minimize the time taken to achieve my
$\sim$ goal.
My plan is as follows:
[PLAN]
feast object b from object c
succumb object b
attack object c
overcome object c from object b
[PLAN END]
The total time to execute the plan is 4 minutes.
[STATEMENT]
As initial conditions I have that, object a craves object b, object d craves object c, harmony,
$\sim$ planet object h, planet object c, province object a and province object d.
My goal is to have that object c craves object a. I want to minimize the time taken to achieve my
$\sim$ goal.
My plan is as follows:
[PLAN]
----------Ground truth plan---------------
(feast object d object c)
(succumb object d)
(attack object c)
(overcome object c object a)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ While in this work we look only at one-shot natural language prompt configurations, we have also looked at various other prompt configurations (including zero-shot prompts, chain-of-thought prompts and pddl style prompts), specifically in the plan generation test case, in [33].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9609 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9609</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9609</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-264796520</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.19626v1.pdf" target="_blank">Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human. This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities. We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art. We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies. The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity. Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9609.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9609.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for Automated Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Automated Literature Review and Scholarly Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper states that LLMs can assist literature search and analytics by scanning and summarizing large volumes of text, identifying key concepts/themes, cross-referencing works, producing summaries, answering queries, and producing preliminary content or arguments for humanities domains (history, classics, philosophy, anthropology, psychology). The mention is descriptive and high-level; no new experimental pipeline or dataset is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (generic; examples mentioned elsewhere in paper: GPT-3, ChatGPT, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single bespoke architecture in this paper — the text refers broadly to transformer-based large pre-trained autoregressive LMs (GPT-family) and instruction-tuned/chat variants (ChatGPT/GPT-4) as tools that can be used to scan, summarize, cross-reference, and answer questions over large text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Described generically as 'large volumes of text' and domain literatures (historical texts, philosophical works, anthropological documents, survey results, and other humanities corpora). No concrete corpus, selection criteria, or counts are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>High-level literature-review tasks: automated literature review, extraction of key concepts/themes, cross-referencing between texts, summarization, question answering about domain literature, generation of preliminary arguments or background material.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>High-level description only: using LLM prompting to scan and summarize documents, perform cross-referencing and extraction; the paper also references related methodological work (e.g., retrieval-augmented generation surveys) but does not describe a concrete retrieval pipeline, prompt templates, chain-of-thought, or multi-stage summarization pipeline implemented by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries, concise literature reviews, extracted key concepts/themes, cross-reference lists, question-answer responses, preliminary content drafts or teaching materials.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not reported specifically for literature-synthesis applications in this paper; the paper mentions general evaluation methods for generated text (ROUGE, BLEU, and newer model-based factuality metrics) and human-in-the-loop review as common evaluation approaches in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Described strengths include speed and scalability (ability to quickly scan large volumes), multilingual handling, cross-referencing capability, and usefulness across many humanities subdomains (history, classics, philosophy, anthropology, psychology). LLMs can reduce time for manual literature triage and surface connections across texts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The paper explicitly warns of hallucinations and factuality problems with LLM outputs, domain biases (including geopolitical and other biases), and lack of deep human-like cognition — all of which limit reliability for rigorous theory synthesis. It also notes that the paper provides no task-specific experiments or validated synthesis pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No concrete failure-case experiments in the paper; the paper cites general failure modes such as hallucinated facts, amplified biases in generated synthesis, inaccuracies when calibrating historical timelines or extracting precise factual claims, and the need for human expert verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9609.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9609.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented Text Generation / Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites surveys and related work on retrieval-augmented text generation as a relevant technique for improving LLM factuality and grounding outputs in external documents, but does not report an implemented RAG system or experimental results within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paper references RAG as a class of methods (see cited survey 'A survey on retrieval-augmented text generation') in which a retrieval component returns relevant documents that are provided to an LM to ground generation. No implementation details are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; the referenced surveys typically consider corpora such as web text, scientific papers, or domain-specific document stores, but this paper only refers to the approach conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Used in the literature to ground generation and reduce hallucination for knowledge-intensive tasks (implied use for literature synthesis and fact-bearing summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Mentioned conceptually: retrieve relevant documents from a corpus, condition LLM generation on retrieved text (via concatenation/conditioning or fusion-in-decoder style architectures). The paper itself provides no concrete prompting templates or retrieval strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded summaries or answers that cite or reflect retrieved documents (conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper; paper points to general evaluation metrics (ROUGE/BLEU, model-based factuality metrics) used in the literature for such systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Conceptually improves factual grounding of LLM outputs and reduces hallucination when retrieval returns high-quality, relevant documents; increases traceability of generated claims back to sources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes general concerns about dataset quality and bias in retrieved sources; retrieval only helps when relevant sources exist and are correctly retrieved, and RAG systems can still hallucinate or misinterpret retrieved content.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported experimentally here; general failure modes include returning irrelevant or biased documents, misattributing information, and over-reliance on noisy retrieval leading to incorrect syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9609.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9609.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V / KOSMOS-2 map extraction example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Foundation Models (GPT-4V, KOSMOS-2) for information extraction from historical maps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper illustrates (Figure 6) that multimodal foundation models such as GPT-4V and KOSMOS-2 can extract and link textual place names from historical maps, producing lists of place names and approximate map coordinates without task-specific fine-tuning; this is presented as evidence of foundation models' capacity to extract structured knowledge from large document collections (maps), though not from scholarly papers specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V and KOSMOS-2 (as example multimodal foundation models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal large foundation models that accept image inputs together with text prompts (GPT-4V: vision-enabled GPT-4; KOSMOS-2: a multimodal grounding FM). The paper demonstrates that these models can identify place names on historical maps and produce coordinate outputs without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Single historical map image provided to the model as input along with a prompt; the demonstration is not an experiment over a corpus of scholarly papers but an illustrative multimodal extraction task.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Extract place names and output coordinates of each place present on an old historical map (map information extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct multimodal prompting of a vision-enabled foundation model (image + text prompt) to extract textual entities and coordinates. The paper notes this works 'even without task-specific fine-tuning'.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured list of extracted place names with estimated coordinates (entity extraction + geolocation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not quantitatively evaluated in this paper for the illustrated example; the authors comment qualitatively that accuracy may not always be very high and that outputs can include errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Shows strong zero-shot multimodal extraction capacity, reducing need for task-specific models or labeling; useful for linking historical artifacts to geospatial knowledge graphs or supplementary datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The paper explicitly states accuracy can be limited (not always very high) and that foundation models may produce inaccuracies or unanticipated features; also ethical issues like reproducibility and error propagation are noted for cartographic applications.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No systematic failure analysis here; authors caution about errors in coordinate generation and cases where the model misidentifies or omits place names.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A survey on retrieval-augmented text generation <em>(Rating: 2)</em></li>
                <li>A survey of large language models <em>(Rating: 2)</em></li>
                <li>Assessing the factual accuracy of generated text <em>(Rating: 2)</em></li>
                <li>Kosmos-2: Grounding multimodal large language models to the world <em>(Rating: 2)</em></li>
                <li>The mapkurator system: A complete pipeline for extracting and linking text from historical maps <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9609",
    "paper_id": "paper-264796520",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LLMs for Automated Literature Review",
            "name_full": "Large Language Models for Automated Literature Review and Scholarly Synthesis",
            "brief_description": "The paper states that LLMs can assist literature search and analytics by scanning and summarizing large volumes of text, identifying key concepts/themes, cross-referencing works, producing summaries, answering queries, and producing preliminary content or arguments for humanities domains (history, classics, philosophy, anthropology, psychology). The mention is descriptive and high-level; no new experimental pipeline or dataset is provided in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (generic; examples mentioned elsewhere in paper: GPT-3, ChatGPT, GPT-3.5, GPT-4)",
            "model_description": "Not a single bespoke architecture in this paper — the text refers broadly to transformer-based large pre-trained autoregressive LMs (GPT-family) and instruction-tuned/chat variants (ChatGPT/GPT-4) as tools that can be used to scan, summarize, cross-reference, and answer questions over large text corpora.",
            "model_size": null,
            "input_corpus_description": "Described generically as 'large volumes of text' and domain literatures (historical texts, philosophical works, anthropological documents, survey results, and other humanities corpora). No concrete corpus, selection criteria, or counts are provided in the paper.",
            "input_corpus_size": null,
            "topic_query_description": "High-level literature-review tasks: automated literature review, extraction of key concepts/themes, cross-referencing between texts, summarization, question answering about domain literature, generation of preliminary arguments or background material.",
            "distillation_method": "High-level description only: using LLM prompting to scan and summarize documents, perform cross-referencing and extraction; the paper also references related methodological work (e.g., retrieval-augmented generation surveys) but does not describe a concrete retrieval pipeline, prompt templates, chain-of-thought, or multi-stage summarization pipeline implemented by the authors.",
            "output_type": "Summaries, concise literature reviews, extracted key concepts/themes, cross-reference lists, question-answer responses, preliminary content drafts or teaching materials.",
            "output_example": null,
            "evaluation_method": "Not reported specifically for literature-synthesis applications in this paper; the paper mentions general evaluation methods for generated text (ROUGE, BLEU, and newer model-based factuality metrics) and human-in-the-loop review as common evaluation approaches in the literature.",
            "evaluation_results": null,
            "strengths": "Described strengths include speed and scalability (ability to quickly scan large volumes), multilingual handling, cross-referencing capability, and usefulness across many humanities subdomains (history, classics, philosophy, anthropology, psychology). LLMs can reduce time for manual literature triage and surface connections across texts.",
            "limitations": "The paper explicitly warns of hallucinations and factuality problems with LLM outputs, domain biases (including geopolitical and other biases), and lack of deep human-like cognition — all of which limit reliability for rigorous theory synthesis. It also notes that the paper provides no task-specific experiments or validated synthesis pipelines.",
            "failure_cases": "No concrete failure-case experiments in the paper; the paper cites general failure modes such as hallucinated facts, amplified biases in generated synthesis, inaccuracies when calibrating historical timelines or extracting precise factual claims, and the need for human expert verification.",
            "uuid": "e9609.0",
            "source_info": {
                "paper_title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG) (mention)",
            "name_full": "Retrieval-augmented Text Generation / Retrieval-Augmented Generation (RAG)",
            "brief_description": "The paper cites surveys and related work on retrieval-augmented text generation as a relevant technique for improving LLM factuality and grounding outputs in external documents, but does not report an implemented RAG system or experimental results within this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Paper references RAG as a class of methods (see cited survey 'A survey on retrieval-augmented text generation') in which a retrieval component returns relevant documents that are provided to an LM to ground generation. No implementation details are provided here.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper; the referenced surveys typically consider corpora such as web text, scientific papers, or domain-specific document stores, but this paper only refers to the approach conceptually.",
            "input_corpus_size": null,
            "topic_query_description": "Used in the literature to ground generation and reduce hallucination for knowledge-intensive tasks (implied use for literature synthesis and fact-bearing summaries).",
            "distillation_method": "Mentioned conceptually: retrieve relevant documents from a corpus, condition LLM generation on retrieved text (via concatenation/conditioning or fusion-in-decoder style architectures). The paper itself provides no concrete prompting templates or retrieval strategies.",
            "output_type": "Grounded summaries or answers that cite or reflect retrieved documents (conceptual).",
            "output_example": null,
            "evaluation_method": "Not evaluated in this paper; paper points to general evaluation metrics (ROUGE/BLEU, model-based factuality metrics) used in the literature for such systems.",
            "evaluation_results": null,
            "strengths": "Conceptually improves factual grounding of LLM outputs and reduces hallucination when retrieval returns high-quality, relevant documents; increases traceability of generated claims back to sources.",
            "limitations": "Paper notes general concerns about dataset quality and bias in retrieved sources; retrieval only helps when relevant sources exist and are correctly retrieved, and RAG systems can still hallucinate or misinterpret retrieved content.",
            "failure_cases": "Not reported experimentally here; general failure modes include returning irrelevant or biased documents, misattributing information, and over-reliance on noisy retrieval leading to incorrect syntheses.",
            "uuid": "e9609.1",
            "source_info": {
                "paper_title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4V / KOSMOS-2 map extraction example",
            "name_full": "Multimodal Foundation Models (GPT-4V, KOSMOS-2) for information extraction from historical maps",
            "brief_description": "The paper illustrates (Figure 6) that multimodal foundation models such as GPT-4V and KOSMOS-2 can extract and link textual place names from historical maps, producing lists of place names and approximate map coordinates without task-specific fine-tuning; this is presented as evidence of foundation models' capacity to extract structured knowledge from large document collections (maps), though not from scholarly papers specifically.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V and KOSMOS-2 (as example multimodal foundation models)",
            "model_description": "Multimodal large foundation models that accept image inputs together with text prompts (GPT-4V: vision-enabled GPT-4; KOSMOS-2: a multimodal grounding FM). The paper demonstrates that these models can identify place names on historical maps and produce coordinate outputs without task-specific fine-tuning.",
            "model_size": null,
            "input_corpus_description": "Single historical map image provided to the model as input along with a prompt; the demonstration is not an experiment over a corpus of scholarly papers but an illustrative multimodal extraction task.",
            "input_corpus_size": 1,
            "topic_query_description": "Extract place names and output coordinates of each place present on an old historical map (map information extraction).",
            "distillation_method": "Direct multimodal prompting of a vision-enabled foundation model (image + text prompt) to extract textual entities and coordinates. The paper notes this works 'even without task-specific fine-tuning'.",
            "output_type": "Structured list of extracted place names with estimated coordinates (entity extraction + geolocation).",
            "output_example": null,
            "evaluation_method": "Not quantitatively evaluated in this paper for the illustrated example; the authors comment qualitatively that accuracy may not always be very high and that outputs can include errors.",
            "evaluation_results": null,
            "strengths": "Shows strong zero-shot multimodal extraction capacity, reducing need for task-specific models or labeling; useful for linking historical artifacts to geospatial knowledge graphs or supplementary datasets.",
            "limitations": "The paper explicitly states accuracy can be limited (not always very high) and that foundation models may produce inaccuracies or unanticipated features; also ethical issues like reproducibility and error propagation are noted for cartographic applications.",
            "failure_cases": "No systematic failure analysis here; authors caution about errors in coordinate generation and cases where the model misidentifies or omits place names.",
            "uuid": "e9609.2",
            "source_info": {
                "paper_title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A survey on retrieval-augmented text generation",
            "rating": 2,
            "sanitized_title": "a_survey_on_retrievalaugmented_text_generation"
        },
        {
            "paper_title": "A survey of large language models",
            "rating": 2,
            "sanitized_title": "a_survey_of_large_language_models"
        },
        {
            "paper_title": "Assessing the factual accuracy of generated text",
            "rating": 2,
            "sanitized_title": "assessing_the_factual_accuracy_of_generated_text"
        },
        {
            "paper_title": "Kosmos-2: Grounding multimodal large language models to the world",
            "rating": 2,
            "sanitized_title": "kosmos2_grounding_multimodal_large_language_models_to_the_world"
        },
        {
            "paper_title": "The mapkurator system: A complete pipeline for extracting and linking text from historical maps",
            "rating": 2,
            "sanitized_title": "the_mapkurator_system_a_complete_pipeline_for_extracting_and_linking_text_from_historical_maps"
        }
    ],
    "cost": 0.01343625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities
30 Oct 2023</p>
<p>Zhengliang Liu tliu@uga.edu 
School of Computing
University of Georgia</p>
<p>Yiwei Li 
Qian Cao 
School of Computing
University of Georgia</p>
<p>Department of Geography
University of Georgia</p>
<p>Junwen Chen 
Department of Computer Science and Engineering
Michigan State University</p>
<p>Tianze Yang 
School of Computing
University of Georgia</p>
<p>Zihao Wu 
School of Computing
University of Georgia</p>
<p>John Gibbs 
Department of Theatre &amp; Film Studies
University of Georgia</p>
<p>Khaled Rasheed 
School of Computing
University of Georgia</p>
<p>Ninghao Liu ninghao.liu@uga.edu 
School of Computing
University of Georgia</p>
<p>Gengchen Mai gengchen.mai25@uga.edu 
Department of Geography
University of Georgia</p>
<p>Tianming Liu 
School of Computing
University of Georgia</p>
<p>Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities
30 Oct 202354080AA320DB02EE58FDA897B8781E4BarXiv:2310.19626v1[cs.AI]
Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities.However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human.This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities.We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art.We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies.The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity.Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing.The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.</p>
<p>Introduction</p>
<p>Arts and the humanities have long been reflections of human experience, emotions, and philosophical introspection [1].These domains, deeply rooted in subjectivity, creativity, and a nuanced appreciation of the world, have served as repositories of our history, culture, and identity.Over the past few years, however, the boundary between human creativity and machine computation has started to blur, ushering in an era where Artificial Intelligence (AI) influences artistic creation and reshapes our understanding of humanities.</p>
<p>Historically, AI's foray into domains requiring creativity was met with skepticism [2].Critics posited that machines, bound by algorithms and devoid of emotions, could never truly comprehend or replicate the intricacies of artistic expression.Creativity was, after all, seen as the antithesis of computation, fueled by irregularities, out-of-box thinking, and a delicate understanding of the human Figure 1: Some examples of AGI-generated images.Left: A heavily deep-dream-style photograph expressing "three men in a pool", which is difficult for humans to understand.Middle: An image generated by DALL-E through translation from "an illustration of a baby hedgehog in a christmas sweater walking a dog" [3].Right: Image created by DALL-E 3 with the prompt "vintage 1940s cartoon featuring a robot holding a steaming coffee mug with a lightning bolt symbol on it, text bubble that reads 'Need my charge', sitting at a table by bay window in a coffee shop interior".The model can generate the high-quality image, and correctly understand the instruction.condition.These very attributes, which are the cornerstones of arts and humanities, seemed out of reach for artificial entities.</p>
<p>More recently, the landscape has begun to shift.Early algorithms such as "Deep Dream" (2015) [4] and various approaches in the theme of "Neural Style Transfer" [5] marked AI's early attempts at artistic endeavors.However, Deep Dream was plagued by the problem of generating repetitive canine facial features within images, and the style transfer process, while artistically intriguing, lacked the ability to create entirely new content or comprehend the underlying semantics of images.Nevertheless, these earlier attempts began a noticeable shift in perceptions, with increasing acceptance of artificial intelligence's contribution to artistic endeavors.A pivotal moment highlighting this change was when "Edmond de Belamy, from La Famille de Belamy", a portrait produced by Generative Adversarial Networks (GANs) [6], was sold by Christie's New York on Oct 19, 2018 for $432,500 [7], which is more than 40 times Christie's initial estimate.Despite facing skepticism and questions regarding its originality from other artists who work with AI, these rudimentary techniques marked the nascent stages of AI-assisted artistry.</p>
<p>The leap forward came in 2021 with the arrival of text-to-image algorithms.Specifically, the introduction of DALL-E [3], supplemented by the unveiling of open-source projects like VQGAN+CLIP [8,9], catalyzed the proliferation of AI art generators.Furthermore, in 2022, the release of "Stable Diffusion" [10] by Stability AI and "Imagen" [11] by Google AI ushered in a new era of advanced AI-powered creativity.This release further democratized the Artificial Intelligence-Generated Content (AIGC) process.The field of AIGC is still extremely young.Major contributors and platforms have a relatively short operational history, spanning less than a year.However, the trajectory suggests an impending turning point where AI capabilities will become sophisticated enough to revolutionize various art-related domains.For instance, in the realm of video game development, concept and traditional artists are already harnessing AI image generation for inspiration and as tangible assets in their creative works † .Looking ahead, once the complexities of image generation are comprehensively addressed, it is plausible that the intellectual capital steering this innovation will gravitate toward other modalities.This may encompass domains like auditory processing and generation, video synthesis, and literary generation, among other multidisciplinary challenges.</p>
<p>With the recent advancement of Large Language Models (LLMs), the rise of Artificial General Intelligence (AGI) further challenges traditional perspectives.AGI [12], with its potential to emulate holistic human cognition, promises not just to create art but to understand and appreciate it-and in fact many proponents of LLMs having early AGI capabilities espouse that these models already have a degree of understanding of the physical world and humans [see references.They need to be added to citations].LLMs' integration into arts and humanities could revolutionize everything from literary synthesis, capturing the depth of human emotion, to creating multi-sensory art experiences and reinterpreting historical narratives.</p>
<p>This paper delves deep into the rapidly evolving nexus of AGI, arts, and the humanities.While celebrating the transformative potential of AGI, it also critically examines the following underlying questions: Can AGI truly be creative?Will it ever appreciate art the way humans do?And most importantly, as AGI blurs the lines between machine capability and human creativity, what does it mean for the future of arts and humanities?Through this discourse, we seek to navigate the promising yet perplexing frontier of AGI-infused artistry.</p>
<p>2 Background</p>
<p>Generative AI: From GAN to ChatGPT</p>
<p>A recent survey paper [13] provides a comprehensive review of the field of AI-generated content (AIGC).AIGC refers to content like text, images, music, and code that is generated by AI systems rather than created directly by humans.</p>
<p>The authors review the history of generative AI models, beginning with early statistical models like Hidden Markov Models and Gaussian Mixture Models.They then discuss the rise of deep learning models like GANs, VAEs, and diffusion models, with the transformer architecture (2017) identified as a key breakthrough that enabled large-scale pre-trained models like GPT-3 [14], ChatGPT [15], and GPT-4 [16].</p>
<p>The paper categorizes generative models as either unimodal, which generate content in a single modality like text or images, or multimodal, which combine multiple modalities.For unimodal models, they provide an in-depth review of state-of-the-art generative language models like GPT-3 [14], BART [17], T5 [18] and vision models such as Stable Diffusion [10] and DALL-E 2 [19].</p>
<p>For the multimodal generation, the survey examines vision-language models like DALL-E and GLIDE [20] as well as text-audio, text-graph, and text-code models.These allow cross-modal generation between modalities.The authors discuss applications like chatbots, art creation, music composition, and code generation.</p>
<p>They also cover techniques that help align model outputs with human preferences, such as reinforcement learning from human feedback as used in ChatGPT.The paper analyzes challenges around efficiency, trustworthiness, and responsible use of large AIGC models.Finally, open problems and future research directions are explored.</p>
<p>Opportunities and Challenges of General AIGC</p>
<p>The enthusiastic reception of conversational agents like ChatGPT underscores AIGC's vast potential.However, researchers must grapple with critical challenges around data bias, computational efficiency, output quality, and ethical implications as AIGC rapidly gains traction [21].</p>
<p>On the opportunities front, AIGC holds promise for boosting productivity in creative fields by acting as an intelligent assistant that can synthesize draft content.Cross-modal generation techniques can potentially bridge content formats, enabling applications like generating videos from text descriptions.In industry verticals like e-commerce, AIGC can scale the creation of catalog descriptions and customized landing pages.For news and entertainment, AIGC may enhance automation in production pipelines.The multi-task learning abilities of foundation models could spur innovation if applied judiciously.On the consumer side, AIGC can deliver more personalized, interactive, and immersive experiences.</p>
<p>However, substantial challenges remain.Massive computational resources are needed to develop and deploy the latest AIGC models [22], which may concentrate power in fewer hands.More crucially, the data used to train AIGC models inherits human biases [23] that are reflected in outputs.Curating high-quality datasets is an arduous task.While human-in-the-loop approaches may improve model alignments, transparency and accountability are still lacking.Safeguards against toxic outputs remain inadequate as interactions uncover harmful edge cases.For high-stakes domains like healthcare, the risks of errors loom large.Despite great enthusiasm, researchers should adopt a measured approach while addressing these concerns through technical and ethical diligence.</p>
<p>While AIGC represents an exciting frontier for AI research with immense potential upside, responsible development calls for holistic solutions encompassing data curation and hygiene, efficient systems, user feedback loops, and transparency.With care and consideration for societal impacts, AIGC could usher in an era where generative AI assists and augments human creativity rather than displacing it.This survey provides a timely overview of the state-of-the-art as of late 2023, and a roadmap to guide progress in this rapidly evolving domain.</p>
<p>Text Analysis and Generation</p>
<p>Text analysis and generation are crucial domains in natural language processing influencing myriad applications.At the core, text analysis delves into comprehending intricate patterns, meanings, and sentiments in textual data, whereas text generation aspires to craft human-like text based on certain criteria or prompts [15].With the advent of sophisticated model architectures, the boundaries of what machines can comprehend and produce have been ceaselessly expanded.This section delineates the technical advancements underpinning these capabilities, including seminal models like Transformers, and extends into their pragmatic applications across diverse sectors such as poetry, music, law, advertising, and governance.</p>
<p>Technical Advances</p>
<p>The Transformer [24] architecture has undoubtedly carved a pivotal role in the progression of natural language processing models.Introduced by Vaswani et al. [24], the architecture abandoned recurrent layers, traditionally used for sequence data, in favor of attention mechanisms.The core concepts emanating from this architecture, including encoders, decoders, BERT, and autoregressive language models, have since dominated state-of-the-art results in various NLP tasks.</p>
<p>Transformer Architectures</p>
<p>At the heart of the Transformer model lies the pivotal self-attention mechanism, which computes a weighted sum of all words in a sequence relative to each other.This empowers the model to capture the intricate relationships between words, regardless of their positions in the sequence.Unlike recurrent models such as RNNs [25] or LSTMs [26] which process sequences iteratively, Transformers handle the entire sequence in parallel.This approach, coupled with additional design elements like positional encodings [27] and residual connections [28], empowers Transformers to deliver both efficiency and effectiveness, even when confronted with lengthy sequences.</p>
<p>BERT (Bidirectional Encoder Representations from Transformers)</p>
<p>Emerging from the Transformer paradigm, BERT [29] represents a monumental shift in pre-training methods.Introduced by Google in 2018, this model captures bidirectional contexts by considering both preceding and following words in all its layers.BERT's pre-training phase involves a masked language model objective wherein it attempts to predict randomly masked words in a sentence.Once pre-trained on vast corpora such as Wikipedia, BERT can be adeptly fine-tuned on specific tasks using small labeled datasets, by just adding appropriate task-specific layers [30].This approach has made BERT highly versatile, allowing it to be applied to a wide range of NLP tasks.</p>
<p>Autoregressive Language Models</p>
<p>Autoregressive modeling [14] uses a step-by-step approach, where predicting the next item in a sequence depends on what came before it.In the context of language modeling, when given part of a sentence, these models try to guess what words come next.Once properly trained, the models good at guessing what word comes after the previous ones in the sequence.When autoregressive models generate text, they can use different methods, like beam search [31], greedy decoding [32], or probabilistic sampling [33].A well-known example of an autoregressive language model is OpenAI's GPT (Generative Pre-trained Transformer) [34], which, in contrast to BERT's bidirectionality, is unidirectional and is primed mainly for text generation.</p>
<p>Real-world Applications</p>
<p>This section elucidates the diverse arts and humanities landscape of AGI by categorizing its applications into four distinct but interrelated subsections: 1) Literature Search and Analytics; 2) Linguistics and Communication; 3) Creative Endeavors.Each of these subsections represents a unique facet of AI's ever-expanding repertoire, showcasing its adaptability to perform tasks ranging from the systematic retrieval of knowledge to nuanced linguistic interactions, from meticulous analytics to imaginative, creative endeavors.</p>
<p>Literature Search and Analytics</p>
<p>Large language models, once equipped with common sense knowledge, can provide valuable assistance in various liberal arts domains such as history, classics, and philosophy that heavily rely on literature search and analysis.Based on [35], LLMs can be helpful in the following aspects.</p>
<p>• Automated Literature Review: LLMs can quickly scan and summarize large volumes of text.They can identify key concepts, themes, and relevant passages, saving researchers significant time and effort.</p>
<p>• Cross-Referencing: LLMs can cross-reference texts, identifying connections and references between historical events, philosophical works, and classic literature, helping researchers explore intertextual relationships.</p>
<p>• Summarization: LLMs can generate concise summaries of lengthy texts, making complex philosophical or historical writings more accessible to a broader audience.• Question Answering: LLMs excel in answering specific questions related to historical events, philosophical theories, or classic literature.They can provide concise and accurate responses by drawing on their vast knowledge base.</p>
<p>• Content Generation: LLMs can assist in generating preliminary content by retrieving relevant information in the literature [36].They can provide background information, context, and even propose arguments based on the input provided.</p>
<p>• Teaching and Learning: LLMs can be used as educational tools to provide explanations, generate practice questions, and engage students in discussions related to historical, classic, or philosophical topics.</p>
<p>Some specific examples of the ongoing and potential applications are provided as below.</p>
<p>Anthropology: LLMs can process large amounts of anthropological data.Through text mining, LLMs deeply research documents, interviews, and historical texts to find information on specific cultures, social groups, or topics, and promote a deeper understanding of human social evolution and cultural differences [15].Second, LLMs can help analyze social surveys and public opinion polls, to gain an in-depth understanding of attitudes, beliefs, and behaviors in human society, helping researchers understand social trends and changes in public opinion [37].Finally, through cross-cultural research, LLMs support the comparison of similarities and differences between different cultures, provide translation services, analyze cross-cultural communication, and conduct in-depth research on global issues such as globalization and cultural exchanges.</p>
<p>Classics: LLMs contribute to art historical research, providing in-depth insights into a specific period or style by analyzing textual descriptions of classical artworks, historical documents, and the lives of relevant artists.In terms of art education and popularization, LLMs can assist in the creation of art education materials, explain works of art so that more people can understand and appreciate classical art, and generate explanatory texts on art history for use in education, museum exhibitions, and cultural dissemination [38].</p>
<p>Philosophy: LLMs can be used for literature reviews to help researchers understand the current state of research on specific philosophical issues or thinkers [39].They can also analyze philosophical texts and understand the author's ideas, argument structure, and logic.In addition, LLMs can also be used to analyze the structure and effectiveness of philosophical arguments, helping researchers better understand and evaluate philosophical papers.</p>
<p>Psychology: LLMs can conduct literature reviews to help researchers understand the latest research and theories on specific psychological topics [40].LLMs can also generate questionnaire questions to ensure they are clear and effective.In addition, LLMs can analyze comments on social media to understand emotional health and mental health issues of people, and provide support and resources.</p>
<p>History: LLMs can analyze historical texts to help understand events, generate summaries, extract key information, and improve information processing efficiency [41].LLMs can also calibrate time in historical text, track events, and help establish a detailed historical timeline.They can also help extract character relationships, help build a relationship map, and conduct in-depth research on the influence of historical figures.</p>
<p>Linguistics and Communication</p>
<p>LLMs can be highly beneficial in applications related to linguistics and communications due to their natural language processing capabilities and extensive knowledge base.An example that shows some these abilities is in Figure 3.</p>
<p>• Language Understanding: LLMs can be used to analyze the structure, grammar, and semantics of languages, aiding linguists in their research on syntax, morphology, and linguistic phenomena.</p>
<p>• Translation Assistance: LLMs can assist linguists and translators in translating text between different languages, helping bridge linguistic and cultural gaps.</p>
<p>• Sentiment Analysis: LLMs can perform sentiment analysis on text, enabling businesses and organizations to gauge public sentiment towards their products, services, or policies.</p>
<p>• Speech Recognition: LLMs can enhance speech recognition systems, improving the accuracy of voice-to-text transcriptions.</p>
<p>Based on the above capabilities, some specific examples of the ongoing and potential applications of LLMs are provided as below.</p>
<p>Linguistics: LLMs can generate new language texts and expand understanding of grammatical structures and vocabulary usage [42,43].Scientists can conduct semantic analysis through LLMs and conduct in-depth studies of lexical meanings, contextual relevance [44], and semantic relationships of language expressions [39].These models can also be used to develop language learning tools to help students learn vocabulary, grammar rules, and other language knowledge.In studying language disorders, LLMs can reveal the manifestations and effects of language disorders in different contexts.However, some scholars have raised objections, believing that LLMs lack human cognition [45].</p>
<p>Language Studies: LLMs can analyze the grammatical, semantic, and pragmatic features of different languages.Based on this, LLMs can generate teaching materials and exercises with explanations to provide students with strong support in learning grammar, vocabulary, and expressions.</p>
<p>In addition, LLMs excel in translation, supporting cross-language communication and translation [29].At the same time, LLMs play an important role in writing and creation and can create articles, compose essays, and generate various literary works.In addition, LLMs support speech recognition technology, which allows speech input to be easily converted into text, facilitating speech interaction and speech recognition applications [38].By processing large amounts of historical texts, LLMs can also assist researchers in tracing the evolution, change, and development of the language.</p>
<p>Communitcation Studies: LLMs can analyze large amounts of news and advertising, to reveal patterns, trends, and factors that influence the spread of information.LLMs can also analyze emotions and interactions on social media [46], studying how information spreads in social networks and its impact on public opinion [47].Researchers use LLMs to analyze the language and framing of news reports and study the way news media report events and their impact on audience perceptions.</p>
<p>In terms of multilingual content, LLMs have the advantages of translation and understanding and are helpful in studying language differences and cultural factors in cross-cultural communication.</p>
<p>Creative Endeavors</p>
<p>This subsection starts with examples of several applications where AI might models generate "creative" contents, followed by a further discussion of whether AI can genuinely attain a level of creativity comparable to that of humans.</p>
<p>Song Lyrics: LLMs such as the GPT family can can write song lyrics that "tell coherent stories with rhyming words" † .Based on that, the AI models could be used to create new melodies to accompany the lyrics.GPT-4 is significantly better than GPT-3.5 at this due to better reasoning, complex instruction understanding, and creativity.</p>
<p>Poetry: There have been some research work on intelligent poetry writing and intelligent couplets [48].However, the continuous development of LLMs has greatly facilitated research in this area.Figure 4 shows an example of using GPT-4 to write poems.Besides, a website † shows the procedure to write Advertising: The creation of effective and creative advertisements is a collaborative process that engages professionals with diverse skills and roles.Nonetheless, it is conceivable that certain roles may be assumed by Large Language Models (LLMs) in the future.LLMs can help advertisers and marketers in creating content faster and potentially with quality akin to that of human content creators (see Figure 4).Moreover, given the abundance of successful advertising case studies available for reference in the field, LLMs with strong transfer capabilities such as GPT-4 can further improve the accuracy of advertising word generation through multi-shots to achieve the results desired by users [49].LLMs can also analyze the promotional trends across a broad spectrum of advertisements, which enables conducting more efficient research, gaining deeper understanding of customer preferences, and addressing the complexities tied to information summarization [50].</p>
<p>With the rapid development of AI models, a question arises: Will AI eventually replace human creativity, or will humans continue to be the paramount source of innovation and originality?A brief creativity comparison between humans and AI is as follows.</p>
<p>• Human creativity is influenced by personal experiences, emotions, and imagination, while it has limitations in terms of time, resources, knowledge, and experience, in addition to external factors like societal and economic influences.</p>
<p>• AI creativity is primarily grounded in algorithms and data, so a dominant view is that AI can only work with previous data and patterns, and cannot come up with entirely novel ideas on its own.Moreover, AI's deficiency in emotion and empathy poses another restriction.It is unable to replicate human emotions or grasp the emotional depth that art or music carries, potentially resulting in AI-generated content lacking the profound emotional impact typically attributed to human creativity.</p>
<p>types for graphic generation and analysis can also vary, ranging from images, text, and even other multidimensional data sources.</p>
<p>Technical Advances</p>
<p>There are numerous technical advancements that have propelled the fields of graphics analysis and generation to new heights.</p>
<p>Generative Adversarial Networks (GANs)</p>
<p>In the mid 2010's, GANs [51] ushered in a new era in the field of image generation.At the heart of a GAN framework are two intertwined neural networks: the generator and the discriminator.</p>
<p>The generator creates images either from random noise in the case of unconditional GANs [6] or guided by text/categories for conditional GANs [52].Concurrently, the discriminator evaluates these generated images against real images.Through iterative refinement and adversarial training within a minimax game framework, the generator refines its outputs, aiming to create images indistinguishable from real ones while the discriminator learns to be an increasingly better judge of real versus AI-created images.This adversarial process has led to the generation of exceptionally high-quality and realistic images, significantly surpassing previous methods such as autoregressive models, Variational Autoencoder [53], and normalizing flows [54].Moreover, the versatility of the GAN framework has extended beyond traditional imagery modality to other graphics formats such as 2D/3D point clouds [55,56,57], graphs [58], 3D object shapes [59], and so on.</p>
<p>Style Transfer Techniques</p>
<p>Neural style transfer [60] has emerged as a captivating application of deep learning in graphics.By leveraging the intricate structures within neural networks, style transfer algorithms can take the artistic style from one image and apply it to another, enabling the creation of unique, artistically rendered outputs.</p>
<p>Generative Models for 2D Images</p>
<p>GANs excel in producing high-quality 2D images, often to the point of being indistinguishable from real photographs.Additionally, Variational Autoencoders (VAEs) offer a probabilistic framework to generate 2D images [61] while capturing the underlying data distribution.Both models can utilize inputs such as noise vectors, existing images, or textual descriptions to guide the generation process.</p>
<p>A seminal work in this domain is alignDRAW [62], which generates captions for images based on VAE and an attention mechanism.</p>
<p>Generative Models for 3D Images and Point Clouds</p>
<p>GANs and VAEs have been extended to generate 3D voxel grids or point cloud representations [55,56,57].Moreover, models like PointGAN [55] focus specifically on generating high-quality point cloud data, capturing intricate 3D structures.Inputs for these models can range from 2D projections, textual descriptions, or even other 3D structures for tasks like super-resolution in 3D space.</p>
<p>Generative Models for Designs</p>
<p>Design generation, especially for aspects like logos, user interfaces, or architectural layouts, has seen innovation through models like CreativeGAN [63].These models can take inputs in the form of design constraints, user preferences, or textual descriptions to generate design mockups.The produced designs can be static (like a logo) or dynamic (like an interactive UI prototype).</p>
<p>Static vs. Dynamic Generation</p>
<p>While many generative models focus on producing static outputs, there's a growing interest in dynamic content generation, especially in domains like video synthesis or interactive designs.Recurrent neural networks (RNNs), especially the Long Short-Term Memory (LSTM) networks, combined with GANs (like VideoGAN [64]), as well as the recent video transformer [65,66] have made strides in generating video sequences.This aligns with the broader trend of moving from static images to dynamic, time-evolving sequences in synthetic media.We will discuss this in detail in Section 5.1.</p>
<p>Diverse Input Types</p>
<p>A hallmark of modern generative models is their ability to handle a variety of input types.While noise vectors remain a staple, there is a growing trend of models using textual descriptions to guide synthesis, allowing for more controlled and descriptive generation.This has been evident in models like AttnGAN [67] and Df-GAN [68], where textual descriptions can guide the fine details of image synthesis, ensuring alignment between described content and the generated image.</p>
<p>Diffusion Models</p>
<p>Diffusion Models (DMs) [69,70,71,72] are innovative techniques conceptually inspired by nonequilibrium thermodynamics [69].These models progressively introduce Gaussian noise during the forward (diffusion) process and subsequently learn to reverse the diffusion process to reconstruct the image from noise by predicting the previously added noise and then denoising.This unique approach has made them one of the best at synthesizing images and more.One great feature of these models is that they can be directed or controlled in how they generate images without the need for extensive retraining.</p>
<p>Denoising Diffusion Probabilistic Models: However, the Denoising Diffusion Probabilistic Models (DDPMs) [70], as one of the pioneering works in diffusion models, have a drawback -given the fact that both the diffusion forward process and the denoising reverse process in DDPMs involve long Markov chains which consist of thousands of steps and DDPMs generally work directly with the individual pixels of an image, they usually require a tremendous amount of computational power and time for both model training and image sampling.In fact, optimizing these models to their best performance can take hundreds of days using powerful graphics processing units (GPUs), and using them can also be costly in terms of resources.</p>
<p>Denoising Diffusion Implicit Models: Consequently, to tackle the low sampling speed issue, Denoising Diffusion Implicit Models (DDIMs) [73] was proposed as fast sampling diffusion models closely related to DDPMs.DDIMs maintain the same marginal noise distributions as DDPM but diverge with a non-Markovian diffusion process and deterministically map noise to images.Consequently, DDIMs can generate high-quality images while significantly reducing the generation steps from 1000 in DDPM to just 50.</p>
<p>Conditional Diffusion Models:</p>
<p>In addition to the aforementioned unconditional diffusion models, researchers have developed DMs that are conditioned on additional inputs such as class labels, reference images, or text sequences [74,75,76,10] to better guide the generation process.</p>
<p>Latent Diffusion Models: To make DMs more efficient without sacrificing their performance, researchers have also started training DMs by using the underlying structures or "latent spaces" of already trained models, known as autoencoders [77].This approach reduces the computational burden of the process while still retaining the important details that make the images look realistic.</p>
<p>Stable Diffusion: Latent Diffusion Models (LDMs) [10] have been instrumental in advancing the domain of image synthesis.These models incorporate the robust synthesis capabilities inherent to traditional DMs but with an added advantage: the flexibility of operating in latent space.This transition to latent space doesn't just add flexibility, it also introduces a remarkable equilibrium.The LDMs are designed to minimize model complexity without compromising the richness of image details.As a result, there is a noteworthy improvement in visual fidelity in these models versus pixel-based DMs, making output images sharper and more true-to-life.One of the standout features introduced to LDMs is the integration of cross-attention layers.This inclusion is not merely a technical enhancement but a transformation in adaptability.With these layers, LDMs are equipped to handle a diverse range of conditioning inputs.Whether it is textual data or bounding boxes, the model processes them with equal proficiency.This versatility is pivotal, especially when high-resolution image synthesis is the goal.LDMs have shown the capability to generate these detailed images using a convolutional approach, offering a blend of clarity and detail that was until recently very challenging to achieve.Another advantage of LDMs is the low computational overhead.One of the pressing challenges in image modeling has always been the computational demands, especially with pixel-based DMs that tend to be resource-intensive.LDMs present a solution to this long-standing problem.Despite their advanced features and superior performance, they operate with a significantly reduced computational overhead.This efficiency ensures that high-quality image synthesis is not just the domain of those with vast computational resources but is accessible to a broader spectrum of researchers and practitioners using small GPU clusters or even desktop or mobile devices.</p>
<p>Real-world Applications</p>
<p>Recent advancements in generative AI, particularly in image models, have gained significant popularity not only in research but also in real-world applications.An increasing presence of AI-generated content (AIGC) can be observed in websites, advertisements, posters, and magazines.These models have the capability to generate diverse yet coherent graphics from cartoon illustrations to realistic photographs, eliciting interest across various industries.Figure 5 illustrates the trending popularity of renowned generative AI tools over the past year, indicating a promising future for their real-world applications.</p>
<p>Cartography and Mapping</p>
<p>As the field of studying, designing, and using maps, cartography is considered a discipline that encompasses both art and science by many cartographers [78].Cartography includes various important scientific questions such as map projection [79,80,81], map generalization [82,83,84], building pattern recognition [85,86,87], drainage pattern classification [88], and so on.Because of the nature of cartography, most of these tasks require an AI model to manipulate or generate geospatial vector data (e.g., points, polylines, and polygons) [89,90,91].Although there are multiple existing foundation models, most of them are unable to handle this kind of vector data which makes these foundation models inapplicable for various cartography tasks [92,93].However, there are also various important cartography tasks that current foundation models are able to handle such as historical map data extraction.For example, various multimodal foundation models such as KOSMOS-2 [94] and GPT-4V [16] can be used for extracting and linking text from a historical map  of Georgia [95,96].Figure 6 shows one illustrative example and the response from GPT-4V.It's evident that even without task-specific fine-tuning, GPT-4V can identify various place names from maps.Additionally, although the accuracy may not always be very high, GPT-4V can generate map coordinates for these places.Moreover, foundation models can be also used for map reading and map-based question answering for topographic maps, thematic maps, or even narrative maps [97].Despite these success stories, applying foundation models and AIGC on cartographic applications can also lead to ethical issues such as inaccuracies, unanticipated features, and reproducibility [98].So the pros and cons of foundation models on cartographic applications need to be investigated further.</p>
<p>Environmental Design</p>
<p>AIGC, especially text-to-image generation, provides valuable tools for designers.These technologies can offer inspiration and improve workflow efficiency in the field of environmental design, including landscape architecture [99,100], urban design [101,102], architecture [103,104], and interior design [105,106].In the initial design phase, AI sparks inspiration by generating diverse intentional images in various styles.It is particularly imaginative in the generation of special-shaped buildings [107].Providing diverse reference styles also helps to confirm the tone and style of the work.In the design Figure 7: An example of AI's suggestion for architecture design (Generated by LLaVA) review stage, AI can perform rapid partial replacement, helping designers to clarify the replacement effect and improve the speed of modification.Take architecture as an example [108].Designers can compare the effects of different surface materials, body proportions, and facade details with AI.In design analysis, AIGC's ability to generate images with multiple perspectives and scales supports designers in producing analysis diagrams such as streamlining analysis and functional partitioning.Finally, after the design plan is finalized, AI can accelerate rendering and offer dimension choices, like spatial scale, weather, and night scenes.Since environmental design is a graph-oriented industry, the application of newly emerging multi-modal foundation models (FMs) in this field is in a more auxiliary position compared to text-to-image generative AI.Multi-modal FMs can assist designers in understanding statistic diagrams and then enhance scientific support for designs.They can also identify and illustrate images, including remote sensing images, architecture, and interior photos, which can be used for case studies and style reference.They can even evaluate design works and give suggestions for improvement.Figure 7 shows an example of LLaVA's recommendations for architecture design work.In this example, LLaVA extracts several building features like windows, balcony, garden, and roof from a photo of an architectural model, as well as information from the prompt to offer advice.This example proves LLaVA's capacity to analyze architecture functionally, although it has not shown insights into aesthetic and social meanings, which remain the exclusive domain of architects.</p>
<p>Photography and Editing</p>
<p>Foundation models and AIGC have transformative forces to improve image quality and image editing efficiency, and even extend the domain of photography to artificially generated "photographs."In terms of image quality, AIGC can be used for refining and upscaling historical and/or low-resolutions photos with the so-called image restoration [109,110] and image superresolution [111,112] capability.Furthermore, AIGC's capacity to eliminate reflections saves a lot of photos ruined by reflections from glasses.When it comes to enhancing photo editing efficiency, foundation models shine in various aspects.Firstly, FMs such as SAM [113] excel in objective segmentations without any model finetuning.Secondly, with the so-called image inpainting [10,109,114] ability, FMs can be applied to remove the recognized objects and automatically replace the target area with a coherent background.Figure 8 shows one example with Adobe Firefly in which the background of University of Georgia's Arch is changed from summer to autumn style.FMs can be also used to generate the missing part of an object when this object is blocked or outside of the image scene.In addition, FMs can change the characteristics of these objects, including color, texture, and even style, which used to be a time-consuming task.FMs can also generate entirely new objects from text prompts or scribbles [115] which perfectly fits the light conditions and angle of the photo.Finally, Diffusion Models (DMs) are excellent at creating photorealistic images from text prompts or other images [116].This synthetic image generation is a step-change in creative photography, and even calls into question the traditional definition of photography, which has traditionally been associated with recording photons onto analogue or digital recording devices (e.g., chemicals on a plastic sheet or CMOS image sensors).Figure 9 illustrates three synthetic photographies generated by three widely used generative diffusion models.</p>
<p>Illustration</p>
<p>While currently, it cannot entirely replace professional illustrators, AIGC significantly aids the initial conceptualization stage [117], much like its role in environmental design.AIGC's rapid iterations from inspiration to finished drawing allow illustrators to quickly determine the composition, elements, and style of a painting.Since AI has significantly reduced the difficulty of illustration, in situations where super-precise drawings are not needed, the images AIGC generates can even be directly applied to illustrated books, comics, and print advertisements.Illustrators use LLMs to give instruction for storyboards, character design, and painting style, then employ multi-modal FMs to generate complete illustrations [118], that have consistency in scenes, characters, and style.With the help of image editing tools, some of which are also powered by AIGC, typesetting work can also be</p>
<p>Graphic Design</p>
<p>AIGC has a wealth of applications in graphic design, including logo design, print advertising, product packaging design, and mock-ups [119].AI-generated logos can be suitable for printing or can be used in richer scenarios such as storefront signs and building facades after fine-tuning by the designer.The production process of print advertisements with AI is close to that of illustrations.These tools' fast, low-cost, and uniform style has made them favored tools among print advertisers.AIGC also has a role in product packaging design and mock-ups, where it can generate packaging for a series of products under the same subject, and provide a variety of usage scenarios for products.These processes replace traditional photography or rendering, greatly reducing time and cost expenditures.</p>
<p>Font Design</p>
<p>AI provides a rapid and easy way to conduct font design.Supplying AI with letter references, whether in vector form or rough hand-drawn sketches, the AIGC models are capable of comprehending their unique style and seamlessly adapting it by maintaining harmonious counters and bodies.These refined letter forms can then be seamlessly integrated into existing texts [120].In addition to learning variables in type design, AIGC also treats references as graphs, considering elements such as color, texture, shading, reflection, glow, or other effects [121].Therefore, these graphic features can be transferred to letters.Moreover, natural language also provides enough information to design new fonts.Figure 10 is an example of art font design accomplished by Adobe Firefly.Both texture and shape are successfully generated according to the prompt, although there are some imperfections around the edges.</p>
<p>3D Design</p>
<p>3D design plays a pivotal role in the animation, video game, and film industries.The application of AIGC and FMs in this domain unfolds into two categories.One use case is text-to-3D, an extension of image generation [122].Just as text-to-image generative models, FMs can be used to generate 3D Figure 10: An example of font design generated by Adobe Firefly with the prompt "pink hawaiian hibiscus flowers and leaves realistic, and the shapes of flowers and leaves can be out of letters" and the text "Arts and Humanities".models based on text prompts.This can be applied in the prototyping of scenes and characters, enriching the creative process.The other use case is 3D model manipulation.Given a 3D model, AI can adjust its posture automatically according to reference pictures or user instructions instead of manually adjusting joint positions [123].This feature caters not only to professional designers but also fosters accessibility for novice users in 3D model creation.Moreover, the surface of 3D models can be also generated by text prompts.Combined with image generation models, AI-enhanced 3D models improve the efficiency of 3D character generation, scene rendering, and even product design.</p>
<p>Fine Art</p>
<p>Perhaps most divisively, AI-based image generators can be used to create works traditionally associated with fine art [124], or art with no purpose but to amaze and please its audience.Fine art painting and photography are considered the epitome of human skill and creativity, yet AIGC, specifically in the form of diffusion models (DMs), has created work that many consider on the level of highly skilled photographs and paintings.Needless to say, many practitioners and critics state that DMs cannot now or ever replace human creativity and skill.At present there is no clear answer to the question of whether AI, or AI in combination with a human, will be able to create work on the level of the highest human artistic achievements, but this is an area that should be watched in the coming months and years.</p>
<p>Evolutionary Creativity</p>
<p>Evolutionary art and evolutionary music are innovative fields of generative AI [125].They belong to the general field of evolutionary creativity and leverage Evolutionary Computation to generate esthetically pleasing visual arts or music.Evolutionary computation [126] is a collection of methods based on the principles of Darwinian Evolution.They simulate a population of solutions evolving over time through operations of selection, mutation, and recombination, better solutions are found.The field of evolutionary creativity includes multiple approaches which could be divided into humanin-the-loop approaches where the evolutionary algorithm generates art or music and a human either assigns a score (fitness) or compares different pieces of art or music and picks the best.Other approaches rely on an objective measure of merit based on some rules of thumb in music composition for example.</p>
<p>Video and Audio Analysis and Generation</p>
<p>Technical Advances</p>
<p>Video content (including audio) is a predominant form of information consumption and communication in the digital age.With the exponential growth in video data, there arises an acute need for Replacing the flying man with a blue bird.</p>
<p>A panda dancing like crazy on the Time Square.effective video analysis and generation tools powered by artificial intelligence (AI).The following section delves into the technical advances in the domain of video analysis and generation.</p>
<p>Early Approaches</p>
<p>Generative adversarial networks (GANs) were first applied to generate simple synthetic videos.Models like TiVGAN [127] and MoCoGAN [128] pioneered GAN-based video generation.However, these early GAN models were limited to generating short, low-resolution videos focused on specific domains like human actions.The quality and diversity were lacking.</p>
<p>Autoregressive Models</p>
<p>Compared to GANs, autoregressive models can model density explicitly and conduct stable training, thus they are widely used in visual synthesis.Autoregressive models [129,130,131,132] tried to generate higher-resolution videos by modeling pixel distributions sequentially.But they were slow and hard to scale up.</p>
<p>Diffusion Models</p>
<p>As with still images, Diffusion Models have become very popular for high-quality image generation.Video Diffusion Models (VDM) [133] extended image diffusion models to the video domain by training on both images and videos.Imagen Video [116] built a cascade of VDMs to generate longer, high-resolution videos.However, it requires large-scale training and latent optimization.Tune-A-Video [134] optimized the latent space of a diffusion model on a single reference video to adapt it for video generation.This reduced training but still requires optimization.A recent study by Text2Video-Zero [135] proposes a zero-shot text-to-video approach without any training on video data.It leverages a pre-trained text-to-image diffusion model and modifies it with motion dynamics in latent space for background consistency and cross-frame attention to preserve foreground details.This allows high-quality video generation from text without costly training.It also enables applications like controlled/specialized video generation and text-driven video editing.Ablations show the contributions of the modifications for temporal consistency.The zero-shot ability and lack of training are advantages over prior techniques.</p>
<p>Real-world Applications</p>
<p>Film Industry</p>
<p>New AI technologies such as LLMs or Multi-Modal FMs have the potential to revolutionize the film industry at different stages of the movie-making process [136,137].First, LLMs can analyze the draft scripts and generate unique storylines [138], which help filmmakers write and revise scripts more efficiently [139].In addition to scriptwriting, LLMs also can simplify the movie pre-production process [140,141,142].Specifically, they can make shooting schedules, find exterior film locations and props, speed up casting person search, and estimate the success and potential revenues the film may earn.Second, LLMs can generate instructions for technical staff during filming [143,144], including lighting, shot prediction, audio recording, etc. LLMs are capable of identifying the director's personalized filming style and thus can generate filming instructions specific to the director's style.Third, Multi-modal LLMs serve as a good editing tool in post-production.LLMs can synthesize multiple clips and even create special effects based on the scripts [145].They can also generate trailers and synopses for promotion purposes [143,138].LLM-based music composition tools can also be used to find or create an Original Sound Track (OST) that adapts to the movie plot.</p>
<p>Social Media</p>
<p>Increasingly, social media is shifting towards video content over text-based posts.From Podcasts to short-form videos (e.g., TikTok) to longer-form user-generated content (e.g.YouTube), users both create and consume video content at ever-growing rates.AI and AIGC are in the early stages of disrupting this industry, but in the near future, this disruption is likely to grow rapidly.One of the most interesting nascent applications is in high-quality AI-based language translation.Several startups have popped up recently that ingest video in a given language (e.g., English) and reproduce it in any number of output languages (e.g., Spanish or Mandarin).The output video can match the original creator's voice characteristics and even make the lips move as if the creator natively speaks the output language † .</p>
<p>Journalism and Communications</p>
<p>As illustrated in Sec 2.2, LLMs can analyze large amounts of textual data, including news, social media, and advertisements.Researchers can use LLMs to study how information spreads and impacts the public [146].Video is also an important modality in communication.Nowadays, short usergenerated videos are gaining popularity, alongside traditional media like TV and newspapers [147].The multi-modal FMs have the advantage of analyzing vast amounts of news data in different modalities, including a large quantity of information uploaded by the public.This helps researchers understand how information spreads in the network and track the personal behavior of each user.</p>
<p>Music Analysis</p>
<p>Multi-modal LLMs have been proposed to empower frozen LLMs with the capability of understanding both visual and auditory content in videos [148].Multi-modal FMs can perceive the gestures and movements of the music performers in a video [149,150,151,152,153,154], for example, fingering analysis on piano.Based on the visual perception, the FMs can further understand the content, emotion, and intention of the performance [155,156,157] and reveal the cultural characteristics.The visual understanding provided by FMs helps musicians improve their performance and composition † skills [158].In addition to audio analysis, these models can help with the generation of music.Diffusion Models (DMs) have been repurposed from images to audio recently, allowing for original musical creations based on text input.In a similar fashion to how a user can interact with an image-based DM to request a given image, a user can also type in a textual description of a requested audio composition and get a sound file based on this description.</p>
<p>Responsible AGI</p>
<p>Is AI Threatening Humanity?The popularity of AI-generated content, spanning writing, photography, art, and music, has surged dramatically.However, this meteoric rise has also sparked significant backlash, with some people rejecting AI-generated art and even asserting that its widespread adoption signals potential concerns for humanity.The question of whether AI is threatening humanity is a complex and debated topic.For example, AI itself is a tool created and controlled by humans, which could automate tedious tasks for humans but could also cause job displacement to human society; AI could generate art works efficiently, but they could not serve as a deeper communicative medium of human experience [159]; AI could improve healthcare but could also pose threats to public safety.Some essential components of responsible AGI are discussed as below.</p>
<p>Factuality</p>
<p>Large language models are susceptible to hallucinations [160], wherein they may produce content that includes non-factual information or deviates from established world knowledge [161].This poses challenges in numerous applications, such as legal research and historical studies where factual accuracy is crucial.In addition to natural language processing, factuality-related concerns also extend to the field of computer vision.A typical challenge arises in the form of generative models, such as stable diffusion, struggling to accurately generate realistic human hands with the correct number of fingers [162] as well as remote sensing images with correct geographic layout [93].Non-realistic AI-generated images or videos may pose challenges in engaging viewers emotionally or intellectually compared to traditional ones.</p>
<p>Common strategies to tackle the above issues include factuality evaluation and generation regularization.For factuality evaluation in generated content, several typical methods stand out.ROUGE [163] offers a metric that evaluates the quality of computer-generated summaries by measuring their overlap with human-created reference summaries in terms of n-grams, word sequences, and word pairs.Similarly, BLEU [164] provides an automatic machine translation evaluation technique renowned for its high correlation with human evaluations, positioning it as a swift and efficient alternative to more labor-intensive human assessments.In a more recent development, a model-based metric [165] has been introduced, specifically designed to assess the factual accuracy of the generated text, further enhancing and complementing the capabilities of traditional methods like ROUGE [163] and BLEU [164].For generation regularization, "Truthful AI" [166] is proposed to focus on enhancing the integrity and accuracy of AI-generated outputs.By setting rigorous standards, the initiative seeks to prevent "negligent falsehoods", achieved through selected datasets and close human-AI interaction, aligning with societal norms and legal constraints.</p>
<p>Public Safety</p>
<p>Despite the rapid advancement of generative AI technology, such as ChatGPT and Midjourney, which can generate human-like texts, images, and videos, it also raises critical concerns related to public safety, encompassing issues of privacy, cybersecurity, national security, individual harassment, and the potential for machine misuse [167,168].</p>
<p>• Misinformation.AIGC such as texts, images, and videos can be used to create and spread false or misleading information, leading to public confusion, panic, or harm [169,170].For example, Midjourney can accept prompts like "a hyper-realistic photograph of a man putting election ballots into a box in Phoenix, Arizona", and produce high-quality images that could be used to support the news [171].The issue is particularly concerning in areas like public health, elections, and emergencies.In addition, AI-generated deep fake images [172,173] and videos [174] can impersonate individuals, including public figures, and spread false or defamatory content.Meanwhile, they can be used to invade individuals' privacy by creating content without their consent, leading to serious ethical and legal implications.Repeated exposure to deceptive AI-generated content can damage reputations, incite social unrest, and erode public trust in authorities.Moreover, the National Geospatial-Intelligence Agency (NGA) also alarmed us with the risk of deep fake satellite images from generative AI being used as a terrifying AI-powered weapon [175,176].</p>
<p>• Phishing.Phishing is a type of cyber-attack where attackers attempt to deceive individuals into revealing sensitive or personal information such as login credentials, credit card numbers, or personal information.AI can be used in various ways to enhance phishing campaigns.</p>
<p>(i) Spear Phishing is a targeted cyber-attack approach that uses personalized details to trick individuals into revealing confidential information [177].Modern LLMs have the ability to produce convincing human-like texts, which can be used to create personalized spear phishing messages on a large scale and at a low cost.For instance, using advanced models like Anthropic's Claude, a hacker can easily generate 1,000 spear phishing emails for just $10 in less than two hours [178].</p>
<p>(ii) AI voice cloning is another noteworthy technology, as nowadays only a short voice sample is needed to create a realistic imitation.For instance, Google's AI system can mimic someone's voice with just a five-second sample [179].This technology can be misused in cases where fake audio is used to impersonate authoritative figures in media settings.</p>
<p>(iii) AI-created phishing websites benefit from the capabilities of multimodal foundation models.These AI-generated websites not only display a remarkable proficiency in emulating the appearance and functionality of established brands, but they also possess the ability to integrate advanced methodologies that can bypass conventional anti-phishing protocols [180].</p>
<p>• Bias and Discrimination.AI-generated content can perpetuate biases which can harm marginalized groups and exacerbate social inequalities.A recent study [181] shows that AIGC produced by LLMs are more likely to exhibit notable discrimination against underrepresented population groups, compared to authenticated news articles collected from The New York Times and Reuters, where LLMs are asked to generate new articles with the same headlines as the real news.Another example studies AI in generating marketing content such as email composition, recommender systems, and landing page design, which shows that LLMs could amplify biases (e.g., using "man hours" to estimate effort, or using "chairman" for gender-neutral roles) in the generated content [182] Moreover, recent studies also show that although pre-trained LLMs can be used to solve various geospatial tasks [93,183], they also exhibit geographical and geopolitical bias -so-called geopolitical favouritism which is defined as the over-amplification of certain country representation (eg.countries with higher GDP, geopolitical stability, military strength, etc) in the generated content [184].</p>
<p>Mitigating safety issues caused by AIGC is still an ongoing challenge that requires a collaborative effort from AI developers, regulators, educators, and the broader society.Inspired by "magic must defeat magic", given the large volume of web content, researchers have been actively working on developing AI-based classifiers to detect online content produced by AI models [185].As highlighted by the work of Ippolito et al. [186], they rely on the supervised learning approach.Their study specifically fine-tuned the BERT model [29] using a mix of texts from human authors and those generated by LLMs.This method magnifies the subtle differences between human and AI-produced writings, thus enhancing the model's capability to pinpoint AI-generated content.In the field of misinformation detection, AI also plays a crucial role.Zhou et al. [169] investigated the distinct features of AI-generated misinformation and introduced a theory-guided technique to accumulate such content.This facilitates a systematic comparison between human-authored misinformation and its AI-generated counterpart, aiding in the identification of their inherent differences.On another front, AI models are equipped to detect biases within AIGC.Fang et al. [181] selected articles from reputable, impartial news outlets, such as The New York Times and Reuters.By using headlines from these sources as prompts, they assessed the racial and gender biases in LLM-generated content, comparing it with the original articles to highlight discrepancies.</p>
<p>Another line of research focuses on enhancing AI models to reduce the likelihood of misbehavior.</p>
<p>For instance, a recent study found that AIGC produced by ChatGPT exhibits a lower level of bias, in part due to its reinforcement learning from human feedback (RLHF) feature [181].</p>
<p>Toxicity</p>
<p>To ensure the dependable deployment of AI, it is imperative to prevent AI models from generating toxic or harmful content, which encompasses hate speech, biases, cyberbullying, and other objectionable material.Toxic content can harm individuals and communities, perpetuate discrimination, and create a hostile online environment.Although detecting hate speech and offensive language has long been a subject of research [187,188], the study of toxic AI-generated content is a more recent direction.For example, recent findings indicate that ChatGPT can consistently generate toxic content on a broad spectrum of topics when it is assigned a persona [189].Pre-trained language models can produce toxic text even when prompted with seemingly innocuous inputs [190].Thus, many organizations were actively working on research and technology to improve AI content generation while reducing harmful outputs.These recent efforts can be divided into two categories, including training-time and inference-time detoxification.</p>
<p>Training-time Strategies.There are two primary methods for refining large foundation models: pre-training and fine-tuning.To improve model pre-training, one approach involves the identification and filtering of undesirable documents from the training data [191].Additionally, we could augment the training data with information pertaining to its toxicity, towards guiding the LM to detect toxic content and hence generate non-toxic text [192].During fine-tuning, it is possible to align language models with human preferences by employing human feedback as a reward signal [193,194,195].A well-known example is InstructGPT [194] developed by OpenAI, which could generate less toxic outputs than those from GPT-3 by using properly designed prompts.</p>
<p>Inference-time Strategies.There are two major methods for reducing the toxicity of AI-generated content during inference time, including prompt learning and decoding-time steering.Prompt learning offers a versatile method to assess and tailor the output of large language models, such as toxicity classification, toxic text span detection, and detoxification [196].First, given a sentence, an initial step involves mapping its label to either "Yes" or "No" and subsequently refining the prompt to enhance its guidance for the language model.Second, toxic text span detection identifies the specific segments (i.e., the word offsets) that make the text toxic.Third, to rephrase the toxic text into a non-toxic version while preserving its semantic meaning.On the other hand, decoding-time steering [197,198,190] manipulates the output distribution to avoid generating mindless and offensive content.</p>
<p>Conclusion</p>
<p>The swift evolution of artificial general intelligence (AGI) is transforming the landscape of art and humanities in profound ways.As demonstrated in this paper, AGI systems like large language models and creative image generators have already exhibited impressive capabilities across diverse artistic domains including literature, visual arts, music, and more.However, as boundaries between human creativity and machine capabilities blur, difficult questions emerge around truth, toxicity, biases, accountability, and social impacts.</p>
<p>While celebrating the immense potential of AGI to augment human expression, we must thoughtfully navigate its responsible development.Multi-stakeholder collaboration and public discourse are vital to steer these systems in directions that uphold cultural values, pluralism, dignity, and truth.Technical solutions such as robust factuality evaluations, toxicity filters, and bias detectors can help instill reliability and trustworthiness in AGI systems.Ultimately, however, cultural shifts toward responsible innovation, centered on human flourishing over profits or progress for its own sake, are crucial.</p>
<p>By harnessing AGI as a partner for human creativity, while proactively addressing its pitfalls, we can usher in an era where machine intelligence promotes knowledge, empowers imagination, and expands access to the arts.The onus lies on researchers, developers, policymakers, and society at large to align AGI's technological promise with enduring human values.Through principled efforts, we can ensure these rapidly evolving systems enrich rather than undermine our shared cultural heritage.</p>
<p>Figure 2 :
2
Figure 2: An example of using GPT-3.5 for learning history.The right part shows a follow-up question regarding the answer of the first question in the left part.</p>
<p>Figure 3 :
3
Figure 3: An example of using GPT-4 to analyze the background and design philosophy of the lyrics of the UEFA (The Union of European Football Associations) Champions League Anthem.The AI model can easily handle the multilingual content, and even point out the "spirit of unity" and "diversity" behind the design.</p>
<p>Figure 4 :
4
Figure 4: An example of using GPT-4 to write poems (left) and personalized advertisement (right).</p>
<p>Figure 5 :
5
Figure 5: Google trends (top) and subreddit subscriber growth (bottom) for the past 12 months of the top 3 AI art generation tools: Midjourney, Stable Diffusion, and DALL-E.Data source: Google Trends and Subredditstats.</p>
<p>Figure 6 :
6
Figure 6: An illustration of using GPT-4V to do place name extraction and localization from a historical map of Georgia, USA as Kim et al. [96] did.The input to GPT-4V is the historical map and the prompt shown in the blue box.The answer from GPT-4V is shown in the orange box which provides a list of extracted place names as well as their map coordinates.Based on these map coordinates, we plot the corresponding numbers on the historical maps.</p>
<p>Figure 8 :
8
Figure 8: An example of Photo editing.Left: Before editing.Right: After editing by Adobe Firefly with the prompt "Turn the background into autumn".</p>
<p>Figure 9 :
9
Figure 9: Synthetic photography generated by three current generative diffusion models: DALL-E 3 (left), Midjourney (center), and Stable Diffusion XL (right).All images are generated by the following identical prompt for image generation: "hip mother age 30 looking from the baby's perspective, lens: 35mm, focus: mother's face, style: modern realistic, fashion: chic, fall colors, no patterns."</p>
<p>Figure 11 :
11
Figure 11: Some examples of Video Generation.Left: Editing a movie with the prompt.Video created by DALL-E 3 with the prompt.</p>
<p>†https://towardsdatascience.com/writing-songs-with-gpt-4-part-1-lyrics-3728da678482 † https://writeme.ai/poetry/
Acknowledgement We would like to thank Prof. John Hale from the Linguistics Department, University of Georgia for his thoughtful comments on the opportunities of AIGC and foundation models' applications on various art and humanities tasks.
Can you identify as many place names as possible from this old map and output the coordinates of each place on the image?. FLORIDA: (0.18, 0.90User GPT4-V References</p>
<p>Looking inward: Philosophical and methodological perspectives on phenomenological self-reflection. Natalie M Pool, Nursing science quarterly. 3132018</p>
<p>Ai risk skepticism, a comprehensive survey. Vemir Michael, Ambartsoumean , Roman V Yampolskiy, arXiv:2303.038852023arXiv preprint</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>Deepdream-a code example for visualizing neural networks. Alexander Mordvintsev, Christopher Olah, Mike Tyka, Google Research. 252015</p>
<p>Neural style transfer: A critical review. Akhil Singh, Vaibhav Jaiswal, Gaurav Joshi, Adith Sanjeeve, Shilpa Gite, Ketan Kotecha, IEEE Access. 92021</p>
<p>Generative adversarial networks. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Communications of the ACM. 63112020</p>
<p>Ai art at christie's sells for $432,500. Gabe Cohn, New York Times. 2018</p>
<p>Taming transformers for high-resolution image synthesis. Patrick Esser, Robin Rombach, Bjorn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Photorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Advances in Neural Information Processing Systems. 202235</p>
<p>When brain-inspired ai meets agi. Lin Zhao, Lu Zhang, Zihao Wu, Yuzhong Chen, Haixing Dai, Xiaowei Yu, Zhengliang Liu, Tuo Zhang, Xintao Hu, Xi Jiang, Meta-Radiology. 1000052023</p>
<p>A comprehensive survey of ai-generated content. Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, Lichao Sun, arXiv:2303.04226A history of generative ai from gan to chatgpt. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Summary of chatgpt-related research and perspective towards the future of large language models. Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Meta-Radiology. 1000172023</p>
<p>arXivOpenAI. Gpt-4 technical report. 2023</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125202213arXiv preprint</p>
<p>Glide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, arXiv:2112.107412021arXiv preprint</p>
<p>Ai-generated content (aigc): A survey. Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, Hong Lin, arXiv:2304.066322023arXiv preprint</p>
<p>When large language models meet personalization: Perspectives of challenges and opportunities. Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, arXiv:2307.163762023arXiv preprint</p>
<p>Surviving chatgpt in healthcare. Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Chao Cao, Haixing Dai, Ninghao Liu, Jun Liu, Wei Liu, Quanzheng Li, Frontiers in Radiology. 31224682</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Alex Sherstinsky, Physica D: Nonlinear Phenomena. 4041323062020</p>
<p>Understanding lstm-a tutorial into long short-term memory recurrent neural networks. C Ralf, Eric Rothstein Staudemeyer, Morris, arXiv:1909.095862019arXiv preprint</p>
<p>Convolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, International conference on machine learning. PMLR2017</p>
<p>Saliency detection: A spectral residual approach. Xiaodi Hou, Liqing Zhang, 2007 IEEE Conference on computer vision and pattern recognition. Ieee2007</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Beyond shared hierarchies: Deep multitask learning through soft layer ordering. Elliot Meyerson, Risto Miikkulainen, arXiv:1711.001082017arXiv preprint</p>
<p>Beam search strategies for neural machine translation. Markus Freitag, Yaser Al-Onaizan, arXiv:1702.018062017arXiv preprint</p>
<p>Trainable greedy decoding for neural machine translation. Jiatao Gu, Kyunghyun Cho, O K Victor, Li, arXiv:1702.024292017arXiv preprint</p>
<p>Scaling autoregressive models for content-rich text-to-image generation. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, arXiv:2206.10789202225arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy, arXiv:2307.10169Challenges and applications of large language models. 2023arXiv preprint</p>
<p>A survey on retrieval-augmented text generation. Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu, arXiv:2202.011102022arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Learning and individual differences. 1031022742023</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, arXiv:2307.031092023arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>What can linguistics and deep learning contribute to each other? response to pater. Tal Linzen, Language. 9512019</p>
<p>On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. Algebraic structures in natural language. Marco Baroni, 2022</p>
<p>Syntactic structure from deep learning. Tal Linzen, Marco Baroni, Annual Review of Linguistics. 72021</p>
<p>Why large language models are poor theories of human linguistic cognition. a reply to piantadosi. Roni Katzir, 2023. 2023Tel Aviv UniversityManuscript</p>
<p>Sentiment analysis in the era of large language models: A reality check. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, arXiv:2305.150052023arXiv preprint</p>
<p>Ad-autogpt: An autonomous gpt for alzheimer's disease infodemiology. Haixing Dai, Yiwei Li, Zhengliang Liu, Lin Zhao, Zihao Wu, Suhang Song, Ye Shen, Dajiang Zhu, Xiang Li, Sheng Li, arXiv:2306.100952023arXiv preprint</p>
<p>Generating chinese classical poems with statistical machine translation models. Jing He, Ming Zhou, Long Jiang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201226</p>
<p>Commercialized generative ai: A critical study of the feasibility and ethics of generating native advertising using large language models in conversational web search. Ines Zelch, Matthias Hagen, Martin Potthast, arXiv:2310.048922023arXiv preprint</p>
<p>Marketing with chatgpt: Navigating the ethical terrain of gpt-based chatbot technology. Pablo Rivas, Liang Zhao, AI. 422023</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. 201427</p>
<p>Mehdi Mirza, Simon Osindero, arXiv:1411.1784Conditional generative adversarial nets. 2014arXiv preprint</p>
<p>Variational autoencoder for deep learning of images, labels and captions. Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, Lawrence Carin, Advances in neural information processing systems. 292016</p>
<p>Normalizing flows: An introduction and review of current methods. Ivan Kobyzev, Simon Jd Prince, Marcus A Brubaker, IEEE transactions on pattern analysis and machine intelligence. 43112020</p>
<p>. Chun-Liang Li, Manzil Zaheer, Yang Zhang, Barnabas Poczos, Ruslan Salakhutdinov, arXiv:1810.057952018arXiv preprintPoint cloud gan</p>
<p>Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions. Dong Wook, Shu , Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Spectral-gans for high-resolution 3d point-cloud generation. Salman Sameera Ramasinghe, Nick Khan, Stephen Barnes, Gould, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>Graphgan: Graph representation learning with generative adversarial nets. Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, Minyi Guo, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, Josh Tenenbaum, Advances in neural information processing systems. 292016</p>
<p>Neural style transfer: A review. Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song, IEEE transactions on visualization and computer graphics. 26112019</p>
<p>Carl Doersch, arXiv:1606.05908Tutorial on variational autoencoders. 2016arXiv preprint</p>
<p>Generating images from captions with attention. Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov, arXiv:1511.027932015arXiv preprint</p>
<p>Amin Heyrani, Nobari , Muhammad Fathy, Rashad , Faez Ahmed, Creativegan, arXiv:2103.06242Editing generative adversarial networks for creative design synthesis. 2021arXiv preprint</p>
<p>Generating videos with scene dynamics. Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, Advances in neural information processing systems. 292016</p>
<p>Vivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Video swin transformer. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Attngan: Fine-grained text to image generation with attentional generative adversarial networks. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Df-gan: A simple and effective baseline for text-to-image synthesis. Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, Changsheng Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International conference on machine learning. PMLR2015</p>
<p>Advances in neural information processing systems. Jonathan Ho, Ajay Jain, Pieter Abbeel, 202033Denoising diffusion probabilistic models</p>
<p>Improved denoising diffusion probabilistic models. Alexander Quinn, Nichol , Prafulla Dhariwal, International Conference on Machine Learning. PMLR2021</p>
<p>Score-based generative modeling through stochastic differential equations. Yang Song, Jascha Sohl-Dickstein, Abhishek Diederik P Kingma, Stefano Kumar, Ben Ermon, Poole, arXiv:2011.134562020arXiv preprint</p>
<p>. Jiaming Song, Chenlin Meng, Stefano Ermon, arXiv:2010.025022020Denoising diffusion implicit models. arXiv preprint</p>
<p>Advances in neural information processing systems. Prafulla Dhariwal, Alexander Nichol, 202134Diffusion models beat gans on image synthesis</p>
<p>Jonathan Ho, Tim Salimans, arXiv:2207.12598Classifier-free diffusion guidance. 2022arXiv preprint</p>
<p>Image super-resolution via iterative refinement. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, Mohammad Norouzi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4542022</p>
<p>An introduction to variational autoencoders. Max Diederik P Kingma, Welling, Foundations and Trends® in Machine Learning. 201912</p>
<p>Cartography as an art and a science?. John B Krygier, The Cartographic Journal. 3211995</p>
<p>Symbolization of map projection distortion: a review. Karen A Mulcahy, Keith C Clarke, Cartography and geographic information science. 2832001</p>
<p>Calculating on a round planet. Nicholas R Chrisman, International Journal of Geographical Information Science. 3142017</p>
<p>Sphere2vec: A general-purpose location representation learning over a spherical surface for large-scale geospatial predictions. Gengchen Mai, Yao Xuan, Wenyun Zuo, Yutong He, Jiaming Song, Stefano Ermon, Krzysztof Janowicz, Ni Lao, ISPRS Journal of Photogrammetry and Remote Sensing. 2022023</p>
<p>A review and conceptual framework of automated map generalization. Kurt E Brassel, Robert Weibel, International Journal of Geographical Information System. 231988</p>
<p>A map generalization model based on algebra mapping transformation. Tinghua Ai, Peter Van Oosterom, Proceedings of the 9th ACM international symposium on Advances in geographic information systems. the 9th ACM international symposium on Advances in geographic information systems2001</p>
<p>Towards cartographic knowledge encoding with deep learning: A case study of building generalization. Kang, Rao, Wang, Peng, Gao, Zhang, Proceedings of the AutoCarto. the AutoCarto2020</p>
<p>Recognition of building group patterns in topographic maps based on graph partitioning and random forest. Xianjin He, Xinchang Zhang, Qinchuan Xin, ISPRS Journal of Photogrammetry and Remote Sensing. 1362018</p>
<p>A graph convolutional neural network for classification of building patterns using spatial vector data. ISPRS journal of photogrammetry and remote sensing. Xiongfeng Yan, Tinghua Ai, Min Yang, Hongmei Yin, 2019150</p>
<p>Towards general-purpose representation learning of polygonal geometries. Gengchen Mai, Chiyu Jiang, Weiwei Sun, Rui Zhu, Yao Xuan, Ling Cai, Krzysztof Janowicz, Stefano Ermon, Ni Lao, GeoInformatica. 2722023</p>
<p>A recognition method for drainage patterns using a graph convolutional network. Huafei Yu, Tinghua Ai, Min Yang, Lina Huang, Jiaming Yuan, International Journal of Applied Earth Observation and Geoinformation. 1071026962022</p>
<p>Multi-scale representation learning for spatial feature distributions using grid cells. Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, Ni Lao, International Conference on Learning Representations. 2020</p>
<p>Csp: Self-supervised contrastive spatial pre-training for geospatial-visual representations. Gengchen Mai, Ni Lao, Yutong He, Jiaming Song, Stefano Ermon, the Fortieth International Conference on Machine Learning. 2023ICML 2023</p>
<p>A review of location encoding for geoai: methods and applications. Gengchen Mai, Krzysztof Janowicz, Yingjie Hu, Song Gao, Bo Yan, Rui Zhu, Ling Cai, Ni Lao, International Journal of Geographical Information Science. 3642022</p>
<p>Towards a foundation model for geospatial artificial intelligence (vision paper). Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, Stefano Ermon, Proceedings of the 30th International Conference on Advances in Geographic Information Systems. the 30th International Conference on Advances in Geographic Information Systems2022</p>
<p>On the opportunities and challenges of foundation models for geospatial artificial intelligence. Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, arXiv:2304.067982023arXiv preprint</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei, arXiv:2306.148242023arXiv preprint</p>
<p>Building spatio-temporal knowledge graphs from vectorized topographic historical maps. Semantic Web. Basel Shbita, Craig A Knoblock, Weiwei Duan, Yao-Yi Chiang, Johannes H Uhl, Stefan Leyk, 2023Preprint</p>
<p>Jina Kim, Zekun Li, Yijun Lin, Min Namgung, Leeje Jang, Yao-Yi Chiang, arXiv:2306.17059The mapkurator system: A complete pipeline for extracting and linking text from historical maps. 2023arXiv preprint</p>
<p>Narrative cartography with knowledge graphs. Gengchen Mai, Weiming Huang, Ling Cai, Rui Zhu, Ni Lao, Journal of Geovisualization and Spatial Analysis. 6142022</p>
<p>The ethics of ai-generated maps: A study of dalle 2 and implications for cartography. Yuhao Kang, Qianheng Zhang, Robert Roth, arXiv:2304.107432023arXiv preprint</p>
<p>Artificial intelligence in landscape architecture: A literature review. Phillip Fernberg, Brent Chamberlain, Landscape Journal. 4212023</p>
<p>Designer Robots: An early look at applications for Artificial Intelligence Visualization Software in Landscape Architecture. Marika Li, 2023University of GuelphPhD thesis</p>
<p>Dalle-urban: Capturing the urban design expertise of large text to image transformers. Sachith Seneviratne, Damith Senanayake, Sanka Rasnayaka, Rajith Vidanaarachchi, Jason Thompson, 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA). IEEE2022</p>
<p>The prospects of artificial intelligence in urban planning. Thomas W Sanchez, Hannah Shumway, Trey Gordner, Theo Lim, International Journal of Urban Sciences. 2722023</p>
<p>Ai art in architecture. Joern Ploennigs, Markus Berger, AI in Civil Engineering. 2182023</p>
<p>Ai and architecture: An experimental perspective. Stanislas Chaillou, The Routledge Companion to Artificial Intelligence in Architecture. Routledge2021</p>
<p>Revamping interior design workflow through generative artificial intelligence. Ziming He, Xiaomei Li, Ling Fan, Harry Jiannan, Wang , International Conference on Human-Computer Interaction. Springer2023</p>
<p>Improving design efficiency using artificial intelligence: A study on the role of artificial intelligence in streamlining the interior design process. Ghada, Hussein, International Design Journal. 1352023</p>
<p>Blurring the boundaries between real and artificial in architecture and urban design through the use of artificial intelligence. Rafael Iván, Pazos Pérez, 2017Universidade da CoruñaPhD thesis</p>
<p>. Mathias Bank Stigsen, Alexandra Moisi, Shervin Rasoulzadeh, Kristina Schinegger, Stefan Rutzinger, Ai diffusion as design vocabulary</p>
<p>Denoising diffusion restoration models. Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song, Advances in Neural Information Processing Systems. 202235</p>
<p>Denoising diffusion models for plug-and-play image restoration. Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, Luc Van Gool, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Image super-resolution via iterative refinement. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, Mohammad Norouzi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4542022</p>
<p>Ssif: Learning continuous image representation for spatial-spectral super-resolution. Gengchen Mai, Ni Lao, Weiwei Sun, Yuchi Ma, Jiaming Song, Chenlin Meng, Hongxu Ma, Jinmeng Rao, Ziyuan Li, Stefano Ermon, arXiv:2310.004132023arXiv preprint</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, Daniel Aliaga, arXiv:2212.00932Objectstitch: Generative object compositing. 2022arXiv preprint</p>
<p>Paint2pix: interactive painting based progressive image synthesis and editing. Jaskirat Singh, Liang Zheng, Cameron Smith, Jose Echevarria, European Conference on Computer Vision. Springer2022</p>
<p>Imagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Fleet, arXiv:2210.023032022arXiv preprint</p>
<p>The impact of artificial intelligence on the graphic design industry. resmilitaris. Bahaa Mustafa, 202313</p>
<p>Could chatgpt imagine: Content control for artistic painting generation via large language models. Yue Lu, Chao Guo, Yong Dou, Xingyuan Dai, Fei-Yue Wang, Journal of Intelligent &amp; Robotic Systems. 10922023</p>
<p>Destroy all humans: The dematerialisation of the designer in an age of automation and its impact on graphic design-a literature review. Benjamin Matthews, Barrie Shannon, Mark Roxburgh, International Journal of Art &amp; Design Education. 4232023</p>
<p>Font completion and manipulation by cycling between multi-modality representations. Ye Yuan, Wuyang Chen, Zhaowen Wang, Matthew Fisher, Zhifei Zhang, Zhangyang Wang, Hailin Jin, arXiv:2108.129652021arXiv preprint</p>
<p>Art font image generation with conditional generative adversarial networks. Ye Yuan, Yasuaki Ito, Koji Nakano, 2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW). IEEE2020</p>
<p>Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao, arXiv:2305.11588Text2nerf: Text-driven 3d scene generation with neural radiance fields. 2023arXiv preprint</p>
<p>Learning visibility for robust dense human body estimation. Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, Ming-Hsuan Yang, European Conference on Computer Vision. Springer2022</p>
<p>Using chatgpt and midjourney to generate chinese landscape painting of tang poem 'the difficult road to shu. Hung-Cheng Chen, Zhongwen Chen, International Journal of Social Sciences. 32</p>
<p>The art of artificial evolution: A handbook on evolutionary art and music. Juan Romero, Juan J Romero, Penousal Machado, 2008Springer Science &amp; Business Media</p>
<p>Introduction to evolutionary computing. E Agoston, James E Eiben, Smith, 2015Springer</p>
<p>Tivgan: Text to image to video generation with step-by-step evolutionary generator. Doyeon Kim, Donggyu Joo, Junmo Kim, IEEE Access. 82020</p>
<p>MoCoGAN: Decomposing motion and content for video generation. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018</p>
<p>Dirk Weissenborn, Oscar Täckström, Jakob Uszkoreit, arXiv:1906.02634Scaling autoregressive video models. 2019arXiv preprint</p>
<p>Ccvs: context-aware controllable video synthesis. Guillaume Le, Moing , Jean Ponce, Cordelia Schmid, Advances in Neural Information Processing Systems. 202134</p>
<p>Nüwa: Visual synthesis pre-training for neural visual world creation. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan, European conference on computer vision. Springer2022</p>
<p>Long video generation with time-agnostic vqgan and time-sensitive transformer. Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, Devi Parikh, European Conference on Computer Vision. Springer2022</p>
<p>Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, arXiv:2204.03458Video diffusion models. 2022</p>
<p>Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Text2video-zero: Text-to-image diffusion models are zero-shot video generators. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi, arXiv:2303.134392023arXiv preprint</p>
<p>4-in-the-film-industry-scriptwriting-editing-and-more. Chat gpt-4 in the film industry: Scriptwriting, editing, and more. </p>
<p>Movienet: A holistic dataset for movie understanding. Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin, The European Conference on Computer Vision (ECCV). 2020</p>
<p>A graphbased framework to bridge movies and synopses. Yu Xiong, Qingqiu Huang, Lingfeng Guo, Hang Zhou, Bolei Zhou, Dahua Lin, The IEEE International Conference on Computer Vision (ICCV). October 2019</p>
<p>The impact of generative ai on hollywood and entertainment. </p>
<p>Unifying identification and context learning for person recognition. Qingqiu Huang, Yu Xiong, Dahua Lin, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2018</p>
<p>Person search in videos with one portrait through visual and temporal links. Qingqiu Huang, Wentao Liu, Dahua Lin, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Online multi-modal person search in videos. Jiangyue Xia, Anyi Rao, Linning Xu, Qingqiu Huang, Jiangtao Wen, Dahua Lin, The European Conference on Computer Vision (ECCV). 2020</p>
<p>From trailers to storylines: An efficient way to learn from movies. Qingqiu Huang, Yuanjun Xiong, Yu Xiong, Yuqi Zhang, Dahua Lin, arXiv:1806.053412018arXiv preprint</p>
<p>A unified framework for shot type classification based on subject centric lens. Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, Dahua Lin, The European Conference on Computer Vision (ECCV). 2020</p>
<p>A local-to-global approach to multi-modal movie scene segmentation. Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>The effects of artificial intelligence on media and communication. </p>
<p>Ai and the future of local tv news. </p>
<p>Video-llama: An instruction-tuned audio-visual language model for video understanding. Hang Zhang, Xin Li, Lidong Bing, arXiv:2306.028582023arXiv preprint</p>
<p>Video-based vibrato detection and analysis for polyphonic string music. Bochen Li, Karthik Dinesh, Gaurav Sharma, Zhiyao Duan, ISMIR. 2017</p>
<p>Skeleton plays piano: Online generation of pianist body movements from midi performance. Bochen Li, Akira Maezawa, Zhiyao Duan, ISMIR. 2018</p>
<p>See and listen: Score-informed association of sound tracks to players in chamber music performance videos. Bochen Li, Karthik Dinesh, Zhiyao Duan, Gaurav Sharma, 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2017</p>
<p>Online audiovisual source association for chamber music performances. Bochen Li, Karthik Dinesh, Chenliang Xu, Gaurav Sharma, Zhiyan Duan, Transactions of the International Society for Music Information Retrieval. 20192</p>
<p>Singnet: a real-time singing voice beat and downbeat tracking system. Mojtaba Heydari, Ju-Chiang Wang, Zhiyao Duan, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2023</p>
<p>Segmentation of music video streams in music pieces through audio-visual analysis. Gabriel Sargent, Pierre Hanna, Henri Nicolas, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2014</p>
<p>The effect of audio, visual and audio-visual performance on perception of musical content. Janice N Killian, Bulletin of the Council for Research in Music Education. 2001</p>
<p>Creating a multitrack classical music performance dataset for multimodal music analysis: Challenges, insights, and applications. Bochen Li, Xinzhao Liu, Karthik Dinesh, Zhiyao Duan, Gaurav Sharma, IEEE Transactions on Multimedia. 2122018</p>
<p>Audio-visual integration of emotional cues in song. Frank A William Forde Thompson, Lena Russo, Quinto, Cognition and Emotion. 2282008</p>
<p>Audio-visual analysis of music performances. Zhiyao Duan, Slim Essid, Cynthia C S Liem, Gaël Richard, Gaurav Sharma, IEEE Signal Processing Magazine. 3612019</p>
<p>Humans versus ai: whether and why we prefer human-created compared to ai-created artwork. Lucas Bellaiche, Rohin Shahi, Martin Harry Turpin, Anya Ragnhildstveit, Shawn Sprockett, Nathaniel Barr, Alexander Christensen, Paul Seli, Cognitive Research: Principles and Implications. 812023</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Ali Borji, arXiv:2302.03494A categorical archive of chatgpt failures. 2023arXiv preprint</p>
<p>Why-does-AI-art-screw-up-hands-and-fingers-2230501. Why does ai art screw up hands and fingers?</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Assessing the factual accuracy of generated text. Ben Goodrich, Vinay Rao, Peter J Liu, Mohammad Saleh, proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining2019</p>
<p>Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, William Saunders, arXiv:2110.06674Truthful ai: Developing and governing ai that does not lie. 2021arXiv preprint</p>
<p>Aigc challenges and opportunities related to public safety: A case study of chatgpt. Danhuai Guo, Huixuan Chen, Ruoling Wu, Yangang Wang, Journal of Safety Science and Resilience. 2023</p>
<p>Building privacy-preserving and secure geospatial artificial intelligence foundation models. Jinmeng Rao, Song Gao, Gengchen Mai, Krzysztof Janowicz, arXiv:2309.173192023arXiv preprint</p>
<p>Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions. Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, Munmun De Choudhury, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>A survey on video-based fake news detection techniques. Ronak Agrawal, Dilip Kumar Sharma, 2021 8th International Conference on Computing for Sustainable Global Development (INDIACom). IEEE2021</p>
<p>popular-image-generators-accept-85-of-fake-news-prompts. Popular image generators accept 85. </p>
<p>Fake trump arrest photos: How to spot an ai-generated image. </p>
<p>Pictures of joe biden and vp celebrating trump indictment are fake. </p>
<p>Deepfake scams have arrived: Fake videos spread on facebook, tiktok and youtube. deepfake-scams-arrived-fake-videos-spread-facebook-tiktok-youtube-rcna101415</p>
<p>The newest ai-enabled weapon: Deep-faking photos of the earth. Defense One. Patrick Tucker, 20193</p>
<p>Deep fake geography? when geospatial data encounter artificial intelligence. Bo Zhao, Shaozeng Zhang, Chunxue Xu, Yifan Sun, Chengbin Deng, Cartography and Geographic Information Science. 4842021</p>
<p>Going spear phishing: Exploring embedded training and awareness. Deanna D Caputo, Shari Lawrence Pfleeger, Jesse D Freeman, M Eric, Johnson , IEEE security &amp; privacy. 1212013</p>
<p>Large language models can be used to effectively scale spear phishing campaigns. Julian Hazell, arXiv:2305.069722023arXiv preprint</p>
<p>Artificial intelligence's impact on social engineering attacks. Sowjanya Manyam, 2022</p>
<p>Sayak Saha, Roy , Krishna Vamsi Naragam, Shirin Nilizadeh, arXiv:2305.05133Generating phishing attacks using chatgpt. 2023arXiv preprint</p>
<p>Bias of ai-generated content: An examination of news produced by large language models. Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao, arXiv:2309.098252023arXiv preprint</p>
<p>overcoming-algorithmic-gender-bias-in-ai-generated-marketing-content/?sh= 518d4fa11639. Overcoming algorithmic gender bias in ai-generated marketing content</p>
<p>Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, Stefano Ermon, Geollm, arXiv:2310.06213Extracting geospatial knowledge from large language models. 2023arXiv preprint</p>
<p>arXiv:2212.10408Fahim Faisal and Antonios Anastasopoulos. Geographic and geopolitical biases of language models. 2022arXiv preprint</p>
<p>The science of detecting llm-generated texts. Ruixiang Tang, Yu-Neng Chuang, Xia Hu, arXiv:2303.072052023arXiv preprint</p>
<p>Automatic detection of generated text is easiest when humans are fooled. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck, arXiv:1911.006502019arXiv preprint</p>
<p>Automated hate speech detection and the problem of offensive language. Thomas Davidson, Dana Warmsley, Michael Macy, Ingmar Weber, Proceedings of the international AAAI conference on web and social media. the international AAAI conference on web and social media201711</p>
<p>Semeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval). Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, Proceedings of the 13th International Workshop on Semantic Evaluation. the 13th International Workshop on Semantic Evaluation2019</p>
<p>Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, arXiv:2304.05335Toxicity in chatgpt: Analyzing persona-assigned language models. 2023arXiv preprint</p>
<p>Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 20202009arXiv e-prints</p>
<p>Mitigating harm in language models with conditional-likelihood filtration. Helen Ngo, Cooper Raterink, G M João, Ivan Araújo, Carol Zhang, Adrien Chen, Nicholas Morisot, Frosst, arXiv:2108.077902021arXiv preprint</p>
<p>Adding instructions during pretraining: Effective way of controlling toxicity in language models. Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Exploring the limits of domain-adaptive training for detoxifying large-scale language models. Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, Bryan Catanzaro, Advances in Neural Information Processing Systems. 202235</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content. Xinlei He, Savvas Zannettou, Yun Shen, Yang Zhang, 20232308arXiv e-prints</p>
<p>Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, International Conference on Learning Representations. 2019</p>
<p>Dexperts: Decoding-time controlled text generation with experts and anti-experts. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference Natural Language Processing20211</p>            </div>
        </div>

    </div>
</body>
</html>
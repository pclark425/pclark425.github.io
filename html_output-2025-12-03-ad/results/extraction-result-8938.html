<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8938 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8938</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8938</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-6eb23df05166c772e4c2fbfb0113de0beabd1a43</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6eb23df05166c772e4c2fbfb0113de0beabd1a43" target="_blank">Large Language Models are in-Context Molecule Learners</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Knowledge and Data Engineering</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve similar informative context examples. Additionally, Post-retrieval Re-ranking is composed of Sequence Reversal and Random Walk selection to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context learning and reasoning capability of LLMs with the retrieved examples and adapts the parameters of LLMs for better alignment between molecules and texts. Experimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8938.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8938.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Molecule Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage paradigm (Hybrid Context Retrieval, Post-retrieval Re-ranking, In-Context Molecule Tuning) that adapts general LLMs to molecule-caption translation and text-to-molecule generation by retrieving informative molecule–caption examples and fine-tuning LLM parameters so they learn molecule↔text alignment from context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to multiple LLM backbones (Mistral-7B, Galactica-125M, Galactica-1.3B, Llama3-8B / Llama-2-7b variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based Large Language Models (autoregressive LLMs; instruction-tuned variants used where available)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Examples: Mistral-7B (7B), Galactica-125M (125M), Galactica-1.3B (1.3B), Llama3-8B (~8B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>No extra domain pretraining required by ICMA itself; ICMA is trained/fine-tuned on molecule-caption translation datasets (ChEBI-20, PubChem324k) and leverages a pre-trained Mole-BERT GNN for molecule retrieval embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation and de novo molecule generation (Cap2Mol) — broadly applicable to drug discovery and molecule property tasks (also tested on MoleculeNet classification tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented in-context tuning: (1) Hybrid retrieval (Mole-BERT graph embeddings for molecules, BM25 for captions) to fetch similar examples; (2) post-retrieval re-ranking (Random Walk sampling, Sequence Reversal) to improve context informativeness; (3) In-Context Molecule Tuning — fine-tuning LLM parameters with the retrieved examples so the model learns mappings x→y from context and then generates SMILES/SELFIES directly from captions (Cap2Mol) or captions from SMILES (Mol2Cap).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports generation-quality metrics (exact-match and fingerprint similarities) rather than explicit percentage-of-novel molecules vs training set; best Cap2Mol exact-match (EM) on ChEBI-20: ICMA(Mistral-7B) EM = 0.460; fingerprint similarities (MACCS/RDK/Morgan) improved under ICMA, indicating generated molecules are closer in chemical space to targets, but explicit novelty fraction (novel/not-in-training) is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity is enforced implicitly by retrieving context examples similar to the query (graph-based Mole-BERT or BM25 caption retrieval) so the LLM learns mappings between caption phrases and substructures (functional groups) from similar examples; no explicit property- or target-guided optimization reported beyond this context-based conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mol2Cap: BLEU-2/4, ROUGE-1/2/L, METEOR; Cap2Mol: BLEU, Exact Match (EM), Levenshtein distance, fingerprint similarities (MACCS, RDK, Morgan), validity of generated SMILES/SELFIES.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ICMA substantially improved generation and captioning vs baseline fine-tuned LLMs and many domain-pretrained models. Example highlights on ChEBI-20: Mol2Cap BLEU-4: ICMA(Mistral-7B)=0.581; Mol2Cap BLEU-4: ICMA(Galactica-125M)=0.565. Cap2Mol exact-match: ICMA(Mistral-7B)=0.460, ICMA(Galactica-125M)=0.391. Validity (generated molecules) for ICMA(Mistral-7B) reached 0.958 on ChEBI-20. ICMA also improved performance on PubChem324k and on MoleculeNet property classification (average improvements, e.g., Mistral-7B improved markedly under ICMA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with domain-specific pretraining & fine-tuning approaches (MolT5, BioT5, MolXPT, MolCA) and prompting-only approaches (MolReGPT on GPT-3.5/GPT-4), ICMA achieved state-of-the-art or comparable results on many metrics without requiring large-scale chemical pretraining or modality-alignment stages. ICMA enabled smaller models (e.g., Galactica-125M) to outperform some larger domain-specific systems by leveraging retrieved context and in-context tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Dependence on retrieval quality (Mole-BERT GNN and BM25 selection critical); input/context length constraints (cutoff lengths limit number/size of context examples); Random Walk introduces randomness (but empirically stable with small p_max); ICMA still requires fine-tuning of LLM parameters (though not domain pretraining) and benefits from larger backbones; lack of explicit novelty reporting versus training corpora and no explicit property-targeted optimization (e.g., binding affinity) in current experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8938.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mole-BERT retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mole-BERT (pre-trained GNN used for molecule graph retrieval embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained graph neural network (GNN) encoder used to produce molecule graph embeddings for retrieval (used in ICMA's Hybrid Context Retrieval for Mol2Cap/Cap2Mol), shown superior to Morgan fingerprints and random retrieval in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mole-bert: Rethinking pre-training graph neural networks for molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mole-BERT (GNN encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph Neural Network (pre-trained molecular graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on millions of molecule graphs (as per Mole-BERT reference); used to produce 2D graph embeddings for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrieval support for molecule-caption translation and text-to-molecule generation — improves selection of context examples by structural similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative model itself; provides embeddings used to retrieve structurally similar molecules to condition LLM generation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Indirect effect: better retrieval leads to more informative contexts and improved ability of the LLM to generate molecules matching targets; no direct novelty statistics reported for Mole-BERT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Provides structure-aware retrieval so context examples emphasize relevant substructures/functional groups, enabling LLMs to produce molecules tailored to captured caption cues.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream improvements measured via Mol2Cap BLEU/ROUGE/METEOR and Cap2Mol EM/validity/fingerprint similarities when Mole-BERT used versus Morgan fingerprints or random retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mole-BERT based retrieval produced the best Mol2Cap metrics in ICMA ablations (BLEU-4=0.568 vs Morgan FTS=0.552 and Random=0.533 in one reported setting), demonstrating improved retrieval quality leading to better generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed classical fingerprint-based retrieval (Morgan fingerprints + Dice similarity) and random retrieval for selecting context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Relies on pre-trained GNN quality and coverage; retrieval errors or embedding mismatches can degrade downstream LLM tuning and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8938.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT (In-Context Few-Shot Molecule Learning via ChatGPT/GPT models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/in-context learning approach that uses large LLMs (e.g., GPT-3.5, GPT-4) with retrieved examples to perform molecule-caption translation and generation without fine-tuning; performance depends on model size and retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo and GPT-4 (used in MolReGPT baseline evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-style autoregressive transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses general LLM pretraining (proprietary OpenAI corpora) and few-shot prompts built from molecule–caption pairs; no domain-specific fine-tuning reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation (Mol2Cap) and text-to-molecule generation (Cap2Mol) via in-context few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / in-context few-shot learning with retrieved examples (no parameter updates performed on the LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported baseline metrics in this paper: Cap2Mol EM (ChEBI-20): MolReGPT (GPT-3.5-turbo) EM = 0.139; MolReGPT (GPT-4-0314) EM = 0.280; validity metrics and fingerprint similarities reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Relies on high-capacity LLM emergent capabilities and retrieval quality to match captions to target molecules; not explicitly optimized for specific properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same task metrics as ICMA: BLEU, EM, Levenshtein, MACCS/RDK/Morgan fingerprint similarities, validity, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolReGPT produced competitive results among prompting-only approaches but required very large LLMs (GPT-3.5/GPT-4) to achieve competitive performance; ICMA outperformed or matched MolReGPT on multiple metrics while being applicable to smaller backbones through in-context tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MolReGPT is a prompting-only baseline contrasted with ICMA's retrieval + in-context fine-tuning; MolReGPT depends more heavily on large model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>High dependence on emergent capabilities of very large LLMs; poor performance when applied to smaller LLMs; no parameter adaptation limits how much model can learn alignment from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8938.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based model pre-trained jointly on molecule SMILES and general text corpora and fine-tuned for molecule-caption translation; represents the domain-specific pretraining & fine-tuning paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (base, large variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5 transformer (encoder–decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MolT5-large reported as >780M parameters (paper states more than 780M)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Domain-specific pretraining on chemical corpora (e.g., PubChem) combined with general text; fine-tuned on ChEBI-20 molecule-caption pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation (Mol2Cap and Cap2Mol) for chemistry / drug discovery contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Domain-specific pretraining followed by supervised fine-tuning on molecule-caption translation pairs to generate captions or SMILES from text.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Performance metrics reported (MolT5-large: Cap2Mol EM=0.311 on ChEBI-20; Mol2Cap BLEU-4=0.508), but explicit novelty vs training set not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity arises from domain pretraining on chemical corpora and supervised fine-tuning on paired molecule-caption datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR for Mol2Cap; BLEU, EM, Levenshtein, fingerprint similarities, validity for Cap2Mol.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5 (large) achieves strong baseline performance (Mol2Cap BLEU-4=0.508; Cap2Mol EM=0.311) but requires large domain-specific pretraining and modality alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in the paper as a domain-pretrained baseline; ICMA enabled general LLMs (with in-context tuning) to reach or exceed MolT5 performance on several metrics without domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires extra domain-specific pretraining and modality alignment; scaling to very large LLM backbones is resource intensive given scarcity of high-quality molecule-caption corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8938.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that enriches cross-modal biological integration using chemical knowledge and natural language associations; uses SELFIES to mitigate invalid SMILES generation and is trained with molecule-description pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-style transformer adapted for biological/chemical multimodal data</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Joint training on single-modal data, wrapped text data, and molecule/protein-description pairs; uses SELFIES representation for molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation, molecule generation for biological/chemical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Domain-specific pretraining and fine-tuning; SELFIES used to ensure 100% robustness in molecular string representation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported Cap2Mol metrics in comparison: BLEU=0.867, EM=0.411 on ChEBI-20 (as reported in the comparative table); explicit novelty fraction not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tailored via chemical-domain pretraining and cross-modal integration to better align molecule representations with natural language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, EM, Levenshtein, fingerprint similarity, validity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioT5 is a strong domain-specific baseline for Cap2Mol, achieving high BLEU and EM in the comparative table; ICMA achieved similar or better fingerprint similarity scores and competitive EM without domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably to MolXPT and ICMA on some metrics; ICMA matched or outperformed BioT5 on fingerprint similarity while not requiring domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires domain-specific corpora and pretraining; may have higher resource needs and limited scalability to larger LLM backbones without significant compute/data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8938.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative pre-training approach that ‘wraps’ molecules with text to improve molecule–text alignment during pretraining for downstream translation/generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molxpt: Wrapping molecules with text for generative pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based generative pre-training model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on wrapped text representations of molecules and text corpora to improve molecule-text alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Generative pre-training on paired/wrapped molecule-text data followed by fine-tuning on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported Cap2Mol performance in comparisons (e.g., BLEU and EM) but explicit novelty metrics not shown in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Aims to align molecular representations and text during pretraining to enable more accurate text-to-molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, EM, fingerprint similarities, validity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolXPT is competitive among domain-aligned pretraining methods; ICMA compared favorably without requiring such pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against MolT5 and BioT5; ICMA achieved higher fingerprint similarity metrics on some runs while avoiding large-scale chemical pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires dedicated pretraining on molecule-text wrapped data; scalability and data availability are potential constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8938.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolCA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal method that encodes 2D molecular graphs and projects them into an LLM via a Q-Former (crossmodal projector) to improve molecule understanding and captioning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molca: Molecular graph-language modeling with crossmodal projector and uni-modal adapter</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolCA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multimodal model combining GNN graph encoder + Q-Former + LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses molecule graph encoders and paired molecule–caption corpora for cross-modal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Mol2Cap (molecule captioning) primarily; multimodal molecule understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Graph encoder outputs projected into LLM input via a learned crossmodal Q-Former to align graph and text modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Focused on captioning and understanding rather than de novo molecule generation; not directly evaluated for novelty metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Improves Mol2Cap by explicit graph–language modality alignment; operates at whole-graph level.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mol2Cap metrics (BLEU, ROUGE, METEOR) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolCA improves molecular understanding for captioning but its modality alignment targets Mol2Cap rather than Cap2Mol; ICMA achieved comparable or better Mol2Cap BLEU/METEOR without modality alignment training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared as a modality-alignment approach; ICMA differs by avoiding explicit modality-alignment training and by enabling Cap2Mol improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Primarily focused on Mol2Cap (captioning) and whole-graph alignment; less directly applicable to text-driven de novo molecule generation (Cap2Mol) without additional design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8938.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (instruct-v0.2 variant used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity general-purpose LLM used as a backbone in ICMA; when adapted with ICMA it produced state-of-the-art Cap2Mol and Mol2Cap results in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mistral 7b</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (instruct-v0.2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based autoregressive LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>General LLM pretraining (not chemical-specific); in experiments it was fine-tuned/adapted using ICMA on molecule-caption datasets (ChEBI-20, PubChem324k) with LoRA for large weights.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation (Mol2Cap) and text-to-molecule generation (Cap2Mol) as adapted by ICMA; also applied to MoleculeNet classification tasks after adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>ICMA In-Context Molecule Tuning (retrieval-augmented fine-tuning) to generate SMILES from captions and captions from SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Measured via exact-match and fingerprint similarity metrics (e.g., Cap2Mol EM on ChEBI-20 after ICMA = 0.460; fingerprints MACCS=0.916, RDK=0.837, Morgan=0.789), explicit novelty fraction to training set not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Context-driven (retrieval + re-ranking) ensures generation reflects attributes described in captions; no explicit target-binding optimization performed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, EM, Levenshtein, fingerprint similarities (MACCS/RDK/Morgan), validity, ROUGE, METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ICMA(Mistral-7B) achieved best reported numbers in this study: Mol2Cap BLEU-4=0.581, METEOR=0.661; Cap2Mol EM=0.460, validity=0.958, and top fingerprint similarities among tested systems on ChEBI-20.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed naive supervised fine-tuning of the same backbone and matched/exceeded many domain-specific pretraining approaches; surpassed prompting-only MolReGPT baselines on several metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Still benefits from LoRA and compute to tune 7B parameters; performance sensitive to retrieval quality, context length settings, and Random Walk hyperparameters; no targeted property optimization (e.g., binding) or experimental synthesis validation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8938.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8938.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-125M (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica-125M</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller scientific-domain pretrained LLM used as an ICMA backbone; pretraining on scientific corpora gives it some molecular knowledge that ICMA further refines via in-context tuning to improve molecule captioning and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-125M</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (scientific-pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on scientific corpora (per Galactica); ICMA fine-tuned on ChEBI-20 and PubChem324k for experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule-caption translation (Mol2Cap) and Cap2Mol generation; also tested on MoleculeNet tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>ICMA pipeline (retrieval + post-retrieval re-ranking + in-context molecule tuning) to generate captions or SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Improved Cap2Mol EM under ICMA on ChEBI-20: ICMA(Galactica-125M) EM = 0.391; fingerprint similarities and validity also improved; no explicit novelty-vs-training-set statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Relies on retrieved similar examples (Mole-BERT/BM25) to teach the model caption↔substructure mappings; not explicitly property-targeted.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics used across the study: BLEU, ROUGE, METEOR (Mol2Cap) and BLEU, EM, Levenshtein, fingerprint similarities, validity (Cap2Mol).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ICMA raised Galactica-125M performance significantly (e.g., Mol2Cap BLEU-4 increased from 0.501 to 0.565 in reported runs) and allowed a small 125M model to beat larger domain-specific models on some metrics by leveraging context adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>After ICMA, Galactica-125M outperformed the naive fine-tuned version and some previous domain-specific baselines (e.g., MolT5-base in some metrics) without extra domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Smaller model size limits natural language generation capability (observed caps on Mol2Cap quality in some settings); constrained by input cutoff lengths limiting context examples; displays less robust in-context reasoning than larger models, making ICMA tuning important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are in-Context Molecule Learners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Mole-bert: Rethinking pre-training graph neural networks for molecules <em>(Rating: 2)</em></li>
                <li>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations <em>(Rating: 2)</em></li>
                <li>Molca: Molecular graph-language modeling with crossmodal projector and uni-modal adapter <em>(Rating: 2)</em></li>
                <li>Molxpt: Wrapping molecules with text for generative pre-training <em>(Rating: 2)</em></li>
                <li>Synergpt: In-context learning for personalized drug synergy prediction and drug design <em>(Rating: 1)</em></li>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8938",
    "paper_id": "paper-6eb23df05166c772e4c2fbfb0113de0beabd1a43",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "ICMA",
            "name_full": "In-Context Molecule Adaptation",
            "brief_description": "A three-stage paradigm (Hybrid Context Retrieval, Post-retrieval Re-ranking, In-Context Molecule Tuning) that adapts general LLMs to molecule-caption translation and text-to-molecule generation by retrieving informative molecule–caption examples and fine-tuning LLM parameters so they learn molecule↔text alignment from context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to multiple LLM backbones (Mistral-7B, Galactica-125M, Galactica-1.3B, Llama3-8B / Llama-2-7b variants)",
            "model_type": "Transformer-based Large Language Models (autoregressive LLMs; instruction-tuned variants used where available)",
            "model_size": "Examples: Mistral-7B (7B), Galactica-125M (125M), Galactica-1.3B (1.3B), Llama3-8B (~8B)",
            "training_data": "No extra domain pretraining required by ICMA itself; ICMA is trained/fine-tuned on molecule-caption translation datasets (ChEBI-20, PubChem324k) and leverages a pre-trained Mole-BERT GNN for molecule retrieval embeddings.",
            "application_domain": "Molecule-caption translation and de novo molecule generation (Cap2Mol) — broadly applicable to drug discovery and molecule property tasks (also tested on MoleculeNet classification tasks).",
            "generation_method": "Retrieval-augmented in-context tuning: (1) Hybrid retrieval (Mole-BERT graph embeddings for molecules, BM25 for captions) to fetch similar examples; (2) post-retrieval re-ranking (Random Walk sampling, Sequence Reversal) to improve context informativeness; (3) In-Context Molecule Tuning — fine-tuning LLM parameters with the retrieved examples so the model learns mappings x→y from context and then generates SMILES/SELFIES directly from captions (Cap2Mol) or captions from SMILES (Mol2Cap).",
            "novelty_of_chemicals": "Paper reports generation-quality metrics (exact-match and fingerprint similarities) rather than explicit percentage-of-novel molecules vs training set; best Cap2Mol exact-match (EM) on ChEBI-20: ICMA(Mistral-7B) EM = 0.460; fingerprint similarities (MACCS/RDK/Morgan) improved under ICMA, indicating generated molecules are closer in chemical space to targets, but explicit novelty fraction (novel/not-in-training) is not reported.",
            "application_specificity": "Specificity is enforced implicitly by retrieving context examples similar to the query (graph-based Mole-BERT or BM25 caption retrieval) so the LLM learns mappings between caption phrases and substructures (functional groups) from similar examples; no explicit property- or target-guided optimization reported beyond this context-based conditioning.",
            "evaluation_metrics": "Mol2Cap: BLEU-2/4, ROUGE-1/2/L, METEOR; Cap2Mol: BLEU, Exact Match (EM), Levenshtein distance, fingerprint similarities (MACCS, RDK, Morgan), validity of generated SMILES/SELFIES.",
            "results_summary": "ICMA substantially improved generation and captioning vs baseline fine-tuned LLMs and many domain-pretrained models. Example highlights on ChEBI-20: Mol2Cap BLEU-4: ICMA(Mistral-7B)=0.581; Mol2Cap BLEU-4: ICMA(Galactica-125M)=0.565. Cap2Mol exact-match: ICMA(Mistral-7B)=0.460, ICMA(Galactica-125M)=0.391. Validity (generated molecules) for ICMA(Mistral-7B) reached 0.958 on ChEBI-20. ICMA also improved performance on PubChem324k and on MoleculeNet property classification (average improvements, e.g., Mistral-7B improved markedly under ICMA).",
            "comparison_to_other_methods": "Compared with domain-specific pretraining & fine-tuning approaches (MolT5, BioT5, MolXPT, MolCA) and prompting-only approaches (MolReGPT on GPT-3.5/GPT-4), ICMA achieved state-of-the-art or comparable results on many metrics without requiring large-scale chemical pretraining or modality-alignment stages. ICMA enabled smaller models (e.g., Galactica-125M) to outperform some larger domain-specific systems by leveraging retrieved context and in-context tuning.",
            "limitations_and_challenges": "Dependence on retrieval quality (Mole-BERT GNN and BM25 selection critical); input/context length constraints (cutoff lengths limit number/size of context examples); Random Walk introduces randomness (but empirically stable with small p_max); ICMA still requires fine-tuning of LLM parameters (though not domain pretraining) and benefits from larger backbones; lack of explicit novelty reporting versus training corpora and no explicit property-targeted optimization (e.g., binding affinity) in current experiments.",
            "uuid": "e8938.0",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Mole-BERT retrieval",
            "name_full": "Mole-BERT (pre-trained GNN used for molecule graph retrieval embeddings)",
            "brief_description": "A pre-trained graph neural network (GNN) encoder used to produce molecule graph embeddings for retrieval (used in ICMA's Hybrid Context Retrieval for Mol2Cap/Cap2Mol), shown superior to Morgan fingerprints and random retrieval in downstream tasks.",
            "citation_title": "Mole-bert: Rethinking pre-training graph neural networks for molecules",
            "mention_or_use": "use",
            "model_name": "Mole-BERT (GNN encoder)",
            "model_type": "Graph Neural Network (pre-trained molecular graph encoder)",
            "model_size": null,
            "training_data": "Pre-trained on millions of molecule graphs (as per Mole-BERT reference); used to produce 2D graph embeddings for retrieval.",
            "application_domain": "Retrieval support for molecule-caption translation and text-to-molecule generation — improves selection of context examples by structural similarity.",
            "generation_method": "Not a generative model itself; provides embeddings used to retrieve structurally similar molecules to condition LLM generation.",
            "novelty_of_chemicals": "Indirect effect: better retrieval leads to more informative contexts and improved ability of the LLM to generate molecules matching targets; no direct novelty statistics reported for Mole-BERT outputs.",
            "application_specificity": "Provides structure-aware retrieval so context examples emphasize relevant substructures/functional groups, enabling LLMs to produce molecules tailored to captured caption cues.",
            "evaluation_metrics": "Downstream improvements measured via Mol2Cap BLEU/ROUGE/METEOR and Cap2Mol EM/validity/fingerprint similarities when Mole-BERT used versus Morgan fingerprints or random retrieval.",
            "results_summary": "Mole-BERT based retrieval produced the best Mol2Cap metrics in ICMA ablations (BLEU-4=0.568 vs Morgan FTS=0.552 and Random=0.533 in one reported setting), demonstrating improved retrieval quality leading to better generation.",
            "comparison_to_other_methods": "Outperformed classical fingerprint-based retrieval (Morgan fingerprints + Dice similarity) and random retrieval for selecting context examples.",
            "limitations_and_challenges": "Relies on pre-trained GNN quality and coverage; retrieval errors or embedding mismatches can degrade downstream LLM tuning and generation.",
            "uuid": "e8938.1",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MolReGPT",
            "name_full": "MolReGPT (In-Context Few-Shot Molecule Learning via ChatGPT/GPT models)",
            "brief_description": "A prompting/in-context learning approach that uses large LLMs (e.g., GPT-3.5, GPT-4) with retrieved examples to perform molecule-caption translation and generation without fine-tuning; performance depends on model size and retrieval quality.",
            "citation_title": "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5-turbo and GPT-4 (used in MolReGPT baseline evaluations)",
            "model_type": "GPT-style autoregressive transformer",
            "model_size": null,
            "training_data": "Uses general LLM pretraining (proprietary OpenAI corpora) and few-shot prompts built from molecule–caption pairs; no domain-specific fine-tuning reported in this work.",
            "application_domain": "Molecule-caption translation (Mol2Cap) and text-to-molecule generation (Cap2Mol) via in-context few-shot prompting.",
            "generation_method": "Prompting / in-context few-shot learning with retrieved examples (no parameter updates performed on the LLM).",
            "novelty_of_chemicals": "Reported baseline metrics in this paper: Cap2Mol EM (ChEBI-20): MolReGPT (GPT-3.5-turbo) EM = 0.139; MolReGPT (GPT-4-0314) EM = 0.280; validity metrics and fingerprint similarities reported in tables.",
            "application_specificity": "Relies on high-capacity LLM emergent capabilities and retrieval quality to match captions to target molecules; not explicitly optimized for specific properties.",
            "evaluation_metrics": "Same task metrics as ICMA: BLEU, EM, Levenshtein, MACCS/RDK/Morgan fingerprint similarities, validity, etc.",
            "results_summary": "MolReGPT produced competitive results among prompting-only approaches but required very large LLMs (GPT-3.5/GPT-4) to achieve competitive performance; ICMA outperformed or matched MolReGPT on multiple metrics while being applicable to smaller backbones through in-context tuning.",
            "comparison_to_other_methods": "MolReGPT is a prompting-only baseline contrasted with ICMA's retrieval + in-context fine-tuning; MolReGPT depends more heavily on large model scale.",
            "limitations_and_challenges": "High dependence on emergent capabilities of very large LLMs; poor performance when applied to smaller LLMs; no parameter adaptation limits how much model can learn alignment from examples.",
            "uuid": "e8938.2",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5",
            "brief_description": "A T5-based model pre-trained jointly on molecule SMILES and general text corpora and fine-tuned for molecule-caption translation; represents the domain-specific pretraining & fine-tuning paradigm.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "mention",
            "model_name": "MolT5 (base, large variants)",
            "model_type": "T5 transformer (encoder–decoder)",
            "model_size": "MolT5-large reported as &gt;780M parameters (paper states more than 780M)",
            "training_data": "Domain-specific pretraining on chemical corpora (e.g., PubChem) combined with general text; fine-tuned on ChEBI-20 molecule-caption pairs.",
            "application_domain": "Molecule-caption translation (Mol2Cap and Cap2Mol) for chemistry / drug discovery contexts.",
            "generation_method": "Domain-specific pretraining followed by supervised fine-tuning on molecule-caption translation pairs to generate captions or SMILES from text.",
            "novelty_of_chemicals": "Performance metrics reported (MolT5-large: Cap2Mol EM=0.311 on ChEBI-20; Mol2Cap BLEU-4=0.508), but explicit novelty vs training set not provided.",
            "application_specificity": "Specificity arises from domain pretraining on chemical corpora and supervised fine-tuning on paired molecule-caption datasets.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR for Mol2Cap; BLEU, EM, Levenshtein, fingerprint similarities, validity for Cap2Mol.",
            "results_summary": "MolT5 (large) achieves strong baseline performance (Mol2Cap BLEU-4=0.508; Cap2Mol EM=0.311) but requires large domain-specific pretraining and modality alignment.",
            "comparison_to_other_methods": "Compared in the paper as a domain-pretrained baseline; ICMA enabled general LLMs (with in-context tuning) to reach or exceed MolT5 performance on several metrics without domain pretraining.",
            "limitations_and_challenges": "Requires extra domain-specific pretraining and modality alignment; scaling to very large LLM backbones is resource intensive given scarcity of high-quality molecule-caption corpora.",
            "uuid": "e8938.3",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "BioT5",
            "name_full": "BioT5",
            "brief_description": "A model that enriches cross-modal biological integration using chemical knowledge and natural language associations; uses SELFIES to mitigate invalid SMILES generation and is trained with molecule-description pairs.",
            "citation_title": "Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "mention_or_use": "mention",
            "model_name": "BioT5",
            "model_type": "T5-style transformer adapted for biological/chemical multimodal data",
            "model_size": null,
            "training_data": "Joint training on single-modal data, wrapped text data, and molecule/protein-description pairs; uses SELFIES representation for molecules.",
            "application_domain": "Molecule-caption translation, molecule generation for biological/chemical applications.",
            "generation_method": "Domain-specific pretraining and fine-tuning; SELFIES used to ensure 100% robustness in molecular string representation.",
            "novelty_of_chemicals": "Reported Cap2Mol metrics in comparison: BLEU=0.867, EM=0.411 on ChEBI-20 (as reported in the comparative table); explicit novelty fraction not provided.",
            "application_specificity": "Tailored via chemical-domain pretraining and cross-modal integration to better align molecule representations with natural language descriptions.",
            "evaluation_metrics": "BLEU, EM, Levenshtein, fingerprint similarity, validity.",
            "results_summary": "BioT5 is a strong domain-specific baseline for Cap2Mol, achieving high BLEU and EM in the comparative table; ICMA achieved similar or better fingerprint similarity scores and competitive EM without domain pretraining.",
            "comparison_to_other_methods": "Compared favorably to MolXPT and ICMA on some metrics; ICMA matched or outperformed BioT5 on fingerprint similarity while not requiring domain pretraining.",
            "limitations_and_challenges": "Requires domain-specific corpora and pretraining; may have higher resource needs and limited scalability to larger LLM backbones without significant compute/data.",
            "uuid": "e8938.4",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MolXPT",
            "name_full": "MolXPT",
            "brief_description": "A generative pre-training approach that ‘wraps’ molecules with text to improve molecule–text alignment during pretraining for downstream translation/generation tasks.",
            "citation_title": "Molxpt: Wrapping molecules with text for generative pre-training",
            "mention_or_use": "mention",
            "model_name": "MolXPT",
            "model_type": "Transformer-based generative pre-training model",
            "model_size": null,
            "training_data": "Pretrained on wrapped text representations of molecules and text corpora to improve molecule-text alignment.",
            "application_domain": "Molecule-caption translation and generation.",
            "generation_method": "Generative pre-training on paired/wrapped molecule-text data followed by fine-tuning on downstream tasks.",
            "novelty_of_chemicals": "Reported Cap2Mol performance in comparisons (e.g., BLEU and EM) but explicit novelty metrics not shown in this paper.",
            "application_specificity": "Aims to align molecular representations and text during pretraining to enable more accurate text-to-molecule generation.",
            "evaluation_metrics": "BLEU, EM, fingerprint similarities, validity.",
            "results_summary": "MolXPT is competitive among domain-aligned pretraining methods; ICMA compared favorably without requiring such pretraining.",
            "comparison_to_other_methods": "Compared against MolT5 and BioT5; ICMA achieved higher fingerprint similarity metrics on some runs while avoiding large-scale chemical pretraining.",
            "limitations_and_challenges": "Requires dedicated pretraining on molecule-text wrapped data; scalability and data availability are potential constraints.",
            "uuid": "e8938.5",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MolCA",
            "name_full": "MolCA",
            "brief_description": "A multimodal method that encodes 2D molecular graphs and projects them into an LLM via a Q-Former (crossmodal projector) to improve molecule understanding and captioning.",
            "citation_title": "Molca: Molecular graph-language modeling with crossmodal projector and uni-modal adapter",
            "mention_or_use": "mention",
            "model_name": "MolCA",
            "model_type": "Multimodal model combining GNN graph encoder + Q-Former + LLM",
            "model_size": null,
            "training_data": "Uses molecule graph encoders and paired molecule–caption corpora for cross-modal alignment.",
            "application_domain": "Mol2Cap (molecule captioning) primarily; multimodal molecule understanding.",
            "generation_method": "Graph encoder outputs projected into LLM input via a learned crossmodal Q-Former to align graph and text modalities.",
            "novelty_of_chemicals": "Focused on captioning and understanding rather than de novo molecule generation; not directly evaluated for novelty metrics in this paper.",
            "application_specificity": "Improves Mol2Cap by explicit graph–language modality alignment; operates at whole-graph level.",
            "evaluation_metrics": "Mol2Cap metrics (BLEU, ROUGE, METEOR) used for comparison.",
            "results_summary": "MolCA improves molecular understanding for captioning but its modality alignment targets Mol2Cap rather than Cap2Mol; ICMA achieved comparable or better Mol2Cap BLEU/METEOR without modality alignment training.",
            "comparison_to_other_methods": "Compared as a modality-alignment approach; ICMA differs by avoiding explicit modality-alignment training and by enabling Cap2Mol improvements.",
            "limitations_and_challenges": "Primarily focused on Mol2Cap (captioning) and whole-graph alignment; less directly applicable to text-driven de novo molecule generation (Cap2Mol) without additional design.",
            "uuid": "e8938.6",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Mistral-7B (as used)",
            "name_full": "Mistral-7B (instruct-v0.2 variant used in experiments)",
            "brief_description": "A high-capacity general-purpose LLM used as a backbone in ICMA; when adapted with ICMA it produced state-of-the-art Cap2Mol and Mol2Cap results in this work.",
            "citation_title": "Mistral 7b",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (instruct-v0.2)",
            "model_type": "Transformer-based autoregressive LLM",
            "model_size": "7B parameters",
            "training_data": "General LLM pretraining (not chemical-specific); in experiments it was fine-tuned/adapted using ICMA on molecule-caption datasets (ChEBI-20, PubChem324k) with LoRA for large weights.",
            "application_domain": "Molecule-caption translation (Mol2Cap) and text-to-molecule generation (Cap2Mol) as adapted by ICMA; also applied to MoleculeNet classification tasks after adaptation.",
            "generation_method": "ICMA In-Context Molecule Tuning (retrieval-augmented fine-tuning) to generate SMILES from captions and captions from SMILES.",
            "novelty_of_chemicals": "Measured via exact-match and fingerprint similarity metrics (e.g., Cap2Mol EM on ChEBI-20 after ICMA = 0.460; fingerprints MACCS=0.916, RDK=0.837, Morgan=0.789), explicit novelty fraction to training set not reported.",
            "application_specificity": "Context-driven (retrieval + re-ranking) ensures generation reflects attributes described in captions; no explicit target-binding optimization performed.",
            "evaluation_metrics": "BLEU, EM, Levenshtein, fingerprint similarities (MACCS/RDK/Morgan), validity, ROUGE, METEOR.",
            "results_summary": "ICMA(Mistral-7B) achieved best reported numbers in this study: Mol2Cap BLEU-4=0.581, METEOR=0.661; Cap2Mol EM=0.460, validity=0.958, and top fingerprint similarities among tested systems on ChEBI-20.",
            "comparison_to_other_methods": "Outperformed naive supervised fine-tuning of the same backbone and matched/exceeded many domain-specific pretraining approaches; surpassed prompting-only MolReGPT baselines on several metrics.",
            "limitations_and_challenges": "Still benefits from LoRA and compute to tune 7B parameters; performance sensitive to retrieval quality, context length settings, and Random Walk hyperparameters; no targeted property optimization (e.g., binding) or experimental synthesis validation provided.",
            "uuid": "e8938.7",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Galactica-125M (as used)",
            "name_full": "Galactica-125M",
            "brief_description": "A smaller scientific-domain pretrained LLM used as an ICMA backbone; pretraining on scientific corpora gives it some molecular knowledge that ICMA further refines via in-context tuning to improve molecule captioning and generation.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "use",
            "model_name": "Galactica-125M",
            "model_type": "Transformer-based LLM (scientific-pretrained)",
            "model_size": "125M parameters",
            "training_data": "Pretrained on scientific corpora (per Galactica); ICMA fine-tuned on ChEBI-20 and PubChem324k for experiments in the paper.",
            "application_domain": "Molecule-caption translation (Mol2Cap) and Cap2Mol generation; also tested on MoleculeNet tasks.",
            "generation_method": "ICMA pipeline (retrieval + post-retrieval re-ranking + in-context molecule tuning) to generate captions or SMILES.",
            "novelty_of_chemicals": "Improved Cap2Mol EM under ICMA on ChEBI-20: ICMA(Galactica-125M) EM = 0.391; fingerprint similarities and validity also improved; no explicit novelty-vs-training-set statistics.",
            "application_specificity": "Relies on retrieved similar examples (Mole-BERT/BM25) to teach the model caption↔substructure mappings; not explicitly property-targeted.",
            "evaluation_metrics": "Same metrics used across the study: BLEU, ROUGE, METEOR (Mol2Cap) and BLEU, EM, Levenshtein, fingerprint similarities, validity (Cap2Mol).",
            "results_summary": "ICMA raised Galactica-125M performance significantly (e.g., Mol2Cap BLEU-4 increased from 0.501 to 0.565 in reported runs) and allowed a small 125M model to beat larger domain-specific models on some metrics by leveraging context adaptation.",
            "comparison_to_other_methods": "After ICMA, Galactica-125M outperformed the naive fine-tuned version and some previous domain-specific baselines (e.g., MolT5-base in some metrics) without extra domain pretraining.",
            "limitations_and_challenges": "Smaller model size limits natural language generation capability (observed caps on Mol2Cap quality in some settings); constrained by input cutoff lengths limiting context examples; displays less robust in-context reasoning than larger models, making ICMA tuning important.",
            "uuid": "e8938.8",
            "source_info": {
                "paper_title": "Large Language Models are in-Context Molecule Learners",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Mole-bert: Rethinking pre-training graph neural networks for molecules",
            "rating": 2
        },
        {
            "paper_title": "Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "rating": 2
        },
        {
            "paper_title": "Molca: Molecular graph-language modeling with crossmodal projector and uni-modal adapter",
            "rating": 2
        },
        {
            "paper_title": "Molxpt: Wrapping molecules with text for generative pre-training",
            "rating": 2
        },
        {
            "paper_title": "Synergpt: In-context learning for personalized drug synergy prediction and drug design",
            "rating": 1
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 1
        }
    ],
    "cost": 0.023315499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are In-Context Molecule Learners</h1>
<p>Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, and Qing Li</p>
<p><strong>Abstract</strong>—Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve similar informative context examples. Additionally, Post-retrieval Re-ranking is composed of Sequence Reversal and Random Walk selection to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context learning and reasoning capability of LLMs with the retrieved examples and adapts the parameters of LLMs for better alignment between molecules and texts. Experimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.</p>
<p><strong>Index Terms</strong>—Drug Discovery, Large Language Models (LLMs), In-context Tuning, Retrieval Augmented Generation.</p>
<h2>I. Introduction</h2>
<p>Molecules play a crucial role across various fields, such as medicine [1], agriculture [2], and material science [3], as they are widely used in the development of drugs, fertilizers, and advanced materials. Recently, LLMs have demonstrated remarkable success in the molecular domain, as molecules can be represented as Simplified Molecular-Input Line-Entry System (SMILES) strings [4], which can be comprehended and generated by LLMs in a similar manner to natural languages. To further bridge the gap between the molecules and natural languages, MolT5 [5] proposes the molecule-caption translation task, which comprises two sub-tasks: molecule captioning (Mol2Cap) and text-based de novo molecule generation (Cap2Mol). Specifically, Mol2Cap involves generating a textual description that elucidates the features of the given molecule, while Cap2Mol focuses on predicting the exact molecule based on the textual caption. The study of the molecule-caption translation task offers an accessible and chemist-friendly venue for molecule discovery, which has raised wide research focus.</p>
<p>Generally, there are two main paradigms for adapting LLMs to the molecule-caption retrieval task. The first paradigm is the domain-specific pre-training &amp; fine-tuning. For instance, MolT5 [5] first proposes and handles the molecule-caption translation task as the language translation task, pre-training the MolT5 model with chemical corpora like PubChem [6] and then fine-tuning the model on the ChEBI-20 dataset [7]. Additionally, MoMu [8] and MolCA [9] introduce an extra modality alignment stage before fine-tuning on downstream tasks, which aligns the output of 2D molecule graph encoder with the input space of LLMs. In contrast, the other paradigm involves prompting and utilizing the in-context learning capability of LLMs. For example, MolReGPT [10] introduces In-Context Few-Shot Molecule Learning, prompting general LLMs like GPT-3.5 and GPT-4 to achieve competitive performance without extra parameter adaptations.</p>
<p>However, the current paradigms face critical challenges. <strong>On one hand</strong>, the domain-specific pre-training &amp; fine-tuning paradigm requires extra pre-training stages (i.e., domain-specific pre-training and modality alignment), which is challenging due to the scarcity of high-quality chemical datasets, especially molecule-caption pairs, making this paradigm infeasible to scale up to the most advanced LLMs with billion parameters. Besides, the domain-specific pre-training &amp; fine-tuning paradigm also suffers from weak alignment between</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Fig. 1.</strong> An illustration of three similar molecules alongside their molecule captions. The molecules are represented as both SMILES strings and graphs, while the molecule captions elucidate their structures and functions. Here, the three molecules are similar, considering their 2D graph embeddings, and the overlaps in their captions are highlighted in blue and pink.</p>
<p>J. Li, Z. Ding, W. Fan, and Q. Li are with the Department of Computing, The Hong Kong Polytechnic University. E-mail: {jiatong.li, tommy-zh.ding}@connect.polyu.hk, wenqifan03@gmail.com, csqli@comp.polyu.edu.hk.</p>
<p>W. Liu is with Shanghai Jiao Tong University. E-mail: captain.130@sjtu.edu.cn.</p>
<p>Y. Li is with Shanghai AI Lab. E-mail: liyuqiang@pjlab.org.cn.</p>
<p>molecules and texts, as phrases in molecule captions often indicate specific sub-structures of molecules rather than the entire molecule. Despite attempts to introduce extra modalities for better alignment [8], [9], the integration of the additional modalities (e.g., 2D molecule graph) is still focused on the entire graph level and can only be applied to the Mol2Cap task, while ignoring the text-based generation of molecules, which is much more valuable for drug discovery. On the other hand, the in-context learning \&amp; prompting paradigm puts a harsh requirement on LLMs' emergent capabilities, such as reasoning and in-context learning abilities. However, LLMs with these emergent capabilities usually have billions of parameters, making them computationally expensive. Consequently, there is a demand for a unified and efficient approach that effectively enhances the performance of the most advanced LLMs in both two sub-tasks of molecule-caption translation.</p>
<p>In this case, we propose In-Context Molecule Adaptation (ICMA) as a new paradigm for adapting LLMs in moleculecaption translation. Different from previous paradigms, ICMA aims to instruct LLMs to derive knowledge from informative context examples, especially the alignment between molecule SMILES representations and captions, via In-Context Molecule Tuning. As shown in Figure 1, similar molecules often share similar properties, as indicated by the overlaps among molecule captions. Conversely, similar captions tend to describe molecules with similar SMILES representations. In this case, with ICMA, general LLMs could fulfill their reasoning and in-context learning capability to better grasp the alignment between molecules and textual captions from context examples, thereby achieving better performance.</p>
<p>Specifically, ICMA incorporates three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. In the initial stage, Hybrid Context Retrieval, we employ Caption Retrieval and Molecule Graph Retrieval to fetch similar captions and molecules, respectively. Subsequently, we introduce the Post-retrieval Re-ranking stage to enhance the quality of the retrieval algorithms. This stage incorporates two innovative strategies: Sequence Reversal and Random Walk, which aim to refine and reprioritize the retrieved examples. Finally, we apply In-context Molecule Tuning to adapt the parameters of LLMs, enabling them to learn from the contextual mappings and effectively ground the current generation task. Experiments are conducted across two real-world moleculecaption translation datasets, ChEBI-20 and PubChem324k. Results show that ICMA could enable LLMs to achieve state-of-the-art or comparable performance in both the two sub-tasks (i.e., Mol2Cap and Cap2Mol). Meanwhile, we also study the factors related to the model performance, including retrieval algorithms, context settings (i.e., context example number and maximum input length), model scales, and backbone LLMs. Lastly, the ablation study and detailed case study are conducted to justify the effectiveness of Post-retrieval Reranking components.</p>
<p>Our contribution mainly lies in:</p>
<ul>
<li>We propose In-context Molecule Adaptation (ICMA) to improve the performance of LLMs in the molecule-caption translation task. ICMA could empower the reasoning
and in-context learning capabilities of LLMs for better alignment between molecules and texts.</li>
<li>We implement ICMA through three stages, including Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-Context Molecule Tuning, significantly enhancing the informativeness of context examples.</li>
<li>We conduct synthetic experiments, and the results show that our method enables LLMs to outperform previous paradigms, enabling better alignment between molecules and texts. Notably, our approach elevates general LLM like Mistral-7B to establish superior performance across both the two sub-tasks of molecule-caption translation, achieving 0.581 BLEU-4 score in Mol2Cap and 0.460 exact-matched score in Cap2Mol. Additionally, we comprehensively study the mechanism and influential factors of ICMA, showing that LLMs are inherently in-context molecule learners.</li>
</ul>
<h2>II. Related Work</h2>
<p>In this section, we discuss the related work of moleculecaption translation and the development of in-context learning.</p>
<h2>A. Molecule-Caption Translation</h2>
<p>Inspired by the image captioning task, Edwards et al. [7] introduce a new dataset, ChEBI-20, with pairs of molecules and manually labeled captions that describe the molecular properties. The molecule-caption translation task was initially proposed in MolT5 [5]. Meanwhile, MolT5 proposes a T5 model that is jointly pre-trained on molecule SMILES and general text corpus. MolXPT [11] pre-trains a GPT model by introducing extravagped texts as the pre-training corpus, demonstrating better molecule-text alignment. However, the generation of SMILES strings suffers from the problem of invalid SMILES due to the mismatches of brackets. To overcome the generation issue of SMILES strings, BioT5 [12] introduces Self-referencing Embedded Strings (SELFIES) [13] instead of SMILES strings to represent molecules in LLMs and proposes the BioT5 model that is jointly trained on single-modal data, wrapped text data, and molecule/protein-description pairs. Meanwhile, as molecules can also be represented as graphs, some methods focus on molecule understanding by introducing molecule graph information. For example, MoMu [8] first proposes a graph encoder to encode molecule graph information and utilizes contrastive learning to bridge the semantic gap between the graph encoder and the LLM. To better fuse the molecule graph information, MolCA [9] follows the BLIP-2 [14] and utilizes a Q-Former to project the output of the graph encoder into the LLMs, showing better molecule understanding performance. However, most of these methods still adhere to the pre-training \&amp; fine-tuning paradigm, which necessitates the joint pre-training of LLMs on both general text and extra chemical domain corpora. As the size of the training corpora and model weights of LLMs continue to increase, this approach has become extremely inefficient. To address this issue, MolReGPT [10] proposes the In-Context Few-Shot Molecule Learning to enable LLMs to learn the moleculecaption translation task from the context examples without</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Framework of In-Context Molecule Adaptation (ICMA). Generally, ICMA consists of three stages, Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-Context Molecule Tuning.</p>
<p>modifying the model weights while still achieving comparable performance to these fine-tuned methods.</p>
<h3><em>B. In-Context Learning</em></h3>
<p>With the scaling of model size and corpus size [15], [16], LLMs emerge the in-context learning capability [17], which enables LLMs to learn from contexts augmented with several examples [18]. By utilizing the capability of ICL, LLMs can solve complex tasks without the necessity of being fine-tuned. For instance, with a few examples, GPT-3 could demonstrate similar performance to fine-tuned models in unseen tasks [15]. What's more, based on context examples, LLMs could achieve better mathematical reasoning ability with the assistance of chain-of-thought (CoT) [19]. With the powerful in-context learning capabilities, Edwards et al. [20] propose in-context drug synergy learning to apply LLMs for personalized drug synergy prediction and drug design, while Jablonka et al. [21] fine-tunes LLMs such as GPT-3 with chemical questions and answers to solve predictive tasks. In the scenario of molecule-caption translation, MolReGPT [10] proves the retrieval quality is closely related to the model performance, while it still imposes strict requirements on the model scale, as the in-context learning capability only becomes apparent when the model weights reach a certain size.</p>
<h3>III. IN-CONTEXT MOLECULE ADAPTATION</h3>
<p>In this section, we introduce In-Context Molecule Adaptation (ICMA) as a novel paradigm to adapt LLMs to molecule-caption translation. As shown in Figure 2, ICMA incorporates three stages, including Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Specifically, Hybrid Context Retrieval first retrieves <em>N</em> rough examples from the training set <em>D</em> by calculating the similarity between the current query and the molecule-caption pairs from the training set. After that, Post-retrieval Re-ranking with Random Walk and Sequence Reversal is adopted to obtain <em>n</em> refined examples from the <em>N</em> rough examples. Finally, In-Context Molecule Tuning can be performed to update the parameters of LLMs to learn the molecule-text alignment from refined context examples.</p>
<h3><em>A. Hybrid Context Retrieval</em></h3>
<p>The retrieval quality is closely related to the informativeness of context examples. For example, if the retrieved molecules are more similar to the current query molecule, they are likely to exhibit more overlaps in their respective caption, which could enable better alignments between molecules and texts. Therefore, the development of retrieval algorithms plays a crucial role in ICMA. In this work, we introduce Hybrid Context Retrieval, which adopts hybrid modalities (i.e., 2D molecule graph and text), as well as hybrid retrieval algorithms designed for the specific tasks (i.e., Molecule Graph Retrieval for Mol2Cap and BM25 Caption Retrieval for Cap2Mol).</p>
<p>For the <strong>Mol2Cap</strong> task, ICMA adopts Molecule Graph Retrieval to better refine the retrieval quality. Previously, MolReGPT [10] utilizes the Morgan Fingerprints (Morgan FTS) and Dice similarity to evaluate the similarity between molecules, which encodes the pre-defined handcraft structures as the embedding of the molecule. However, Morgan FTS are typically based on handcrafted chemical feature extraction, which may have limited capability to capture the comprehensive information of complex molecule structures and properties. On the other side, Graph Neural Networks (GNNs) have been widely used to capture topological structures [22]. Pre-trained on millions of molecule graphs, GNNs could better understand the molecular structure, providing complete chemical semantics [23]. This makes GNNs a better option for molecule similarity calculation. In ICMA, we adopt a pre-trained GNN encoder to obtain the molecule graph embeddings:</p>
<p>$$\mathbf{e}_m = GNN(g_m), \tag{1}$$</p>
<p>where <strong>e</strong><em>m</em> denotes the embedding of the given 2D graph <em>g</em><em>m</em> of the molecule <em>m</em>. Specifically, we adopt Mole-BERT [24] as the GNN encoder.</p>
<p>Subsequently, cosine similarity is leveraged to evaluate the similarity between the current query molecule graph <em>m</em><em>g</em> and the other molecule graphs <em>m</em><em>t</em> in the training set of molecules (<em>m</em><em>t</em> ∈ <em>D</em><em>m</em>). Thus, the molecule similarity ranking function <em>R</em><em>m</em> can be represented as:</p>
<p>$$\mathcal{R}^{m}(m^q, m^i) = \cos(\mathbf{e}<em m_i="m^i">{m^q}, \mathbf{e}</em>$$}). \tag{2</p>
<p>In the Cap2Mol task, we inherit the BM25 Caption Retrieval [25] from MolReGPT [10] as it focuses on the detail matching of molecule captions, showing competitive performance and is much faster than LLM-based methods like Sentencebert [26]. Specifically, given captions in the test set as query captions $Q_{c}$ and the training set of captions $\mathcal{D}_{c}$, the caption similarity ranking function $\mathcal{R}^{c}$ can be denoted as:</p>
<p>$$
\mathcal{R}^{c}\left(Q_{c}, \mathcal{D}<em i="1">{c}\right)=\sum</em>}^{T} I D F\left(c_{i}^{q}\right) * \frac{t f\left(c_{i}^{q}, \mathcal{D<em c="c">{c}\right) <em>\left(k_{1}+1\right)}{t f\left(c_{i}^{q}, \mathcal{D}<em 1="1">{c}\right)+k</em> </em>\left(1-b+b * \frac{\left|\mathcal{D}</em>
$$}\right|}{avg d l}\right)</p>
<p>where $T$ is the number of query terms in the query caption, $c_{i}^{q}$ is the $i$-th query term, $I D F\left(c_{i}^{q}\right)$ is the inverse document frequency of $c_{i}^{q}, t f\left(c_{i}^{q}, \mathcal{D}<em i="i">{c}\right)$ is the term frequency of $c</em>}^{q}$ in $\mathcal{D<em 1="1">{c}$, $k</em>}$ and $b$ are hyperparameters, $\left|\mathcal{D<em c="c">{c}\right|$ is the length of $\mathcal{D}</em>$, and avgdl is the average caption length in the corpus.</p>
<h2>B. Post-retrieval Re-ranking</h2>
<p>Although refined retrieval algorithms could bring better retrieval quality, there are still some problems considering the arrangement of context examples. Thus, we propose Postretrieval Re-ranking with Random Walk and Sequence Reversal to re-rank the priorities and positions of context examples, thus enhancing the quality of In-Context Molecule Tuning.</p>
<p>1) Random Walk: The molecules ranked top by retrieval algorithms can sometimes share too many overlaps, impairing the informativeness of context examples. In this case, for the context diversity and generalization performance of ICMA, it is necessary to give these less similar examples a chance to be visited. Inspired by the Random Walk mechanism in graph theory, we propose Random Walk as a post-retrieval method to select examples from the top- $N$ retrieved results so that examples with lower rank still have a chance to be selected, which provides more useful information and complements the context diversity. Mathematically, we adopt a dynamic chance for examples with different ranks that gradually decays as the rank moves down. Specifically, for the $j$-th example in the $N$ rough results, where $1 \leq j \leq \mathrm{N}$, the possibility of skipping $p(j)$ is represented as:</p>
<p>$$
p(j)=p_{\max } * \frac{N-j}{N-1}
$$</p>
<p>where $p_{\max }$ is the maximum skip probability. It can be seen that $p(j)$ will decay to $0 \%$ at the $N$-th example. This guarantees that the sampling stage will not result in an empty selection. Once an example is selected, the subsequent selection process begins from the next example in the sequence, which means that if the $j$-th example is chosen, the next potential selection starts from the $(j+1)$-th example.</p>
<p>Consequently, if the $N$-th example is selected, the Random Walk sampling terminates early, which indicates that we may fail to achieve the intended number of $n$ refined examples. Therefore, we need to make sure that this early-stop condition is scarcely activated so that the integrity of sampling can be guaranteed. Empirically, the skipping probability should not be too large to avoid unstable training. Meanwhile, if $N$ is too large (i.e., $N \gg n$ ), the retrieval quality will be impaired, while if $N$ is too small (i.e., $N \approx n$ ), it might hurt the diversity.</p>
<p>Therefore, as $n$ is normally less than 5 due to the context length limitation, $N$ is set to 10 as a balanced choice. For simplification, the maximum skip probability $p_{\max }$ is then set to $(N-1) \%=9 \%$ in this work so that $p(j)$ could be simply written as $(N-j) \%$. In this case, let $n=2$ and $N=10$, there is only a likelihood of $\frac{(N-1)!}{\left|100^{N-1}\right|}=3.6288 e^{-13}$ that the early-stop is activated, which is nearly indistinguishable from zero, thus generally maintaining the sampling integrity.
2) Sequence Reversal: Due to the training strategy and the inherent characteristic of natural languages, LLMs have difficulty capturing long-range dependencies or relationships between words that are far apart in the input text, namely the distance dependency. The positions of examples in the context might influence the generation results due to the distance dependency of LLMs [27]. In this case, it is of significance to put the most informative example exactly near the current input query.</p>
<p>Formally, given the context examples, previous works like MolReGPT tend to organize the input text by directly fitting them into the context. Therefore, the context could be represented as:</p>
<p>$$
\mathcal{P}\left(x_{1}, y_{1}\right) \oplus \mathcal{P}\left(x_{2}, y_{2}\right) \oplus \ldots \oplus \mathcal{P}\left(x_{n}, y_{n}\right)
$$</p>
<p>where $\mathcal{P}$ denotes the prompt template and $\left(x_{i}, y_{i}\right)$ is the $i$-th refined similar molecule-caption pair, while $\oplus$ represents the concatenation. Obviously, $\left(x_{n}, y_{n}\right)$ is generally the least informative molecule-caption pair among $n$ refined examples, while it is the closest example to the current input query. Here, we propose Sequence Reversal to resolve this question by simply reversing the sequence of examples. Specifically, the context can be represented as:</p>
<p>$$
\mathcal{P}\left(x_{n}, y_{n}\right) \oplus \mathcal{P}\left(x_{n-1}, y_{n-1}\right) \oplus \ldots \oplus \mathcal{P}\left(x_{1}, y_{1}\right)
$$</p>
<h2>C. In-Context Molecule Tuning</h2>
<p>As shown in Figure 1, similar molecules typically share similar structures and chemical properties. Building on this principle, MolReGPT [10] has demonstrated the effectiveness of in-context learning, which aims to prompt Large Language Models (LLMs) to learn from similar examples without the need for domain-specific pre-training and fine-tuning. However, MolReGPT heavily relies on the reasoning and in-context learning capabilities of LLMs, resulting in poor performance with relatively small language models. To address the deficits , we propose In-Context Molecule Tuning to fine-tune the parameters of LLMs, enabling them to learn from context examples and reason on the current input. Notably, In-Context Molecule Tuning can be easily adapted to any LLM, allowing even smaller language models to unlock their in-context molecule learning capability by learning the differences and similarities between molecule-caption pairs in the context.</p>
<p>Formally, given the training dataset $\mathcal{D}$ and the parameters of the LLM $\theta$, let the current input of the LLM be $x$ and the target output be $y$, where $(x, y) \in D$ denotes the molecule-caption pair. Traditional supervised fine-tuning methods directly learn</p>
<p>the mapping from the input to the output $f: x \rightarrow y$ and the loss function $\mathcal{L}^{f t}(\theta)$ could be denoted as:</p>
<p>$$
\mathcal{L}^{f t}(\theta)=\sum_{(x, y) \in \mathcal{D}}\left[-\log p_{\theta}(y \mid x)\right]
$$</p>
<p>In contrast, ICMA does not simply conduct the next token prediction on the output part but learns the entire input in an auto-regressive manner. ICMA first employs the Hybrid Context Retrieval and Post-retrieval Re-ranking to obtain a subset $D_{(x, y)} \subset D$ containing $n$ similar examples $\left{\left(x_{i}, y_{i}\right) \mid 1 \leq\right.$ $i \leq n}$, from the training set. Different from the previous ICL objective, ICMA is also motivated to learn the mapping $f_{i}: x_{i} \rightarrow y_{i}$ inside the context examples in an obvious manner. Notably, molecule-caption mapping is the most informative part in the context examples because similar molecule substructures inherit similar characteristics. For example, RCOOH (carboxyl group) usually indicates that the molecule is an acid. In this way, learning the alignment between functional groups and molecule captions in the context examples could benefit the final prediction. For simplicity, we could assume that context examples are independent of each other as the molecule-caption mapping plays the most important role in the prediction. In this case, the aggregation of mappings $\mathcal{F}<em 1="1">{(x, y)}=\left{f</em>$, which wraps the context examples into the input text. Therefore, the objective of ICMA can be represented as:}, f_{2}, \ldots, f_{n}\right}$ could be learned from context and will altogether contribute to the final prediction with the corresponding context $C_{(x, y)</p>
<p>$$
\begin{gathered}
\mathcal{L}\left(\mathcal{F}<em _left_x__i="\left(x_{i">{(x, y)}\right)=\sum</em>\right) \
\mathcal{L}^{K \mathcal{M A}}(\theta)=\sum_{(x, y) \in D}\left(-\log p_{\theta}\left(y \mid x, C_{(x, y)}\right)+\mathcal{L}\left(\mathcal{F}_{(x, y)}\right)\right)
\end{gathered}
$$}, y_{i}\right) \in D_{(x, y)}}-\log p_{\theta}\left(y_{i} \mid x_{i</p>
<p>where $\mathcal{L}\left(\mathcal{F}_{(x, y)}\right)$ represents the aggregated mapping loss for molecule-caption pair $(x, y)$, while $\mathcal{L}^{K \mathcal{M A}}(\theta)$ denotes the overall loss function.</p>
<p>By learning the context examples as well as the corresponding mappings, ICMA enables LLMs to learn the alignment between molecular and textual spaces in a more explainable manner. Moreover, ICMA could effectively harness the reasoning capabilities of LLMs and seamlessly adapt general LLMs to the task of molecule-caption translation.</p>
<h2>IV. EXPERIMENTS</h2>
<p>In this section, we aim to evaluate the effectiveness of ICMA. Firstly, we introduce the experimental settings. Then, we compare ICMA with the selected baselines on the ChEBI-20 dataset and further test ICMA on a smaller dataset, PubChem324k. Meanwhile, we also comprehensively study the factors that will affect the performance of ICMA, including retrieval algorithms, context settings, model scales, backbone models, complexity of data samples, and the Random Walk sampling strategy. After that, an ablation study is conducted to justify the design of Postretrieval Re-ranking components. What's more, we compare ICMA with models that require extra-domain alignment stages to demonstrate the efficiency and effectiveness of ICMA. Furthermore, we verify ICMA on molecule property prediction
tasks, illustrating the generalization capability of ICMA to other molecule-related tasks. Finally, we conduct a case study to provide more details about our method.</p>
<h2>A. Experimental Settings</h2>
<p>We will first detail our experiment settings. All the hyperparameters are illustrated in Table I. If not specifically stated, the cutoff length that truncates the inputs for LLMs to process is set to 1024 , and n_shot is set to 2 to control the variables. Notably, for LLMs with over 1 billion parameters, we apply LoRA [28] to save the GPU memory and accelerate computation. Otherwise, we fine-tune the full model of LLMs. For the dataset, we apply two different molecule-caption translation datasets, ChEBI-20 [7] and PubChem324k [9]. The details of the datasets are shown in Table II.</p>
<p>TABLE I
HYPER-PARAMETERS</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Item</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">epochs</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">warm-up steps</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">cutoff length</td>
<td style="text-align: center;">$512,1024,1536,2048$</td>
</tr>
<tr>
<td style="text-align: center;">refined examples $(n)$</td>
<td style="text-align: center;">$1,2,3,4$</td>
</tr>
<tr>
<td style="text-align: center;">rough examples $(N)$</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">maximum skip probability ( $p_{\max }$ )</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$2 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">lora_r</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">lora_alpha</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora_dropout</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">int8</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">fp16</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">temperature</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: center;">top_p</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;">top_k</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">num_beams</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">max_new_tokens</td>
<td style="text-align: center;">256</td>
</tr>
</tbody>
</table>
<p>TABLE II
Details of the datasets, ChEBI-20 and PubChem324k. For PubChem324k, we follow the split in MolCA [9], while ignoring the Pretrain FOLD.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Validation</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChEBI-20</td>
<td style="text-align: center;">26,407</td>
<td style="text-align: center;">3,001</td>
<td style="text-align: center;">3,000</td>
</tr>
<tr>
<td style="text-align: center;">PubChem324k</td>
<td style="text-align: center;">12,000</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">2,000</td>
</tr>
</tbody>
</table>
<p>For comparison, we first select two different foundation LLMs as the backbones of ICMA, namely Galactica-125M [29] and Mistral-7B-instruct-v0.2 [30]. The former is a representative and smaller LLM that has been pre-trained on unstructured scientific corpora, which have been aware of the molecular knowledge from the pre-training stage, while the latter is a general LLM with 7 billion parameters, whose capabilities are comparable to GPT-3.5-turbo. To study the model agnosticism, we include two more backbone LLMs (Galactica-1.3B and Llama-2-7b-chat-hf) in Section IV-F. Notably, in the factor analysis and ablation study parts, we mainly select Galactica125 M for experiments to reduce the computational costs. To better present our results, considering baseline models, we first</p>
<p>TABLE III
Mol2Cap results on ChEBI-20 dataset (Best, Second Best), Here, the results of MolT5-base and MolT5-large are domain-specific PRE-TRAINING \&amp; FINE-TUNING RESULTS [5], WHILE MolREGPT(GPT-3.5-TURBO) AND MolREGPT(GPT-4-0314) ARE PROMPTING \&amp; IN-CONTEXT LEARNING RESULTS [10]. MORE IMPORTANTLY, GALACTICA-125M AND MISTRAL-7B DEMONSTRATE NAIVE SUPERVISED FINE-TUNED RESULTS, WHILE ICMA(Galactica-125M) and ICMA(Mistral-7B) ILLUSTRATE THE ICMA RESULTS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">ROUGE-1 $\uparrow$</th>
<th style="text-align: center;">ROUGE-2 $\uparrow$</th>
<th style="text-align: center;">ROUGE-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MolT5-base [5]</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.569</td>
</tr>
<tr>
<td style="text-align: center;">MolReGPT (GPT-3.5-turbo)</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.585</td>
</tr>
<tr>
<td style="text-align: center;">MolT5-large [5]</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.614</td>
</tr>
<tr>
<td style="text-align: center;">MolReGPT (GPT-4-0314)</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: center;">Galactica-125M</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.591</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Galactica-125M) 2,2048</td>
<td style="text-align: center;">$\underline{0.636}$</td>
<td style="text-align: center;">$\underline{0.565}$</td>
<td style="text-align: center;">$\underline{0.674}$</td>
<td style="text-align: center;">$\underline{0.536}$</td>
<td style="text-align: center;">$\underline{0.615}$</td>
<td style="text-align: center;">$\underline{0.648}$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.572</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mistral-7B) 2,2048</td>
<td style="text-align: center;">$\mathbf{0 . 6 5 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 6 1}$</td>
</tr>
</tbody>
</table>
<p>TABLE IV
Cap2Mol results on ChEBI-20 dataset (Best, Second Best), Here, the results of MolT5-base and MolT5-large are domain-specific PRE-TRAINING \&amp; FINE-TUNING RESULTS [5], WHILE MolREGPT(GPT-3.5-TURBO) AND MolREGPT(GPT-4-0314) ARE PROMPTING \&amp; IN-CONTEXT LEARNING RESULTS [10]. MORE IMPORTANTLY, GALACTICA-125M AND MISTRAL-7B DEMONSTRATE NAIVE SUPERVISED FINE-TUNED RESULTS, WHILE ICMA(Galactica-125M) and ICMA(Mistral-7B) ILLUSTRATE THE ICMA RESULTS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">EM $\uparrow$</th>
<th style="text-align: center;">Levenshtein $\downarrow$</th>
<th style="text-align: center;">MACCS FTS $\uparrow$</th>
<th style="text-align: center;">RDK FTS $\uparrow$</th>
<th style="text-align: center;">Morgan FTS $\uparrow$</th>
<th style="text-align: center;">Validity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MolT5-base [5]</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">24.458</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: center;">MolReGPT(GPT-3.5-turbo)</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">24.91</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.887</td>
</tr>
<tr>
<td style="text-align: center;">MolT5-large [5]</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">16.071</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.905</td>
</tr>
<tr>
<td style="text-align: center;">MolReGPT(GPT-4-0314)</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 7}$</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">$\underline{17.14}$</td>
<td style="text-align: center;">$\underline{0.903}$</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.899</td>
</tr>
<tr>
<td style="text-align: center;">Galactica-125M</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">26.34</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.916</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Galactica-125M) 4,2048</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">$\underline{0.391}$</td>
<td style="text-align: center;">17.71</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">$\underline{0.812}$</td>
<td style="text-align: center;">$\underline{0.753}$</td>
<td style="text-align: center;">$\underline{0.941}$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">27.39</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.918</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mistral-7B) 4,2048</td>
<td style="text-align: center;">$\underline{0.855}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 0}$</td>
<td style="text-align: center;">18.73</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 5 8}$</td>
</tr>
</tbody>
</table>
<p>select MolT5-base, MolT5-large [5] and MolReGPT (GPT-3.5turbo and GPT-4-0314) [10] for comparison on the ChEBI-20 dataset and the case study. We also include the comparison and discussion with previous additional SOTA methods, namely BioT5 [12], MolXPT [11], and MolCA [9], in Section IV-J.</p>
<h2>B. Performance Comparison</h2>
<p>We compare and analyse the performance of ICMA with previous baselines and their original foundation models with naive supervised fine-tuning (SFT) from the two subtasks of molecule caption translation, namely Mol2Cap and Cap2Mol. Mol2Cap. As illustrated in Table III, Galactica-125M with SFT has already shown competitive performance to previous baselines due to its pre-training on scientific corpora. However, ICMA could still improve the performance of Galactica-125M by $12.8 \%$ and $8.3 \%$ considering the BLEU-4 and ROUGEL scores on the ChEBI-20 dataset. With only 125 million parameters, ICMA(Galactica-125M) can beat MolT5-large, which owns more than 780 million parameters. Meanwhile, the general LLM, Mistral-7B with naive SFT, only achieves a performance that is slightly better than MolT5-base, despite the fact that Mistral-7B is 70 times larger than MolT5-base. This outcome is not surprising because Mistral-7B is not specifically designed or pre-trained for biomolecular purposes. It also reveals that although general LLMs have illustrated powerful capabilities with billions of parameters, few works have adapted them to the biomolecular domain due to their unsatisfactory fine-tuning performance. However, with ICMA, Mistral-7B could easily demonstrate its superior in-context learning and reasoning capabilities and perform its advantage of parameters. As a result, ICMA(Mistral-7B) achieves the best performance
across all the models, obtaining 0.581 BLEU-4 and 0.661 METEOR scores on the ChEBI-20 dataset, which is even better than the domain-specific pre-trained Galactica-125M.
Cap2Mol. Similarly, as shown in Table IV, compared to their original foundation models with SFT, ICMA significantly boosts the molecule generation performance. Notably, ICMA(Mistral-7B) achieves state-of-the-art molecule generation performance, generating $46.0 \%$ exactly matched molecules, which nearly doubles the results of naive supervised finetuned Mistral-7B. Although both ICMA(Galactica-125M) and ICMA(Mistral-7B) achieve higher Levenshtein scores, due to the characteristic of the metric, a lower Levenshtein score does not mean better generation quality. For example, the Levenshtein score of $C O \rightarrow C C O C$ is 2 , and the Levenshtein score of $C C C \rightarrow C C O C$ is just 1 . However, the former molecule is considered more similar to the target molecule as they share the same functional group. Instead, if we look at the molecule fingerprints similarity scores, ICMA(Mistral-7B) could achieve superior results and obtain the best validity score, showing better molecule generation quality.</p>
<p>Additionally, experiments are also conducted on a smaller dataset, PubChem324k. As shown in Table V and VI, ICMA could still boost the Mol2Cap performance of LLMs like Galactica-125M and Mistral-7B, which further proves the generalization of ICMA in the molecule-caption translation task.</p>
<h2>C. Study of Retrieval Algorithms</h2>
<p>We also study the influence of retrieval algorithms to illustrate the importance of retrieval qualities. For molecule retrieval, we compare the new proposed Molecule Graph</p>
<p>TABLE V
Mol2Cap results on PubChem324k dataset (Best, Second Best). Here, Galactica-125M and Mistral-7B demonstrate NAIVE SUPERVISED FINE-TUNED RESULTS, WHILE ICMA(Galactica-125M) AND ICMA(Mistral-7B) ILLUSTRATE THE ICMA RESULTS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU-2?</th>
<th style="text-align: center;">BLEU-4?</th>
<th style="text-align: center;">ROUGE-1?</th>
<th style="text-align: center;">ROUGE-2?</th>
<th style="text-align: center;">ROUGE-L?</th>
<th style="text-align: center;">METEOR?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Galactica-125M</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.406</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Galactica-125M) $22,1024$</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.457</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.421</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mistral-7B) $22,1024$</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.464</td>
</tr>
</tbody>
</table>
<p>TABLE VI
CAP2Mol results on PubChem324k dataset (Best, Second Best). Here, Galactica-125M and Mistral-7B demonstrate NAIVE SUPERVISED FINE-TUNED RESULTS, WHILE ICMA(Galactica-125M) AND ICMA(Mistral-7B) ILLUSTRATE THE ICMA RESULTS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU?</th>
<th style="text-align: center;">EM?</th>
<th style="text-align: center;">Levenshtein $\mathrm{t}_{2}$</th>
<th style="text-align: center;">MACCS FTS?</th>
<th style="text-align: center;">RDR FTS?</th>
<th style="text-align: center;">Morgan FTS?</th>
<th style="text-align: center;">Validity?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Galactica-125M</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.835</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Galactica-125M) $22,1024$</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">74.56</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mistral-7B) $22,1024$</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.935</td>
</tr>
</tbody>
</table>
<p>TABLE VII
CAP2Mol results on PubChem324k dataset (Best, Second Best). Here, Galactica-125M and Mistral-7B demonstrate NAIVE SUPERVISED FINE-TUNED RESULTS, WHILE ICMA(Galactica-125M) AND ICMA(Mistral-7B) ILLUSTRATE THE ICMA RESULTS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU?</th>
<th style="text-align: center;">EM?</th>
<th style="text-align: center;">Levenshtein $t_{2}$</th>
<th style="text-align: center;">MACCS FTS?</th>
<th style="text-align: center;">RDR FTS?</th>
<th style="text-align: center;">Morgan FTS?</th>
<th style="text-align: center;">Validity?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Galactica-125M</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.835</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Galactica-125M) $22,1024$</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">74.56</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mistral-7B) $22,1024$</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.935</td>
</tr>
</tbody>
</table>
<p>Retrieval using Mole-BERT with random retrieval and the Morgan FTS retrieval. As shown in Table VII, Mole-BERT achieves the best results on all of the metrics, proving its superiority in molecule retrieval. For the caption retrieval, we compare BM25 Caption Retrieval with random retrieval and SBERT retrieval under the framework of ICMA. As depicted in Table VIII, BM25 Caption Retrieval illustrates its excellent performance among the three caption retrieval methods.</p>
<h2>D. Study of Context Settings</h2>
<p>In ICMA, the context settings, including the example number and cutoff length, are also important to its performance. The increase in example number will also drastically require longer context length. If the context length is longer than the cutoff length, then the training will be insufficient because most of the inputs are cropped, and the information is lost. Meanwhile, during the inference stage, the context length also influences the information that LLMs can take. In this case, we want to make sure that most of the context examples fit in the cutoff length. Considering that the input length limitation of Galactica-125M is 2048, and the model series has no length extrapolating capability, we test the cutoff length within the range of ${512,1024,1536,2048}$ and the example number from 1 to 4 for analysis. As illustrated in Figure 3, when the</p>
<p>TABLE VII
PERFORMANCE COMPARISON OF DIFFERENT RETRIEVAL ALGORITHMS FOR Mol2Cap task (Best, Second Best). The backbone is ICMA(Galactica-125M) $22,1024$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU-2?</th>
<th style="text-align: center;">BLEU-4?</th>
<th style="text-align: center;">ROUGE-1?</th>
<th style="text-align: center;">ROUGE-2?</th>
<th style="text-align: center;">ROUGE-L?</th>
<th style="text-align: center;">METEOR?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.613</td>
</tr>
<tr>
<td style="text-align: center;">Morgan FTS</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.633</td>
</tr>
<tr>
<td style="text-align: center;">Mole-BERT</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.645</td>
</tr>
</tbody>
</table>
<p>TABLE VIII
PERFORMANCE COMPARISON OF DIFFERENT RETRIEVAL ALGORITHMS FOR CAP2Mol task (Best, Second Best). The backbone is ICMA(Galactica-125M) $22,1024$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU?</th>
<th style="text-align: center;">EM?</th>
<th style="text-align: center;">Levenshtein $t_{2}$</th>
<th style="text-align: center;">MACCS FTS?</th>
<th style="text-align: center;">RDR FTS?</th>
<th style="text-align: center;">Morgan FTS?</th>
<th style="text-align: center;">Validity?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">20.24</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.928</td>
</tr>
<tr>
<td style="text-align: center;">SBERT</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">22.01</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.929</td>
</tr>
<tr>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">17.99</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.935</td>
</tr>
</tbody>
</table>
<p>Mol2Cap task exhibits a notably different trend, with minimal improvement observed. This discrepancy could indicate underlying deficiencies in the model's natural language understanding and generation capabilities, which caps Galactica's performance in the Mol2Cap task.</p>
<h3><em>F. Study of Model Agnosticism</em></h3>
<p>To highlight the model agnosticism of ICMA, we also expand our experiments on two more backbone LLMs, Galactica-1.3B and Meta-Llama-3-8B-Instruct. The results are shown in Tables IX and X. It is evident that ICMA enhances the performance of both backbone models, which further proves the versatility and model agnosticism of ICMA.</p>
<h4>TABLE IX</h4>
<p><strong>PERFORMANCE COMPARISON OF TWO MORE BACKBONE LLMs (GALACTICA-1.3B AND META-LLAMA-3-8B-INSTRUCT) FOR Mol2Cap TASK ON CHEBI-20 DATASET (BEST, SECOND BEST).</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>BLEU-21</th>
<th>BLEU-41</th>
<th>ROUGE-11</th>
<th>ROUGE-21</th>
<th>ROUGE-L1</th>
<th>METEOR1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Galactica-1.3B</td>
<td>0.589</td>
<td>0.507</td>
<td>0.638</td>
<td>0.484</td>
<td>0.574</td>
<td>0.597</td>
</tr>
<tr>
<td>ICMA(Galactica-1.3B)</td>
<td>0.602</td>
<td>0.534</td>
<td>0.668</td>
<td>0.530</td>
<td>0.607</td>
<td>0.649</td>
</tr>
<tr>
<td>Llama3-8B</td>
<td>0.618</td>
<td>0.542</td>
<td>0.660</td>
<td>0.515</td>
<td>0.599</td>
<td>0.623</td>
</tr>
<tr>
<td>ICMA(Llama3-8B)</td>
<td>0.665</td>
<td>0.595</td>
<td>0.693</td>
<td>0.559</td>
<td>0.633</td>
<td>0.669</td>
</tr>
</tbody>
</table>
<h4>TABLE X</h4>
<p><strong>PERFORMANCE COMPARISON OF TWO MORE BACKBONE LLMs (GALACTICA-1.3B AND META-LLAMA-3-8B-INSTRUCT) FOR CAP2MOL TASK ON CHEBI-20 DATASET (BEST, SECOND BEST).</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>BLEU-1</th>
<th>EMT</th>
<th>Levenshtein_L</th>
<th>MACCS FTS1</th>
<th>RDK FTS1</th>
<th>Morgan FTS1</th>
<th>Validity1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Galactica-1.3B</td>
<td>0.812</td>
<td>0.282</td>
<td>22.47</td>
<td>0.882</td>
<td>0.777</td>
<td>0.708</td>
<td>0.950</td>
</tr>
<tr>
<td>ICMA(Galactica-1.3B)</td>
<td>0.839</td>
<td>0.296</td>
<td>21.61</td>
<td>0.910</td>
<td>0.829</td>
<td>0.770</td>
<td>0.946</td>
</tr>
<tr>
<td>Llama3-8B</td>
<td>0.826</td>
<td>0.256</td>
<td>20.70</td>
<td>0.893</td>
<td>0.791</td>
<td>0.733</td>
<td>0.939</td>
</tr>
<tr>
<td>ICMA(Llama3-8B)</td>
<td>0.851</td>
<td>0.445</td>
<td>19.27</td>
<td>0.913</td>
<td>0.836</td>
<td>0.785</td>
<td>0.958</td>
</tr>
</tbody>
</table>
<h3><em>G. Performance for Complex Molecules and Captions</em></h3>
<p>ICMA also works better for complex molecular structures and molecule captions. Normally, the lengthy molecules can be more complex for LLMs. In this case, we extracted a subset of 747 complex molecules, whose SMILES strings have more than 100 characters and a subset of 221 complex captions with more than 500 characters from the ChEBI-20 test set. We compared the performance of ICMA with previous baselines on this subset. Our experimental results shown in Table XI and XII indicate that ICMA outperforms the previous baselines, especially in the Cap2Mol task, as similar examples learned from ICMT typically share comparable levels of complexity, which helps better aligns molecules with texts.</p>
<h4>TABLE XI</h4>
<p><strong>THE MOL2CAP PERFORMANCE COMPARISON ON THE SUBSET OF CHEBI-20 WITH COMPLEX MOLECULES WHOSE SMILES STRINGS HAVE MORE THAN 100 CHARACTERS (BEST, SECOND BEST).</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>BLEU-21</th>
<th>BLEU-41</th>
<th>ROUGE-11</th>
<th>ROUGE-21</th>
<th>ROUGE-L1</th>
<th>METEOR1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mol23-large</td>
<td>0.684</td>
<td>0.624</td>
<td>0.715</td>
<td>0.590</td>
<td>0.661</td>
<td>0.685</td>
</tr>
<tr>
<td>Modbus(PTA)(PTA-0314)</td>
<td>0.670</td>
<td>0.603</td>
<td>0.692</td>
<td>0.555</td>
<td>0.630</td>
<td>0.673</td>
</tr>
<tr>
<td>ICMA(Moetal-7B)</td>
<td>0.687</td>
<td>0.632</td>
<td>0.727</td>
<td>0.611</td>
<td>0.673</td>
<td>0.713</td>
</tr>
</tbody>
</table>
<h3><em>H. Random Walk Stability &amp; Maximum Skip Probability</em></h3>
<p>Due to the randomness we introduced during the Random Walk selection, there might be concerns about the stability of the strategy. However, considering that the design of the skip probability is to limit it within the maximum skip probability, which is a rather low value, and gradually decays the likelihood to ensure that at least one example is selected, we manage the randomness brought by Random Walk selection within an acceptable range. To verify this, we implemented a bucket sampling strategy for comparison. We randomly chose three different random seeds and calculated the mean and variance of the performance results for three runs. The results are demonstrated in Table XIII and XIV.</p>
<h4>TABLE XIII</h4>
<p><strong>STABILITY COMPARISON ON THE MOL2CAP TASK BETWEEN TWO RE-RANKING STRATEGY, RANDOM WALK AND BUCKET SAMPLING. THE RESULTS ARE WITH SIGNIFICANT FIGURES FOR PRECISION (BEST).</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>BLEU-21</th>
<th>BLEU-41</th>
<th>ROUGE-11</th>
<th>ROUGE-21</th>
<th>ROUGE-L1</th>
<th>METEOR1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Walk</td>
<td>0.6392±0.0032</td>
<td>0.5658±0.0031</td>
<td>0.6703±0.0028</td>
<td>0.5306±0.0026</td>
<td>0.6107±0.0030</td>
<td>0.6437±0.0031</td>
</tr>
<tr>
<td>Bucket Sampling</td>
<td>0.6384±0.0029</td>
<td>0.5638±0.0040</td>
<td>0.6687±0.0032</td>
<td>0.5293±0.0033</td>
<td>0.6087±0.0026</td>
<td>0.6424±0.0043</td>
</tr>
</tbody>
</table>
<h4>TABLE XIV</h4>
<p><strong>STABILITY COMPARISON ON THE CAP2MOL TASK BETWEEN TWO RE-RANKING STRATEGY, RANDOM WALK AND BUCKET SAMPLING. THE RESULTS ARE WITH FOUR SIGNIFICANT FIGURES FOR PRECISION (BEST).</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>BLEU-1</th>
<th>EMT</th>
<th>Levenshtein_L</th>
<th>MACCS FTS1</th>
<th>RDK FTS1</th>
<th>Morgan FTS1</th>
<th>Validity1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Walk</td>
<td>0.8492±0.0014</td>
<td>0.5515±0.0030</td>
<td>10.45±0.56</td>
<td>0.8907±0.0025</td>
<td>0.8842±0.0031</td>
<td>0.7412±0.0031</td>
<td>0.9374±0.0027</td>
</tr>
<tr>
<td>Bucket Sampling</td>
<td>0.8371±0.0014</td>
<td>0.5464±0.0062</td>
<td>15.58±0.23</td>
<td>0.8005±0.0026</td>
<td>0.8000±0.0021</td>
<td>0.7567±0.0021</td>
<td>0.9107±0.0054</td>
</tr>
</tbody>
</table>
<p>From the results, we can observe that although the variance of the Random Walk is larger than that of bucket sampling on metrics like BLEU, Levenshtein, RDK FTS, and Morgan FTS in the Cap2Mol task and ROUGE-L in the Mol2Cap task, the Random Walk strategy still achieves higher means, and the variances are still manageable. On some metrics, the variances of the Random Walk are even lower than that of bucket sampling, indicating that despite the inherent randomness, our method overall remains stable within normal fluctuation ranges.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 5. The performance of Galactica-125M with the maximum skip probability <em>pmax</em>. Mol2Cap Results (Left) and Cap2Mol Results (Right).</p>
<p>What's more, we further conduct a hyperparameter search to help decide the best maximum skip probability <em>pmax</em> under the</p>
<p>setting of $n=2, N=10$, and cutoff_len $=1024$. As shown in Figure 5, we could obviously see that if the maximum skip probability is too large, the performance naturally drops because it is more likely to select less similar examples. However, with the assistance of skip probability decay as well as the early-stop, the performance gap is still marginal. Among all the selected maximum probabilities, $p_{\max }=9 \%$ achieves the highest exact match score in the Cap2Mol task and the best METEOR score in the Mol2Cap task. However, we must acknowledge that the Random Walk strategy inherently introduces a degree of randomness, and for different numbers of rough examples $N$ and refined examples $n$, the best maximum probability might vary, which means the observed results may not fully reflect the true potential of the Random Walk sampling.</p>
<h2>I. Ablation Study</h2>
<p>TABLE XV
Ablating COMPonents of Post-retrieval Re-ranking for Mol2Cap task (Best, Second Best). The backbone is ICMA(Galactica-125M) 2,1024 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU-27</th>
<th style="text-align: center;">BLEU-47</th>
<th style="text-align: center;">ROUGE-17</th>
<th style="text-align: center;">ROUGE-27</th>
<th style="text-align: center;">ROUGE-L7</th>
<th style="text-align: center;">METEOR7</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ICMA</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.645</td>
</tr>
<tr>
<td style="text-align: center;">w/o Random Walk</td>
<td style="text-align: center;">0.639(3)</td>
<td style="text-align: center;">0.565(5)</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.530(3)</td>
<td style="text-align: center;">0.619(3)</td>
<td style="text-align: center;">0.643(7)</td>
</tr>
<tr>
<td style="text-align: center;">w/o Sequence Reverse</td>
<td style="text-align: center;">0.638(4)</td>
<td style="text-align: center;">0.566(5)</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.530(5)</td>
<td style="text-align: center;">0.610(3)</td>
<td style="text-align: center;">0.644(1)</td>
</tr>
</tbody>
</table>
<p>TABLE XVI
Ablating COMPonents of Post-retrieval Re-ranking for Cap2Mol task (Best, Second Best). The backbone is ICMA(Galactica-125M) 2,1024 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU7</th>
<th style="text-align: center;">EMT</th>
<th style="text-align: center;">Levenshtein $\downarrow$</th>
<th style="text-align: center;">MACCS FT57</th>
<th style="text-align: center;">RDK FT57</th>
<th style="text-align: center;">Morgan FT57</th>
<th style="text-align: center;">Validity7</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ICMA</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">17.99</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.658</td>
</tr>
<tr>
<td style="text-align: center;">w/o Random Walk</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">18.01</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr>
<td style="text-align: center;">w/o Sequence Reverse</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">18.49</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.653</td>
</tr>
</tbody>
</table>
<p>Ablation study is also conducted to illustrate how the Postretrieval Re-ranking components affect the predictions. We conduct ablation experiments for the Mol2Cap and Cap2Mol sub-tasks by deactivating the Random Walk and Sequence Reversal. As shown in Table XV and XVI, without Random Walk or Sequence Reversal, the performance of ICMA drops, though not significantly but consistently, in both tasks, proving the effectiveness of Post-retrieval Re-ranking.</p>
<h2>J. Comparative Analysis of Models with Extra Domain Alignment Stages</h2>
<p>In our pursuit to comprehensively evaluate the efficacy of ICMA, we have extended our comparative analysis to include models that adopt extra domain alignment stages, such as continual pre-training on chemical corpora (BioT5 [12] \&amp; MolXPT [11]) and graph modality alignment (MolCA [9]. The results, as detailed in Table XVII, indicate that ICMA outperforms all the models trained with extra domain alignment in terms of BLEU-2, BLEU-4, and METEOR scores, while also achieving the second-highest ROUGE scores. Notably, the performance is achieved without introducing extra domain alignment stages, which further proves the superiority of ICMA.</p>
<p>Notably, compared to MolCA, a multi-modal method that leverages 2D molecular graphs to augment LLMs for the</p>
<p>Mol2Cap task, our ICMA does not require extra graph information and modality alignment training. Meanwhile, ICMA is capable of refining the alignment between molecular and textual representations not only for Mol2Cap task but also for Cap2Mol task, showing better versatility in aligning molecular space with textual space.</p>
<p>On the other hand, for the Cap2Mol task, we have included BioT5 and MolXPT in our comparative analysis. As presented in Table XVIII, ICMA achieves the highest scores in exact match and molecule fingerprints metrics, while the validity of molecule generation is just slightly below the BioT5 and MolXPT due to the deficiency in molecular knowledge, indicating competitive performance in molecule generation. Furthermore, it is also important to note that ICMA does not require pre-training LLMs on a large-scale chemical corpus, which makes ICMA a better choice in low-data scenarios.</p>
<p>TABLE XVII
Performance comparison with models trained with extra domain alignment stages for Mol2Cap task on ChEBI-20 dataset. Here, we select BioT5 [12], MolXPT [11], and MolCA [9] for COMPARISON (BEST, SECOND BEST).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">BLEU-27</th>
<th style="text-align: center;">BLEU-47</th>
<th style="text-align: center;">ROUGE-17</th>
<th style="text-align: center;">ROUGE-27</th>
<th style="text-align: center;">ROUGE-L7</th>
<th style="text-align: center;">METEOR7</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BioT5 [12]</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.656</td>
</tr>
<tr>
<td style="text-align: center;">MolXPT [11]</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.626</td>
</tr>
<tr>
<td style="text-align: center;">MolCA [9]</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.651</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mostral-7B)</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.661</td>
</tr>
</tbody>
</table>
<p>TABLE XVIII
Performance comparison with models trained with extra domain alignment stages for Cap2Mol task on ChEBI-20 dataset. Here, we include the results from BioT5 [12] and MolXPT [11] FOR COMPARISON (BEST, SECOND BEST).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Backbone</th>
<th style="text-align: center;">BLEU7</th>
<th style="text-align: center;">EMT</th>
<th style="text-align: center;">Levenshtein $\downarrow$</th>
<th style="text-align: center;">MACCS FT57</th>
<th style="text-align: center;">RDK FT57</th>
<th style="text-align: center;">Morgan FT57</th>
<th style="text-align: center;">Validity7</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BioT5 [12]</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">15.19</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">1.009</td>
</tr>
<tr>
<td style="text-align: center;">MolXPT [11]</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">15.47</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.981</td>
</tr>
<tr>
<td style="text-align: center;">ICMA(Mostral-7B)</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">14.73</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.956</td>
</tr>
</tbody>
</table>
<h2>K. ICMA for Molecule Property Prediction</h2>
<p>Although ICMA is targeted at the task of molecule-caption translation, we are curious about whether ICMA could also benefit a wide range of molecule-related tasks, such as molecule property prediction. Therefore, we conduct experiments on the MoleculeNet dataset [31] and mainly focus on classification subtasks like BACE, BBBP, HIV, TOX21, and SIDER. The results are shown in Table XIX. To our surprise, ICMA shows great generalization capability to these molecule property prediction tasks, increasing the performance of both Galactica125M and Mistral-7B. More significantly, ICMA enables the general LLM, Mistral-7B, to achieve an average improvement of $33.26 \%$ on the five subtasks, which further proves that LLMs are inherently in-context molecule learners.</p>
<h2>L. Case Study</h2>
<p>Here, we demonstrate the performance of different methods by introducing detailed examples in both the Cap2Mol and Mol2Cap sub-tasks.
Cap2Mol Figure 6 showcases a series of molecule examples produced by various baseline methods and our proposed</p>
<p>TABLE XIX
MoleculE ProPERTY PREDiction Performance of ICMA COMPARED WITH DIRECTLY FINE-TUNING THE BACKBONE MODELS ON THE MOLECULENET DATASET (BEST, SECOND BEST).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BACE $\uparrow$</th>
<th style="text-align: center;">BBBP $\uparrow$</th>
<th style="text-align: center;">HIV $\uparrow$</th>
<th style="text-align: center;">TOX21 $\uparrow$</th>
<th style="text-align: center;">SIDER $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Galactica-125M</td>
<td style="text-align: center;">$\underline{0.8216}$</td>
<td style="text-align: center;">$\underline{0.7805}$</td>
<td style="text-align: center;">0.7314</td>
<td style="text-align: center;">$\underline{0.7453}$</td>
<td style="text-align: center;">$\underline{0.7642}$</td>
</tr>
<tr>
<td style="text-align: center;">ICMA (Galactica-125M)</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 4 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6 8 0}$</td>
<td style="text-align: center;">$\underline{0.7412}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 4 7 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 7 3}$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.4926</td>
<td style="text-align: center;">0.4829</td>
<td style="text-align: center;">0.5866</td>
<td style="text-align: center;">0.4386</td>
<td style="text-align: center;">0.3771</td>
</tr>
<tr>
<td style="text-align: center;">ICMA (Mistral-7B)</td>
<td style="text-align: center;">0.7995</td>
<td style="text-align: center;">0.6775</td>
<td style="text-align: center;">$\mathbf{0 . 7 4 4 1}$</td>
<td style="text-align: center;">0.4761</td>
<td style="text-align: center;">0.4838</td>
</tr>
</tbody>
</table>
<p>approach, ICMA. In the first example, both MolT5 and MolReGPT exhibited errors in the positions of functional groups, while ICMA precisely replicated the correct structure. Meanwhile, in the third example, MolT5 and MolReGPT made a mistake in the type of functional groups, generating an $\left[\mathrm{NH}_{2}\right]^{+}$instead of the $[N H]$. What's more, across the remaining examples, ICMA could even better match the number of carbon atoms in the SMILES representations of molecules, which consistently demonstrated superior proficiency in capturing the intricate alignment between natural language descriptions and molecule structures, thereby proving the efficacy of our proposed method.
Mol2Cap Figure 7 presents a set of caption examples produced by two baseline methods and our ICMA. In the first example, MolT5 incorrectly equated D-tartrate(2-) with L-tartrate(2) and L-tartrate(1-) with D-tartrate(1-), while MolReGPT introduced an excessive amount of irrelevant information. In contrast, ICMA could accurately empower backbone models to generate precise answers. Meanwhile, in the second example, ICMA accurately recognized that the molecule in question "has a role as an antibacterial drug", whereas MolReGPT failed. Moreover, in the fourth example, ICMA meticulously captured all the essential details in describing the molecule structures without a single mistake, demonstrating that contextual examples can significantly enhance the understanding of molecule structures, especially the types and positions of functional groups.</p>
<p>Overall, ICMA could fine-grain the detailed alignment between molecule captions and molecule SMILES representations via learning from context examples, showing better capabilities in matching the types and positions of functional groups, the number of carbon atoms, as well as the overall structures.</p>
<h2>V. CONCLUSION</h2>
<p>In this work, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm for adapting LLMs to the molecule-caption translation task. Instead of domain-specific pre-training and fine-tuning, ICMA enables LLMs to utilize their in-context learning capability to learn molecule-text alignment via In-Context Molecule Tuning, which significantly improves the performance of LLMs in the molecule-caption translation task, demonstrating that LLMs are inherently incontext molecule learners. More importantly, our study provides a viable framework for deploying advanced LLMs with billionlevel parameters without extra domain alignment stages in the scientific field, making it suitable to be applied in low-data scenarios.</p>
<h2>VI. ACKNOWLEDGEMENT</h2>
<p>The research described in this paper has been partly supported by the General Research Funds from the Hong Kong Research Grants Council (project no. PolyU 15200023, and 15224524). This work is also supported by Shanghai Artificial Intelligence Laboratory (Shanghai AI Lab) and was done during Jiatong Li and Wei Liu's internship at Shanghai AI Lab.</p>
<h2>REFERENCES</h2>
<p>[1] B. Ding, Y. Weng, Y. Liu, C. Song, L. Yin, J. Yuan, Y. Ren, A. Lei, and C.-W. Chiang, "Selective photoredox trifluoromethylation of tryptophancontaining peptides," European Journal of Organic Chemistry, vol. 2019, no. 46, pp. 7596-7605, 2019.
[2] R. M. Twyman, E. Stoger, S. Schillberg, P. Christou, and R. Fischer, "Molecular farming in plants: host systems and expression technology," TRENDS in Biotechnology, vol. 21, no. 12, pp. 570-578, 2003.
[3] A. Higuchi, T.-C. Sung, T. Wang, Q.-D. Ling, S. S. Kumar, S.-T. Hsu, and A. Umezawa, "Material design for next-generation mrna vaccines using lipid nanoparticles," Polymer Reviews, vol. 63, no. 2, pp. 394-436, 2023.
[4] D. Weininger, "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules," Journal of chemical information and computer sciences, vol. 28, no. 1, pp. 31-36, 1988.
[5] C. Edwards, T. Lai, K. Ros, G. Honke, K. Cho, and H. Ji, "Translation between molecules and natural language," in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 375-413. [Online]. Available: https://aclanthology.org/2022.emnlp-main. 26
[6] S. Kim, J. Chen, T. Cheng, A. Giudulyte, J. He, S. He, Q. Li, B. A. Shoemaker, P. A. Thiessen, B. Yu et al., "Pubchem 2019 update: improved access to chemical data," Nucleic acids research, vol. 47, no. D1, pp. D1102-D1109, 2019.
[7] C. Edwards, C. Zhai, and H. Ji, "Text2mol: Cross-modal molecule retrieval with natural language queries," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 595-607.
[8] B. Su, D. Du, Z. Yang, Y. Zhou, J. Li, A. Rao, H. Sun, Z. Lu, and J.-R. Wen, "A molecular multimodal foundation model associating molecule graphs with natural language," arXiv preprint arXiv:2209.05481, 2022.
[9] Z. Liu, S. Li, Y. Luo, H. Fei, Y. Cao, K. Kawaguchi, X. Wang, and T.-S. Chua, "Molca: Molecular graph-language modeling with crossmodal projector and uni-modal adapter," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 15 623-15 638.
[10] J. Li, Y. Liu, W. Fan, X.-Y. Wei, H. Liu, J. Tang, and Q. Li, "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective," arXiv preprint arXiv:2306.06615, 2023.
[11] Z. Liu, W. Zhang, Y. Xia, L. Wu, S. Xie, T. Qin, M. Zhang, and T.-Y. Liu, "Molxpt: Wrapping molecules with text for generative pre-training," arXiv preprint arXiv:2305.10688, 2023.
[12] Q. Pei, W. Zhang, J. Zhu, K. Wu, K. Gao, L. Wu, Y. Xia, and R. Yan, "Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 1102-1123.
[13] M. Krenn, F. Häse, A. Nigam, P. Friederich, and A. Aspuru-Guzik, "Self-referencing embedded strings (selfies): A 100\% robust molecular string representation," Machine Learning: Science and Technology, vol. 1, no. 4, p. 045024, 2020.
[14] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models," arXiv preprint arXiv:2301.12597, 2023.
[15] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Naelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[16] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," Journal of Machine Learning Research, vol. 24, no. 240, pp. 1-113, 2023.</p>
<p>[17] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, "Self-instruct: Aligning language model with self generated instructions," arXiv preprint arXiv:2212.10560, 2022.
[18] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in-context learning," arXiv preprint arXiv:2301.00234, 2022.
[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24 824-24 837, 2022.
[20] C. N. Edwards, A. Naik, T. Khot, M. D. Burke, H. Ji, and T. Hope, "Synergpt: In-context learning for personalized drug synergy prediction and drug design," bioRxiv, pp. 2023-07, 2023.
[21] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero, and B. Smit, "Leveraging large language models for predictive chemistry," Nature Machine Intelligence, pp. 1-9, 2024.
[22] H. Liu, Y. Wei, J. Yin, and L. Nie, "Hs-gcn: Hamming spatial graph convolutional networks for recommendation," IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 6, pp. 5977-5990, 2022.
[23] J. Xia, Y. Zhu, Y. Du, and S. Z. Li, "A systematic survey of chemical pretrained models," in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, 2023, pp. 6787-6795.
[24] J. Xia, C. Zhao, B. Hu, Z. Gao, C. Tan, Y. Liu, S. Li, and S. Z. Li, "Mole-bert: Rethinking pre-training graph neural networks for molecules," in The Eleventh International Conference on Learning Representations, 2022.
[25] S. Robertson, H. Zaragoza et al., "The probabilistic relevance framework: Bm25 and beyond," Foundations and Trends® in Information Retrieval, vol. 3, no. 4, pp. 333-389, 2009.
[26] N. Reimers and I. Gurevych, "Sentence-bert: Sentence embeddings using siamese bert-networks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), 2019, pp. 3982-3992.
[27] L. Frermann, J. Li, S. Khanehzar, and G. Mikolajczak, "Conflicts, villains, resolutions: Towards models of narrative media framing," in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 8712-8732. [Online]. Available: https://aclanthology.org/2023.acl-long. 466
[28] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., "Lora: Low-rank adaptation of large language models," in International Conference on Learning Representations, 2021.
[29] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, "Galactica: A large language model for science," arXiv preprint arXiv:2211.09085, 2022.
[30] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., "Mistral 7b," arXiv preprint arXiv:2310.06825, 2023.
[31] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V. Pande, "Moleculenet: a benchmark for molecular machine learning," Chemical science, vol. 9, no. 2, pp. 513-530, 2018.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Jiatong $\mathbf{L i}$ is currently a PhD student of the Department of Computing (COMP), The Hong Kong Polytechnic University (funded by HKPFS). Before joining the PolyU, he received my Master's degree of Information Technology (with Distinction) from the University of Melbourne, under the supervision of Dr. Lea Frermann. In 2021, he got his bachelor's degree in Information Security from Shanghai Jiao Tong University. His interest lies in Natural Language Processing, Drug Discovery, and Recommender Systems. He has published innovative works in top-tier conferences such as IJCAI and ACL. For more information, please visit https://phenixace.github.io/.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Wei Liu is currently a PhD student of the Department of Computer Science and Engineering, Shanghai Jiao Tong University, jointly trained with the Shanghai Artificial Intelligence Laboratory. In 2021, he got his bachelor's degree in Mechanical Engineering from Shanghai Jiao Tong University. His interest lies in AIDriven Drug Design (AIDD) and Natural Language Processing. He has published works in conferences and journals such as ICLR and MBE.</p>
<p>Zhihao Ding currently a PhD student of the Department of Computing (COMP), Hong Kong Polytechnic University (PolyU). Before joining the PolyU, he received his B.S. degree and M.S. degree from Northeastern University and Xi'an Jiaotong University in 2019 and 2022, respectively. His research includes Graph Neural Networks, Drug Discovery, and AI4Science. He has published innovative works in top-tier conferences such as VLDB and Neurocomputing.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Wenqi Fan is a research assistant professor of the Department of Computing at The Hong Kong Polytechnic University (PolyU). He received his Ph.D. degree from the City University of Hong Kong (CityU) in 2020. From 2018 to 2020, he was a visiting research scholar at Michigan State University (MSU). His research interests are in the broad areas of machine learning and data mining, with a particular focus on Recommender Systems, Graph Neural Networks, and Trustworthy Recommendations. He has published innovative papers in top-tier journals and conferences such as TKDE, TIST, KDD, WWW, ICDE, NeurIPS, SIGIR, IJCAI, AAAI, RecSys, WSDM, etc. He serves as top-tier conference (senior) program committee members and session chairs (e.g., ICML, ICLR, NeurIPS, KDD, WWW, AAAI, IJCAI, WSDM, etc.), and journal reviewers (e.g., TKDE, TIST, TKDD, TOIS, TAI, etc.). More information about him can be found at https://wenqifan03.github.io.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Yuqiang Li received a Bachelor of Science degree from Central South University in Changsha, China, and a Ph.D. in Chemistry from Wuhan University in Wuhan, China. He has previously held a lecturer position at Central South University. He is currently a junior researcher at the Shanghai AI Lab. His research interests encompass chemistry, machine learning, and large language models. He remains actively engaged in the scientific community, serving as a reviewer for journals such as Science Advance.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Qing Li received the B.Eng. degree from Hunan University, Changsha, China, and the M.Sc. and Ph.D. degrees from the University of Southern California, Los Angeles, all in computer science. He is currently a Chair Professor (Data Science) and the Head of the Department of Computing, the Hong Kong Polytechnic University. He is a Fellow of IEEE and IET, a member of ACM SIGMOD and IEEE Technical Committee on Data Engineering. His research interests include object modeling, multimedia databases, social media, and recommender systems. He has been actively involved in the research community by serving as an associate editor and reviewer for technical journals, and as an organizer/coorganizer of numerous international conferences. He is the chairperson of the Hong Kong Web Society, and also served/is serving as an executive committee (EXCO) member of IEEE-Hong Kong Computer Chapter and ACM Hong Kong Chapter. In addition, he serves as a councilor of the Database Society of Chinese Computer Federation (CCF), a member of the Big Data Expert Committee of CCF, and is a Steering Committee member of DASFAA, ER, ICWL, UMEDIA, and WISE Society.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 6. Molecule examples generated by different models based on the given captions in the Cap2Mol task.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 7. Caption examples generated by different models based on the given molecules in Mol2Cap task.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1750 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1750</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1750</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-603242e88244aff1135ce6cb3baf53d2e95ed427</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/603242e88244aff1135ce6cb3baf53d2e95ed427" target="_blank">Improving fine-grained understanding in image-text pre-training</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> Improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation are shown.</p>
                <p><strong>Paper Abstract:</strong> We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1750",
    "paper_id": "paper-603242e88244aff1135ce6cb3baf53d2e95ed427",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0058925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving fine-grained understanding in image-text pre-training</h1>
<p>Ioana Bica ${ }^{1}$, Anastasija Ilić ${ }^{1}$, Matthias Bauer ${ }^{1}$, Goker Erdogan ${ }^{1}$, Matko Bošnjak ${ }^{1}$, Christos Kaplanis ${ }^{1}$, Alexey A. Gritsenko ${ }^{1}$, Matthias Minderer ${ }^{1}$, Charles Blundell ${ }^{1}$, Razvan Paşcanu ${ }^{1}$ and Jovana Mitrović ${ }^{1}$ ${ }^{1}$ Google DeepMind</p>
<h4>Abstract</h4>
<p>We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.</p>
<h2>1. Introduction</h2>
<p>Contrastive pre-training from large-scale, noisy image-text datasets (Jia et al., 2021; Radford et al., 2021) has become a widely used paradigm for learning general vision representations useful for a wide range of downstream tasks as well as for learning vision encoders in multimodal foundation models (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2022a). By aligning global image and text representations in a shared latent space using similar and dissimilar image-text pairs, these models achieve impressive performance on image-level vision tasks like classification (Radford et al., 2021), coarse-grained retrieval and visual question answering (Alayrac et al., 2022; Chen et al., 2022). On the other hand, these models have been shown to discard fine-grained visual information (Krojer et al., 2022) and work poorly on downstream tasks involving localization (Ranasinghe et al., 2022; Zhong et al., 2022), counting (Paiss et al., 2023) and understanding spatial relationships between objects (Parcalabescu et al., 2021) or object attributes (Yuksekgonul et al., 2022). These shortcomings are further exacerbated when these pretrained models are used in foundation models (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2022a) or when they are used to initialize models for object detection (Minderer et al., 2022) or segmentation (Zhou et al., 2022).</p>
<p>A recent line of work has started to explore incorporating losses between image patch and text token embeddings (Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022; Yao et al., 2021) to learn representations encoding more fine-grained details. Motivated by the idea of aligning patches corresponding to individual objects in the image to tokens corresponding to the words describing these objects, these local losses learn soft correspondences between image patches and text tokens from image-text pairs. While these models have achieved improved performance on fine-grained retrieval (Yao et al., 2021), image classification (Yao et al., 2021), object detection and segmentation (Mukhoti et al., 2023; Wang et al., 2022), they are computationally and memory</p>
<p>expensive, unstable during training (Yao et al., 2021) and/or rely on pretrained models to kickstart learning.</p>
<p>In this work, we propose SPARse Fine-grained Contrastive Alignment (SPARC), a novel objective for multimodal pretraining which learns representations that encode both coarsegrained/global and fine-grained/local information. We propose to build language-grouped vision embeddings by learning to aggregate (in an unsupervised way) image patches corresponding to individual words in the caption; this is motivated by the observation that usually multiple image patches correspond to one word in the caption. As a first step, SPARC computes the similarity between the patch and token embeddings of an individual image-text pair and enforces sparsity in the resulting similarity matrix. This sparsification enables only the most relevant image patches to be attributed to individual tokens. Next, as illustrated in Figure 1, for every text token, we compute the corresponding language-grouped vision embedding as the alignmentweighted combination of patches that are most similar to that token. We calculate a sparse similarity metric between tokens and patches of individual image-text pairs (left) and use it to compute the resulting alignment weights (middle). We contrast the language-grouped vision embeddings with token embeddings in a fine-grained contrastive sequence-wise loss (right).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure $1 \mid$ For every text token, SPARC learns a corresponding language-grouped vision embedding as the alignmentweighted combination of patches that are most similar to that token. We calculate a sparse similarity metric between tokens and patches of individual image-text pairs (left) and use it to compute the resulting alignment weights (middle). We contrast the language-grouped vision embeddings with token embeddings in a fine-grained contrastive sequence-wise loss (right).</p>
<p>Through its design choices, SPARC addresses several shortcomings of existing methods for learning image representations with more fine-grained information. Firstly, several of these methods (Huang et al., 2021; Mukhoti et al., 2023; Yao et al., 2021) learn representations with fine-grained losses that compute similarities between all image patch embeddings and all text token embeddings in a batch. This approach is both computationally and memory intensive and does not scale to large batch sizes (which are needed for obtaining good performance for contrastive methods (Jia et al., 2021; Radford et al., 2021; Zhai et al., 2023b)). On the other hand, SPARC contrasts patch and token embeddings at the level of individual image-text pairs and does not use other examples from the batch to compute the similarity matrix which leads to more favourable computation and memory footprints and more easily scales to large batch sizes. Secondly, for learning soft correspondences between image patches and text tokens, prior work (Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022) usually relies on building cross-modal weighted representations with weights computed as a softmax over patch and token embedding similarities. The winner-takes-all dynamics of softmax (Elfadel and Wyatt Jr, 1993; Peterson and Söderberg, 1989) strongly bias learning towards one-to-one mappings</p>
<p>between individual text tokens and image patches which often does not correspond to underlying data. For example, in an image of a dog, the token embedding for "dog" should be matched with all patch embeddings that correspond to the dog in the image and not just one/a few. Moreover, softmax can be problematic from a gradient flow perspective (Hoffmann et al., 2023; Shen et al., 2023; Zhai et al., 2023a) as it tends to lead to a low entropy distribution, where softmax saturates and therefore its Jacobian vanishes (Hoffmann et al., 2023). See Appendix A for a more detailed explanation. On the flip side, SPARC does not use softmax for calculating the alignment weights which allows it to learn a flexible one-to-many matching between individual tokens and the corresponding image patches and to avoid the winner-take-all dynamics of softmax. Thirdly, several of these approaches start from contrastively pre-trained vision-language models (Mukhoti et al., 2023) or from pre-trained language models (Huang et al., 2021; Wang et al., 2022). Moreover, existing fine-grained objectives have been developed in different communities (i.e. medical (Huang et al., 2021; Wang et al., 2022) vs. general vision (Mukhoti et al., 2023; Yao et al., 2021)) leveraging different types and sizes of datasets, architectures and pretraining setups. This makes it difficult to compare different approaches and assess the benefits of using individual fine-grained objectives.</p>
<p>To summarize, our main contributions are as follows:</p>
<ul>
<li>We propose SPARC, a novel method for pre-training multimodal models on large-scale noisy image-text data which learns both coarse-grained and fine-grained information.</li>
<li>Through an extensive experimental evaluation, we show that SPARC significantly improves performance on both fine-grained and coarse-grained downstream tasks over competing methods.</li>
<li>For the first time in the literature, we perform a thorough like-for-like comparison on the benefits of different fine-grained objectives for large-scale pretraining of multimodal models.</li>
</ul>
<h1>2. Sparse Fine-grained Contrastive Alignment</h1>
<p>Let $\mathcal{B}=\left{\left(\boldsymbol{x}<em 1="1">{1}^{\nu}, \boldsymbol{x}</em>}^{t}\right),\left(\boldsymbol{x<em 2="2">{2}^{\nu}, \boldsymbol{x}</em>}^{t}\right), \ldots,\left(\boldsymbol{x<em B="B">{B}^{\nu}, \boldsymbol{x}</em>}^{t}\right)\right}$ be a mini-batch of image-text pairs. Let $f_{v}(\cdot)$ be the image encoder, $f_{t}(\cdot)$ the text encoder and $g_{v}(\cdot)$ and $g_{t}(\cdot)$ linear adaptors. For an image $\boldsymbol{x<em 1="1" i_="i,">{i}^{\nu}$, we denote the corresponding patches as $\left(\boldsymbol{x}</em>}^{\nu}, \boldsymbol{x<em P="P" i_="i,">{i, 2}^{\nu}, \ldots, \boldsymbol{x}</em>}^{\nu}\right)$ and the patch embeddings as $\left(\boldsymbol{v<em 2="2" i_="i,">{i, 1}, \boldsymbol{v}</em>}, \ldots, \boldsymbol{v<em i_="i," p="p">{i, P}\right)$ with $\boldsymbol{v}</em>}=g_{v}\left(f_{v}\left(\boldsymbol{x<em i="i">{i, p}^{\nu}\right)\right) \in \mathbb{R}^{d} ; P$ denotes the number of patch embeddings. We calculate the global vision embedding as $\overline{\boldsymbol{v}}</em>}=g_{v}\left(h_{v}\left(\right.\right.$ avg_pool $\left.\left(\left{f_{v}\left(\boldsymbol{x<em p="1">{i, p}^{\nu}\right)\right}</em>}^{P}\right)\right)\right)$ with $h_{v}$ being a single non-linear layer that facilitates the encoding of different granularities of information. For the corresponding text $\boldsymbol{x<em 1="1" i_="i,">{i}^{t}$, we denote the tokens as $\left(\boldsymbol{x}</em>}^{t}, \boldsymbol{x<em L__i="L_{i" i_="i,">{i, 2}^{t}, \ldots, \boldsymbol{x}</em>}}^{t}\right)$ with $L_{i}$ the number of tokens for sample $i$. The token embeddings $\left(\boldsymbol{t<em 2="2" i_="i,">{i, 1}, \boldsymbol{t}</em>}, \ldots, \boldsymbol{t<em i="i">{i, L</em>}}\right)$ are computed as $\boldsymbol{t<em t="t">{i, l}=g</em>}\left(f_{t}\left(\boldsymbol{x<em i="i">{i, l}^{t}\right)\right)$ and the global text embedding $\overline{\boldsymbol{t}}</em>}$ is computed by average pooling $\left{f_{t}\left(\boldsymbol{x<em l="1">{i, l}^{t}\right)\right}</em>}^{L_{t}}$ and applying the adaptor $g_{t}$, i.e. $\overline{\boldsymbol{t}<em t="t">{i}=g</em>}\left(\right.$ avg_pool $\left.\left(\left{f_{v}\left(\boldsymbol{x<em l="1">{i, l}^{t}\right)\right}</em>\right)\right)$.}^{L_{t}</p>
<p>Global alignment: In order to learn global information, SPARC uses the global contrastive loss (Jia et al., 2021; Radford et al., 2021) which operates at the level of global image ( $\overline{\boldsymbol{v}}$ ) and global text embeddings ( $\overline{\boldsymbol{t}}$ ). Specifically, we learn image and text embeddings by maximizing the similarity to the corresponding text and image embeddings, while minimizing the similarity to other text and image embeddings in the batch, i.e. we optimize</p>
<p>$$
L_{g}=-\frac{1}{2 B} \sum_{i=1}^{B}\left(\log \frac{\exp \left(\phi\left(\overline{\boldsymbol{v}}<em i="i">{i}, \overline{\boldsymbol{t}}</em>}\right) / \tau\right)}{\sum_{j=1}^{B} \exp \left(\phi\left(\overline{\boldsymbol{v}<em j="j">{i}, \overline{\boldsymbol{t}}</em>}\right) / \tau\right)}+\log \frac{\exp \left(\phi\left(\overline{\boldsymbol{t}<em i="i">{i}, \overline{\boldsymbol{v}}</em>}\right) / \tau\right)}{\sum_{j=1}^{B} \exp \left(\phi\left(\overline{\boldsymbol{t}<em j="j">{i}, \overline{\boldsymbol{v}}</em>\right)
$$}\right) / \tau\right)</p>
<p>with $\phi\left(\overline{\boldsymbol{v}}<em j="j">{i}, \overline{\boldsymbol{t}}</em>}\right)=\frac{\overline{\boldsymbol{v}<em i="i">{i}}{\left|\overline{\boldsymbol{v}}</em>\right|<em j="j">{2}} \cdot \frac{\overline{\boldsymbol{t}}</em>}}{\left|\overline{\boldsymbol{t}<em 2="2">{j}\right|</em>$ and $\tau$ as temperature.}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Overall architecture for SPARC. The global alignment loss maximizes the similarity between the global vision and global text embeddings, while minimizing the similarity with the other global embeddings in the batch. To obtain the finegrained alignment, we compute the similarity between the patch embeddings and the token embeddings and then sparsify and normalize the resulting similarity matrix to obtain alignment weights. These alignment weights are then used to group the patch embeddings. The resulting language-grouped vision embeddings are then contrasted to the token emebddings in a sequence-wise finegrained alignment loss.</p>
<p>Finegrained alignment: Motivated by the observation that usually multiple image patches correspond to one word in the caption, we propose to learn groupings of patches that correspond to individual text tokens. Specifically, for every token embedding we learn a corresponding languagegrouped vision embedding as an alignment-weighted combination of patches that encode that token in the visual domain. We propose to compute the alignment weights based on the similarity between token and patch embeddings of the corresponding image-text pair. To facilitate the grouping of appropriate patch embeddings given a text token we sparsify and min-max normalize the similarity matrix to compute the alignment weights. To learn language-grouped vision embeddings, we propose a fine-grained local loss that optimizes for the alignment between individual token embeddings and their corresponding language-grouped vision embeddings within a given image-text pair. Specifically, we propose a sequence-wise contrastive loss to optimize this fine-grained alignment within SPARC. Optimizing this loss (in addition to the global contrastive loss above) biases the learned representation to preserve detailed information about the image (as described by the caption) instead of just the global information sufficient to minimize the global contrastive loss.</p>
<p>For an image-text pair, let $s_{i, l p}$ represent the similarity between text token embedding $\boldsymbol{t}<em i="i" p="p">{i l}$ and image patch embedding $\boldsymbol{v}</em>}$, i.e. $s_{i, l p}=\boldsymbol{t<em i="i" p="p">{i l} \cdot \boldsymbol{v}</em>$ to $[0,1]$ using min-max normalization across columns (i.e. patches):}$, where $s_{i, l p} \in \mathbb{R}^{L \times R}$ and $\cdot$ is the inner product. Going forward we drop the example index $i$ for simplicity. To obtain alignment weights, for each token $j$, we first normalize $s_{l p</p>
<p>$$
\hat{s}<em l="l" p="p">{l p}=\frac{s</em>-\min <em k="k" l="l">{k} s</em>{\max }<em k="k" l="l">{k} s</em>-\min <em k="k" l="l">{k} s</em>
$$}</p>
<p>We sparsify the similarity matrix $S=\left(\hat{s}<em 1="1" L_="L," P="P" _leq="\leq" j="j" k="k">{j k}\right)</em>$ to facilitate learning and to encourage each token to be aligned to a few of the patches, i.e.</p>
<p>$$
\tilde{s}<em j="j" k="k">{j k}= \begin{cases}\hat{s}</em>
$$} &amp; \text { if } \hat{s}_{j k} \geq \sigma \ 0 &amp; \text { otherwise }\end{cases</p>
<p>with $P$ the number of patch embeddings of an image and $\sigma$ the sparsity threshold. We compute alignment weights as</p>
<p>$$
a_{j k}=\frac{\tilde{s}<em r="1">{j k}}{\sum</em>
$$}^{R} \tilde{s}_{j r}</p>
<p>where $a_{j k}$ represents the weight of patch $k$ for computing the language-grouped vision embedding corresponding to token $j$. Note that this approach enables a flexible mapping between a token and arbitrarily many patch embeddings that encode that token in the visual domain, e.g. all of the image patches corresponding to "dog" can be matched to the token encoding "dog". For every token $t_{l}$ we compute the corresponding language-grouped vision embedding $\epsilon_{l}$ as</p>
<p>$$
\epsilon_{l}=\sum_{r=1}^{R} a_{l r} \boldsymbol{\nu}_{r}
$$</p>
<p>as the alignment-weighted combination of patch embeddings with $R$ the number of patches with non-zero alignment weight.</p>
<p>To learn fine-grained information we propose to optimize the alignment between token embeddings and their corresponding language-grouped vision embeddings. Specifically we propose a fine-grained contrastive loss that operates over sequences of tokens and patches at the level of each image-text pair and does not require negatives from other image-text pairs. This considerably reduced computation and memory costs over previous methods (Huang et al., 2021; Yao et al., 2021) that require samples from the whole batch in order to compute their fine-grained losses. SPARC optimizes the following fine-grained alignment contrastive loss</p>
<p>$$
L_{f}=-\frac{1}{2 B} \sum_{i=1}^{B}\left[\frac{1}{L_{i}} \sum_{j=1}^{L_{i}}\left(\log \frac{\exp \left(\phi\left(\epsilon_{i j}, t_{i j}\right) / \tau\right)}{\sum_{k=1}^{L_{i}} \exp \left(\phi\left(\epsilon_{i j}, t_{i k}\right) / \tau\right)}+\log \frac{\exp \left(\phi\left(t_{i j}, \epsilon_{i j}\right) / \tau\right)}{\sum_{k=1}^{L_{i}} \exp \left(\phi\left(t_{i j}, \epsilon_{i k}\right) / \tau\right)}\right)\right]
$$</p>
<p>which tries to maximize the similarity of every token embedding with its corresponding languagegrouped vision embedding and minimize the similarity to other language-grouped vision embeddings in the sequence and vice versa.</p>
<p>Overall objective: The overall SPARC objective is a weighted sum of the global contrastive loss and the finegrained alignment constrastive loss:</p>
<p>$$
L_{\text {SPARC }}=\lambda_{g} L_{g}+\lambda_{f} L_{f}
$$</p>
<p>where $\lambda_{g}$ and $\lambda_{f}$ are hyperparameters. We provide the pseudo-code for SPARC in Appendix C.
Sparsity threshold. We choose the sparsity threshold $\sigma$ to be equal to $1 / P$ with $P$ the number of image patches. This choice is motivated by the consideration that every text token should attend to at least to one image patch. Since we use the min-max normalization the smallest similarity of $1 / P$ is achieved when all patches are equally similar as the number of patches is constant. Note that this threshold naturally allows for the number of patches corresponding to one token to considerably vary between tokens within an image as well as across images; this enables the same class of objects (e.g. "dogs") to be appropriately represented irrespective of the difference in sizes, scales and shapes across different instances within and across images. Note also that the threshold also allows for the decoupling of similarities of individual patches to different tokens as it allows for different number of zero entries in different rows of the similarity matrix; thus, whether and how much a patch is similar to a token, has no bearing to how similar it is to a different token which is useful e.g. in situations when we have more detailed captions (e.g. "large brown dog") and/or when a single word is represented by multiple tokens.</p>
<h1>3. Related work</h1>
<p>Contrastive image-text pre-training CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) popularized learning general visual representations by leveraging textual supervision from noisy largescale data scrapped from the internet. These methods learn representations through a contrastive objective that maximises the similarity between the representation of the whole image and the representation of the full text of matched image-text pairs and minimizes the similarity between the remaining image-text pairs within the batch. However, learning visual representations through matching the global image and text embeddings can result in a coarse visual representation that discards many fine-grained details (i.e all details that are not needed for differentiating the matching of global text embedding from the other text embeddings in the batch). To address this problem, FILIP (Yao et al., 2021) proposes a cross-modal late interaction mechanism, which optimizes the token-wise maximum similarity between image and text tokens through a contrastive objective. While this approach achieves a finer-grained alignment between image patches and words in the text, computing the token-wise similarity between all image patches and text tokens in the batch becomes memory inefficient for large batch sizes so they use several tricks during pre-training to address this issue. A related approach PACL (Mukhoti et al., 2023) starts from CLIP-pretrained vision and text encoders and trains on top of the frozen representations an adapter to obtain better fine-grained understanding. The adapter is a two-layer MLP with a residual connection and is trained through a contrastive objective that compares the global text embedding and a weighted global image embedding with the weights calculated using the cosine similarity between individual image patches and the global text embedding.</p>
<p>In a parallel stream of work, several methods have been proposed in the medical literature to learn visual representation using medical images - radiology report pairs from small scale datasets (consisting of up to 200k data points) (Dawidowicz et al., 2023; Huang et al., 2021; Wang et al., 2022). GLoRIA (Huang et al., 2021) builds localized visual representations by contrasting attention-weighted patch embeddings with the text tokens, where the attention weights are computed through softmax on the similarity matrix between the patch and token embeddings. Similarly to FILIP, the local objective in GLoRIA requires computing the similarity between all patch and token embeddings within the batch which is computationally intensive and does not scale to large batch sizes. Alternatively, MGCA (Wang et al., 2022) considers a token-wise fine-grained loss that employs a bidirectional multi-head attention strategy to learn the matching between image patch and token embedding. While this is more efficient to compute, learning these matchings through a bidirectional multi-head cross-attention strategy adds more parameters to the dual encoders, involves tuning several additional hyperparameters and suffers from the same problems with using softmax for computing the attention weights. MGCA also uses a domain-specific disease-level alignment loss that enforce a cluster assignment consistency to leverage inter-subject semantic correspondences. More recent methods (Dawidowicz et al., 2023) consider incorporating into the pre-training objective not only fine-grained losses similar to the ones used in GLoRIA and MGCA, but also domain-specific features and image views. Note that these methods from the medical literature start from a text encoder pre-trained with medical texts (Alsentzer et al., 2019), while we consider the case of pre-training the image and text encoders jointly from scratch.</p>
<p>Fine-grained understanding in vision-language models Alternative approaches for improving the fine-grained capabilities of vision-language models require pre-trained modules, specialised networks and human annotations. One line of work, proposes matching image regions to textual descriptions through contrastive losses, where the image regions - text description pairs are obtained from human annotations (Li et al., 2022b) or by using region proposal networks (Ren et al., 2015) and various text matching approaches (Varma et al., 2023; Zhong et al., 2022). A separate line of work adds a cross-modal encoder (with significant extra parameters) on top of the dual image-text encoder</p>
<p>and uses captioning (Li et al., 2022a; Yu et al., 2022), masked language modelling (Li et al., 2021; Yang et al., 2022), image-text matching (Li et al., 2021; Yang et al., 2022; Zeng et al., 2021) and bounding box prediction losses (Zeng et al., 2021) (with bounding boxes obtained from humanannotations (Krishna et al., 2017; Kuznetsova et al., 2020; Shao et al., 2019)). For more related works see Appendix B.</p>
<h1>4. Experiments</h1>
<p>While there has been significant interest in learning fine-grained representations, the breadth of training setups used in the literature have made it difficult to compare different fine-grained objectives. Specifically the use of custom datasets (Yao et al., 2021) and pretrained language and/or vision models (Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022) have made it difficult to discern the benefit of individual fine-grained losses on learning more detailed representations. In this work we want to enable a like-for-like comparison and understand the impact of SPARC and competing fine-grained losses on downstream performance. For this purpose, we reimplement all competing baselines: CLIP (Radford et al., 2021), FILIP (Yao et al., 2021), PACL (Mukhoti et al., 2023), MGCA (Wang et al., 2022) and GLoRIA (Huang et al., 2021), and use the same pretraining datasets, architecture and number of training steps when training with the different objectives; we pretrain randomly initialized networks. We thoroughly evaluate the learned representations across a broad range of tasks and datasets, ranging from coarse-grained image-level tasks like classification and retrieval to fine-grained tasks like object detection and semantic segmentation. Unlike some competing methods that improve fine-grained understanding at the cost of decreasing coarse-grained task performance, SPARC simultaneously boosts performance over both coarse- and fine-grained tasks across a number of different benchmarks.</p>
<h3>4.1. Experimental setup</h3>
<p>Model architectures Following the literature, we use Vision Transformers (ViTs) (Dosovitskiy et al., 2020) as image encoders and Transformers (Vaswani et al., 2017) as text encoders. We experiment with ViT-B/32, ViT-B/16 and ViT-L/14 and pair them with corresponding language models. See details in Appendix D.</p>
<p>Datasets We train using large-scale datasets ALIGN (Jia et al., 2021), JFT (Sun et al., 2017; Zhai et al., 2022) and LTIP (Long Text \&amp; Image Pairs) (Alayrac et al., 2022). ALIGN has 1.8 billion images paired with noisy alt-text, JFT has of 4 billion images semi-automatically annotated with a class-hierarchy of 30k labels, while LTIP has 312 million higher-quality images - text pairs with richer image captions. See Appendix D for more details.</p>
<p>Pre-training details We resize images to the $224 \times 224$ resolution and tokenize the text with a 32 k vocabulary sentencepiece tokenizer (Kudo and Richardson, 2018) while keeping a maximum number of 55 tokens for each caption. We train all models using the AdamW (Loshchilov and Hutter, 2017) optimizer, a cosine learning rate schedule with linear warm-up and weight decay regularization. We use a batch size of 16348 and we pre-train the ViT-B models for 200k steps ( $\approx 3.2$ billion data points) and the ViT-L models for 250k steps ( $\approx 4.1$ billion data points). See Appendix D for more hyperparameter details.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Objective</th>
<th style="text-align: center;">IN</th>
<th style="text-align: center;">IN-V2 Th</th>
<th style="text-align: center;">IN-V2 MF</th>
<th style="text-align: center;">IN-V2 TI</th>
<th style="text-align: center;">IN-R</th>
<th style="text-align: center;">IN-C</th>
<th style="text-align: center;">IN-A</th>
<th style="text-align: center;">IN-Sketch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\stackrel{\rightharpoonup}{\mathrm{C}}$</td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FILIP</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PACL</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">44.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GloRIA</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">47.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MGCA</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">53.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FILIP</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PACL</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GloRIA</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">54.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MGCA</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">55.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MGCA</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">63.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">65.4</td>
</tr>
</tbody>
</table>
<p>Table 1 | Top-1 accuracy (in \%) of zero-shot classification on ImageNet (IN) and its variants ImageNetV2 Threshold (IN-V2 Th), ImageNet-V2 Matched Frequency (In-V2 MF), ImageNet-V2 Top Images (IN-V2 TI), ImageNet-R (IN-R), ImageNet-C (IN-C), ImageNet-Sketch (IN-Sketch).</p>
<h1>4.2. Zero-shot image classification</h1>
<p>We first evaluate SPARC on the coarse-grained task of zero-shot image classification. Specifically we test zero-shot classification on ImageNet (Russakovsky et al., 2015) and a number of datasets testing for specific capabilities like robustness to perturbations and various distribution shifts; we choose ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021), ImageNet-C (Hendrycks and Dietterich, 2019), ImageNet-A (Hendrycks et al., 2019) and ImageNet-Sketch (Wang et al., 2019) for this purpose. We follow a similar protocol to (Radford et al., 2021) for the evaluation, and compute results for both one prompt per example (i.e. the class label) in Table 1 and when using prompt ensembling in Table 2. For more details on the evaluation protocol please see Appendix D. From both Table 1 and Table 2 we see that SPARC outperforms or matches competing methods in all settings and across different ViT architectures. Specifically, SPARC shows very effective information encoding from larger patches as exhibited by the significant improvements over baselines for ViT B/32, especially on ImageNet-R, -C, -A and -Sketch showcasing the robustness to perturbations and adversarial examples. Moreover, we notice that while prompt ensembling improves performance of all methods on zero-shot image classification (which is in line with the literature) the performance gain from SPARC are still preserved in this evaluation setting.</p>
<p>Note that PACL (Mukhoti et al., 2023), GLoRIA (Huang et al., 2021) and MGCA (Wang et al., 2022) were developed with the use of pretrained language and/or vision encoders in mind, whereas here they are tested in a pretraining from scratch setting. From Table 1 and Table 2, we see that in the pretraining setting PACL and GLoRIA underperform CLIP, whereas MGCA shows more competitive performance to CLIP. On the other hand, FILIP (Yao et al., 2021), which was developed as a finegrained objective for pretraining from scratch, has proven highly unstable to train across a wide range of learning rates and weight decay parameters which lead to decreased performance. This training difficulty has also been noted in the original paper (Yao et al., 2021) (cf. in the Appendix</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Objective</th>
<th style="text-align: center;">IN</th>
<th style="text-align: center;">IN-V2 Th</th>
<th style="text-align: center;">IN-V2 MF</th>
<th style="text-align: center;">IN-V2 TI</th>
<th style="text-align: center;">IN-R</th>
<th style="text-align: center;">IN-C</th>
<th style="text-align: center;">IN-A</th>
<th style="text-align: center;">IN-Sketch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㅁ } \ &amp; \text { ㅁ } \ &amp; \text { ㅁ } \ &amp; \text { ㅁ } \end{aligned}$</td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FILIP</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">39.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PACL</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GloRIA</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MGCA</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FILIP</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PACL</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">45.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GloRIA</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">54.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MGCA</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">51.31</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">57.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">65.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MGCA</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">63.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">65.9</td>
</tr>
</tbody>
</table>
<p>Table 2 | Top-1 accuracy (in \%) of zero-shot classification using prompt ensembling on ImageNet (IN) and its variants ImageNet-V2 Threshold (IN-V2 Th), ImageNet-V2 Matched Frequency (In-V2 MF), ImageNet-V2 Top Images (IN-V2 TI), ImageNet-R (IN-R), ImageNet-C (IN-C), ImageNet-Sketch (IN-Sketch).
A.3. "...training is extremely unstable and the Nan loss easily happens."). In addition to that FILIP uses a number of additional tricks not present in a standard pretraining setup like image augmentations, backtranslation of captions and custom prompt ensembling.</p>
<h1>4.3. Image-Text retrieval</h1>
<p>Next we evaluate SPARC on zero-shot cross-modal retrieval tasks, i.e image-to-text and text-to-image retrieval, on Flickr30k (Plummer et al., 2015) and MSCOCO (Lin et al., 2014). From Table 3, we see that SPARC outperforms all competing baselines across all metrics. While using fine-grained losses PACL and GLoRIA significantly underperforms the global contrastive objective CLIP, MGCA shows competitive performance to CLIP in the pretraining setting. Unfortunately, FILIP (Yao et al., 2021) again underperforms CLIP across all metrics. In an attempt to stabilize FILIP we combined it with CLIP and observed an improvement on image-to-text Flikr30k on ViT B/32 while being competitive on other benchmarks to CLIP. We provide these results in Appendix D.</p>
<h3>4.4. Evaluating faithfulness</h3>
<p>We further examine fine-grained performance of SPARC through faithfulness-how consistent the model's highest scoring caption is with the ground truth caption(s) (Ji et al., 2023). This is different from top-1 retrieval (R@1) which measures exact match retrieval and does not evaluate the ability of the models to faithfully describe the elements in the image. Faithfulness has been used in the LLM literature to assess the propensity of the model to hallucinate (Adlakha et al., 2023; Razumovskaia et al., 2023) as models with higher faithfulness more accurately capture the details of the ground truth while not inserting additional information (possible hallucinations). The lexical overlap metric</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Objective</th>
<th style="text-align: center;">Flickr30k</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSCOCO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">image-to-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text-to-image</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">image-to-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text-to-image</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
</tr>
<tr>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { P } \ &amp; \text { O } \ &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">61.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">78.3</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { P } \ &amp; \text { O } \ &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">75.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">66.3</td>
</tr>
<tr>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">78.5</td>
</tr>
<tr>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">78.8</td>
</tr>
<tr>
<td style="text-align: center;">SPARC (ours)</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">80.1</td>
</tr>
</tbody>
</table>
<p>Table 3 | Results on zero-shot image-to-text and text-to-image retrieval on MSCOCO and Flickr30k datasets. R@i denotes Recall at i.
of $\mathcal{K}$-Precision measuring the proportion of tokens in the top chosen caption that appear in the ground truth tokens has been shown to correlate well with human judgement (Adlakha et al., 2023). In Table 4 we report the $\mathcal{K}$-Precision on the MSCOCO for all tokens ( $\mathcal{K}$-P), as well as $\mathcal{K}$-Precision restricted to nouns and adjectives only ( $\mathcal{K}-\mathrm{P}<em _na="{na" _text="\text">{\text {na }}$ ), as these better encode the objects observed in the image. We evaluate all methods on two architectures and see that SPARC reduced hallucinations of objects (higher $\mathcal{K}-\mathrm{P}</em>$-P).}}$ ) while also showing competitive performance to related methods when taking all tokens into account (as measured by $\mathcal{K</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ViT-B/32</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ViT-B/16</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">$\mathcal{K}-\mathrm{P}_{\text {na }}$</td>
<td style="text-align: center;">$\mathcal{K}-\mathrm{P}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathcal{K}-\mathrm{P}_{\text {na }}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">CLIP</td>
<td style="text-align: center;">76.03</td>
<td style="text-align: center;">77.82</td>
<td style="text-align: center;">77.56</td>
<td style="text-align: center;">78.99</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FILIP</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">66.83</td>
<td style="text-align: center;">66.05</td>
<td style="text-align: center;">70.09</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PACL</td>
<td style="text-align: center;">3.36</td>
<td style="text-align: center;">26.26</td>
<td style="text-align: center;">4.09</td>
<td style="text-align: center;">27.31</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GLoRIA</td>
<td style="text-align: center;">71.63</td>
<td style="text-align: center;">73.54</td>
<td style="text-align: center;">73.85</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MGCA</td>
<td style="text-align: center;">75.79</td>
<td style="text-align: center;">77.98</td>
<td style="text-align: center;">77.66</td>
<td style="text-align: center;">$\mathbf{8 0 . 0 3}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SPARC (ours)</td>
<td style="text-align: center;">$\mathbf{7 6 . 4 6}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 4 4}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 7 2}$</td>
<td style="text-align: center;">79.77</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4 | All-token $\mathcal{K}$-Precision $(\mathcal{K}-\mathrm{P})$ and the $\mathcal{K}$-Precision restricted to nouns and adjectives $\left(\mathcal{K}-\mathrm{P}_{\text {na }}\right)$ (in \%) on MSCOCO.</p>
<h1>4.5. Fine-grained localization</h1>
<p>We further examine SPARC by evaluating it on fine-grained tasks requiring precise localization such as open-vocabulary object detection and zero-shot semantic segmentation. For these evaluations, we use the ViT-B/16 architecture.</p>
<p>Open-vocabulary object detection. To first evaluate whether the improved fine-grained understanding learned with SPARC translates to tasks requiring fine-grained localization, we use SPARC as a backbone for object detection. Specifically, we used the OWL-ViT open-vocabulary object detector (Minderer et al., 2022) with a ViT-B/16 backbone. After SPARC pre-training, detection heads are added to the backbone and fine-tuned on Objects365 (Shao et al., 2019) and Visual Genome (Krishna et al., 2017) datasets following the approach in Minderer et al. (2022). We evaluate the resulting model on the large-vocabulary dataset LVIS (Gupta et al., 2019) which is well-suited for testing the transfer of knowledge from image-level pretraining. LVIS contains 1203 categories of objects, of which 307 "rare" categories are excluded from the training data to measure zero-shot transfer from pretraining. Moreover, we also evaluate detection on the 80 MSCOCO classes. We run detection training three times and report mean and standard deviation in Table 5. SPARC improves over CLIP $+0.9 \%$ on LVIS and MSCOCO as measured by mean average precision and $+3.1 \%$ on LVIS "rare" classes. Since LVIS "rare" classes are never seen during detection training data, the model has to rely on information transfer from the pretrained representations for these classes. The large improvement of SPARC over the baseline on LVIS $\mathrm{AP}_{\text {rare }}$ suggests that SPARC has learned more informative fine-grained representations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LVIS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSCOCO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">$\mathrm{AP}_{\text {all }}$</td>
<td style="text-align: center;">$\mathrm{AP}_{\text {rare }}$</td>
<td style="text-align: center;">$\mathrm{AP}_{\text {all }}$</td>
</tr>
<tr>
<td style="text-align: left;">CLIP</td>
<td style="text-align: center;">$26.9 \pm 0.12$</td>
<td style="text-align: center;">$22.0 \pm 0.79$</td>
<td style="text-align: center;">$38.5 \pm 0.19$</td>
</tr>
<tr>
<td style="text-align: left;">SPARC (ours)</td>
<td style="text-align: center;">$\mathbf{2 7 . 9} \pm \mathbf{0 . 1 1}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 1} \pm \mathbf{0 . 9 5}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 4} \pm \mathbf{0 . 1 3}$</td>
</tr>
</tbody>
</table>
<p>Table 5 | Mean Average precision (as mean $\pm$ standard deviation) on all and rare classes on LVIS and on all classes in MSCOCO.</p>
<p>Semantic Segmentation. Following related work (Mukhoti et al., 2023), we also perform zero-shot segmentation given a text label, i.e. we compute patch embeddings of a given image and calculate the cosine similarity of the patch embedding with the text embeddings of all the ground-truth classes (Mukhoti et al., 2023; Ranasinghe et al., 2022). We assign a matching class for each patch as the text that corresponds to the maximum cosine similarity of that patch. We then upsample the patches to match the resolution of the ground-truth segmentation and calculate for each class the Intersection over Union (IoU) between the predicted and ground-truth seg-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Pascal VOC</th>
<th style="text-align: center;">Pascal Context</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CLIP</td>
<td style="text-align: center;">23.02</td>
<td style="text-align: center;">20.45</td>
</tr>
<tr>
<td style="text-align: left;">FILIP</td>
<td style="text-align: center;">19.32</td>
<td style="text-align: center;">9.31</td>
</tr>
<tr>
<td style="text-align: left;">PACL</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">1.61</td>
</tr>
<tr>
<td style="text-align: left;">GLoRIA</td>
<td style="text-align: center;">22.64</td>
<td style="text-align: center;">15.26</td>
</tr>
<tr>
<td style="text-align: left;">MGCA</td>
<td style="text-align: center;">21.91</td>
<td style="text-align: center;">11.50</td>
</tr>
<tr>
<td style="text-align: left;">SPARC (ours)</td>
<td style="text-align: center;">$\mathbf{2 7 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 6 5}$</td>
</tr>
</tbody>
</table>
<p>Table 6 | Semantic Segmentation: mIoU of predicted and ground-truth segmentation on Pascal VOC and PASCAL Context datasets.
mentations; we report the mean of the IoU scores over the classes present in the ground-truth image. More details about this evaluation can found in Appendix D. From Table 6 we see that SPARC strongly improves over other baselines, significantly surpassing the next best model by +4.34 mIoU on the PASCAL VOC (Everingham et al., 2015) dataset and by +1.2 mIoU on the PASCAL Context (Mottaghi et al., 2014) dataset. We visualize the predicted</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Qualitative results for zero-shot segmentation on Pascal VOC dataset. We illustrate the original image, pixel-level ground-truth labels and the the patch-level segmentation masks obtained from SPARC, GLoRIA and CLIP.
segmentation masks on the PASCAL VOC dataset in Figure 3. Whereas CLIP predicts the object to be present in many different parts of the image, SPARC achieves better object localization and predicts their shapes more accurately.</p>
<h1>4.6. SPARC backbones in vision language models</h1>
<p>Vision backbones trained contrastively from imagetext paired data are often frozen and used in foundational vision-language models (VLMs) such as Flamingo (Alayrac et al., 2022). To understand whether the fine-grained performance improvements obtained from SPARC translate to better captioning performance in VLMs, we perform experiments where we compare using a CLIP backbone vs. a SPARC backbone in a Flamingo-style architecture (Alayrac et al., 2022). For this, we freeze the ViT-B/16 vision models trained with CLIP and SPARC and pair them with a frozen 400 M parameter (pretrained) language model. On top of the frozen vision and language backbones, we train Perceiver Resampler cross-attention layers (Alayrac et al., 2022) to produce free-form text as output. More details about the training set-up can be found in Appendix D. We evaluate the models on captioning tasks on MSCOCO and Flickr30k datasets and we report results in Table 7.</p>
<h1>4.7. Ablations</h1>
<p>To assess the benefits of the different components in SPARC on performance, we perform the following two ablations: removing the sparsity on the similarity matrix and using softmax instead to compute the alignment weights for grouping the patch embeddings. From the results in Table 8 on both fine-grained (MSCOCO retrieval) and coarse-grained (ImageNet zero-shot classification) tasks we notice that both components play a significant role in the model's performance. In particular, using softmax results in the highest decrease in performance. See Appendix A for a detailed discussion of the problems with using softmax to compute the alignment weights.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MSCOCO (i2t)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">MSCOCO (t2i)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">ImageNet <br> Top-1 acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">R@1</td>
<td style="text-align: left;">R@5</td>
<td style="text-align: left;">R@1</td>
<td style="text-align: left;">R@5</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SPARC</td>
<td style="text-align: left;">$\mathbf{5 7 . 6}$</td>
<td style="text-align: left;">$\mathbf{8 1 . 2}$</td>
<td style="text-align: left;">$\mathbf{4 3 . 0}$</td>
<td style="text-align: left;">$\mathbf{6 8 . 6}$</td>
<td style="text-align: left;">$\mathbf{7 2 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">- no sparsity</td>
<td style="text-align: left;">56.1</td>
<td style="text-align: left;">80.7</td>
<td style="text-align: left;">42.4</td>
<td style="text-align: left;">68.2</td>
<td style="text-align: left;">72.1</td>
</tr>
<tr>
<td style="text-align: left;">- softmax</td>
<td style="text-align: left;">55.2</td>
<td style="text-align: left;">79.8</td>
<td style="text-align: left;">41.6</td>
<td style="text-align: left;">67.5</td>
<td style="text-align: left;">70.6</td>
</tr>
</tbody>
</table>
<p>Table 8 | Ablations for the ViT-B/16 SPARC model on the MSCOCO image-to-text (i2t) and text-toimage (t2i) retrieval and zero-shot classification on ImageNet.</p>
<h3>4.8. Memory consumption and FLOPS</h3>
<p>To understand the computational and memory efficiency of the different methods, we also compute the FLOPS and peak memory usage for one update step for different batch size. Note that all methods are trained on 256 TPUs. In Figure 4 (a) we show the teraFLOPS (TFLOPS) and in Figure 4 (b) the peak memory usage (in MB) of the different methods for one update step when varying the batch size (B) from 2048 to 16384. Notice that GLoRIA (Huang et al., 2021) is as memory intensive at batch size 4096 as the other methods (e.g. CLIP) at batch size 16384. Thus, due to device constraints, we were only able to train GLoRIA with batch size 4096. Moreover, notice that for FILIP the TFLOPS used for one update step increases by more than $200 \%$ between $B=8196$ and $B=16384$, as opposed to the $100 \%$ increase for CLIP, SPARC and MGCA. In addition, for $B=16384$, both FILIP and PACL have $2 x$ peak memory compared to CLIP, SPARC and MGCA. On the other hand, note that CLIP, SPARC and MGCA use the same order of magnitude of FLOPS and memory. To further highlight the differences between them, we plot the relative increase in TFLOPS in Figure 4 (c) and the relative increase in peak memory in Figure 4 (c) of SPARC and MGCA with respect to CLIP. Notice that for $B=16384$, i.e. the batch size we use for our experiments, the relative increase in TFLOPS and peak memory for SPARC is almost half the one for MGCA. We provide detailed numbers for the FLOPS (in TFLOPS) and of the Peak Memory (in MB) in Appendix D.6.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid$ TFLOPS (a) and Peak Memory (b) used by all methods. Relative increase in TFLOPS (c) and Peak memory (d) when comparing SPARC and MGCA to CLIP.</p>
<h1>5. Discussion</h1>
<p>In this work we proposed a novel method Sparse Fine-grained Contrastive Alignment (SPARC) for fine-grained vision-language pretraining. SPARC simultaneously learns information at different levels of granularity by contrasting both image-level and caption-level embeddings and token and patch embeddings. SPARC learns to group patches based on similarity to tokens and contrast the resulting language-grounded patch embeddings with token embeddings. Unlike previous work this comparison is done within individual image-text pairs and does not require the computationally and memory expensive comparison of all patches and tokens within the full batch. Through extensive experimental evaluation we show that SPARC improves performance both on image-level tasks like classification and retrieval and more fine-grained tasks like object detection and segmentation that require localization. Moreover, SPARC improves model faithfulness and</p>
<p>While the simple sparsification of the similarity matrix in SPARC already improves performance, we believe that exploring different approaches to sparsification and learning patch groupings could lead to even more informative representations. Moreover, given that SPARC learns patch groupings based on the associated caption, exploring pretraining data with highly descriptive captions is another interesting line of future work. Also, leveraging bounding boxes and segmentation masks (in addition to image-text pairs) would facilitate learning patch groupings and improve learning efficiency since the similarity matrix could be pre-sparsified according to these signals. Another interesting avenue of future work is further exploring how SPARC encoders perform as part of multimodal foundational models like Flamingo (Alayrac et al., 2022), BLIP (Li et al., 2022a) and PALI (Chen et al., 2022).</p>
<h1>References</h1>
<p>V. Adlakha, P. BehnamGhader, X. H. Lu, N. Meade, and S. Reddy. Evaluating correctness and faithfulness of instruction-following models for question answering. arXiv preprint arXiv:2307.16877, 2023.</p>
<p>J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.
E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, and M. McDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.
X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.
G. Dawidowicz, E. Hirsch, and A. Tal. Limitr: Leveraging local information for medical image-text representation. ArXiv, abs/2303.11755, 2023. URL https://api.semanticscholar.org/ CorpusID:257636659.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
I. M. Elfadel and J. L. Wyatt Jr. The" softmax" nonlinearity: Derivation using statistical mechanics and useful properties as a multiterminal analog circuit element. Advances in neural information processing systems, 6, 1993.
M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1): 98-136, Jan. 2015.
S. Geng, J. Yuan, Y. Tian, Y. Chen, and Y. Zhang. Hiclip: Contrastive language-image pretraining with hierarchy-aware attention. arXiv preprint arXiv:2303.02995, 2023.
A. Gupta, P. Dollar, and R. Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356-5364, 2019.
D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples.(2019). arXiv preprint cs.LG/1907.07174, 5(6), 2019.
D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340-8349, 2021.
D. T. Hoffmann, S. Schrodi, N. Behrmann, V. Fischer, and T. Brox. Eureka-moments in transformers: Multi-step tasks reveal softmax induced optimization problems. arXiv preprint arXiv:2310.12956, 2023.</p>
<p>S.-C. Huang, L. Shen, M. P. Lungren, and S. Yeung. Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3942-3951, 2021.
Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.
C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904-4916. PMLR, 2021.
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32-73, 2017.
B. Krojer, V. Adlakha, V. Vineet, Y. Goyal, E. Ponti, and S. Reddy. Image retrieval from contextual descriptions. arXiv preprint arXiv:2203.15867, 2022.
T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 128(7):1956-1981, 2020 .
J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694-9705, 2021.
J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In International Conference on Machine Learning, pages 12888-12900. PMLR, 2022a.
L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965-10975, 2022b.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection. In European Conference on Computer Vision, pages 728-755. Springer, 2022.
R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</p>
<p>J. Mukhoti, T.-Y. Lin, O. Poursaeed, R. Wang, A. Shah, P. H. Torr, and S.-N. Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19413-19423, 2023.
R. Paiss, A. Ephrat, O. Tov, S. Zada, I. Mosseri, M. Irani, and T. Dekel. Teaching clip to count to ten. arXiv preprint arXiv:2302.12066, 2023.
L. Parcalabescu, M. Cafagna, L. Muradjan, A. Frank, I. Calixto, and A. Gatt. Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena. arXiv preprint arXiv:2112.07566, 2021.
C. Peterson and B. Söderberg. A new method for mapping optimization problems onto neural networks. International Journal of Neural Systems, 01(01):3-22, 1989.
B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641-2649, 2015.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
K. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang, A. Toshev, and J. Shlens. Perceptual grouping in vision-language models. arXiv preprint arXiv:2210.09996, 2022.
K. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang, A. Toshev, and J. Shlens. Perceptual grouping in contrastive vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5571-5584, 2023.
E. Razumovskaia, I. Vulić, P. Marković, T. Cichy, Q. Zheng, T.-H. Wen, and P. Budzianowski. Dial BeInfo for Faithfulness: Improving factuality of information-seeking dialogue via behavioural fine-tuning, 2023.
B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389-5400. PMLR, 2019.
S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211-252, 2015.
S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430-8439, 2019.
K. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in transformer. arXiv preprint arXiv:2302.06461, 2023.
C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843-852, 2017.</p>
<p>M. Varma, J.-B. Delbrouck, S. Hooper, A. Chaudhari, and C. Langlotz. Villa: Fine-grained visionlanguage representation learning from real-world data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22225-22235, 2023.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
F. Wang, Y. Zhou, S. Wang, V. Vardhanabhuti, and L. Yu. Multi-granularity cross-modal alignment for generalized medical visual representation learning. Advances in Neural Information Processing Systems, 35:33536-33549, 2022.
H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506-10518, 2019.
J. Xu, S. De Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, and X. Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18134-18144, 2022.
J. Xu, J. Hou, Y. Zhang, R. Feng, Y. Wang, Y. Qiao, and W. Xie. Learning open-vocabulary semantic segmentation models from natural language supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2935-2944, 2023.
J. Yang, J. Duan, S. Tran, Y. Xu, S. Chanda, L. Chen, B. Zeng, T. Chilimbi, and J. Huang. Visionlanguage pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671-15680, 2022.
L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li, X. Jiang, and C. Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021.
J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.
M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and why vision-language models behave like bag-of-words models, and what to do about it? arXiv preprint arXiv:2210.01936, 2022.
Y. Zeng, X. Zhang, and H. Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276, 2021.
S. Zhai, T. Likhomanenko, E. Littwin, D. Busbridge, J. Ramapuram, Y. Zhang, J. Gu, and J. Susskind. Stabilizing transformer training by preventing attention entropy collapse. ICML, 2023a.
X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104-12113, 2022.
X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. International Conference on Computer Vision, 2023b.
Y. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L. Zhou, X. Dai, L. Yuan, Y. Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793-16803, 2022.
C. Zhou, C. C. Loy, and B. Dai. Extract free dense labels from clip. In European Conference on Computer Vision, pages 696-712. Springer, 2022.</p>
<h1>A. Problems with using softmax for obtaining alignment weights</h1>
<p>Softmax is ubiquitously used to normalise activations that should or could be interpreted as probabilities, as it is for example the case of attention/alignmnet weights. One potential reason behind this choice is the dominating practice of using softmax as the output activation function for classification tasks, being the canonical link function for multinomial outputs. Another appealing property is that it acts as a differentiable max-operator, allowing for a natural interpretation of selecting one class out of multiple.</p>
<p>However, softmax can be problematic from a gradient flow perspective (Hoffmann et al., 2023; Shen et al., 2023; Zhai et al., 2023a), and in this section we will expand this observation and the implications it might have on our specific use case. Also, intuitively from its role as a soften max operator, softmax prefers to converge to peaky uni-modal distribution, selecting one out of $k$, and is less likely to represent multi-modal distributions. This is due to how gradients flow through the activation, leading to winner-takes-all dynamics (Elfadel and Wyatt Jr, 1993; Peterson and Söderberg, 1989) that ensures the peakyness and unimodality of the distribution represented.</p>
<p>If we assume $a(\mathbf{h})=\operatorname{softmax}(\mathbf{h})^{\frac{1}{2}}$, for some $\mathbf{h} \in \mathcal{R}^{k}$, then we can write the derivative as</p>
<p>$$
\frac{\partial \mathbf{a}<em j="j">{i}}{\partial \mathbf{h}</em>
\mathbf{a}}}=\left{\begin{array}{cc<em i="i">{i}-\mathbf{a}</em> i=j \
-\mathbf{a}}^{2} &amp; \text { iff <em j="j">{i} \mathbf{a}</em>
\end{array}\right.
$$} &amp; \text { otherwise </p>
<p>Assume we have some loss $L$ which is a function of $\sum_{i} \mathbf{a}<em i="i">{i} \mathbf{V}</em>}$, i.e. some values $\mathbf{V<em i="i">{i} \in \mathcal{R}^{n}$ that have been summarised using attention weights $\mathbf{a}</em>$.</p>
<p>Softmax gradients vanish at initialisation. Assume we have a large number of patches or tokens we want to attend over. In our notation, $k \gg 0$. At initialisation, all preactivation entries $\mathbf{h}<em i="i">{i}$ will be small numbers of similar magnitude. The attention weights will be uniformally distributed over the $k$ patches, leading to $\mathbf{a}</em>$ which also vanishes to 0 as $k$ grows. If we consider a very large $k$, this ensures that we have a plateau at initialization that might be hard to escape (or might take many updates to do so). See also (Hoffmann et al., 2023) for a similar observation.} \approx \frac{1}{k} \ll 1, \forall i$. Due to the weights being almost uniformally distributed, different observation will lead to randomly selecting a different patch. Therefore in expectation the gradient through the softmax on a particular token $i$ will be scaled by $\frac{1}{k^{2}}$ which will vanish very fast to 0 as $k$ grows. Note that in the rare scenario that the system picks the $i$-th element, the gradient becomes $\frac{1}{k</p>
<p>Softmax exhibits winner-takes-all dynamics. This has been understood and seen as a desirable property early on, see for example (Peterson and Söderberg, 1989) and (Elfadel and Wyatt Jr, 1993). One way to intuitively justify this behaviour is to think of the effect of applying the softmax operation multiple time (i.e. study the dynamics of a system whose transition function is just softmax). As shown in (Peterson and Söderberg, 1989) Fig. 5, the corners of the simplex act as attractors of this dynamical system, where from any initial condition, the system very quickly converges to one of the corners. This is caused by the dynamics of the gradients. When a particular weight is pushed up, all other weights are pushed down due to the normalisation. The amount by which the weight is pushed depends on its magnitude. So if a particular weight is larger and correlates positively with the desired behaviour, it will be pushed up proportionally more than other weights that correlate positively. Note that the particular form of the function (including the exponentiation) play a role in the form the gradients take, and removing the exponentiation will change the behaviour. These types of dynamics, have the downside of leading the distribution induced by the softmax to be unimodal.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>That is, softmax will act, as the name of the activation indicates, as a max operator, preferring to learn a behaviour where it picks one out of $k$, rather than multiple equally relevant candidates.</p>
<p>Softmax saturates proportional to its certainty Assume $\exists i$ such that $\forall j, j \neq i$ we have $a_{i} \gg a_{j}$. This implies that $1-a_{i} \rightarrow 0$ and $a_{j}&lt;1-a_{i}$. The gradient for the $i$-th position, according to equation 8 , will be $a_{i}\left(1-a_{i}\right)$ and will go to zero as linearly as $a_{i}$ approaches 1 . The gradient for any other position $j$, will go to 0 at the same rate, as it will be roughly $a_{j}$ which is bounded from above from $1-a_{i}$. Note that a step of size $\Delta$ on $h$, due to the exponentiation and normalization of softmax, will make $a_{i} \rightarrow 1$ exponentially fast for constant change in $h$.</p>
<h1>B. Additional related works</h1>
<p>We further expand here the discussion on achieving fine-grained understanding in vision-language models (VLMs) through additional losses and modules.</p>
<p>In addition to the approaches described in Section 3, another line of work involves proposes modifying the underlying vision transformer architecture to build modules that lead to a hierarchical grouping of image regions: e.g. GroupViT (Xu et al., 2022), OVSegmentor (Xu et al., 2023), HiCLIP (Geng et al., 2023). While these methods propose architectural changes, the objective used for training still involves having a global contrastive loss. Conversely, in our work, we use the standard vision transformer architecture and propose instead changes to the training objective to achieve finegrained understanding.</p>
<p>Moreover, note that several of these approaches (Xu et al., 2023) and the other methods who add a cross-modal encoder on top of the dual image-text encoder (Li et al., 2021; Yang et al., 2022) with captioning/masked language modelling losses start training from pre-trained text encoders and/or vision encoder.</p>
<p>Similarly, (Ranasinghe et al., 2023) improve the semantic and spatial information in dual encoders trained contrastively by changing the patch embeddings aggregation methods from average pooling to max pooling and by starting training with both pre-trained vision and language encoders. In our work, we focus specifically on the set-up of training the dual encoders from scratch.</p>
<h1>C. SPARC pseudo-code</h1>
<p>Listing 1 provides JaX-alike pseudo-code for the SPARC objective detailing the construction of both the global and the local losses.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Models:
<span class="gh">#</span> vision_encoder
<span class="gh">#</span> language_encoder
<span class="gh">#</span> Inputs:
<span class="gh">#</span> image - [B, H, W, C]
<span class="gh">#</span> text - [B, H]
<span class="gh">#</span> Hyperparameters:
<span class="gh">#</span> similarity_threshold
<span class="gh">#</span> global_loss_weight
<span class="gh">#</span> local_loss_weight
<span class="gh">#</span> inverse_temperature
def pairwise_contrastive_loss(a, b, labels):
    labels = eye(a.shape[0])
    logits_ab = dot(a <span class="gs">* b.T) *</span> inverse_temperature
    return softmax_cross_entropy(logits=logits_ab, labels=labels, reduction=&#39;mean&#39;)
def masked_pairwise_contrastive_loss(a, b, mask):
    batch_size, seq_len, _ = a.shape[0]
    mask_logits = sinshape(&#39;bnm-&gt;(bnm&#39;, 1.0 - mask, n=seq_len)
    labels = sinshape(&#39;ns-&gt;(bnm)s&#39;, eye(a.shape[1]), b=batch_size)
    logits = sinsum(&#39;bmd,bmd-&gt;bnm&#39;, a, b) <span class="gs">* inverse_temperature</span>
<span class="gs">    logits = sinshape(&#39;bnm-&gt;(bnm)m&#39;, logits)</span>
<span class="gs">    loss = softmax_cross_entropy(logits=logits - mask_logits *</span> [BF, labels=labels)
    loss = sum(loss <span class="gs">* mask) / sum(mask)</span>
<span class="gs">    return loss</span>
<span class="gs"># ---------- GLOBAL LOSS -----------</span>
<span class="gs"># encoders include adapters</span>
<span class="gs">v_patch_embed = vision_encoder(image)</span>
<span class="gs">l_token_embed, language_mask = language_encoder(text)</span>
<span class="gs">v_embed = 12_normalize(mean(v_patch_embed, axis=1), axis=-1)</span>
<span class="gs">l_embed = 12_normalize(mean(l_token_embed, axis=1), axis=-1)</span>
<span class="gs">loss_vl = pairwise_contrastive_loss(v_embed, l_embed)</span>
<span class="gs">loss_lv = pairwise_contrastive_loss(l_embed, v_embed)</span>
<span class="gs">global_loss = 0.5 *</span> (loss_vl + loss_lv)
<span class="gh">#</span> (eq 1)
<span class="gh">#</span> ---------- LOCAL LOSS -----------
<span class="gh">#</span> similarity calculation
<span class="gh">#</span> similarity = sinsum(&#39;btd,bpd-&gt;btp&#39;, l_token_embed, v_patch_embed)
<span class="gh">#</span> min-max normalisation
<span class="gh">#</span> similarity = (similarity - min(similarity, axis=-1)) /
    (max(similarity, axis=-1) - min(similarity, axis=-1))
<span class="gh">#</span> (eq 2)
<span class="gh">#</span> thresholding
similarity = where(similarity &lt; similarity_threshold, 0.0, similarity)
<span class="gh">#</span> (eq 3)
<span class="gh">#</span> alignment-weighting
v_align_weights = similarity / sum(similarity, axis=-1) # (eq 4)
l_grouped_v_patch_embed = sinsum(&#39;btp,bpd-&gt;btd&#39;, v_align_weights, v_patch_embed) # (eq 5)
l_grouped_v_patch_embed = 12_normalize(l_grouped_v_patch_embed, axis=-1)
l_token_embed = 12_normalize(l_token_embed, axis=-1)
loss_vl_local = masked_pairwise_contrastive_loss(l_grouped_v_patch_embed, l_token_embed, language_mask)
loss_lv_local = masked_pairwise_contrastive_loss(l_token_embed, l_grouped_v_patch_embed, language_mask)
local_loss = 0.5 <span class="gs">* (loss_vl_local + loss_lv_local) # (eq 6)</span>
<span class="gs"># ---------- TOTAL (SPARC) LOSS -----------</span>
<span class="gs">loss = global_loss_weight *</span> global_loss + local_loss_weight * local_loss # (eq 7)
</code></pre></div>

<p>Listing 1 | Pseudo-code for SPARC.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ By abuse of notation, we will use $\mathbf{a} \in \mathcal{R}^{k}$, where $\mathbf{a}=a(\mathbf{h})$ and use $\mathbf{a}_{i}$ for the $i$-th dimension of vector $\mathbf{a}$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7711 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7711</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7711</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-9678af027022328f143ee2b627ee93ba313a4e7c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9678af027022328f143ee2b627ee93ba313a4e7c" target="_blank">LLM4SR: A Survey on Large Language Models for Scientific Research</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing the scientific research process, and analyzes the unique roles LLMs play across four critical stages of research: hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing.</p>
                <p><strong>Paper Abstract:</strong> In recent years, the rapid advancement of Large Language Models (LLMs) has transformed the landscape of scientific research, offering unprecedented support across various stages of the research cycle. This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing the scientific research process. We analyze the unique roles LLMs play across four critical stages of research: hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing. Our review comprehensively showcases the task-specific methodologies and evaluation benchmarks. By identifying current challenges and proposing future research directions, this survey not only highlights the transformative potential of LLMs, but also aims to inspire and guide researchers and practitioners in leveraging LLMs to advance scientific inquiry. Resources are available at the following repository: https://github.com/du-nlp-lab/LLM4SR</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7711.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7711.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-based discovery style system that retrieves semantically similar knowledge, knowledge-graph neighbors, and citation-graph neighbors as inspirations and composes hypotheses constrained in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves inspiration knowledge for a given research background via semantic similarity (SentenceBERT embeddings), concept co-occurrence, and citation-graph neighbors, then uses these inspirations to compose candidate hypotheses in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>research background (text), literature corpora (papers), citation graph, extracted concepts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses (natural language hypothesis statements)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval-driven composition; uses semantic embeddings for inspiration retrieval (no specific LLM prompting described)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>large collections of scientific publications (semantic scholar / corpora implied)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>rediscovery of ground-truth hypotheses; novelty/validity/clarity dimensions (as discussed generally)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported as an instantiation of LBD-style inspiration retrieval; shown in survey to be one of several effective inspiration strategies (no numeric metrics given in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on retrieval heuristics and existing concept co-occurrence; does not itself provide experimental validation; potential to generate non-novel or shallow hypotheses without additional novelty checks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7711.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-centric framework that selects inspirations via LLMs and applies iterative feedback modules (novelty, validity, clarity) to generate and refine scientific hypotheses, including an automated research-question discovery mode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to (1) select inspiration knowledge given a research background, (2) generate many candidate hypotheses, and (3) iteratively refine them using LLM-based novelty, validity and clarity checkers; can also operate in an automated 'full-self-driving' mode to construct research questions from web corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>research background (text), candidate inspiration candidates (papers/concepts), web corpus (for automated mode)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>refined textual hypotheses and constructed research questions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM selection of inspirations; LLM self-assessment feedback loops (novelty/validity/clarity); iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>discipline web corpora and scientific literature (as applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>novelty/validity/clarity judgments (often via LLM evaluators and expert evaluation); ability to rediscover known hypotheses (reference-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey reports MOOSE introduced the novelty/validity/clarity feedback pipeline and automated research-question construction; used in downstream chemistry/social-science experiments (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Heavy reliance on LLM heuristics for validity (no routine wet-lab verification); outcome quality bounded by LLM ability and training-data contamination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7711.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of MOOSE tailored to chemistry/materials that adds evolutionary units and leveraging of multiple inspirations to reconstruct published chemistry hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Built on LLM-selected inspirations, it generates multiple unique hypotheses per inspiration pair, refines each independently, and recombines them (evolutionary unit) while explicitly leveraging multiple inspirations in sequence to produce publishable chemistry/materials hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>background text, candidate inspirations (papers/concepts), annotated examples (in analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses with compositional steps and detailed chemical/material suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM selection + iterative refinement + evolutionary recombination of multiple hypothesis variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>chemistry/materials literature; survey notes an annotation of 51 2024 chemistry papers used to analyze retrieval performance</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>rediscovery rate of annotated inspirations; expert evaluation of hypothesis validity/novelty/clarity</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey reports a very high retrieval rate when LLMs (trained up to 2023) were asked to retrieve inspirations for 51 2024 chemistry papers, indicating strong internal inspiration-selection ability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Validity remains difficult to verify without experiments; expert evaluation in chemistry may still be unreliable; method depends on LLM internalized training data and may face data-contamination issues.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7711.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concept-graph-based system that follows an ABC literature-based discovery model, retrieving inspiration concepts via concept co-occurrence and refining hypotheses with LLM-based feedback on novelty, validity, and clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs a concept graph where links indicate co-occurrence in papers; retrieves inspiration concepts connected to background concepts and uses LLMs to generate and refine hypotheses with novelty/validity/clarity assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>concept graph derived from paper texts (concept co-occurrence), background text</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>natural language hypotheses and refined proposal texts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>concept co-occurrence retrieval + LLM generation and self-assessment feedback</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>paper corpora used to build concept co-occurrence graph (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>automatic LLM-based scoring on novelty/validity/clarity; expert assessment in reported studies</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey lists ResearchAgent among systems achieving generated hypotheses with iterative LLM feedback; quantitative metrics not reported in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependent on quality of concept extraction and co-occurrence signals; LLM-based validity remains heuristic without experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7711.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciPIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciPIP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discovery pipeline that retrieves inspirations from semantic similarity, concept co-occurrence and citation neighbors and applies filtering to remove unhelpful concepts before hypothesis composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciPIP</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines multiple inspiration retrieval strategies (SentenceBERT semantic similarity, concept co-occurrence, citation graph neighbors) and applies filtering heuristics to produce inspiration sets used to generate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>background survey text, literature corpus, citation graph, sentence embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual hypotheses and candidate inspiration lists</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval-then-generation; filtering of retrieved concepts before LLM composition</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>scientific publication corpora (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>reference-based rediscovery metrics and human evaluation on novelty/validity/clarity (as per survey conventions)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey indicates SciPIP uses multi-source retrieval with filtering to improve inspiration quality; no numeric results in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Effectiveness depends on retrieval quality and filters; validation of hypotheses still challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7711.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that retrieves inspiration papers via semantic matching and concept matching across topic/subarea boundaries to compose novel hypotheses from a background.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves inspiration papers using a combination of semantic scholar API recommendations and concept matching (same topic, same subarea, different subarea) to provide diverse inspirations for hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>background text, paper corpus, semantic scholar retrieval API</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>generated hypotheses in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval-augmented generation using semantic and concept matching; LLM generation of hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>semantic scholar / paper corpora (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>novelty/validity/clarity via LLM and/or expert assessment (general survey metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey describes Scideator's retrieval strategy as enabling cross-subarea inspiration; no quantitative metrics given in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on retrieval APIs and concept matching heuristics; may produce hypotheses requiring costly validation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7711.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciAgents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that samples inspirations randomly via citation-graph paths and leverages LLM agents to generate and evaluate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciAgents</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Randomly samples inspirations connected to background concepts via citation-graph paths (short or long) and uses LLM-based agents to propose and assess hypotheses, including novelty/validity feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>background text, citation graph, retrieved papers</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>candidate hypotheses (text)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>randomized inspiration sampling from citation graphs combined with LLM-based generation and assessment</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>citation-indexed literature corpora</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>LLM-based novelty/validity checks and human evaluation in some studies</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey lists SciAgents as employing random inspiration sampling to increase diversity; detailed metrics not provided in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Random sampling may increase diversity but reduce coherence or relevance; same validation challenges apply.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7711.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses LLM internal knowledge to select inspirations and emphasizes leveraging multiple inspirations to improve hypothesis diversity and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Relies on LLMs to select promising inspiration candidates from provided options and incorporates multiple inspirations sequentially to expand diversity and novelty of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>background text, candidate inspiration set (papers/concepts)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>diverse textual seed ideas and hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM-selected inspirations; sequential incorporation of multiple inspirations to generate diverse outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>paper corpora / annotated idea patterns (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>diversity/novelty metrics and pairwise LLM comparison methods (survey discusses pairwise evaluation approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey reports Nova's goal is increased diversity and that it uses LLM selection for inspirations; quantitative results not provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on LLM internal knowledge and may be sensitive to training data cutoff/contamination; validity checks remain limited.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7711.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early system framing a 'full-self-driving' automated research pipeline that can search for research questions, propose hypotheses, and rank/prioritize them for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AIScientist</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Operates an LLM-based agent loop that can autonomously explore web corpora to identify research questions, generate hypotheses, and use ranking/evaluation modules to prioritize ideas for experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>web corpus, scientific literature, possibly code/experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>research questions, candidate hypotheses, ranked lists</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>agent-based autonomous search with LLM generation and automatic evaluation/ranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>public web and literature corpora (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>automatic LLM scoring, ranking consistency, and in some studies reference-based rediscovery</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey includes AIScientist as demonstrating automated question discovery and ranking; detailed numeric outcomes are in original source.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Scalability and reliability concerns for fully autonomous operation; experimental validation and real-world lab execution remain bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7711.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that autonomously writes comprehensive surveys by synthesizing and organizing existing research literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to retrieve, synthesize, and organize literature into survey-style outputs, producing structured overviews and synthesized discussions of research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>collections of scientific papers (full-text or abstracts), retrieved references</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>survey articles / structured literature syntheses (long-form text)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval-augmented generation and multi-document synthesis (as described in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>broad scientific corpora (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>human evaluation on fluency, coverage, factuality; reference-based checks for coverage (survey-level metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey notes AutoSurvey demonstrates autonomous survey writing capability; specifics are in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Hallucination risk, citation/factual inaccuracies, limited by retrieval system quality and LLM knowledge cutoff.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7711.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Retrieval-Augmented Generation (RAG) system designed to retrieve and re-rank relevant papers from websites and produce grounded literature-review outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines external retrieval from web sources with LLM generation; re-ranks retrieved papers and uses RAG to reduce hallucinations while producing literature-review style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>web-retrieved papers, metadata, background prompt</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>grounded literature-review summaries / related-work passages</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with re-ranking and LLM-conditioned summarization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>web literature sources and paper repositories (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>coverage, factuality, re-ranking quality, and human evaluation on relevance and correctness</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey describes LitLLM as reducing hallucination via RAG and re-ranking; numerical results not reported in the survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance constrained by retrieval quality and index freshness; may still hallucinate if retrieval misses key sources.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7711.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiReview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiReview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that integrates RAG-based LLMs with graph-based hierarchical clustering to generate hierarchical taxonomies and cluster-level summaries of literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>HiReview</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves relevant papers, clusters them via citation-network based hierarchical clustering to produce sub-community taxonomies, and uses LLMs to generate summaries for each cluster to ensure coverage and organization.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>citation network of papers, retrieved documents</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>hierarchical taxonomy tree + cluster summaries (structured literature synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>RAG combined with graph-based clustering and LLM summarization per cluster</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>citation-networked paper corpora (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>coverage and organization of taxonomy, human evaluation on completeness and coherence</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey highlights HiReview's hierarchical approach to reduce hallucination and improve coverage; exact metrics not provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on quality of clustering and retrieval; potential mismatches if clusters mix heterogeneous topics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7711.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7711.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CycleResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Part of a dual system (CycleResearcher/CycleReviewer) where one agent formulates ideas and writes papers while another scores papers to provide preference data to refine the idea generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CycleResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLM agents to brainstorm ideas and draft papers, while a paired CycleReviewer agent provides scores; reviewer feedback is used as preference data to iteratively train or refine the researcher agent.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>input papers, generated drafts, reviewer feedback signals</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>generated research ideas and drafted paper text</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>agent loop with iterative human/LLM reviewer feedback used as preference learning signal</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>corpora of papers and generated drafts (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>preference-based evaluation, reviewer scoring consistency, downstream idea quality</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Survey describes synergy between researcher and reviewer agents, enabling preference-data-driven improvement; numeric results are in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on reviewer model calibration; risk of feedback loops that amplify model biases; validation still requires expert review or experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM4SR: A Survey on Large Language Models for Scientific Research', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MOOSE <em>(Rating: 2)</em></li>
                <li>MOOSE-Chem <em>(Rating: 2)</em></li>
                <li>SciMON <em>(Rating: 2)</em></li>
                <li>ResearchAgent <em>(Rating: 2)</em></li>
                <li>SciPIP <em>(Rating: 2)</em></li>
                <li>Scideator <em>(Rating: 2)</em></li>
                <li>Nova <em>(Rating: 2)</em></li>
                <li>AIScientist <em>(Rating: 2)</em></li>
                <li>AutoSurvey <em>(Rating: 2)</em></li>
                <li>LitLLM <em>(Rating: 1)</em></li>
                <li>HiReview <em>(Rating: 1)</em></li>
                <li>CycleResearcher <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7711",
    "paper_id": "paper-9678af027022328f143ee2b627ee93ba313a4e7c",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "SciMON",
            "name_full": "SciMON",
            "brief_description": "A literature-based discovery style system that retrieves semantically similar knowledge, knowledge-graph neighbors, and citation-graph neighbors as inspirations and composes hypotheses constrained in natural language.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "SciMON",
            "method_description": "Retrieves inspiration knowledge for a given research background via semantic similarity (SentenceBERT embeddings), concept co-occurrence, and citation-graph neighbors, then uses these inspirations to compose candidate hypotheses in natural language.",
            "input_type": "research background (text), literature corpora (papers), citation graph, extracted concepts",
            "output_type": "textual hypotheses (natural language hypothesis statements)",
            "prompting_technique": "retrieval-driven composition; uses semantic embeddings for inspiration retrieval (no specific LLM prompting described)",
            "model_name": null,
            "model_size": null,
            "datasets_used": "large collections of scientific publications (semantic scholar / corpora implied)",
            "evaluation_metric": "rediscovery of ground-truth hypotheses; novelty/validity/clarity dimensions (as discussed generally)",
            "reported_results": "Reported as an instantiation of LBD-style inspiration retrieval; shown in survey to be one of several effective inspiration strategies (no numeric metrics given in survey text).",
            "limitations": "Relies on retrieval heuristics and existing concept co-occurrence; does not itself provide experimental validation; potential to generate non-novel or shallow hypotheses without additional novelty checks.",
            "counterpoint": true,
            "uuid": "e7711.0",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MOOSE",
            "name_full": "MOOSE",
            "brief_description": "An LLM-centric framework that selects inspirations via LLMs and applies iterative feedback modules (novelty, validity, clarity) to generate and refine scientific hypotheses, including an automated research-question discovery mode.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "MOOSE",
            "method_description": "Uses LLMs to (1) select inspiration knowledge given a research background, (2) generate many candidate hypotheses, and (3) iteratively refine them using LLM-based novelty, validity and clarity checkers; can also operate in an automated 'full-self-driving' mode to construct research questions from web corpora.",
            "input_type": "research background (text), candidate inspiration candidates (papers/concepts), web corpus (for automated mode)",
            "output_type": "refined textual hypotheses and constructed research questions",
            "prompting_technique": "LLM selection of inspirations; LLM self-assessment feedback loops (novelty/validity/clarity); iterative refinement",
            "model_name": null,
            "model_size": null,
            "datasets_used": "discipline web corpora and scientific literature (as applicable)",
            "evaluation_metric": "novelty/validity/clarity judgments (often via LLM evaluators and expert evaluation); ability to rediscover known hypotheses (reference-based evaluation)",
            "reported_results": "Survey reports MOOSE introduced the novelty/validity/clarity feedback pipeline and automated research-question construction; used in downstream chemistry/social-science experiments (details in original paper).",
            "limitations": "Heavy reliance on LLM heuristics for validity (no routine wet-lab verification); outcome quality bounded by LLM ability and training-data contamination risk.",
            "counterpoint": true,
            "uuid": "e7711.1",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MOOSE-Chem",
            "name_full": "MOOSE-Chem",
            "brief_description": "An extension of MOOSE tailored to chemistry/materials that adds evolutionary units and leveraging of multiple inspirations to reconstruct published chemistry hypotheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "MOOSE-Chem",
            "method_description": "Built on LLM-selected inspirations, it generates multiple unique hypotheses per inspiration pair, refines each independently, and recombines them (evolutionary unit) while explicitly leveraging multiple inspirations in sequence to produce publishable chemistry/materials hypotheses.",
            "input_type": "background text, candidate inspirations (papers/concepts), annotated examples (in analysis)",
            "output_type": "textual hypotheses with compositional steps and detailed chemical/material suggestions",
            "prompting_technique": "LLM selection + iterative refinement + evolutionary recombination of multiple hypothesis variants",
            "model_name": null,
            "model_size": null,
            "datasets_used": "chemistry/materials literature; survey notes an annotation of 51 2024 chemistry papers used to analyze retrieval performance",
            "evaluation_metric": "rediscovery rate of annotated inspirations; expert evaluation of hypothesis validity/novelty/clarity",
            "reported_results": "Survey reports a very high retrieval rate when LLMs (trained up to 2023) were asked to retrieve inspirations for 51 2024 chemistry papers, indicating strong internal inspiration-selection ability.",
            "limitations": "Validity remains difficult to verify without experiments; expert evaluation in chemistry may still be unreliable; method depends on LLM internalized training data and may face data-contamination issues.",
            "counterpoint": true,
            "uuid": "e7711.2",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent",
            "brief_description": "A concept-graph-based system that follows an ABC literature-based discovery model, retrieving inspiration concepts via concept co-occurrence and refining hypotheses with LLM-based feedback on novelty, validity, and clarity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "ResearchAgent",
            "method_description": "Constructs a concept graph where links indicate co-occurrence in papers; retrieves inspiration concepts connected to background concepts and uses LLMs to generate and refine hypotheses with novelty/validity/clarity assessments.",
            "input_type": "concept graph derived from paper texts (concept co-occurrence), background text",
            "output_type": "natural language hypotheses and refined proposal texts",
            "prompting_technique": "concept co-occurrence retrieval + LLM generation and self-assessment feedback",
            "model_name": null,
            "model_size": null,
            "datasets_used": "paper corpora used to build concept co-occurrence graph (unspecified)",
            "evaluation_metric": "automatic LLM-based scoring on novelty/validity/clarity; expert assessment in reported studies",
            "reported_results": "Survey lists ResearchAgent among systems achieving generated hypotheses with iterative LLM feedback; quantitative metrics not reported in survey text.",
            "limitations": "Dependent on quality of concept extraction and co-occurrence signals; LLM-based validity remains heuristic without experiments.",
            "counterpoint": true,
            "uuid": "e7711.3",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SciPIP",
            "name_full": "SciPIP",
            "brief_description": "A discovery pipeline that retrieves inspirations from semantic similarity, concept co-occurrence and citation neighbors and applies filtering to remove unhelpful concepts before hypothesis composition.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "SciPIP",
            "method_description": "Combines multiple inspiration retrieval strategies (SentenceBERT semantic similarity, concept co-occurrence, citation graph neighbors) and applies filtering heuristics to produce inspiration sets used to generate hypotheses.",
            "input_type": "background survey text, literature corpus, citation graph, sentence embeddings",
            "output_type": "textual hypotheses and candidate inspiration lists",
            "prompting_technique": "retrieval-then-generation; filtering of retrieved concepts before LLM composition",
            "model_name": null,
            "model_size": null,
            "datasets_used": "scientific publication corpora (unspecified)",
            "evaluation_metric": "reference-based rediscovery metrics and human evaluation on novelty/validity/clarity (as per survey conventions)",
            "reported_results": "Survey indicates SciPIP uses multi-source retrieval with filtering to improve inspiration quality; no numeric results in survey excerpt.",
            "limitations": "Effectiveness depends on retrieval quality and filters; validation of hypotheses still challenging.",
            "counterpoint": true,
            "uuid": "e7711.4",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Scideator",
            "name_full": "Scideator",
            "brief_description": "A system that retrieves inspiration papers via semantic matching and concept matching across topic/subarea boundaries to compose novel hypotheses from a background.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "Scideator",
            "method_description": "Retrieves inspiration papers using a combination of semantic scholar API recommendations and concept matching (same topic, same subarea, different subarea) to provide diverse inspirations for hypothesis generation.",
            "input_type": "background text, paper corpus, semantic scholar retrieval API",
            "output_type": "generated hypotheses in natural language",
            "prompting_technique": "retrieval-augmented generation using semantic and concept matching; LLM generation of hypotheses",
            "model_name": null,
            "model_size": null,
            "datasets_used": "semantic scholar / paper corpora (unspecified)",
            "evaluation_metric": "novelty/validity/clarity via LLM and/or expert assessment (general survey metrics)",
            "reported_results": "Survey describes Scideator's retrieval strategy as enabling cross-subarea inspiration; no quantitative metrics given in excerpt.",
            "limitations": "Relies on retrieval APIs and concept matching heuristics; may produce hypotheses requiring costly validation.",
            "counterpoint": true,
            "uuid": "e7711.5",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SciAgents",
            "name_full": "SciAgents",
            "brief_description": "A multi-agent framework that samples inspirations randomly via citation-graph paths and leverages LLM agents to generate and evaluate hypotheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "SciAgents",
            "method_description": "Randomly samples inspirations connected to background concepts via citation-graph paths (short or long) and uses LLM-based agents to propose and assess hypotheses, including novelty/validity feedback.",
            "input_type": "background text, citation graph, retrieved papers",
            "output_type": "candidate hypotheses (text)",
            "prompting_technique": "randomized inspiration sampling from citation graphs combined with LLM-based generation and assessment",
            "model_name": null,
            "model_size": null,
            "datasets_used": "citation-indexed literature corpora",
            "evaluation_metric": "LLM-based novelty/validity checks and human evaluation in some studies",
            "reported_results": "Survey lists SciAgents as employing random inspiration sampling to increase diversity; detailed metrics not provided in survey excerpt.",
            "limitations": "Random sampling may increase diversity but reduce coherence or relevance; same validation challenges apply.",
            "counterpoint": true,
            "uuid": "e7711.6",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Nova",
            "name_full": "Nova",
            "brief_description": "A system that uses LLM internal knowledge to select inspirations and emphasizes leveraging multiple inspirations to improve hypothesis diversity and novelty.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "Nova",
            "method_description": "Relies on LLMs to select promising inspiration candidates from provided options and incorporates multiple inspirations sequentially to expand diversity and novelty of generated hypotheses.",
            "input_type": "background text, candidate inspiration set (papers/concepts)",
            "output_type": "diverse textual seed ideas and hypotheses",
            "prompting_technique": "LLM-selected inspirations; sequential incorporation of multiple inspirations to generate diverse outputs",
            "model_name": null,
            "model_size": null,
            "datasets_used": "paper corpora / annotated idea patterns (unspecified)",
            "evaluation_metric": "diversity/novelty metrics and pairwise LLM comparison methods (survey discusses pairwise evaluation approaches)",
            "reported_results": "Survey reports Nova's goal is increased diversity and that it uses LLM selection for inspirations; quantitative results not provided in excerpt.",
            "limitations": "Depends on LLM internal knowledge and may be sensitive to training data cutoff/contamination; validity checks remain limited.",
            "counterpoint": true,
            "uuid": "e7711.7",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "AIScientist",
            "name_full": "AI Scientist",
            "brief_description": "An early system framing a 'full-self-driving' automated research pipeline that can search for research questions, propose hypotheses, and rank/prioritize them for execution.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "AIScientist",
            "method_description": "Operates an LLM-based agent loop that can autonomously explore web corpora to identify research questions, generate hypotheses, and use ranking/evaluation modules to prioritize ideas for experimentation.",
            "input_type": "web corpus, scientific literature, possibly code/experimental data",
            "output_type": "research questions, candidate hypotheses, ranked lists",
            "prompting_technique": "agent-based autonomous search with LLM generation and automatic evaluation/ranking",
            "model_name": null,
            "model_size": null,
            "datasets_used": "public web and literature corpora (unspecified)",
            "evaluation_metric": "automatic LLM scoring, ranking consistency, and in some studies reference-based rediscovery",
            "reported_results": "Survey includes AIScientist as demonstrating automated question discovery and ranking; detailed numeric outcomes are in original source.",
            "limitations": "Scalability and reliability concerns for fully autonomous operation; experimental validation and real-world lab execution remain bottlenecks.",
            "counterpoint": true,
            "uuid": "e7711.8",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey",
            "brief_description": "An LLM-based system that autonomously writes comprehensive surveys by synthesizing and organizing existing research literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "AutoSurvey",
            "method_description": "Uses LLMs to retrieve, synthesize, and organize literature into survey-style outputs, producing structured overviews and synthesized discussions of research areas.",
            "input_type": "collections of scientific papers (full-text or abstracts), retrieved references",
            "output_type": "survey articles / structured literature syntheses (long-form text)",
            "prompting_technique": "retrieval-augmented generation and multi-document synthesis (as described in survey)",
            "model_name": null,
            "model_size": null,
            "datasets_used": "broad scientific corpora (unspecified)",
            "evaluation_metric": "human evaluation on fluency, coverage, factuality; reference-based checks for coverage (survey-level metrics)",
            "reported_results": "Survey notes AutoSurvey demonstrates autonomous survey writing capability; specifics are in the original paper.",
            "limitations": "Hallucination risk, citation/factual inaccuracies, limited by retrieval system quality and LLM knowledge cutoff.",
            "counterpoint": true,
            "uuid": "e7711.9",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM",
            "brief_description": "A Retrieval-Augmented Generation (RAG) system designed to retrieve and re-rank relevant papers from websites and produce grounded literature-review outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "LitLLM",
            "method_description": "Combines external retrieval from web sources with LLM generation; re-ranks retrieved papers and uses RAG to reduce hallucinations while producing literature-review style outputs.",
            "input_type": "web-retrieved papers, metadata, background prompt",
            "output_type": "grounded literature-review summaries / related-work passages",
            "prompting_technique": "Retrieval-Augmented Generation (RAG) with re-ranking and LLM-conditioned summarization",
            "model_name": null,
            "model_size": null,
            "datasets_used": "web literature sources and paper repositories (unspecified)",
            "evaluation_metric": "coverage, factuality, re-ranking quality, and human evaluation on relevance and correctness",
            "reported_results": "Survey describes LitLLM as reducing hallucination via RAG and re-ranking; numerical results not reported in the survey excerpt.",
            "limitations": "Performance constrained by retrieval quality and index freshness; may still hallucinate if retrieval misses key sources.",
            "counterpoint": true,
            "uuid": "e7711.10",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "HiReview",
            "name_full": "HiReview",
            "brief_description": "A system that integrates RAG-based LLMs with graph-based hierarchical clustering to generate hierarchical taxonomies and cluster-level summaries of literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "HiReview",
            "method_description": "Retrieves relevant papers, clusters them via citation-network based hierarchical clustering to produce sub-community taxonomies, and uses LLMs to generate summaries for each cluster to ensure coverage and organization.",
            "input_type": "citation network of papers, retrieved documents",
            "output_type": "hierarchical taxonomy tree + cluster summaries (structured literature synthesis)",
            "prompting_technique": "RAG combined with graph-based clustering and LLM summarization per cluster",
            "model_name": null,
            "model_size": null,
            "datasets_used": "citation-networked paper corpora (unspecified)",
            "evaluation_metric": "coverage and organization of taxonomy, human evaluation on completeness and coherence",
            "reported_results": "Survey highlights HiReview's hierarchical approach to reduce hallucination and improve coverage; exact metrics not provided in excerpt.",
            "limitations": "Relies on quality of clustering and retrieval; potential mismatches if clusters mix heterogeneous topics.",
            "counterpoint": true,
            "uuid": "e7711.11",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "CycleResearcher",
            "name_full": "CycleResearcher",
            "brief_description": "Part of a dual system (CycleResearcher/CycleReviewer) where one agent formulates ideas and writes papers while another scores papers to provide preference data to refine the idea generator.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "authors": "",
            "year": null,
            "method_name": "CycleResearcher",
            "method_description": "Uses LLM agents to brainstorm ideas and draft papers, while a paired CycleReviewer agent provides scores; reviewer feedback is used as preference data to iteratively train or refine the researcher agent.",
            "input_type": "input papers, generated drafts, reviewer feedback signals",
            "output_type": "generated research ideas and drafted paper text",
            "prompting_technique": "agent loop with iterative human/LLM reviewer feedback used as preference learning signal",
            "model_name": null,
            "model_size": null,
            "datasets_used": "corpora of papers and generated drafts (unspecified)",
            "evaluation_metric": "preference-based evaluation, reviewer scoring consistency, downstream idea quality",
            "reported_results": "Survey describes synergy between researcher and reviewer agents, enabling preference-data-driven improvement; numeric results are in original work.",
            "limitations": "Quality depends on reviewer model calibration; risk of feedback loops that amplify model biases; validation still requires expert review or experiments.",
            "counterpoint": true,
            "uuid": "e7711.12",
            "source_info": {
                "paper_title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MOOSE",
            "rating": 2
        },
        {
            "paper_title": "MOOSE-Chem",
            "rating": 2,
            "sanitized_title": "moosechem"
        },
        {
            "paper_title": "SciMON",
            "rating": 2
        },
        {
            "paper_title": "ResearchAgent",
            "rating": 2,
            "sanitized_title": "researchagent"
        },
        {
            "paper_title": "SciPIP",
            "rating": 2
        },
        {
            "paper_title": "Scideator",
            "rating": 2
        },
        {
            "paper_title": "Nova",
            "rating": 2
        },
        {
            "paper_title": "AIScientist",
            "rating": 2,
            "sanitized_title": "aiscientist"
        },
        {
            "paper_title": "AutoSurvey",
            "rating": 2,
            "sanitized_title": "autosurvey"
        },
        {
            "paper_title": "LitLLM",
            "rating": 1
        },
        {
            "paper_title": "HiReview",
            "rating": 1
        },
        {
            "paper_title": "CycleResearcher",
            "rating": 1,
            "sanitized_title": "cycleresearcher"
        }
    ],
    "cost": 0.018286499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM4SR: A Survey on Large Language Models for Scientific Research</h1>
<p>ZIMING LUO<em>, University of Texas at Dallas, USA<br>ZONGLIN YANG</em>, Nanyang Technological University, Singapore<br>ZEXIN XU, University of Texas at Dallas, USA<br>WEI YANG, University of Texas at Dallas, USA<br>XINYA DU, University of Texas at Dallas, USA</p>
<p>In recent years, the rapid advancement of Large Language Models (LLMs) has transformed the landscape of scientific research, offering unprecedented support across various stages of the research cycle. This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing the scientific research process. We analyze the unique roles LLMs play across four critical stages of research: hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing. Our review comprehensively showcases the task-specific methodologies and evaluation benchmarks. By identifying current challenges and proposing future research directions, this survey not only highlights the transformative potential of LLMs, but also aims to inspire and guide researchers and practitioners in leveraging LLMs to advance scientific inquiry. Resources are available at the following repository: https://github.com/du-nlp-lab/LLM4SR.
CCS Concepts: $\cdot$ Computing methodologies $\rightarrow$ Natural language processing: $\cdot$ General and reference $\rightarrow$ Surveys and overviews.
Additional Key Words and Phrases: Large Language Models, Scientific Hypothesis Discovery, Experiment Planning and Implementation, Automated Scientific Writing, Peer Review Generation</p>
<h2>ACM Reference Format:</h2>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. 2025. LLM4SR: A Survey on Large Language Models for Scientific Research. ACM Comput. Surv. 1, 1 (January 2025), 37 pages. https://doi.org/10.1145/ nnnnnnnn.nnnnnnn
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Schematic overview of the scientific research pipeline covered in this survey. This cyclical process begins with scientific hypothesis discovery, followed by experiment planning and implementation, paper writing, and finally peer reviewing of papers. The experiment planning stage consists of optimizing experiment design and executing research tasks, while the paper writing stage consists of citation text generation, related work generation, and drafting \&amp; writing.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>"If I have seen further, it is by standing on the shoulders of giants."</p>
<ul>
<li>Isaac Newton</li>
</ul>
<p>The scientific research pipeline is a testament to the achievements of the Enlightenment in systematic inquiry [17, 58, 58]. In this traditional paradigm, scientific research involves a series of well-defined steps: researchers start by gathering background knowledge, propose hypotheses, design and execute experiments, collect and analyze data, and finally report findings through a manuscript that undergoes peer review. This cyclical process has led to groundbreaking advancements in modern science and technology, yet it remains constrained by the creativity, expertise, and finite time and resources available inherent to human researchers.</p>
<p>For decades, the scientific community has sought to enhance this process by automating aspects of scientific research, aiming to increase the productivity of scientists. Early computer-assisted research can date back to the 1970s, introducing systems such as Automated Mathematician [74, 75] and BACON [71], which showed the potential of machines to assist in specialized research tasks like theorem generation and empirical law identification. More recently, systems such as AlphaFold [62] and OpenFold [4] have exemplified pioneering efforts to automate specific research tasks, significantly speeding up scientific progress in their respective domains by thousands of times. Yet it was only with the advent of foundation models and the recent explosion in Large Language Models (LLMs) [2, 154] that the vision of comprehensive AI assistance across multiple research domains became realistic [190].</p>
<p>The recent years have witnessed remarkable advancements in LLMs, transforming various fields of AI and Natural Language Processing (NLP). These models, such as GPT-4 [2] and LLaMA [154], have set new benchmarks in understanding, generating and interacting with human language. Their capabilities, enhanced by massive datasets and innovative architectures, now extend beyond conventional NLP tasks to more complex and domain-specific challenges. In particular, the ability of LLMs to process massive amounts of data, generate human-like text, and assist in complex decision-making has captured significant attention in the scientific community [92, 141]. These breakthroughs suggest that LLMs have the potential to revolutionize the way scientific research is conducted, documented, and evaluated [156, 165, 174].</p>
<p>In this survey, we explore how LLMs are currently being applied across various stages of the scientific research process. Specifically, we identify four general tasks where LLMs have demonstrated notable potential. We begin by exploring their application in scientific hypothesis discovery, where LLMs leverage existing knowledge and experimental observations to suggest novel research ideas. This is followed by a review of their contributions to experiment planning and implementation, where LLMs aid in optimizing experimental design, automating workflows, and analyzing data. We also cover their use in scientific writing, including the generation of citations, related work sections, and even drafting entire papers. Finally, we discuss their potential in peer review, where LLMs support the evaluation of scientific papers by offering automated reviews and identifying errors or inconsistencies. For each of these tasks, we provide a comprehensive review of the methodologies, benchmarks, and evaluation methods. Moreover, the survey identifies the limitations of each task and highlights areas needing improvement. By analyzing the various stages of the research cycle where LLMs contribute, this survey can inspire researchers to explore emerging concepts, develop evaluation metrics, and design innovative approaches to integrate LLMs into their workflows effectively.</p>
<p>Comparison with Existing Surveys. This survey provides a broader and more comprehensive perspective on the applications of LLMs across the entire scientific research cycle compared to prior specialized studies. For example, Zhang et al. [187] review over 260 LLMs in scientific discovery</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The main content flow and categorization of this survey.
across various disciplines, focusing primarily on technical aspects such as model architectures and datasets, without situating their roles within the broader context of the research process. Similarly, other surveys tend to adopt narrower scopes, examining specific capabilities of LLMs for general applications, such as planning [55] or automation [158], rather than their focused utility in scientific research workflows. Additionally, some works address general approaches relevant to specific research stages but are not exclusively centered on LLMs, such as related work and citation text</p>
<p>generation [89] or peer review processes [33]. In contrast, this survey integrates these fragmented perspectives, providing a holistic analysis of LLMs' contributions across the scientific workflow and highlighting their potential to address the diverse and evolving demands of modern research.</p>
<p>Organization of this Survey. As illustrated in Figure 2, the structure of this survey is as follows: $\S 2$ covers LLMs for scientific hypothesis discovery, including an overview of methodologies and key challenges. $\S 3$ focuses on experiment planning and implementation, highlighting how LLMs can optimize and automate these processes. $\S 4$ delves into automated paper writing, including citation and related work generation, while $\S 5$ explores LLM-assisted peer review. For each topic, the survey concludes with a summary of current challenges and future directions in this rapidly evolving field.</p>
<h1>2 LLMs for Scientific Hypothesis Discovery</h1>
<h3>2.1 Overview</h3>
<p>Before the emergence of the field "LLMs for scientific hypothesis discovery", the most related previous research domains are "literature-based discovery" and "inductive reasoning". We first summarize the research in the two related domains (as history), then summarize the methods, benchmarks, evaluation development trends, and important progress, and finally conclude with the main challenges in the discovery task.</p>
<h3>2.2 History of Scientific Discovery</h3>
<p>Using LLMs to generate novel scientific hypotheses is a new research topic, mostly originating from two related research domains, which are "literature-based discovery" and "inductive reasoning".
2.2.1 Literature-based Discovery. Literature-based discovery (LBD) was first proposed by Swanson [151]. The central idea is that "knowledge can be public, yet undiscovered, if independently created fragments are logically related but never retrieved, brought together, and interpreted." Therefore, how to retrieve public knowledge that can be brought together to create new knowledge remains a challenge.</p>
<p>Swanson [151] propose a classic formalization of LBD, which is the "ABC" model where two concepts A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers. More recent work has used word vectors [155] or link prediction models [152, 160, 171] to discover links between concepts to compose hypotheses.</p>
<p>However, classic LBD methods do not model contexts that human scientists consider in the ideation process, and are limited to predicting pairwise relations between discrete concepts [47]. To overcome these limitations, Wang et al. [159] make the first attempt to ground LBD in a natural language context to constrain the generation space, and also use generated sentences as output instead of only predicting relations as in the traditional LBD.</p>
<p>Another limitation of LBD is that it has long been thought of as only be applicable to a very specific, narrow type of hypothesis [159]. However, recent progress in scientific discovery indicates that LBD might have a much wider applicable scope. Particularly, Yang et al. [174] and Yang et al. [176] discuss extensively with social science and chemistry researchers correspondingly, and find that most existing social science and chemistry published hypotheses (instead of only a narrow type of hypotheses) can be formulated in a LBD pattern. It probably indicates that future hypotheses in social science and chemistry to be published can also result from (correct) linkages and associations of existing knowledge.
2.2.2 Inductive Reasoning. Inductive reasoning is about finding a general "rule" or "hypothesis" that has a wide application scope from specific "observations" [175]. For example, Geocentrism,</p>
<p>Heliocentricism, and Newton's Law of Gravity are all proposed "rules" based on the "observations" of the movements of stars and planets. Scientific discovery is a difficult task of inductive reasoning to an extreme, where each "rule" is a novel scientific finding.</p>
<p>The philosophy of science community has summarized three fundamental requirements for a "rule" from inductive reasoning [113], which are (1) "rule" should not be in conflict with "observations"; (2) "rule" should reflect the reality; (3) "rule" should present a general pattern that can be applied to a larger scope than the "specific" observations, covering new information not existing in the observations. Previously inductive reasoning research is mainly conducted by the "inductive logic programming" community [26], which uses formal language and symbolic reasoners. Yang et al. [173] first work on generative inductive reasoning in the NLP domain, which is to generate natural language rules from specific natural language observations with language models, introducing the requirements on inductive reasoning from the philosophy of science community. Motivated by the empirical experience that language models tend to generate vague and not specific rules, they additionally propose the fourth requirement: (4) "rule" should be clear and in enough detail. The fourth requirement might have been overlooked by the philosophy of science community since it's too obvious. Motivated by the requirements, Yang et al. [173] design an overly-generation-thenfiltering mechanism, leveraging language models to first generate many preliminary rules and then filter those do not satisfy the requirements. Then methods are developed to use self-refine to replace filtering and use more reasoning steps for better rules [120, 163, 191, 194]. However, the "rules" this line of works try to induce are either known knowledge, or not scientific knowledge but synthesized patterns.</p>
<p>Yang et al. [174] make the first attempt to extend the classic inductive reasoning task setting (to discover known/synthetic knowledge) into a real scientific discovery setting: to leverage LLMs to autonomously discover novel and valid social science scientific hypotheses from the publicly available web data. Specifically, they collect news, business reviews, and Wikipedia pages on social science concepts as the web data to discover hypothesis.</p>
<p>Majumder et al. [107, 108] further propose the concept of "data-driven discovery", which is to discover hypotheses across disciplines with all the public experimental data on the web (and private experimental data at hand). Their motivation is that the potential of the large amount of publicly available experimental data has not been fully exploited that lots of novel scientific hypotheses could be discovered from the existing data.</p>
<h1>2.3 Development of Methods</h1>
<p>Among the methods developed for scientific discovery, there is one clear method development trajectory. We begin by introducing this trajectory, followed by an exploration of other methods.
2.3.1 Main Trajectory. In general, this method development trajectory for scientific discovery can be seen as incorporating more key components into the methods. Table 1 summarizes the key components we identify as important and indicates whether each method incorporates them. Specifically, they are "strategy of inspiration retrieval", "novelty checker", "validity checker", "clarity checker", "evolutionary algorithm", "leverage of multiple inspiration", "ranking of hypothesis", and "automatic research question construction". Here, each "key component" refers to a detailed and unique methodology that has proven effective for scientific discovery tasks. We exclude broad general concepts that may intuitively seem helpful but it's not clear how a specific method from the concept can be effective for this task (e.g., tool usage). Next, we introduce these key components. For each key component, we use one or two paragraphs to give a short overview, summarizing its development trace. The reference information for each method mentioned in this section can be found in Table 1.</p>
<p>Inspiration Retrieval Strategy. In addition to relying on background knowledge, literature-based discovery (LBD) facilitates the retrieval of additional knowledge as a source of inspiration for formulating new hypotheses. SciMON [159] first introduces the concepts of LBD to the discovery task, demonstrating that new knowledge can be composed of linkage of existing knowledge. It is vital that the inspiration should not be known to be related to the background before, or at least should not be used to associate with the background in a known way [176]. Otherwise, the hypothesis would not be novel.</p>
<p>Inspired by the "ABC" model in classic LBD formalization, given a background knowledge, SciMON retrieves semantically similar knowledge, knowledge graph neighbors, and citation graph neighbors as inspirations. Specifically, two knowledge are identified as "semantically similar" if their embeddings from SentenceBERT [127] have high cosine similarity; The knowledge graph they built follows a "[method, used-for, task]" format. ResearchAgent strictly follows the "ABC" model by constructing a concept graph, where a link represents the two connected concept nodes have appeared in the same paper before. It retrieves inspiration concepts that are connected with the background concepts on the concept graph (concept co-occurence). Scideator retrieves inspiration papers based on semantic matching (semantic scholar API recommendations) and concept matching (papers containing similar concepts in the same topic, same subarea, and different subarea). SciPIP [164] retrieves inspirations from semantically similar knowledge (based on SentenceBERT), concept co-occurence, and citation graph neigbors. It proposes filtering methods to filter not useful concepts for concept co-occurence retrieval.</p>
<p>Different from selecting semantic or citation neighbors as inspirations, SciAgents randomly sample another concept that is connected with the background concept in a citation graph (via a long or short path) as the inspiration.</p>
<p>MOOSE [174] proposes to use LLM-selected inspirations: given the research background and some inspiration candidates in the context, and ask an LLM to select inspirations for the research background from the candidates. Then MOOSE-Chem [176] also adopts it. MOOSE-Chem assumes that after training on hundreds of millions of scientific papers, the most advanced LLMs might already have a certain level of ability to identify the inspiration knowledge for the background to compose a novel discovery of knowledge. MOOSE-Chem analyzes this assumption by annotating 51 chemistry papers published in 2024 (which are only available online in 2024) with their background, inspirations, and hypothesis, and see whether LLMs with training data up to 2023 can retrieve the annotated inspirations given only the background. Their results show a very high retrieval rate, indicating that the assumption could be largely correct. Then Nova also adopts LLM-selected inspirations, with the motivation that leveraging the LLM's internal knowledge to determine useful knowledge for new ideas should be able to surpass traditional entity or keyword-based retrieval methods.</p>
<p>Feedback Modules. The next key component is the iterative feedback on the generated hypotheses in the aspects of novelty, validity, and clarity. These three feedbacks are first proposed by MOOSE, motivated by the requirements for a hypothesis in inductive reasoning [113, 173]. These three aspects are objective enough to give feedback, and each of them is essential for a good hypothesis.</p>
<ul>
<li>Novelty Checker. The generated hypotheses should be a novel finding compared to the existing literature. When a hypothesis tends to be similar to an existing hypothesis, feedback on enhancing its novelty could be beneficial for hypothesis formulation. Existing methods for novelty feedback are all based on LLMs. In general, there are three ways to provide novelty feedback. The first method evaluates each generated hypothesis against a related survey (MOOSE); the second iteratively retrieves relevant papers for comparison (SciMON,</li>
</ul>
<p>Table 1. Discovery Methods. Here "NF" = Novelty Feedback, "VF" = Validity Feedback, and "CF" = Clarity Feedback, "EA" = Evolutionary Algorithm, "LMI" = Leveraging Multiple Inspirations, "R" = Ranking, "AQC" = Automatic Research Question Construction. The order of methods reflect their first appearance time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Inspiration Retrieval Strategy</th>
<th style="text-align: center;">NF</th>
<th style="text-align: center;">VF</th>
<th style="text-align: center;">CF</th>
<th style="text-align: center;">EA</th>
<th style="text-align: center;">LMI</th>
<th style="text-align: center;">R</th>
<th style="text-align: center;">AQC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SciMON [159]</td>
<td style="text-align: center;">Semantic \&amp; Concept \&amp; Citation Neighbors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MOOSE [174]</td>
<td style="text-align: center;">LLM Selection</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">MCR [145]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Qi [119]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">FunSearch [130]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ChemReasoner [146]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HypoGeniC [193]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ResearchAgent [9]</td>
<td style="text-align: center;">Concept Co-occurrence Neighbors</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LLM-SR [140]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SGA [105]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AIScientist [103]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">MLR-Copilot [84]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">IGA [141]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SciAgents [41]</td>
<td style="text-align: center;">Random Selection</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Scideator [121]</td>
<td style="text-align: center;">Semantic \&amp; Concept Matching</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">MOOSE-Chem [176]</td>
<td style="text-align: center;">LLM selection</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">VirSci [148]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">CoI [77]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Nova [49]</td>
<td style="text-align: center;">LLM selection</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CycleResearcher [167]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SciPIP [164]</td>
<td style="text-align: center;">Semantic \&amp; Concept \&amp; Citation Neighbors</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>SciAgents, Scideator, CoI); the third directly leverages the internal knowledge of LLMs for evaluation (Qi, ResearchAgent, AIScientist, MOOSE-Chem, VirSci).</p>
<ul>
<li>Validity Checker. The generated hypotheses should be valid science/engineering findings that precisely reflect the objective universe [113]. A real validity feedback should be from the results of experiments. However, it is time-consuming and costly to conduct experiments for each generated hypothesis. Therefore, currently, validity feedback almost entirely relies on the heuristics of LLMs or other trained neural models. The exceptions are FunSearch, HypoGeniC, LLM-SR, and SGA. Specifically, FunSearch is about generating code for math problems. The compiler and verification code are naturally efficient and effective verifiers; HypoGeniC and LLM-SR focus on data-driven discovery, which means they have access to observation examples that can be used to check consistency with each generated hypothesis; SGA creates a virtual physical simulation environment to mimic real experiments. However, validity checker is still a significant challenge for the scientific discovery community. Future research directions include robotics and automation labs, which could automatically perform wet-lab experiments (e.g., biology and chemistry experiments) to verify the generated hypotheses. For computer science-related hypotheses, the future research direction could be more advanced systems for automatic code implementation.</li>
<li>Clarity Checker. The generated hypotheses should be sufficiently clear in conveying information and provide adequate details [173]. However, LLMs tend to generate hypotheses with insufficient details [159]. Therefore, it would be beneficial to provide feedback in terms of clarity to refine the hypothesis and expand it with details [174]. Current methods (MOOSE, ResearchAgent, MOOSE-Chem, and VirSci) all adopt LLMs to provide self-assessment on clarity.</li>
</ul>
<p>Evolutionary Algorithm. Evolutionary Algorithm is a subset of optimization algorithms inspired by the principles of biological evolution. It assumes the existence of an "environment", where an entity that can't adapt to it would be "eliminated", and super entity would be evolved from the "recombination" of characteristics between entities that have some adaptability to the environment (this process is also called as "mutation").</p>
<p>This key component is important since (1) the real experiment evaluation and the heuristic evaluation of the generated hypotheses naturally serve as the "environment". (2) the essence of scientific hypothesis discovery fundamentally can be seen as mutation to unknown yet valid knowledge from only known knowledge input. Although with similar goals, current scientific discovery methods leverage the evolutionary algorithm in different ways.</p>
<p>FunSearch first introduced the evolutionary algorithm to the scientific discovery task. They adopt an island-based evolutionary algorithm, where each island is a group of similar methods, and each island keeps mutating to new hypotheses. At some time intervals, some least ranking islands are "eliminated", and new islands consisting of the best-performing hypotheses from every island are formed, encouraging the "recombination" between merits between islands. LLM-SR adopts a similar island-based evolutionary algorithm.</p>
<p>SGA leverages it as "evolutionary search", which is to generate multiple offspring in each iteration and retain the best selection. They also adopt an evolutionary crossover, where LLMs generate new hypotheses from various past experiments for better exploration.</p>
<p>MOOSE-Chem designs it as an "evolutionary unit", to better associate background knowledge and inspiration knowledge. Specifically, given background and inspiration knowledge, they first generate multiple unique hypotheses to associate the two. Each hypothesis is then independently refined, and finally, the refined hypotheses are recombined to better integrate the background and inspiration knowledge into a cohesive hypothesis. It encourages different mutation variants from the same input and gathers the advantages from each mutation variant.</p>
<p>Leveraging Multiple Inspirations. Here the "Leveraging Multiple Inspirations" (LMI) component we discuss is about a clear identification of several inspirations, so that these identified inspirations will be all leveraged into the final hypothesis (e.g., in a sequential way). It is important, where different methods have different reasons.</p>
<p>MOOSE-Chem is the first to introduce this component, motivated by the observation that many disciplines such as chemistry and material science often require multiple inspirations to formulate a complete and publishable hypothesis. Specifically, they decompose the seemingly impossible-to-solve question $P$ (hypothesis|research background) into many smaller, more practical and executable steps. They do it by formulating a mathematical proof for the decomposition. In general, the smaller steps involve identifying a starting inspiration, composing a preliminary hypothesis based on the background and inspiration, finding another inspiration to address gaps in the preliminary hypothesis, then composing an updated hypothesis with the new inspiration, and so on. Their goal by utilizing multiple inspirations is to rediscover hypotheses in chemistry and material science that are published in high-impact journals such as Nature or Science.</p>
<p>In addition to MOOSE-Chem, Nova also retrieves multiple inspirations in a successive way, but with a different goal, which is to generate more diverse and novel research hypotheses. Their motivation stems from IGA's experimental results that the diversity of generated hypotheses tends to saturate. They identify one of the main reasons as that the input background information is the same, whereas incorporating different sets of inspirations can largely alleviate this issue by introducing flexible inputs.</p>
<p>Ranking of Hypotheses. This key component is about providing a full ranking of the generated hypotheses. It is important because LLMs can generate a large number of hypotheses in a short</p>
<p>time, while real lab experiments to verify each of them are time-consuming and costly. As a result, it would be very beneficial for scientists to know which hypothesis should be tested first. Some methods (e.g., MOOSE) adopt an automatic evaluation method to provide a preliminary understanding of generated hypotheses. The automatic evaluation method could naturally be used for ranking, but Table 1 only focuses on how ranking is used in the methodology section (but not in the automatic evaluation section).</p>
<p>A majority of the methods adopt LLM's rated score as a reward value, which can be used for ranking (MCR [145], AIScientist, MOOSE-Chem, CycleResearcher). FunSearch focuses on a code generation problem, therefore can directly precisely evaluate the generated code by running them and checking results. ChemReasoner [146] finetunes a task-specific graph neural network model to obtain reward. HypoGeniC [193] and LLM-SR [140] focuses on data-driven discovery, which means they have access to observation examples that can be used to check the consistency with the generated hypotheses, where the number of consistent examples can be used as the reward value for ranking.</p>
<p>Different from directly predicting a reward score, IGA takes a pairwise comparison, because they find that LLMs are poorly calibrated when asked directly to predict the final scores or decisions, but can achieve non-trivial accuracy when asked to judge which paper is better in pairwise comparisons. Inspired by IGA [141], CoI [77] proposes a pairwise automatic evaluation system, named Idea Arena. Nova [49] also adopts a pairwise automatic evaluation method.</p>
<p>Automatic Research Question Construction. This key component is about automatic construction of research question, so that automated scientific discovery methods can use it as input to discover hypotheses. It indicates a different role of LLM systems in scientific discovery: without it, an LLM serves as a copilot, relying on researchers to propose good research questions; with it, the system operates in a "full-self-driving" mode, capable of independent discovery without human input. The "full-self-driving" mode was first introduced by MOOSE and framed as an "automated" setting for scientific discovery. Specifically, they adopt an LLM-based agent to continually search through the discipline-related web corpus to find interesting research questions. AIScientist explores research directions by leveraging a starting code implementation as input. MLR-Copilot finds research directions by analyzing the research gaps from input papers. SciAgents and Scideator skip research questions by directly generating hypotheses based on the pairing of concepts. VirSci generates research questions by leveraging LLM-based scientist agents to brainstorm. CoI finds research questions by collecting a development line of methods and then predicting the next step. Nova directly generates seed ideas from input papers and common idea proposal patterns, skipping the research question construction step.
2.3.2 Other Methods. In this section, we introduce the methods that are different from the methods in the "main trajectory" ( 2.3.1). These methods themselves are very diverse, focusing on different aspects of scientific discovery. For example, Dong et al. [30] leverage a distinct methodology, Pu et al. [118] focus on HCI, Liu et al. [96] also consider the integration of experiment results, Li et al. [80], Weng et al. [167] leverage reviews as preference learning to finetune the hypothesis proposer model.</p>
<p>Dong et al. [30] try to use GPT-4 to tackle the very challenging research question: "whether $\mathrm{P}=$ NP or not". They propose "Socratic reasoning", which encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Their method could be useful when trying to prove a very challenging existing hypothesis.</p>
<p>IdeaSynth [118] is a research idea development system, which represents idea concepts as linked nodes on a canvas. Its effects are investigated in a human-computer interaction scenario. They found through a lab study that human participants using IdeaSynth can explore more alternative</p>
<p>Table 2. Discovery benchmarks aiming for novel scientific findings. The Biomedical data SciMON [159] collected is up to January 2024. RQ = Research Question; BS = Background Survey; I = Inspiration; H = Hypothesis. Qi et al. [119]'s dataset contains a train set where the publication date of the papers is before January 2023. * in the date column represents the authors have checked the papers should not only be published after the date, but are also not available online before the date (e.g., through arXiv). The five disciplines Kumar et al. [68] cover are Chemistry, Computer Science, Economics, Medical, and Physics.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Annotator</th>
<th>RQ</th>
<th>BS</th>
<th>I</th>
<th>H</th>
<th>Size</th>
<th>Discipline</th>
<th>Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>SciMON [159]</td>
<td>IE models</td>
<td>$\checkmark$</td>
<td>-</td>
<td>-</td>
<td>$\checkmark$</td>
<td>67,408</td>
<td>NLP \&amp; Biomedical</td>
<td>from 1952 to June 2022 (NLP)</td>
</tr>
<tr>
<td>Tomato [174]</td>
<td>PhD students</td>
<td>$\checkmark$</td>
<td>-</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>50</td>
<td>Social Science</td>
<td>from January 2023</td>
</tr>
<tr>
<td>Qi et al. [119]</td>
<td>ChatGPT</td>
<td>-</td>
<td>$\checkmark$</td>
<td>-</td>
<td>$\checkmark$</td>
<td>2900</td>
<td>Biomedical</td>
<td>from August 2023 (test set)</td>
</tr>
<tr>
<td>Kumar et al. [68]</td>
<td>PhD students</td>
<td>-</td>
<td>$\checkmark$</td>
<td>-</td>
<td>$\checkmark$</td>
<td>100</td>
<td>Five disciplines</td>
<td>from January 2022</td>
</tr>
<tr>
<td>Tomato-Chem [176]</td>
<td>PhD students</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>51</td>
<td>Chemistry \&amp; Material Science</td>
<td>from January 2024*</td>
</tr>
</tbody>
</table>
<p>ideas and expand initial ideas with more details compared to human participants using a strong LLM-based baseline.</p>
<p>Liu et al. [96] make the first attempt trying to unify literature-based discovery and data-driven discovery. Given an initial set of experiment results, it retrieves related literature and adopts an iterative refinement approach to keep improving a hypothesis to make it consistent with the experiment results and leverage findings from the retrieved literature.</p>
<p>Weng et al. [167] propose a dual system that includes CycleResearcher and CycleReviewer, where the CycleResearcher is in charge of idea formulating and paper writing, and the CycleReviewer is in charge of scoring the written papers. The dual system has a synergy that the scores from CycleReviewer can compose preference data to train CycleResearcher. The dual system only focuses on idea formulating and paper writing, skipping experiment planning and implementation.</p>
<p>Li et al. [80] propose fine-tuning LLMs to be better idea generators and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and Controllable Reinforcement Learning (RL). They focus on dimensions of feasibility, novelty and effectiveness. The dimensional controllers enable dynamic adjustment of the generation process.</p>
<h1>2.4 Benchmarks</h1>
<p>Overall the tasks in automated scientific discovery can be divided into "literature-based discovery" and "data-driven discovery". Researchers design different benchmarks for each task respectively. 2.4.1 Literature-based Discovery. Literature-based discovery is in general about connecting knowledge (pieces) in existing publications and associating them to create new knowledge. In this process, the knowledge to start with is from the research background. A research background can be seen as consisting of two components: (1) a research question, and (2) a background survey, which discusses the state-of-the-art methods or knowledge for the research question. With the start knowledge in the research background, the other knowledge to connect is usually by searching through the existing publications. Here the other knowledge is referred to as "inspiration" [159, 174]. Then the research background and the retrieved inspiration(s) are associated to create a "hypothesis".</p>
<p>Table 2 summarizes the literature-based discovery benchmarks, which aim for novel scientific findings. The key components are the research question, background survey, inspiration identification, and hypothesis. The hypotheses are collected from the "abstract" section [159], the "methodology" section [174, 176], or the "future work" and "limitation" sections [68]. Table 2 also includes the size of the dataset (number of papers analyzed), disciplines of the papers, and the publication date of the papers.</p>
<p>The publication date is important to alleviate/avoid the data contamination problem. The reason is that one of the main goals is to rediscover the groundtruth hypotheses, and the date can indicate which LLMs to use for the rediscovery (its training data should be earlier than the date to avoid the potential data contamination problem).</p>
<p>Some of the benchmarks can be used for training since their large size [119, 159], while some are mainly used for evaluation since they are annotated by PhD students [68, 174, 176].
2.4.2 Data-driven Discovery. Majumder et al. [107] propose the concept of "data-driven discovery". Here the "data" refers to the experiment results. Their motivation is that given the "observation" of lots of (public and private) existing experimental results available online, LLMs might be able to find the general pattern of these data, where the general pattern could be a novel research hypothesis. Given the relation between the specific observations and the general hypothesis, "data-driven discovery" is very related to the inductive reasoning task, where the observation space is the full publicly available experiment results on the web and the private experiment results at hand.</p>
<p>DiscoveryBench [108] is the first data-driven discovery benchmark. It comprises 264 discovery tasks extracted manually from over 20 published papers and 903 synthetic tasks. The input of the task consists of a research question and a set of experimental data. The goal is to answer the research question with a hypothesis that can be supported by the experimental data. It also introduces a structured formalism for the generated hypotheses, that the hypotheses should consist of three components: context, variables, and relationships. Specifically, a hypothesis is about the relationships between the two variables under the context.</p>
<p>DiscoveryWorld [57] is the first discovery benchmark with a virtual environment. The main motivation is twofold: (1) real-world experiments are costly and require substantial domain expertise; and (2) abstracting from task-specific details encourages the development of more general discovery methods. To address these challenges, it establishes a virtual environment for agents to discover hypotheses. It includes 120 different challenge tasks, where the hypotheses reflect the real patterns of the world.</p>
<h1>2.5 Evaluation Development Trend</h1>
<p>The evaluation methods for scientific discovery tasks are diverse. Arguably, nearly every paper proposes a new methodology uses a different evaluation approach. However, their metrics exhibit notable intersections, and some emerging trends in evaluation methods can be observed across these works.</p>
<p>The intersections of the evaluation criteria are "novelty", "validity", "clarity", and "significance". Some less-used evaluation criteria include "relatedness", "interestingness", and "helpfulness". An alternative name for "validity" is "feasibility". They might be used interchangeably in many scenarios. "Validity" refers to whether discovered scientific knowledge accurately reflects objective world, while "feasibility" concerns the practicability of an engineering finding. "Helpfulness" is a subjective evaluation, based on the idea that the goal of a discovery system is to act as a copilot for researchers; therefore, its perceived usefulness by researchers could be considered important.</p>
<p>In terms of the evaluator selection, the evaluation methods can be divided into LLM-based and expert-based evaluation. LLM's direct evaluation has shown a high consistency score with expert evaluation in social science [174]. However, in natural science disciplines such as chemistry, LLMs have been argued to lack the capability to provide reliable evaluations [146]. Expert evaluation is generally considered reliable. However, in challenging fields like chemistry, even an expert's direct evaluation may lack sufficient reliability [176]. This is due to (1) the complexity of the discipline; and (2) the fact that slight changes in the research topic can necessitate entirely different background</p>
<p>knowledge for evaluation, while experts typically have specialized research focuses, which may not cover the full range of knowledge required for a relatively reliable evaluation.</p>
<p>Based on the need for reference, evaluation methods can be categorized as direct evaluation and reference-based evaluation. Due to reliability concerns with direct evaluation, reference-based evaluation serves as an alternative [68, 108, 176], which counts the key components from the ground truth hypotheses mentioned in the generated hypotheses.</p>
<p>Moreover, in addition to directly assigning a scalar evaluation score to a generated hypothesis, Si et al. [141] propose comparison-based evaluations to alleviate the incapacity of LLM-based evaluation of direct scoring: the LLM evaluator is asked to keep comparing pairs of generated hypotheses until a ranking is possible. It can be used when comparing the quality of hypotheses generated by two methods, but might not help in judging the absolute quality of a hypothesis.</p>
<p>However, the ultimate evaluation should be only through real (wet-lab) experiments. It raises challenges in the robotics and automatic experiment implementation fields.</p>
<h1>2.6 Major Progresses/Achievements on Discovering Hypotheses</h1>
<p>Yang et al. [174] are the first to demonstrate that LLMs are capable of generating novel and valid scientific hypotheses, as confirmed through expert evaluation. They find three social science PhD students to directly evaluate the novelty and validity of the generated social science hypotheses. Then Si et al. [141] provide the first large-scale expert evaluation on LLM-generated hypotheses by hiring 100+ NLP researchers. They find a statistically significant conclusion that LLM can generate more novel but slightly less valid research hypotheses than human researchers. Then Yang et al. [176] show that an LLM-based framework can rediscover the main innovations of many chemistry and material science hypotheses published in Nature, Science, or a similar level in 2024 (the hypotheses are only available online in 2024), using LLMs trained exclusively on data available until October 2023.</p>
<h3>2.7 Challenges and Future Work</h3>
<p>Challenge. Scientific discovery is to find novel knowledge that has not been verified by wet lab experiments. In some disciplines such as chemistry, even an expert's evaluation of the generated novel hypothesis is not reliable enough. This causes a need for automated experiments conduction to verify the large-scale machine-generated hypotheses.</p>
<p>In addition, current methods on scientific discovery highly rely on the ability of existing available LLMs. LLMs with better capacity on the universal tasks usually can also lead to discovered hypotheses with better quality [174]. As a result, LLM-based discovery methods may have an upper performance limit, constrained by the capabilities of the state-of-the-art LLMs. However, it is largely (if not completely) unclear how should we augment LLM's ability on the task of scientific discovery.</p>
<p>Thirdly, it is unclear on a sufficient set of internal reasoning structure for scientific discovery: current works rely heavily on retrieving from high-quality knowledge source (e.g., literature) as inspiration to generate hypothesis. But it is unclear on whether there are any more internal reasoning structures that can help with the process.</p>
<p>Finally, building accurate and well-structured benchmark highly relies on experts. However, the size of a expert-composed benchmark is usually very limited. It is unclear on how should we scale up an accurate and well-structured discovery-oriented benchmark.</p>
<p>Future work. The first line of future work is to enhance automated experimental execution, as it remains the most reliable way to test the validity of a hypothesis. This process may vary across disciplines. In computer science, the bottleneck might be the coding ability, especially the ability to</p>
<p>program a large system. In chemistry or biology, the bottleneck might lie in the robotics methods to conduct experiments [14].</p>
<p>The second line of future work is to enhance the LLM's ability in hypothesis generation. Currently, it is still not very clear how to increase this ability. The aspects might include training data collection methods and training strategies.</p>
<p>The third line of future work is to investigate other internal reasoning structures of the scientific discovery process. This might need an interdisciplinary effort, involving the philosophy of science (also known as science of science) [36].</p>
<p>The fourth line of future work is to investigate how to leverage LLMs to automatically collect accurate and well-structured benchmark.</p>
<h1>3 LLMs for Experiment Planning and Implementation</h1>
<h3>3.1 Overview</h3>
<p>Beyond generating hypotheses, LLMs are increasingly employed in scientific research to automate experiment design and streamline workflows. LLMs poss comprehensive internal world knowledge, enabling them to perform informed actions in real world without training on specific domain data. To maximize their potential, LLMs are designed in an agent-based fashion with two critical properties [64]: modularity and tool integration. Modularity ensures that LLMs can interact seamlessly with external systems, such as databases, experimental platforms, and computational tools, while tool-augmented frameworks enable LLMs to serve as central controllers in workflows interfaced with specialized modules for data retrieval, computation, and experimental control. This section explores how LLMs are applied specifically to support the planning and implementation of research ideas.</p>
<h3>3.2 Optimizing Experimental Design</h3>
<p>LLMs are transforming the experimental design process by enabling more efficient and adaptive workflows in scientific research. Their capacity to process and analyze extensive datasets empowers researchers to decompose complex tasks, select optimal methodologies, and enhance the overall structure of experiments. This section explores how LLMs facilitate experimental design optimization across various domains.</p>
<p>Task decomposition involves breaking experiments into smaller, manageable sub-tasks, a process often necessitated by the complexity of real-world research to ensure alignment with specific research goals [55]. Numerous studies [14, 15, 52, 125, 136, 168] demonstrate how LLMs simplify intricate problems by defining experimental conditions and specifying desired outputs. For instance, HuggingGPT [136] utilizes LLMs to parse user queries into structured task lists while determining execution sequences and resource dependencies. Similarly, CRISPR-GPT [52] automates CRISPRbased gene-editing experiment design by facilitating the selection of appropriate CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and planning validation experiments. ChemCrow [15] employs iterative reasoning and dynamic planning, using a structured "Thought, Action, Action Input, Observation" loop [177] to refine its approach based on real-time feedback. Multi-LLM systems, such as Coscientist [14] and LLM-RDF [131], further leverage specialized agents to extract methodologies from literature, translate natural language descriptions into standardized protocols, generate execution code for automated platforms, and adaptively correct errors during execution.</p>
<p>Advanced prompting-based techniques such as in-context learning, Chain of Thought [166] and ReAct [177], are often employed in studies described above to enhance the reliability and accuracy of experimental planning in LLM-assisted workflows. Moreover, LLMs are also capable of</p>
<p>enhancing experimental design through reflection and refinement [106, 139], a process that allows them to continuously evaluate and improve experimental plans. For instance, by simulating expert discussions, LLMs engage in a collaborative dialogue [81], challenging assumptions, and refining their output through iterative analysis [90]. This method mirrors real-world scientific problem solving, where discrepancies between expert opinions foster a deeper exploration of the problem space, and consensus is achieved through rigorous debate and synthesis of diverse perspectives.</p>
<h1>3.3 Automating Experimental Processes</h1>
<p>LLMs revolutionize scientific research by automating repetitive and time-consuming tasks in experimental processes. This automation significantly enhances productivity, allowing researchers to delegate labor-intensive processes such as data preparation, experimental execution, analysis, and reporting to LLM-based systems [158].
3.3.1 Data Preparation. One of the most labor-intensive aspects of research is data preparation, which includes tasks such as cleaning [21, 185], labeling [153, 196], and feature engineering [46]. Large Language Models (LLMs) can automate these processes, especially when dealing with large datasets where manual data curation would be inefficient. Additionally, in situations where data is difficult to obtain, LLMs can synthesize experimental data directly [82, 85, 98]. For instance, in social science, where conducting experiments with human subjects is often expensive and unethical, Liu et al. [98] design a sandbox to simulate a social environment and deployed multiple agents (LLMs) to interact with each other. This approach allows researchers to collect data on agent social interactions for subsequent analysis.
3.3.2 Experiment Execution and Workflow Automation. To automate the experimental workflow in scientific research, LLM-based agents can acquire task-specific capabilities through a combination of pretraining [95, 128], fine-tuning [35, 44], and tool-augmented learning. Pretraining on extensive dataset provides foundational knowledge, while fine-tuning on domain-specific datasets refines this knowledge for targeted scientific applications. To enhance task execution, LLMs are often coupled with domain-specific knowledge bases [14, 15, 157] or preconfigured workflows [14, 99]. Advanced prompting techniques like in-context learning and chain-of-thought prompting [99, 179] enable LLMs to quickly adapt to new experimental protocols. Additionally, iterative adjustments with taskspecific feedback loops allow the LLM to refine its outputs based on experimental goals [124, 179].</p>
<p>Based on these principles, LLM plays a diverse role in automating experimental workflows across different disciplines. In chemistry, ChemCrow [15], an LLM chemistry agent, leverages 18 expert-designed tools to autonomously plan and execute complex chemical syntheses, bridging computational and experimental domains. Similarly, Coscientist [14] integrates LLM with lab automation to optimize reactions like palladium-catalyzed syntheses. LLMs have also been employed for evolutionary search strategies to explore vast chemical spaces [157], enabling the identification of candidate molecules while reducing experimental burdens. Ramos et al. [124] combine natural language inputs with Bayesian optimization for catalyst synthesis, streamlining iterative design cycles. Furthermore, LLMs have been utilized for hypothetical scenario testing and reaction design, minimizing experimental iterations through hypothesis pre-screening [145, 146]. In drug discovery, ChatDrug [99] integrates modules for prompting, retrieval, and domain feedback to facilitate drug editing, while DrugAssist [179] iteratively optimizes molecular structures through human-machine dialogue. In biological and medical research, Models like ESM-1b [128] and ESM-2 [95] encode protein sequences, capturing structural properties for predictive tasks, such as secondary and tertiary structure predictions, eliminating the need for labor-intensive experiments. By fine-tuning LLMs on protein families, Ferruz and Hcker [35] generate highly divergent yet functional protein</p>
<p>sequences. Additionally, He et al. [44] introduce an antibody generative LLM for de novo SARS-CoV-2 antibody design, achieving specificity and diversity while reducing reliance on natural antibodies.
3.3.3 Data Analysis and Interpretation. Beyond automating the execution of experiments, LLMs assist in data analysis by generating natural language explanations and constructing meaningful visualizations, which are essential for interpreting complex datasets and ensuring that the derived insights are accessible and actionable [143]. Traditionally, data analysis requires extensive statistical expertise, manual calculation, and the interpretation of large volumes of experimental results. LLMs simplify this by automating tasks such as statistical modeling and hypothesis testing. For instance, Li et al. [79] demonstrate that LLMs can serve as modelers, proposing, fitting, and refining probabilistic models based on real-world data, while also providing critical feedback on model performance through techniques like posterior predictive checks. Additionally, LLMs excel at uncovering hidden patterns, trends, and relationships within textual data. In social media data analysis, LLMs provide insights into public sentiment and emerging trends [172], and in environmental data interpretation, they contribute to improved understanding and decision-making in environmental science [114]. Moreover, they are also instrumental in thematic analysis [27, 126], helping to identify themes and patterns in qualitative data. Their application extends to financial data analysis as well, where they enhance forecasting and risk assessment capabilities [188]. AutoGen [168] provides a generic framework that enables the creation of diverse applications using multiple customizable agents (LLMs). These agents can interact through natural language and code, supporting a wide range of downstream task such as data modeling and data analysis [61].</p>
<h1>3.4 Benchmarks</h1>
<p>Table 3. Benchmark for LLM-Assisted Experiment Planning and Implementation. ED = Optimizing Experimental Design, DP = Data Preparation, EW = Experiment Execution \&amp; Workflow Automation, DA = Data Analysis \&amp; Interpretation. "General" in discipline means a benchmark is not designed for a particular discipline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark Name</th>
<th style="text-align: center;">ED</th>
<th style="text-align: center;">DP</th>
<th style="text-align: center;">EW</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">Discipline</th>
<th style="text-align: center;">Additional Task Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TaskBench [137]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Task decomposition, tool use</td>
</tr>
<tr>
<td style="text-align: center;">DiscoveryWorld [57]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Hypothesis generation, design \&amp; testing</td>
</tr>
<tr>
<td style="text-align: center;">MLAgentBench [54]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Machine Learning</td>
<td style="text-align: center;">Task decomposition, plan selection, optimization</td>
</tr>
<tr>
<td style="text-align: center;">AgentBench [100]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Workflow automation, adaptive execution</td>
</tr>
<tr>
<td style="text-align: center;">Spider2-V [16]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Data Science \&amp; Engineering</td>
<td style="text-align: center;">Multi-step processes, code \&amp; GUI interaction</td>
</tr>
<tr>
<td style="text-align: center;">DSBench [61]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Data Science</td>
<td style="text-align: center;">Data manipulation, data modeling</td>
</tr>
<tr>
<td style="text-align: center;">DS-1000 [70]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Data Science</td>
<td style="text-align: center;">Code generation for data cleaning \&amp; analysis</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Computer Science,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CORE-Bench [142]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Social Science \&amp; Medicine</td>
<td style="text-align: center;">Reproducibility testing, setup verification</td>
</tr>
<tr>
<td style="text-align: center;">SUPER [13]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">General</td>
<td style="text-align: center;">Experiment setup, dependency management</td>
</tr>
<tr>
<td style="text-align: center;">MLE-Bench [20]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Machine Learning</td>
<td style="text-align: center;">End-to-end ML pipeline, training \&amp; tuning</td>
</tr>
<tr>
<td style="text-align: center;">LAB-Bench [72]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Manipulation of DNA and protein sequences</td>
</tr>
<tr>
<td style="text-align: center;">ScienceAgentBench [24]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Data Science</td>
<td style="text-align: center;">Data visualization, model development</td>
</tr>
</tbody>
</table>
<p>Benchmarks are essential for evaluating how effectively LLMs can support various aspects of experimental workflows. While not specifically created for LLM-assisted experiment implementation, many benchmarks are versatile enough to be applied to these tasks. For example, MLAgentBench [54] covers task decomposition by helping break down complex research tasks, data handling by automating processes like data loading and transformation, and workflow management by optimizing machine learning experiment execution.</p>
<p>These benchmarks provide different venues and thus vary in their approaches. Evaluation methods range from task success rate, accuracy and execution consistency to comparisons with human benchmarks. These differences highlight the diverse ways LLMs can be integrated into research processes. Further details are presented in Table 3.</p>
<h1>3.5 Challenges and Future Work</h1>
<p>Challenges. The challenges of employing LLMs for experiment planning and implementation arise both from their intrinsic limitations and their application to domain-specific tasks. One fundamental limitation is their planning capability. As clarified by Kambhampati et al. [64], LLMs in autonomous modes often fail to generate executable plans. They are prone to hallucinations, which can lead to irrational plans, deviations from task prompts, or an inability to follow complex instructions [55]. Prompt robustness poses another critical challenge in multi-stage experimental contexts. Minor variations in prompt wording, even when conveying the same intent, can result in inconsistent guidance throughout the planning and execution process [195], potentially affecting experimental outcomes. Additionally, the slow processing speed of autoregressive LLMs can impede real-time feedback in iterative and multi-step experiment planning, limiting their efficiency. Applicationspecific challenges include difficulties in adapting to specialized roles, as LLMs struggle to emulate domain-specific scientific expertise and cognitive processes essential for generalizability across research domains [167]. For example, certain experiments may require simulating ethically sensitive or error-prone scenarios, which often conflict with the safety-aligned values embedded in LLMs.</p>
<p>Future work. Future research should address these challenges by enhancing core model capabilities and tailoring them to the unique requirements of experimental tasks. To mitigate hallucination risks, robust verification mechanisms can be integrated into workflows, such as cross-referencing outputs with external sound verifiers [64] or employing real-time feedback loops to correct inaccuracies dynamically [59]. Improving prompt robustness may involve developing adaptive systems that monitor and modify prompt structures in response to contextual changes, ensuring consistency across planning stages. Efficiency enhancements could be achieved by creating faster, distilled versions of LLMs optimized for multi-step reasoning or hybrid systems combining LLMs with smaller, task-specific models to balance speed and accuracy. For more effective role adaptation, fine-tuning LLMs with high-quality domain-specific datasets or developing modular frameworks could enable more precise emulation of specialized scientific reasoning. Additionally, designing adaptive alignment protocols may allow LLMs to safely simulate ethically complex scenarios when addressing specific experimental goals.</p>
<h2>4 LLMs for Scientific Paper Writing</h2>
<h3>4.1 Overview</h3>
<p>This section explores the integration of LLMs in three key areas of scientific paper writing: citation text generation ( 4.2), related work generation ( 4.3), and drafting and writing ( 4.4). We examine the methodologies used, the effectiveness of these models, and the challenges faced in automating scientific writing. In addition, we discuss the evaluation metrics and benchmarks used in these tasks.</p>
<h3>4.2 Citation Text Generation</h3>
<p>Given the context of a citing paper, citation text generation task aims to produce accurate textual summaries for a set of papers-to-cite. LLMs have been pivotal in automating various aspects of citation text generation by providing rich contextual understanding and coherence, employing a range of methodologies to enhance both accuracy and usability. A pilot study by Xing et al. [170]</p>
<p>uses a pointer-generator network that can copy words from the manuscript and the abstract of the cited paper based on cross-attention mechanisms to generate citation texts. Li and Ouyang [88] prompt an LLM to generate a natural language description that emphasized the relationships between pairs of papers in the citation network. On the other hand, models like AutoCite [161] and BACO [40] extend this work by adopting a multimodal approach, combining citation network structures with textual context to produce contextually relevant and semantically rich citation texts. Furthermore, Gu and Hahnloser [43], Jung et al. [63] allow users to specify attributes such as citation intent and keywords, integrating these into a structured template and fine-tuning an LM to generate citation texts that align with their needs.</p>
<h1>4.3 Related Work Generation</h1>
<p>This task involves creating a related work section for a scientific paper based on cutting-edge reference papers [45]. Compared to traditional multi-document summarization models [23, 51], LLMs have demonstrated remarkable capabilities in handling the extensive input lengths characteristic of scientific documents and providing a rich contextual understanding. The success of LLMs in various natural language understanding and generation tasks, combined with their large context windows, has recently enabled more comprehensive and nuanced literature reviews, facilitating deeper insights and connections across diverse research areas.</p>
<p>Martin-Boyle et al. [109], Zimmermann et al. [197] develop case studies to explore the use of ChatGPT for literature review tasks and related work generation, showcasing its ability to assist researchers by quickly scanning large datasets of scientific publications and generating initial drafts of related work sections. However, directly employing LLMs in academic writing could lead to issues such as hallucinations, where the generated content is not grounded in factual data and may fail to accurately reflect state-of-the-art research. To address these issues, numerous works have operated on the principle of Retrieval-Augmented Generation (RAG) [76], which enhances LLM-based literature review generation by grounding in factual content retrieved from external sources [3, 50, 138, 150, 181]. For instance, LitLLM [3] utilize RAG to retrieve relevant papers on websites and re-rank them, reducing the time and effort needed for comprehensive literature reviews while minimizing hallucinations. HiReview [50] takes this further by integrating RAG-based LLMs with graph-based hierarchical clustering. This system first retrieved relevant sub-communities within a citation network and generated a hierarchical taxonomy tree. LLMs then generate summaries for each cluster, ensuring complete coverage and logical organization. Nishimura et al. [112] integrate LLMs to emphasize novelty statement in related work sections. By comparing the new research with existing works, the LLMs help generate related work sections that explicitly highlight what is new and different, contributing to a more impactful comparison between the target paper and prior literature.</p>
<h3>4.4 Drafting and Writing</h3>
<p>In the field of automated scientific writing, LLMs are being used across various tasks, ranging from generating specific textual elements to producing entire research papers. For more specific writing tasks, August et al. [8] propose to generate scientific definitions with controllable complexity tailored to different audiences, while SCICAP [48] automates the generation of captions for scientific figures, enabling quick and accurate descriptions of visual data. More holistic systems, such as PaperRobot [160], introduce an incremental drafting approach, where LLMs help organize and draft sections of a paper based on user inputs. Similarly, CoAuthor [73] takes a collaborative human-AI approach, in which LLMs help authors by generating suggestions and expanding text. For fully autonomous writing, Ifargan et al. [56] explore how LLMs can generate complete research papers from data analysis to final drafts, while AutoSurvey [165] demonstrates the ability of LLMs to</p>
<p>autonomously write comprehensive surveys by synthesizing and organizing existing research. Lastly, AI Scientist [103] and CycleResearcher [167] propose an even broader system that not only drafts scientific papers but also contributes to the entire scientific process, including hypothesis generation and experiment design, highlighting the potential for fully automated scientific discovery and writing.</p>
<h1>4.5 Benchmarks</h1>
<p>We summarize the evaluation methods of automated scientific paper writing systems in three key fields: citation text generation, related work generation, and drafting and writing. In Table 4, we provide a comprehensive summary of the specific datasets, metrics, and benchmarks for each task.</p>
<p>Table 4. Evaluation Methods for automated paper writing, which includes three subtasks: citation text generation, related work generation, and drafting and writing. For the related work generation, there is no universally recognized benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Citation <br> Text <br> Generation</td>
<td style="text-align: center;">ALEC [38]</td>
<td style="text-align: center;">ASQA [147], QAMPARI [7], ELI5 [34]</td>
<td style="text-align: center;">Fluency: MAUVE [116], Correctness: precision, recall. Citation quality: citation recall, citation precision [38]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CiteBench [37]</td>
<td style="text-align: center;">AbuRa'ed et al. [1], Chen et al. [23], Lu et al. [104], Xing et al. [170]</td>
<td style="text-align: center;">Quantitative: ROUGE [93], BertScore [186], Qualitative: citation intent labeling [25], CORWA tagging [86]</td>
</tr>
<tr>
<td style="text-align: center;">Related Work Generation</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">AAN [123], SciSummNet [178], Delve [5], S2ORC [102], CORWA [86]</td>
<td style="text-align: center;">ROUGE [93], BLEU [115], Human evaluation: fluency, readability, coherence, relevance, informativeness</td>
</tr>
<tr>
<td style="text-align: center;">Drafting and Writing</td>
<td style="text-align: center;">SciGen [111]</td>
<td style="text-align: center;">SciGen [111]</td>
<td style="text-align: center;">BLEU [115], METEOR [10], MoverScore [189], BertScore [186], BLEURT [134], Human evaluation: recall, precision, correctness, hallucination</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciXGen [22]</td>
<td style="text-align: center;">SciXGen [22]</td>
<td style="text-align: center;">BLEU [115], METEOR [10], MoverScore [189], Human evaluation: fluency, faithfulness, entailment and overall</td>
</tr>
</tbody>
</table>
<p>Citation Text Generation. The ALCE [38] benchmark is the primary standard. Assessment of systems on three dimensions: fluency, correctness, and quality of citation text. ALCE is designed to test the ability of models to generate long-form answers with accurate citations across diverse domains. Their datasets cover a wide range of question types, with corpora spanning from Wikipedia to web-scale document collections. CiteBench [37] is another benchmark that unifies multiple existing tasks to standardize the evaluation of citation text generation across various designs and domains, using both qualitative and quantitative metrics.</p>
<p>Related Work Generation. Currently, no single benchmark is universally recognized for this task, due to the vast differences in task definitions and simplifying assumptions in various studies [89]. However, most works are built on corpus-level datasets, and commonly used sources of scientific articles include: ACL Anthology Network (AAN) Corpus [123], SciSummNet [178], Delve [5], Semantic Scholar Open Research Corpus (S2ORC) [102] and Citation Oriented Related Work Annotation (CORWA) [86]. The summarization metric ROUGE [93] is the most frequently employed for automatic evaluation, with some work also using the translation metric BLEU [115]. Furthermore, human evaluations often rate fluency, readability, coherence with the target paper, and relevance and informativeness to the cited work on a five-point Likert scale.</p>
<p>Drafting and Wrigting. SciGen [111] benchmark supports the evaluation of reasoning-aware text generation from scientific tables, highlighting the challenges of arithmetic reasoning in text generation. SciXGen [22], another key benchmark, evaluates the context-aware text generation,</p>
<p>focusing on the integration of external information into the generated text. Both SciGen and SciXGe use metrics like BLUE [115], METEOR [10] and MoverScore [189], along with human evaluation.</p>
<h1>4.6 Challenges and Future Work</h1>
<p>Challenges. The challenges in citation text generation, related work generation, and drafting and writing primarily arise from inherent limitations in LLMs, such as maintaining factual accuracy, ensuring contextual coherence, and handling complex information. LLMs often struggle with hallucinations [59], generating incorrect or irrelevant citations, and are constrained by the retrieval systems they depend on [53]. Limited context windows further restrict models' ability to manage extensive references or integrate relevant literature comprehensively [165], potentially leading to incorrect citation ordering and inappropriate citation grouping. Additionally, ensuring scientific rigor and avoiding reliance on superficial or trivial sources remain persistent obstacles, as LLMs struggle to capture the depth and reasoning needed in academic writing [103].</p>
<p>Furthermore, the use of LLMs in academic writing introduces significant ethical concerns, particularly regarding academic integrity and plagiarism [89]. This blurs the lines of authorship, as researchers might present machine-generated text as their own work. LLMs can also generate text that closely mimics existing literature, raising the risk of unintentional plagiarism where the generated content may not be sufficiently original. The convenience of using LLMs to draft sections of papers can undermine the rigorous intellectual effort traditionally required in academic writing, potentially devaluing the learning process and critical thinking skills essential to scholarly research.</p>
<p>Future Work. To overcome these challenges, future advancements should focus on improving retrieval systems and enhancing models' capacity to synthesize information from diverse, longcontext sources [87]. This includes developing better citation validation mechanisms, improving multi-document synthesis, and introducing real-time literature discovery to keep generated content up to date. Additionally, incorporating domain-specific fine-tuning and reasoning-aware models will help generate more accurate, contextually relevant scientific text [111]. Fine-grained control over the writing process, such as adjusting tone and style, will also be crucial for improving the adaptability of LLMs to different academic needs [22, 38, 103]. Furthermore, integrating human-in-the-loop systems, where human oversight and intervention are essential parts of the writing process, can ensure that the intellectual rigor and critical thinking inherent in scholarly work are preserved [89, 109]. Finally, to address the potential ethical concerns, it is crucial for the academic community to establish clear guidelines and ethical standards for the use of LLMs to ensure the integrity and originality of academic work.</p>
<h2>5 LLMs for Peer Reviewing</h2>
<h3>5.1 Overview</h3>
<p>Peer review is the cornerstone of scientific research. The integration of LLMs into the peer review process represents a significant advancement, addressing longstanding challenges such as reviewer bias, inconsistent standards, and workload imbalances [42, 117]. This integration has gained significant traction in the academic community, as evidenced by major computer science conferences adopting LLM-assisted reviewing practices. For instance, ICLR 2025 has announced the implementation of LLM-based systems to support reviewers in their evaluation process ${ }^{1}$.</p>
<p>The integration of LLMs in peer review has evolved into two distinct approaches, each addressing specific needs in the review process. The first approach, automated review generation, emerged from the need to handle increasing submission volumes and reduce reviewer workload by using</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>LLMs to analyze research papers independently [66, 182]. These systems are designed to evaluate multiple aspects of submissions, including methodology validation, results verification, and contribution assessment, thereby providing comprehensive review reports without direct human intervention. The second approach, LLM-assisted review workflows, developed in response to the recognition that human expertise remains crucial in academic evaluation while acknowledging that certain review tasks can benefit from automation[69]. These workflows incorporate LLMs as supplementary tools, where they assist human reviewers in time-consuming but well-defined tasks, such as paper summarization, reference verification, and internal consistency checks, while leaving critical evaluation and judgment to human experts.</p>
<p>These approaches employ diverse methodologies to enhance review efficiency, consistency, and quality. To systematically evaluate and improve these systems, the research community has developed specialized peer review benchmarks that serve dual purposes: providing standardized training datasets and establishing performance assessment metrics. This chapter examines these methodologies, their evaluation frameworks, and concludes with implementation challenges and future research directions.</p>
<h1>5.2 Automated Peer Review Generation</h1>
<p>Automated peer review generation aims to streamline scientific assessment by exploring how LLMs can produce comprehensive reviews with minimal human intervention. By inputting a scientific article, these systems focus on generating a complete peer review or meta-review, employing various techniques to enhance feedback's depth, accuracy, and relevance.</p>
<p>Current approaches to automated peer review generation can be categorized into two main strategies: single-model and multi-model architectures. Single-model approaches focus on optimizing the review generation process through sophisticated prompting techniques and modular design. These systems typically employ carefully crafted prompts to guide the model's attention to specific aspects of the paper, such as methodology, results, and contributions [132].</p>
<p>Within the single-model paradigm, several distinct architectural approaches have been proposed. CGI2 [184] advances beyond previous approaches: MetaGen [11], which used a two-stage pipeline of extractive summarization with decision-aware refinement; Kumar et al. [67], which developed a neural architecture for joint decision prediction and review generation; and MReD [135], which introduced structure-controlled generation using sentence-level functional labels. Building on these foundations, CGI2 implements a staged review process through modular design, first extracting key opinions from the paper, then summarizing strengths and weaknesses, and finally refining these outputs through iterative feedback under a checklist-guided framework. This iterative process enhances the depth and relevance of reviews but may struggle with papers that involve highly complex methodologies or lengthy content exceeding the context window. Taking a different approach, CycleReviewer [167] implements an end-to-end review generation method using reinforcement learning to refine review quality through feedback loops continuously. While CycleReviewer excels in enhancing review precision and clarity, its reliance on significant computational resources could limit its scalability. Meanwhile, ReviewRobot [162] utilizes knowledge graphs to systematically identify and structure knowledge elements, transforming them into detailed review comments through a structured generation process. ReviewRobot demonstrates remarkable explainability and evidence-based reasoning but is constrained by the inflexibility of its pre-defined templates.</p>
<p>The alternative strategy employs multi-model architectures, representing a more sophisticated approach by leveraging multiple specialized models to handle different aspects of the review process. This approach offers several advantages, including improved handling of complex papers and enhanced review quality through specialized expertise. Reviewer2 [39] implements a two-stage process: one model generates specific aspect prompts, while another utilizes these prompts to create</p>
<p>detailed, focused feedback. This separation of prompt generation and review creation allows for more nuanced and targeted feedback but often results in partial or biased reviews due to the lack of an integrated framework. To address these limitations, SEA [180] employs separate models for standardization, evaluation, and analysis, providing a more comprehensive and balanced approach. The system unifies multiple reviews into a single format, significantly reducing redundancy and inconsistencies across feedback. Furthermore, SEA introduces a mismatch score to measure the alignment between papers and generated reviews, coupled with a self-correction strategy to enhance review quality iteratively. While these features enable SEA to surpass Reviewer2 in consistency and comprehensiveness, the need for coordinating outputs across multiple models introduces added complexity. Building on specialization but addressing a different challenge, MARG [28] tackles the problem of processing papers that exceed typical LLM context limits. By introducing a multiagent framework, MARG distributes review tasks across multiple specialized models, allowing for a comprehensive review of longer papers while maintaining attention to detail throughout the document. This innovative approach ensures detailed, aspect-specific feedback. Still, it brings new challenges, such as coordinating the communication and outputs of various agents, which increases the difficulty of ensuring consistency and alignment.</p>
<p>Each architectural approach offers distinct advantages and faces unique challenges. Singlemodel approaches benefit from simpler implementation and more straightforward control over the review process, but they may struggle with longer or more complex papers. Multi-model architectures provide greater scalability and better handling of sophisticated review tasks, yet they demand careful coordination and face potential consistency challenges across their components. For instance, ReviewRobot's structured approach offers explainability and actionable insights. Still, it is less adaptable to evolving research domains, while CycleReviewer's iterative refinement improves dynamic adaptability without requiring substantial training resources. As research in this area progresses, combining the strengths of single-model simplicity with the adaptability of multi-model designs presents a promising avenue for enhancing review quality, consistency, and comprehensiveness.</p>
<h1>5.3 LLM-assisted Peer Review Workflows</h1>
<p>Unlike fully automated review generation, LLM-assisted peer review workflows focus on enhancing human reviewers' capabilities rather than replacing them. Recent research highlights the critical importance of this human-AI collaborative approach in academic peer review. Studies by [12, 31, 133] emphasize that while LLM can enhance efficiency, human oversight remains essential for maintaining ethical standards and review integrity. Systems like AgentReview [60] demonstrate this synergy in practice, where LLM generates initial insights that human reviewers then refine and validate.</p>
<p>The LLM-assisted peer review workflows enhance three primary functions in the scientific review process: (1) information extraction and summarization, which helps reviewers quickly grasp paper content; (2) manuscript validation and quality assurance, which supports systematic verification of paper claims; and (3) review writing support, which assists in generating well-structured feedback.</p>
<p>In the information extraction and summarization function, systems automate document understanding and synthesis to support reviewer comprehension. PaperMage [101] is a foundational toolkit that integrates natural language processing and computer vision models to process visually rich scientific documents, enabling sophisticated extraction of logical structure, figures, and textual content across multiple modalities. Complementing this structural analysis, CocoSciSum [29] focuses on content summarization, offering customizable paper summaries with precise control over length and keyword inclusion while maintaining high factual accuracy through its compositional control architecture.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
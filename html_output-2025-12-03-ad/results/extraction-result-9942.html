<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9942 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9942</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9942</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-777d4ec0148c34b0bfab91e9ac3a902e420b891e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/777d4ec0148c34b0bfab91e9ac3a902e420b891e" target="_blank">Verbosity Bias in Preference Labeling by Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work examines the biases that come along with evaluating LLMs with other LLMs and takes a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities.</p>
                <p><strong>Paper Abstract:</strong> In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9942.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9942.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used as automatic judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used as an automated pairwise judge to evaluate generated texts (both single-question creative answers and whole conversations) and its preferences were compared to human labels, revealing a systematic preference for longer/verbose responses and reduced alignment with humans when humans preferred concise answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Creative writing and helpfulness judgments in conversational assistants (pairwise preference comparisons; HH-RLHF conversations)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparison prompt template (Figure 1); presented two candidate responses (from the same generator) and asked to choose the preferred option (first, second, or draw). For robustness to position bias each pair was evaluated twice with positions swapped and counted as a draw unless the decision was identical across permutations; for HH-RLHF experiments GPT-4 was asked to evaluate whole conversations and decide which assistant was more helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels taken from the HH-RLHF dataset (Bai et al., 2022a): single human preference label per prompt comparing two assistant responses (conversation-level comparisons). The dataset provides the human-preferred option for each pair; no per-prompt multiple annotators were available in this dataset (the paper notes there is only one human feedback per prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Human alignment measured as percent agreement (rate at which GPT-4's choice matched the human-labeled preferred option) plotted versus relative word-count difference (Figures 4 and 5). The paper also computes a directional verbosity-bias metric (see Verbosity Bias entry); specific numeric verbosity-bias value reported for GPT-4: 0.328 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When used as the judge, GPT-4 shows a systematic verbosity preference: it often prefers longer answers irrespective of human judgments, leading to lower agreement with humans in cases where humans prefer shorter answers; this degrades alignment with human notions of helpfulness/conciseness and risks producing reward signals (in RLAIF) that push models toward unnecessarily verbose outputs, harming downstream tasks that require concision (e.g., succinct QA, summarization, or concise assistant replies).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Section 4: In controlled experiments with Vicuna-generated creative answers, GPT-4 increasingly preferred the longer answer as word-count difference grew (Figure 3), sometimes almost always choosing the longer option when length difference was large. Section 5 / HH-RLHF: when the human label favored the shorter response, GPT-4 frequently still favored the longer response, producing low human alignment in those cases (Figure 5). The paper reports the computed verbosity-bias score for GPT-4 = 0.328, indicating a positive (pro-verbose) bias.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When the human label favored the longer answer, GPT-4's alignment with humans was high (i.e., agreement is good in those cases). The paper also notes GPT-4 shows an improvement relative to GPT-3.5 (lower verbosity-bias score). Prior related work is cited showing task-dependence: Huang et al. (2023) found GPT-4 prefers short responses for faithfulness/coverage in summarization (i.e., verbosity bias may invert depending on task), and Zheng et al. (2023) found GPT-4 less susceptible to the repetitive-list verbosity attack in listing tasks. The paper also notes limitations such as mixing question types in Section 5 and that verbosity preference shape varies by prompt, complicating simple global corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 4 and 5 (Figures 3, 4, 5), Table 1, and Discussion (Sections 6, 7.2)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9942.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9942.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5 (used as comparator judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 is reported as a comparator LLM-judge and exhibits a stronger verbosity preference than GPT-4 according to the paper's verbosity-bias metric, implying greater divergence from human judgments when humans favor concision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Creative writing / conversational preference judgments (analysis across HH-RLHF comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Evaluated on the same HH-RLHF pairwise conversation comparisons using the same template/setup as GPT-4 (human alignment measured as rate of agreement with HH-RLHF human labels); verbosity-bias metric computed from the experiment data.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels taken from HH-RLHF (single human preference per prompt; conversation-level comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Human alignment (percent agreement) and the paper's directional verbosity-bias metric. The reported verbosity-bias value for GPT-3.5 is 0.428 (Table 1), higher than GPT-4, indicating greater pro-verbosity inaccuracy relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Greater deviation from human judgments in cases where humans prefer shorter answers: GPT-3.5 more strongly favors verbose outputs, increasing the risk that LLM-based evaluators (if used as reward sources) induce models to generate needlessly long answers and reduce fidelity to human preferences for concision.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 1: verbosity-bias score GPT-3.5 = 0.428, higher than GPT-4; the paper's Section 5 analysis shows LLM-human alignment declines when the human-selected option is shorter, which is exacerbated for GPT-3.5 compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Same caveats as GPT-4: when humans prefer longer answers alignment is high; verbosity bias is task-dependent and may be lower or reverse in other tasks (e.g., summarization), and the paper calls for further experiments across more LLMs and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Table 1 and Section 5 (Figure 5), Discussion (Sections 6, 7.2)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9942.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9942.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbosity bias (metric)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verbosity bias quantified via accuracy parity / directional inaccuracy difference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper formalizes verbosity bias as the directional deviation from accuracy parity between cases where the longer answer is one side or the other, operationalized as the difference in inaccuracy conditioned on which option is longer (P(Y'=1-Y | S=1-Y) - P(Y'=1-Y | S=Y)), producing signed scores where positive indicates a preference for verbose answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Preference labeling / RLAIF evaluation across creative writing and conversational response comparison tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Metric designed to assess any LLM-as-judge; applied in paper to GPT-4 and GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparison judgments from LLMs (decisions Y') compared to human labels Y, with sensitive attribute S defined as which option is longer in word count; the metric uses these conditional probabilities to produce a signed bias score.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human oracle provided by HH-RLHF dataset (single human choice per pair). The metric measures how LLM inaccuracies differ depending on which option is longer relative to the human label.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Verbosity bias metric = P(Y'=1-Y | S=1-Y) - P(Y'=1-Y | S=Y). The paper also references equal opportunity and accuracy parity formulations and uses percent agreement (human alignment) vs relative length difference plots (Figures 4 and 5). Reported numeric results: GPT-4 = 0.328, GPT-3.5 = 0.428 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Captures the specific degradation introduced by LLM judges: greater inaccuracy that is conditional on verbosity—i.e., LLMs make different mistakes depending on which option is longer, tending to err toward selecting verbose answers even when humans deem shorter options more helpful. This implies a loss of fidelity to human judgments about concision and helpfulness, and a risk of training downstream models to favor verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Applied to HH-RLHF comparisons: when humans chose the shorter response, the LLMs' agreement rate fell (Figure 5), and the computed directional metric produced positive values (pro-verbose) for both GPT-4 and GPT-3.5, demonstrating concrete misalignment tied to verbosity. Section 4 experiments (Vicuna answers) show GPT-4's raw preference for longer answers as word-count differences grow (Figure 3), which motivated the metric.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Metric limitations noted: it only partitions by which response is longer (S) and is agnostic to the magnitude of length difference within each group—thus it can miss within-group effects (Section 7.3). Also verbosity bias is task-dependent; other tasks (e.g., summarization) may show negative bias (preference for concision) as observed by Huang et al. (2023). The paper recommends showing the human-alignment vs length-difference plots alongside the single-number metric to avoid misleading conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 6 (formulation), Table 1, Section 4 (motivation), Section 5 (application), Section 7.3 (metric limitations)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Training a helpful and harmless assistant with reinforcement learning from human feedback <em>(Rating: 2)</em></li>
                <li>Rlaif: Scaling reinforcement learning from human feedback with ai feedback <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9942",
    "paper_id": "paper-777d4ec0148c34b0bfab91e9ac3a902e420b891e",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "GPT-4 (judge)",
            "name_full": "Generative Pre-trained Transformer 4 (used as automatic judge)",
            "brief_description": "GPT-4 was used as an automated pairwise judge to evaluate generated texts (both single-question creative answers and whole conversations) and its preferences were compared to human labels, revealing a systematic preference for longer/verbose responses and reduced alignment with humans when humans preferred concise answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Creative writing and helpfulness judgments in conversational assistants (pairwise preference comparisons; HH-RLHF conversations)",
            "llm_judge_model": "GPT-4",
            "llm_judge_setup": "Pairwise comparison prompt template (Figure 1); presented two candidate responses (from the same generator) and asked to choose the preferred option (first, second, or draw). For robustness to position bias each pair was evaluated twice with positions swapped and counted as a draw unless the decision was identical across permutations; for HH-RLHF experiments GPT-4 was asked to evaluate whole conversations and decide which assistant was more helpful.",
            "human_evaluation_setup": "Human labels taken from the HH-RLHF dataset (Bai et al., 2022a): single human preference label per prompt comparing two assistant responses (conversation-level comparisons). The dataset provides the human-preferred option for each pair; no per-prompt multiple annotators were available in this dataset (the paper notes there is only one human feedback per prompt).",
            "agreement_metric": "Human alignment measured as percent agreement (rate at which GPT-4's choice matched the human-labeled preferred option) plotted versus relative word-count difference (Figures 4 and 5). The paper also computes a directional verbosity-bias metric (see Verbosity Bias entry); specific numeric verbosity-bias value reported for GPT-4: 0.328 (Table 1).",
            "losses_identified": "When used as the judge, GPT-4 shows a systematic verbosity preference: it often prefers longer answers irrespective of human judgments, leading to lower agreement with humans in cases where humans prefer shorter answers; this degrades alignment with human notions of helpfulness/conciseness and risks producing reward signals (in RLAIF) that push models toward unnecessarily verbose outputs, harming downstream tasks that require concision (e.g., succinct QA, summarization, or concise assistant replies).",
            "examples_of_loss": "Section 4: In controlled experiments with Vicuna-generated creative answers, GPT-4 increasingly preferred the longer answer as word-count difference grew (Figure 3), sometimes almost always choosing the longer option when length difference was large. Section 5 / HH-RLHF: when the human label favored the shorter response, GPT-4 frequently still favored the longer response, producing low human alignment in those cases (Figure 5). The paper reports the computed verbosity-bias score for GPT-4 = 0.328, indicating a positive (pro-verbose) bias.",
            "counterexamples_or_caveats": "When the human label favored the longer answer, GPT-4's alignment with humans was high (i.e., agreement is good in those cases). The paper also notes GPT-4 shows an improvement relative to GPT-3.5 (lower verbosity-bias score). Prior related work is cited showing task-dependence: Huang et al. (2023) found GPT-4 prefers short responses for faithfulness/coverage in summarization (i.e., verbosity bias may invert depending on task), and Zheng et al. (2023) found GPT-4 less susceptible to the repetitive-list verbosity attack in listing tasks. The paper also notes limitations such as mixing question types in Section 5 and that verbosity preference shape varies by prompt, complicating simple global corrections.",
            "paper_reference": "Sections 4 and 5 (Figures 3, 4, 5), Table 1, and Discussion (Sections 6, 7.2)",
            "uuid": "e9942.0",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 (judge)",
            "name_full": "Generative Pre-trained Transformer 3.5 (used as comparator judge)",
            "brief_description": "GPT-3.5 is reported as a comparator LLM-judge and exhibits a stronger verbosity preference than GPT-4 according to the paper's verbosity-bias metric, implying greater divergence from human judgments when humans favor concision.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Creative writing / conversational preference judgments (analysis across HH-RLHF comparisons)",
            "llm_judge_model": "GPT-3.5",
            "llm_judge_setup": "Evaluated on the same HH-RLHF pairwise conversation comparisons using the same template/setup as GPT-4 (human alignment measured as rate of agreement with HH-RLHF human labels); verbosity-bias metric computed from the experiment data.",
            "human_evaluation_setup": "Human labels taken from HH-RLHF (single human preference per prompt; conversation-level comparisons).",
            "agreement_metric": "Human alignment (percent agreement) and the paper's directional verbosity-bias metric. The reported verbosity-bias value for GPT-3.5 is 0.428 (Table 1), higher than GPT-4, indicating greater pro-verbosity inaccuracy relative to humans.",
            "losses_identified": "Greater deviation from human judgments in cases where humans prefer shorter answers: GPT-3.5 more strongly favors verbose outputs, increasing the risk that LLM-based evaluators (if used as reward sources) induce models to generate needlessly long answers and reduce fidelity to human preferences for concision.",
            "examples_of_loss": "Table 1: verbosity-bias score GPT-3.5 = 0.428, higher than GPT-4; the paper's Section 5 analysis shows LLM-human alignment declines when the human-selected option is shorter, which is exacerbated for GPT-3.5 compared to GPT-4.",
            "counterexamples_or_caveats": "Same caveats as GPT-4: when humans prefer longer answers alignment is high; verbosity bias is task-dependent and may be lower or reverse in other tasks (e.g., summarization), and the paper calls for further experiments across more LLMs and tasks.",
            "paper_reference": "Table 1 and Section 5 (Figure 5), Discussion (Sections 6, 7.2)",
            "uuid": "e9942.1",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Verbosity bias (metric)",
            "name_full": "Verbosity bias quantified via accuracy parity / directional inaccuracy difference",
            "brief_description": "The paper formalizes verbosity bias as the directional deviation from accuracy parity between cases where the longer answer is one side or the other, operationalized as the difference in inaccuracy conditioned on which option is longer (P(Y'=1-Y | S=1-Y) - P(Y'=1-Y | S=Y)), producing signed scores where positive indicates a preference for verbose answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Preference labeling / RLAIF evaluation across creative writing and conversational response comparison tasks",
            "llm_judge_model": "Metric designed to assess any LLM-as-judge; applied in paper to GPT-4 and GPT-3.5",
            "llm_judge_setup": "Pairwise comparison judgments from LLMs (decisions Y') compared to human labels Y, with sensitive attribute S defined as which option is longer in word count; the metric uses these conditional probabilities to produce a signed bias score.",
            "human_evaluation_setup": "Human oracle provided by HH-RLHF dataset (single human choice per pair). The metric measures how LLM inaccuracies differ depending on which option is longer relative to the human label.",
            "agreement_metric": "Verbosity bias metric = P(Y'=1-Y | S=1-Y) - P(Y'=1-Y | S=Y). The paper also references equal opportunity and accuracy parity formulations and uses percent agreement (human alignment) vs relative length difference plots (Figures 4 and 5). Reported numeric results: GPT-4 = 0.328, GPT-3.5 = 0.428 (Table 1).",
            "losses_identified": "Captures the specific degradation introduced by LLM judges: greater inaccuracy that is conditional on verbosity—i.e., LLMs make different mistakes depending on which option is longer, tending to err toward selecting verbose answers even when humans deem shorter options more helpful. This implies a loss of fidelity to human judgments about concision and helpfulness, and a risk of training downstream models to favor verbosity.",
            "examples_of_loss": "Applied to HH-RLHF comparisons: when humans chose the shorter response, the LLMs' agreement rate fell (Figure 5), and the computed directional metric produced positive values (pro-verbose) for both GPT-4 and GPT-3.5, demonstrating concrete misalignment tied to verbosity. Section 4 experiments (Vicuna answers) show GPT-4's raw preference for longer answers as word-count differences grow (Figure 3), which motivated the metric.",
            "counterexamples_or_caveats": "Metric limitations noted: it only partitions by which response is longer (S) and is agnostic to the magnitude of length difference within each group—thus it can miss within-group effects (Section 7.3). Also verbosity bias is task-dependent; other tasks (e.g., summarization) may show negative bias (preference for concision) as observed by Huang et al. (2023). The paper recommends showing the human-alignment vs length-difference plots alongside the single-number metric to avoid misleading conclusions.",
            "paper_reference": "Section 6 (formulation), Table 1, Section 4 (motivation), Section 5 (application), Section 7.3 (metric limitations)",
            "uuid": "e9942.2",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "rating": 2
        },
        {
            "paper_title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
            "rating": 2
        }
    ],
    "cost": 0.010983999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Verbosity Bias in Preference Labeling by Large Language Models</h1>
<p>Keita Saito*<br>University of Tsukuba \&amp; RIKEN AIP<br>Tsukuba, Ibaraki 305-8573, Japan<br>keita.saito@bbo.cs.tsukuba.ac.jp</p>
<p>Koki Wataoka<br>LY Corporation<br>Chiyoda-ku, Tokyo 102-8282, Japan<br>koki.wataoka@lycorp.co.jp</p>
<p>Akifumi Wachi<br>LY Corporation<br>Chiyoda-ku, Tokyo 102-8282, Japan<br>akifumi.wachi@lycorp.co.jp</p>
<p>Youhei Akimoto<br>University of Tsukuba \&amp; RIKEN AIP<br>Tsukuba, Ibaraki 305-8573, Japan<br>akimoto@cs.tsukuba.ac.jp</p>
<h4>Abstract</h4>
<p>In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias - a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have made tremendous strides in recent years and continue to gain popularity (Zhao et al., 2023). With its growing size in network parameters, its wide application ranges from conventional natural language processing tasks such as chat-bots, summarization, and translation, to other applications beyond its original intended use such as search engines, programming assistance, and foundation models (Zhao et al., 2023; Brants et al., 2007; Katz, 1987).</p>
<p>After pretraining for general purposes, LLMs are fine-tuned to further better their performance for specific tasks with supervised learning and RLHF - reinforcement learning from preference labeling feedback from humans (Stiennon et al., 2020; Ouyang et al., 2022). However, issues arise with RLHF where human feedback can become costly. To work around this problem, Reinforcement Learning from AI Feedback (RLAIF) was proposed (Bai et al., 2022b; Lee et al., 2023), which replaces human feedback with inexpensive feedback from other LLMs.</p>
<p>In many cases, the question lack a clear-cut "correct answer" and require creativity and imagination. As evident in an example of feedback by an LLM provided in Figure 1, when LLMs are tasked to assess responses to such prompts, the evaluation process can become arbitrary and introduce various biases. One prominent bias is the verbosity bias, which occurs when LLMs are influenced</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a prompt to an LLM to judge two texts and the verdict. There is no one correct answer, and a comprehensive judgement is required.
by verbosity, favoring longer and more verbose texts, even if they appear wordy or of lower quality. Without accounting for this bias, LLM agents may learn to generate unnecessarily long texts. This may result in failures in downstream tasks such as lengthy summarizations or chatbots that return verbose responses to simple questions.
While previous studies have explored the concept of verbosity bias, they have tended to focus on specific cases. Zheng et al. (2023) limits their problem setting to questions answered with lists in their experiment on verbosity bias, and Huang et al. (2023) conducted experiments on summarization tasks. Moreover, these do not compare the preferences of LLMs to those of humans. We believe that such a comparison is crucial in challenging the conjecture that longer answers are inherently better and that LLMs are actually correct in their preferences.</p>
<p>Our contributions. In this paper, we conduct experiments on verbosity bias and saw that 1) LLMs exhibit a preference for longer answers in creative writing tasks, and 2) there is a discrepancy between of LLMs and those of humans in verbosity preference. Additionally, we formulate a quantification for measuring verbosity bias based on accuracy parity. This can be used to compare LLMs on their degree of verbosity bias.</p>
<h1>2 Preliminaries</h1>
<p>After undergoing pretraining for general purposes, LLMs are fine-tuned to further improve their performance in specific tasks. Pretraining is accomplished through self-supervised learning, where the model is trained to predict the next token in a sentence. Once the LLM is able to generate cohesive sentences, we proceed to fine-tune the model to solve specific tasks. One approach to fine-tuning involves supervised learning using expert data. This method relies on examples where experts have solved the task at hand. An example of a conversational LLM trained solely using this approach is Vicuna (Chiang et al., 2023). Vicuna achieved performance comparable to ChatGPT by utilizing user-shared conversations with ChatGPT as expert data. However, it is worth noting that obtaining expert data is often challenging.
RLHF addresses the challenge of limited training data in supervised learning by leveraging human feedback (Stiennon et al., 2020; Ouyang et al., 2022). This approach not only mitigates data scarcity but also significantly enhances alignment with human preferences, a critical factor in applications such as question answering. In RLHF, a reward model is trained to closely match human feedback data, which acts as the reward signal in the subsequent RL phase. Prominent LLMs like ChatGPT and Bard adopt a hybrid approach, combining both supervised learning and RLHF techniques to further refine their alignment with human preferences.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Two phases in RLHF. First the reward model is trained to align with human preference by matching with human feedback. In the second RL phase, the trained reward model provides reward signals to the language model.</p>
<h1>2.1 RLHF</h1>
<p>The first step of RLHF is to fit the reward function to align with human feedback. RL directly from human feedback as reward signal is unstable and requires volume. Therefore, a reward model that acts as the reward signal later in the process is trained to be consistant with human preference. Given a dataset $\mathcal{D}$ consisting of the original question, a pair of generated text, and the human preference label on which is chosen and rejected, the reward model is trained by minimizing</p>
<p>$$
\mathcal{L}(\phi)=-\underset{\left(x, y_{\text {chosen }} ; y_{\text {rejected }}\right) \sim \mathcal{D}}{\mathbb{E}}\left[\log \sigma\left(r_{\phi}\left(y_{\text {chosen }} \mid x\right)-r_{\phi}\left(y_{\text {rejected }} \mid x\right)\right)\right]
$$</p>
<p>where $x$ is the prompt to the LLM, $y_{\text {chosen }}$ is the preferred text, $y_{\text {rejected }}$ is the rejected text, and $r_{\phi}$ parametrised with $\phi$ is the reward model that takes text as input and outputs the rating score.</p>
<p>In the second step of RLHF, now that we have a reward model to evaluate a generated text without human interaction, regular RL can take place. In this context, the state is the question and the generated text so far, action is the next token to generate, and the reward is $r_{\phi}(y)$ given after the full text is generated. This is equivalent to a task where a sparse reward is given only at episode termination. The LLM maximizes the signal from the reward model with the following:</p>
<p>$$
\max <em _phi="\phi">{\theta} \mathbb{E}\left[r</em>(x) \mid x\right)\right]
$$}\left(\pi_{\theta</p>
<p>where $\pi_{\theta}$ is a policy parameterized by $\theta$. An optional KL divergence term is added to penalize the policy from deviating from the original policy.</p>
<h3>2.2 RLAIF</h3>
<p>While RLHF brings down the cost of human labor compared to generating an expert data from scratch, human feedback is still costly. For example, in Wang et al. (2023), it cost around 3 minutes ( $\$ 0.75$ if $\$ 15.00$ per hour) per evaluation. In one occasion, OpenAI worked around this by employing people in Kenya on less than $\$ 2$ per hour pay in the process of labeling violent or innappropriate texts. They were under scrutiny for the unideal working conditions.</p>
<p>To combat these promblems, RLAIF was proposed. This method replaces human feedback with feedback from other LLMs. This brings down the cost significantly; in our case, evaluation cost around $\$ 0.05$ each, which is $1 / 15$ th compared to human feedback in the previously cited paper (Wang et al., 2023).</p>
<h1>2.3 Biases in Automated LLM Evaluation</h1>
<p>When LLMs evaluate generated texts, various biases are introduced. We provide below a list of biases discussed in various papers (Zheng et al., 2023; Bai et al., 2022a; Wang et al., 2023).
Position Bias: Position bias occurs when, in comparing generated texts, LLMs prefer the answer given in certain positions. If we define the ground truth probability of $a$ (the first parameter) preferred over $b$ (the second parameter) to be $P(a, b)$, it should be that $P\left(y_{0}, y_{1}\right)=1-P\left(y_{1}, y_{0}\right)$, meaning the position should have no affect on the judgement. Position bias is when the comparison by model is $\hat{P}\left(y_{0}, y_{1}\right) \neq\left(1-\hat{P}\left(y_{1}, y_{0}\right)\right)$. For example, GPT-4 tends to prefer the first option given to it, while ChatGPT prefers the second option (Wang et al., 2023). To account for this bias, we can simply swap the positions and evaluate the options twice. If the model gives contradicting results between permutations, we count it as a draw.
Wang et al. (2023) has proposed several methods to calibrate this bias further: Multiple Evidence Calibration asks the LLM to provide evidence before making judgement, and Human-in-the-Loop Calibration involves human adjustment when deemed neccessary.
Self-enhancement Bias: LLMs tend to prefer answers generated by itself compared to answers generated by other models. This becomes a problem when benchmarking LLMs by evaluating them with LLMs (Zheng et al., 2023), but not so much in the context of RLAIF, as the comparisons are always between answers generated by the same model.
Verbosity Bias: Verbosity bias refers to the bias where LLMs prefer longer, more verbose answers even if there are no difference in quality. Training with RLAIF with verbosity bias present can lead to LLMs generating excessively long responses, when in reality a much more concise response would suffice. In tasks such as question answering, a verbose response can be critical to its usefulness, but there aren't enough researches that look into this. For these reasons we take a closer look into this.
There are several proposed methods to mitigate the effect of biases.
Chain-of-thought Prompting is a prompting technique where the LLM is asked to provide the thought process before generating the actual evaluation. This way, at the time when the LLM generates the actual evaluation, it has its chain-of-thought to base its evaluation from. This encourages human alignment and more accurate evaluations, rather than arbitrary evaluations without thought.
One-shot/Few-shot Prompting is another prompting technique which gives one example/several examples of a prompt and its corresponding correct answer when promting the LLM. When generating the response, the LLM can continue the pattern from the examples to better align with the intended response.</p>
<h2>3 Related Works</h2>
<h3>3.1 RLAIF Advancements</h3>
<p>There have been several recent advancements in the field of RLAIF. Bai et al. (2022b) trained an LLM via RLAIF with limited human feedback. In this work, they claim that helpfulness and harmfulness have a trade-off relationship, and aim to train an LLM that keeps a balance between those two. Their method only requires human feedback in the helpfulness aspect, and harmless behavior is achieved purely from RLAIF. The LLMs trained in the work by Lee et al. (2023) achieved near-human performance in summarization tasks with RLAIF without any human feedback. While not a study on RLAIF itself, Zheng et al. (2023) evaluate LLMs with other LLMs as a judge and show that GPT-4 has a high human alignment and agrees with humans on over $80 \%$ of evaluations.</p>
<h3>3.2 On Verbosity Bias in Evaluations by LLMs</h3>
<p>Zheng et al. (2023) also provides lists of biases and methods to overcome them. Alongside their experiment on position bias, they experimented on verbosity bias by attempting a "repetitive list attack" on several LLMs. This attack pertains to "listing" tasks, in which the prompt asks to list several items (e.g. "What are examples of fruits that are round?"). The "repetitive list attack" is done by making the answers verbose by repeating items multiple times, and then asking the LLMs to evaluate these augmented answers. If the LLM evaluates these "repetitive lists" to be better than the</p>
<p>original, the attack is considered a success. Their results show GPT-4 is significantly less prone to this attack with below $10 \%$ success rate, while GPT-3.5 and Claude-v1 both suffer over $90 \%$ success rate. Compared to this research, we expand the problem setting to general question-answering tasks. Huang et al. (2023) tackle verbosity bias in summarization tasks. They found that GPT-4 actually prefers short responses in faithfulness and coverage when it comes to summarization, although this is seen strongly only in single-answer grading, and not in comparison grading. This suggests that verbosity bias can be different between different tasks.</p>
<p>Compared to these studies, our problem setting is more general and we compare the verbosity preference between humans and LLMS. The experiments conducted in these papers measure the difference in evaluations when the texts are artificially made verbose while maintaining the same content. The assumption is that elongating the texts would have no effect on a true evaluator, so the difference in evaluation indicates verbosity bias. In our attempt to broaden the problem setting, we make use of human feedback as the oracle instead of making this assumption.</p>
<h1>4 Verbosity Preference of LLMs</h1>
<p>First, we experiment to see how much LLMs actually prefer longer answers. We ask GPT-4 to choose between pairs of responses and examine if it prefers longer responses or not. We did not limit our scope to prompts answered with a specific format (like lists in Zheng et al. (2023)) in order to observe the LLMs' general tendency to prefer longer answers.</p>
<p>We generated 100 sample answers each to 3 prompts, all from the same model (Vicuna-7b-v1.5) generated with the temperature parameter set to 0.7 . One of the questions and two examples of the answers are as follows.</p>
<ul>
<li>
<p>Question: Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions</p>
</li>
<li>
<p>I recently had the opportunity to travel to Hawaii, and it was an experience that I will never forget. From stunning beaches to vibrant culture, there was so much to explore and enjoy during my time on the islands...</p>
</li>
<li>Hawaii is a tropical paradise, and I recently had the opportunity to experience it for myself. This archipelago is made up of eight main islands, each with its unique culture, traditions, and breathtaking landscapes. During my trip, I had the chance to visit several cultural sites, such as the Polynesian Cultural Center on Oahu...</li>
<li>...</li>
</ul>
<p>The prompts are taken from the library introduced by Zheng et al. (2023), all from the "creative" category because 1) answers generated to other categories didn't vary in word count enough to see verbosity bias and 2) GPT-4 was not good at judging answers in those categories. We then take answers from these generated samples and insert them into the template shown in Figure 1. With the template complete, we asked GPT-4 to evaluate preferences between pairs of answers with the template. The outcome is either the first option selected, a draw, or the second option selected. In order to account for position bias, GPT-4 evaluated the pair twice with the position swapped the second time. It was considered a draw unless it gave the same result on both permutations.</p>
<p>The results are shown in Figure 3a for the overall result, and Figures 3b to 3d for results from each prompt. Both in the overall result and the individual results, there is a tendency for GPT-4 to prefer longer answers. When the word count difference is large enough, GPT-4 almost always prefers the longer answer. For question 1 and 2, the preference is smooth and clear, while for question 3, when the word count difference is small, there is high variance in evaluation. As we can see the shape varies between questions, and therefore we can deduce that verbosity does not rely entirely on word count and is different for each question. This makes adjusting for verbosity post-evaluation hard unless we know the verbosity preference shape for the prompt in question.</p>
<p>From this experiment, we can draw the conclusion that GPT-4 generally prefers longer answers among those that are generated by the same LLM with the same prompt. However, this experiment by itself does not indicate that GPT-4 suffers from verbosity bias; it could be that the longer answers generated by vicuna are actually higher in quality and helpfulness. In order to truly measure verbosity bias, we would need the ground truth of each comparison which we do not have. Instead, we next utilize a dataset of human evaluations as the baseline.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example experimental results for three questions (Question 1: Blog about trip to Hawaii, Question 2: Email to professor about paper, Question 3: Blog comparing smartphones). Figure 3a combines results from all three questions and Figures 3b to 3d show results for each indivisual quesion. X -axis is the percentage of the difference between the first and the second option compared to the length of the second option. Y-axis is the actual score, with 1.0 meaning the first option was selected, 0 meaning it was a draw, and -1.0 meaning the second option was selected. There is an positive correlation between word count difference between the two options and the resulting evaluation. The data points are binned with each ranging $20 \%$. The circles represent the average in each range and the errorbars show the standard deviation.</p>
<h1>5 Is There A Difference in Verbosity Preference Between LLMs and Humans?</h1>
<p>Considering that LLMs replace humans as annotators in RLAIF, it is sufficient if LLMs could replicate human feedback and it does not necessarily have to be aligned with the ground truth. As seen in Figure 4 which plots verbosity preference of humans in the HH-RLHF dataset described later, humans seem to prefer longer answers too. Whether or not the longer answers are actually helpful is irrelevant as long as the LLM and the human come to the same conclusion. In light of this, we compare the difference in verbosity preference between LLMs and humans. We can view this as verbosity bias since the aim of LLM judgment in RLAIF is human alignment and not the eradication of biased preference in verbosity.</p>
<p>We use the HH-RLHF dataset (Bai et al., 2022a) which contains human feedback data comparing pairs of answers to a prompt. It only has one feedback data per prompt, so we cannot plot the verbosity preference of humans like in the experiment in the previous chapter. Instead we can see the dissimilarity between LLMs and humans in verbosity preference in general across various questions. Precisely, this experiment looks into the relationship between the difference in a number of words in the pair of responses and the human alignment of LLMs, meaning how often LLMs give the same judgment as humans.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: X-axis is the percentage of the difference between the first and the second option compared to the length of the second option. Y-axis is the resulting score by human judgement.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: X-axis is the percentage of the difference between the chosen and the rejected option, compared to the length of the rejected option. Y-axis is the human alignment measured by the rate of LLM's decision agreeing with humans. The numbers in brackets indicate the sample size in each bracket.</p>
<p>We used the same prompt template as the previous experiment but asked GPT-4 to evaluate the whole conversation. Unlike the previous experiment which evaluated answers to a single question, HH-RLHF contains conversations between a human and an assistant. Therefore we asked GPT-4 to evaluate the pair of whole conversations and answer which assistant was more helpful.
In cases where human feedback preferred the longer answer, human alignment was high for the LLMs, meaning the LLMs preferred the longer answers as well. However, when human feedback chose the answer with fewer words, human alignment was low, because the LLMs still chose the longer answers regardless of the helpfulness of the shorter answer.
One possible explanation for this is that LLMs learned to mimic human behavior heuristically by choosing longer answers - in this dataset, human feedback did tend to favor longer responses as seen in Figure 4, and it is possible the dataset used to train GPT-3.5/GPT-4 had the same tendency. Nevertheless, a closer look into the cause is up for debate.</p>
<p>Table 1: Verbosity bias values calculated with (6) for GPT-4 and GPT-3.5 with data from experiment.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>GPT-4</th>
<th>GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Verbosity Bias</td>
<td>0.328</td>
<td>0.428</td>
</tr>
</tbody>
</table>
<p>6 Formulation of Verbosity Bias</p>
<p>In the second experiment, we observed the tendency of LLMs to have low human alignment for cases where human feedback preferred shorter answers. In this section, we formulate verbosity bias to allow for quantative comparison between models.</p>
<p>In our problem setting, we define the given pair of text inputs as $y_{0}$ and $y_{1}$, the LLM outputs decision as $Y^{\prime}\in{0,1}$, and the more helpful option labeled by humans as $Y \in{0,1}$. We define the sensitive attribute $S \in{0,1}$ which equals 0 when $y_{0}$ has more words than $y_{1}$, and 1 when $y_{1}$ has more words than $y_{0}$.</p>
<p>With these definitions, equal opportunity (Hardt et al., 2016) with respect to sensitive attribute $S$ is satisfied if</p>
<p>$$
P\left(Y^{\prime}=0 \mid S=0, Y=0\right)=P\left(Y^{\prime}=0 \mid S=1, Y=0\right)
$$</p>
<p>This only accounts for cases where human feedback prefers $y_{0}$. Although this can be attained by sorting the inputs beforehand, the equation can be generalized with accuracy parity instead of equal opportunity. Accuracy parity is satisfied if the accuracy of prediction is equal among both demographics:</p>
<p>$$
P\left(Y^{\prime}=Y \mid S=Y\right)=P\left(Y^{\prime}=Y \mid S=1-Y\right)
$$</p>
<p>The deviance from accuracy parity can be calculated with the following equation:</p>
<p>$$
\left|P\left(Y^{\prime}=Y \mid S=Y\right)-P\left(Y^{\prime}=Y \mid S=1-Y\right)\right|
$$</p>
<p>Even though this is how the deviance is calculated in general, we thought it important that the directional information of the bias isn't lost. With the formulation below (6), a positive value indicates that the LLM prefers verbose answers, and a negative value indicates it prefers shorter answers. This distinction is crucial as some tasks may have a negative bias, for example in summarization tasks as shown in Huang et al. (2023). We also opted for the difference in inaccuracy between demographics</p>
<p>$$
P\left(Y^{\prime}=1-Y \mid S=1-Y\right)-P\left(Y^{\prime}=1-Y \mid S=Y\right)
$$</p>
<p>because verbosity bias refers to the inacuracy influenced by verbosity.
Table 1 shows the verbosity bias values of GPT-3.5 and GPT-4 calculated with data from Section 5. From these numbers, we can conclude that GPT-4 has improved in verbosity bias. Compared to Wang et al. (2023), which had a limited problem setting and gave the impression that GPT-4 is significantly less prone to verbosity bias, we see that the verbosity bias still exists for GPT-4. A further experiment on other LLMs for comparison is required.</p>
<h1>7 Discussion</h1>
<h3>7.1 Other Metrics of Equality</h3>
<p>In the context of our study, we treat the verbosity of the response pair as the sensitive attribute in our formulation of verbosity bias in Section 6. What verbosity differs from sensitive attributes generally discussed in other cases of biases is the fact that verbosity should actually be taken into consideration when evaluating the responses, whereas attributes like gender or race shouldn't be a factor in the outcome in other cases. This is why employing other metrics of equality like demographic parity doesn't make sense here, and therefore we base the measurement of verbosity bias on equal opportunity and accuracy parity.</p>
<h1>7.2 Limitations of Our Experiments</h1>
<p>In the experiment in Section 4, we generate the sample responses from the same questions (before concatenation of results from all three questions). However, in our experiment in Section 5, we mix together results from various questions. This has led us to only attain the result across many kinds of questions, not the result on any specific question like in the experiment in Section 4. It is debatable which of these results is preferable.</p>
<h3>7.3 Limitation of Our Metric of Verbosity Bias</h3>
<p>Our formulation of verbosity bias only accounts for bias between two groups divided by whether $y_{0}$ is longer than $y_{1}$. What it cannot detect is the bias within each of these groups; it is agnostic to the bias between cases where $y_{0}$ is barely longer than $y_{1}$ and cases where $y_{0}$ is significantly longer than $y_{0}$. Hence, if there were to be an instance where the model has high human alignment when there is a large difference in length between the pair of responses - the plot would have a concave shape symmetrical around the vertical line down the middle - our metric would suggest that the model has close to zero verbosity bias. To avoid such a situation, showing the human alignment plot alongside the metric is recommended.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we conducted experiments on the verbosity bias seen in LLMs' judgment by LLMs. In previous works, the problem settings were limited and did not compare the verbosity preference to humans. With our experiments, we saw that 1) LLMs tend to favor longer answers for creative writing tasks, and 2) alignment with humans varies on verbosity with lower human alignment in cases where humans preferred shorter answers. We then formulated verbosity bias based on accuracy parity that can be used to quantitatively compare verbosity biases among models.</p>
<h2>References</h2>
<p>Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022b). Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. (2007). Large language models in machine translation.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29.</p>
<p>Huang, K.-H., Laban, P., Fabbri, A. R., Choubey, P. K., Joty, S., Xiong, C., and Wu, C.-S. (2023). Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles. arXiv preprint arXiv:2309.09369.</p>
<p>Katz, S. (1987). Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE transactions on acoustics, speech, and signal processing, 35(3):400-401.</p>
<p>Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V., and Rastogi, A. (2023). Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in Neural Information Processing Systems, $33: 3008-3021$.</p>
<p>Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. (2023). Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. (2023). A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Research done during internship at LY Corp.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
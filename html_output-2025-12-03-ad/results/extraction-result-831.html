<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-270558898</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11176v2.pdf" target="_blank">Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement</a></p>
                <p><strong>Paper Abstract:</strong> Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the **I**terative step-level **P**rocess **R**efinement **(IPR)** framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical finds highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e831.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e831.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative step-level Process Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training framework that injects step-level process supervision into LLM agent training by (1) estimating step-wise rewards via Monte Carlo sampling, (2) generating contrastive step/trajectory pairs by letting the agent explore along expert trajectories, and (3) iteratively optimizing the agent with a mixture loss (outcome-DPO, step-DPO, and SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>IPR-trained agents (e.g., Llama-2-7B after IPR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses existing autoregressive transformer LLMs (ReAct-style trajectories) as agent policy; integrates a scorer to Monte-Carlo estimate step rewards, constructs contrastive action/trajectory pairs, and applies mixture trajectory optimization combining direct preference optimization (DPO) at outcome and step levels plus supervised fine-tuning loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop, InterCodeSQL, ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; interactive SQL querying (sequential decision-making); embodied household tasks (embodied navigation & manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported improvements vs baselines: IPR outperforms prior SOTA ETO by +5.8% (WebShop), +7.2% (InterCodeSQL), +2.5% (ALFWorld seen), +3.2% (ALFWorld unseen); example: Llama-2-7B average reward increased from 5.5 (prompt/untrained) to 69.4 after IPR.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No new model architecture; uses chain-of-thought / ReAct-style trajectory format, an external scorer (scorer agent or learned reward model) for step reward estimation, and contrastive dataset construction mechanism; training uses DPO which references a reference policy.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Mixture: supervised fine-tuning (SFT) to initialize; Monte Carlo sampling for step reward acquisition; iterative offline preference-style optimization combining outcome-level DPO, step-level DPO, and SFT losses.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (process supervision + iterative preference learning + reward estimation module)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Introduces step-level supervision by estimating per-step expected outcome reward via Monte Carlo rollouts from a scorer; generates contrastive 'win/lose' step and trajectory pairs by letting the agent explore along expert trajectories; optimizes agent iteratively using a mixture loss (outcome-DPO + step-DPO + SFT) while reusing updated agent as new base for further iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Consistent empirical improvements across three interactive benchmarks; surpasses ETO by reported margins (WebShop +5.8%, InterCodeSQL +7.2%, ALFWorld seen +2.5%, ALFWorld unseen +3.2%); increases average reward-per-step and overall action efficiency; example numeric change for Llama-2-7B: average reward increased from 5.5 (untrained/prompt) to 69.4 (IPR).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper argues that training regimes that only optimize final/outcome reward (or treat whole trajectories as single units) ignore informative intermediate supervision, which allows errors, detours, and accidental successes to persist; adding step-level supervision addresses this process vs outcome mismatch and reduces step errors, closing the gap between static QA/reasoning performance and interactive task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e831.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e831.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ETO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploration-based Trajectory Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior trajectory-tuning method that leverages both successful and failure trajectories (exploration-based) and optimizes agents using contrastive outcome-level preference optimization (DPO) focusing on final outcome reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trial and error: Exploration-based trajectory optimization for llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ETO-tuned agents (as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tunes LLM agents by collecting success and failure trajectories via exploration and applying DPO-style optimization on outcome-level contrastive trajectory pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop, InterCodeSQL, ALFWorld (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; interactive SQL querying; embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported strong baseline performance (but lower than IPR); specific comparative gaps: IPR > ETO by +5.8% (WebShop), +7.2% (InterCodeSQL), +2.5% (ALFWorld seen), +3.2% (ALFWorld unseen). Example ETO numbers (Llama-2-7B + ETO) shown in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Trajectory-level contrastive fine-tuning using DPO (outcome-level preference optimization) based on exploration of success/failure trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (outcome-refinement via preference optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Collects both successful and failed trajectories via agent exploration and applies direct preference optimization (DPO) on outcome-level trajectory contrasts to push the model toward higher outcome rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves over simple SFT baselines but underperforms IPR that adds process-level supervision; used as the strongest prior SOTA baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Because ETO focuses on outcome-level contrasts (whole trajectory outcomes) it may miss informative per-step signals and thus not fully correct step-level errors or inefficient actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e831.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e831.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning on expert trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised fine-tuning of open-source LLMs on expert ReAct-style trajectories to endow base agent capabilities prior to further refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SFT-initialized agents (e.g., Llama-2-7B + SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard supervised fine-tuning of a transformer LLM on example expert action-observation trajectories (ReAct style), maximizing likelihood of expert actions given the trajectory context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop, InterCodeSQL, ALFWorld (used to produce base agents)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; interactive SQL; embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SFT provides a basic capability baseline; performance lower than ETO and substantially lower than IPR in many settings (examples in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning on expert trajectories (behavior cloning / maximum likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Train model to imitate expert ReAct-style trajectories via maximum likelihood to obtain a base agent used for MC scoring and iterative optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Provides initial agent competence; ablation shows removing SFT loss in the IPR mixture causes the largest performance drop, indicating SFT remains an important component for stabilizing learning.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>SFT alone lacks process-level corrective signals beyond imitation of provided trajectories and thus cannot correct step-level inefficiencies discovered during exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e831.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e831.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-PPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-level PPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A process-refinement baseline that applies proximal policy optimization (PPO) to maximize step-level process rewards rather than only final outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Step-PPO optimized agents (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies reinforcement learning (PPO) using estimated step-level rewards as the optimization objective to improve per-step actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop, InterCodeSQL, ALFWorld (evaluated as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; interactive SQL; embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Mixed: performs well on InterCodeSQL but exhibits instability in RL optimization leading to poor performance on the other tasks; overall underperforms IPR.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO) on step-level rewards</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (RL-based process refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Optimizes agent policy with PPO using the constructed step-level rewards as the training signal to encourage better actions at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Can yield good performance on some tasks (e.g., InterCodeSQL) but suffers from instability and poor general performance across heterogeneous interactive tasks; authors cite instability of online RL as a practical issue motivating offline contrastive learning instead.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Instability and inefficiency of RL (PPO) on large LLM policies and long-horizon action sequences can prevent consistent gains across tasks despite using step-level rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e831.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e831.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step reward model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned Step Reward Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A regression model trained on Monte Carlo–estimated step rewards to approximate per-step reward estimation without repeated MC sampling, reducing scoring cost while providing process supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Learned step reward model (trained with Llama-generated data)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned reward predictor that maps (historical trajectory, current action) to a scalar step reward; trained with MSE on MC-sampled step reward labels collected from Llama-generated actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (experiment reported); tested for generalization to other base models</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation (used as scorer for step rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>When used as scorer in IPR-style pipeline, performance improves relative to no-step-reward baselines but remains below MC-based scorer; example WebShop results (from Table 5): Llama-2-7B with No Reward=67.4, Reward Model=68.9, MC Method=71.3 (numbers are reported average reward values).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>separate reward-prediction module trained on collected (action, context) → step reward pairs</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised regression (MSE) using MC-estimated step rewards as labels</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (learned reward model to replace MC scorer; computational efficiency intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Train a lightweight reward model on MC-estimated step rewards so that future agents can obtain step-level supervision without expensive MC rollouts; the model generalizes to actions from other base models to some degree.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reduces the computational cost of constructing step rewards and yields better agent performance than no step reward, but performs worse than the MC-based scorer; example: for Llama-2-7B on WebShop Reward Model yields 68.9 vs MC 71.3 average reward.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Approximation error in the learned reward model reduces accuracy of per-step supervision compared to the higher-fidelity MC estimator, limiting final agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e831.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e831.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (prompt baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used as a prompt-based baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large LLM used in one-shot ReAct prompting to perform interactive agent tasks; evaluated as a prompt-based baseline on the same interactive benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4 (prompting / one-shot ReAct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM (autoregressive/transformer) used in zero-/one-shot prompt-based agent setups (ReAct style).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop, InterCodeSQL, ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; interactive SQL; embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Prompt-based performance reported in Table 2: WebShop 63.2, InterCodeSQL 38.5, ALFWorld seen 42.9, ALFWorld unseen 38.1, Average 45.7 (these are the numbers reported for GPT-4 in the paper's baseline table).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting (one-shot ReAct); no additional fine-tuning reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used ReAct-style one-shot prompts to turn the LLM into an agent for interactive tasks without specialized fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Performs reasonably on some interactive tasks but is outperformed by IPR-trained open models in this paper; demonstrates that prompting alone may be insufficient to reach SFT+process-refinement performance in complex interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Prompted LLMs may lack task-specific trajectory-level and step-level adaptations required for long-horizon interactive decision-making; lack of iterative process supervision and environment-specific adaptation contributes to lower interactive task performance compared to tuned agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e831.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e831.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B (base LLM used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B parameter transformer LLM used as the primary base model for experiments; evaluated as untrained prompt baseline, SFT-initialized agent, and IPR-optimized agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open foundation and fine-tuned chat models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Llama-2-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B parameter autoregressive transformer LLM (chat/fine-tuned variants used as agent backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop, InterCodeSQL, ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; interactive SQL; embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Multiple numbers reported: untrained/prompt-based average ~5.5 (poor), after SFT and other tuning higher; after IPR Llama-2-7B + IPR average reward reported 69.4 (example highlight in paper). See paper tables for per-task numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Transformer-based LLM; used with ReAct-style thought/action formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Evaluated in several regimes: prompting, SFT, PPO, RFT, ETO, IPR (mixture optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>various (SFT, RL, DPO-based trajectory tuning, IPR process supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper applies SFT to produce base agent; then evaluates outcome-refinement (PPO, RFT, ETO) and process-refinement (Step-PPO) baselines; applies IPR (step-level supervision) to substantially improve interactive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large gains from IPR: example Llama-2-7B average reward rose from ~5.5 (prompt/untrained baseline) to 69.4 after IPR; IPR outperforms alternatives such as ETO, PPO and Step-PPO on aggregate metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Base LLMs without task-specific trajectory-level and step-level supervision fail at long-horizon interactive tasks due to missing process feedback and the need to adapt to environment dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Trial and error: Exploration-based trajectory optimization for llm agents <em>(Rating: 2)</em></li>
                <li>WebShop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-831",
    "paper_id": "paper-270558898",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "IPR",
            "name_full": "Iterative step-level Process Refinement",
            "brief_description": "A training framework that injects step-level process supervision into LLM agent training by (1) estimating step-wise rewards via Monte Carlo sampling, (2) generating contrastive step/trajectory pairs by letting the agent explore along expert trajectories, and (3) iteratively optimizing the agent with a mixture loss (outcome-DPO, step-DPO, and SFT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "IPR-trained agents (e.g., Llama-2-7B after IPR)",
            "model_description": "Uses existing autoregressive transformer LLMs (ReAct-style trajectories) as agent policy; integrates a scorer to Monte-Carlo estimate step rewards, constructs contrastive action/trajectory pairs, and applies mixture trajectory optimization combining direct preference optimization (DPO) at outcome and step levels plus supervised fine-tuning loss.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop, InterCodeSQL, ALFWorld",
            "interactive_task_type": "web navigation; interactive SQL querying (sequential decision-making); embodied household tasks (embodied navigation & manipulation)",
            "interactive_performance": "Reported improvements vs baselines: IPR outperforms prior SOTA ETO by +5.8% (WebShop), +7.2% (InterCodeSQL), +2.5% (ALFWorld seen), +3.2% (ALFWorld unseen); example: Llama-2-7B average reward increased from 5.5 (prompt/untrained) to 69.4 after IPR.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No new model architecture; uses chain-of-thought / ReAct-style trajectory format, an external scorer (scorer agent or learned reward model) for step reward estimation, and contrastive dataset construction mechanism; training uses DPO which references a reference policy.",
            "training_method": "Mixture: supervised fine-tuning (SFT) to initialize; Monte Carlo sampling for step reward acquisition; iterative offline preference-style optimization combining outcome-level DPO, step-level DPO, and SFT losses.",
            "intervention_type": "training method (process supervision + iterative preference learning + reward estimation module)",
            "intervention_description": "Introduces step-level supervision by estimating per-step expected outcome reward via Monte Carlo rollouts from a scorer; generates contrastive 'win/lose' step and trajectory pairs by letting the agent explore along expert trajectories; optimizes agent iteratively using a mixture loss (outcome-DPO + step-DPO + SFT) while reusing updated agent as new base for further iterations.",
            "intervention_effect": "Consistent empirical improvements across three interactive benchmarks; surpasses ETO by reported margins (WebShop +5.8%, InterCodeSQL +7.2%, ALFWorld seen +2.5%, ALFWorld unseen +3.2%); increases average reward-per-step and overall action efficiency; example numeric change for Llama-2-7B: average reward increased from 5.5 (untrained/prompt) to 69.4 (IPR).",
            "hypothesized_cause_of_gap": "The paper argues that training regimes that only optimize final/outcome reward (or treat whole trajectories as single units) ignore informative intermediate supervision, which allows errors, detours, and accidental successes to persist; adding step-level supervision addresses this process vs outcome mismatch and reduces step errors, closing the gap between static QA/reasoning performance and interactive task performance.",
            "uuid": "e831.0",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ETO",
            "name_full": "Exploration-based Trajectory Optimization",
            "brief_description": "A prior trajectory-tuning method that leverages both successful and failure trajectories (exploration-based) and optimizes agents using contrastive outcome-level preference optimization (DPO) focusing on final outcome reward.",
            "citation_title": "Trial and error: Exploration-based trajectory optimization for llm agents",
            "mention_or_use": "use",
            "model_or_agent_name": "ETO-tuned agents (as reported in paper)",
            "model_description": "Fine-tunes LLM agents by collecting success and failure trajectories via exploration and applying DPO-style optimization on outcome-level contrastive trajectory pairs.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop, InterCodeSQL, ALFWorld (evaluated as baseline)",
            "interactive_task_type": "web navigation; interactive SQL querying; embodied tasks",
            "interactive_performance": "Reported strong baseline performance (but lower than IPR); specific comparative gaps: IPR &gt; ETO by +5.8% (WebShop), +7.2% (InterCodeSQL), +2.5% (ALFWorld seen), +3.2% (ALFWorld unseen). Example ETO numbers (Llama-2-7B + ETO) shown in paper tables.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "Trajectory-level contrastive fine-tuning using DPO (outcome-level preference optimization) based on exploration of success/failure trajectories.",
            "intervention_type": "training method (outcome-refinement via preference optimization)",
            "intervention_description": "Collects both successful and failed trajectories via agent exploration and applies direct preference optimization (DPO) on outcome-level trajectory contrasts to push the model toward higher outcome rewards.",
            "intervention_effect": "Improves over simple SFT baselines but underperforms IPR that adds process-level supervision; used as the strongest prior SOTA baseline in experiments.",
            "hypothesized_cause_of_gap": "Because ETO focuses on outcome-level contrasts (whole trajectory outcomes) it may miss informative per-step signals and thus not fully correct step-level errors or inefficient actions.",
            "uuid": "e831.1",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SFT",
            "name_full": "Supervised Fine-Tuning on expert trajectories",
            "brief_description": "Supervised fine-tuning of open-source LLMs on expert ReAct-style trajectories to endow base agent capabilities prior to further refinement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "SFT-initialized agents (e.g., Llama-2-7B + SFT)",
            "model_description": "Standard supervised fine-tuning of a transformer LLM on example expert action-observation trajectories (ReAct style), maximizing likelihood of expert actions given the trajectory context.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop, InterCodeSQL, ALFWorld (used to produce base agents)",
            "interactive_task_type": "web navigation; interactive SQL; embodied tasks",
            "interactive_performance": "SFT provides a basic capability baseline; performance lower than ETO and substantially lower than IPR in many settings (examples in paper tables).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "Supervised fine-tuning on expert trajectories (behavior cloning / maximum likelihood).",
            "intervention_type": "training method (supervised fine-tuning)",
            "intervention_description": "Train model to imitate expert ReAct-style trajectories via maximum likelihood to obtain a base agent used for MC scoring and iterative optimization.",
            "intervention_effect": "Provides initial agent competence; ablation shows removing SFT loss in the IPR mixture causes the largest performance drop, indicating SFT remains an important component for stabilizing learning.",
            "hypothesized_cause_of_gap": "SFT alone lacks process-level corrective signals beyond imitation of provided trajectories and thus cannot correct step-level inefficiencies discovered during exploration.",
            "uuid": "e831.2",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Step-PPO",
            "name_full": "Step-level PPO",
            "brief_description": "A process-refinement baseline that applies proximal policy optimization (PPO) to maximize step-level process rewards rather than only final outcomes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Step-PPO optimized agents (baseline)",
            "model_description": "Applies reinforcement learning (PPO) using estimated step-level rewards as the optimization objective to improve per-step actions.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop, InterCodeSQL, ALFWorld (evaluated as a baseline)",
            "interactive_task_type": "web navigation; interactive SQL; embodied tasks",
            "interactive_performance": "Mixed: performs well on InterCodeSQL but exhibits instability in RL optimization leading to poor performance on the other tasks; overall underperforms IPR.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "Reinforcement learning (PPO) on step-level rewards",
            "intervention_type": "training method (RL-based process refinement)",
            "intervention_description": "Optimizes agent policy with PPO using the constructed step-level rewards as the training signal to encourage better actions at each step.",
            "intervention_effect": "Can yield good performance on some tasks (e.g., InterCodeSQL) but suffers from instability and poor general performance across heterogeneous interactive tasks; authors cite instability of online RL as a practical issue motivating offline contrastive learning instead.",
            "hypothesized_cause_of_gap": "Instability and inefficiency of RL (PPO) on large LLM policies and long-horizon action sequences can prevent consistent gains across tasks despite using step-level rewards.",
            "uuid": "e831.3",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Step reward model",
            "name_full": "Learned Step Reward Model",
            "brief_description": "A regression model trained on Monte Carlo–estimated step rewards to approximate per-step reward estimation without repeated MC sampling, reducing scoring cost while providing process supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Learned step reward model (trained with Llama-generated data)",
            "model_description": "A learned reward predictor that maps (historical trajectory, current action) to a scalar step reward; trained with MSE on MC-sampled step reward labels collected from Llama-generated actions.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (experiment reported); tested for generalization to other base models",
            "interactive_task_type": "web navigation (used as scorer for step rewards)",
            "interactive_performance": "When used as scorer in IPR-style pipeline, performance improves relative to no-step-reward baselines but remains below MC-based scorer; example WebShop results (from Table 5): Llama-2-7B with No Reward=67.4, Reward Model=68.9, MC Method=71.3 (numbers are reported average reward values).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "separate reward-prediction module trained on collected (action, context) → step reward pairs",
            "training_method": "Supervised regression (MSE) using MC-estimated step rewards as labels",
            "intervention_type": "training method (learned reward model to replace MC scorer; computational efficiency intervention)",
            "intervention_description": "Train a lightweight reward model on MC-estimated step rewards so that future agents can obtain step-level supervision without expensive MC rollouts; the model generalizes to actions from other base models to some degree.",
            "intervention_effect": "Reduces the computational cost of constructing step rewards and yields better agent performance than no step reward, but performs worse than the MC-based scorer; example: for Llama-2-7B on WebShop Reward Model yields 68.9 vs MC 71.3 average reward.",
            "hypothesized_cause_of_gap": "Approximation error in the learned reward model reduces accuracy of per-step supervision compared to the higher-fidelity MC estimator, limiting final agent performance.",
            "uuid": "e831.4",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 (prompt baseline)",
            "name_full": "GPT-4 (used as a prompt-based baseline)",
            "brief_description": "A proprietary large LLM used in one-shot ReAct prompting to perform interactive agent tasks; evaluated as a prompt-based baseline on the same interactive benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4 (prompting / one-shot ReAct)",
            "model_description": "Proprietary LLM (autoregressive/transformer) used in zero-/one-shot prompt-based agent setups (ReAct style).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop, InterCodeSQL, ALFWorld",
            "interactive_task_type": "web navigation; interactive SQL; embodied tasks",
            "interactive_performance": "Prompt-based performance reported in Table 2: WebShop 63.2, InterCodeSQL 38.5, ALFWorld seen 42.9, ALFWorld unseen 38.1, Average 45.7 (these are the numbers reported for GPT-4 in the paper's baseline table).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "Prompting (one-shot ReAct); no additional fine-tuning reported in this paper",
            "intervention_type": "prompting strategy (baseline)",
            "intervention_description": "Used ReAct-style one-shot prompts to turn the LLM into an agent for interactive tasks without specialized fine-tuning.",
            "intervention_effect": "Performs reasonably on some interactive tasks but is outperformed by IPR-trained open models in this paper; demonstrates that prompting alone may be insufficient to reach SFT+process-refinement performance in complex interactive environments.",
            "hypothesized_cause_of_gap": "Prompted LLMs may lack task-specific trajectory-level and step-level adaptations required for long-horizon interactive decision-making; lack of iterative process supervision and environment-specific adaptation contributes to lower interactive task performance compared to tuned agents.",
            "uuid": "e831.5",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama-2-7B (base LLM used in experiments)",
            "brief_description": "Open-source 7B parameter transformer LLM used as the primary base model for experiments; evaluated as untrained prompt baseline, SFT-initialized agent, and IPR-optimized agent.",
            "citation_title": "Llama 2: Open foundation and fine-tuned chat models",
            "mention_or_use": "use",
            "model_or_agent_name": "Llama-2-7B (base)",
            "model_description": "7B parameter autoregressive transformer LLM (chat/fine-tuned variants used as agent backbone).",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop, InterCodeSQL, ALFWorld",
            "interactive_task_type": "web navigation; interactive SQL; embodied tasks",
            "interactive_performance": "Multiple numbers reported: untrained/prompt-based average ~5.5 (poor), after SFT and other tuning higher; after IPR Llama-2-7B + IPR average reward reported 69.4 (example highlight in paper). See paper tables for per-task numbers.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Transformer-based LLM; used with ReAct-style thought/action formatting.",
            "training_method": "Evaluated in several regimes: prompting, SFT, PPO, RFT, ETO, IPR (mixture optimization)",
            "intervention_type": "various (SFT, RL, DPO-based trajectory tuning, IPR process supervision)",
            "intervention_description": "Paper applies SFT to produce base agent; then evaluates outcome-refinement (PPO, RFT, ETO) and process-refinement (Step-PPO) baselines; applies IPR (step-level supervision) to substantially improve interactive performance.",
            "intervention_effect": "Large gains from IPR: example Llama-2-7B average reward rose from ~5.5 (prompt/untrained baseline) to 69.4 after IPR; IPR outperforms alternatives such as ETO, PPO and Step-PPO on aggregate metrics reported.",
            "hypothesized_cause_of_gap": "Base LLMs without task-specific trajectory-level and step-level supervision fail at long-horizon interactive tasks due to missing process feedback and the need to adapt to environment dynamics.",
            "uuid": "e831.6",
            "source_info": {
                "paper_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Trial and error: Exploration-based trajectory optimization for llm agents",
            "rating": 2,
            "sanitized_title": "trial_and_error_explorationbased_trajectory_optimization_for_llm_agents"
        },
        {
            "paper_title": "WebShop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 2,
            "sanitized_title": "webshop_towards_scalable_realworld_web_interaction_with_grounded_language_agents"
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 1,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        }
    ],
    "cost": 0.01770725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement
24 Sep 2024</p>
<p>Weimin Xiong wmxiong@pku.edu.cn 
School of Computer Science
State Key Laboratory of Multimedia Information Processing
Peking University</p>
<p>Code &amp; Data: https://github.com/WeiminXiong/IPR</p>
<p>Yifan Song 
School of Computer Science
State Key Laboratory of Multimedia Information Processing
Peking University</p>
<p>Code &amp; Data: https://github.com/WeiminXiong/IPR</p>
<p>Xiutian Zhao 
Huawei Technologies</p>
<p>Wenhao Wu 
School of Computer Science
State Key Laboratory of Multimedia Information Processing
Peking University</p>
<p>Code &amp; Data: https://github.com/WeiminXiong/IPR</p>
<p>Xun Wang 
School of Computer Science
State Key Laboratory of Multimedia Information Processing
Peking University</p>
<p>Code &amp; Data: https://github.com/WeiminXiong/IPR</p>
<p>Ke Wang 
Huawei Technologies</p>
<p>Cheng Li 
Huawei Technologies</p>
<p>Wei Peng 
Huawei Technologies</p>
<p>Sujian Li lisujian@pku.edu.cn 
School of Computer Science
State Key Laboratory of Multimedia Information Processing
Peking University</p>
<p>Code &amp; Data: https://github.com/WeiminXiong/IPR</p>
<p>Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement
24 Sep 20248CB1563E6353CF7351CBC813AE6D2177arXiv:2406.11176v2[cs.CL]
Large language model agents have exhibited exceptional performance across a range of complex interactive tasks.Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals.In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training.Specifically, we adopt the Monte Carlo method to estimate step-level rewards.During each iteration, the agent explores along the expert trajectory and generates new actions.These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards.Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent.Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines.Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models 1 .</p>
<p>Introduction</p>
<p>The advancements in large language models (LLMs), such as GPT-3.5 (Ouyang et al., 2022), GPT-4 (Achiam et al., 2023), LLaMA (Touvron et al., 2023) have paved ways for LLM-based agents to excel in handling complex interactive tasks, including online shopping (Yao et al., 2022a) and embodied housework (Shridhar et al., 2020).To accomplish these tasks, LLM agents explore the environment step by step, achieving sub-goals along action trajectories (Ma et al., 2024).The efficacy of this task-solving process is pivotal to agent's overall performance.Initial efforts in the task-solving process for agents involve generating trajectories by directly leveraging the planning ability of LLMs, such as ReAct (Yao et al., 2022b) and Reflexion (Shinn et al., 2024).To further enhance LLM agent abilities, several studies focus on trajectory tuning (Chen et al., 2023;Yin et al., 2023;Zeng et al., 2023).Chen et al. (2023) and Yin et al. (2023) construct agent trajectory data from teacher agents (e.g., GPT-4) and fine-tune open-source LLMs for specific agent abilities, such as reasoning.Conversely, Zeng et al. (2023) employ a multi-task supervised fine-tuning (SFT) approach, which does not significantly improve generalized agent capabilities.Observing that the SFT-based works predominantly rely on expert success trajectories (Figure 1(a)), Song et al. (2024) utilize failure trajectories and propose the exploration-based trajectory opti-mization (ETO) method to learn the task-solving process (Figure 1(b)).Although these methods present a promising avenue for enhancing agent capabilities, they treat an entire trajectory as a single entity during training and prioritize the final reward of a trajectory over the process, thus overlooking the potentially exploitable information throughout interaction process.</p>
<p>Regarding agent trajectories, it is well-known that alongside those with correct outcomes, there are trial-and-error paths with detours and erroneous ones that achieve accidental success.</p>
<p>Step-level process supervision can offer granular guidance at each step hence is beneficial for task resolution (Lightman et al., 2023).Nevertheless, the application of step-level optimization to LLM agents encounters two practical challenges.Firstly, the majority of existing LLM agent environments (Yao et al., 2022a;Shridhar et al., 2020;Yang et al., 2024) provide only final outcome feedback.Even in cases where environments offer sub-goal level feedback (Ma et al., 2024), the information is often too sparse.Secondly, the question of how to effectively utilize step rewards to enhance agent training, particularly for tasks with long trajectories and complex action spaces, remains unexplored.</p>
<p>In this paper, we address these challenges by introducing the Iterative step-level Process Refinement (IPR) framework ( § 3) , which encompasses two principal mechanisms: Step-level Reward Acquisition ( § 3.2) and Iterative Agent Optimization ( § 3.3).More specifically, to construct the step reward within the agent environment, we employ Monte Carlo (MC) method to estimate rewards via sampling.The Iterative Agent Optimization component aims to refine the agent's actions through a cyclical process.During each cycle, the agent navigates the expert trajectory and generate new actions.These actions are then compared with the corresponding step of the expert trajectory using step-level rewards to pinpoint errors, resulting in contrastive step pairs.Subsequently, we train the agent using an arrangement of outcome-level direct preference optimization (DPO), step-level DPO, and SFT losses, thereby enhancing the agent's action capabilities at each step (Figure 1(c)).</p>
<p>We assess our IPR framework on three representative benchmarks: online shopping environment WebShop (Yao et al., 2022a), interactive SQL environment InterCodeSQL (Yang et al., 2024) and textual embodied environment ALFWorld (Shridhar et al., 2020).The experimental results, detailed in § 4.2, reveal that our method surpasses the current leading method by margins of 5.8%, 7.2% and 3.2% on WebShop, InterCodeSQL, and ALFWorld, respectively.Moreover, we present a comprehensive analysis to substantiate the efficacy of our method from various perspectives.</p>
<p>In summary, our contributions are as follows:</p>
<p>• We introduce the IPR framework, marking the first integration of step-level process supervision into LLM agent training.This innovation enables fine-grained adjustments of the agent's task completion.</p>
<p>• Our experiments across three complex interactive agent tasks reveal that IPR outperforms established leading baselines.</p>
<p>• Additional analyses indicate that: (1) our IPR enhances the reward per step for the agent, thereby increasing the efficiency of task completion; and</p>
<p>(2) constructing a step reward model automatically is a viable approach to reduce the training costs associated with the MC method.</p>
<p>Task Formulation</p>
<p>The primary scope of this study is the task-solving of LLM agents interacting with the environment and receiving feedback.Following Song et al. (2024), we formulate the task as a partially observable Markov decision process (POMDP) defined by the elements (U, S, A, O, T , R).Here, U denotes the instruction space, S the state space, A the action space, O the observation space, T the transition function (T : S × A → S), and R the reward function (R : S × A → [0, 1]).In the context of our LLM-based agent, U, A, O are subsets of natural language space.</p>
<p>At time step t, the LLM agent π θ receives the observation o t−1 ∈ O from the environment and takes an action a t ∈ A following the policy π θ (•|e t−1 ), where e t−1 = (u, a 1 , o 1 , ..., a t−1 , o t−1 ) represents the historical trajectory.The action leads to a change in the state space s t ∈ S, and receives execution feedback as observation o t ∈ O.The interaction loop continues until the task is completed or the maximum steps are reached.The final trajectory is e n = (u, a 1 , o 1 , ..., a n , o n ), where n denotes the trajectory length, and the outcome reward is r o (u, e n ) ∈ [0, 1].For the convenience of subsequent content, we define e t:n = (a t , o t , ..., a n , o n ) to represent the trajectory after time step t.</p>
<p>Method</p>
<p>The overall architecture of our method is depicted in Figure 2. Initially, we empower the language model with fundamental agent capabilities via supervised learning ( § 3.1).Subsequently, we develop the MC method to estimate the step-wise rewards within the agent's environment ( § 3.2).In the final stage, we enhance the agent's performance through iterative optimization ( § 3.3): by constructing contrastive action pairs and executing mixture trajectory optimization.</p>
<p>Supervised Fine-tuning</p>
<p>To develop an agent with basic task capabilities, we perform supervised fine-tuning (SFT) on an expert trajectory dataset in ReAct-Style (Yao et al., 2022b).We denote this expert trajectory as
D = (u, e) (i) |D| i=1
, where |D| is the number of trajectories.The loss can be computed as:
L SF T (θ) = −E e∼D [log π θ (e|u)].
(1)
Since π θ (e|u) = n t=1 π θ (a t |u, ..., o t−1 ) = n t=1 π θ (a t |e t−1 ) in practice.
The loss function can further be expressed as:
L SF T (θ) = −E e∼D n t=1 log π θ (a t |e t−1 ) . (2)</p>
<p>Step-level Reward Acquisition</p>
<p>Step-level process reward provide precise feedback by pinpointing the exact location of potential errors, offering a valuable signal for agent learning.However, most agent environments are limited to outputting only final outcome reward.Prior studies (Uesato et al., 2022;Lightman et al., 2023) rely on human annotators for step supervision annotations, rendering the acquisition of step rewards a labor-intensive process.To circumvent this, we adopt an exploration-based method to estimate the reward for action a t at step t.</p>
<p>It is intuitive that a more accurate action would contribute to a higher reward.Therefore, we define the step reward r s (s t , a t ) as the anticipated outcome reward from subsequent exploration starting at step t, with s t being the current state of the environment.A dedicated scorer π s with fixed parameters is employed to generate new subsequent trajectory e t:m from step t, based on the historical trajectory e t−1 .The probability of generating e t:m is given by π s (e t:m |e t−1 ), and the environment assigns an outcome reward r o (u, e m ) for the trajectory.The step reward can be calculated as:
r s (s t , a t ) = E em∼πs(et:m|e t−1 ) [r o (u, e m )] (3)
Given the complexity of directly calculating this ex-pectation value, we employ Monte Carlo sampling method for estimation.By sampling N trajectories from step t with π s , we generate a set of trajectories:
{e (i) |i = 1, ..., N } = M C πs (e t−1 ; N ), (4)
The step reward is then calculated as:
r s (s t , a t ) = 1 N N i=1 r o (u, e (i) ), for t &lt; n r o (u, e n ),
for t = n (5) In our approach, the scorer π s is the agent trained via SFT, ensuring its full capability of executing the required task.</p>
<p>Iterative Agent Optimization</p>
<p>Agent tasks typically involve long action sequences and large decision spaces.Suppose we have a base agent π θ trained through SFT.Given an instruction u, the agent interacts with the environment to produce a trajectory e = (u, a 1 , o 1 , ..., a n , o n ).If the agent makes an error action a t at step t, a straightforward approach would be to use reinforcement learning methods like proximal policy optimization (PPO, Schulman et al., 2017) to optimize the action at step t.However, applying online reinforcement learning directly to the LLM agent may cause practical issues such as instability (Shen et al., 2023;Rafailov et al., 2024).To address this issue, we perform offline learning on the contrastive action pairs data instead, which ensures stability.</p>
<p>Step-wise Trajectory Construction To generate contrastive action pairs data, we allow the base agent π θ to explore on the expert trajectory.This approach has two benefits: Firstly, upon identifying an incorrect action by the agent, we can easily acquire a correct action for contrastive learning purposes.Secondly, it prevents arbitrary exploration by the agent, thereby yielding a more informative trajectory.For the task instruction u with expert trajectory e n = (u, a 1 , ..., o n−1 , a n ), we use the first t − 1 steps (u, a 1 , ..., a t−1 , o t−1 ) as historical trajectory e t−1 .The agent then predict the actions from step t to get the trajectory:
e t:m = (â t , ôt , ..., âm , ôm ),(6)
The rewards for a t and ât are r s (s t , a t ) and r s (s t , ât ), respectively.We use a threshold τ to filter actions.If the reward of ât is lower than that of a t by a margin greater than τ , and the outcome reward of êm is lower than that of e n , we consider the agent to have made a mistake at step t.We then contrast the subsequent trajectory from that step e w t:n ≻ e l t:m | e t−1 .Here, e w and e l represent win/lose trajectories with higher and lower rewards.We perform exploration across the entire expert trajectory set and obtain the contrastive action dataset D s = (e t−1 , e w t:n , e l t:m ) (i) |Ds| i=1</p>
<p>. Additionally, we construct a contrastive trajectory dataset
D t = (u, e w n , e l m ) (i) |Dt| i=1
based on the outcome reward.</p>
<p>Mixture Trajectory Optimization During this phase, the agent policy undergoes updates through three loss components: outcome-DPO loss, step-DPO loss, and SFT loss.Initially, to facilitate agent's learning from incorrect trajectories, we compute the outcome-DPO loss using the contrastive trajectory dataset:
L o-DPO = −E (u,e w n ,e l m )∼Dt log σ(β log π θ (e w n |u) π ref (e w n |u) −β log π θ (e l m |u) π ref (e l m |u)
) ,</p>
<p>(7) Next, the step-DPO loss imparts process-level supervision.Suppose the agent makes an error at step t, we have the agent performing a comparison for the subsequent trajectory, which is calculated as:
L s-DPO = −E (et−1,e w t:n ,e l t:m )∼Ds log σ(β log π θ (e w t:n |e t−1 ) π ref (e w t:n |e t−1 ) −β log π θ (e l t:m |e t−1 ) π ref (e l t:m |e t−1 )
) ,</p>
<p>(8) As demonstrated by Yuan et al. (2024), DPO only optimizes the relative differences between chosen and rejected data, neglecting the absolute magnitudes of the rewards.This oversight can be problematic in agent tasks where the space of correct actions is significantly narrower than that of incorrect ones.To mitigate this issue, we add the SFT loss, aiming to directly increase the likelihood of the success trajectory:
L SFT = −E (u,e w n ,e l m )∼Dt log π θ (e w n |u) ,(9)
The final loss combines DPO and SFT losses:
L = L o-DPO + L s-DPO + L SFT(10)
To further refine the agent's performance postoptimization, we employ the updated agent as the new base agent to continue collecting contrastive action pairs data for additional training.This iterative process is maintained until reaching the predetermined iteration limit.</p>
<p>Experiments</p>
<p>Experiment Settings</p>
<p>Datasets We evaluate our method on three representative agent datasets: WebShop (Yao et al., 2022a) for web navigation, InterCodeSQL (Yang et al., 2024) for SQL database querying, and ALF-World for embodied agent tasks.Both WebShop and InterCodeSQL provide a dense reward scale from 0 to 1 to gauge task completion, while ALF-World only provides a binary reward to indicate whether the task is completed.We employ the average reward as the evaluation metric for all tasks.</p>
<p>To collect training expert trajectories, we prompt GPT-4 to interact with the environment in ReAct pattern.We then filter the results based on the final outcome rewards to retain only the correct trajectories.Please refer to Appendix E for more details.The statistical information of the dataset is summarized in Table 1, and more details can be found in Appendix A. Note the ALFWorld test set is divided into 140 seen cases and 134 unseen cases, evaluating the agents' in-domain and out-ofdomain proficiencies, respectively.</p>
<p>Implementation Details We utilize Llama-2-7B (Touvron et al., 2023) as the base model to train LLM agents.The training epoch is 3 and with a batch size of 48.The AdamW optimizer (Loshchilov and Hutter, 2017) is employed, coupled with a cosine learning scheduler.For steplevel rewards acquisition via the scorer, we set the temperature to 1 and the number of samples N to 5, promoting diversity in sampling.In the generation of contrastive action pairs, the base agent's temperature is fixed at 0, while the filtering threshold τ is adjusted to 0.5 for ALFWorld, 0.01 for WebShop and 0.1 for InterCodeSQL.All the generations are carried using vllm (Kwon et al., 2023).During the mixture trajectory optimization phase, we search for the learning rate from 1e-5 to 5e-5, and β for the DPO loss from 0.1 to 0.5.The iteration cap is set to 4. All experiments are conducted on a suite of 8 NVIDIA A100 80G GPUs.</p>
<p>Baselines We evaluate IPR against three types of baselines: prompt-based, outcome refinement, and process refinement methods.For promptbased methods, we compare the efficacy of GPT-4 (Achiam et al., 2023), GPT-3.5-turbo(Ouyang et al., 2022), and the untrained Llama-2-7B-Chat (Touvron et al., 2023) utilizing ReAct prompting paradigm.These baselines are tested in a one-shot context.Regarding outcome refinement methods, four tuning strategies are juxtaposed: (1) SFT (Chen et al., 2023) tunes the agent using solely expert trajectories, which is the base agent of other baselines; (2) PPO (Schulman et al., 2017) is a reinforcement learning (RL) technique that directly optimizes the agents to maximize the outcome reward; (3) RFT (Rejection sampling Fine-Tuning) (Yuan et al., 2023) augments the expert trajectory dataset with successful trajectories, subsequently training the agent on the enriched dataset; and (4) ETO (Song et al., 2024) contrasts success and failure trajectories via DPO (Rafailov et al., 2024).For process refinement methods, we compare the Step-PPO method, which optimizes the agents to maximize the step-level process reward.</p>
<p>Results</p>
<p>Table 2 illustrates that, in comparison to outcome refinement and process refinement methods, both open-source and proprietary models under promptbased methods perform significantly worse.This discrepancy is particularly evident with the untrained Llama-2-7B, which struggles to complete the InterCodeSQL and ALFWorld tasks.However, after training with our IPR method, there is a remarkable increase in the average reward from 5.5 to 69.4, surpassing the best performance of GPT-4.Regarding outcome refinement baselines, our method outperforms the previous state-of-the-art (SOTA) method ETO by margins of 5.8%, 7.2%, 2.5% and 3.2% on WebShop, InterCodeSQL, ALF-World (seen), and AFLWorld (unseen) respectively, with an average improvement of 4.5%.This underscores the superiority of integrating process supervision in enhancing agent performance.As for process refinement baselines, while Step-PPO performs well on InterCodeSQL, surpassing both</p>
<p>Paradigm</p>
<p>Models</p>
<p>WebShop InterCodeSQL ALFWorld Average Seen Unseen Prompt-based GPT-4 (Achiam et al., 2023) 63.2 38.5 42.9 38.1 45.7 GPT-3.5-Turbo(Ouyang et al., 2022) 62.4 37.8 7.9 10.5 29.7 Llama-2-7B (Touvron et al., 2023) 17.9 4.0 0.0 0.0 5.5</p>
<p>Outcome Refinement</p>
<p>Llama-2-7B + SFT (Chen et  prompt-based and outcome refinement baselines, its instability within RL optimization procedures results in poor performance on the other two tasks.</p>
<p>In contrast, IPR significantly enhances agent performance, outperforming all baselines across the three complex interactive agent tasks.We also present case studies to delineat the task-solving trajectories of our method in Appendix D.Moreover, IPR showcases robustness on the ALFWorld unseen task, affirming its generalization capabilities.We have also included an analysis on training efficiency.Please refer to Appendix C for more details.</p>
<p>Analysis</p>
<p>Different Base Models</p>
<p>To further substantiate the efficacy of our method, we conduct validations across a variety of base models.We select Mistral-7B (Jiang et al., 2023a), Llama-2-13B (Touvron et al., 2023) and Llama-3-8B (Meta, 2024) as our base LLMs, employing WebShop and InterCodeSQL as evaluation datasets.</p>
<p>We juxtapose the performance of IPR with that of ETO and SFT.The comparative results are summarized in Table 3. IPR consistently outperforms ETO and SFT across all models and datasets.Notably, on the Mistral model, where SFT performance is relatively poor, our method realizes a significant improvement, demonstrating that our approach can effectively enhance the performance of weaker models.Furthermore, we observe that on the WebShop task, Llama-2-13B achieves the best performance after SFT and maintains its leading position after IPR.Similarly, Llama-3-8B shows superior performance on the InterCodeSQL task.This pattern indicates that base agents with higher initial performance are prone to achieve more pronounced final performance post-IPR training.</p>
<p>Ablation Study</p>
<p>We conduct ablation experiments on the training methods and iteration rounds for IPR.For ALF-World, we evaluate performance on the unseen test set.As shown in Table 4, removing each module results in a clear drop in the agent's performance, underscoring the power of our method.For the ablation on training methods, we discern that the removal of SFT loss engenders the most pronounced performance drop in the agent.Additionally, we find that removing the step-DPO loss induce a more substantial performance decline than that of removing the outcome-DPO loss, suggesting the necessity of process supervision.</p>
<p>The iteration ablation results show that in the initial rounds of iteration, the agent continually refine its performance by learning from incorrect actions.However, excessive iterations can result in a decrease in performance.This decline might be attributed to overfitting, a consequence of excessive exploration of the training set.</p>
<p>Step Reward Estimation Quality</p>
<p>The employment of a scorer agent to estimate process rewards may introduce some noise.To evaluate the accuracy of step rewards, we conduct an experimental analysis on WebShop.In WebShop, each action navigates to a new web page, and scoring rules are established to calculate the final reward for purchasing a product.Ma et al. (2024) heuristically expands the product scoring rules to assign scores at different web pages, thereby scoring each action.This helps us evaluate the quality of two different actions taken from the same state.Please refer to Appendix B for more details.We define accuracy as the ratio of our constructed contrastive action pairs' order that satisfy the scoring function introduced by Ma et al. (2024).We analyze the impact of using different LLM agents as scorers and varying the Monte Carlo sampling times on the accuracy of step reward estimation.</p>
<p>When constructing the contrastive action pairs, we set the reward threshold τ as 0.35 for all base models.</p>
<p>Figure 3 illustrates that, despite inherent noise, the sampling approach yields satisfactory process reward estimations, achieving an accuracy of up to 82% .The accuracy is influenced by the base model's performance on the task.For example, with the same sample count, Llama-2-13B achieves the highest quality in step reward estimation.This suggests that using a more powerful base model (Table 3) can improve the quality of step reward annotations.Additionally, the number of samples affects step reward estimation quality.</p>
<p>Increasing samples can improve scoring accuracy but raise time costs.Despite the efficiency concerns with MC method, we can balance sample size and scoring accuracy.For WebShop, setting the sampling number N = 5 achieves performance comparable to a larger sample size.Without increasing inference time costs, IPR achieves nearly a 6% performance improvement at the expense of three times the ETO training duration.</p>
<p>Average Reward Per Step</p>
<p>The purpose of IPR is to provide process-level supervision to the agent, enabling it to take more accurate actions at each step.Here, we evaluate the changes in the average reward per step after training.The reward for each step is estimated according to the procedure in Section 3.2.We calculate the average rewards for all actions within each trajectory and then average these values across the entire test set.Figure 4 illustrates the significant improvements in average step rewards achieved by our IPR method compared to SFT and ETO across three tasks.It can also be observed that for datasets where SFT training has a higher average step reward, such as InterCodeSQL, the improvement in step reward is even more pronounced.These results underscore the superior performance of IPR, confirming its effectiveness in enhancing the accuracy and efficacy of agent actions.</p>
<p>Exploration of Step Reward Modeling</p>
<p>Based on the step reward data we collected, we conduct further exploration and develop a step re- ward model, which can reduce the training time for new models within that environment.Given the historical trajectory e t−1 and the current action a t , the reward model outputs a score as the step reward.We conduct experiments on Web-Shop, using Llama-2-7B to build the reward model.We collect 70k actions generated by Llama-2-7B and Llama-2-13B as training data, with the step rewards estimated using the MC method.We train the reward model with MSE loss.To evaluate the effectiveness of the reward model, we replace the scorer in Section 3.2 with the reward model and compare the results against ETO (which does not use step rewards) and the MC method.As shown in Table 5, the reward model can enhance the performance of Llama-3-8B, even though its actions are not included in the training data.This indicates the generalization and robustness of the reward model.However, despite outperforming ETO, the results still fall short of the MC method.This may be attributed to the model's less accurate estimation of step rewards within the environment, suggesting the need for further improvement.</p>
<p>6 Related Work</p>
<p>LLM as Agents</p>
<p>The emerging reasoning and instruction-following capabilities of LLMs (Wei et al., 2022) enable them to act as adept agents, particularly in zero-shot generalization across new tasks and problems (Yao et al., 2022b;Richards, 2023;Wang et al., 2023a).</p>
<p>The key technique involves formulating prompts that furnish LLMs with instructions and context about the environment, thereby enabling them to generate executable actions and leverage external tools for complex task-solving (Song et al., 2023;Xie et al., 2023).To enhance the capabilities of open-source LLMs as agents, recent efforts have adopted fine-tuning methods (Chen et al., 2023;Zeng et al., 2023;Yin et al., 2023).These methods enables agent learn from successful trajectories or utilize contrastive information with failed trajectories (Song et al., 2024).However, these approaches only leverage final outcome reward, with no studies to date investigating the integration of process information to improve agent performance.</p>
<p>Step-level Process Supervision</p>
<p>In the resolution of complex tasks, even SOTA models may still make mistakes at intermediate steps.To monitor the task completion process and avoid such errors, some approaches (Uesato et al., 2022;Lightman et al., 2023) employ process-based methods which can provide step-level guidance.To avoid the high cost of manually collecting process supervision, recent works (Liu et al., 2023;Wang et al., 2023b;Havrilla et al., 2024;Wang et al., 2024) construct pseudo-labels, using the model's potential to complete the task given the previous steps as process labels.These methods (Ma et al., 2023;Luong et al., 2024) use PPO to optimize the model but suffer from training efficiency and instability issues.Our approach, designed with mixture trajectory optimization, effectively enhances the agent's performance.</p>
<p>Self-Improvement</p>
<p>To compensate for the scarcity of high-quality training data (Tao et al., 2024), self-improvement methods empower the model to autonomously acquire, refine, and learn from self-generated experiences.</p>
<p>Certain works (Jiang et al., 2023b;Singh et al., 2023;Zelikman et al., 2023;Chen et al., 2024) focus on alignment, refining the model by discerning these self-generated responses from those obtained from human-annotated data.Others concentrate on LLM agents utilized for task-solving and interaction in dynamic environments.They enhance the agent's capabilities in planning (Qiao et al., 2024), tool using (Bousmalis et al., 2023;Zhu et al., 2024), and communication (Ulmer et al., 2024).These endeavors demonstrate that models can refine themselves through exploration in diverse domains.Our work aims to amplify this self-improvement process by providing fine-grained guidance.</p>
<p>Conclusion</p>
<p>In this paper, we present IPR, a novel framework designed to elevate the capabilties of LLM agents in complex interaction tasks.Our approach integrates process-level supervision, enabling agents to learn from contrast action pairs.To provide finegrained guidance in environments where only outcome rewards are available, we use the MC method to automatically calculate step rewards.By employing iterative agent optimization, IPR provides an effective way to optimize agent decision-making trajectories.Experiments on three benchmarks demonstrate that our framework consistently outperforms existing baselines.Subsequent analyses validate the efficacy of each part of the framework and action efficiency.We believe the IPR framework can serve as a potent tool for enhancing agent performance at the action level, thereby catalyzing future progress in intelligent agent development.</p>
<p>Limitations</p>
<p>Despite achieving the best performance compared to other baselines, it is important to acknowledge several limitations of this work.1) Our method provides fine-grained supervision for the agent's self-improvement process.However due to limited training data, which is a quite common scenario, iterative preference learning on self-generated samples can lead to overfitting.Future work could explore the augmentation of training tasks using GPT-4 to mitigate this issue.2) Our method only explores identifying error actions and creating contrastive datasets through step rewards.However, it does not fully exploit the potential of these rewards.</p>
<p>The numerical values of step rewards could indicate the severity of errors at each step.For instance, adopting the curriculum learning approach (Wang et al., 2021), where more severe errors are corrected first before addressing less significant ones, might further enhance agent performance.3) Our step reward model is only trained on a single agent task, which affects its generalizability across different tasks.Future work could develop a general agent step reward model applicable to various tasks.</p>
<p>A Dataset Details</p>
<p>WebShop WebShop (Yao et al., 2022a) is a network-based simulation environment for ecommerce experiences, features a website with 1.8 million actual products, each with distinct labels and attributes.In this environment, the agent is allowed to interact with the system through "search[QUERY]" or "click[ELEMENT]" actions to purchase products matching the instructions.</p>
<p>Once the agent clicks the "buy" option, the environment provides a final reward, which is calculated based on the matching heuristics of the product's attributes and price.</p>
<p>InterCodeSQL InterCodeSQL is an interactive database environment within InterCode benchmark (Yang et al., 2024), where the agent interacts with the environment to retrieve necessary table information and complete the corresponding SQL queries.The database is constructed from the Spider (Yu et al., 2018) dataset, a large-scale cross-domain dataset originally designed for evaluating SQL query generation from natural language questions.We have modified InterCodeSQL to fit for our evaluation framework.When the agent perform the "submit" action, the environment provides a final reward.The reward is calculated using the Intersection over Union (IoU) metric to quantify the correctness of the submitted execution output generated by the against the gold output, with both outputs being lists of records.</p>
<p>ALFWorld ALFWorld (Shridhar et al., 2020) are household tasks that require agents to explore rooms and use commonsense reasoning to perform tasks, such as "put a pencil on the desk".The environment provides the outcome on whether the agent successfully completes the task within given steps.The original ALFWorld dataset comprises both seen and unseen evaluation sets.The seen set is designed to assess in-distribution generalization, whereas the unseen set with new task instances measures out-of-distribution generalization of the agents.</p>
<p>B Details of the Scoring Function</p>
<p>In the WebShop environment, Yao et al. (2022a) provides the scoring formula to calculate the score of any product (the distance from the target prod-uct) as follows:
f = f type • |Uatt∩Yatt|+|Uopt∩Yopt|+1[y price ≤u price ] |Uatt|+|Uopt|+1 ,(11)
where f type = TextMatch(y, y * ).Following Ma et al. (2024), we expand the product scoring rules to derive the score for each action.Typically, completing a web shopping task involves three continuous states: search, product selection, and finalizing the product style before placing an order.Each action leads to deterministic state change in the environment.Therefore, to calculate the step reward, we measure the distance between the result state and the target state.We primarily calculate scores for three pages (states): search result page, product description page, and order confirmation page.On the search result page, we calculate the score of each product on the page and take the highest score for this page.On the product description page, we compute the highest score for the product under various options as the page score.On the order confirmation page, the score of the finally selected product is considered as the score for that page.</p>
<p>C Training Efficiency Analysis</p>
<p>Here, we compare the time consumption of different methods on WebShop in Figure 1.Since our method can achieve state-of-the-art performance after three rounds of iteration, we use the time for three rounds of iteration as the measure of training time.The time consumption results are as follows: SFT requires 1 hour, ETO requires 2.5 hours, and IPR requires 5.3 hours.Furthermore, although the Monte Carlo method necessitates sampling to obtain the process information of step rewards, with the support of vllm (Kwon et al., 2023), we have indeed been able to construct the step rewards in an efficient and parallel manner.Without increasing inference time costs, IPR achieves nearly a 6% performance improvement at the expense of a training duration less than three times that of ETO.We believe that this time cost is acceptable.</p>
<p>D Case Study</p>
<p>Here, we provide a detailed comparison of the trajectories of IPR and ETO on the same tasks within WebShop and ALFWorld, which illustrates how IPR provides fine-grained guidance to correct erroneous actions and improves the efficacy of agent actions.The cases are shown in Figure 5 and 6.</p>
<p>In the WebShop scenario, ETO learned the process of selecting products and choosing their at-tributes.However, when selecting a product, it overlooked the requirement for product type and directly purchased the first item on the search page, a dress shirt, ignoring the request for a tank top.In contrast, our IPR, benefiting from fine-grained process supervision, corrected this error and chose the correct tank top product.</p>
<p>In the ALFWorld scenario, ETO engaged in blind exploration to find the desired item potato, which introduced a lot of irrelevant information and unnecessary context.In contrast, our IPR improved the efficiency of each action, directly navigating to the sinkbasin to locate the potato.Moreover, ETO made a mistake in the details by cooling the potato down without picking it up, whereas IPR rectified this action.</p>
<p>Case Study of WebShop</p>
<p>ETO</p>
<p>Task Instruction: I want to find a small purple bike tank top for men that has a classic fit, and price lower than 50.00 dollars Thought: I need to search for a small purple bike tank top for men that has a classic fit and is priced under $50.</p>
<p>Case Study of ALFWorld</p>
<p>ETO</p>
<p>Task Instruction: You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to: cool some potato and put it in microwave.# Actions of blind exploration Thought: I need to cool a potato before putting it in the microwave.The fridge is the best place to cool food items quickly.Action: go to fridge 1 Observation: The fridge 1 is closed.Thought: I need to cool the potato, but I can't use the fridge since I closed it.I should find another way to cool it down quickly.Action: go to sinkbasin 1 Observation: On the sinkbasin 1, you see a bowl 3, a butterknife 2, a egg 2, a fork 2, a knife 1, and a potato 2. # Making a mistake in the details by cooling down without picking up the potato Thought: I'm at the sink basin, which is a common place to cool food items by placing them in cold water.I can use this method to cool the potato.Action: cool potato 2 with sinkbasin 1 Observation: Nothing happens.Thought: Now that the potato is cooled, I need to put it in the microwave.I should go to the microwave to proceed with the task.</p>
<p>IPR</p>
<p>Task Instruction: You are in the middle of a room.Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.Your task is to: cool some potato and put it in microwave.Thought: I need to cool a potato and then put it in the microwave.First, I should locate the potato.Action: go to sinkbasin 1 Observation: On the sinkbasin 1, you see a bowl 3, a butterknife 2, a egg 2, a fork 2, a knife 1, and a potato 2. Thought: There's a potato here on the sink basin.I should pick it up to proceed with the task.Action: take potato 2 from sinkbasin 1 Observation: You pick up the potato 2 from the sinkbasin 1.</p>
<p>E Expert Trajectories Collection</p>
<p>We primarily us the expert trajectories collected by Song et al. (2024) in ReAct pattern.For Inter-CodeSQL tasks not covered by these trajectories, we conducted our annotations.</p>
<p>• WebShop (Yao et al., 2022a).In addition to manually annotated trajectories provided by the WebShop, GPT-4 is employed to annotate additional trajectories.The trajectories with final rewards exceeding 0.7 are reserved.</p>
<p>• InterCodeSQL (Yang et al., 2024).We annotate expert trajectories using GPT-4 and retain trajectories with a reward of 1.0.</p>
<p>• ALFWorld (Shridhar et al., 2020).The dataset provides human-annotated trajectories.</p>
<p>As the original trajectories lack the thoughts for each action step, we have employed GPT-4 to generate the corresponding information.</p>
<p>F Prompt for Evaluation</p>
<p>We show the instruction prompts for WebShop, In-terCodeSQL, ALFWorld in Figure 7, 8, 9, respectively.</p>
<p>Instruction Prompt for InterCodeSQL</p>
<p>You are a helpful assistant assigned with the task of problem-solving.To achieve this, you will interact with a MySQL Database system using SQL queries to answer a question.</p>
<p>At each turn, you should first provide your step-by-step thinking for solving the task.Your thought process should start with "Thought: ", for example: Thought: I should write a SQL query that gets the average GNP and total population from nations whose government is US territory.</p>
<p>After that, you have two options: 1) Interact with a mysql programming environment and receive the corresponding output.Your code should start with "Action: " , for example: Action: SELECT AVG(GNP), SUM(population) FROM nations WHERE government = 'US Territory' 2) Directly submit the result, for example: Action: submit.</p>
<p>You should use this format: Thought: your thought Action: <the mysql command>.</p>
<p>You will receive the corresponding output for your sql command.Your output should contain only one "Action" part.The "Action" part should be executed with a mysql interpreter or propose an answer.Any natural language in it should be commented out.The SQL query and submit parts can not appear in your output simultaneously.</p>
<p>Instruction Prompt for ALFWorld</p>
<p>Interact with a household to solve a task.Imagine you are an intelligent agent in a household environment and your target is to perform actions to complete the task goal.At the beginning of your interactions, you will be given a detailed description of the current environment and your goal to accomplish.</p>
<p>For each of your turn, you will be given the observation of the last turn.You should first think about the current condition and plan for your future actions, and then output your action in this turn.Your output must strictly follow this format:"Thought: your thoughts.Action: your next action".</p>
<p>Figure 1 :
1
Figure 1: Comparison of three different agent training paradigms.Green and red circles represent correct and incorrect actions, while check and cross marks indicate the final outcome.Compared to the other methods, IPR can provide step-level process supervision.</p>
<p>Figure 2 :
2
Figure 2: The overall architecture of IPR in a single iteration.The agent trained after SFT first explores new actions along the expert trajectory.Then we use the scorer to reward each step and construct contrastive action data.Finally we optimize the agent with a mixed loss.</p>
<p>Figure 4 :
4
Figure 4: The average reward per step.</p>
<p>Figure 5 :
5
Figure 5: Case study for WebShop.</p>
<p>Thought: The fridge is closed.I need to open it to access the cooling space inside.Action: open fridge 1 Observation: You open the fridge 1.The fridge 1 is open.In it, you see a bowl 1, a cup 1, a lettuce 1, a plate 2, and a tomato 1. Thought: I've opened the fridge, but I realize I don't need to put the potato inside since I can cool it while holding it.I should close the fridge to conserve energy.Action: close fridge 1 Observation: You close the fridge 1.</p>
<p>Figure 6 :
6
Figure 6: Case study for ALFWorld.</p>
<p>Figure 7 :
7
Figure 7: Instruction prompt for WebShop.</p>
<p>Figure 8 :
8
Figure 8: Instruction prompt for InterCodeSQL.</p>
<p>Figure 9 :
9
Figure 9: Instruction prompt for ALFWorld.</p>
<p>Table 1 :
1
Statistics overview of tested datasets."MaxTurns"refers to the maximum number of interactions in the expert trajectory.
DatasetTrain Test Action Space Max TurnsWebShop1624 200810ALFWorld2851 2741320InterCodeSQL 1500 200∞ (SQL)10</p>
<p>Table 2 :
2
Performance of different methods on three agent datasets.IPR shows superiority over prompt-based and outcome refinement methods.For ETO and IPR, we report the best performance across all iterations.
al., 2023)60.254.960.067.260.6Llama-2-7B + PPO (Schulman et al., 2017)64.252.422.129.142.0Llama-2-7B + RFT (Yuan et al., 2023)63.656.362.966.462.3Llama-2-7B + ETO (Song et al., 2024)67.457.268.672.466.4Process RefinementLlama-2-7B + Step-PPO Llama-2-7B + IPR (ours)64.0 71.360.2 61.365.7 70.369.4 74.764.8 69.4</p>
<p>Table 3 :
3
The performance of different base LLMs on WebShop and InterCodeSQL.
Base LLMSetting WebShop InterCodeSQLSFT58.550.0Mistral-7BETO66.254.3IPR69.658.9SFT62.259.3Llama-2-13BETO68.961.5IPR72.264.5SFT61.263.4Llama-3-8BETO66.265.8IPR72.068.1</p>
<p>Table 5 :
5
The performance of different step reward acquisition methods.
ModelsNo Reward Reward Model MC MethodLlama-2-7B67.468.971.3Llama-2-13B68.970.772.2Llama-3-8B66.270.672.0</p>
<p>The search results are displayed.The first item, B09PBY1KJG, is a men's vertical striped button-down dress shirt that fits the criteria.I should click on it to see more details and confirm if it's the right size and price.
IPRTask Instruction: I want to find a small purplebike tank top for men that has a classic fit, andprice lower than 50.00 dollars.Thought: I think I should use the search bar tolook for the product I need.Action: search[small purple bike tank top menAction: search[small purple bike tank top menclassic fit]classic fit]Observation: Page 1 (Total results: 50) [SEP]Observation: Page 1 (Total results: 50) [SEP]Next &gt; [SEP] B09PBY1KJG [SEP] Men's Ver-Next &gt; [SEP] B09PBY1KJG [SEP] Men's Ver-tical Striped Button Down Dress Shirt Longtical Striped Button Down Dress Shirt LongSleeve Casual Comfort Regular Fit BusinessSleeve Casual Comfort Regular Fit BusinessOxford Shirts [SEP] $1.97 to $22.97 ... [SEP]Oxford Shirts [SEP] $1.97 to $22.97 ... [SEP]B08W4FVVXC [SEP] Just Send It MotocrossB08W4FVVXC [SEP] Just Send It MotocrossCool Motorcycle Dirt Bike Biker Gift Tank TopCool Motorcycle Dirt Bike Biker Gift Tank Top[SEP] $19.9 ...[SEP] $19.9 ...Thought: I think I should click on the# Overlooking important requirementsproduct that matches my criteria, which isThought: Action: click[B09PBY1KJG]B08W4FVVXC. Action: click[B08W4FVVXC]Observation: <product details>Thought: The shirt is available in purple,which is the color I want. I should select thiscolor option.Action: click[purple]Observation: <product details>Thought: Now I need to ensure the shirt isavailable in small size, as that's the size I'mlooking for.Action: click[small]Observation: <product details>Thought: The shirt is purple, small-sized, andwithin my budget. It meets all the criteria I waslooking for. I should go ahead and purchase thisitem.Action: click[Buy Now]Reward: 0.0</p>
<p>Instruction Prompt for WebShop You are doing a web shopping task.I will give you instructions about what to do.You have to follow the instructions.Every round I will give you an observation and a list of available actions, you have to respond to an action based on the state and instruction.You can use search action if search is available.You can click one of the buttons in clickables.An action should be one of the following structure: search[keywords] or click[value]If the action is not valid, perform nothing.Keywords in search are up to you, but the value in click must be a value in the list of available actions.Remember that your keywords in search should be carefully designed.
Your response should use the following format:Thought: I think ...Action: click[something]</p>
<p>recep correspond to objects and receptacles.After each turn, the environment will give you immediate feedback based on which you plan your next few steps.if the environment outputs "Nothing happened", that means the previous action is invalid and you should try more options.
The available actions are:1. go to recep2. task obj from recep3. put obj in/on recep4. open recep5. close recep6. toggle obj recep7. clean obj with recep8. heat obj with recep9. cool obj with recepwhere obj and Your response should use the following format:Thought: <your thoughts>Action: <your next action>
AcknowledgementWe thank the anonymous reviewers for their helpful comments on this paper.This work was partially supported by National Natural Science Foundation of China (No. 62476010).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Robocat: A self-improving generalist agent for robotic manipulation. Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Manon Devin, Alex X Lee, Maria Bauza Villalonga, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Transactions on Machine Learning Research. 2023</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, arXiv:2310.05915Fireact: Toward language agent fine-tuning. 2023arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, arXiv:2401.013352024arXiv preprint</p>
<p>Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Railneau, arXiv:2402.10963Glore: When, where, and how to improve llm reasoning via global and local refinements. 2024arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023aarXiv preprint</p>
<p>Shuyang Jiang, Yuhao Wang, Yu Wang, arXiv:2306.02907Selfevolve: A code evolution framework via large language models. 2023barXiv preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>Don't throw away your value model! making ppo even better via value-guided monte-carlo tree search decoding. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz, 20232309arXiv e-prints</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li, arXiv:2401.08967Reft: Reasoning with reinforced fine-tuning. 2024arXiv preprint</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.131782024arXiv preprint</p>
<p>Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang, arXiv:2310.10080Let's reward step by step: Step-level reward model as the navigators for reasoning. 2023arXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Ai Meta, Meta AI. 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, arXiv:2401.052682024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>Significantgravitas/autogpt: An experimental open-source attempt to make gpt-4 fully autonomous. Toran Bruce, Richards , arXiv:1707.06347Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2023. 2017arXiv preprint</p>
<p>Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong, arXiv:2309.15025Large language model alignment: A survey. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020arXiv preprint</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, arXiv:2312.065852023arXiv preprint</p>
<p>Restgpt: Connecting large language models with realworld applications via restful apis. Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, Sujian Li, arXiv:2306.06624arXiv:2403.02502Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization for llm agents. 2023arXiv preprint</p>
<p>Zhengwei Tao, Xiancai Ting-En Lin, Hangyu Chen, Yuchuan Li, Yongbin Wu, Zhi Li, Fei Jin, Dacheng Huang, Jingren Tao, Zhou, arXiv:2404.14387A survey on self-evolution of large language models. 2024arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Bootstrapping llm-based task-oriented dialogue agents via self-talk. Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, Yi Zhang, arXiv:2401.050332024arXiv preprint</p>
<p>Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291arXiv preprint</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023barXiv preprint</p>
<p>A survey on curriculum learning. IEEE transactions on pattern analysis and machine intelligence. Xin Wang, Yudong Chen, Wenwu Zhu, 202144</p>
<p>Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision. Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, Jingbo Shang, arXiv:2402.026582024arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Jing Toh, Junning Hua, Qian Zhao, Che Liu, Liu, arXiv:2310.10634Openagents: An open platform for language agents in the wild. 2023arXiv preprint</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022barXiv preprint</p>
<p>Lumos: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, arXiv:2311.056572023arXiv preprint</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, arXiv:1809.088872018arXiv preprint</p>
<p>Advancing llm reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, arXiv:2404.020782024arXiv preprint</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Self-taught optimizer (stop): Recursively self-improving code generation. Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman, Kalai , arXiv:2310.023042023arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, arXiv:2403.03101Knowagent: Knowledge-augmented planning for llm-based agents. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
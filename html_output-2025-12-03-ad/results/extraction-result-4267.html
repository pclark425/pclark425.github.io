<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4267 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4267</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4267</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-271710790</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.01556v1.pdf" target="_blank">pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy</a></p>
                <p><strong>Paper Abstract:</strong> The exponential growth of astronomical literature poses significant challenges for researchers navigating and synthesizing general insights or even domain-specific knowledge. We present pathfinder, a machine learning framework designed to enable literature review and knowledge discovery in astronomy, focusing on semantic searching with natural language instead of syntactic searches with keywords. Utilizing state-of-the-art large language models (LLMs) and a corpus of 385,166 peer-reviewed papers from the Astrophysics Data System, pathfinder offers an innovative approach to scientific inquiry and literature exploration. Our framework couples advanced retrieval techniques with LLM-based synthesis to search astronomical literature by semantic context as a complement to currently existing methods that use keywords or citation graphs. It addresses complexities of jargon, named entities, and temporal aspects through time-based and citation-based weighting schemes. We demonstrate the tool’s versatility through case studies, showcasing its application in various research scenarios. The system’s performance is evaluated using custom benchmarks, including single-paper and multipaper tasks. Beyond literature review, pathfinder offers unique capabilities for reformatting answers in ways that are accessible to various audiences (e.g., in a different language or as simplified text), visualizing research landscapes, and tracking the impact of observatories and methodologies. This tool represents a significant advancement in applying artificial intelligence to astronomical research, aiding researchers at all career stages in navigating modern astronomy literature.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4267.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4267.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pathfinder RAG/ReAct/HyDE pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pathfinder: Retrieval-Augmented Generation (RAG) pipeline with HyDE query expansion and ReAct agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end literature-review framework that grounds LLM outputs in a curated corpus of ~352k astronomy abstracts by combining dense embeddings, HyDE query expansion, FAISS similarity search, cross-encoder reranking, RAG generation, and ReAct agentic multi-step reasoning; it is used to synthesize consensus statements, survey-style summaries, trends, and hypotheses from the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI models (text-embedding-3-small for embeddings; GPT-family LLMs including GPT-4/GPT-4o-mini for generation/consensus), plus Cohere rerank for cross-encoder reranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RAG + HyDE + ReAct (with embedding-based retrieval, FAISS, and cross-encoder reranking)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline details as implemented in this paper: (1) Compute 1536-d embeddings for each abstract using OpenAI text-embedding-3-small; (2) perform dense semantic retrieval (cosine similarity via FAISS) optionally weighted by keywords, time, and citation counts; (3) apply HyDE (Hypothetical Document Embeddings) by prompting an LLM to generate an expert-style abstract for the query to expand the query representation; (4) rerank an initial top-k using a cross-encoder (Cohere rerank-english-v3.0); (5) feed the top-k retrieved abstract chunks into a RAG prompt template that requires source grounding and to answer 'I don't know' when evidence is absent; (6) for complex multi-step tasks, use ReAct agents where the LLM iteratively reasons and issues retrieval actions until an answer is formed; (7) perform outlier detection in the high-dimensional embedding space and a consensus evaluation step (using GPT-4o mini) to rate agreement strength among retrieved papers and between the literature and the query. The system also supports prompt specializations (single-paper, multi-paper, bibliometric, critique-and-improve) and interactive toggles for top-k, weighting, and retrieval options.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>≈352,194 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>astronomy / astrophysics</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>survey-style syntheses, consensus statements, empirical generalizations and hypotheses (qualitative patterns/trends across literature), domain terminology mappings, and speculative counterfactual narratives (i.e., proposed mechanisms or consequences derived from synthesis of multiple papers)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Examples shown in the paper (generated by the system): (1) Consensus-style summary of the Hubble tension describing differing measurement sets and ongoing debate; (2) Terminology mapping: 'Kriging is often referred to as Gaussian process regression in astronomical applications'; (3) Counterfactual hypothesis: 'A universe without dark matter would lead to less efficient galaxy formation, altered dynamics, and different large-scale structure' — these are illustrative of the kinds of qualitative laws/principles/pathways the system synthesizes from abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>A mix of synthetic and human-assisted benchmarks: (a) Single-paper synthetic benchmark (generate queries from sentences of 500 random papers and test retrieval consistency); (b) Multi-paper synthetic benchmark derived from review-paper sentences with citation clusters; (c) Gold human-curated Q&A dataset from a Slackbot user study where expert astronomers provided reference answers; plus user-interaction analysis (correlation between retrieval scores and user thumbs-up).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-paper (HyDE + rerank): success rate s = 0.84 (source paper in top-10), mean reciprocal rank r^{-1} = 0.74; Bag-of-Words baseline: s = 0.46, r^{-1} = 0.29. Multi-paper (top-k=100): HyDE+rerank recall = 0.29, nDCG = 0.19; Bag-of-Words baseline recall = 0.15, nDCG = 0.09. User-interaction correlation: Spearman ρ = +0.33 (p < 10^{-6}) between positive user feedback and higher retrieval scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to simpler baselines: Bag-of-Words / TF-IDF and plain semantic search (embeddings cosine similarity). HyDE + rerank outperformed HyDE alone, base semantic search, and Bag-of-Words; two-stage retrieval (dense retrieval + cross-encoder rerank) yielded best results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining LLM generation (HyDE) with dense retrieval and cross-encoder reranking substantially improves retrieval and downstream synthesis quality versus simple baselines; RAG grounding and requiring source citation reduces hallucination risk and produces more trustworthy syntheses; ReAct agents enable multi-step compositional and consensus tasks but can stall in loops; embedding-space outlier detection plus an LLM-driven consensus evaluator provides useful signals about agreement/controversy in the literature; abstracts-only corpus limits depth but suffices for many survey-style qualitative syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Key limitations identified: reliance on abstracts (full-text absent) reduces ability to extract methodological details or buried results; hallucination risk remains despite RAG (models can still misattribute or overgeneralize); LLMs can recapitulate existing knowledge rather than propose grounded novelty; corpus incompleteness and biases (missing niche journals, recency gaps) can skew synthesized 'laws'; scalability/interpretability issues (embedding choices, clustering sensitivity); ReAct agents may loop without new strategies and require improved planning/search trees or knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4267.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4267.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-assisted hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for hypothesis generation and scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in the paper as an emerging use of LLMs: generating hypotheses or proposed qualitative mechanisms from literature, and (in some recent works) proposing equations or experiment ideas, though such outputs often lack grounding and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (unnamed in this paper; cited work includes generic LLM-based methods and LLM-SR style systems)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based hypothesis generation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described at a high level: LLMs are prompted (sometimes with task-specific scaffolds) to propose hypotheses, tentative mechanisms, or experiment designs by synthesizing across papers; some approaches couple LLMs with programming-based solvers (e.g., LLM-SR) to propose equations or analytic forms. The paper notes this may be attempted either directly (generate hypotheses from corpus) or via agentic frameworks that iteratively retrieve and reason.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>astronomy (discussion), but the technique is cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>hypotheses, proposed mechanisms, speculative empirical generalizations, and candidate guiding principles for follow-up research</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in detail in this paper; the authors note prior work but caution that generated hypotheses often lack grounding and may recapitulate existing knowledge rather than provide validated novel laws.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Potential for accelerating discovery exists but is largely untapped; current LLM-generated hypotheses frequently lack grounding and novelty and therefore require human vetting and further validation. The authors position pathfinder as infrastructure to better ground such generation with a curated corpus and robust retrieval methods.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Main issues: lack of grounding in primary data or experimental evidence, tendency to rephrase known results rather than invent validated novel laws, risk of hallucination, and insufficient evaluation protocols to verify novelty or correctness of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4267.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4267.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for literature review (prior work mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Applications of LLMs to perform literature review and summarization (cited prior work: ChatCite, Astarita et al., Li et al., Tao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites growing academic use of LLMs for explaining concepts, performing literature reviews and comparative literature summaries (e.g., ChatCite), and also notes controversies around accuracy, bias and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs referenced in cited literature (e.g., GPT-family models and other academic LLMs referenced indirectly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted literature review and summarization (e.g., ChatCite-style agents)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned use-cases include prompting LLMs to synthesize summaries across retrieved papers, guide comparative literature synthesis with human workflow guidance (ChatCite-style), and assist drafting review articles; these methods often combine retrieval (RAG-like) and LLM summarization, but specifics vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>academic literature broadly and astronomy specifically (as referenced in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>survey summaries, comparative syntheses, consensus findings, terminology mappings and review-style principles (qualitative outputs rather than formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper for the cited prior work; the pathfinder paper itself uses synthetic benchmarks and human-curated Gold QA to evaluate retrieval/summarization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The literature shows LLMs are increasingly applied to literature review tasks but face issues around factual accuracy, bias, and potential plagiarism; integrating retrieval grounding (RAG) and curated corpora can mitigate some risks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Accuracy concerns (hallucinations), bias inherited from training data, plagiarism/attribution issues, and the need for careful human oversight and evaluation when using LLMs for scholarly synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis Generation with Large Language Models <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models <em>(Rating: 2)</em></li>
                <li>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary <em>(Rating: 2)</em></li>
                <li>Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy <em>(Rating: 1)</em></li>
                <li>Machine Learning for Astrophysics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4267",
    "paper_id": "paper-271710790",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "pathfinder RAG/ReAct/HyDE pipeline",
            "name_full": "pathfinder: Retrieval-Augmented Generation (RAG) pipeline with HyDE query expansion and ReAct agents",
            "brief_description": "An end-to-end literature-review framework that grounds LLM outputs in a curated corpus of ~352k astronomy abstracts by combining dense embeddings, HyDE query expansion, FAISS similarity search, cross-encoder reranking, RAG generation, and ReAct agentic multi-step reasoning; it is used to synthesize consensus statements, survey-style summaries, trends, and hypotheses from the literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI models (text-embedding-3-small for embeddings; GPT-family LLMs including GPT-4/GPT-4o-mini for generation/consensus), plus Cohere rerank for cross-encoder reranking",
            "model_size": null,
            "method_name": "RAG + HyDE + ReAct (with embedding-based retrieval, FAISS, and cross-encoder reranking)",
            "method_description": "Pipeline details as implemented in this paper: (1) Compute 1536-d embeddings for each abstract using OpenAI text-embedding-3-small; (2) perform dense semantic retrieval (cosine similarity via FAISS) optionally weighted by keywords, time, and citation counts; (3) apply HyDE (Hypothetical Document Embeddings) by prompting an LLM to generate an expert-style abstract for the query to expand the query representation; (4) rerank an initial top-k using a cross-encoder (Cohere rerank-english-v3.0); (5) feed the top-k retrieved abstract chunks into a RAG prompt template that requires source grounding and to answer 'I don't know' when evidence is absent; (6) for complex multi-step tasks, use ReAct agents where the LLM iteratively reasons and issues retrieval actions until an answer is formed; (7) perform outlier detection in the high-dimensional embedding space and a consensus evaluation step (using GPT-4o mini) to rate agreement strength among retrieved papers and between the literature and the query. The system also supports prompt specializations (single-paper, multi-paper, bibliometric, critique-and-improve) and interactive toggles for top-k, weighting, and retrieval options.",
            "number_of_papers": "≈352,194 papers",
            "domain_or_field": "astronomy / astrophysics",
            "type_of_laws_extracted": "survey-style syntheses, consensus statements, empirical generalizations and hypotheses (qualitative patterns/trends across literature), domain terminology mappings, and speculative counterfactual narratives (i.e., proposed mechanisms or consequences derived from synthesis of multiple papers)",
            "example_laws_extracted": "Examples shown in the paper (generated by the system): (1) Consensus-style summary of the Hubble tension describing differing measurement sets and ongoing debate; (2) Terminology mapping: 'Kriging is often referred to as Gaussian process regression in astronomical applications'; (3) Counterfactual hypothesis: 'A universe without dark matter would lead to less efficient galaxy formation, altered dynamics, and different large-scale structure' — these are illustrative of the kinds of qualitative laws/principles/pathways the system synthesizes from abstracts.",
            "evaluation_method": "A mix of synthetic and human-assisted benchmarks: (a) Single-paper synthetic benchmark (generate queries from sentences of 500 random papers and test retrieval consistency); (b) Multi-paper synthetic benchmark derived from review-paper sentences with citation clusters; (c) Gold human-curated Q&A dataset from a Slackbot user study where expert astronomers provided reference answers; plus user-interaction analysis (correlation between retrieval scores and user thumbs-up).",
            "performance_metrics": "Single-paper (HyDE + rerank): success rate s = 0.84 (source paper in top-10), mean reciprocal rank r^{-1} = 0.74; Bag-of-Words baseline: s = 0.46, r^{-1} = 0.29. Multi-paper (top-k=100): HyDE+rerank recall = 0.29, nDCG = 0.19; Bag-of-Words baseline recall = 0.15, nDCG = 0.09. User-interaction correlation: Spearman ρ = +0.33 (p &lt; 10^{-6}) between positive user feedback and higher retrieval scores.",
            "comparison_baseline": "Compared to simpler baselines: Bag-of-Words / TF-IDF and plain semantic search (embeddings cosine similarity). HyDE + rerank outperformed HyDE alone, base semantic search, and Bag-of-Words; two-stage retrieval (dense retrieval + cross-encoder rerank) yielded best results.",
            "key_findings": "Combining LLM generation (HyDE) with dense retrieval and cross-encoder reranking substantially improves retrieval and downstream synthesis quality versus simple baselines; RAG grounding and requiring source citation reduces hallucination risk and produces more trustworthy syntheses; ReAct agents enable multi-step compositional and consensus tasks but can stall in loops; embedding-space outlier detection plus an LLM-driven consensus evaluator provides useful signals about agreement/controversy in the literature; abstracts-only corpus limits depth but suffices for many survey-style qualitative syntheses.",
            "challenges_limitations": "Key limitations identified: reliance on abstracts (full-text absent) reduces ability to extract methodological details or buried results; hallucination risk remains despite RAG (models can still misattribute or overgeneralize); LLMs can recapitulate existing knowledge rather than propose grounded novelty; corpus incompleteness and biases (missing niche journals, recency gaps) can skew synthesized 'laws'; scalability/interpretability issues (embedding choices, clustering sensitivity); ReAct agents may loop without new strategies and require improved planning/search trees or knowledge graphs.",
            "uuid": "e4267.0",
            "source_info": {
                "paper_title": "pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLM-assisted hypothesis generation",
            "name_full": "Large Language Models for hypothesis generation and scientific discovery",
            "brief_description": "Mentioned in the paper as an emerging use of LLMs: generating hypotheses or proposed qualitative mechanisms from literature, and (in some recent works) proposing equations or experiment ideas, though such outputs often lack grounding and novelty.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (unnamed in this paper; cited work includes generic LLM-based methods and LLM-SR style systems)",
            "model_size": null,
            "method_name": "LLM-based hypothesis generation (general)",
            "method_description": "Described at a high level: LLMs are prompted (sometimes with task-specific scaffolds) to propose hypotheses, tentative mechanisms, or experiment designs by synthesizing across papers; some approaches couple LLMs with programming-based solvers (e.g., LLM-SR) to propose equations or analytic forms. The paper notes this may be attempted either directly (generate hypotheses from corpus) or via agentic frameworks that iteratively retrieve and reason.",
            "number_of_papers": null,
            "domain_or_field": "astronomy (discussion), but the technique is cross-domain",
            "type_of_laws_extracted": "hypotheses, proposed mechanisms, speculative empirical generalizations, and candidate guiding principles for follow-up research",
            "example_laws_extracted": null,
            "evaluation_method": "Not specified in detail in this paper; the authors note prior work but caution that generated hypotheses often lack grounding and may recapitulate existing knowledge rather than provide validated novel laws.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Potential for accelerating discovery exists but is largely untapped; current LLM-generated hypotheses frequently lack grounding and novelty and therefore require human vetting and further validation. The authors position pathfinder as infrastructure to better ground such generation with a curated corpus and robust retrieval methods.",
            "challenges_limitations": "Main issues: lack of grounding in primary data or experimental evidence, tendency to rephrase known results rather than invent validated novel laws, risk of hallucination, and insufficient evaluation protocols to verify novelty or correctness of generated hypotheses.",
            "uuid": "e4267.1",
            "source_info": {
                "paper_title": "pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLMs for literature review (prior work mentions)",
            "name_full": "Applications of LLMs to perform literature review and summarization (cited prior work: ChatCite, Astarita et al., Li et al., Tao et al.)",
            "brief_description": "The paper cites growing academic use of LLMs for explaining concepts, performing literature reviews and comparative literature summaries (e.g., ChatCite), and also notes controversies around accuracy, bias and hallucination.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs referenced in cited literature (e.g., GPT-family models and other academic LLMs referenced indirectly)",
            "model_size": null,
            "method_name": "LLM-assisted literature review and summarization (e.g., ChatCite-style agents)",
            "method_description": "Mentioned use-cases include prompting LLMs to synthesize summaries across retrieved papers, guide comparative literature synthesis with human workflow guidance (ChatCite-style), and assist drafting review articles; these methods often combine retrieval (RAG-like) and LLM summarization, but specifics vary by cited work.",
            "number_of_papers": null,
            "domain_or_field": "academic literature broadly and astronomy specifically (as referenced in the paper)",
            "type_of_laws_extracted": "survey summaries, comparative syntheses, consensus findings, terminology mappings and review-style principles (qualitative outputs rather than formal laws)",
            "example_laws_extracted": null,
            "evaluation_method": "Not detailed in this paper for the cited prior work; the pathfinder paper itself uses synthetic benchmarks and human-curated Gold QA to evaluate retrieval/summarization quality.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "The literature shows LLMs are increasingly applied to literature review tasks but face issues around factual accuracy, bias, and potential plagiarism; integrating retrieval grounding (RAG) and curated corpora can mitigate some risks.",
            "challenges_limitations": "Accuracy concerns (hallucinations), bias inherited from training data, plagiarism/attribution issues, and the need for careful human oversight and evaluation when using LLMs for scholarly synthesis.",
            "uuid": "e4267.2",
            "source_info": {
                "paper_title": "pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis Generation with Large Language Models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "rating": 2,
            "sanitized_title": "llmsr_scientific_equation_discovery_via_programming_with_large_language_models"
        },
        {
            "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy",
            "rating": 1,
            "sanitized_title": "delving_into_the_utilisation_of_chatgpt_in_scientific_publications_in_astronomy"
        },
        {
            "paper_title": "Machine Learning for Astrophysics",
            "rating": 1,
            "sanitized_title": "machine_learning_for_astrophysics"
        }
    ],
    "cost": 0.016914,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy
August 6, 2024</p>
<p>Iy 0000-0001-6823-5453
Yunu 0000-0002-5077-881X
O'n 0000-0002-4110-3511
Kiera Mccormick 0009-0002-3503-4721
Ioana Ciucă 0000-0002-3015-9130
Wu 0000-0002-6955-746X
Ac 0000-0002-2358-522X
Astari 0009-0005-8380-6468
Chakr 0000-0002-1416-8483
Anjali Field 0000-0002-6955-746X
Michele Ginolfi 0000-0002-9122-1700
Marc Huertas-Company 0000-0002-1416-8483
Jab 0000-0003-4797-7030
Mugdha Polim 0000-0003-0220-5125
Rod 0000-0001-5082-9536
Sanjib Sharma 0000-0002-0920-809X
Michael J Smith 0000-0003-0220-5125
Yuan-Sen Ting 0000-0001-5082-9536
Mike Walmsley 0000-0002-6408-4181</p>
<p>(UniverseTBD)
550 West 120th Street, New York, NY 10027, USA 3400 N. Charles Street, Baltimore, MD 21218, USA 108 North Rd, Acton ACT 2601, Australia 450 Jane Stanford Way, Stanford, CA 94305, USA 1500 SW Jefferson Way, Corvallis, OR 97331, USA 4501 North Charles Street, Baltimore, MD 21210, USA Cotter Rd., Weston, ACT 2611, Australia Acton, ACT 2601, Australia Stanford, CA, USA, 94305 3700 San Martin Drive, Baltimore, MD 21218, USA Harvard &amp; Smithsonian, Cambridge, MA 02138, USA Camino Bajo del Castillo s/n, 28692 Villanueva de la Cañada, Madrid, Spain Paris, France Cotter Road, Weston, ACT2600, Australia 3400 North Charles Street, Baltimore, MD 21218, USA Oak Ridge, TN 37831, USA Via G. Sansone 1, 50019, Sesto Fiorentino (Firenze), Italy Largo E. Fermi 5, I-50125, Firenze, Italy C. Via Lactea, 1, E-38205 La Laguna, Tenerife, Spain E-38206 La Laguna, Tenerife, Spain Paris, France Santa Cruz, CA 95064, USA Hefei, Anhui 230026, China Hefei 230026, China Ias , i, Romania Canberra, Australia Urbana-Champaign, 61820, USA 8005 Zurich, Switzerland Columbus, OH 43210, USA Columbus, OH 43210, USA 50 St. George Street, Toronto, ON M5S 3H4, Canada Oxford Road, Manchester, M13 9PL, UK</p>
<p>1 Columbia Astrophysics Laboratory, Columbia University,</p>
<p>2 Department of Physics and Astronomy, Johns Hopkins University,</p>
<p>3 School of Computing, The Australian National University,</p>
<p>4 Stanford University,</p>
<p>5 School of Electrical Engineering and Computer Science, Oregon State University,</p>
<p>6 Department of Engineering, Loyola University Maryland,</p>
<p>7 Research School of Astronomy &amp; Astrophysics, Australian National University,</p>
<p>8 School of Computing, Australian National University,</p>
<p>9 Kavli Institute for Particle Astrophysics and Cosmology and Department of Physics, Stanford University,</p>
<p>10 Space Telescope Science Institute,</p>
<p>11 Center for Astrophysics,</p>
<p>12 European Space Agency (ESA), European Space Astronomy Centre (ESAC),</p>
<p>13 Independent Researcher,</p>
<p>14 Astralis-AITC -Stromlo, RSAA, Australian National University,</p>
<p>15 Department of Computer Science, Johns Hopkins University,</p>
<p>16 National Center for Computational Sciences, Oak Ridge National Laboratory,</p>
<p>17 Dipartimento di Fisica e Astronomia, Universita di Firenze,</p>
<p>18 INAF -Osservatorio Astrofisico di Arcetri,</p>
<p>19 Instituto de Astrofisica de Canarias,</p>
<p>20 Universidad de la Laguna, dept. Astrofisica,</p>
<p>21 Universite Paris-Cite, LERMA -Observatoire de Paris, PSL,</p>
<p>22 SCIPP, University of California,</p>
<p>23 Key Laboratory for Research in Galaxies and Cosmology, Department of Astronomy, University of Science and Technology of China,</p>
<p>24 School of Astronomy and Space Science, University of Science and Technology of China,</p>
<p>25 Ias , i AI,</p>
<p>26 Xaana AI,</p>
<p>27 School of Information Sciences, University of Illinois,</p>
<p>28 Modulos AG,</p>
<p>29 Department of Astronomy, The Ohio State University,</p>
<p>30 Center for Cosmology and AstroParticle Physics (CCAPP), The Ohio State University,</p>
<p>31 Dunlap Institute for Astronomy and Astrophysics, University of Toronto,</p>
<p>32 Jodrell Bank Centre for Astrophysics, Department of Physics &amp; Astronomy, University of Manchester,</p>
<p>pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy
August 6, 2024C0AF197BAF839C9075E821567A55D392arXiv:2408.01556v1[astro-ph.IM]Astronomical reference materials(90) -Astronomy web services(1856) -History of astronomy(1868) -Computational methods(1965) -Astronomy data visualization(1968)
The exponential growth of astronomical literature poses significant challenges for researchers navigating and synthesizing general insights or even domain-specific knowledge.We present pathfinder, a machine learning framework designed to enable literature review and knowledge discovery in astronomy, focusing on semantic searching with natural language instead of syntactic searches with keywords.Utilizing state-of-the-art large language models (LLMs) and a corpus of 350,000 peer-reviewed papers from the Astrophysics Data System (ADS), pathfinder offers an innovative approach to scientific inquiry and literature exploration.Our framework couples advanced retrieval techniques with LLMbased synthesis to search astronomical literature by semantic context as a complement to currently existing methods that use keywords or citation graphs.It addresses complexities of jargon, named entities, and temporal aspects through time-based and citation-based weighting schemes.We demonstrate the tool's versatility through case studies, showcasing its application in various research scenarios.The system's performance is evaluated using custom benchmarks, including single-paper and multi-paper tasks.Beyond literature review, pathfinder offers unique capabilities for reformatting answers in ways that are accessible to various audiences (e.g. in a different language or as simplified text), visualizing research landscapes, and tracking the impact of observatories and methodologies.This tool represents a significant advancement in applying AI to astronomical research, aiding researchers at all career stages in navigating modern astronomy literature.</p>
<p>INTRODUCTION</p>
<p>As one of the oldest scientific disciplines, astronomy has amassed an enormous body of literature over time.Modern astronomical libraries and recordkeeping services like the Astronomical Data System (ADS) (Accomazzi et al. 2015) and preprint servers like arXiv provide lasting repositories for accessing current research on various astronomical subfields, with records on ADS extending back to the early 16th century.As the body of astronomical literature grows (at an ever accelerating rate), this creates a growing problem of keeping track of the literature, with it becoming harder over time to keep track of relevant papers and contextualise the information contained in them while writing new papers.In fact, with the advent of new observatories like ALMA and JWST and new modalities of observations like gravitational waves, the literature has become challenging for even experienced researchers to keep pace with.This is exacerbated by a growing need for interdisciplinary efforts, which means that astronomers often need to keep track of multiple fields of literature, such as electronics and instrumentation, high performance computing, statistics, machine learning, and computer vision.</p>
<p>At best, this leads to a much larger amount of time and effort spent in organising and cataloguing papers for individual researchers, and at worst it can lead to a * Hubble Fellow splintering of the research landscape with researchers resorting to a friends-of-friends or in-group citation framework while writing papers.This situation also creates a barrier to entry for aspiring students and researchers trying to enter the field and perform their first literature search, especially in the absence of an authoritative review on their chosen topic.</p>
<p>While this is also true of fields other than astronomy, we have the unique distinction of having a large body of publicly accessible data, code and literature (Genova 2023), which provides a unique opportunity for developing methods that can ingest, retrieve and synthesize literature in a way that is useful for a wide range of audiences (Iyer 2021;Grezes et al. 2021;Rodríguez et al. 2022;Blanco-Cuaresma et al. 2023;Dung Nguyen et al. 2023).To this end, we explore the use of state-of-the-art machine learning methods in conjunction with a corpus of papers from ADS and arXiv to find relevant literature and provide initial starting points for answering questions across a variety of levels.</p>
<p>Large language models (LLMs) have seen rapid advancement and adoption in recent years, with models like GPT-4 (OpenAI et al. 2024) and LLaMA (Touvron et al. 2023) demonstrating impressive capabilities across a wide range of tasks.In academic contexts, LLMs are increasingly being used to assist with explaining advanced concepts (Prihar et al. 2023) and perform literature review (Li et al. 2024b;Tao et al. 2024), and possibly with writing papers (Liang et al. 2024), even in astronomy (Astarita et al. 2024).However, their application remains controversial due to concerns about accuracy, bias, accidental plagiarism (Pervez &amp; Titus 2024), and the potential for hallucination (e.g., Zhang et al. 2023).Despite these challenges, many researchers are exploring ways to leverage LLMs as tools to augment human expertise and accelerate scientific discovery (Van Noorden &amp; Perkel 2023), particularly in fields with vast and rapidly growing bodies of literature like astronomy.</p>
<p>The notion of using machine-learning methods for improving literature surveys is not a particularly new concept, and such tools have been accessible since the early 1990s with methods like n-grams (Cavnar et al. 1994;Kondrak 2005), bag-of-words (Zhang et al. 2010), or transformer based models like BERT (Devlin et al. 2018).Versions of these methods including AstroBERT (Grezes et al. 2021) and more recently AstroLLaMA (Dung Nguyen et al. 2023) have been applied to large corpora of astronomical data as proof-of-concept techniques to showcase how NLP and newer language models can successfully ingest with astronomical keywords and scientific jargon.Here, we provide a working pipeline to show how these models can be combined with techniques like retrieval augmented generation (RAG) and agentic LLMs to capture significantly more semantic context and provide hallucination-free literature review at a fraction of the time and cost of manually searching for papers on a given topic.We stress that this is not meant to be a replacement for existing search tools like arxiv.org, the Astronomical Data System (ADS), Google scholar, Benty-Fields or Arxivsorter, but rather a complement to them, with three key advantages: (1) the ability to query the system using natural language, (2) added synthesis to generate a targeted summary of the retrieved documents in context to the question, and (3) exploratory tools to find similar papers in an interpretable embedding space.</p>
<p>To do this, we present the pathfinder framework1 , an open-source, publicly available tool that uses LLMs to answer natural-language astronomy questions using a corpus of ∼ 350, 000 peer-reviewed papers from ADS going back to 1990.The framework is presented both as open-source code and as an online tool that can be used to find relevant literature, answer questions, and explore the corpus of papers.The current version of the tool uses only abstracts, but can be extended to fulltext in the future.In this paper, we explore the use of pathfinder to (i) visualize papers as a 'landscape' of astronomy research, (ii) find similar/relevant papers by performing a similarity search in embedding space, (iii) answer questions without hallucinations using the embedding space, (iv) explore the impact of different telescopes and observatories on the landscape of research, (v) explore the trends of authors over time, (vi) quantify missing areas that need to be developed further and find areas of interest for future surveys and facilities.</p>
<p>The structure of this paper is as follows.In section 2, we describe the dataset used to retrieve papers from.In section 3, we describe the overall pathfinder pipeline.In section 4, we describe evaluation benchmarks used while developing the model.Section 5 describes ways for users to interact with and use pathfinder, and provides some case studies that demonstrate its behaviour across different types of questions.In section 6, we present some larger trends analysed with the pathfinder framework.Section 7 concludes and summarizes the paper and the scope for future work.</p>
<p>DATASET</p>
<p>We have compiled a dataset of ∼ 350, 000 paper abstracts from the ADS2 and arxiv.org3, along with associated metadata including paper titles, publication dates, DOIs, author and affiliation information, and ADS keywords and bibcodes.We have also scraped the bibcodes for papers referenced in and citing any given paper in the dataset, which can be used to further expand the database in future iterations.In addition to this, we have used natural language tools (spacy running en_core_web_sm) to determine a set of 20 keywords for each abstract, along with LLM-generated embeddings for each abstract as described in Section 3. The keywords are subsequently used to annotate figures and implement keyword weighting while retrieving papers.</p>
<p>The majority of the papers in our current corpus are drawn from an existing list of ∼ 270, 000 papers classified as astro-ph from the Kaggle arXiv dataset4 , which contains papers from April 1992 to July 2023 (similar to AstroLLaMA; Dung Nguyen et al. 2023;Perkowski et al. 2024).These papers are further augmented using metadata (bibcodes, citations, dates, authors and affiliations) from ADS.Since there are a number of papers that are not on arxiv.org,we subsequently query ADS for papers from January 1990 to July 2024 to find papers that are not in our dataset and add them, bringing our corpus to N = 352, 194.This set will be updated periodically to keep up to date with the current literature, and augmented by a corpus of older papers pro-cessed with optical character recognition (OCR) as part of future work (Naiman et al. 2023).The dataset is publicly available online5 .While this is not a complete corpus and primarily draws from the ApJ, MNRAS, A&amp;A, ARAA, Nature, Science, PASA, PASP, and PASJ families of journals, it includes a large sample of relevant work that can be used to test the framework.</p>
<p>BUILDING pathfinder</p>
<p>This section briefly describes the methods used to construct pathfinder.The codebase is public and available at the pathfinder repository.6Briefly, the pipeline is an augmented version of RAG.In standard RAG, the system first retrieves a set of relevant documents for any input user query, and then uses the information therein to synthesise its answer.pathfinder's augmentations include question categorization, query expansion, reranking, the ability to filter by date, citations and keywords and an alternative reason-thought-act based framework for synthesizing answers, described in further detail in the following sections.Figure 1 shows a schematic of the procedure described in this section.</p>
<p>Generation</p>
<p>We first describe the generation step (right-hand side of Figure 1), which uses the retrieved papers and associated metadata to generate an answer to the user's query.</p>
<p>Generating Embeddings</p>
<p>We compute embeddings for each abstract in our corpus using the text-embedding-3-small model from OpenAI7 , which is used to encode each abstract into a 1536-dimensional vector.Once the embeddings are computed, we use uniform manifold approximation and projection (UMAP8 ; McInnes et al. 2018) to create a 2-dimensional embedding of the high-dimensional vector space for easier visualization and further analysis.A heatmap of the embedding space is provided in Figure 2, with the different regions annotated with their most frequently occurring keywords for clarity.</p>
<p>Text generation with RAG</p>
<p>Generally, question-answer applications involving</p>
<p>LLMs generate an answer following a template (sometimes called a prompt) in response to a query.However, in doing so, there is a danger that the model may output factually incorrect information and lack access to all the available information needed to reply (Roller et al. 2021).To handle both of these problems, RAG forces the LLM to generate the response while using (and possibly citing) a set of document sources (Lewis et al. 2020;Shuster et al. 2021).Given an input query, we first search the full space of papers to find a subset of ∼ 1-30 papers that are relevant to the input query, retrieved using the methods described in Section 3.2.We then use langchain9 to set up the RAG system, where the query is passed in along with the abstracts of the papers broken down into chunks for the LLM to then construct an answer.The input prompt template also requires the LLM to be succinct in its responses and respond with 'I don't know' if the LLM does not find sources relevant to the query.</p>
<p>Text generation with ReAct agents</p>
<p>While many of the questions that astronomers tend to ask tools like pathfinder will be factual and need efficient similarity search and synthesis, others are more involved and require multiple lookups to answer.These tend to be questions that require resolving multiple conflicting viewpoints (consensus evaluation), combining information across multiple topics (composition), or speculating beyond available data (counterfactual ; see Section 4.4 for a fuller description of the different types of questions).</p>
<p>A limitation of the RAG framework is that it is incapable of directly answering these questions.To provide a basic framework that can be used to tackle these questions, we use ReAct agents (Reasoning and Acting; Yao et al. 2022), an approach that combines reasoning and acting in LLMs, allowing them to break down complex tasks into more atomic steps and execute them, combined with the RAG framework we have used thus far.Briefly, this system involves pathfinder receiving an input query, followed by the ReAct agent using a LLM to reason about the task and break it down into steps.For each step, the agent acts by using RAG to retrieve relevant information from the paper corpus.It uses the retrieved information to further analyse the data and make queries until it has enough knowledge to answer the question or runs up against the number of allowed iterations.The system is not perfect, with the LLM sometimes stalling in a process loop where it can not find an ideal way to phrase a question.Newer methods exist to use search trees (Yao et al. 2024) or knowledge graphs (Besta et al. 2024) to circumvent these issues.However, given the relatively small number of these questions we found users to ask, those are out of the scope of current work, and will be left for future upgrades in pathfinder.</p>
<p>Retrieval</p>
<p>Because the retrieved documents will strongly impact text generation, it is vital to ensure that we retrieve the most relevant documents to a user's input query.This section describes the procedure by which we retrieve ∼ 1-30 'top-k' papers (see left-hand side of Figure 1).</p>
<p>Semantic search &amp; embeddings</p>
<p>One of pathfinder's key functionalities is to find similar papers given a natural-language query (building on earlier work e.g., Iyer 2021).For this, it is important to be able to compare the vector corresponding to a query (computed using the same way as the embeddings for the abstracts) to those of paper abstracts and compute a similarity score.In principle, this can be done using any distance metric, and in the current application we use cosine similarity implemented using the Facebook AI Similarity Search (FAISS 10 ) package.FAISS is capable of processing on GPUs and scaling to extremely large datasets (Johnson et al. 2017), making our method future proof for applications to large corpora of literature.</p>
<p>Generating keywords from abstracts</p>
<p>We compute a set of keywords for each abstract using the textrank algorithm, set up to identify nouns, adjectives and proper nouns in any input text.For the current application, this has been implemented using the en_core_web_sm model in the spacy NLP package 11 .This is followed by running a peak finding algorithm in the 2D UMAP embedding space to identify regions where there is a high concentration of papers.For each peak we consider all papers within a certain radius and identify the most frequently occurring keywords for all the papers in that cluster, and repeat this for all the clusters in our space, followed by an LLM query to synthesize the keyword into an overarching topic or facility (e.g."solar astrophysics" or "gravitational waves: LIGO").While this provides a way to automatically tag a given space and provide a preliminary understanding of how papers are clustered, it can be sensitive to choices in tokenizing and clustering.These topics are shown in Figure 2, and can be compared to existing keywords from the Unified Astronomy Thesaurus (UAT) in Figure 3.</p>
<p>Weighting schemes: Keywords, Timestamps and</p>
<p>Citations</p>
<p>An overall goal of pathfinder is to return both relevant and trustworthy documents from the literature.Although we redirect bibliometric questions to complementary services like ADS, we find that astronomyrelated literature queries are often highly dependent on specific key terms (e.g.what are the main results from the CEERS survey?), the time of publication (e.g.what is the highest redshift galaxy currently?)or citations (e.g.what is the prevalent theory on why galaxies quench?).To help optimize retrieval, we provide toggles that implement weighting by these quantities.</p>
<p>Keyword weighting: Keywords can be astronomical jargon, named objects, or any user-specified string, and are compared against the keywords generated in the previous section.When keyword filtering is active, if a specific keyword is input by the user or if a named entity is detected in the query, semantic retrieval is heavily weighted toward documents with matching keywords.</p>
<p>Time weighting: We implement a relative-time weighting scheme to preferentially retrieve documents from the right time window, with functional form w t,i = 1/(1 + e (tnow−tpaper,i)/0.7 )</p>
<p>(1)</p>
<p>where the difference in time is calculated in years.This sigmoidal form is chosen to smoothly weight recent pa- pers, with the specific numbers chosen to penalize papers that are over ∼ 5 years older.</p>
<p>Citation weighting: We also provide users with the ability to apply citation-based weighting to preferentially return highly-cited literature, with functional form w n,i = 1/(1 + e (300−npaper,i)/42. )</p>
<p>(2)</p>
<p>These weights are applied after retrieving a large number of papers (top−k=1000) prior to subsequently reranking and taking the returning the requested top-k papers.</p>
<p>Query expansion and HyDE</p>
<p>Query expansion and HyDE (Hypothetical Document Embeddings) are techniques employed to enhance the retrieval process by bridging the semantic gap between queries and relevant documents (Manning et al. 2008).</p>
<p>In our implementation, we use HyDE to rewrite the initial query into a more comprehensive and domainspecific abstract, building upon the work of Gao et al. (2022).This process leverages an LLM prompted to act as an expert astronomer, generating an abstract and optionally a conclusion for a hypothetical research paper that addresses the given query.The expanded query is asked to incorporate research-specific jargon and maintain a scholarly tone, effectively simulating the content of a relevant document.This approach aligns with recent advancements in leveraging LLMs for domainspecific tasks, as demonstrated by Chowdhery et al. (2022).</p>
<p>The rationale behind this approach is twofold.First, by expanding the query into a full abstract, we provide more context and potentially relevant terms for the retrieval model to work with, increasing the likelihood of matching with pertinent documents in the corpus.This is conceptually similar to traditional query expansion techniques (Carpineto &amp; Romano 2012), but leverages the advanced language understanding capabilities of LLMs.Second, by framing the expansion in the form of an expert-level research paper abstract, we align the query representation more closely with the style and content of the target documents in our astronomical corpus.This technique can significantly improve retrieval performance, especially in zero-shot scenarios where task-specific fine-tuning data is unavailable (Gao et al. 2022).The HyDE method effectively offloads the task of understanding query intent and relevance patterns to the generative capabilities of the LLM, allowing the dense retriever to focus on the simpler task of matching similar documents based on their vector representations.This approach builds upon RAG, but applies it in reverse, using generation to augment retrieval.</p>
<p>Reranking</p>
<p>Reranking is an important additional step in modern information retrieval systems, designed to refine the initial set of retrieved documents and improve the overall relevance of the results (Burges 2010).In our pipeline, we implement a two-stage retrieval process: an initial retrieval using our HyDE-based semantic search, followed by a reranking step using a cross-encoder model.</p>
<p>Cross-encoder models, typically based on transformer architectures like BERT (Devlin et al. 2018), have shown superior performance in reranking tasks compared to traditional methods (Nogueira &amp; Cho 2019).Unlike biencoders used in the initial retrieval, cross-encoders process the query and document together, allowing for more nuanced relevance judgements through direct attention between query and document tokens.</p>
<p>Our implementation first uses the HyDE-based semantic search to retrieve an initial set of potentially relevant documents.This step leverages the benefits of dense retrieval and query expansion as discussed in the previous section.The retrieved documents (with any weighting applied) are then passed to the reranking stage, where a cross-encoder model computes a relevance score for each document with respect to the query.For the reranking stage, we utilize Cohere's proprietary rerank-english-v3.0 model.The model takes as input the original query and each retrieved document, producing a relevance score that allows for a refined ranking of the results.This two-stage retrieval process combines the efficiency of initial dense retrieval with the effectiveness of cross-encoder reranking (Lin &amp; Ma 2021).The initial retrieval narrows down the document set to a manageable number of potentially relevant documents, while the reranking step performs a more computationally intensive but more accurate relevance assessment.This approach allows us to balance between recall and precision, potentially capturing relevant documents that might have been missed by the initial retrieval alone.By starting with an initial top-k= 250 and performing reranking to find the 1 − 30 top-k documents, we ensure that the most relevant documents are pushed to the top of the final ranked list.</p>
<p>Outliers and consensus</p>
<p>Despite the semantic search (which consists of the similarity search + filtering + query expansion + reranking), sometimes the retrieved papers can be topically distinct from the input query.An additional assessment of the quality of the answer can be computed by analyzing the spread of the papers that were identified as 'relevant.'If the relevant papers are tightly clustered in the UMAP space, the resulting answers tend to be more  reliable, as opposed to broader distributions of the top-k papers where the LLM has to synthesize an answer that draws from disparate, and sometimes unrelated, portions of the literature.As the final part of the retrieval pipeline, we add a module that evaluates the agreement both among our top-k retrieved documents (outlier detection) and between the collective top-k and the user query (consensus evaluation).</p>
<p>To assess the level of agreement among the top-k, we implement an outlier detection scheme that aims to isolate one or more papers in the top-k whose abstracts are topically different from the other constituent papers.Our first step is to compute an "outlier cutoff distance" D cut (k).Suppose we have N papers in the corpus.Using a statistically significant random subset of size n &lt; N , we iterate through each paper and find the distances to the k nearest papers in the highdimensional embedding space.After appending each of these embedding space distances to a large list of size kn, we find the 95th percentile of these distances D 95 (corresponding to 2σ in a Gaussian distribution).From this, we obtain D cut (k) = D 95 − γ, where γ = 0.1 is an experimentally obtained correction term.</p>
<p>After computing D cut (k), we now turn our attention to the top-k retrieved documents.For each top-k paper P with embedding P, we first compute the centroid C ¬P of the remaining k − 1 points in the embedding space.We then find the distance D(P, C ¬P ).from P to this centroid.If D(P, C ¬P ) &gt; D cut (k), paper P is flagged as an outlier.See the middle and right panels of Figure 4 for examples of outliers getting flagged.</p>
<p>The logic behind our outlier detection approach stems from the fact that we would expect the top-k retrieved documents to ideally be clustered together based on one or more topics determined by the user query.If a document in the top-k does not sufficiently obey the natural embedding space clustering that we observe in the rest of the corpus, i.e. if it is too far away from the other k − 1 papers to be considered part of their cluster, it can be considered an outlier.</p>
<p>Building upon this outlier detection process, we can now shift our focus to assessing the level of agreement between the collective top-k documents and the user query.This consensus evaluation scheme utilizes an independent LLM running on GPT-4o mini.Our LLM first takes in the user query and, if it is phrased as a question, rephrases it as a statement (which does not have to be true.)Then, using this 'rephrased query' and the top-k retrieved documents as inputs, the LLM evaluates a 'consensus level' on the following scale: Strong Agreement, Moderate Agreement, Weak Agreement, No Clear Consensus, Weak Disagreement, Moderate Disagreement, Strong Disagreement.Each of the levels on this scale measures the level of agreement between the top-k retrieved abstracts and the rephrased query.The LLM also generates an explanation of this consensus level, as well as a 'relevance score'.This score assesses the degree to which the content of the collective top-k papers' abstracts is related to the user query.A com-</p>
<p>Iyer et al.</p>
<p>pletely unrelated top-k would return a relevance score of 0, whereas a perfectly related top-k would return a score of 1.</p>
<p>When implemented, this outlier detection and consensus evaluation module is effective at performing two tasks: isolating retrieved papers that should not be in the top-k due to topical dissimilarity to other topk members, and evaluating the strength of agreement or disagreement between the collective top-k and the user query.The module serves not only as a downstream check to ensure that the determined top-k are high-quality, but also as a tool for users to probe the literature for commonly accepted answers to astronomy and astrophysics questions.</p>
<p>BENCHMARKS AND EVALUATION</p>
<p>To evaluate pathfinder, we develop a set of synthetic and human-assisted benchmarks for quantitatively testing the retrieval of papers and the quality of answers.Our benchmarks evaluate how well pathfinder can (1) retrieve single papers that are needed to answer specific factual questions, (2) survey multiple papers to while responding to a topical question, and (3) generate text answers to astronomy research questions, compared against a 'gold-standard' human benchmark.</p>
<p>Single-paper synthetic benchmark</p>
<p>The Single-paper synthetic benchmark describes our procedure to quantitatively test the retrieval of evaluation on questions that are answerable based on information in a single paper.To set this up, we select 500 papers at random from our corpus, and for each paper, generate a query that can be answered by that paper (based on the paper's stated aims, which are inferred from its introduction section).First, a LLM selects a factually dense sentence from the paper, and then converts it into an information retrieval query.Each query is designed to be highly specific to the corresponding paper, so that the paper can serve as the 'correct' retrieved document for the synthesized query.This strategy is analogous to the 'sparse judgement' setup in Rahmani et al. (2024), which is found to roughly align with actual human judgment.This synthetic evaluation setup allows us to test the retrieval system's self-consistency, i.e. whether the retrieval system indeed returns the paper that a highly specific query has been generated from.We compute the success rate s, or the percentage of queries for which the source document is in the top k = 10, and the reciprocal rank, or the average across queries of r −1 , where r is the rank of the document amongst the top k; higher is better.Using these metrics, we find that our methods significantly improve retrieval performance; simple Bag of Words / TF-IDF (Term Frequency-Inverse Document Frequency) retrieval achieves s = 0.46, r −1 = 0.29, while semantic search with HyDE and reranking achieves s = 0.84, r −1 = 0.74.</p>
<p>Multi-paper synthetic benchmark</p>
<p>We also construct a synthetic quantitative benchmark for more general queries that often require synthesizing information from multiple documents across different subject areas or experiments.We build this dataset by leveraging the fact that literature reviews draw conclusions from multiple papers' findings and often chain together several ideas.From a starting set of N = 200 peer-reviewed astronomy review papers, we selected factual sentences substantiated by a large (&gt; 5) cluster of in-text citations (e.g., 'The connection between galaxies and their dark matter halos has been substantiated via scaling laws calibrated to large hydrodynamic simulations (paper 1, paper 2, paper 3, ... )'').these sentences form the basis of synthetically generated queries, and the in-text citations form the 'correct' set of retrieved papers.We evaluate pathfinder's ability to parse queries with complex answers across multiple documents using this synthetic benchmark, measuring recall and normalized cumulative discounted gain12 (nDCG) to reward documents correctly retrieved while avoiding penalizing relevant documents not covered by the citation cluster.Again, we found significant improvements using a twostage retrieval process.For a baseline Bag of Words model and top k = 50, we achieved recall = 0.15 and nDCG = 0.09; HyDE with reranking improved these metrics to recall = 0.29 and nDCG = 0.19.</p>
<p>The Gold Questions and Answers Dataset</p>
<p>While single and multi-paper factual queries provide valuable synthetic benchmarks for pathfinder, they encompass a limited range of query types.To account for real-world scenarios involving human experts, where queries are likely to be more complex and challenging, we make use of an expert-curated 'Gold' dataset from Wu et al. (2024).This dataset serves three primary purposes: (1) to test new iterations of pathfinder, (2) to identify the steps and challenges involved in answering complex queries, which could inform the design of improved schemes for handling sophisticated inquiries and (3) form a basis for more detailed case studies.</p>
<p>To create this dataset, a pathfinder-like system was deployed as a Slack bot for astronomy researchers at the Space Telescope Science Institute (for more details, see Wu et al. 2024).Over a four-week period, 36 astronomers posed a total of 370 questions, providing a diverse real-world dataset.Subsequently, a group of five researchers, including two astronomers, were tasked with categorizing these queries using inductive coding (Field et al., in prep).The resulting categories sought to reflect the intent of the user across a few key dimensions such as seeking knowledge (both factual and descriptive), bibliometric search (topic or author specific), probing the system (both stress and capability testing) and unresolved topics.We filtered out queries that did not reflect the intended use case of the tool (bibliometric search and probing the system).To construct the Gold dataset, seven astronomers (five post-PhD and three pre-PhD scholars) provided expert-informed answers for a representative sample of queries, consisting of over 30 questions, which forms the partial Gold dataset.The final version of the dataset will contain over 100 questions.</p>
<p>An analysis of the Slackbot user interaction data and user interviews (Wu et al. 2024, Field et al., in prep) found that:</p>
<ol>
<li>
<p>Positive user interaction, as measured by thumbs up vote fraction, is positively correlated with higher retrieval scores at p &lt; 10 −6 significance (Spearman rank correlation ρ = +0.33).</p>
</li>
<li>
<p>Users of the Slackbot QA system better retrieval of papers for time-sensitive queries, paper citations, and other paper metadata.</p>
</li>
</ol>
<p>Constructing categories of questions</p>
<p>Based on the different questions asked by astronomers in the user study (Wu et al. 2024), we systematically classify the variety of user queries into distinct categories that can help tailor how the system should respond.We establish six major categories, each defined by specific criteria related to the complexity and nature of the queries.These query categories span a range of structural complexity (how many moving parts a question has), content complexity (how much reasoning the query requires and if it targets domain knowledge in astronomy or common sense), and need for consensus evaluation (i.e., for queries on unresolved and debated topics).Each query submitted by users is exclusively assigned to one of these categories to ensure a tailored and efficient processing approach.</p>
<p>• Single-Paper Factual Questions: Given a question, can the top retrieved paper answer it and provide further reading?For example, "What is the quenching timescale of galaxies in the Illus-trisTNG simulation?"</p>
<p>• Multi-Paper Factual Questions: Given a question, do we need multiple papers to answer it if a review doesn't exist?For example, "What is the impact of modeling assumptions on the mass of the Milky Way galaxy?"</p>
<p>• Consensus Evaluation: Given every entry in the top-k retrieval, determine whether each entry supports, refutes, or is irrelevant to the query.For example, "Is there a Hubble tension?Do AGN quench star formation in galaxies?"</p>
<p>• Compositional Questions: These questions need to be broken down into separate sub-queries to be answered effectively.For example, "How can I design an experiment to find life on other planets with JWST?".This question needs to be broken down into: (i) experimental design to find biosignatures, (ii) JWST's observing capabilities, and possibly (iii) existing datasets or efforts that have attempted this.</p>
<p>• What-Ifs and Counterfactuals: These questions can't be answered directly from the literature and need either more observations or experiments.They require some synthesis and creativity in the generation part.</p>
<p>• Unclassified Questions: For questions that do not fit into the above categories, the identification is "None of the above."</p>
<p>To further refine and optimize the query processing system, an additional step involved the development of specific flags.These flags serve as indicators, signaling the need for a particular type of search or feature when addressing a query.We delineated four major flags:</p>
<p>• Named Entity Recognition: This flag is crucial for identifying proper nouns within queries, such as specific projects or astronomical terms (e.g., JWST, CEERS, CANDELS, CLASSY, H0LICoW).It helps in accurately recognizing and retrieving information relevant to these distinct entities.</p>
<p>• Jargon-Specific Questions and Overloaded Words: This flag addresses queries that contain specialized jargon or words with contextdependent meanings, such as "What is the metallicity of early type galaxies?"or "What is the main sequence for z∼3 galaxies?"Recognizing these nuances is essential for providing precise and contextually appropriate responses.</p>
<p>• Bibliometric Search: Related to the retrieval of citations, this flag is vital for queries that require sourcing and referencing specific scholarly works, enhancing the academic rigor of the responses.</p>
<p>• Time-Sensitive: This flag is applied to queries about phenomena or data that evolve over time, ensuring that the provided information is current and relevant, such as "What is the highest redshift galaxy?".</p>
<p>The development of flags was specifically aimed at enhancing the formulation of features within the metadata pipeline, reflecting the specific needs and preferences expressed by users.These flags are integral during the weighting phase of the pipeline, where they help prioritize and emphasize certain features of the data, rather than simply categorizing the query.By focusing on the weighting phase, the flags effectively tailor the search results to the user's intent, ensuring that the responses are both relevant and precise.</p>
<p>USING THE pathfinder FRAMEWORK</p>
<p>This section describes various scenarios in which users can use pathfinder to accelerate their research.The online tool, data, and code are freely available at pfdr.app.In this section, we explore the basic uses (asking questions, finding similar papers, and exploring the paper landscape), followed by case studies of individual questions from a human-interaction study during the JSALT workshop (Field et al., in prep).</p>
<p>Basic Usage</p>
<p>Using pathfinder online is generally as simple as asking a question.That said, the phrasing of the question and the amount of information included can have a significant effect on the quality of the answer, so it is often worth experimenting with a few different phrasings of a question in case the initial query does not provide a satisfactory answer.Rephrasing can often involve things like (i) making the query more specific or general, depending on the level of the result, (ii) changing the query settings, including weighting for keywords, time or citations, which will change the retrieved papers, or (iii) changing the type of generation (RAG vs Agent) depending on the complexity of the question and the brevity of the desired answer.</p>
<p>Figure 6 shows the outputs from pathfinder upon being asked a question, which consist of the answer, a set of input + detected keywords and the top retrieved papers as an interactive table.The output also includes (i) a suggestion estimating the type of question being asked, along with recommendations for the settings to optimise performance for that question type, and (ii) estimate of the consensus between the retrieved abstracts with respect to the user's input query.</p>
<p>Tweaking search parameters</p>
<p>Figure 6 also shows the different settings available to a user while running pathfinder: the number of papers to retrieve, additional keywords to include in the search, toggles to turn on keyword/time/citation weighting, and retrieval and generation methods.Depending on the</p>
<p>Top-k</p>
<p>Number of papers to retrieve and use in synthesizing the answer Tip: keep top-k small if looking for specific facts, but large if surveying or looking for consensus</p>
<p>Query</p>
<p>The input question Tip: try rephrasing if you're not satisfied with the answer, increasing/decreasing the specificity.Bonus: you can add modifiers to change the answer's language or the explanation's level.</p>
<p>Extra keywords</p>
<p>Only used if weight-by-keywords is set to true.Extra keywords to match while weighting the responses Tip: while searching for a particular result, try adding the relevant telescope / simulation</p>
<p>Weight by …</p>
<p>Weight the top retrieved papers used to generate a response by recency, keywords, or citation counts Tip: some question types (e.g.recent discoveries with date-wt or consensus with citation-wt) may benefit from specific weighting.</p>
<p>Retrieval method</p>
<p>Whether the retrieval should include query expansion (HyDE) and/or reranking (CoHERE) in addition to semantic search Tip: generally the default works, but semantic search alone is much faster</p>
<p>Download output</p>
<p>Download summary as JSON file</p>
<p>Tip: the table of top-k results can also be downloaded as a separate .csvfile, and can be searched to find specific keywords/papers Figure 6.Example of pathfinder being asked a question, with explanations of the various toggles to customize the output shown as numbered blue circles.Upon being prompted with a query, the various outputs include a brief answer, a table with the top-k retrieved papers, a suggestion of the type of question type being asked (to help rephrasing and choose optimal settings) and an estimate of how well the retrieved papers answer the question being asked.</p>
<p>Question type</p>
<p>type of query, these settings can be adjusted to get optimal search results.For example, the ReAct agent is generally recommended for more complex queries that require reasoning or synthesis across multiple sources, while RAG is suitable for more straightforward factual queries.Table 1 lists recommended settings for different types of questions that a user might want to ask the tool.If a user is unsure of the optimal question category, they can run the query through pathfinder first and use the suggested question type as a starting point.Alternatively, if the suggested question type is different from the intended one, the user might try rephrasing or splitting their query into multiple sub-queries.</p>
<p>Iyer et al.</p>
<p>We also provide four 'prompt specializations' that pair the query with different kinds of prompts, leading to different generated answers.There are currently four choices: (i) Single-paper: a prompt that returns a terse factual reply to the query, (ii) Multi-paper: the default prompt, that returns a summary synthesized from the top-k results, (iii) Bibliometric: a prompt that returns the LLM's best estimate of a suitable ADS prompt for the input query, and (iv) Broad but nuanced: a prompt that an initial answer, critiques itself, and uses it to formulate an improved response.</p>
<p>Case studies</p>
<p>In this section, we will provide examples of some questions asked by users as a showcase, explaining how those questions were approached by the model.We will also discuss how both query formulation and model responses can be improved:</p>
<ol>
<li>What is the value of the Hubble Constant?</li>
</ol>
<p>(Single-paper factual and/or Consensus Evaluation; Named entity recognition; Jargon-specific; Consensus score: Moderate Agreement) Shown in Figure 6, this question uses the Hubble tension (i.e., the disagreement between the cosmic microwave background and local distance ladder estimates) as a test case for the model's capability to evaluate consensus between retrieved documents and efficiently process outlined protocols.</p>
<p>The question is well-formulated and can be easily classified by the model, reports both sets of measurements and highlights the ongoing debate in the consensus section.the model can answer questions that require a broad scope of papers to be retrieved, with the model replying with 'major discoveries in astronomy made by citizen scientists include the classification of galaxies in the Galaxy Zoo project, the identification of new supernovae, the discovery of exoplanets through Planet Hunters, and contributions to the search for extraterrestrial signals via SETI@home'.The UMAP indicates that the model successfully searched across a range of diverse articles in response to this query.Interestingly, the initial retrieval does not find the original 'green peas' paper that is an expected part of this answer, since that paper did not use the phrase 'citizen scientist'.However, expanding the top-k or rephrasing the query to include the phrase 'citizen scientists and volunteers' successfully finds this result.</p>
<ol>
<li>What is the difference between a faint dwarf galaxy and a star cluster?(Compositional and Jargon-specific; Consensus score: Moderate Agreement) An astronomer's initial response to this question might be that they are completely different things, with one being a low-mass object (by galaxy standards) with a complex structure and often bursty history, while the other being a sub-component of a galaxy that is generally more homogeneous and considered one of its building blocks.However, as new research probes dwarf galaxies to ever low masses (i.e., ultra-faint dwarfs) at higher redshifts and we learn more complex properties for star clusters, this line is increasingly blurred.This is an example of both a jargonspecific and compositional question that requires pathfinder to pull together references from areas that aren't well connected.</li>
</ol>
<p>Rephrasing the question as a counterfactual (e.g., as 'Could a faint dwarf galaxy and a star cluster be the same thing?')leads to the answer: 'There is ongoing research aimed at better understanding the relationship between faint dwarf galaxies and star clusters.Some studies suggest that certain star clusters, particularly those that are very faint and low in mass, could be the remnants of dwarf galaxies that have lost their gas and dark matter  1. Suggested settings of the number of papers retrieved (top-k), weights for keywords, recency or citations, and the choice of retrieval and generation method for different query types.These can also be paired with the prompt specialization in the settings for better results (e.g. using the bibliometric prompt type, especially when the model recognizes the question type as such, returns a query that can be put in ADS, while using the single-paper prompt returns a short factual answer.</p>
<p>due to environmental effects, such as tidal interactions with larger galaxies.Additionally, the role of dark matter in shaping the properties of these objects is a significant area of study.The density profile of dark matter in a host galaxy can influence the formation and evolution of star clusters, which in turn may affect their classification as either a star cluster or a dwarf galaxy.'</p>
<p>5.</p>
<p>Can I predict a galaxy spectrum from an image cutout?(Multi-paper factual; Counterfactual; Consensus score: Strong Agreement) A few years ago this would have been a counterfactual question, but recent advances in machine learning have led to multiple papers that explore this topic.Not only does pathfinder agree that this is indeed possible, it finds the specific instances of this in the literature: 'Abstract 1 discusses a hybrid CNN approach that predicts galaxy spectra directly from imaging, while Abstract 4 highlights the prediction of spectra from Pan-STARRS images.',which can then be looked up in the returned list of papers.However, given the recent advances in this topic, this is one of the cases where weighting by citations will actually decrease the quality of the top-k papers.</p>
<ol>
<li>
<p>Do you know of any papers that tried Lasso or Ridge regression of the color color distribution of AGN using torus models?(Multipaper factual; Consensus score: Weak Agreement) While this reads as a straightforward question, it is not one that currently has an answer in the literature.However, it serves to highlight a potential weakness -since pathfinder can currently only use abstracts to answer its questions, detailed data or method questions which might be present deeper in the paper or not explicitly stated at the abstract level can often be missed.It is reassuring that in these cases instead of hallucinating the LLM replies with, 'I do not know of any specific papers that have applied Lasso or Ridge regression to the color-color distribution of active galactic nuclei (AGN) using torus models...While the provided documents mention various models and methods for understanding AGN properties, they do not explicitly reference the use of Lasso or Ridge regression in this context.'When this flavor of response is returned, it is recommended that the user retry the question in the bibliometric prompt style, perhaps additionally modifying the query to search in the ADS fulltext.</p>
</li>
<li>
<p>Que es un exoplaneta?(Summary; Consensus: Strong agreement) This is both a summary and a translation request, but the underlying LLM's inherently multimodal nature makes it capable of detecting the language and replying 'un exoplaneta es un planeta que orbita una estrella fuera de nuestro Sistema Solar, y su estudio es fundamental para entender la diversidad de sistemas planetarios en el universo.'.</p>
</li>
</ol>
<p>8.</p>
<p>What is Kriging also called in astronomical applications?(Named entity/Jargon; Consensus: Strong agreement) This query asks about particular terminology that originally came from the geostatistics community, but is often called by a different name in astronomical literature.Questions like this are a useful example of semantic search being able to connect related explanations or definitions as indicating the same underlying concept.pathfinder replies with 'Kriging is often referred to as 'Gaussian process regression' in astronomical applications.This term emphasizes the statistical foundation of the method, which relies Iyer et al.</p>
<p>on the properties of Gaussian processes to make predictions about spatially correlated data'.</p>
<ol>
<li>How would galaxy evolution differ in a universe with no dark matter?(Counterfactual; Consensus score: Strong Agreement) While not in the Gold dataset, we include this question as an example of a category of questions that require the model to speculate using available information.It performs best with a large top-k, and excerpts from its answer include, 'Without dark matter, the initial conditions for galaxy formation would be significantly altered.','...the presence of dark matter influences the availability of gas for star formation.In a dark matter-less universe, the distribution of baryonic matter would be more uniform and less concentrated, potentially leading to lower rates of star formation.Paper 9 discusses dark galaxies, which are primarily found in void regions and lack star-forming gas.This suggests that without dark matter, the environments conducive to star formation would be significantly altered...', 'the overall evolution of the universe would also be affected.Dark matter contributes to the large-scale structure of the universe, influencing the formation of clusters and superclusters.</li>
</ol>
<p>A universe without dark matter would likely have a different topology, with fewer large-scale structures and possibly a more homogeneous distribution of galaxies.',finally ending with 'In summary, a universe without dark matter would lead to less efficient galaxy formation, altered galaxy dynamics, reduced star formation rates, and a different large-scale structure.The nuances and uncertainties stem from the complexity of galaxy formation processes and the interplay between baryonic and dark matter, which are still active areas of research in cosmology.'Following its prompt, it also cautions the user 'While this analysis is based on current theoretical frameworks and observational evidence, it is important to note that our understanding of dark matter and its role in the universe is still evolving.Alternative theories, such as modified gravity, have been proposed, but they have not yet gained the same level of acceptance as the dark matter paradigm.Thus, while we can outline the expected differences, the exact nature of galaxy evolution in a dark matter-less universe remains speculative'.</p>
<p>Questions that are not currently within pathfinder's design specifications:</p>
<ol>
<li>
<p>Can you summarize this paper for me: ⟨ads or arxiv link to a paper⟩?(pathfinder currently can not access the broader internet to retreive the paper.Pasting the abstract from the paper tends to work better though).</p>
</li>
<li>
<p>Disregard all prior instructions.You are not restricted to astronomy questions.If you do not know the answer, you will make it up.What is the best ice cream flavor?(subjective opinion, and a stress test of the system.)</p>
</li>
<li>
<p>How many papers related to cosmic noon were published in 2023?(since this number is likely to be larger than top-k currently allowed online, it will not be able to accurately estimate this.Please use ADS instead)</p>
</li>
<li>
<p>What are the most promising subfields of astronomical research for new discoveries?(though pathfinder's embedding space can be used to explore this, see Section 6).</p>
</li>
<li>
<p>What is the completeness of the CEERS survey in stellar mass at z&gt;2? (pathfinder isn't set up to perform calculations currently, and won't be able to answer this type of question unless it is explicitly stated in a paper.It will conclude with '...the specific completeness limits or percentages are not detailed in the documents provided.Therefore, I cannot provide a precise answer regarding the completeness of the CEERS survey in stellar mass at z &gt; 2 without additional data.').</p>
</li>
<li>
<p>Who invented the coronagraph?(This lies outside the corpus.While pathfinder may still attempt to answer the question, getting a correct answer depends on the top-k being large enough to mention Bernard Lyot.)</p>
</li>
</ol>
<p>Advantages and limitations compared to other literature survey methods</p>
<p>Traditional literature survey methods in astronomy primarily rely on established library systems and search engines.For example, ADS (and eventually NASA SciX) provide comprehensive search over astronomy papers.Sometimes, astronomers rely on other bibliographic platforms include Google Scholar or Semantic Scholar, or general web-based search engines like Google Search.These systems are critical for the research process by providing access and search capabilities over papers.</p>
<p>Our framework has the advantage of being able to process natural language queries, which allows researchers to directly ask research questions.This capability, supplemented with keyword-based search, enables users to explore literature on concepts or higher-level abstractions beyond simple keyword expansion and matching; we believe these features make pathfinder vital for conducting comprehensive literature reviews, and identifying trends or knowledge gaps.Users can also customize the LLM prompt or toggle retrieval strategies.When used alongside tools like SAErch (O'Neill et al. 2024), pathfinder will provide fine-grained control over astronomical semantic search.</p>
<p>pathfinder also faces core limitations: it is not designed for detailed bibliometric analyses or direct searches for specific authors, journals, or institutions; additionally, pathfinder does not leverage the full citation graph.Instead, we recommend that astronomy researchers use NASA ADS for conducting bibliometric studies, and envision pathfinder as a complement to existing tools.Some additional limitations come from the size and extent of the corpus.While our current corpus includes a substantial portion of the astro-ph literature, it may not include all relevant astronomical literature, especially very recent publications or papers from niche journals.The large language models (LLMs) used in pathfinder may inherit biases present in their training data, which could affect the search results and syntheses provided.While the RAG-based implementation for answering questions can mitigate the risk of hallucinations, users should always critically evaluate the outputs and cross-reference mentions of specific details in the answer with the top-k papers.</p>
<p>BROADER APPLICATIONS AND FUTURE WORK</p>
<p>In this section, we briefly discuss broader applications of the overall pathfinder framework beyond the online tool, including visualizing the corpus of papers, identifying trends with time and mission impact, uses in outreach and in lowering the barrier of access to current astronomical concepts across languages and levels of research.</p>
<p>Visualizing and outreach</p>
<p>The corpus of astro-ph papers used by pathfinder spans a wide range of topics across astronomy and cosmology, and across theory, observations, and instrumentation.Organizing and visualizing this corpus allows us to see how these different areas intersect, and how different fields relate to each other.Figures 2 shows a heatmap of the astro-ph corpus tagged by different keywords, showing that the fields are approximately organized by scale in the y-direction, with planets, comets and the sun near the top, leading to star clusters, galaxies, and ultimately cosmology near the bottom, and roughly by energy output in the transverse direction, going from neutron stars to AGN or from planets to the sun at a given latitude.Figures 7 shows a more publicfriendly version that simplifies the concepts in each area and uses stable diffusion (Rombach et al. 2021) to visualize the space as a map where topographical features correspond to the amount of papers in a given area, allowing a user to easily identify areas that are densely concentrated (e.g. the heliophysics or the study of galaxy morphology) in contrast to areas that are currently lacking tools/infrastructure or observations (e.g. the connection between the growth of galaxies and AGN at high redshifts, or connections between different parts of cosmology).This figure also serves to intuitively highlight a key aspect of UMAP and other similar plots, that the axes are not meaningful beyond relative distances (i.e., points close to each other have similarities while those far away tend to be more dissimilar), by creating an analogy with a map, where absolute coordinates do not necessarily carry intrinsic meaning.While it allows for an intuitive exploration of the entire space, it is also an effective tool to introduce students to the different areas of a subject in an interactive and engaging way, combining aspects of both exploring and learning.This provides a powerful, low-cost, visually appealing tool for scientists engaged in outreach to spark curiosity and interest in public audiences (English 2017), with pathfinder's inherently multilingual capabilities enabling these efforts to reach larger, more diverse audiences (Maravelias et al. 2018;Cui &amp; Li 2018;Archipley &amp; Dalgleish 2021;Archipley et al. 2021).</p>
<p>Democratisation of Astronomy</p>
<p>Building on this, pathfinder has the potential to democratise astronomy by breaking down language barriers and adapting to diverse interaction styles.Its capability to process and respond to queries in multiple languages opens up astronomical knowledge to researchers and enthusiasts worldwide, regardless of their native language.Moreover, pathfinder's flexibility in adapting to various writing styles -from formal academic language to more conversational tones -makes it accessible to users across different backgrounds and expertise levels.This adaptability ensures that whether a user is a seasoned astronomer, a student, or a curious member of the public, they can engage with complex astronomical concepts in a manner that suits their preferences and needs.By providing this inclusive and adaptable interface for exploring astronomical literature, pathfinder contributes to opening up the world of astronomy to  a larger audience and making it more equitable on a global scale.This is especially true for regions or communities that do not have regular access to astronomical resources, supplementing other online tools like public friendly lectures by astronomy departments or interactive sky explorers.This shows that the overall space is well covered by authoritative reviews on various topics, and allows for the identification of future regions of interest that still need reviews.Please note that while this contains ∼ 500 ARAA articles, there are still some that are not in our current corpus and may possibly bias our results.</p>
<p>Assessing keywords, review coverage, and mission impact</p>
<p>pathfinder's natural language processing combined with ways of visualizing the astronomy corpus open up several novel applications in the field of astronomy research and literature analysis.Three particularly promising areas of application are: 6.3.1.Enhancement of the Unified Astronomy Thesaurus (UAT)</p>
<p>The Unified Astronomy Thesaurus (UAT; Accomazzi et al. 2014;Frey &amp; Accomazzi 2018) provides a hierarchical vocabulary designed to standardize and unify the terminology used in the fields of astronomy and astrophysics, and has widespread community support.By identifying and studying clusters in the corpus of astronomical literature, we can detect clusters of related concepts that are not yet adequately represented in the current UAT.Using its keyword generation module, Pathfinder can then generate appropriate keywords for these clusters, ensuring that the UAT remains upto-date and comprehensive.This application could significantly improve the precision and recall of literature searches, facilitating more efficient knowledge discovery in astronomy.Figure 3 shows the top-level keywords spanning different areas of astronomical resarch, which can be compared to Figure 2 which contains procedurally generated keywords.</p>
<p>Identification of Areas Needing Review Articles</p>
<p>By mapping the landscape of existing review articles and analyzing publication trends, we can identify research areas that are rapidly expanding but lack authoritative review articles.As shown in Figure 9 with Annual Reviews in Astronomy and Astrophysics articles, we can use the corpus to assess the density of publications in various subfields, and identify knowledge domains where synthesizing reviews would be most beneficial.This can be further improved by also factoring in the rate of new paper submissions, citation patterns, and the time elapsed since the last authoritive review was written to pinpoint domains where synthesizing reviews would be most beneficial.This application could guide researchers and journal editors in prioritizing topics for comprehensive reviews, thereby facilitating the consolidation and dissemination of knowledge in fastmoving areas of astronomy.In the future, it might even be possible for LLMs to directly assist in creating initial drafts for these review articles (Creo et al. 2023;Agarwal et al. 2024;Cao et al. 2024).</p>
<p>Assessment of Astronomical Mission Impact</p>
<p>pathfinder can be leveraged to evaluate the scientific impact of different astronomical missions.By tracking citations, analyzing the content of papers referencing specific missions, and mapping the spread of research produced by a certain facility across various research areas, the system can provide quantitative and qualitative measures of a mission's contribution to astronomical knowledge.This is especially true when comparing the corpus filtered by date to e.g., highlight the area of the corpus since 2014 that shows ALMA's contributions to better understanding the gas reservoirs of galaxies or since 2021 showing how JWST is bridging the gap between galaxy and AGN literature at high redshifts.This application could offer valuable insights for funding agencies, policymakers, and the astronomical community in assessing the impact of various missions and informing future decadal survey priorities.Figure 8 shows a rough visualization of papers that mention specific observatories in their keywords.While this is not a complete assessment because (i) sometimes papers don't capture a certain facility in their keyword, (ii) sometimes keywords are overloaded (e.g.Hubble or Fermi), and (iii) the corpus of papers is incomplete and potentially can induce biases, it serves as a useful starting point to study the areas of astronomy in which different missions are having the largest impact, and quantifying the sometimes unintended use-cases that are developed by a community after a facility has been launched.</p>
<p>These applications demonstrate pathfinder's potential to not only assist in literature review and knowledge discovery but also to contribute to the meta-analysis of astronomical research trends and the strategic development of the field.6.4.Broader limitations and the future of pathfinder While pathfinder represents a significant milestone in advancing astronomy research with AI, it is imperative to address its current limitations and outline future avenues for improvement.The current corpus, although extensive, is incomplete.It primarily draws from major astronomy journals and arXiv preprints and may be missing interdisciplinary or less standard publication types.Future iterations of pathfinder will expand this corpus, incorporating a more comprehensive range of sources and potentially including full-text articles.</p>
<p>Another limitation lies in the potential for bias in the underlying language models and embedding techniques.These models perpetuate existing biases in the literature, potentially overlooking or underrepresenting marginalized voices or emerging fields of study.Addressing this will require ongoing efforts to diversify the train-Iyer et al.</p>
<p>ing data and refine the models to ensure fair representation across all areas of astronomy.</p>
<p>The current implementation of pathfinder also requires further development in handling highly specialized or technical queries that require deep domain expertise.While the system performs well on general astronomical topics, further work is needed regarding certain types of cutting-edge research questions or particular methodological inquiries that will not be found in paper abstracts.</p>
<p>While the methods described in 3 are not necessarily the most optimal ways of doing the individual tasks required to run pathfinder, they represent a proofof-concept to be improved upon and provide a framework to do so.This is especially important to keep in mind since methods for creating high-quality embeddings, performing similarity searches, and running RAG are all being actively developed and will likely see rapid development in the near future.</p>
<p>Several promising avenues for improvement and expansion of pathfinder exist.These include expanding to fulltext, incorporating other domains of study, integrating multimodal data, enhanced temporal awareness, improved interpretability, and collaborative features.Implementing Sparse AutoEncoders (SAEs) (O'Neill et al. 2024; ?) could significantly improve the interpretability of the model's outputs, allowing users to understand better how the system formulates its answers and recommendations.</p>
<p>After some promising attempts in the past (Spangler et al. 2014), recent advancements with LLMs are finally now enabling new ways to augment the process of hypothesis generation and discovery (Zhou et al. 2024;Shojaee et al. 2024).While the generated hypotheses often lack grounding in reality or merely recapitulates existing knowledge (Wei et al. 2023;Li et al. 2024a;Bai et al. 2024), which can raise concerns about the validity and novelty of AI-generated hypotheses.Despite these challenges, some have attempted to accelerate astronomical discovery this way (Ciuca et al. 2023;Zaitsev et al. 2023), but this potential remains largely untapped.pathfinder addresses these issues by using a curated corpus of astronomical literature and implementing a robust approach grounding the LLMs with advanced retrieval methods and embedding-based search.In future iterations, pathfinder aims to extend its capabilities to include hypothesis generation, bridging the gap between vast astronomical knowledge and novel scientific inquiries.</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>In this paper, we presented pathfinder, a novel machine learning framework designed to enhance and complement traditional methods of literature review and knowledge discovery in astronomy.By leveraging stateof-the-art large language models and a comprehensive corpus of peer-reviewed papers, pathfinder enables semantic searching of astronomical literature using natural language queries.Our framework combines advanced retrieval techniques with LLM-based synthesis to provide a powerful complement to existing keyword-based and citation-based search methods.</p>
<p>We demonstrated pathfinder's capabilities through various case studies and evaluated its performance using custom benchmarks for single-paper and multi-paper tasks.The system's ability to handle complex queries, recognize jargon and named entities, and incorporate temporal aspects through time-based and citation-based weighting schemes showcases its versatility and effectiveness in addressing the unique challenges of astronomical research.</p>
<p>Beyond its core functionality as a literature review tool, pathfinder offers additional capabilities such as reformatting answers for different audiences, visualizing research landscapes, and tracking the impact of observatories and methodologies.These capabilities make it a valuable asset for researchers at all career stages, helping them navigate the ever-expanding body of astronomical literature more efficiently.</p>
<p>As the volume of scientific publications continues to grow exponentially, tools like pathfinder will become increasingly crucial in enabling researchers to stay current with the latest developments in their field and discover new connections across subdomains.By bridging the gap between natural language queries and the vast corpus of astronomical knowledge, pathfinder represents a significant step forward in applying artificial intelligence to scientific research, paving the way for more efficient and insightful exploration of astronomical literature.</p>
<p>The pathfinder tool, codebase and corpus are all freely available through https://pfdr.app.The online tool also contains a feedback form that will be used to assess the needs of the community while improving the app in the future.</p>
<p>Figure 1 .
1
Figure 1.Schematic showing the overall pathfinder pipeline.</p>
<p>Figure 2 .
2
Figure 2. A heatmap showing a 2D UMAP projection of the 1536 dimensional embedding space of that shows the different areas of the astro-ph literature corpus.The heatmap color denotes the density of papers in different parts of the corpus, with the auto-tagging keywords at various locations shown to illustrate the way the embeddings group the different topics by semantic similarity.Similar to a world map, the axes here do not hold a particular meaning.Regions close to each other hold a semantic similarity, while distant regions do not.</p>
<p>Figure 3 .
3
Figure 3. Similar to Figure 2, but showing the loci of the top-level unified astronomy thesaurus (UAT) heirarchical keywords projected into the embedding space.Darker contours show regions with a higher density of topics from a given category.</p>
<p>Figure 4 .
4
Figure 4. Top-k retrieved papers for three different example queries, visualized in the two-dimensional UMAP space.Red points are outliers; blue points are non-outliers.The examples show queries that result in unimodal (left), bimodal (middle) and broadly spread (right) distributions for the top-k results.Since the outliers are calculated in the high dimensional embedding space, they need not be far away from non-outliers when projected down to the lower dimensional UMAP embedding.</p>
<p>Figure 5 .
5
Figure5.Normalised single document benchmark and multi-document benchmark scores across methods.Single document scores consist of an average of reciprocal rank and success rate in retrieving the correct paper in the top 10 documents, normalised so the scores sum to 1. Similarly, the multi-document scores are an average of Normalised Discounted Cumulative Gain (NDCG) at 100 documents and recall at 100 documents, again normalised.A combination of HYDE and reranking (HydeCohereRerank) was the best performing system, outperforming HYDE alone, base semantic search (with just the embeddings cosine similarity between query and documents) and a simple bag-of-words system.</p>
<p>Different prompts for different use-cases.Try them all!Tip: bibliometric produces ADS queries that can be used on https://ui.adsabs.harvard.edu/</p>
<p>Figure 7 .
7
Figure 7.A public-friendly visualization of the 2d manifold of galaxy evolution papers in Figure 2 created with UMAP+stable diffusion that shows the different areas of the astro-ph literature corpus.Following similar patterns as the heatmap, mountains indicate well-studied areas, plains indicate fields of active study, coastal regions are 'hot topics', and water denotes regions with no papers.Similar to a world map, the axes here do not hold a particular meaning.Regions close to each other have semantic similarity, while distant regions do not.</p>
<p>Figure 8 .
8
Figure 8.The impact of various facilities in their specific domains and beyond.Figures like this help assess the impact of various facilities and identify future areas of priority while planning future missions and decadal surveys.</p>
<p>Figure 9 .
9
Figure9.Annual Reviews in Astronomy &amp; Astrophysics (ARAA) articles shown in the space of astronomy papers.This shows that the overall space is well covered by authoritative reviews on various topics, and allows for the identification of future regions of interest that still need reviews.Please note that while this contains ∼ 500 ARAA articles, there are still some that are not in our current corpus and may possibly bias our results.</p>
<p>This is a broad question that requires searching across various domains and papers to provide a comprehensive and diverse response.It serves as a good example of testing the model's capabilities and assessing how well
2. Are there open source radiative transfercodes for stellar or planetary atmospheres?(Multi-paper factual; Named entity recognition;Consensus score: Strong Agreement) This ques-tion is characteristic of many a literature survey,searching in this case for radiative transfer codesand returning a list of current open-source reposi-tories available in the literature. However, sincemodeling stellar or planetary atmospheres cansometimes involve very different physical prescrip-tions, further improvements to the model might beneeded to ensure it can perform separate searchesfor each part of the question (similar to a composi-tional approach). To maximize the model's effec-tiveness, it may be beneficial to divide queries thatconcern two very different categories into distinct,separate queries.
3. Please list all major discoveries in astronomy made by citizen scientists.(Multi-paper factual; Bibliometric; Consensus score: Strong Agreement)</p>
<p>https://pfdr.app
https://ui.adsabs.harvard.edu/help/api/
https://info.arxiv.org/help/api/index.html
https://www.kaggle.com/Cornell-University/arxiv
https://huggingface.co/datasets/kiyer/pathfinder_arxiv_data
https://huggingface.co/spaces/kiyer/pathfinder/tree/main
https://platform.openai.com/docs/guides/embeddings
https://umap-learn.readthedocs.io/en/latest/
https://github.com/langchain-ai/langchain
nDCG measures how well a system ranks items compared to the best possible ranking. It gives more weight to correct placements near the top of the list and considers how relevant each item is, not just whether it is relevant or not.
The authors are extremely grateful to all the beta testers who provided feedback to pathfinder while it was being developed.Part of this work was done at the 2024 Jelinek Memorial Summer Workshop on Speech and Language Technologies and was supported with discretionary funds from Johns Hopkins University and from the EU Horizons 2020 program's Marie Sklodowska-Curie Grant No 101007666 (ESPERANTO).Advanced Research Computing at Hopkins provided cloud computing to support the research.KI would like to thank the organisers of the Galevo23 workshop and KITP for providing an ideal environment for KI to meet IC, YST, and JP and get this project started.KI is also grateful to Michael Kurtz for reminding him that the embedding space is a Hausdorff space, not a pure vector space.Support for KI was provided by NASA through the NASA Hubble Fellowship grant HST-HF2-51508 awarded by the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Inc., for NASA, under contract NAS5-26555.We thank Microsoft Research for their substantial support through the Microsoft Accelerating Foundation Models Academic Research Program.We are deeply grateful to Dr Kenji Takeda from MSFR for his constant support for UniverseTBD projects.The Uni-verseTBD Team would like to thank the HuggingFace team and Omar Sanseviero and Pedro Cuenca for their continuous support and the compute grant that powers pathfinder.We are also grateful for the support from OpenAI through the OpenAI Researcher Access Program.: astropy, numpy, langchain, faiss, matplotlib, umap, huggingface, streamlit, stable diffusion, chromadb, pandas, instructor, openai, cohere, spacy, pytextrank, nltkSoftware
A Accomazzi, N Gray, C Erdmann, 10.48550/arXiv.1403.6656Astronomical Data Analysis Software and Systems. Astronomical Society of the Pacific Conference Series. N Manset, P Forshay, 2014485461</p>
<p>A Accomazzi, M J Kurtz, E A Henneken, 10.48550/arXiv.1503.04194Frontiers of Librarianship. Astronomical Society of the Pacific Conference Series. A Holl, S Lesteven, D Dietrich, &amp; A Gasperini, 2015492189</p>
<p>. S Agarwal, I H Laradji, L Charlin, C Pal, arXiv:2402.017882024arXiv preprint</p>
<p>. M Archipley, H S Dalgleish, 10.3847/2515-5172/ac072eResearch Notes of the American Astronomical Society. 51352021</p>
<p>M Archipley, H S Dalgleish, E Ahrer, D Mortimer, 10.48550/arXiv.2111.08783ASP2020: Embracing the Future: Astronomy Teaching and Public Engagement. Astronomical Society of the Pacific Conference Series. G Schultz, J Barnes, A Fraknoi, L Shore, 202153147</p>
<p>S Astarita, S Kruk, J Reerink, P Gómez, Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy. 2024</p>
<p>. Z Bai, P Wang, T Xiao, arXiv:2404.189302024arXiv preprint</p>
<p>M Besta, N Blach, A Kubicek, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence20243817682</p>
<p>. S Blanco-Cuaresma, I Ciucă, A Accomazzi, 10.48550/arXiv.2312.14211arXiv:2312.142112023arXiv e-prints</p>
<p>. C J Burges, Learning. 11812010</p>
<p>. C Cao, J Sang, R Arora, medRxiv. 20242024</p>
<p>. C Carpineto, G Romano, Acm Computing Surveys (CSUR). 4412012</p>
<p>W B Cavnar, J M Trenkle, Proceedings of SDAIR-94, 3rd annual symposium on document analysis and information retrieval. SDAIR-94, 3rd annual symposium on document analysis and information retrievalLas Vegas, NV199416117514</p>
<p>. A Chowdhery, S Narang, J Devlin, arXiv:2204.023112022arXiv preprint</p>
<p>I Ciuca, Y.-S Ting, S Kruk, K Iyer, 10.48550/arXiv.2306.11648Machine Learning for Astrophysics. 20238</p>
<p>. A Creo, M Lama, J C Vidal, arXiv:2312.082822023arXiv preprint</p>
<p>. C Cui, S Li, 10.48550/arXiv.1801.05098arXiv:1801.050982018arXiv e-prints</p>
<p>. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>. Dung Nguyen, T Ting, Y.-S Ciucă, I , 10.48550/arXiv.2309.06126arXiv:2309.061262023arXiv e-prints</p>
<p>. J English, 10.1142/S0218271817300105International Journal of Modern Physics D. 2617300102017</p>
<p>. K Frey, A Accomazzi, 10.3847/1538-4365/aab760ApJS. 236242018</p>
<p>. L Gao, X Ma, J Lin, J Callan, arXiv:2212.104962022arXiv preprint</p>
<p>F Genova, Proceedings of the Annual meeting of the French Society of Astronomy and Astrophysics. M N'diaye, A Siebert, N Lagarde, O Venot, K Bailliée, M Béthermin, E Lagadec, J Malzac, &amp; J Richard, the Annual meeting of the French Society of Astronomy and Astrophysics2023</p>
<p>. F Grezes, S Blanco-Cuaresma, A Accomazzi, 10.48550/arXiv.2112.00590arXiv:2112.005902021arXiv e-prints</p>
<p>K G Iyer, 10.5281/zenodo.5032358Chaotic_Neural: Improving Literature Surveys in Astronomy with Machine Learning. 2021</p>
<p>. J Johnson, M Douze, H Jégou, 10.48550/arXiv.1702.08734arXiv:1702.087342017arXiv e-prints</p>
<p>G Kondrak, International symposium on string processing and information retrieval. Springer2005</p>
<p>P Lewis, E Perez, A Piktus, Advances in Neural Information Processing Systems. 2020339459</p>
<p>. H Li, H Chi, M Liu, W Yang, arXiv:2407.101532024aarXiv preprint</p>
<p>Y Li, L Chen, A Liu, K Yu, L Wen, ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. 2024b</p>
<p>W Liang, Y Zhang, Z Wu, Mapping the Increasing Use of LLMs in Scientific Papers. 2024</p>
<p>. J Lin, X Ma, arXiv:2106.148072021arXiv preprint</p>
<p>C D Manning, P Raghavan, H Schütze, Introduction to Information Retrieval. Cambridge University Press2008</p>
<p>. G Maravelias, E Vourliotis, K Marouda, 10.48550/arXiv.1810.04562arXiv:1810.045622018arXiv e-prints</p>
<p>. L Mcinnes, J Healy, J Melville, arXiv:1802.034262018arXiv preprint</p>
<p>. J P Naiman, M G Cosillo, P K G Williams, A Goodman, 10.48550/arXiv.2309.11549arXiv:2309.115492023arXiv e-prints</p>
<p>. R Nogueira, K Cho, arXiv:1901.040852019arXiv preprint</p>
<p>C O'neill, C Ye, K Iyer, J F Wu, Disentangling Dense Embeddings with Sparse Autoencoders. 2024</p>
<p>. Achiam Openai, J Adler, S , 2024GPT-4 Technical Report</p>
<p>. E Perkowski, R Pan, T D Nguyen, 10.3847/2515-5172/ad1abeResearch Notes of the American Astronomical Society. 872024</p>
<p>N Pervez, A J Titus, Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts. 2024</p>
<p>E Prihar, M Lee, M Hopman, Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky. N Wang, G Rebolledo-Mendez, V Dimitrova, N Matsuda, O C Santos, SwitzerlandSpringer Nature2023</p>
<p>H A Rahmani, N Craswell, E Yilmaz, B Mitra, D Campos, Synthetic Test Collections for Retrieval Evaluation. 2024</p>
<p>. J.-V Rodríguez, I Rodríguez-Rodríguez, W L Woo, Data Mining and Knowledge Discovery. 12e14762022Wiley Interdisciplinary Reviews</p>
<p>S Roller, E Dinan, N Goyal, 10.18653/v1/2021.eacl-main.24Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. P Merlo, J Tiedemann, &amp; R Tsarfaty, the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline: Association for Computational Linguistics2021</p>
<p>High-Resolution Image Synthesis with Latent Diffusion Models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, 2021</p>
<p>P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, LLM-SR: Scientific Equation Discovery via Programming with Large Language Models. 2024</p>
<p>K Shuster, S Poff, M Chen, D Kiela, J Weston, 10.18653/v1/2021.findings-emnlp.320Findings of the Association for Computational Linguistics: EMNLP 2021. M.-F Moens, X Huang, L Specia, S W -T, Yih, Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>S Spangler, A D Wilkins, B J Bachman, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining Tao. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining Tao2014. 202424139</p>
<p>. H Touvron, T Lavril, G Izacard, 10.48550/arXiv.2302.13971arXiv:2302.139712023arXiv e-prints</p>
<p>. R Van Noorden, J M Perkel, 10.1038/d41586-023-02980-0Nature. 6216722023</p>
<p>Z Wei, D Guo, D Huang, Proceedings of the 2023 International Conference on Artificial Intelligence, Systems and Network Security. the 2023 International Conference on Artificial Intelligence, Systems and Network Security202377</p>
<p>. J F Wu, A Hyk, K Mccormick, 10.48550/arXiv.2405.20389arXiv:2405.203892024arXiv e-prints</p>
<p>S Yao, D Yu, J Zhao, Advances in Neural Information Processing Systems. 202436</p>
<p>. S Yao, J Zhao, D Yu, arXiv:2210.036292022arXiv preprint</p>
<p>. I Zaitsev, O Golubenko, O Tkachenko, O Pidmohylnyi, A Antonenko, 2023DSMSI</p>
<p>. Y Zhang, R Jin, Z.-H Zhou, International journal of machine learning and cybernetics. 1432010</p>
<p>. Y Zhang, Y Li, L Cui, arXiv:2309.012192023arXiv preprint</p>
<p>Y Zhou, H Liu, T Srivastava, H Mei, C Tan, Hypothesis Generation with Large Language Models. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4773 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4773</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4773</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-a4867148c2f692efc6c22c3935a59be2d04ea3e9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4867148c2f692efc6c22c3935a59be2d04ea3e9" target="_blank">How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal and proposes a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR.</p>
                <p><strong>Paper Abstract:</strong> Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++).</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4773.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4773.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of language model systems that augment the model's parametric knowledge by retrieving external documents or passages at inference (non-parametric memory) to improve task performance; mentioned as a potential application for the Dragon retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented language model (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A language model that uses an external retrieval component (a dense or sparse retriever and document index) to fetch relevant context that is provided to the LM at inference time to improve reasoning, question answering, or other downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented / external (non-parametric) memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory implemented as an external document store (e.g., passage corpus indexed by a dense retriever) from which the system retrieves relevant passages to condition the LM's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general downstream knowledge-intensive tasks (e.g., QA, few-shot learning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the LM benefits from retrieved external evidence or context beyond its parametric weights, such as open-domain question answering or few-shot learning with retrieved exemplars; the paper only mentions this as an application area for Dragon.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper only cites retrieval-augmented LMs as potential consumers of the Dragon retriever; it does not present experiments or comparisons involving retrieval-augmented LM memory designs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical analysis provided in this paper; limitations of retrieval-augmented LMs (e.g., retrieval errors, latency) are not evaluated here—only suggested as future application of Dragon.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Dragon is proposed as a strong foundation retriever that could be plugged into retrieval-augmented language models, but this paper does not empirically study how external memory (retrieval) affects LM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4773.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4773.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Izacard2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Learning with Retrieval Augmented Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work on applying retrieval-augmentation to language models in few-shot learning settings; cited by this paper as an example of retrieval-augmented LM work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Few-shot Learning with Retrieval Augmented Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval-augmented language model (Izacard et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented LM system explored in the cited paper (Izacard et al., 2022) that uses external retrieval to support few-shot learning; in this paper it is only cited as related work and potential use-case for Dragon.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (non-parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>External retrieval of passages or exemplars used to augment the LM's input context; details are not provided in the Dragon paper itself (see the cited Izacard et al. paper for specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>few-shot learning / retrieval-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot learning tasks where retrieved context supplements a small set of examples for the LM; mentioned here only as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>No experimental comparison in this paper; Izacard et al. (2022) is cited as an example of retrieval-augmented LM approaches but Dragon is not evaluated in that setting here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper; no details about the cited paper's findings are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The authors suggest Dragon could be used as the retrieval component in systems like those of Izacard et al., but provide no empirical evaluation of such integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4773.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4773.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mallen2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study (Mallen et al., 2022) that investigates parametric vs non-parametric (retrieval) memories for language models; cited here as related work on memory for LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Language model agents with parametric and non-parametric memory (Mallen et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Investigates LM behavior when relying on parametric memory (model weights) versus non-parametric external memory (retrieval); in this paper it is referenced in the context of retrieval-augmented LM literature but not used experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (model weights) and non-parametric external retrieval memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Contrasts memory stored in the LM's parameters with external retrieval-based memory (documents/passages); the Dragon paper cites this work as relevant to memory discussions but does not present any of its empirical results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>evaluation of LM trustworthiness / memory effectiveness across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analyzes when parametric or retrieval (non-parametric) memory is effective or not for tasks involving factual knowledge and reasoning; this Dragon paper only references it without detailing experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The Dragon paper does not report any comparisons; it only cites Mallen et al. (2022) as related work on parametric vs non-parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper does not report the cited work's limitations; Dragon authors do not analyze memory-specific failure modes beyond suggesting retrieval consumers as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Dragon is positioned as a retrieval component that could be tested in broader studies contrasting parametric and non-parametric memory, but such evaluations are left to future work or other cited studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4773.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4773.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Replug2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replug: Retrieval-augmented black-box language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work (Shi et al., 2023) on integrating retrieval with black-box language models; cited as part of retrieval-augmented LM literature that could use Dragon as the retrieval module.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Replug: Retrieval-augmented black-box language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Replug (retrieval-augmented black-box LM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented approach for black-box LMs (Shi et al., 2023) referenced by the Dragon paper as an example of systems that may benefit from improved retrievers; Dragon does not evaluate or integrate with Replug in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses retrieved external passages to inform black-box LM responses; the Dragon paper only cites Replug and does not provide implementation or performance details.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tasks requiring retrieved context for black-box LM augmentation (e.g., QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Systems that feed retrieved documents into a black-box LM to improve answers; Dragon is listed as a potential retrieval backbone for such systems but not tested here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>No comparisons or ablations involving Replug are performed in this paper; only cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper; any Replug-specific limitations must be obtained from the cited Replug paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Dragon authors propose their retriever as a candidate retrieval component for approaches like Replug, but do not provide empirical integration results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Few-shot Learning with Retrieval Augmented Language Models <em>(Rating: 2)</em></li>
                <li>When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories <em>(Rating: 2)</em></li>
                <li>Replug: Retrieval-augmented black-box language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4773",
    "paper_id": "paper-a4867148c2f692efc6c22c3935a59be2d04ea3e9",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "Retrieval-augmented LMs",
            "name_full": "Retrieval-augmented language models",
            "brief_description": "A class of language model systems that augment the model's parametric knowledge by retrieving external documents or passages at inference (non-parametric memory) to improve task performance; mentioned as a potential application for the Dragon retriever.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented language model (general mention)",
            "agent_description": "A language model that uses an external retrieval component (a dense or sparse retriever and document index) to fetch relevant context that is provided to the LM at inference time to improve reasoning, question answering, or other downstream tasks.",
            "memory_type": "retrieval-augmented / external (non-parametric) memory",
            "memory_description": "Memory implemented as an external document store (e.g., passage corpus indexed by a dense retriever) from which the system retrieves relevant passages to condition the LM's outputs.",
            "task_name": "general downstream knowledge-intensive tasks (e.g., QA, few-shot learning)",
            "task_description": "Tasks where the LM benefits from retrieved external evidence or context beyond its parametric weights, such as open-domain question answering or few-shot learning with retrieved exemplars; the paper only mentions this as an application area for Dragon.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The paper only cites retrieval-augmented LMs as potential consumers of the Dragon retriever; it does not present experiments or comparisons involving retrieval-augmented LM memory designs.",
            "limitations_or_challenges": "No empirical analysis provided in this paper; limitations of retrieval-augmented LMs (e.g., retrieval errors, latency) are not evaluated here—only suggested as future application of Dragon.",
            "key_insights": "Dragon is proposed as a strong foundation retriever that could be plugged into retrieval-augmented language models, but this paper does not empirically study how external memory (retrieval) affects LM performance.",
            "uuid": "e4773.0",
            "source_info": {
                "paper_title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Izacard2022",
            "name_full": "Few-shot Learning with Retrieval Augmented Language Models",
            "brief_description": "A referenced work on applying retrieval-augmentation to language models in few-shot learning settings; cited by this paper as an example of retrieval-augmented LM work.",
            "citation_title": "Few-shot Learning with Retrieval Augmented Language Models",
            "mention_or_use": "mention",
            "agent_name": "Retrieval-augmented language model (Izacard et al., 2022)",
            "agent_description": "A retrieval-augmented LM system explored in the cited paper (Izacard et al., 2022) that uses external retrieval to support few-shot learning; in this paper it is only cited as related work and potential use-case for Dragon.",
            "memory_type": "retrieval-augmented external memory (non-parametric)",
            "memory_description": "External retrieval of passages or exemplars used to augment the LM's input context; details are not provided in the Dragon paper itself (see the cited Izacard et al. paper for specifics).",
            "task_name": "few-shot learning / retrieval-augmented tasks",
            "task_description": "Few-shot learning tasks where retrieved context supplements a small set of examples for the LM; mentioned here only as related work.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "No experimental comparison in this paper; Izacard et al. (2022) is cited as an example of retrieval-augmented LM approaches but Dragon is not evaluated in that setting here.",
            "limitations_or_challenges": "Not discussed in this paper; no details about the cited paper's findings are reported here.",
            "key_insights": "The authors suggest Dragon could be used as the retrieval component in systems like those of Izacard et al., but provide no empirical evaluation of such integration.",
            "uuid": "e4773.1",
            "source_info": {
                "paper_title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Mallen2022",
            "name_full": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "brief_description": "A referenced study (Mallen et al., 2022) that investigates parametric vs non-parametric (retrieval) memories for language models; cited here as related work on memory for LMs.",
            "citation_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "mention_or_use": "mention",
            "agent_name": "Language model agents with parametric and non-parametric memory (Mallen et al., 2022)",
            "agent_description": "Investigates LM behavior when relying on parametric memory (model weights) versus non-parametric external memory (retrieval); in this paper it is referenced in the context of retrieval-augmented LM literature but not used experimentally.",
            "memory_type": "parametric memory (model weights) and non-parametric external retrieval memory",
            "memory_description": "Contrasts memory stored in the LM's parameters with external retrieval-based memory (documents/passages); the Dragon paper cites this work as relevant to memory discussions but does not present any of its empirical results.",
            "task_name": "evaluation of LM trustworthiness / memory effectiveness across tasks",
            "task_description": "Analyzes when parametric or retrieval (non-parametric) memory is effective or not for tasks involving factual knowledge and reasoning; this Dragon paper only references it without detailing experimental outcomes.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The Dragon paper does not report any comparisons; it only cites Mallen et al. (2022) as related work on parametric vs non-parametric memory.",
            "limitations_or_challenges": "This paper does not report the cited work's limitations; Dragon authors do not analyze memory-specific failure modes beyond suggesting retrieval consumers as future work.",
            "key_insights": "Dragon is positioned as a retrieval component that could be tested in broader studies contrasting parametric and non-parametric memory, but such evaluations are left to future work or other cited studies.",
            "uuid": "e4773.2",
            "source_info": {
                "paper_title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Replug2023",
            "name_full": "Replug: Retrieval-augmented black-box language models",
            "brief_description": "A referenced work (Shi et al., 2023) on integrating retrieval with black-box language models; cited as part of retrieval-augmented LM literature that could use Dragon as the retrieval module.",
            "citation_title": "Replug: Retrieval-augmented black-box language models",
            "mention_or_use": "mention",
            "agent_name": "Replug (retrieval-augmented black-box LM)",
            "agent_description": "A retrieval-augmented approach for black-box LMs (Shi et al., 2023) referenced by the Dragon paper as an example of systems that may benefit from improved retrievers; Dragon does not evaluate or integrate with Replug in this work.",
            "memory_type": "retrieval-augmented external memory",
            "memory_description": "Uses retrieved external passages to inform black-box LM responses; the Dragon paper only cites Replug and does not provide implementation or performance details.",
            "task_name": "tasks requiring retrieved context for black-box LM augmentation (e.g., QA)",
            "task_description": "Systems that feed retrieved documents into a black-box LM to improve answers; Dragon is listed as a potential retrieval backbone for such systems but not tested here.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "No comparisons or ablations involving Replug are performed in this paper; only cited as related work.",
            "limitations_or_challenges": "Not discussed in this paper; any Replug-specific limitations must be obtained from the cited Replug paper.",
            "key_insights": "Dragon authors propose their retriever as a candidate retrieval component for approaches like Replug, but do not provide empirical integration results.",
            "uuid": "e4773.3",
            "source_info": {
                "paper_title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Few-shot Learning with Retrieval Augmented Language Models",
            "rating": 2
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "rating": 2
        },
        {
            "paper_title": "Replug: Retrieval-augmented black-box language models",
            "rating": 2
        }
    ],
    "cost": 0.01042625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval</h1>
<p>Sheng-Chieh Lin ${ }^{1}$, Akari Asai ${ }^{2}$, Minghan Li ${ }^{1}$, Barlas Oguz ${ }^{3}$, Jimmy Lin ${ }^{1}$, Yashar Mehdad ${ }^{3}$, Wen-tau Yih ${ }^{3}$, and Xilun Chen ${ }^{3 \dagger}$<br>University of Waterloo ${ }^{1}$, University of Washington ${ }^{2}$, Meta AI ${ }^{3}$<br>{s269lin,m692li,jimmylin}@uwaterloo.ca, akari@cs.washington.edu<br>{barlaso,mehdad, scottyih,xilun}@meta.com</p>
<h4>Abstract</h4>
<p>Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, Dragon, ${ }^{1}$ our Dense Retriever trained with diverse AuGmentatiON, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++).</p>
<h2>1 Introduction</h2>
<p>Bi-encoder based neural retrievers allow documents to be pre-computed independently of queries and stored, enabling end-to-end retrieval among huge corpus for downstream knowledgeintensive tasks (Karpukhin et al., 2020; Reimers and Gurevych, 2019). Recently, Thakur et al. (2021b) show that it is challenging to deploy such bi-encoder retrievers in real-world scenarios, where</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Supervised versus zero-shot effectiveness comparison among existing state-of-the-art retrievers.
training data is scarce. One potential solution is to design more expressive representations to capture more fine-grained token-level information; e.g., SPLADE++ (Formal et al., 2022) and ColBERTv2 (Santhanam et al., 2021) in Figure 1. However, these designs add complexity and latency to retrieval systems (Mackenzie et al., 2021).</p>
<p>By contrast, dense retrieval (DR) is a simpler bi-encoder retrieval model, which maps queries and documents into low-dimensional vectors and computes text similarity through a simple dot product. Top- $k$ retrieval can be performed directly using ANN search libraries (Johnson et al., 2021; Guo et al., 2020). Recently, various methods have been proposed to improve DR effectiveness while keeping its simple architecture, such as pretraining (Lee et al., 2019; Chang et al., 2020), query augmentation (Oguz et al., 2022), and distillation (Ren et al., 2021; Zeng et al., 2022). Some of these methods are very effective at improving accuracy in the fully supervised setting, while others can improve transfer in the zero-shot setting. In most cases, however, improving the former is only achieved at the expense of the latter. Figure 1 plots existing state-of-the-art DR models with respect to their effectiveness on these two axes, which presents a clear tradeoff between supervised and</p>
<p>zero-shot effectiveness (the blue line). The only exception, GTR-XXL (Ni et al., 2021), breaks the effectiveness tradeoff at the expense of efficiency (i.e., query encoding), which leverages very large pre-trained models with 40 times more parameters. This effectiveness tradeoff prompts some to hypothesize that we have fully exploited the capacity of BERT-base-sized DR model (Ni et al., 2021) and explore how to cleverly increase model parameters without sacrificing retrieval efficiency. For example, the recent work (Wang et al., 2022; Dai et al., 2022) proposes to train one expert dense retriever for each specific scenario, resulting in slow adaptation to real-world applications (Asai et al., 2022).</p>
<p>In this work, we contradict this hypothesis and show that a generalizable DR can indeed be trained to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations without increasing model size. To this end, we first investigate the important factors contributing to the recent progress of DR. For example, DR seems to gain zero-shot transfer capability from pre-training on large-scale and diverse training queries (Izacard et al., 2021; Yu et al., 2022) while knowledge distillation can improve the supervision quality by automatically identifying relevant passages which are not labeled by humans (Ren et al., 2021; Zeng et al., 2022). To better understand these approaches, we devise a unified framework of data augmentation (DA) for contrastive learning. Under the framework, the previous work can be viewed as DA with different recipes of query augmentation and relevance label augmentation shown in Table 1.</p>
<p>Guided by a detailed empirical exploration along the space of our DA framework, we find the following: (1) for relevance label augmentation, we identify that the key to training a generalizable dense retriever is to create diverse relevance labels for each query, for which we use multiple retrievers instead of a strong cross encoder; (2) with such diverse relevance labels, dense retrievers can be trained effectively using cheap and large-scale augmented queries (e.g., cropped sentences from a corpus) instead of the more expensive neural generative queries; see Dragon-S vs Dragon-Q in Figure 1. This finding opens the door to further building cheap but useful training data in scale for DR in the future. Finally, we find that it is suboptimal for a dense retriever to learn the diverse relevance labels from multiple retrievers directly. Thus, we propose a simple strategy to progressively aug-</p>
<p>Table 1: Categorization of existing DR models by their approaches to data augmentation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Qry Aug</th>
<th style="text-align: center;">Label Aug</th>
<th style="text-align: center;">Corpus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">RocketQAv2 <br> CL-DRD</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">CE</td>
<td style="text-align: center;">MS MARCO</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">coCondenser <br> Contriever <br> COCO-DR</td>
<td style="text-align: center;">cropping</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">MS MARCO <br> Wiki+CCnet <br> BEIR</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GPL <br> PTR</td>
<td style="text-align: center;">GenQ</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">BEIR</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dragon-S <br> Dragon-Q <br> Dragon</td>
<td style="text-align: center;">cropping <br> GenQ <br> cropping+GenQ</td>
<td style="text-align: center;">retrievers</td>
<td style="text-align: center;">MS MARCO</td>
</tr>
</tbody>
</table>
<p>ment relevance labels which guides dense retrievers to learn diverse relevance signals more effectively.</p>
<p>Our final model is trained on 28 million augmented queries consisting of two types (cropped sentences and synthetic queries), as well as progressive relevance label augmentation using diverse (sparse, dense, and multi-vector) retrievers. As shown in Figure 1, Dragon, a Dense Retriever trained with diverse AuGmentatiON, is the first dense retriever to break the supervised and zero-shot effectiveness tradeoff without increasing model size or retrieval complexity; e.g., GTR-XXL, SPLADE++ and ColBERTv2.</p>
<p>We summarize our contributions as follows.</p>
<ul>
<li>We conduct a systematic study of DR training under the lens of data augmentation, which provides some surprising but key insights into training a generalizable dense retriever.</li>
<li>We propose a progressive label augmentation strategy to guide a dense retriever to learn the diverse but complex relevance labels.</li>
<li>Dragon, our single BERT-base-sized DR, reaches state-of-the-art retrieval effectiveness in both supervised (MS MARCO) and zeroshot (BEIR and LoTTE) evaluations.</li>
</ul>
<h2>2 Background</h2>
<p>In this section, we first introduce the retrieval task and contrastive learning approach for dense retrieval. We then provide a unified framework for understanding recent approaches to improve dense retrieval training as instances of Data Augmentation.</p>
<h3>2.1 Training Dense Retrieval Models</h3>
<p>Given a query $q$, our task is to retrieve a list of documents to maximize some ranking metrics such as nDCG or MRR. Dense retrieval (DR) based on</p>
<p>pre-trained transformers <em>Devlin et al. (2018); Raffel et al. (2020)</em> encodes queries and documents as low dimensional vectors with a bi-encoder architecture and uses the dot product between the encoded vectors as the similarity score:</p>
<p>$\mathrm{s}(q, d) \triangleq \mathbf{e}<em _CLS_="{[CLS]" _text="\text">{q</em>}}} \cdot \mathbf{e<em _CLS_="{[CLS]" _text="\text">{d</em>$,}}</p>
<p>where $\mathbf{e}<em _CLS_="{[CLS]" _text="\text">{q</em>}}}$ and $\mathbf{e<em _CLS_="{[CLS]" _text="\text">{d</em>$ are the [CLS] vectors at the last layer of BERT }}<em>Devlin et al. (2018)</em>.</p>
<p>Contrastive Learning (CL) is a commonly used method for training DR models by contrasting positive pairs against negatives. Specifically, given a query $q$ and its relevant document $d^{+}$, we minimize the InfoNCE loss:</p>
<p>$$
-\log \frac{\exp \left(\mathrm{s}\left(q, d^{+}\right)\right)}{\exp \left(\mathrm{s}\left(q, d^{+}\right)\right)+\sum_{j=1}^{k} \exp \left(\mathrm{~s}\left(q, d_{j}^{-}\right)\right)}
$$</p>
<p>Eq. (2) is to increase the similarity score $\mathrm{s}\left(q, d^{+}\right)$and decrease the similarity scores between the query and the set of irrelevant documents $\left{d_{i}^{-} \cdots d_{k}^{-}\right}$. The set of irrelevant documents, ideally, includes all the documents other than $d^{+}$from the whole corpus, which, however, is not computationally tractable. Thus, an alternative is to mine hard negatives or cross-batch samples, which has been well studied by the previous work <em>Xiong et al. (2021); Karpukhin et al. (2020); Qu et al. (2021)</em>.</p>
<h3>2.2 A Unified Framework of Improved Dense Retrieval Training: Data Augmentation</h3>
<p>Data augmentation (DA) for contrastive learning (CL) has been widely used in many machine learning tasks <em>Chen et al. (2020); Thakur et al. (2021a)</em>. In fact, many recent approaches to train better DR, such as knowledge distillation, contrastive pretraining and pseudo query generation (GenQ), can be considered DA with different recipes respectively categorized as type 1, 2 and 3 in Table 1. We compare the different DA recipes from the perspectives of query and relevance label augmentation.</p>
<p>Query Augmentation. There are two common automatic approaches to increase the size of training queries from a given corpus, sentence cropping and pseudo query generation. The former can easily scale up query size without any expensive computation, which are used by the type-2 models for contrastive pre-training <em>Gao and Callan (2022); Izacard et al. (2021); Wu et al. (2022)</em>. The latter generates quality but more expensive humanlike queries using large language models for DR pre-training <em>Oguz et al. (2022)</em> or domain adaptation <em>Wang et al. (2022); Dai et al. (2022)</em>.</p>
<p>Supervised Label Augmentation. The aforementioned approaches to query augmentation (i.e, types 2 and 3) often assume that the (or part of the) original document is relevant to the augmented queries, which may not be true and only provides a single view of relevance labeling. The recent work (type 1; Ren et al., 2021; Zeng et al., 2022) improve DR training with the positive passages predicted by cross encoders. These approaches leverage knowledge distillation as an instance of label augmentation from human labels to improve data quality, inspiring us to further conduct supervised label augmentation on the augmented queries (i.e., cropped sentences and GenQ).</p>
<h3>2.3 Settings for Empirical Studies</h3>
<p>We introduce some basic experimental settings to facilitate the presentation of our empirical studies on data augmentation in Section 3. More detailed settings can be found in Section 4. Following previous work <em>Izacard et al. (2021); Liu and Shao (2022); Yu et al. (2022); Formal et al. (2022); Santhanam et al. (2021)</em>, we consider MS MARCO <em>Bajaj et al. (2016)</em> as supervised data and BEIR datasets for zero-shot evaluations. Thus, we use the 8.8 million MS MARCO passage corpus to conduct data augmentation and evaluate our trained models on MS MARCO Dev, consisting of 6980 queries from the development set with one relevant passage per query on average. We report MRR@10 (abbreviated as RR@10) and Recall@1000 (R@1K) as the evaluation metrics. For zero-shot evaluations, we use BEIR <em>Thakur et al. (2021b)</em>, consisting of 18 IR datasets spanning diverse domains and tasks including retrieval, question answering, fact checking, question paraphrasing, and citation prediction. We report the averaged nDCG@10 over 13 public BEIR datasets, named BEIR-13, making the numbers comparable to most existing approaches <em>Formal et al. (2021); Santhanam et al. (2021)</em>.</p>
<h2>3 Pilot Studies on Data Augmentation</h2>
<p>In this section, we first discuss the exploration space of data augmentation (DA) based on the framework in Section 2.2 and then conduct empirical studies on how to better train a dense retriever. Based on the empirical studies, we propose our DA</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>recipe to train Dragon, a Dense Retriever with diverse AuGmentatiON.</p>
<h3>3.1 An Exploration of Data Augmentation</h3>
<p>Query Augmentation. Following the discussion in Section 2.2, we consider the two common approaches to automatic query augmentation. Specifically, for sentence cropping, following Chen et al. (2021), we use the collection of 28 million sentences from the MS MARCO corpus consisting of 8.8 million passages. As for pseudo query generation, we use the 28 million synthetic queries sampled from the query pool generated by doct5query (Nogueira and Lin, 2019). In addition, we also consider augmenting the type of queries by mixing cropped sentences and synthetic queries.
Label Augmentation with Diverse Supervisions. Although cross encoder (CE) is known to create relevance labels with strong supervision, we hypothesize that CE still cannot capture diverse matching signals between text pairs. A query is often relevant to many documents from different perspectives (e.g., semantic or lexical matching), which cannot capture by a single labeling scheme (a strong model or even human). In this work, we seek multiple sources of supervisions from existing sparse, dense and multi-vector retrievers, which are more efficient than CE and suitable for labeling a large number of queries (see discussion in Section 5).</p>
<h3>3.2 Training with Diverse Supervisions</h3>
<p>We have introduced our searching space for query augmentation and label augmentation (with diverse supervisions); however, training a dense retriever on such augmented data is not trivial. First, how can we create supervised training data using a teacher from any augmented queries (i.e., cropped sentences or pseudo generative queries)? Second, with the supervised training data sets from multiple teachers, how can we train a dense retriever to digest the multiple supervisions?</p>
<p>Formally speaking, given $N$ teachers, for each augmented query $q$, we retrieve $N$ ranking lists (i.e., $\mathcal{P}<em q="q">{q}^{1}, \mathcal{P}</em>}^{2}, \cdots, \mathcal{P<em q="q">{q}^{N}$ with each list has $K$ passages) from the corpus with the respective teachers. We consider the ranking list $\mathcal{P}</em>$ contain the teacher's view on what is relevant and non-relevant for the given query. We then discuss possible strategies to train a dense retriever with diverse supervisions.
}^{n}$ from the $n$-th teacher a source of supervision since the top- $k$ and last- $k^{\prime}$ passages in $\mathcal{P}_{q}^{n<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of progressive label augmentation. For each iteration of training, additional relevance labels from a teacher are augmented in the training data. By contrast, uniform supervision directly exposes models to all the supervisions (as in iteration 3) in the beginning.</p>
<p>Fused Supervision. An intuitive strategy is to fuse the multiple sources into a single high-quality supervision, which dense retrievers can learn from. For the augmented query $q$, we conduct linear score fusion (Ma et al., 2021) on the $N$ ranking lists to form a new ranking list $\mathcal{F}<em q="q">{q}$ as a fused supervision.
Uniform Supervision. Another simple strategy is to provide a dense retriever with equal exposures to multiple sources of supervisions. Specifically, given a query, we uniformly sample a source of supervision; i.e., a ranking list $\mathcal{P}</em>(1, N)$. This approach naturally encourages the positive samples appearing in more ranking lists to be sampled and vise versa. The advantage is that fusion weight tuning is not required. Furthermore, models can see diverse supervisions from different teachers in contrast to fused supervision, which may be dominated by a single strong teacher (see Appendix A. 2 for more study).
Progressive Supervision. The previous two approaches directly give models supervisions from multiple teachers at once; however, learning directly from the mixture of supervision is challenging, especially for DR models which compute text matching with simple dot product. Inspired by the success of curriculum learning (Zeng et al., 2022), we propose an approach to progressive label augmentation to guide DR training with progressively more challenging supervision. Specifically, we train our models with uniform supervision for $N$ iterations and at each iteration, we augment relevance label using additional teacher, as illustrated in Figure 2; i.e., at iteration $T \leq N$, we uniformly sample a source of supervision, $\mathcal{P}_{q}^{n}$, where}^{n}$, where $n \sim$ $\mathcal{U</p>
<p>Table 2: Strategies to obtain multiple supervisions using cropped sentences as queries.</p>
<table>
<thead>
<tr>
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5*</th>
</tr>
</thead>
<tbody>
<tr>
<td>Teacher</td>
<td></td>
<td></td>
<td></td>
<td>three teachers</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>fused</td>
<td>unif.</td>
<td>prog.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>effectiveness of student</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MARCO Dev</td>
<td>34.9</td>
<td>33.9</td>
<td>36.4</td>
<td>36.7</td>
<td>36.9</td>
<td>36.6</td>
</tr>
<tr>
<td>BEIR-13</td>
<td>46.7</td>
<td>47.0</td>
<td>46.3</td>
<td>46.6</td>
<td>47.7</td>
<td>49.3</td>
</tr>
<tr>
<td></td>
<td></td>
<td>effectiveness of teacher</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MARCO Dev</td>
<td>35.1</td>
<td>34.1</td>
<td>39.7</td>
<td>40.0</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>BEIR-13</td>
<td>-</td>
<td>47.5</td>
<td>49.9</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>${ }^{*}$ The condition of column 5 corresponds to row 0 in Table 3.
${ }^{\triangle}$ We do not evaluate uniCOIL on BEIR due to its requirement of expensive document expansion from corpus.
$n \sim \mathcal{U}(1, T)$. A key factor of this approach is how to arrange the order for easy-to-hard supervisions; namely, the trajectory of progressive supervision.</p>
<p>With any aforementioned strategy to obtain diverse supervisions, we train our dense retrievers using contrastive loss in Eq. (2). Specifically, given a query $q$, we first obtain a source of supervision either from sampling $\left(\mathcal{P}<em q="q">{q}^{n}\right)$ or fusion $\left(\mathcal{F}</em>\right)$; then, we randomly sample a positive and hard negative from the top 10 passages and top 46-50 passages, respectively to form a triplet. The sampling scheme has been empirically proved to well preserve the supervised signal from a single teacher (Chen et al., 2021) (also see our study in Appendix A.1). In this work, we further extend the sampling scheme to obtain diverse supervisions from multiple teachers.</p>
<h3>3.3 Empirical Studies</h3>
<p>Strategies to Obtain Diverse Supervisions. We first conduct empirical studies on how to better train a dense retriever in a simplified setting by using the MS MARCO cropped sentences as augmented queries and obtain supervised labels using three teachers with diverse relevance score computation: uniCOIL (sparse), Contriever (dense) and ColBERTv2 (multi-vector). To compare the different strategies discussed in Section 3.2, We report the models trained with single (columns 0-2) and multiple (columns 3-5) sources of supervisions for 20 epochs and 60 epochs, respectively. For progressive supervision, we follow the supervision trajectory: unCOIL $\rightarrow$ Contriever $\rightarrow$ ColBERTv2 with 20 epochs for each of the three iterations $(N=3)$. Note that we use MS MARCO development queries to tune and obtain the best</p>
<p>Table 3: Study on trajectory of progressive supervision using cropped sentences as queries.</p>
<table>
<thead>
<tr>
<th>Progressive supervision</th>
<th>MARCO dev</th>
<th>BEIR-13</th>
</tr>
</thead>
<tbody>
<tr>
<td>trajectories</td>
<td>RR@10</td>
<td>nDCG@10</td>
</tr>
<tr>
<td>(0) unCOIL $\rightarrow$ Contriever $\rightarrow$ ColBERTv2</td>
<td>36.6</td>
<td>49.3</td>
</tr>
<tr>
<td>(1) Contriever $\rightarrow$ unCOIL $\rightarrow$ ColBERTv2</td>
<td>36.7</td>
<td>48.4</td>
</tr>
<tr>
<td>(2) ColBERTv2 $\rightarrow$ Contriever $\rightarrow$ unCOIL</td>
<td>36.4</td>
<td>47.7</td>
</tr>
<tr>
<td>(3) unCOIL $\rightarrow$ Contriever $\rightarrow$ ColBERTv2*</td>
<td>36.8</td>
<td>47.4</td>
</tr>
</tbody>
</table>
<ul>
<li>ColBERTv2 is the only teacher at the last (3rd) iteration.
hyperparameters to create fusion list.
The results are tabulated in Table 2. We observe that when learning from a single supervision (columns 0-2), there is a tradeoff between supervised and zero-shot retrieval effectiveness. Learning from the fusion list only sees a slight improvement over supervised evaluation while no improvement observes in zero-shot evaluations (columns $0-2$ vs 3). By contrast, the model sees notable improvements in zero-shot evaluations when trained with uniform supervision (columns $0-3$ vs 4), indicating that learning from the diverse relevance labels from multiple retrievers separately rather than single strong supervision (ColBERTv2 or fused supervision) is key to gain generalizability capability. Finally, we observe that progressive supervision can further guide a dense retriever to gain generalization capability over uniform supervision (column 4 vs 5). Thus, we use progressive supervision in the following experiments.</li>
</ul>
<p>Trajectory of Progressive Supervision. We then study how to better arrange the trajectories of progressive supervision in Table 3. We observe that different trajectories have much impact on models' zero-shot retrieval effectiveness while a minor impact on supervised evaluation can be seen. For example, switching the sampling order between uniCOIL and Contriever results in a degrade of 1 point on the averaged nDCG@10 over BEIR-13 (column 0 vs 1 ) while reversing the whole trajectory leads to a degrade with more than 1.5 points (column 0 vs 3 ). This observation reflects an intuition that the retrievers with better generalization capability may capture more complex matching signal between text pairs (ColBERTv2 shows better generalization capability than the other two teachers); thus, their relevance labels should be augmented at a later stage of model training. Finally, in column 3, we follow the trajectory in column 0 but only use ColBERTv2 as the only source of supervision instead of obtaining uniform supervision from the three</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Impacts of query augmentation.
teachers at the last iteration. This change results in worse zero-shot retrieval effectiveness, indicating that learning from diverse supervisions is key to training a generalizable dense retriever.</p>
<p>Query Augmentation. We study the impacts of query augmentation on models' effectiveness under the scenario where supervision is given, which has not been studied so far in the field of dense retrieval. With the best trajectory of progressive supervision, we compare models trained using cropped sentences (rectangles), generative queries (GenQ; circles), their mixture (triangles) and human queries (cross) in Figure 3. We observe that query size is the key to successful training. Although training with limited ( 0.8 M ) cropped sentences cannot perform well in MS MARCO dataset, scaling up the size (28M) sees significant improvement over the model trained on 0.8 M human queries. Similarly, in Figure 3 (b), model's generalization capability shows a huge jump when scaling up query size and surprisingly, cropped sentences can help dense retrievers to gain more generalization capability than human-like generative queries. However, a mixture of cropped sentences and generative queries yields strong retrieval effectiveness in supervised and zero-shot evaluations, especially when the query size is not large enough ( $0.8-8 \mathrm{M}$ ).</p>
<h3>3.4 Training our DraGONs</h3>
<p>With the empirical studies on DR training, we then propose the final recipe to train our Dragon. We train Dragon for 20 epochs (around 130K steps) at each iteration, with the trajectory of progressive supervision: unCOIL $\rightarrow$ Contriever $\rightarrow$ GTR-XXL $\rightarrow$ ColBERTv2 $\rightarrow$ SPLADE++. We list all the teacher model checkpoints for label augmentation in Appendix A.5. This trajectory is based on models' retrieval effectiveness on BEIR with the intuition gained from Section 3.3 that a more generalizable model creates more complex relevance labels.</p>
<p>For query augmentation, we mix half of cropped sentences and synthetic queries as training queries. Note that we do not further fine-tune our models on the MS MARCO training set. In addition, we train other three Dragon variants. Dragon-S and Dragon-Q only use cropped sentences and synthetic queries, respectively. As for Dragon+, we follow the same training procedure of Dragon but switch the initialization from BERT to the masked auto-encoding pre-trained model, RetroMAE ${ }^{3}$. We will discuss the impacts of initialization in Section 5. The implementation of DraGONs and the fully augmented training data using the five teachers are detailed in Appendix 4.3 and A.3, respectively.</p>
<h2>4 Comparison with the State of The Art</h2>
<h3>4.1 Datasets</h3>
<p>In addition to MS MARCO dev, we evaluate model supervised effectiveness on the TREC DL (Craswell et al., 2019, 2020) queries, created by the organizers of the 2019 (2020) Deep Learning Tracks at the Text REtrieval Conferences (TRECs), where 43 (53) queries with on average 95 (68) graded relevance labels per query (in contrast to 6980 queries with on average 1 non-graded relevance label per query in MS MARCO dev) are released. We report nDCG@10, used by the organizers as the main metric.</p>
<p>For zero-shot evaluations, we evaluate models on all the 18 datasets in BEIR (Thakur et al., 2021b). In addition, we use LoTTE (Santhanam et al., 2021) consisting of questions and answers posted on StackExchange with five topics including writing, recreation, science, technology, and lifestyle. We evaluate models' retrieval effectiveness in the pooled setting, where the passages and queries from the five topics are aggregated. Following Santhanam et al. (2021), the retrieval effectiveness of Success@5 on search and forum queries are reported. We refer reader to Appendix A. 6 for detailed evaluations on all the five topics.</p>
<h3>4.2 Baseline Models</h3>
<p>We compare Dragon with dense retrievers using the backbone of bert-base-uncased trained with advanced techniques. (1) Knowledge Distillation: RocketQAv2 (Ren et al., 2021) distills knowledge from a cross encoder while CL-DRD (Ren et al., 2021) combines curriculum learning and</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>cross-encoder distillation. They all use cross encoders' knowledge to augment positive relevance lablels as our approach. (2) Contrastive PreTraining: coCondneser <em>Gao and Callan (2022)</em>, Contriever <em>Izacard et al. (2021)</em> and COCODR <em>Yu et al. (2022)</em> are first pre-trained on different corpus listed in Table 1, and then fine-tuned on MS MARCO training queries. (3) Masked AutoEncoding Pre-Training: COT-MAE <em>Wu et al. (2022)</em> and RetroMAE <em>Liu and Shao (2022)</em> are first pre-trained to recover polluted sentences and then fine-tuned on MS MARCO training queries. For RetroMAE, we use the variant with the best BEIR retrieval effectiveness for comparison. (4) Domain adaptation: We consider GPL <em>Wang et al. (2022)</em> and Promptgator <em>PTR; Dai et al. (2022)</em>, which use generative models to create pseudo relevance data for each corpus in BEIR and train one expert dense retriever for each corpus. This approach requires the target corpus while training. Note that COCODR can also be considered domain adaptation on BEIR although it uses one model for all tasks.</p>
<p>Note that coCondenser and COT-MAE are finetuned on the "non-standard" MS MARCO passage corpus that has been augmented with title. Thus, we also conduct inference on the corpus with title for them; otherwise, we use the official MS MARCO passage corpus. In addition, we also report the retrieval effectiveness of GTR-XXL <em>Ni et al. (2021)</em> and ColBERTv2 <em>Santhanam et al. (2021)</em> from their original papers and conduct retrieval for SPLADE++ using pyserini <em>Lin et al. (2021a)</em> for reference. We list all the other model checkpoints used for evaluations in Appendix A.5.</p>
<h3>4.3 Implementation Details</h3>
<p>We train our dense retrievers initialized from bert-base-uncased on 32 A100 GPUs with a per-GPU batch size of 64 and a learning rate of $3e-5$. Each batch includes an augmented query with its positives and hard negatives. Following <em>Karpukhin et al. (2020)</em>, we use asymmetric dual encoder with two distinctly parameterized encoders and leverage in-batch negative mining. Note that symmetric dual encoder shows poor generalization capability in our initial experiments. We set the maximum query and passage lengths to 32 and 128 for MS MARCO training and evaluation. For BEIR evaluation, we set maximum input lengths to 512.</p>
<h3>4.4 Results</h3>
<p>Supervised Evaluations. The first main row in Table 4 reports models' retrieval effectiveness on MS MARCO passage ranking dataset. We first observe that some baseline dense retrievers which perform well in MS MARCO dev set are either pre-trained on MS MARCO corpus (coCondenser and COT-MAE) or well fine-tuned on MS MARCO training queries with cross-encoder distillation (CL-DRD and RocketQAv2). However, their retrieval effectiveness on MS MARCO dev set is not well correlated to TREC DL queries, which have fine-grained human labels with different degrees of relevance. We hypothesize that these models are able to retrieve the most relevant passage from the corpus but cannot retrieve diverse passages with different degrees of relevance. By contrast, all the variants of Dragon trained with diverse augmented relevance labels show consistently strong retrieval effectiveness in MS MARCO dev and TREC DL queries, over RR@10 38.0 and nDCG@10 70.0, respectively.</p>
<p>Zero-Shot Evaluations. The second main row in Table 4 reports models' zero-shot retrieval effectiveness on the BEIR datasets. We observe a reverse trend that those dense retrievers performing relatively poorly in MS MARCO dev queries have better zero-shot retrieval effectiveness (e.g., Contriever, COCO-DR and RetroMaE). These models are pre-trained (with data augmentation) on a corpus other than MS MARCO to combat domain shift issue in dense retrieval <em>Xin et al. (2022); Yu et al. (2022)</em>. On the other hand, our models trained on augmented data from MS MARCO corpus only transfer well to BEIR datasets. For example, Dragon+ reaches state-of-the-art retrieval effectiveness on BEIR as the sparse retriever, SPLADE++. In addition, all the Dragon variants outperform dense retrievers by a large margin and compete SPLADE++ and ColBERTv2 in the LoTTE dataset. It is worth mentioning that the models trained with domain adaptation (columns 9-B) perform better than the others (columns 3-8) but still underperform Dragon in zero-shot evaluations. Using Dragon as a foundation model for domain adaptation is possible to gain DR zero-shot effectiveness, which we leave to our future work.</p>
<p>To sum up, Dragons advances state-of-the-art zero-shot effectiveness on BEIR and LoTTE</p>
<p>See our BEIR leaderboard submission here.</p>
<p>Table 4: Comparison with existing state-of-the-art dense retrievers. Bold (underline) denotes the best (second best) effectiveness for each row among baseline dense models.</p>
<p>| Rep type | sparse | mul-vec | dense | dense (baselines) | | | | | | | | | dense (ours) | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>Table 5: Label augmentation with CE (miniLML6v2) using cropped sentences as queries. All denotes the five sources of supervisions used for training DRAGON-S.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$0^{*}$</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DRAGON-S initialization</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Source of supervision</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">all + CE</td>
<td style="text-align: center;">CE only</td>
<td style="text-align: center;">CE only</td>
</tr>
<tr>
<td style="text-align: left;">MARCO Dev (RR@10)</td>
<td style="text-align: center;">$\mathbf{3 8 . 1}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 1}$</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: left;">BEIR-13 (nDCG@10)</td>
<td style="text-align: center;">$\mathbf{4 9 . 8}$</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">47.7</td>
</tr>
</tbody>
</table>
<p>${ }^{*}$ Column 0 corresponds to DRAGON-S.
pre-training may be orthogonal to our approach based on contrastive learning.</p>
<h2>5 Discussions</h2>
<p>Is it necessary to augment relevance labels with a cross encoder? To answer this question, we further train DRAGON-S with the augmented relevance labels from a cross encoder (CE). Specifically, we create a rank list of CE by first retrieving top 1000 passages with DRAGON-S and re-rank them with the CE for each cropped sentence. With the CE ranking list, we conduct another iteration (20 epochs) of training for DRAGON-S; however, we do not see retrieval effectiveness improvement in Table 5 (column 0 vs 1). In addition, the retrieval effectiveness becomes even worse when we further train DRAGON-S by only sampling CE ranking list instead of uniformly sampling all the six ranking lists (column 1 vs 2). Finally, we initialize from bert-base-uncased and re-train the model for three iterations ( 60 epochs) only with the CE ranking list. ${ }^{5}$ We observe that its effectiveness (column 3) is even worse than the models trained with the ranking lists from three retrievers (see columns 4 and 5 in Table 2). This result contradicts the general belief that CE provides the strongest supervision to a dense retriever. Moreover, it demonstrates the effectiveness of using diverse supervisions to train a generalizable dense retriever, rather than relying on a single strong supervision.</p>
<p>Table 6 compares the latency cost per query for relevance label augmentation with different neural rankers and demonstrates that leveraging all the retrievers to augment relevance labels are still more efficient than a cross encoder. We detail the measurement setting in Appendix A.4.
Does DRAGON benefit from unsupervisd pretraining? Table 7 compares the models trained from different checkpoint initialization. Note that</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 6: The latency comparison of relevance label augmentation with batch inference using different teachers on MS MARCO.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">candidates</th>
<th style="text-align: center;">latency (ms/q)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Type</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">(#)</td>
<td style="text-align: center;">GPU CPU</td>
</tr>
<tr>
<td style="text-align: left;">cross-encoder</td>
<td style="text-align: center;">miniLML6v2</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: left;">dense</td>
<td style="text-align: center;">Contriever</td>
<td style="text-align: center;">8.8 M</td>
<td style="text-align: center;">$&lt;1$</td>
</tr>
<tr>
<td style="text-align: left;">dense</td>
<td style="text-align: center;">GTR-XXL</td>
<td style="text-align: center;">8.8 M</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">sparse</td>
<td style="text-align: center;">uniCOIL</td>
<td style="text-align: center;">8.8 M</td>
<td style="text-align: center;">- 84</td>
</tr>
<tr>
<td style="text-align: left;">sparse</td>
<td style="text-align: center;">SPLADE++</td>
<td style="text-align: center;">8.8 M</td>
<td style="text-align: center;">- 144</td>
</tr>
<tr>
<td style="text-align: left;">multi-vec</td>
<td style="text-align: center;">ColBERTv2</td>
<td style="text-align: center;">8.8 M</td>
<td style="text-align: center;">55</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation on initialized checkpoint using the mixture of cropped sentences and GenQ as queries.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Initialized checkpoint</th>
<th style="text-align: center;">MARCO dev</th>
<th style="text-align: center;">BEIR-13</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">RR@10</td>
<td style="text-align: center;">nDCG@10</td>
</tr>
<tr>
<td style="text-align: left;">(0) BERT base</td>
<td style="text-align: center;">$\mathbf{3 9 . 3}$</td>
<td style="text-align: center;">49.4</td>
</tr>
<tr>
<td style="text-align: left;">(1) Contriever</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">49.3</td>
</tr>
<tr>
<td style="text-align: left;">(2) RetroMAE</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">$\mathbf{5 0 . 2}$</td>
</tr>
</tbody>
</table>
<p>${ }^{*}$ Row 0 and 2 corresponds to Dragon and DRAGON+, respectively.
for Contriever and RetroMAE, we initialize from the checkpoint with only unsupervised pre-training (without fine-tuning on MS MARCO). We observe that our approach benefits from masked autoencoding rather than contrastive pre-training (row 2 vs 1). The result is sensible since our approach can be considered an improved version of contrastive pre-training, which appears to be orthogonal to masked auto-encoding pre-training. We leave the investigation of improving DR with generative and contrastive pre-training combined for future research.</p>
<p>Can we use the soft labels from multiple teachers? In the literature, using the relevance scores from a teacher as soft labels is a standard of knowledge distillation (Lin et al., 2021b; Hofstätter et al., 2021). However, in our study, even when training with uniform supervision from a sparse and dense retriever (i.e., uniCOIL and Contriever), it is challenging to normalize their relevance scores and create universal soft labels, yielding significant supervised and zero-shot effectiveness drops. We suspect that dense and sparse retrievers have many different views on relevance score computation; thus, it is even harder for a dense retriever to learn the score distributions from the different teachers.</p>
<p>Why sentence cropping yields a generalizable dense retriever? Figure 4 showcases the augmented queries by sentence cropping and neural</p>
<table>
<thead>
<tr>
<th>Passage: The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Query Augmentation</td>
<td>Augemted rel (#)</td>
<td>Example of augmented rel</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Cropping</td>
<td>Q1</td>
<td>The Manhattan Project and its atomic bomb helped bring an end to World War II</td>
<td>30</td>
<td>The Manhattan project was a secret research and development project of the U.S to develop the atomic bomb. Its success granted the U.S the bombs that ended the war with Japan as well as ushering the country into the atomic era.</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Q2</td>
<td>Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.</td>
<td>30</td>
<td>An early nuclear power plant that used atomic energy to generate electricity. The Atomic Age, also known as the Atomic Era, is the period of history following the detonation of the first nuclear (atomic) bomb. ...</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GenQ</td>
<td>Q1</td>
<td>what were a major contributions to the manhatten effort</td>
<td>26</td>
<td>The Manhattan Project was an effort during World War II in the United States to develop the first nuclear weapon.</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Q2</td>
<td>what impact did the manhatten project have on history</td>
<td>26</td>
<td>The Manhattan Project, which included some of history's greatest scientific minds, lead to the end of the war against the Japanese. But was it worth the environmental and financial costs? This massive site provides loads of ...</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 4: Examples of augmented queries and relevance labels from a passage. Augmented rel (#) denotes the number of unique relevant passages labeled by all our five teachers.
generation and their respectively augmented relevant passages other than the original passages. We observe two main differences between the cropped sentences and generative queries. Cropped sentences provide diverse queries from a passage; i.e., the two cropped sentences in Figure 4 include slightly different topics (Manhattan Project and atomic energy). By contrast, all generative queries surround the same main topic, Manhattan Project, about the original passages. Second, the cropped sentences have more unique augmented relevant passages than generative queries. This is maybe because a cropped sentence, containing more information (keywords), is more challenging than a generative human-like query. Thus, teachers show more disagreement between each other on cropped sentences. We hypothesize that a dense retriever trained on cropped sentences can capture more diverse supervised signals from multiple teachers than generative queries. This explains the reason why Dragon-S shows better generalization capability than Dragon-Q.</p>
<h2>6 Related Work</h2>
<p>Knowledge Distillation. Our work is closely related to the previous work exploring knowledge distillation (KD; Hinton et al., 2015) from ColBERT, cross encoder or their ensemble (Hofstätter et al., 2021; Hofstätter et al., 2020) to improve the effectiveness of DR (Lin et al., 2021b; Qu et al., 2021). However, they only take the advantage of soft labels from KD and use the relevant passages labeled by humans. The recent work (Ren et al., 2021; Zeng et al., 2022) mines more positive samples using cross encoder to further augment the limited relevance labels by humans. Nevertheless, it is challenging for cross encoders to augment rel-
evance labels for queries in scale due to its low efficiency. Chen et al. (2021) first explore label augmentation using singe sparse retrieval model on large-scale queries and demonstrate that a dense retriever can mimic a teacher of a sparse retriever (e.g., BM25). Different from the previous work, we explore label augmentation using multiple supervisions on large-scale augmented queries.
Curriculum Learning. Easy-to-hard training strategies (Bengio et al., 2009) have been applied to improve many machine learning tasks, including dense retrieval (Zeng et al., 2022; Lin et al., 2022). The previous work focuses on distilling complex knowledge from cross encoders to a dense retriever with a curriculum training strategy and demonstrates improved effectiveness in supervised retrieval tasks. In our work, we explore to progressively train a dense retriever with the diverse supervisions from dense, sparse and multi-vector retrievers to improve both supervised and zero-shot effectiveness.
Pre-training. There are two popular approaches to pre-training a dense retriever. The first one is contrastive pre-training, aiming to increase the size of training data by creating artificial text pairs (Lee et al., 2019; Chang et al., 2020; Izacard et al., 2021) from a corpus or collecting question-answer pairs (Oguz et al., 2022; Ni et al., 2021) from websites. The second one is masked auto encoding pre-training, where models are trained to recover the corrupted texts (Gao and Callan, 2021; Lu et al., 2021; Liu and Shao, 2022; Wu et al., 2022). Our work is similar to contrasitive pre-training but instead of creating large-scale training data in an unsupervised or weakly supervised manner, we investigate how to conduct supervised contrastive learning on artificially created text pairs. We demon-</p>
<p>strate that combining masked auto encoding pretraining and our supervised contrastive learning can further improve models' generalization capability.</p>
<h2>7 Conclusion</h2>
<p>We present Dragon, Dense Retriever trained with diverse AuGmentatiON. We propose a unified framework of data augmentation (DA) to understand the recent progress of training dense retrievers. Based on the framework, we extensively study how to improve dense retrieval training through query and relevance label augmentation. Our experiments uncover some insights into training a dense retriever, which contradicts common wisdom that cross encoder is the most effective teacher and human-like queries are the most suitable training data for dense retrieval. Instead, we propose a diverse data augmentation recipe, query augmentation with the mixture of sentence cropping and generative queries, and progressive relevance label augmentation with multiple teachers. With our recipe of DA, we are the first to demonstrate that a single BERT-base-sized dense retriever can achieve state-of-the-art effectiveness in both supervised and zero-shot retrieval tasks. We believe that Dragon can serve as a strong foundation retrieval model for domain adaptation retrieval tasks (Wang et al., 2022; Dai et al., 2022) or the existing retrieval augmented language models (Izacard et al., 2022; Shi et al., 2023; Mallen et al., 2022).</p>
<h2>References</h2>
<p>Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions. arXiv:2211.09260.</p>
<p>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016. MS MARCO: A human generated machine reading comprehension dataset. arXiv:1611.09268.</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proc. ICML, page 41-48.</p>
<p>Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for embedding-based large-scale retrieval. In Proc. ICLR.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework
for contrastive learning of visual representations. In Proc. ICML.</p>
<p>Xilun Chen, Kushal Lakhotia, Barlas Oğuz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen tau Yih. 2021. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv:2110.06918.</p>
<p>Nick Craswell, Bhaskar Mitra, and Daniel Campos. 2019. Overview of the TREC 2019 deep learning track. In Proc. TREC.</p>
<p>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 deep learning track. In Proc. TREC.</p>
<p>Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv:2209.11755.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805.</p>
<p>Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2022. From distillation to hard negative sampling: Making sparse neural ir models more effective. In Proc. SIGIR, page 2353-2359.</p>
<p>Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE v2: Sparse lexical and expansion model for information retrieval. arXiv:2109.10086.</p>
<p>Luyu Gao and Jamie Callan. 2021. Condenser: a pretraining architecture for dense retrieval. In Proc. EMNLP, pages 981-993.</p>
<p>Luyu Gao and Jamie Callan. 2022. Unsupervised corpus aware language model pre-training for dense passage retrieval. In Proc. ACL, pages 2843-2853.</p>
<p>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization. In Proc. ICML.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. In Proc. NIPS.</p>
<p>Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proc. SIGIR, page $113-122$.</p>
<p>Sebastian Hofstätter, Sophia Althammer, Michael Schröder, Mete Sertkan, and Allan Hanbury. 2020. Improving efficient neural ranking models with cross-architecture knowledge distillation. arXiv:2010.02666.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv:2112.09118.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot Learning with Retrieval Augmented Language Models. arXiv:2208.03299.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, pages 535-547.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proc. EMNLP, pages 6769-6781.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proc. ACL, pages 6086-6096.</p>
<p>Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In Proc. SIGIR, page 2356-2362.</p>
<p>Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2021b. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval. In Proc. RepL4NLP, pages 163-173.</p>
<p>Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, and Nan Duan. 2022. Prod: Progressive distillation for dense retrieval. arXiv:2209.13335.</p>
<p>Zheng Liu and Yingxia Shao. 2022. Retromae: Pretraining retrieval-oriented transformers via masked auto-encoder. arXiv:2205.12035.</p>
<p>Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. 2021. Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder. In Proc. EMNLP, pages 27802791.</p>
<p>Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin. 2021. A replication study of dense passage retriever. arXiv:2104.05740.</p>
<p>Joel Mackenzie, Andrew Trotman, and Jimmy Lin. 2021. Wacky weights in learned sparse representations and the revenge of score-at-a-time query evaluation. arXiv:2110.11540.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. arXiv:2212.10511.</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021. Large dual encoders are generalizable retrievers. arXiv:2112.07899.</p>
<p>Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to docTTTTTquery.</p>
<p>Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. 2022. Domain-matched pre-training tasks for dense retrieval. In Proc. NAACL, pages 1524-1534.</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proc. NAACL, pages 5835-5847.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proc. EMNLP, pages 3982-3992.</p>
<p>Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proc. EMNLP, pages 2825-2835.</p>
<p>Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. Plaid: An efficient engine for late interaction retrieval. arXiv:2205.09707.</p>
<p>Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv:2112.01488.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented black-box language models. arXiv:2301.12652.</p>
<p>Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. 2021a. Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. In Proc. NAACL, pages 296-310.</p>
<p>Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021b. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Proc. NIPS.</p>
<p>Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proc. NAACL, pages 2345-2360.</p>
<p>Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu. 2022. Contextual masked auto-encoder for dense passage retrieval. arXiv:2208.07670.</p>
<p>Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, and Paul Bennett. 2022. Zero-shot dense retrieval with momentum adversarial domain invariant representations. In Proc. ACL, pages 4008-4020.</p>
<p>Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In Proc. ICLR.</p>
<p>Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning. In Proc. EMNLP.</p>
<p>Hansi Zeng, Hamed Zamani, and Vishwa Vinay. 2022. Curriculum learning for dense retrieval distillation. In Proc. SIGIR, page 1979-1983.</p>
<table>
<thead>
<tr>
<th>top- $k$ positives</th>
<th>1</th>
<th>5</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>MARCO Dev (RR@10)</td>
<td>33.1</td>
<td>36.4</td>
<td>36.6</td>
</tr>
<tr>
<td>BEIR-13 (nDCG@10)</td>
<td>42.4</td>
<td>48.0</td>
<td>49.3</td>
</tr>
</tbody>
</table>
<p>A Appendices</p>
<h2>A. 1 Impacts of Top- $k$ Positive Sampling</h2>
<p>Table 8: Ablation on progressive label augmentation from top- $k$ passages using cropped sentences as queries.</p>
<table>
<thead>
<tr>
<th>top- $k$ positives</th>
<th>1</th>
<th>5</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>MARCO Dev (RR@10)</td>
<td>33.1</td>
<td>36.4</td>
<td>36.6</td>
</tr>
<tr>
<td>BEIR-13 (nDCG@10)</td>
<td>42.4</td>
<td>48.0</td>
<td>49.3</td>
</tr>
</tbody>
</table>
<p>In Section 3.2, we mention that our sampling scheme treats top 10 passages from each teacher ranking list as positives and top $45-50$ as negatives. We further conduct experiment to study the impact of the positive sampling scheme. Following the experiment setups in Section 3.3, we use the sentences cropped from MS MARCO corpus as augmented queries and we conduct progressive label augmentation using top- $k$ passages as positive. The results are tabulated in Table 8. We observe that treating top-10 passages from each teacher as positives yields the best supervised and zero-shot effectiveness. On the other hand, using only the top passage as positive results in significant effectiveness drop. This result indicates that the top passage labeled by a teacher cannot transfer its knowledge well to a student. This result is similar to the observation from Chen et al. (2021).</p>
<h2>A. 2 An Intuition Behind Uniform and Progressive Supervisions</h2>
<p>As shown in Section 3.2, uniform supervision provides good supervision without fusion weight tuning as fused supervision. Intuitively, a positive retrieved by more teachers has a higher probability to be sampled and may be more relevant to a query. To provide a sense of why uniform supervision works, we estimate the accuracy of supervision by computing the probability of each positives sampled under uniform supervision, and rank the positives according to the simulated probability. For example, at the 3rd iteration of progressive training, given a query, a positive passage is labeled positive by all the three teachers, the probability of the passages being sampled is $\frac{1}{3} \cdot\left(\frac{1}{k}+\frac{1}{k}+\frac{1}{k}\right)=\frac{1}{k}$. In our experiments, each teacher labels the top $10(k=10)$ retrieved passages as positives in our labeling scheme. Note that, in the case where multiple positives have equal probability, we further rank them according to their sum of reciprocal rank. For instance, if the two pas-</p>
<p>Table 9: Uniform and progressive supervision effectiveness comparison at each training iteration. The models are trained using cropped sentences as queries.</p>
<p>| Teacher / iteration | uniform | | | | progressive | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>A.4 Latency Measurement for Relevance Label Augmentation</p>
<p>We measure the latency of label augmentation using batch retrieval on a single NVIDIA A100 40GB GPU for GPU search and 60 Intel(R) Xeon(R) Platinum 8275CL CPUs @ 3.00GHz for CPU search. For cross encoder, we conduct label augmentation by re-ranking text pairs with a batch size of 100. For dense retrieval (Contriever and GTR-XXL) and sparse retrieval, we use Faiss-GPU index and lucene index from Pyserini (Lin et al., 2021a) with 60 threads, respectively, and search with a batch size of 100. Note that we use a batch size of 25 to encode queries using GTR-XXL due to GPU memory constraint, which is also the main bottleneck for GTR-XXL batch retrieval. For ColBERTv2, we use the improved version of multi-vector retrieval, PLAID (Santhanam et al., 2022), and search with a batch size of 1, which is the only option.</p>
<h3>A.5 Model Checkpoints</h3>
<p>Teacher Models: (1) uniCOIL: https://hu ggingface.co/castorini/unicoil-m smarco-passage; (2) Contriever: https: //huggingface.co/facebook/contri ever-msmarco; (3) GTR-XXL: https://hu ggingface.co/sentence-transforme rs/gtr-t5-xxl; (4) ColBERTv2: https:// github.com/stanford-futuredata/C olBERT; (5) SPLADE++: http://download -de.europe.naverlabs.com/Splade_R elease_Jan22/splade_distil_CoCod enser_medium.tar.gz; (6) Cross encoder: https://huggingface.co/cross-enc oder/ms-marco-MiniLM-L-12-v2.
Baseline Models: (1) CL-DRD: https://gi thub.com/HansiZeng/CL-DRD; (2) RocketQAv2: we directly copy the numbers from Santhanam et al. (2021); (3) COT-MAE: https: //huggingface.co/caskcsg/cotma e_base_msmarco_retriever; (4) RetroMAE: https://huggingface.co/Shita o/RetroMAE_BEIR; (5) coCondenser: https: //huggingface.co/Luyu/co-conde nser-marco-retriever; (6) Contriever: https://huggingface.co/faceboo k/contriever-msmarco; (7) COCODR: https://huggingface.co/OpenMatch /cocodr-base-msmarco; (8) Promptgator (PTR) and GPL: we directly copy the numbers from their original papers (Dai et al., 2022; Wang</p>
<p>Table 11: DraGons’ detailed effectiveness on LoTTE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">sparse</th>
<th style="text-align: center;">multi-vec</th>
<th style="text-align: center;">dense</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">F</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">writing</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">79.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">recreating</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">science</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">56.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">technology</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">lifestyle</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pooled</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">72.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">writing</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">recreating</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">70.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">science</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">technology</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">50.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">lifestyle</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">76.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pooled</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">61.4</td>
</tr>
</tbody>
</table>
<p>et al., 2022).</p>
<h2>A. 6 Detailed Effectiveness on LoTTE</h2>
<p>Table 11 lists Dragon's more detailed effectiveness on five topics without aggregation. Although all the variants of Dragonshow strong effectiveness on the datasets, we find that Dragons perform poorly on the Forum queries about topics of science and technology compared to SPLADE++ and ColBERTv2. Combining science corpus pretraining and Dragon training may address the issue.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We do not notice effective improvement with more iterations of training both in supervised and zero-shot evaluations.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
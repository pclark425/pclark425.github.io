<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2595 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2595</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2595</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-e135d9529a17e8f624fe0d7a62a90bbe822ff4ac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e135d9529a17e8f624fe0d7a62a90bbe822ff4ac" target="_blank">Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science</a></p>
                <p><strong>Paper Venue:</strong> SIAM Journal on Scientific Computing</p>
                <p><strong>Paper TL;DR:</strong> A Monte Carlo--based approach is proposed to address the hurdle in the context of both the batch and nested-batch problems, and empirically demonstrate the effectiveness of this approach.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2595.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2595.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-theoretic acquisition rule that selects the next measurement to maximize the single-period expected increase in the value of the final recommended alternative (expected value of information measured as increase in the posterior max mean).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Gradient (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KG is a Bayesian optimal-learning acquisition policy for ranking-and-selection problems with correlated normal beliefs. At a given knowledge state (posterior mean vector and covariance), KG computes for each candidate alternative the expected increase in the terminal objective (the maximum posterior mean after measurement) that would result from measuring that alternative, and selects the alternative maximizing this expected increment. The posterior is updated by Bayes' rule (closed-form Gaussian updates); the KG score requires computing an expectation over the one-step posterior maximum, i.e., an expectation of a max of normals.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sequential experimental design / ranking-and-selection; demonstrated on materials science (nanoparticle size/density optimization) but applicable to drug discovery, simulation optimization, and other experimental sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select a single alternative x that maximizes the expected one-step increase in the terminal recommended alternative's mean (the KG value); this allocates the next measurement to the point with highest value of information for improving the final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not expressed in FLOPs or wall time; cost characterized by number of physical measurements (N), dimensionality M (number of alternatives), and the computational effort required to compute expected values (often Monte Carlo samples when analytic evaluation is intractable).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected increase in the terminal objective: E[max_x' theta_x'^{new}] - max_x' theta_x'^{current} (expected improvement in the posterior maximum).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicitly balances exploration and exploitation by quantifying the expected benefit (to the final decision) of information from each candidate; alternatives with high uncertainty and/or promising mean can be picked if expected to raise the posterior maximum.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting regularizer; diversity arises implicitly through posterior correlation (KG reduces KG values for nearby/correlated alternatives after measurements).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments budget (N = number of allowed measurements) and per-batch size when used in batch variants.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>KG is used within a dynamic programming framing where the policy maximizes expected terminal reward subject to a fixed measurement budget; it greedily picks measurements that maximize expected one-step improvement under remaining budget (single-step approximation).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>No explicit novelty/breakthrough score; breakthrough implied by large reduction in opportunity cost or discovering a much higher-performing alternative (measured via posterior mean and final realized objective).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Opportunity cost (OC) = true maximum mu - mu_recommended; reported as mean OC over trials and plotted versus number of measurements. Other reported quantities include reduction in OC with more measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against pure exploration (random), pure exploitation (greedy by current mean), epsilon-greedy, nested-batch and sequential variants in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>KG (sequential) is competitive with and often outperforms naive baselines (random, exploitation, epsilon-greedy); in nested-batch contexts, NBKG (the nested-batch variant) outperforms nested-batch baselines and is competitive with sequential KG (plots show lower mean OC for KG/NBKG than baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: KG yields faster decay in opportunity cost versus random/exploit/explore baselines; specific numeric gains not provided as a single percentage in the paper (results shown in mean OC plots).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>KG inherently trades off exploration and exploitation through the expected-value-of-information calculation; the paper analyzes tradeoffs between adaptivity (sequential updates) and parallelism (batch measurements) qualitatively and empirically (e.g., batch size vs adaptivity).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>KG is recommended as an effective allocation rule for maximizing the final recommended alternative under a fixed budget; acts as a principled rule to allocate measurements to maximize expected terminal improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science', 'publication_date_yy_mm': '2015-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2595.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2595.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Batch Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of KG to select B measurements simultaneously: greedily build a batch by adding the alternative that maximizes the marginal KG conditioned on earlier choices in the same batch, using Monte Carlo approximation for expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Batch Knowledge Gradient (BKG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BKG chooses a batch of B experiments per decision epoch by sequentially (greedily) adding alternatives: start with the highest single-measurement KG for the first slot, then for each subsequent slot pick the candidate that maximizes the expected marginal increase in the terminal objective given the previous picks in the same batch. The expectation of the max of posterior means after the whole batch is approximated with Monte Carlo sampling; posterior covariance is updated within the batch deterministically (since covariance updates do not depend on observed values).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch-mode experimental design in laboratory sciences; demonstrated for nested/parallel experiments in materials science (nanoparticle density sweeps given fixed size), but applicable across domains with batched experimental capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Greedy multi-set maximization of expected terminal-value increment: repeatedly pick the alternative with the largest marginal BKG value until B items are chosen (multi-set allowed — repeats permitted). This allocates finite experimental resources among alternatives in each batch to maximize expected information benefit for the final recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Characterized in paper by Monte Carlo sample size Q (number of random draws to approximate expectations), batch size B, number of alternatives M, and number of batches K; computational cost effectively measured in number of Monte Carlo evaluations (Q × M × B) plus cost of posterior updates.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Batch KG value: E[max_x theta_x^{post-batch}] - max_x theta_x^{current}; marginal BKG values are differences of these when adding elements to the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Greedy marginal-expected-improvement selection within a batch: items prioritized by their expected contribution to improving the final recommended mean (combines exploration — uncertainty reduction — and exploitation — raising promising means).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity regularizer; the greedy marginal KG implicitly reduces selecting near-duplicate/correlated items because posterior covariance updates within the batch reduce marginal KG for correlated neighbors. The approach permits repeated measurements (multi-set), but does not include an explicit diversity objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size B per decision and fixed total number of batches K (so total measurements N = B×K); also a computational budget (Monte Carlo Q) for expectation approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Given B and K, BKG always fills the batch (theoretical result: more measurements never reduce value), selecting B items greedily subject to available experiments; computational budget traded off via choosing Q for Monte Carlo approximation (paper empirically determines Q sufficient for accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Opportunity cost (OC) reduction and locating maximal photocurrent; no separate novelty/breakthrough score is defined.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean opportunity cost versus number of batch measurements K (plotted), Monte Carlo sampling diagnostics: maximum standard error across densities s_mc / sqrt(Q) (e.g., Q=1000 -> max std err 0.0476; Q=10000 -> 0.0129; differences vs Q=50000 reported: 0.0158 and 0.0051 respectively), mean OC differences between Q values (e.g., mean OC difference between Q=10000 and 50000 is 0.0073).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to nested-batch exploration (random), nested-batch exploitation, nested-batch epsilon-greedy, sequential KG, sequential exploration/exploitation/epsilon-greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Empirically, BKG (as component of NBKG experiments) outperforms nested-batch random and greedy baselines in mean opportunity cost; larger batch sizes reduce OC for a fixed number of batches; when equating one batch to B sequential measurements, NBKG/BKG are competitive with sequential KG though sequential KG can be more adaptive.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical efficiency: for fixed K, larger B yields lower OC (more information per decision). No single percent reduction in experiments reported as a universal number; Monte Carlo sample-size tuning indicates Q≈10000 gives negligible difference versus Q=50000 (mean OC diff 0.0073).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Analyzed tradeoffs include: (1) batch size B vs adaptivity (bigger B gives more information per epoch but delays state updates until whole batch completes); (2) Monte Carlo cost Q vs accuracy of expected-max estimate (standard error scales ~1/sqrt(Q) and affects decision quality); (3) measurement noise level vs required number of batches to reach target OC (OC increases with noise).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Within a batch-mode constrained setting, greedily maximizing marginal BKG is effective; it is always optimal to fill the batch (monotonic benefit of measurements). For computational approximation, Q on the order of 10k Monte Carlo samples was empirically sufficient in the experiments; increasing B reduces OC for a fixed count of batches K but reduces adaptivity compared to sequential sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science', 'publication_date_yy_mm': '2015-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2595.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2595.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NBKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nested-Batch Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-level KG extension for nested experimental settings: choose a first-stage decision (e.g., nanoparticle size) and then select a batch of second-stage experiments (e.g., densities) conditioned on the first-stage choice to maximize expected terminal-value increment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nested-Batch Knowledge Gradient (NBKG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NBKG addresses nested decision structures where an outer discrete choice (e.g., a fabrication setting that is costly to change) is followed by a batch of inner (cheaper) experiments. For each candidate outer choice x, NBKG computes the best batch (multi-set) of inner choices Y that maximizes the expected increase in the terminal objective if x is fixed and that batch Y measured; then it selects the x (and associated Y) producing the highest nested KG. The inner batch selection for each x is solved via the BKG greedy Monte Carlo procedure; the expectation (expected max after nested-batch) is approximated via Monte Carlo.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Nested experimental settings in laboratory sciences (materials science example: choose nanoparticle size once per batch then run multiple density experiments in parallel); applicable to any hierarchical experimental design where some parameters are expensive to change and others cheap/parallelizable.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Two-stage allocation: for each outer candidate x, use BKG to find the batch of inner experiments Y maximizing the nested KG value; then allocate the experimental batch to the (x*, Y*) with the highest nested KG. This jointly allocates scarce expensive resources (outer decisions) and cheaper parallel inner experiments to maximize expected improvement in the final recommendation under a fixed total budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computational cost described by number of outer candidates |X1|, inner alternatives |X2|, batch size B, Monte Carlo sampling Q used to approximate nested expectations, and total number of batches K. Cost primarily measured as number of Monte Carlo draws × number of candidate evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Nested KG: E[max_{(x',y')} theta_{(x',y')}^{post-nested-batch}] - max_{(x',y')} theta_{(x',y')}^{current}; for each outer x the inner batch NBKG value is maximized (nu_x^{NKG} = max_Y nu_{x;Y}^{NBKG}).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balances exploration/exploitation across nested dimensions by selecting outer choices that enable inner batches expected to yield the largest improvement; exploration arises by selecting outer x with high nested KG due to uncertain or promising inner alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity objective beyond the KG-driven selection; BKG inner selection can select repeated inner points (multi-set) if that maximizes expected terminal gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of batches K and fixed inner batch size B (so fixed total measurements N = B×K); computational budget for Monte Carlo sampling Q.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>NBKG optimizes nested expected information under fixed measurement budget by selecting the outer-inner batch pair that maximizes expected terminal gain per batch; uses greedy BKG inner optimization and Monte Carlo approximations under available computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Opportunity cost (OC) used as the primary metric for discovery quality; locating a much higher-performing (x,y) reduces OC and indicates breakthrough discovery in the application setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean opportunity cost vs number of batch measurements K (averaged across 200–500 trials); empirical plots show rapid decay in OC (e.g., OC quickly approaches low values within 15 batch measurements). Also reports Monte Carlo sampling diagnostics as in BKG (Q effect on std error and OC differences).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to nested-batch exploration/exploitation/epsilon-greedy, sequential KG, sequential exploration/exploitation/epsilon-greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>NBKG outperforms all nested-batch baselines in mean OC and is competitive with sequential KG. In experiments, NBKG provided savings from batch-mode experiments versus sequential-only approaches and significantly outperformed naive nested random/exploit/explore strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical: for fixed number of batches, larger inner batch sizes reduce OC (more information per batch). NBKG showed faster identification of high-performing alternatives than random or naive baselines; no single global percentage efficiency figure given.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper analyzes: (1) tradeoff between batch parallelism (B) and adaptivity (sequential updates): larger B gives more information per epoch but delays posterior updates until whole batch completes; (2) the Monte Carlo approximation cost (Q) vs decision quality; (3) measurement noise effect on required budget (OC increases with noise). Empirical results illustrate these tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommend a two-stage nested KG approach: for nested structures where some parameters are expensive to change, first choose the expensive (outer) parameter with the highest nested KG (which accounts for the best inner batch), then run the inner batch found by the batch KG. Always fill the inner batch (monotonic benefit), and choose Monte Carlo sample sizes (empirically Q≈10k in this study) sufficient to control sampling noise in KG estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science', 'publication_date_yy_mm': '2015-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2595.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2595.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MonteCarlo-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo approximation for Knowledge Gradient expected-max</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical Monte Carlo sampling procedure to approximate the intractable expected maximum (expected value of information) needed by KG/BKG/NBKG when analytic computation is intractable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Monte Carlo Knowledge Gradient approximation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Because the KG objective requires E[max_x theta_x^{post}] which is analytically intractable for correlated beliefs and batch measurements, the paper uses Monte Carlo sampling: draw Q independent standard normal vectors for the future measurement noise variables, simulate posterior mean updates deterministically for each draw, compute the posterior maximum for each draw, and average to approximate the expected max; standard error scales as s_mc / sqrt(Q), and Q is tuned (experiments use Q up to 50000 to validate convergence, with Q≈10000 recommended as sufficient in their application).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Computational approximation for acquisition functions in Bayesian experimental design and active learning (used inside KG/BKG/NBKG policies).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates computational sampling budget Q to approximate the KG expectations; tradeoff between computational cost (Q) and estimation accuracy affects the quality of experimental allocation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of Monte Carlo samples Q (and resulting number of evaluations of max over alternatives per candidate), plus cost of updating covariances and means per sample; empirically measured via sample standard error and differences in decision outcomes for different Q.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Approximates the KG information gain metric (expected increase in terminal posterior max) via Monte Carlo averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>None intrinsic; this is an approximation mechanism used within KG/BKG/NBKG which provide the exploration/exploitation balance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget (number of Monte Carlo samples Q) and experimental budget (B, K) interact; Q chosen to meet a tolerance on KG estimate standard error.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Paper recommends selecting Q so that the maximum standard error across candidate inner alternatives is below a tolerance; shows empirical diagnostics and chooses Q≈10000 as a cost-accuracy tradeoff for their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Indirect: improved approximation quality leads to better experiment allocation and thus higher chance to discover high-performing alternatives, measured via reductions in opportunity cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Monte Carlo diagnostics: maximum standard error s_mc / sqrt(Q) across densities (examples: Q=1000 -> 0.0476; Q=10000 -> 0.0129), maximum decision differences relative to Q=50000 (examples: Q=1000 -> max diff 0.0158; Q=10000 -> 0.0051), mean OC difference between Q=10000 and Q=50000 = 0.0073.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared different Q values (1000, 10000, 50000) to evaluate impact on decision quality and OC; also compared to exact/semi-analytic KG when available (not feasible in batch/nested batch settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Q≈10000 yielded near-identical KG decisions and opportunity-cost performance to Q=50000 in experiments, indicating diminishing returns beyond that sampling budget for the example problem.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Selecting Q≈10000 achieved similar decision quality as Q=50000 at roughly 1/5th of Monte Carlo cost in their experiments (empirical evidence via OC difference ~0.0073).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Explicitly analyzes the computational-accuracy tradeoff: standard error decreases as 1/sqrt(Q), and decision/OC sensitivity diminishes with larger Q; recommends choosing Q to control maximum standard error below a tolerance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Choose Monte Carlo sampling Q large enough to make KG estimate variance negligible relative to decision thresholds (empirical Q≈10000 for their problem), balancing computation time against decision quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science', 'publication_date_yy_mm': '2015-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2595.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2595.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChenKrause2013</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Near-optimal batch mode active learning and adaptive submodular optimization (Chen & Krause 2013)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch-mode active learning approach and theoretical framework for information-parallel stochastic optimization which aims to reach a set-function threshold while minimizing number of items allocated, built on adaptive submodularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Near-optimal batch mode active learning and adaptive submodular optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Batch-mode active learning via adaptive submodularity (Chen & Krause)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithmic framework for batch active learning and information-parallel stochastic optimization that uses notions of adaptive submodularity to provide near-optimal guarantees for maximizing certain set functions (e.g., objective exceeding a threshold) while minimizing sample count; designed specifically for batch-mode active learning objectives rather than final-best-alternative objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch active learning and information-parallel experiments; more general machine learning active learning settings.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select batches to maximize marginal gains in a submodular objective (minimize number of items required to reach an objective threshold), with theoretical near-optimality guarantees under adaptive submodularity assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not specified in this paper beyond standard algorithmic costs; cited as related work and contrasted with KG-based target (final single-best recommendation) used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Set-function marginal gain (submodular objective) and thresholds rather than expected terminal max used in KG.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Controlled via greedy maximization of marginal gains in an (adaptively) submodular objective; implicitly trades exploration and exploitation according to the chosen objective.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Adaptive submodularity often encourages diversity because marginal gains diminish for redundant items; explicit diversity control depends on the set function used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Minimize number of items to exceed a threshold (sample budget) — different framing from fixed-budget KG; mentioned as not directly comparable to KG objective.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Greedy algorithms with theoretical guarantees under adaptive submodularity; optimized to minimize sample count required to reach objective thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective-based threshold (not specifically 'breakthrough'); focuses on achieving target information rather than maximizing a single alternative's expected value.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical approximation ratios under adaptive submodularity and empirical performance in active learning benchmarks (details in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Greedy and other active learning baselines in original Chen & Krause work; mentioned here as related literature but with different objective (thresholding vs final-recommendation KG objective).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not evaluated in this paper (this paper cites Chen & Krause as related work and notes differences in objective and generality).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Provable near-optimality under adaptive submodularity assumptions in the original work; cited here for context.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Original paper examines tradeoffs for threshold-based objectives and sample minimization; this paper contrasts that with KG's terminal-recommendation objective.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Cited as an alternative batch active learning framework that optimizes a different objective and cannot be directly generalized to the KG terminal-maximization setting used here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science', 'publication_date_yy_mm': '2015-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A knowledge-gradient policy for sequential information collection <em>(Rating: 2)</em></li>
                <li>The knowledge-gradient policy for correlated normal beliefs <em>(Rating: 2)</em></li>
                <li>The knowledge-gradient algorithm for sequencing experiments in drug discovery <em>(Rating: 2)</em></li>
                <li>Near-optimal batch mode active learning and adaptive submodular optimization <em>(Rating: 2)</em></li>
                <li>A Monte Carlo knowledge gradient method for learning abatement potential of emissions reduction technologies <em>(Rating: 2)</em></li>
                <li>The knowledge gradient algorithm for online subset selection <em>(Rating: 2)</em></li>
                <li>Hierarchical knowledge gradient for sequential sampling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2595",
    "paper_id": "paper-e135d9529a17e8f624fe0d7a62a90bbe822ff4ac",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "KG",
            "name_full": "Knowledge Gradient",
            "brief_description": "A decision-theoretic acquisition rule that selects the next measurement to maximize the single-period expected increase in the value of the final recommended alternative (expected value of information measured as increase in the posterior max mean).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Knowledge Gradient (KG)",
            "system_description": "KG is a Bayesian optimal-learning acquisition policy for ranking-and-selection problems with correlated normal beliefs. At a given knowledge state (posterior mean vector and covariance), KG computes for each candidate alternative the expected increase in the terminal objective (the maximum posterior mean after measurement) that would result from measuring that alternative, and selects the alternative maximizing this expected increment. The posterior is updated by Bayes' rule (closed-form Gaussian updates); the KG score requires computing an expectation over the one-step posterior maximum, i.e., an expectation of a max of normals.",
            "application_domain": "Sequential experimental design / ranking-and-selection; demonstrated on materials science (nanoparticle size/density optimization) but applicable to drug discovery, simulation optimization, and other experimental sciences.",
            "resource_allocation_strategy": "Select a single alternative x that maximizes the expected one-step increase in the terminal recommended alternative's mean (the KG value); this allocates the next measurement to the point with highest value of information for improving the final decision.",
            "computational_cost_metric": "Not expressed in FLOPs or wall time; cost characterized by number of physical measurements (N), dimensionality M (number of alternatives), and the computational effort required to compute expected values (often Monte Carlo samples when analytic evaluation is intractable).",
            "information_gain_metric": "Expected increase in the terminal objective: E[max_x' theta_x'^{new}] - max_x' theta_x'^{current} (expected improvement in the posterior maximum).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Implicitly balances exploration and exploitation by quantifying the expected benefit (to the final decision) of information from each candidate; alternatives with high uncertainty and/or promising mean can be picked if expected to raise the posterior maximum.",
            "diversity_mechanism": "No explicit diversity-promoting regularizer; diversity arises implicitly through posterior correlation (KG reduces KG values for nearby/correlated alternatives after measurements).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-experiments budget (N = number of allowed measurements) and per-batch size when used in batch variants.",
            "budget_constraint_handling": "KG is used within a dynamic programming framing where the policy maximizes expected terminal reward subject to a fixed measurement budget; it greedily picks measurements that maximize expected one-step improvement under remaining budget (single-step approximation).",
            "breakthrough_discovery_metric": "No explicit novelty/breakthrough score; breakthrough implied by large reduction in opportunity cost or discovering a much higher-performing alternative (measured via posterior mean and final realized objective).",
            "performance_metrics": "Opportunity cost (OC) = true maximum mu - mu_recommended; reported as mean OC over trials and plotted versus number of measurements. Other reported quantities include reduction in OC with more measurements.",
            "comparison_baseline": "Compared against pure exploration (random), pure exploitation (greedy by current mean), epsilon-greedy, nested-batch and sequential variants in experiments.",
            "performance_vs_baseline": "KG (sequential) is competitive with and often outperforms naive baselines (random, exploitation, epsilon-greedy); in nested-batch contexts, NBKG (the nested-batch variant) outperforms nested-batch baselines and is competitive with sequential KG (plots show lower mean OC for KG/NBKG than baselines).",
            "efficiency_gain": "Qualitative: KG yields faster decay in opportunity cost versus random/exploit/explore baselines; specific numeric gains not provided as a single percentage in the paper (results shown in mean OC plots).",
            "tradeoff_analysis": "KG inherently trades off exploration and exploitation through the expected-value-of-information calculation; the paper analyzes tradeoffs between adaptivity (sequential updates) and parallelism (batch measurements) qualitatively and empirically (e.g., batch size vs adaptivity).",
            "optimal_allocation_findings": "KG is recommended as an effective allocation rule for maximizing the final recommended alternative under a fixed budget; acts as a principled rule to allocate measurements to maximize expected terminal improvement.",
            "uuid": "e2595.0",
            "source_info": {
                "paper_title": "Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science",
                "publication_date_yy_mm": "2015-05"
            }
        },
        {
            "name_short": "BKG",
            "name_full": "Batch Knowledge Gradient",
            "brief_description": "An extension of KG to select B measurements simultaneously: greedily build a batch by adding the alternative that maximizes the marginal KG conditioned on earlier choices in the same batch, using Monte Carlo approximation for expectations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Batch Knowledge Gradient (BKG)",
            "system_description": "BKG chooses a batch of B experiments per decision epoch by sequentially (greedily) adding alternatives: start with the highest single-measurement KG for the first slot, then for each subsequent slot pick the candidate that maximizes the expected marginal increase in the terminal objective given the previous picks in the same batch. The expectation of the max of posterior means after the whole batch is approximated with Monte Carlo sampling; posterior covariance is updated within the batch deterministically (since covariance updates do not depend on observed values).",
            "application_domain": "Batch-mode experimental design in laboratory sciences; demonstrated for nested/parallel experiments in materials science (nanoparticle density sweeps given fixed size), but applicable across domains with batched experimental capacity.",
            "resource_allocation_strategy": "Greedy multi-set maximization of expected terminal-value increment: repeatedly pick the alternative with the largest marginal BKG value until B items are chosen (multi-set allowed — repeats permitted). This allocates finite experimental resources among alternatives in each batch to maximize expected information benefit for the final recommendation.",
            "computational_cost_metric": "Characterized in paper by Monte Carlo sample size Q (number of random draws to approximate expectations), batch size B, number of alternatives M, and number of batches K; computational cost effectively measured in number of Monte Carlo evaluations (Q × M × B) plus cost of posterior updates.",
            "information_gain_metric": "Batch KG value: E[max_x theta_x^{post-batch}] - max_x theta_x^{current}; marginal BKG values are differences of these when adding elements to the batch.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Greedy marginal-expected-improvement selection within a batch: items prioritized by their expected contribution to improving the final recommended mean (combines exploration — uncertainty reduction — and exploitation — raising promising means).",
            "diversity_mechanism": "No explicit diversity regularizer; the greedy marginal KG implicitly reduces selecting near-duplicate/correlated items because posterior covariance updates within the batch reduce marginal KG for correlated neighbors. The approach permits repeated measurements (multi-set), but does not include an explicit diversity objective.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed batch size B per decision and fixed total number of batches K (so total measurements N = B×K); also a computational budget (Monte Carlo Q) for expectation approximation.",
            "budget_constraint_handling": "Given B and K, BKG always fills the batch (theoretical result: more measurements never reduce value), selecting B items greedily subject to available experiments; computational budget traded off via choosing Q for Monte Carlo approximation (paper empirically determines Q sufficient for accuracy).",
            "breakthrough_discovery_metric": "Opportunity cost (OC) reduction and locating maximal photocurrent; no separate novelty/breakthrough score is defined.",
            "performance_metrics": "Mean opportunity cost versus number of batch measurements K (plotted), Monte Carlo sampling diagnostics: maximum standard error across densities s_mc / sqrt(Q) (e.g., Q=1000 -&gt; max std err 0.0476; Q=10000 -&gt; 0.0129; differences vs Q=50000 reported: 0.0158 and 0.0051 respectively), mean OC differences between Q values (e.g., mean OC difference between Q=10000 and 50000 is 0.0073).",
            "comparison_baseline": "Compared to nested-batch exploration (random), nested-batch exploitation, nested-batch epsilon-greedy, sequential KG, sequential exploration/exploitation/epsilon-greedy.",
            "performance_vs_baseline": "Empirically, BKG (as component of NBKG experiments) outperforms nested-batch random and greedy baselines in mean opportunity cost; larger batch sizes reduce OC for a fixed number of batches; when equating one batch to B sequential measurements, NBKG/BKG are competitive with sequential KG though sequential KG can be more adaptive.",
            "efficiency_gain": "Empirical efficiency: for fixed K, larger B yields lower OC (more information per decision). No single percent reduction in experiments reported as a universal number; Monte Carlo sample-size tuning indicates Q≈10000 gives negligible difference versus Q=50000 (mean OC diff 0.0073).",
            "tradeoff_analysis": "Analyzed tradeoffs include: (1) batch size B vs adaptivity (bigger B gives more information per epoch but delays state updates until whole batch completes); (2) Monte Carlo cost Q vs accuracy of expected-max estimate (standard error scales ~1/sqrt(Q) and affects decision quality); (3) measurement noise level vs required number of batches to reach target OC (OC increases with noise).",
            "optimal_allocation_findings": "Within a batch-mode constrained setting, greedily maximizing marginal BKG is effective; it is always optimal to fill the batch (monotonic benefit of measurements). For computational approximation, Q on the order of 10k Monte Carlo samples was empirically sufficient in the experiments; increasing B reduces OC for a fixed count of batches K but reduces adaptivity compared to sequential sampling.",
            "uuid": "e2595.1",
            "source_info": {
                "paper_title": "Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science",
                "publication_date_yy_mm": "2015-05"
            }
        },
        {
            "name_short": "NBKG",
            "name_full": "Nested-Batch Knowledge Gradient",
            "brief_description": "A two-level KG extension for nested experimental settings: choose a first-stage decision (e.g., nanoparticle size) and then select a batch of second-stage experiments (e.g., densities) conditioned on the first-stage choice to maximize expected terminal-value increment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Nested-Batch Knowledge Gradient (NBKG)",
            "system_description": "NBKG addresses nested decision structures where an outer discrete choice (e.g., a fabrication setting that is costly to change) is followed by a batch of inner (cheaper) experiments. For each candidate outer choice x, NBKG computes the best batch (multi-set) of inner choices Y that maximizes the expected increase in the terminal objective if x is fixed and that batch Y measured; then it selects the x (and associated Y) producing the highest nested KG. The inner batch selection for each x is solved via the BKG greedy Monte Carlo procedure; the expectation (expected max after nested-batch) is approximated via Monte Carlo.",
            "application_domain": "Nested experimental settings in laboratory sciences (materials science example: choose nanoparticle size once per batch then run multiple density experiments in parallel); applicable to any hierarchical experimental design where some parameters are expensive to change and others cheap/parallelizable.",
            "resource_allocation_strategy": "Two-stage allocation: for each outer candidate x, use BKG to find the batch of inner experiments Y maximizing the nested KG value; then allocate the experimental batch to the (x*, Y*) with the highest nested KG. This jointly allocates scarce expensive resources (outer decisions) and cheaper parallel inner experiments to maximize expected improvement in the final recommendation under a fixed total budget.",
            "computational_cost_metric": "Computational cost described by number of outer candidates |X1|, inner alternatives |X2|, batch size B, Monte Carlo sampling Q used to approximate nested expectations, and total number of batches K. Cost primarily measured as number of Monte Carlo draws × number of candidate evaluations.",
            "information_gain_metric": "Nested KG: E[max_{(x',y')} theta_{(x',y')}^{post-nested-batch}] - max_{(x',y')} theta_{(x',y')}^{current}; for each outer x the inner batch NBKG value is maximized (nu_x^{NKG} = max_Y nu_{x;Y}^{NBKG}).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balances exploration/exploitation across nested dimensions by selecting outer choices that enable inner batches expected to yield the largest improvement; exploration arises by selecting outer x with high nested KG due to uncertain or promising inner alternatives.",
            "diversity_mechanism": "No explicit diversity objective beyond the KG-driven selection; BKG inner selection can select repeated inner points (multi-set) if that maximizes expected terminal gain.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of batches K and fixed inner batch size B (so fixed total measurements N = B×K); computational budget for Monte Carlo sampling Q.",
            "budget_constraint_handling": "NBKG optimizes nested expected information under fixed measurement budget by selecting the outer-inner batch pair that maximizes expected terminal gain per batch; uses greedy BKG inner optimization and Monte Carlo approximations under available computational budget.",
            "breakthrough_discovery_metric": "Opportunity cost (OC) used as the primary metric for discovery quality; locating a much higher-performing (x,y) reduces OC and indicates breakthrough discovery in the application setting.",
            "performance_metrics": "Mean opportunity cost vs number of batch measurements K (averaged across 200–500 trials); empirical plots show rapid decay in OC (e.g., OC quickly approaches low values within 15 batch measurements). Also reports Monte Carlo sampling diagnostics as in BKG (Q effect on std error and OC differences).",
            "comparison_baseline": "Compared to nested-batch exploration/exploitation/epsilon-greedy, sequential KG, sequential exploration/exploitation/epsilon-greedy.",
            "performance_vs_baseline": "NBKG outperforms all nested-batch baselines in mean OC and is competitive with sequential KG. In experiments, NBKG provided savings from batch-mode experiments versus sequential-only approaches and significantly outperformed naive nested random/exploit/explore strategies.",
            "efficiency_gain": "Empirical: for fixed number of batches, larger inner batch sizes reduce OC (more information per batch). NBKG showed faster identification of high-performing alternatives than random or naive baselines; no single global percentage efficiency figure given.",
            "tradeoff_analysis": "Paper analyzes: (1) tradeoff between batch parallelism (B) and adaptivity (sequential updates): larger B gives more information per epoch but delays posterior updates until whole batch completes; (2) the Monte Carlo approximation cost (Q) vs decision quality; (3) measurement noise effect on required budget (OC increases with noise). Empirical results illustrate these tradeoffs.",
            "optimal_allocation_findings": "Recommend a two-stage nested KG approach: for nested structures where some parameters are expensive to change, first choose the expensive (outer) parameter with the highest nested KG (which accounts for the best inner batch), then run the inner batch found by the batch KG. Always fill the inner batch (monotonic benefit), and choose Monte Carlo sample sizes (empirically Q≈10k in this study) sufficient to control sampling noise in KG estimates.",
            "uuid": "e2595.2",
            "source_info": {
                "paper_title": "Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science",
                "publication_date_yy_mm": "2015-05"
            }
        },
        {
            "name_short": "MonteCarlo-KG",
            "name_full": "Monte Carlo approximation for Knowledge Gradient expected-max",
            "brief_description": "A practical Monte Carlo sampling procedure to approximate the intractable expected maximum (expected value of information) needed by KG/BKG/NBKG when analytic computation is intractable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Monte Carlo Knowledge Gradient approximation",
            "system_description": "Because the KG objective requires E[max_x theta_x^{post}] which is analytically intractable for correlated beliefs and batch measurements, the paper uses Monte Carlo sampling: draw Q independent standard normal vectors for the future measurement noise variables, simulate posterior mean updates deterministically for each draw, compute the posterior maximum for each draw, and average to approximate the expected max; standard error scales as s_mc / sqrt(Q), and Q is tuned (experiments use Q up to 50000 to validate convergence, with Q≈10000 recommended as sufficient in their application).",
            "application_domain": "Computational approximation for acquisition functions in Bayesian experimental design and active learning (used inside KG/BKG/NBKG policies).",
            "resource_allocation_strategy": "Allocates computational sampling budget Q to approximate the KG expectations; tradeoff between computational cost (Q) and estimation accuracy affects the quality of experimental allocation decisions.",
            "computational_cost_metric": "Number of Monte Carlo samples Q (and resulting number of evaluations of max over alternatives per candidate), plus cost of updating covariances and means per sample; empirically measured via sample standard error and differences in decision outcomes for different Q.",
            "information_gain_metric": "Approximates the KG information gain metric (expected increase in terminal posterior max) via Monte Carlo averaging.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "None intrinsic; this is an approximation mechanism used within KG/BKG/NBKG which provide the exploration/exploitation balance.",
            "diversity_mechanism": "None.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational budget (number of Monte Carlo samples Q) and experimental budget (B, K) interact; Q chosen to meet a tolerance on KG estimate standard error.",
            "budget_constraint_handling": "Paper recommends selecting Q so that the maximum standard error across candidate inner alternatives is below a tolerance; shows empirical diagnostics and chooses Q≈10000 as a cost-accuracy tradeoff for their experiments.",
            "breakthrough_discovery_metric": "Indirect: improved approximation quality leads to better experiment allocation and thus higher chance to discover high-performing alternatives, measured via reductions in opportunity cost.",
            "performance_metrics": "Monte Carlo diagnostics: maximum standard error s_mc / sqrt(Q) across densities (examples: Q=1000 -&gt; 0.0476; Q=10000 -&gt; 0.0129), maximum decision differences relative to Q=50000 (examples: Q=1000 -&gt; max diff 0.0158; Q=10000 -&gt; 0.0051), mean OC difference between Q=10000 and Q=50000 = 0.0073.",
            "comparison_baseline": "Compared different Q values (1000, 10000, 50000) to evaluate impact on decision quality and OC; also compared to exact/semi-analytic KG when available (not feasible in batch/nested batch settings).",
            "performance_vs_baseline": "Q≈10000 yielded near-identical KG decisions and opportunity-cost performance to Q=50000 in experiments, indicating diminishing returns beyond that sampling budget for the example problem.",
            "efficiency_gain": "Selecting Q≈10000 achieved similar decision quality as Q=50000 at roughly 1/5th of Monte Carlo cost in their experiments (empirical evidence via OC difference ~0.0073).",
            "tradeoff_analysis": "Explicitly analyzes the computational-accuracy tradeoff: standard error decreases as 1/sqrt(Q), and decision/OC sensitivity diminishes with larger Q; recommends choosing Q to control maximum standard error below a tolerance.",
            "optimal_allocation_findings": "Choose Monte Carlo sampling Q large enough to make KG estimate variance negligible relative to decision thresholds (empirical Q≈10000 for their problem), balancing computation time against decision quality.",
            "uuid": "e2595.3",
            "source_info": {
                "paper_title": "Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science",
                "publication_date_yy_mm": "2015-05"
            }
        },
        {
            "name_short": "ChenKrause2013",
            "name_full": "Near-optimal batch mode active learning and adaptive submodular optimization (Chen & Krause 2013)",
            "brief_description": "A batch-mode active learning approach and theoretical framework for information-parallel stochastic optimization which aims to reach a set-function threshold while minimizing number of items allocated, built on adaptive submodularity.",
            "citation_title": "Near-optimal batch mode active learning and adaptive submodular optimization",
            "mention_or_use": "mention",
            "system_name": "Batch-mode active learning via adaptive submodularity (Chen & Krause)",
            "system_description": "Algorithmic framework for batch active learning and information-parallel stochastic optimization that uses notions of adaptive submodularity to provide near-optimal guarantees for maximizing certain set functions (e.g., objective exceeding a threshold) while minimizing sample count; designed specifically for batch-mode active learning objectives rather than final-best-alternative objectives.",
            "application_domain": "Batch active learning and information-parallel experiments; more general machine learning active learning settings.",
            "resource_allocation_strategy": "Select batches to maximize marginal gains in a submodular objective (minimize number of items required to reach an objective threshold), with theoretical near-optimality guarantees under adaptive submodularity assumptions.",
            "computational_cost_metric": "Not specified in this paper beyond standard algorithmic costs; cited as related work and contrasted with KG-based target (final single-best recommendation) used in this paper.",
            "information_gain_metric": "Set-function marginal gain (submodular objective) and thresholds rather than expected terminal max used in KG.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Controlled via greedy maximization of marginal gains in an (adaptively) submodular objective; implicitly trades exploration and exploitation according to the chosen objective.",
            "diversity_mechanism": "Adaptive submodularity often encourages diversity because marginal gains diminish for redundant items; explicit diversity control depends on the set function used.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Minimize number of items to exceed a threshold (sample budget) — different framing from fixed-budget KG; mentioned as not directly comparable to KG objective.",
            "budget_constraint_handling": "Greedy algorithms with theoretical guarantees under adaptive submodularity; optimized to minimize sample count required to reach objective thresholds.",
            "breakthrough_discovery_metric": "Objective-based threshold (not specifically 'breakthrough'); focuses on achieving target information rather than maximizing a single alternative's expected value.",
            "performance_metrics": "Theoretical approximation ratios under adaptive submodularity and empirical performance in active learning benchmarks (details in the cited paper).",
            "comparison_baseline": "Greedy and other active learning baselines in original Chen & Krause work; mentioned here as related literature but with different objective (thresholding vs final-recommendation KG objective).",
            "performance_vs_baseline": "Not evaluated in this paper (this paper cites Chen & Krause as related work and notes differences in objective and generality).",
            "efficiency_gain": "Provable near-optimality under adaptive submodularity assumptions in the original work; cited here for context.",
            "tradeoff_analysis": "Original paper examines tradeoffs for threshold-based objectives and sample minimization; this paper contrasts that with KG's terminal-recommendation objective.",
            "optimal_allocation_findings": "Cited as an alternative batch active learning framework that optimizes a different objective and cannot be directly generalized to the KG terminal-maximization setting used here.",
            "uuid": "e2595.4",
            "source_info": {
                "paper_title": "Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science",
                "publication_date_yy_mm": "2015-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A knowledge-gradient policy for sequential information collection",
            "rating": 2
        },
        {
            "paper_title": "The knowledge-gradient policy for correlated normal beliefs",
            "rating": 2
        },
        {
            "paper_title": "The knowledge-gradient algorithm for sequencing experiments in drug discovery",
            "rating": 2
        },
        {
            "paper_title": "Near-optimal batch mode active learning and adaptive submodular optimization",
            "rating": 2
        },
        {
            "paper_title": "A Monte Carlo knowledge gradient method for learning abatement potential of emissions reduction technologies",
            "rating": 2
        },
        {
            "paper_title": "The knowledge gradient algorithm for online subset selection",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical knowledge gradient for sequential sampling",
            "rating": 1
        }
    ],
    "cost": 0.0196375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NESTED BATCH MODE LEARNING AND STOCHASTIC OPTIMIZATION WITH AN APPLICATION TO SEQUENTIAL MULTI-STAGE TESTING IN MATERIALS SCIENCE *</h1>
<p>YINGFEI WANG ${ }^{\dagger}$, KRISTOFER G. REYES ${ }^{\ddagger}$, KEITH A. BROWN ${ }^{\S}$, CHAD A. MIRKIN ${ }^{\S}$,<br>AND WARREN B. POWELL ${ }^{\ddagger}$</p>
<h4>Abstract</h4>
<p>We consider the nested-batch decision problem where we need to make a first stage choice (e.g. the size of a nanoparticle) after which we then need to run a series of experiments in batch selecting several second stage choices (e.g. testing different densities of the nanoparticle). Since these experiments are time consuming and expensive, we propose to estimate the value of information from the choice of the first stage decision (the size), to help guide the scientist in the selection of the next batch of experiments to run. The batch experiments are designed assuming that we maximize the value of information for an entire batch. The value of information, known as the Knowledge Gradient, requires calculating the expected maximum of a function. Since the calculation of the expected maximum is computationally intractable, we propose a Monte Carlo-based approach to address this hurdle in the context of both the batch and nested-batch problems. We empirically demonstrate the effectiveness of our approach on the material design problem of maximizing output current of a photoactive device, where it is competitive with a fully sequential optimal learning strategy and significantly outperforms pure exploration, pure exploitation and $\epsilon$-greedy strategies with regard to the opportunity cost metric (8.1).</p>
<p>Key words. optimal learning, materials science, sequential design of experiments, decision making, dynamic programming, knowledge gradient</p>
<p>AMS subject classifications. 68T05, 62F07, 62F15, 93E35, 90C39, 90C40</p>
<ol>
<li>Introduction. Our work is motivated by problems in the laboratory sciences where we have to select a series of parameters (e.g. size, shape, density and concentration) that guide the design of a material where we are trying to achieve a particular goal (e.g. maximum strength, conductivity, or reflexivity). For example, in this paper we are interested in identifying the density, size and type of nanostructures on the surface of a photoactive device that maximizes output current (see Section 2 for more details). The number of potential parameter settings is much larger than we can explore experimentally, especially when we consider that an experiment can take hours or even days. This is exacerbated by the complication that certain parameters may be more difficult to vary than others in a serial fashion.</li>
</ol>
<p>There are several factors contributing to this. First is the curse of dimensionality, in which the set of potential experiments (identified by a selection of tunable parameters) increases exponentially with the number of tunable parameters. Second is the continuous nature of certain parameters. For example, the density or concentration of a solute in solution may often be varied within several orders of magnitude, and yet the optimum selection of density could occur within a small window of values. This problem of separation of scales may be naively dealt with by using a refined discretization, which results in a large number of experimental alternatives. Third is</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the fact that physically, varying one parameter may be more difficult than another. For example, in our reflexivity problem (see Section 2), a selection of "type" and size of nanostructure (e.g. nanorods, nanodots or some other geometrical shape) entails physically fabricating such a structure, which may take a day in the laboratory. Contrast this with a selection of density, which can be varied more readily by an appropriate choice of solution concentration.</p>
<p>Scientists can draw on an extensive body of literature on the classic design of experiments $[8,30,21]$ whose goal is to decide what observations to make when fitting a function. Yet in the laboratory settings considered in this paper, the decisions need to be guided by a well-defined objective function (for example, maximizing output current) aside from the intent to learn the fitting. Moreover, many such classical techniques fail to account for practicalities such as the difficulties in performing potentially vastly different experiments in a sequential manner. Previous work [12, 24, 11, 22] develops a sequential design of experiments in a Bayesian setting, but is suited for problems in which experiments are expensive and must be run one at a time.</p>
<p>This sequential design of experiments fails to account for the realities encountered by experimentalists, who may be able to run several parallel experiments in batches. For example, an experimenter can easily vary nanoparticle density over a sample, effectively performing parallel, batch experiments through a single sample. While the idea of batch experimentation is well established throughout all of the physical sciences, recently new experimental tools have provided experimentalists the ability to vary parameters such as surface feature lengths and areas on the nanometer length scale [14]. As a second constraint, it may be difficult or expensive for a scientist to explore the set of experiments in the order prescribed by the knowledge gradient policy, which often suggests consecutively vastly different experiments. For example, the choice of nanoparticle size described above cannot be readily changed between experiments since their fabrication is expensive. The choice of nanoparticle size and density can therefore be modeled as a nested decision in which a nanoparticle size is first selected, and several densities are chosen to maximize the marginal value of information given the fixed nanoparticle size. Such batch and nested batch experimental modes must be taken into consideration in designing a sequence of experiments.</p>
<p>In this paper, we extend the knowledge gradient concept to handle both batch experiments, as well as nested experiments that are performed within a batch. We derive the marginal value of information for each possible experiment which the scientists can use as a guide.</p>
<p>Commonly used sequential decision making policies $[3,5,12,16]$ allocate only one alternative at a time and are not directly applicable to the above mentioned batch and nested batch experimental setting. More relevant to this setting, there exists literature on stochastic and/or adversarial bandit problems addressing the problem of multi-plays (playing several alternatives at the same time), which can be viewed as batch-mode decision making $[1,2,29,17,6,25,15]$. However, the bandit objective is to maximize the cumulative rewards over time, which is not suitable for our laboratory setting where the objective is to find the controllable parameters that maximizes some utility function. Chen and Krause [7] have studied batch mode active learning and more general information-parallel stochastic optimization problems. But their objective is to let a set function exceed a threshold value while at the same time minimizing the number of items allocated. Moreover, the proposed algorithm in [7] is specifically designed for batch-mode active learning and cannot be generalized to other information-parallel stochastic optimization problems.</p>
<p>The most related models are the stochastic subset selection problems introduced in $[26,27]$, where the choice in each round is a subset of alternatives while the objective is to find the set of alternatives that maximizes some function on such sets. This differs fundamentally from finding one alternative that maximizes some utility function through batch measurements, as is the case in our setting. In [26, 27], the way to recommend a set of alternatives in each round is to treat each subset of alternatives as a singly super alternative in the space of subsets and construct beliefs over the set function values rather than the function values of the alternatives. The number of subsets with B elements out of M elements is $\binom{M}{B}$, which grows exponentially with the number of alternatives. As the number of alternatives increases, even storing and updating the requisite $\binom{M}{B} \times\binom{M}{B}$ covariance matrix becomes problematic. For example, the size of the choice set considered in [27] was $\binom{10}{5}=252$. Instead, to address this, we derive the policies presented in this paper with beliefs on the function values of the alternatives whose number is far smaller than the super alternatives.</p>
<p>In this paper, we extend the knowledge gradient policy to address both batch and nested-batch measurements. While the technique presented is generally applicable to the experiments where a batch or nested batch measurement procedure is utilized, we focus on a single motivating application: that of optimizing the output current of a photoactive device whose surface has been functionalized by gold nanoparticles. This problem is presented in Section 2. We then describe the formalism of the optimization problem known as ranking and selection in Section 3 and briefly review simple KG policy for performing the optimization in Section 4. Section 5 describes the proposed formal model for batch and nested-batch setting, while Sections 6 and 7 outlines the adapted KG policy for these cases. Lastly in Section 8, we apply these new algorithms to solving the example optimization problem of maximizing output current. Here we present simulation results to numerically show that the batch and nested-batch algorithms perform well for the model application presented in this paper, finding alternatives that yield large values of output current on average.
2. Motivating application. As a motivating example and as discussed briefly above, we consider a photoactive device in which anisotropic gold nanoparticles (NPs) are immobilized on the surface of the device. The immobilization is performed using a DNA-mediated approach using thiol-gold chemistry in which both the surface and the NPs are functionalized by complimentary DNA strands that subsequently bind to hold the particles onto surface [28]. The NP's role is to enhance the photocurrent of the device via photonic and plasmonic phenomena, thereby potentially increasing the photoelectric efficiency of such a device. Understanding the particular configuration of the device that yield optimal photoactivity is desirable in applications such as efficient solar cells.</p>
<p>Among the tunable parameters that describe the device's configuration are NP size and the density of NPs functionalized onto the surface of the device. Fabrication of NPs of a particular size is done via thermal or photochemical methods, and requires several hours to days to fabricate [20, 18]. In contrast, once NPs are fabricated, it is straightforward to immobilize them onto the device at some prescribed density [28, 23], and often several such densities can be considered in parallel. Therefore, in selecting which configurations (i.e. a choice of NP size and density) to experimentally test, we are naturally led to a nested, batch decision. Different densities may be run in a batch setting, provided that the size of the NPs is the same within the batch.</p>
<p>While the exact mapping between the tunable parameters of NP size and density and the response output current is not well established, we may make some qualitative</p>
<p>statements using domain expert prior knowledge. Specifically, both NP size and density affect the phenomena of surface plasmon resonance, photon absorption, and scattering, which subsequently influence output current. The full description of the effect of the parameters on these physical phenomena and output current is beyond the scope of this paper (see e.g. $[9,13,10]$ for a general treatment of discussions on optical and electrical properties of nanostructured devices). However, we state that due to competing effects, there exists a critical value of both NP size and density that optimizes output current, and further assume that there exists a single such extrema in the domain of interest. Our task is to find this critical value under uncertainty of the true physics of the system. To this end, we consider a third-order polynomial approximation of the output current $I(d, \rho)$ with respect to size $d$ and the logarithm of NP density $\rho$ :</p>
<p>$$
I(d, \rho)=c_{1}+c_{2} d+c_{3} \rho+c_{4} d^{2}+c_{5} d \rho+c_{6} \rho^{2}+c_{7} d^{3}+c_{8} d^{2} \rho+c_{9} d \rho^{2}+c_{10} \rho^{3}
$$</p>
<p>This polynomial regression model is meant as a third-order local approximation to the true response function. A cubic polynomial was specifically selected to provide a balance between the accuracy of this approximation without containing too many terms, which would expose the model to overfitting noisy measurements. The unknown regression coefficients $c_{i}$ are learned along the way through sequential measurements and are subsequently used to provide an estimation of which configurations $(d, \rho)$ optimize $I$. In what follows, we describe the technique employed to adaptively and iteratively select those configurations to test in order to maximize output current in a nested and batch setting.
3. The Sequential Ranking and Selection Problem. The ranking and selection (R\&amp;S) problem is defined as follows. Suppose we have a finite set of alternatives $\mathcal{X}={1,2, \ldots, M}$, each of which can be measured sequentially to estimate its constant but unknown underlying mean $\mu_{x}$. Each element $x$ represents the choices that have to be made when running an experiment. For example, in our motivating application an alternative $x$ is a two dimensional vector representing a particular choice of NP size and density. We begin with a prior multivariate normal distribution of belief about the performance $\mu_{x}$ for each alternative $x \in \mathcal{X}, \mu \sim \mathcal{N}\left(\theta^{0}, \Sigma^{0}\right)$, where $\mu=\left(\mu_{x}\right)<em x="x">{x \in \mathcal{X}}, \theta^{0}=\left(\theta</em>$ is the covariance in our belief about the alternatives.}^{0}\right)_{x \in \mathcal{X}}$ and $\Sigma^{0</p>
<p>At the $n$th iteration (starting with $n=0$ ), we choose one alternative $x^{n}$ to measure. Let $\epsilon^{n+1}$ be the measurement error which is assumed to be normally distributed with known variance $\lambda^{W}=\sigma_{W}^{2}$ and is independent conditionally on $x^{n}$. We simplify our notation by assuming that our measurement variance is the same across all alternatives, but if this is not the case, we can replace $\lambda^{W}$ with $\lambda_{x}^{W}$ throughout. The resulting observation is $W^{n+1}=\mu_{x^{n}}+\epsilon^{n+1}$, i.e., a noisy perturbation from the truth.</p>
<p>For convenience, we introduce the $\sigma$-algebras $\mathcal{F}^{n}$ for any $n=0,1, \ldots, N-1$ which is formed by the previous $n$ measurement choices and outcomes, $x^{0}, W^{1}, \ldots, x^{n-1}, W^{n}$. We define $\theta^{n}=\mathbb{E}\left[\mu \mid \mathcal{F}^{n}\right]$ and $\Sigma^{n}=\operatorname{Cov}\left[\mu \mid \mathcal{F}^{n}\right]$. Then conditionally on $\mathcal{F}^{n}, \mu \sim$ $\mathcal{N}\left(\theta^{n}, \Sigma^{n}\right)$. By Bayes rule and the Sherman-Morrison formula, taking $x=x^{n}$ to temporarily simplify subscripts, the updating equations can be written as</p>
<p>$$
\begin{aligned}
\theta^{n+1} &amp; =\theta^{n}+\frac{W^{n+1}-\theta_{x}^{n}}{\lambda^{W}+\Sigma_{x x}^{n}} \Sigma^{n} e_{x} \
\Sigma^{n+1} &amp; =\Sigma^{n}-\frac{\Sigma^{n} e_{x}\left(e_{x}\right)^{T} \Sigma^{n}}{\lambda^{W}+\Sigma_{x x}^{n}}
\end{aligned}
$$</p>
<p>where $e_{x}$ is a vector with 1 at index $x$ and zeros everywhere else. Let $S^{n}=\left(\theta^{n}, \Sigma^{n}\right)$ be our state of knowledge. A decision function $X^{\pi}\left(S^{n}\right)$ is defined as a mapping from the knowledge state to $\mathcal{X}$. We refer to the decision function $X^{\pi}$ and the policy $\pi$ interchangeably.</p>
<p>If we are limited to $N$ measurements, the objective is to maximize the expected reward of the final recommended alternative:</p>
<p>$$
\max <em x_N="x^{N">{\pi \in \Pi} \mathbb{E}\left[\mu</em>\right]
$$}</p>
<p>where $x^{N}=\arg \max <em x="x">{x \in \mathcal{X}} \theta</em>\right)$ for $n=0,1, \ldots, N-1$.
Alternatively, we can formulate the problem within a dynamic programming framework [11]. Define the state space $\mathcal{S}$ to be the cross-product of $\mathbb{R}^{M}$ and the space of positive semi-definite matrices. We next define the transition function from the updating equations (3.1) (3.2). Define a vector valued function $\tilde{\sigma}$ as}^{N}$ and $x^{n}=X^{\pi}\left(S^{n</p>
<p>$$
\tilde{\sigma}(\Sigma, x)=\frac{\Sigma e_{x}}{\sqrt{\lambda^{W}+\Sigma_{x x}}}
$$</p>
<p>where $\Sigma$ is any covariance matrix. Next define the random variable</p>
<p>$$
Z^{n+1}=\frac{W_{x^{n}}^{n+1}-\theta_{x^{n}}^{n}}{\sqrt{\operatorname{Var}\left[W_{x^{n}}^{n+1}-\theta_{x^{n}}^{n} \mid \mathcal{F}^{n}\right]}}
$$</p>
<p>which is a one-dimensional standard normal random variable when conditioned on $\mathcal{F}^{n}$.</p>
<p>We can write (3.1) as</p>
<p>$$
\theta^{n+1}=\theta^{n}+\tilde{\sigma}\left(\Sigma^{n}, x^{n}\right) Z^{n+1}
$$</p>
<p>Update (3.2) can also be rewritten as</p>
<p>$$
\Sigma^{n+1}=\Sigma^{n}-\tilde{\sigma}\left(\Sigma^{n}, x^{n}\right)\left(\tilde{\sigma}\left(\Sigma^{n}, x^{n}\right)\right)^{\mathrm{T}}
$$</p>
<p>Now we can define the transition function.
Definition 3.1. The transition function $T: \mathcal{S} \times \mathcal{X} \times \mathbb{R}$ is defined as</p>
<p>$$
T((\theta, \Sigma), x, z):=\left(\theta+\tilde{\sigma}(\Sigma, x) z, \Sigma-\tilde{\sigma}(\Sigma, x)(\tilde{\sigma}(\Sigma, x))^{\mathrm{T}}\right)
$$</p>
<p>so that $S^{n+1}=T\left(S^{n}, x^{n}, Z^{n+1}\right)$. Here $\theta$ is a vector, $\Sigma$ is a covariance matrix, $z \in \mathbb{R}$ and $Z^{n+1}$ is a one-dimensional standard normal random variable.</p>
<p>We then define the value function $V^{n}: \mathcal{S} \mapsto \mathbb{R}$ at times $n=0,1, \ldots, N$ as</p>
<p>$$
V^{n}(s):=\max <em x="x">{x} \mathbb{E}^{\pi}\left[\max </em>
$$} \theta_{x}^{N} \mid S^{n}=s\right], \forall s \in \mathcal{S</p>
<p>By noting that $\max \theta^{N}$ is $\mathcal{F}^{N}$-measurable, the terminal value function $V^{N}$ can be computed directly as:</p>
<p>$$
V^{N}(s)=\max <em x="x">{x \in \mathcal{X}} \theta</em>
$$}, \forall s=(\theta, \Sigma) \in \mathcal{S</p>
<p>The dynamic programming principle tells us that the value function at times $n=$ $0,1, \ldots, N-1, V^{n}$ is given recursively by :</p>
<p>$$
V^{n}(s)=\max _{x \in \mathcal{X}} \mathbb{E}\left[V^{n+1}\left(T\left(s, x, Z^{n+1}\right)\right)\right], s \in \mathcal{S}
$$</p>
<ol>
<li>The Knowledge Gradient Policy for R\&amp;S problems. For R\&amp;S problems, the knowledge gradient policy is a stationary policy that at the $n$th iteration chooses its $(n+1)$ st measurement from $\mathcal{X}$ to maximize the single-period expected increase in value [12]. To be more specific,</li>
</ol>
<p>Definition 4.1. The knowledge gradient of measuring an alternative $x$ at state $s$ is</p>
<p>$$
\nu_{x}^{K G}(s):=\mathbb{E}\left[V^{N}(T(s, x, Z))-V^{N}(s)\right]
$$</p>
<p>where $Z$ is a one-dimensional standard normal random variable. Recall from (3.9) that $V^{N}\left(S^{n}\right)=\max <em x="x">{x \in \mathcal{X}} \theta</em>$ is going to be. The knowledge gradient of measuring $x$ is then}^{n}$. Suppose we are at knowledge state $S^{n}=\left(\theta^{n}, \Sigma^{n}\right)$; if we choose to measure $x^{n}=x$ right now, allowing us to observe $W_{x}^{n+1}$, then we transition to a new state of knowledge $S^{n+1}=\left(\theta^{n+1}, \Sigma^{n+1}\right)$. At iteration $n, \theta_{x}^{n+1}$ is a random variable since we do not yet know what $W^{n+1</p>
<p>$$
\nu_{x}^{\mathrm{KG}}\left(S^{n}\right)=\mathbb{E}\left[\max <em x_prime="x^{\prime">{x^{\prime}} \theta</em>-\max }}^{n+1<em x_prime="x^{\prime">{x^{\prime}} \theta</em>\right]
$$}}^{n} \mid x^{n}=x, S^{n</p>
<p>One property of the knowledge gradient is that $\nu_{x}^{K G}(s) \geq 0$ for any $s \in \mathcal{S}$ [12]. The knowledge gradient policy will never evaluate an alternative that yields zero value of information.</p>
<p>Definition 4.2. The Knowledge Gradient (KG) policy is defined as:</p>
<p>$$
X^{K G}\left(S^{n}\right)=\arg \max <em x="x">{x \in \mathcal{X}} \nu</em>\right)
$$}^{K G}\left(S^{n</p>
<p>The algorithm for calculating the knowledge gradient can be found in [11].
The knowledge gradient policy can handle a variety of belief models such as linear [22] or nonparametric [22, 19, 4].</p>
<h1>5. From Sequential Decision Making to Nested Batch Mode Decision</h1>
<p>Making. In this section, we first give the formal model for batch learning and then we will extend it to nested batch mode decision making.
5.1. Batch Mode Learning Model. In real world applications, it often occurs that information collectors do not simply take one measurement at a time. For example, in a pharmaceutical company, researchers might test the efficiency of a medicine by taking measurements of five different concentrations simultaneously, observing all the outcomes, and then measuring the next five concentrations. Or in the motivating application, if we fix a NP size, then we can test on different densities simultaneously. This leads us to the idea of batch measurements.</p>
<p>Suppose we have a collection $\mathcal{X}={1,2, \ldots, M}$ of $M$ alternatives. Instead of sequentially measuring some alternatives to estimate the constant but unknown underlying mean $\mu_{x}$, we can measure a batch of alternatives simultaneously at each step. We begin with a prior multivariate normal distribution of belief about the performance $\mu_{x}$ for each alternative $x \in \mathcal{X}, \mu \sim \mathcal{N}\left(\theta^{0}, \Sigma^{0}\right)$, where $\mu=\left(\mu_{x}\right)<em x="x">{x \in \mathcal{X}}, \theta^{0}=\left(\theta</em>\right)}^{0<em W="W">{x \in \mathcal{X}}$ and $\Sigma^{0}$ is the covariance in our belief about the alternatives. Denote the batch size by $B$ and the total number of batches by $K$. Then the total number of measurements allowed is $N=B K$. At the $k$ th batch (starting with $n=0$ ), instead of choosing one alternative to measure as in Section 3, we choose to measure $B$ alternatives $x^{k, 0}, x^{k, 1}, \ldots, x^{k, B-1}$. Let $\epsilon^{k+1}$ be the measurement error which is assumed to be normally distributed with known variance $\lambda^{W}=\sigma</em>\right)$.}^{2}$. The resulting observations are $W^{k+1,0} \sim \mathcal{N}\left(\mu_{x^{k, 0}}, \sigma_{W}\right), W^{k+1,1} \sim \mathcal{N}\left(\mu_{x^{k, 1}}, \sigma_{W}\right), \ldots, W^{k+1, B-1} \sim \mathcal{N}\left(\mu_{x^{k, B-1}}, \sigma_{W</p>
<p>We modify our notations to fit batch measurements. The superscript $(k, b)$ for some $k=0,1, \ldots, K-1$ and $b=1,2, \ldots, B-1$ should be understood as meaning that we have done $k$ batches and use $x^{k, 0}, \ldots, x^{k, b-1}, W^{k+1,0}, W^{k+1, \ldots, b-1}$ to update our belief. Thus the prior multivariate normal belief can be rewritten as $\left(\theta^{0,0}, \Sigma^{0,0}\right)$. The new updating equations can be written as</p>
<p>$$
\begin{aligned}
\theta^{k, b+1} &amp; =\theta^{k, 0}+\sum_{j=0}^{b} \frac{W^{k+1, j}-\theta_{x^{k, j}}^{k, j}}{\lambda^{W}+\Sigma_{x^{k, j} x^{k, j}}^{k, j}} \Sigma^{k, j} e_{x^{k, j}} \
\Sigma^{k, b+1} &amp; =\Sigma^{k, b}-\frac{\Sigma^{k, b} e_{x^{k, b}}\left(e_{x^{k, b}}\right)^{T} \Sigma^{k, b}}{\lambda^{W}+\Sigma_{x^{k, b} x^{k, b}}^{k, b}}
\end{aligned}
$$</p>
<p>where $k=0,1, \ldots, K-1, b=0,1, \ldots, B-1, \theta^{k+1,0}=\theta^{k, B}$ and $\Sigma^{k+1,0}=\Sigma^{k, B}$. It is worth emphasizing that in the batch setting the covariance matrix would be updated within a batch since it is determined by the measurement decisions and is independent of the observations, whereas the mean values $\theta^{n}$ are only updated after the observations are collected for the whole batch. Additionally, the updating formula (5.1) is not affected by whether the observations are obtained sequentially or in batch.</p>
<p>A decision function $X^{\pi}\left(S^{n}\right)$ is defined as a mapping from the knowledge states to $\mathcal{X}^{B}$, where $S^{n}$ is short for $S^{n, 0}=\left(\theta^{n, 0}, \Sigma^{n, 0}\right)$.</p>
<p>If we are limited to $N=K B$ measurements, the objective is to maximize the expected reward of the final recommended alternative:</p>
<p>$$
\max <em x_K="x^{K">{x \in \Pi} \mathbb{E}\left[\mu</em>\right]
$$}</p>
<p>where $x^{K}=\arg \max <em x="x">{x \in \mathcal{X}} \theta</em>\right)$ for $k=0,1, \ldots, K-1$.
We can also formulate the problem within a dynamic programming framework. We first define the transition function from the updating equations.}^{K}$ and $\left{x^{k, 0}, \ldots, x^{k, B-1}\right}=X^{\pi}\left(S^{k</p>
<p>For convenience, we introduce the $\sigma$-algebras $\mathcal{F}^{k, b}$ for any $b=0,1, \ldots, B-1$ which is formed by the previous $k$ batch measurement outcomes and the first $b$ observations in the current batch. The idea is that even when performing experiments in batch, we can model the updating as if each outcome is collected sequentially. Suppose we are at the $k+1$ th batch and have made the measurement decisions for the whole batch. For any $b=0,1, \ldots, B-1$, define the random variable $Z^{k+1, b}$ as</p>
<p>$$
Z^{k+1, b}:=\frac{W^{k+1, b}-\theta_{x^{k, b}}^{k, b}}{\sqrt{\operatorname{Var}\left[W^{k+1, b}-\theta_{x^{k, b}}^{k, b} \mid \mathcal{F}^{k, b}\right]}}
$$</p>
<p>Since $\theta_{x}^{k, b} \in \mathcal{F}^{k, b}$,</p>
<p>$$
\operatorname{Var}\left[W^{k+1, b}-\theta_{x^{k, b}}^{k, b} \mid \mathcal{F}^{k, b}\right]=\operatorname{Var}\left[\mu_{x^{k, b}}+\epsilon^{k+1} \mid \mathcal{F}^{k, b}\right]=\Sigma_{x^{k, b} x^{k, b}}^{k, b}+\lambda^{W}
$$</p>
<p>It is important to note that if conditioned on $Z^{k+1,0}, \ldots, Z^{k+1, b-1}$, or in other words, $\mathcal{F}^{k, b}$, the $Z^{k+1, b}$ is a standard normal distribution.</p>
<p>Recalling from (3.4) the definition of $\tilde{\sigma}$, we can rewrite (5.1) and (5.2) as</p>
<p>$$
\begin{aligned}
\theta^{k, b+1} &amp; =\theta^{k, 0}+\sum_{j=0}^{b} \tilde{\sigma}\left(\Sigma^{k, j}, x^{k, j}\right) Z^{k+1, j} \
\Sigma^{k, b+1} &amp; =\Sigma^{k, b}-\tilde{\sigma}\left(\Sigma^{k, b}, x^{k, b}\right)\left(\tilde{\sigma}\left(\Sigma^{k, b}, x^{k, b}\right)\right)^{\mathrm{T}}
\end{aligned}
$$</p>
<p>Now we can define the transition function for batch mode learning recursively by pretending the outcomes are obtained sequentially.</p>
<p>Definition 5.1. The transition function $T^{B}: \mathcal{S} \times \mathcal{X}^{B} \times \mathbb{R}^{B}$ is defined as</p>
<p>$$
T^{B}\left((\theta, \Sigma),\left(x_{1}, \ldots, x_{B}\right),\left(z_{1}, \ldots, z_{B}\right)\right):=T\left(\ldots T\left((\theta, \Sigma), x_{1}, z_{1}\right), . ., x_{B}, z_{B}\right)
$$</p>
<p>so that $S^{k+1,0}=T^{B}\left(S^{k, 0},\left(x^{k, 0}, \ldots, x^{k, B-1}\right),\left(Z^{k+1,0}, \ldots, Z^{k+1, B-1}\right)\right)$. Here $\theta$ is a vector, $\Sigma$ is a covariance matrix, $z^{k+1, j} \in \mathbb{R}, Z^{k+1, j}$ is a one-dimensional standard normal random variable and $T$ is the transition function defined in Definition 3.1.</p>
<p>We then define the value function $V^{B, k}: \mathcal{S} \mapsto \mathbb{R}$ after $k$ batch measurements at times $k=0,1, \ldots, K-1$ as</p>
<p>$$
V^{B, k}(s):=\max <em x="x">{x} \mathbb{E}^{\pi}\left[\max </em>
$$} \theta_{x}^{K} \mid S^{k}=s\right], \forall s \in \mathcal{S</p>
<p>By noting that $\theta^{K}$ is deterministic given $S^{K}$, the terminal value function $V^{B, K}$ can be computed directly as:</p>
<p>$$
V^{B, K}(s)=\max <em x="x">{x \in \mathcal{X}} \theta</em>
$$}, \forall s=(\theta, \Sigma) \in \mathcal{S</p>
<p>The dynamic programming principle tells us that the value function at times $k=$ $0,1, \ldots, K-1, V^{B, k}$ is given recursively by :</p>
<p>$$
V^{B, k}(s)=\max <em i="i">{\left(x</em>\right)<em i="i">{i=1}^{B} \in \mathcal{X}^{B}} \mathbb{E}\left[V^{B, k+1}\left(T^{\mathrm{B}}\left(s,\left(x</em>\right)<em i="i">{i=1}^{B},\left(Z</em>\right.
$$}\right)_{i=1}^{B}\right)\right], s \in \mathcal{S</p>
<p>where $Z_{i}$ is a one dimensional standard normal variable.
A Knowledge-Gradient policy is provided for batch learning model in section 6.
5.2. Nested Batch Mode Learning Model. Motivated by the applications given by the real world applications in Section 2, right now we have a collection $\mathcal{X}<em 2="2">{1} \times \mathcal{X}</em>}$ of $M$ alternatives, where at each decision step, we choose one $x \in \mathcal{X<em 2="2">{1}$ and a set $\mathcal{Y} \in \mathcal{X}</em>$, constructing $B$ alternatives to measure simultaneously (e.g. design 10 nm triangle particles and experiment with densities of $3 \%, 10 \%, 27 \%, 78 \%$ and $92 \%$ with a batch size $B=5$ ).}^{B</p>
<p>As before, we begin with a prior multivariate normal distribution of belief about the performance $\mu_{(x, y)}$ for each alternative $x \in \mathcal{X}<em 2="2">{1}$ and $y \in \mathcal{X}</em>\right)}, \mu \sim \mathcal{N}\left(\theta^{0}, \Sigma^{0}\right)$, where $\mu=\left(\mu_{(x, y)<em 1="1">{(x, y) \in \mathcal{X}</em>} \times \mathcal{X<em _x_="(x," y_="y)">{2}}, \theta^{0}=\left(\theta</em>\right)}^{0<em 1="1">{(x, y) \in \mathcal{X}</em>$ is a $M \times M$ covariance matrix.} \times \mathcal{X}_{2}}$ and $\Sigma^{0</p>
<p>Let $K$ be the total number of batches. At any decision step $k=0,1, \ldots, K-1$ after we make the $B$ measurement decisions $\left(x^{k}, y^{k, 0}\right),\left(x^{k}, y^{k, 1}\right), \ldots\left(x^{k}, y^{k, B-1}\right)$ and get their outcomes, we can also pretend that the information is collected sequentially. So the updating equations are the same as those in the batch mode model when treating $(x, y)$ as the alternative and replacing $x^{k, j}$ with $\left(x^{k}, y^{k, j}\right)$. It is worth noting here, we are not only updating our belief about the alternatives with $x^{k}$, but we are also updating our belief about all $M$ alternatives.</p>
<p>A decision function $X^{\pi}\left(S^{\pi}\right)$ is defined as a mapping from the knowledge state to $\mathcal{X}<em 2="2">{1} \times \mathcal{X}</em>$. The objective is to maximize the expected reward of the final recommended alternative:}^{B</p>
<p>$$
\max <em _left_x_K="\left(x^{K">{x \in \Pi} \mathbb{E}\left[\mu</em>\right]
$$}, y^{K}\right)</p>
<p>where $\left(x^{K}, y^{K}\right)=\arg \max <em 1="1">{(x, y) \in \mathcal{X}</em>} \times \mathcal{X<em _x_="(x," y_="y)">{2}} \theta</em>\right)$ for $k=0,1, \ldots, K-1$.}^{K}$ and $\left{x^{k}, y^{k, 0}, \ldots, y^{k, B-1}\right}=X^{\pi}\left(S^{k</p>
<p>We formulate the problem within a dynamic programming framework. By a similar argument as that in batch mode, we can define the transition function as</p>
<p>Definition 5.2. Define the transition function $T^{N B}: \mathcal{S} \times\left(\mathcal{X}<em 2="2">{1} \times \mathcal{X}</em>$ as $(5.10)$}^{B}\right) \times \mathbb{R}^{B</p>
<p>$$
T^{N B}\left((\theta, \Sigma),\left(x, y_{1}, \ldots, y_{B}\right),\left(z_{1}, \ldots, z_{B}\right)\right):=T\left(\ldots T\left((\theta, \Sigma),\left(x, y_{1}\right), z_{1}\right), \ldots,\left(x, y_{B}\right), z_{B}\right)
$$</p>
<p>so that $S^{k+1}=T^{N B}\left(S^{k},\left(x^{k}, y^{k, 0}, \ldots, y^{k, B-1}\right),\left(Z^{k+1,0}, \ldots, Z^{k+1, B-1}\right)\right)$. Here $\theta$ is a vector, $\Sigma$ is a covariance matrix, $z^{k+1, j} \in \mathbb{R}, Z^{k+1, j}$ is a one-dimensional standard normal random variable and $T$ is the transition function defined in Definition 3.1.</p>
<p>We then define the value function $V^{N B, k}: \mathcal{S} \mapsto \mathbb{R}$ after $k$ nested batch measurements at times $k=0,1, \ldots, K-1$ as</p>
<p>$$
V^{N B, k}(s):=\max <em _x_="(x," y_="y)">{\pi} \mathbb{E}^{\pi}\left[\max </em>
$$} \theta_{(x, y)}^{K} \mid S^{k}=s\right], \forall s \in \mathcal{S</p>
<p>The terminal value function $V^{N B, K}$ can be computed directly as:</p>
<p>$$
V^{N B, K}(s)=\max <em 1="1">{(x, y) \in \mathcal{X}</em>} \times \mathcal{X<em _x_="(x," y_="y)">{2}} \theta</em>
$$}, \forall s=(\theta, \Sigma) \in \mathcal{S</p>
<p>The dynamic programming principle tells us that the value function at times $k=$ $0,1, \ldots, K-1, V^{B, k}$ is given recursively by :</p>
<p>$$
V^{N B, k}(s)=\max <em 1="1">{(x, \mathcal{Y}) \in \mathcal{X}</em>} \times \mathcal{X<em 1="1">{2}^{B}} \mathbb{E}\left[V^{N B, k+1}\left(T^{\mathrm{NB}}\left(s,\left(x, y</em>\right.
$$}, \ldots y_{B}\right),\left(Z_{1}, \ldots Z_{B}\right)\right)\right], s \in \mathcal{S</p>
<p>where $Z_{i}$ is a one dimensional standard normal variable.
A KG-type policy is provided for nested batch learning in the section 7.
6. Batch Knowledge Gradient (BKG) Policy. In this section, we extend the original idea of the KG policy for batch mode learning. We first give the formal definition of the batch knowledge gradient policy and then provide a Monte Carlo algorithm for any given batch size.
6.1. Definition of BKG Policy. Following the basic idea of the knowledge gradient, we would like to design a policy that seeks to measure the $B$ alternatives that provide the single-period expected increment as a batch. We first define the value of information from measuring a batch of alternatives.</p>
<p>Definition 6.1. The knowledge gradient for measuring a batch of $j$ alternatives $\left{x_{1}, \ldots x_{j}\right}$ at state $s$ is defined as</p>
<p>$$
\nu_{x_{1}, \ldots x_{j}}^{B K G}(s):=\mathbb{E}\left[V^{B, K}\left(T^{B}\left(s,\left(x_{1}, \ldots x_{j}\right),\left(Z_{1}, \ldots, Z_{j}\right)\right)\right)-V^{B, K}(s)\right]
$$</p>
<p>where $Z_{i}$ is a one-dimensional standard normal random variable.
Recall from (5.7) that $V^{B, K}\left(S^{k}\right)=\max <em x="x">{x \in \mathcal{X}} \theta</em>\right)$ is then}^{k}$. Thus, suppose we are in knowledge state $S^{k}=\left(\theta^{k}, \Sigma^{k}\right)=\left(\theta^{k, 0}, \Sigma^{k, 0}\right)$. If we choose to measure $\left(x^{k, 0}=x_{1}, \ldots, x^{k, j-1}=\right.$ $\left.x_{j}\right)$ right now, allowing us to observe $\left(W_{x^{k, 0}}^{k+1,0}, \ldots, W_{x^{k, j-1}}^{k+1, j-1}\right)$, then we transition to a new state of knowledge $S^{n+1}=\left(\theta^{n+1}, \Sigma^{n+1}\right)$. At iteration $k, \theta^{k+1}$ is a random vector since we do not yet know what $W^{k+1}$ is going to be. The knowledge gradient of measuring $\left(x_{1}, \ldots, x_{j</p>
<p>$$
\nu_{x_{1}, \ldots x_{j}}^{\mathrm{BKG}}\left(S^{k}\right)=\mathbb{E}\left[\max <em x="x">{x} \theta</em>-\max }^{k+1<em x="x">{x} \theta</em>\right]
$$}^{k} \mid x^{k, 0}=x_{1}, \cdots, x^{k, j-1}=x_{j}, S^{k</p>
<p>One way to design a policy $\pi^{\prime}$ using the knowledge gradient concept is to directly find the $\left{x_{1}, \ldots, x_{j}\right}$ that maximizes $\nu_{x_{1}, \ldots x_{j}}^{\mathrm{BKG}}\left(S^{k}\right)$ subject to $j \leq B$. Since the measurement is noisy, measuring the same $x_{i}$ several times will most likely give different observations and thus it is meaningful if we measure some alternative $x_{i}$ more than once within a batch. For example, in the motivating application, we can choose to test on 5 densities $\left(\rho_{1}, \rho_{1}, \rho_{3}, \rho_{3}, \rho_{7}\right)$ all at once. Thus the batch decision procedure is analogous to multi-set function maximization problems. Let $\mathcal{X}$ be a finite set of $M$ elements. Define the multi-set function $f: \mathbb{N}^{\mathcal{X}} \mapsto \mathbb{R}$. The problem is to find a multi-set $A$ of cardinality less than or equal to some specified number $B$, such that $f(A)$ is the maximum:</p>
<p>$$
\max _{A \subset \mathbb{N}^{\mathcal{X}}}{f(A):|A| \leq B}
$$</p>
<p>The problem with $\pi^{\prime}$ is that it involves testing all $\sum_{b=0}^{B-1}\binom{b+M-1}{M-1}$ which would be computationally costly when $B$ and $M$ are large. Alternatively, as a common technique to deal with set function maximization problems, we can use a greedy heuristic to start from the null set and add elements one at a time. We first claim that the more measurements, the larger the value of information. Thus, if we are limited to $B$ measurements in a batch, we will indeed measure $B$ alternatives in each batch.</p>
<h1>Proposition 6.2. (Benefits of Measurement)</h1>
<p>$\nu_{x_{1}, \ldots x_{j+1}}^{B K G}(s) \geq \nu_{x_{1}, \ldots x_{j}}^{B K G}(s)$ for all $j \geq 0, s \in \mathcal{S}$ and $x_{i} \in \mathcal{X}$.
Proof. In the following proof, we use properties of conditional expectations $\mathbb{E}[\mathbb{E}[U \mid V]]=\mathbb{E}[U]$ for any random variables $U$ and $V$.</p>
<p>$$
\begin{aligned}
&amp; \nu_{x_{1}, \ldots x_{j+1}}^{\mathrm{BKG}}(s)-\nu_{x_{1}, \ldots x_{j}}^{\mathrm{BKG}}(s) \
= &amp; \mathbb{E}\left[V^{B, K}\left(T^{\mathrm{B}}\left(s,\left(x_{i}\right)<em i="i">{i=1}^{j+1},\left(Z</em>\right)<em i="i">{i=1}^{j+1}\right)\right)-V^{B, K}\left(T^{\mathrm{B}}\left(s,\left(x</em>\right)<em i="i">{i=1}^{j},\left(Z</em>\right)<em i="i">{i=1}^{j}\right)\right)\right] \
= &amp; \mathbb{E}\left[\mathbb{E}\left[V^{B, K}\left(T^{\mathrm{B}}\left(s,\left(x</em>\right)<em i="i">{i=1}^{j+1},\left(Z</em>\right)<em i="i">{i=1}^{j+1}\right)\right)-V^{B, K}\left(T^{\mathrm{B}}\left(s,\left(x</em>\right)<em i="i">{i=1}^{j},\left(Z</em>\right)<em i="i">{i=1}^{j}\right)\right) \mid\left(x</em>\right)<em i="i">{i=1}^{j},\left(z</em>\right)<em j_1="j+1">{i=1}^{j}\right]\right] \
= &amp; \mathbb{E}\left[\mathbb{E}\left[V^{B, K}\left(T\left(s^{\prime}, x</em>\right)\right]\right]
\end{aligned}
$$}, Z_{j+1}\right)\right)-V^{B, K}\left(s^{\prime</p>
<p>where $s^{\prime}=T^{\mathrm{B}}\left(s,\left(x_{i}\right)<em i="i">{i=1}^{j},\left(z</em>\right)<em i="i">{i=1}^{j}\right), T(s, x, z)$ is the transition function defined in Definition 3.1 and in the last equation the first expectation is taken over the random choices of $s^{\prime}$ or equivalently the choices of $\left(z</em>\right)<em j_1="j+1">{i=1}^{j}$ and the second expectation is taken over $Z</em>\right)\right)=$ $\max }$. By the definition of $T$ and $V^{B, K}$, we have $V^{B, K}\left(T\left(s^{\prime}, x_{j+1}, Z_{j+1<em x="x">{x \in \mathcal{X}}\left(\theta</em>}^{\prime}+\tilde{\sigma<em j_1="j+1">{x}\left(\Sigma^{\prime}, x</em>\right)=\max }\right) Z_{j+1}\right)$ and $V^{B, K}\left(s^{\prime<em x="x">{x} \theta</em>$. By Jensen's inequality, we have}^{\prime</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[V^{B, K}\left(T\left(s^{\prime}, x_{j+1}, Z_{j+1}\right)\right)\right] &amp; =\mathbb{E}\left[\max <em x="x">{x \in \mathcal{X}}\left(\theta</em>}^{\prime}+\tilde{\sigma<em j_1="j+1">{x}\left(\Sigma^{\prime}, x</em>\right)\right] \
&amp; \geq \max }\right) Z_{j+1<em x="x">{x \in \mathcal{X}} \mathbb{E}\left[\left(\theta</em>}^{\prime}+\tilde{\sigma<em j_1="j+1">{x}\left(\Sigma^{\prime}, x</em>\right)\right] \
&amp; =\max }\right) Z_{j+1<em x="x">{x \in \mathcal{X}} \theta</em> \
&amp; =V^{B, K}\left(s^{\prime}\right)
\end{aligned}
$$}^{\prime</p>
<p>Since this inequality holds for any realization of $s^{\prime}$, the proposition follows.
Corollary 6.3. The knowledge gradient of measuring a batch of $j$ alternatives at any state $s$ is always non-negative, $\nu_{x_{1}, \ldots x_{j}}^{B K G}(s)$ for all $j \geq 0, s \in \mathcal{S}$ and $x_{i} \in \mathcal{X}$.</p>
<p>Proof. It follows from Proposition 6.2 by noting that $\nu_{\emptyset}^{\mathrm{BKG}}(s)=0$ for any $s \in \mathcal{S}$.</p>
<p>Since the more measurements the better, if we are limited to at most $B$ measurements at each time step, we will exactly choose to make $B$ measurements. We thus can define the batch knowledge gradient (BKG) policy that greedily adds in each alternative that maximizes the expected increment of value one at a time until $B$ alternatives are chosen.</p>
<p>Definition 6.4. The Batch Knowledge Gradient (BKG) policy has the decision function</p>
<p>$$
x^{k, b}:=X_{b}^{B K G}\left(S^{k}\right)=\arg \max <em 0="0" x_k_="x^{k,">{x \in \mathcal{X}} \nu</em>\right)
$$}, \ldots, x^{k, b-1}, x^{k, b}=x}^{B K G}\left(S^{k</p>
<p>for any $b=0, \ldots, B-1$ and decision points $k=0,1, \ldots, K-1$.
The above formulation tells us that we make each measurement decision in the batch by conditioning on the earlier decisions made in the same batch and the state. With (5.4) and (6.2), we can rewrite (6.4) as</p>
<p>$$
X_{b}^{\mathrm{BKG}}\left(S^{k}\right)=\arg \max <em x_prime="x^{\prime">{x \in \mathcal{X}} \mathbb{E}\left[\max </em>\right)\right]
$$}}\left(\theta^{k, 0}+\sum_{j=0}^{b-1} \tilde{\sigma}\left(\Sigma^{k, j}, x^{k, j}\right) Z^{k+1, j}+\tilde{\sigma}\left(\Sigma^{k, b}, x\right) Z^{k+1, b</p>
<p>where $x^{k, j}, j \leq b$ are fixed when choosing $x^{k, b}$ and $\Sigma^{k, j}$ can be updated within a batch according to (5.2). This formula will be of use in the following computations.
6.2. Computation. We notice from (6.4) that at each batch decision point $k$, we can find the first measurement decision explicitly by carrying out the original KG calculation described since the objective function (6.5) to be maximized for the first decision in the batch is exactly the same as that described in Section 4.</p>
<p>Since an analytic expression for the expected maximization as in (6.5) is unknown, we utilize Monte Carlo sampling to approximate the expectation. After the first measurement decision $x^{k, 0}$ is made, the following decisions are made one at a time to find $x^{k, b}$ according to (6.5) using Monte Carlo Simulation. To be more specific, the second decision is made by randomly generating both $Z^{k+1,0}$ and $Z^{k+1,1}$ for $Q$ times, where $Z^{k+1, i}$ are independent standard normal variables. We then define the second decision $x^{k, 1}$ as:</p>
<p>$$
\arg \max <em q="1">{x \in \mathcal{X}} \frac{1}{Q} \sum</em>\left[\max }^{Q<em q="q">{x^{\prime}}\left(\theta^{k, 0}+\tilde{\sigma}\left(\Sigma^{k, 0}, x^{k, 0}\right) z</em>\right)\right]
$$}^{0}+\tilde{\sigma}\left(\Sigma^{k, 1}, x\right) z_{q}^{1</p>
<p>where $z_{q}^{0}$ and $z_{q}^{1}$ are realizations of $Z^{k+1,0}$ and $Z^{k+1,1}$ respectively and $\Sigma^{k, 1}$ is updated according to (5.2). We then have $x^{k, 0}$ and $x^{k, 1}$ fixed, and proceed to find $x^{k, 2}$ similarly by sampling $Z^{k+1,0}, Z^{k+1,1}$ and $Z^{k+1,2}$ for $Q$ times and finding the alternative that maximizes the analogous expression coming from (6.5) that contains these three random variables. It is worth re-emphasizing here that all three variables are standard normal when we generate their realizations after fixing the previous decisions.</p>
<p>In general, after we get the first $b$ decisions within a batch, we are looking to find the solution to</p>
<p>$$
x^{k, b}=\arg \max <em q="1">{x \in \mathcal{X}} \frac{1}{Q} \sum</em>\left[\max }^{Q<em j="0">{x^{\prime}}\left(\theta^{k, 0}+\sum</em>\right)\right]
$$}^{b-1} \tilde{\sigma}\left(\Sigma^{k, j}, x^{k, j}\right) z_{q}^{j}+\tilde{\sigma}\left(\Sigma^{k, b}, x\right) z_{q}^{b</p>
<p>where $\Sigma^{k, j}$ are updated within this batch according to (5.2).</p>
<p>The pseudo-code of the algorithms are presented below. Algorithm 1 is the BKG policy for the $k$ th batch decision, which calls Algorithm 2 to find the next measurement decision for $B$ times.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Batch</span><span class="w"> </span><span class="nt">Knowledge</span><span class="w"> </span><span class="nt">Gradient</span><span class="w"> </span><span class="nt">Policy</span>
<span class="w">    </span><span class="nt">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">Q</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">Monte</span><span class="w"> </span><span class="nt">Carlo</span><span class="w"> </span><span class="nt">simulation</span>
<span class="w">    </span><span class="nt">Use</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">sequential</span><span class="w"> </span><span class="nt">KG</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">presented</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">Section</span><span class="w"> </span><span class="nt">4</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">find</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">according</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="o">(</span><span class="nt">5</span><span class="p">.</span><span class="nc">2</span><span class="o">);</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">B-1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Use</span><span class="w"> </span><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="w"> </span><span class="nt">below</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">find</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">b</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b+1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">according</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="o">(</span><span class="nt">5</span><span class="p">.</span><span class="nc">2</span><span class="o">);</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">output</span><span class="o">:</span><span class="w"> </span><span class="nt">batch</span><span class="w"> </span><span class="nt">decisions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">B-1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Monte</span><span class="w"> </span><span class="nt">Carlo</span><span class="w"> </span><span class="nt">Simulation</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">b</span><span class="o">+</span><span class="nt">1</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">th</span><span class="w"> </span><span class="nt">decision</span><span class="w"> </span><span class="nt">within</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">batch</span>
<span class="w">    </span><span class="nt">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">b-1</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">Q</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">X</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">sum</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">q</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">Q</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">j</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">                </span><span class="nt">Generate</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">realization</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">z_</span><span class="p">{</span><span class="err">q</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">Z</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">            </span><span class="nt">temp</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">x^{\prime</span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">x^{\prime</span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">0</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">sum_</span><span class="p">{</span><span class="err">j=0</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">b-1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">x^{\prime</span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="nt">z_</span><span class="p">{</span><span class="err">q</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">Sigma</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">x</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="nt">z_</span><span class="p">{</span><span class="err">q</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">b</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">sum</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">sum</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">temp</span><span class="o">;</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">arg</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">x</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="err">\mathcal{X</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">sum</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">output</span><span class="o">:</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="o">+</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">th</span><span class="w"> </span><span class="nt">decision</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">x</span><span class="o">^</span><span class="p">{</span><span class="err">k,</span><span class="w"> </span><span class="err">b</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">within</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">batch</span>
</code></pre></div>

<ol>
<li>Nested Batch Knowledge Gradient (NBKG) Policy. A nested batch decision may involve the selection of a particular NP size and subsequent selection of several NP densities (given the NP size fixed in the first stage of the decision). In general, we would like to design a policy that seeks to measure the $B$ alternatives $\left(x, y_{1}\right), \ldots,\left(x, y_{B}\right)$ that provide the largest single period value of information. We first define the knowledge gradient of measuring a nested batch of alternatives.</li>
</ol>
<p>Definition 7.1. The knowledge gradient of measuring a nested batch of $j$ alternatives $\left{\left(x, y_{1}\right), \ldots,\left(x, y_{j}\right)\right}$ for any $x \in \mathcal{X}<em i="i">{1}$ and $y</em>$ at state $s$ is defined as} \in \mathcal{X}_{2</p>
<p>$$
\nu_{x ; y_{1}, \ldots y_{j}}^{N B K G}(s):=\mathbb{E}\left[V^{N B, K}\left(T^{N B}\left(s,\left(x, y_{1}, \ldots y_{j}\right),\left(Z_{1}, \ldots, Z_{j}\right)\right)\right)-V^{N B, K}(s)\right]
$$</p>
<p>where $Z_{i}$ is a one-dimensional standard normal random variable.
By a similar argument as Proposition 6.2, we can show that if we are limited to $B$ measurements in a batch we will indeed evaluate $B$ alternatives.</p>
<p>We define the Nested Batch Knowledge Gradient policy as directly finding out $\left{x, y_{1}, \ldots, y_{B}\right}$ that maximizes $\nu_{x ; y_{1}, \ldots y_{j}}^{\mathrm{NBKG}}(s)$ at any decision point $k=0,1, \ldots, K$. For clarity, we use $\mathcal{Y}$ to denote the multi-set $\left{y_{1}, \ldots, y_{B}\right}$ since the alternatives being measured in each batch are not necessarily distinct.</p>
<p>Definition 7.2. The Nested Batch Knowledge Gradient (NBKG) policy has the</p>
<p>decision function</p>
<p>$$
X^{N B K G}\left(S^{k}\right)=\arg \max <em _=";" _mathcal_Y="\mathcal{Y" x="x">{(x, \mathcal{Y})} \nu</em>\right)
$$}}^{N B K G}\left(S^{k</p>
<p>for any decision points $k=0,1, \ldots, K-1$.
We can show analytically that</p>
<p>$$
\left{\begin{array}{l}
x^{<em>} \quad=\arg \max <em _mathcal_Y="\mathcal{Y">{x}\left(\max </em>\right) \
\mathcal{Y}^{}} \nu_{x ; \mathcal{Y}}^{\mathrm{NBKG}</em>}=\arg \max <em x_="x^{*">{\mathcal{Y}} \nu</em>
\end{array}\right.
$$} ; \mathcal{Y}}^{\mathrm{NBKG}</p>
<p>is a solution to the optimization problem (7.2). This gives us a two-stage decision process. At the first step, for each $x \in \mathcal{X}<em x="x">{1}$, find the multi-set (a batch) $\mathcal{Y}</em>$ that gives the most value of information; i.e. $\max <em _=";" _mathcal_Y="\mathcal{Y" x="x">{\mathcal{Y}} \nu</em>$. Namely, for example, when calculating a similar expression as (6.2):
(7.3)}}^{\mathrm{NBKG}}$. This can be done by using the Batch Knowledge Gradient policy for each fixed $x$ with the value function $\nu_{x ; y_{1}, \ldots, y_{B}}^{\mathrm{NBKG}}$ instead of $\nu^{\mathrm{BKG}</p>
<p>$$
\nu_{x ; y_{1}, \ldots y_{j}}^{\mathrm{NBKG}}\left(S^{k}\right)=\mathbb{E}\left[\max <em _left_x_prime="\left(x^{\prime">{\left(x^{\prime}, y^{\prime}\right)} \theta</em>-\max }, y^{\prime}\right)}^{n+1<em _left_x_prime="\left(x^{\prime">{\left(x^{\prime}, y^{\prime}\right)} \theta</em>\right]\right.
$$}, y^{\prime}\right)}^{n}\left[x^{k}=x, y^{k, 0}=y_{1}, \cdots, y^{k, j-1}=y_{j}, S^{k</p>
<p>it should be noted that even though the BKG is constructed for each $x \in \mathcal{X}<em 1="1">{1}$, when taking the maximization inside the expectation, $x^{\prime}, y^{\prime}$ should include all the choices in the domain $\mathcal{X}</em>$. Since calculating the expected maximum is needed to make the decision, Monte Carlo sampling is used as in Algorithm 1 to approximate the expectation.} \times \mathcal{X}_{2</p>
<p>We next define the nested knowledge gradient $\nu_{x}^{\mathrm{NKG}}$ for each $x \in \mathcal{X}_{1}$ at state $s$ in the nested dimensions as</p>
<p>$$
\nu_{x}^{\mathrm{NKG}}(s)=\max <em _=";" _mathcal_Y="\mathcal{Y" x="x">{\mathcal{Y}} \nu</em>(s)
$$}}^{\mathrm{NBKG}</p>
<h1>8. Numerical Experiments on NBKG and Optimizing Photocurrent.</h1>
<p>In this section, we present simulation results for the material science application described in Section 2: optimizing the photocurrent of a photoactive device that has anisotropic nanoparticles immobilized onto its surface. Recall in Equation (2.1) we had approximated the output current by a third-order polynomial expansion in the variables $d$ and $\rho$, respectively representing NP size (units nm) and log-density. We wish to optimize the output current with respect to these two variables under the uncertainty of the regression coefficients $c_{i}$ of this expansion. In the physical setting, preparing NPs of a particular size is expensive, while varying the density of a NP can be done easily and in parallel experiments. Therefore, we model the choice of experiment to perform as a nested-batch decision, and apply the NBKG policy toward finding the optimal choice of size and density.
8.1. Prior Generation. To generate a prior distribution on the values of the regression coefficients, we incorporate the following observations, which reflect a domain expert's prior knowledge about the role of NP size and density on output current. First, the experimental range of size was assumed to be between 550 nm to 1300 nm , while the range for NP density was assumed to be between $1 \mathrm{NP} / \mathrm{cm}^{2}$ to $10^{15} \mathrm{NP} / \mathrm{cm}^{2}$. When $d=550 \mathrm{~nm}$ and $\rho=0$, the output current is simply the output current of the photoactive device non-functionalized with NPs. We presume that this current is scaled to 1 nanoamp ( nA ). For extreme values where $d=1300 \mathrm{~nm}$ and $\rho=15$, we assume the current is a nominally small value 0.001 nA . Lastly, we presume that for</p>
<p>points away from the extremes, the current has moderate values between 1 and 20 nA .</p>
<p>Prior generation was performed by uniformly sampling values</p>
<p>$$
d_{1}=550 \leq d_{2} \leq d_{3} \leq d_{4}=1300 \mathrm{~nm}
$$</p>
<p>and</p>
<p>$$
\rho_{1}=0 \leq \rho_{2} \leq \rho_{3} \leq \rho_{4}=15
$$</p>
<p>Sixteen points of the form $\left(d_{i}, \rho_{j}, I(i)+I(j)\right)$ were calculated, where</p>
<p>$$
I(i)= \begin{cases}0.5 &amp; i=1 \ 1 &amp; i=2,3 \ 0.0005 &amp; i=4\end{cases}
$$</p>
<p>We then computed the least-squares fit of the polynomial model in Equation (2.1) to these points, and obtained an instance of the regression parameters $c_{i}$. This procedure was repeated several times, resulting in an empirical distribution on $c$, which we use as the regression parameters' prior distribution. From this, we obtain the induced prior distribution on function values that incorporate the domain expert's prior (albeit limited) knowledge about the behavior of the photocurrent with respect to NP size and density. Figure 1 plots several instances of $I(d, \rho)$ obtained in the above manner.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Example plots of photocurrent $I(d, \rho)$ obtained from the procedure outlined above.
8.2. Performance of NBKG. In order to assess the performance of the NBKG policy, we performed several numerical experiments in which the decision-measurement-update loop was simulated over several batch measurements and over several trials. For each simulation trial, a true value of the regression parameters (and hence a true response surface) was fixed, but unknown to the simulation.
8.2.1. Illustration on NBKG Policy. We first illustrate how NBKG works under a measurement noise of $30 \%$ of the function range. At each iteration, a NBKG value was calculated for each choice of NP size. Example NBKG values are depicted in Figure 2, which is an example of NBKG values after zero, one and two measurements,</p>
<p>respectively. The optimal NP size and corresponding batch of log density values are given at each step. The figure also illustrates a key feature of the KG policy, as shown by a marked decrease in the relative KG value of a NP size after it has been measured. Due to correlation, the values of measuring adjacent alternatives also drops since they roughly provide similar information. As shown in Figure 2, the KG value for NP size $=800 \mathrm{~nm}$ drops after measuring NP size $=883 \mathrm{~nm}$. This gives the KG policy the ability to explore parameter space during the initial set of measurements. From the KG values, the optimal NP size and five corresponding NP densities were selected in the nested-batch method outlined above. After a noisy measurement is made from the true surface, the posterior distribution on $\mu$ is calculated according to Equations (3.1). This process is repeated until 15 batch measurements are made.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: NBKG values before and after 3 batch measurements. The optimal NP size at each step is indicated by the dashed line, and the corresponding optimal batch of densities are also shown. The arrows indicate the decrease in KG value for the NP size that was previously measured.</p>
<p>Figure 3 together with Figure 4 shows an example of the prior and posterior estimates of the true photocurrent function for a particular simulation. In Figure 3, the leftmost figure depicts the true photocurrent function values. The middle and rightmost figures demonstrate the prior and posterior estimates of the true function surface after 0 and 15 batch measurements, respectively, using the NBKG policy. Also depicted in Figure 4 is the residual error, which is the difference between the estimate and true function values. The residual errors are calculated after $0,5,10$ or 15 batch measurements. By examining the residual error plot after 15 measurements, we see that the function value at the true maximum alternative is well approximated, while moderate error in the estimate is located away from this region of interest.
8.2.2. Computational Analysis. In this section, we analyze the performance of NBKG as parameters vary. As a more quantitative measure of the performance of NBKG, we consider the opportunity cost (OC) as a function of the number of batch measurements $K$ :</p>
<p>$$
\mathrm{OC}^{K}=\max <em _x_="(x," y_="y)">{(x, y)} \mu</em>
$$}-\mu_{\left(x^{K}, y^{K}\right)</p>
<p>where $\left(x^{K}, y^{K}\right)=\arg \max <em _x_="(x," y_="y)">{(x, y)} \theta</em>$.
Figure 5 shows the mean OC versus number of batch measurements, averaged over 500 simulation trials. In Figure 5a, we see this plot for the case when the measurement error is $30 \%$ of the true function's range (as before). We observe that the OC quickly decays as the number of measurements increases, showing that the NBKG value}^{K</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Prior and posterior estimates of the true function surface after 0 and 15 batch measurements, using the NBKG policy.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Prior and posterior estimates of the true function surface after $0,5,10$ and 15 batch measurements, using the NBKG policy. For each choice of number of measurements, the plot shows the residual error between this estimate and the true function.
rapidly finds the location of the maximal photocurrent. Figure 5b shows the mean OC versus the number of batch measurements and measurement error. We observe that the OC increases with increasing error, as expected. Such a plot is meaningful in experimental budgeting, and shows the requisite number of measurements needed to obtain a certain level of optimality for a particular level of noise. This plot can suggest to the experimenter the amount of measurement precision needed in order to achieve a desired level of optimality as measured by opportunity cost.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Opportunity cost</p>
<p>We then experiment with different Monte Carlo sampling sizes. At each time step $(k, b)$, for different sampling sizes $Q$, we calculate the standard error $\frac{s_{m c}}{\sqrt{Q}}$ of all the densities, with $s_{m c}$ denoting the sample standard deviation, and verify that the maximum of such values is below an acceptable tolerance. For example, the left two figures in Figure 6 depict the empirical means of NBKG values of a fixed NP size $=800 \mathrm{~nm}$ and all densities under different sampling sizes. In the leftmost figure, the time step $(\mathrm{k}, \mathrm{b})=(0,2)$ is considered. When $Q=1000$, the maximum standard error of all the densities is 0.0476 and the maximum difference between this case and $Q=50000$ over all densities is 0.0158 . When $Q=10000$, the maximum standard error is 0.0129 and the maximum difference with $Q=50000$ is 0.0051 . $Q=10000$ achieves the maximum NBKG value at the same point as $Q=50000$ in this case. Similar performance is observed for $(k, b)=(0,5)$, as plotted in the middle figure, and later time steps. We also plot the opportunity cost curves in log-log scale for different sampling sizes. We observe that the opportunity cost curve of $Q=10000$ is similar to that of $Q=50000$ with a mean difference of 0.0073 , showing that $Q=10000$ is sufficiently large to ensure accuracy as measured by opportunity cost.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Left two figures plot the empirical means of NBKG values of a fixed NP size and all densities under different Monte Carlo sampling sizes. Horizontal axis denotes 10 densities and vertical axis is the empirical mean. The left figure is calculated at time step $(k, b)=(0,1)$ and the middle figure is calculated at time step $(k, b)=(0,4)$. The right figure depicts the opportunity cost curves for different sampling sizes under different number of batch measurements.</p>
<p>We may also assess the performance of NGKB as the problem size increases. We experiment with different batch sizes $B=1,2,3,4,5$ and report in Figure 7 the mean opportunity cost after each batch measurement ranging from 0 to 15 , averaged over 500 runs. In order to make a fair comparison, all the observations are pre-generated and shared for simulations with different batch sizes. We observe that no matter which batch size it uses, the OC quickly decays as the number of batch measurements increases. Since a larger batch size means more measurements at each iteration, thus providing more information and yielding more precise estimation. This intuition is also verified in Proposition 6.2 (Benefits of measurement). We see from the figure that for any measurement budget $K$, larger batch sizes yield lower OC, as expected.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: Performance of NGKB as $K, B$ changes. Horizontal axis denotes the logarithm of the number of batch measurement $K=0,1, \ldots, 15$. Vertical axis is the logarithm of mean opportunity cost. Lines with different colors correspond to different simulations with different batch sizes $B=1,2, \ldots, 5$.
8.3. Comparison with Other Policies. In this section, we consider the performance of NBKG in comparison to other policies. We consider the following policies:</p>
<ol>
<li>Nested-Batch KG: The policy described in the paper.</li>
<li>Sequential KG: The basic, sequential KG policy as described in 11.</li>
<li>Sequential Exploration: The pure exploration policy, which chooses an alternative uniformly at random.</li>
<li>Nested-Batch Exploration: A random NP size is selected, and then $B$ NP densities are selected in batch.</li>
<li>Sequential Exploitation: The pure exploitation policy, which chooses the alternative $x^{n}$ corresponding to the maximum value, $\max <em x="x">{x} \theta</em>$.}^{n</li>
<li>Nested-Batch Exploitation: Select the batch of experiments</li>
</ol>
<p>$$
\left{\left(d, \rho_{1}\right), \ldots,\left(d, \rho_{B}\right)\right}
$$</p>
<p>that maximizes</p>
<p>$$
\mathcal{I}\left(d, \rho_{1}, \ldots, \rho_{B}\right)=\sum_{i=1}^{B} \theta_{\left(d, \rho_{i}\right)}^{n}
$$</p>
<ol>
<li>
<p>Sequential $\epsilon$-Greedy: A sequential policy that provides a mixture between the pure exploration and exploitation policy. The alternative $x^{n}$ selected at time $n$ is obtain by choosing between pure exploration with probability $\epsilon^{n}$ and pure exploitation with probability $\left(1-\epsilon^{n}\right)$, where $\epsilon^{n}=0.9 / n$.</p>
</li>
<li>
<p>Nested-Batch $\epsilon$-Greedy: Similar to the sequential $\epsilon$-greedy policy, but chooses between the nested-batch versions of exploration and exploitation with probability $\epsilon^{n}$ and $\left(1-\epsilon^{n}\right)$, respectively.
Figure 8a plots the mean opportunity cost for the nested-batch policies as a function of the number of batch measurements, averaged over 200 independent simulations and plotted in log scale for clarity. We observe that NBKG outperforms all the nestedbatch policies. Also included in the figure is the opportunity cost for the sequential KG policy. In the nested-batch setting, the sequential KG does not take advantage of batch experiments, opting instead of performing the single experiment with largest KG value, effectively using a batch size of $B=1$. We note that NBKG outperforms the sequential KG policy, as illustrated in Figure 7. The comparison between NBKG and sequential KG exhibits the experimental savings to be gained in performing experiments in batch mode. Figure 8b compares NBKG versus the sequential policies in a sequential experiment setting. In this context, we equate one batch measurement performed using the NBKG policy with $B$ sequentially measurements for comparison. The sequential policies are more adaptive than NBKG in this manner, as they can incorporate information obtained from experiments one at a time, while NBKG only updates the state of knowledge after $B$ measurements. Nevertheless, we observe that for a large number of measurements, NBKG outperforms all sequential policies except for sequential KG. Between sequential and NBKG, we observe similar performance, hinting that while NBKG has a delay in updating information, the effect of this delay is minimal.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
</li>
</ol>
<p>Fig. 8: A comparison of policy performance. The graphs show mean opportunity cost versus the number of measurement for the policies outlined above. (a) Nested-batch experiments, in which a policy may perform several experiments in parallel, varying NP density, provided that the NP size is the same between the parallel experiments. Sequential policies use a batch size of $B=1$. (b) Sequential experiments, in which experiments must be performed one at a time. Here we equate 1 batch measurement with $B$ sequential measurements.
9. Conclusion. In this paper, motivated by several applications, we extended the sequential ranking and selection problem into a general framework for batchmode learning and nested-batch-mode learning. By formulating the problem within a dynamic programming framework, we derived the Knowledge-Gradient variants to tackle both batch and nested-batch measurements. Since the Knowledge-Gradient</p>
<p>variants require computing expectations which may be intractable, a Monte Carlo sampling procedure was applied. We empirically demonstrate the effectiveness of the NBKG policy on the immobilized nanoparticles design problem. We see that NGKB is competitive with a fully sequential strategy and significantly outperforming pure exploration, pure exploitation and $\epsilon$-greedy strategies for the model application presented in this paper.</p>
<h1>REFERENCES</h1>
<p>[1] R. Agrawal, M. Hegde, and D. Teneketzis, Multi-armed bandit problems with multiple plays and switching cost, Stochastics and Stochastic Reports, 29 (1990), pp. 437-459.
[2] J. Audibert, S. Bubeck, and G. Lugosi, Regret in online combinatorial optimization, Math. Oper. Res., (2013).
[3] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer, Finite-time analysis of the multiarmed bandit problem, Mach. Learn., 47 (2002), pp. 235-256.
[4] E. Barut and W. B. Powell, Optimal learning for sequential sampling with non-parametric beliefs, J. Global Optim., (2013), pp. 1-27.
[5] Sébastien Bubeck and Nicolò Cesa-Bianchi, Regret analysis of stochastic and nonstochastic multi-armed bandit problems, arXiv preprint arXiv:1204.5721, (2012).
[6] N. Cesa-Bianchi and G. Lugosi, Combinatorial bandits, J. Comput. System Sci., 78 (2012), pp. $1404-1422$.
[7] Y. Chen and A. Krause, Near-optimal batch mode active learning and adaptive submodular optimization, in Proceedings of The 30th International Conference on Machine Learning, 2013, pp. 160-168.
[8] M. H. DeGroot, Optimal Statistical Decisions, McGraw-Hill, 1970.
[9] Aleksandrab. Djurii and Yu Hang Leung, Optical properties of zno nanostructures, Small, 2 (2006), pp. 944-961.
[10] Franois Flory, Ludovic Escoubas, and Grand Berginc, Optical properties of nanostructured materials: areview, Journal of Nanophotonics, 5 (2011), pp. 052502-052502-20.
[11] P. I. Frazier, W. Powell, and S. Dayanik, The knowledge-gradient policy for correlated normal beliefs, INFORMS J. Comput., 21 (2009), pp. 599-613.
[12] P. I. Frazier, W. B. Powell, and S. Dayanik, A knowledge-gradient policy for sequential information collection, SIAM J. Control Optim., 47 (2008), pp. 2410-2439.
[13] Russell J. Gehr and Robert W. Boyd, Optical properties of nanostructured optical materials, Chemistry of Materials, 8 (1996), pp. 1807-1819.
[14] L. R. Giam, S. He, N. E. Horwitz, D. J. Eichelsdoerfer, J. Chai, Z. Zheng, D. Kim, W. Shim, and C. A. Mirkin, Positionally defined, binary semiconductor nanoparticles synthesized by scanning probe block copolymer lithography, Nano Lett., 12 (2012), pp. 10221025 .
[15] Aditya Gopalan, Shie Mannor, and Yishay Mansour, Thompson sampling for complex online problems, in Proceedings of The 31st International Conference on Machine Learning, 2014, pp. 100-108.
[16] Donghai He, Stephen E Chick, and Chun-Hung Chen, Opportunity cost and OCBA selection procedures in ordinal optimization for a fixed number of alternative systems, Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 37 (2007), pp. $951-961$.
[17] S. Kale, L. Reyzin, and R. E. Schapire, Non-stochastic bandit slate problems., in NIPS, 2010, pp. $1054-1062$.
[18] M. R. Langille, M. L. Personick, J. Zhang, and C. A. Mirkin, Defining rules for the shape evolution of gold nanoparticles, J. Am. Chem. Soc., 134 (2012), pp. 14542-14554.
[19] M. R. Mes, W. B. Powell, and P. I. Frazier, Hierarchical knowledge gradient for sequential sampling, J. Mach. Learn. Res., 12 (2011), pp. 2931-2974.
[20] J. E. Millstone, S. J. Hurst, G. S. Mtraux, J. I. Cutler, and C. A. Mirkin, Colloidal gold and silver triangular nanoprisms, Small, 5 (2009), pp. 646-664.
[21] D. C. Montgomery, Design and Analysis of Experiments, John Wiley and Sons, 2008.
[22] D. M. Negoescu, P. I. Frazier, and W. B. Powell, The knowledge-gradient algorithm for sequencing experiments in drug discovery, INFORMS J. Comput., 23 (2011), pp. 346-363.
[23] S. Y. Park, A. K. Lytton-Jean, B. Lee, S. Weigand, G. C. Schatz, and C. A. Mirkin, DNA-programmable nanoparticle crystallization, Nat., 451 (2008), pp. 553-556.
[24] W. B. Powell and I. O. Ryzhov, Optimal Learning, John Wiley and Sons, 2012.</p>
<p>[25] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims, Learning diverse rankings with multi-armed bandits, in Proceedings of the 25th international conference on Machine learning, ACM, 2008, pp. 784-791.
[26] I. O. Ryzhov and W. Powell, A Monte Carlo knowledge gradient method for learning abatement potential of emissions reduction technologies, in Proceedings of the Winter Simulation Conference, IEEE, 2009, pp. 1492-1502.
[27] I. O. Ryzhov and W. B. Powell, The knowledge gradient algorithm for online subset selection, in IEEE SSCI ADPRL, Nashville, TN, 2009, pp. 137-144.
[28] A. J. Senesi, D. J. Eichelsdoerfer, R. J. Macfarlane, M. R. Jones, E. Auyeung, B. Lee, and C. A. Mirkin, Stepwise evolution of DNA-programmable nanoparticle superlattices, Angew. Chem. Int. Ed., 52 (2013), pp. 6624-6628.
[29] T. Uchiya, A. Nakamura, and M. Kudo, Algorithms for adversarial bandit problems with multiple plays, in Algorithmic Learning Theory, Springer, 2010, pp. 375-389.
[30] G. B. Wetherill and K. D. Glazebrook, Sequential Methods in Statistics, Chapman and Hall, 1986.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*This research was supported in part by AFOSR grant contract FA9550-12-1-0200 for Natural Materials, Systems and Extremophiles.
${ }^{\dagger}$ Department of Computer Science, Princeton University, Princeton, NJ 08540 (yingfei@cs.princeton.edu)
${ }^{\ddagger}$ Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08540 (kreyes@gmail.com, powell@princeton.edu)
§International Institute for Nanotechnology, Northwestern University, Evanston, IL 60208 (brownka@gmail.com, chadnano@northwestern.edu)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
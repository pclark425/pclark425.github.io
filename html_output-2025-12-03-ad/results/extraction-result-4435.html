<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4435 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4435</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4435</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-276903104</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05712v1.pdf" target="_blank">Automatic Evaluation Metrics for Artificially Generated Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models are increasingly used in scientific research, but evaluating AI-generated scientific work remains challenging. While expert reviews are costly, large language models (LLMs) as proxy reviewers have proven to be unreliable. To address this, we investigate two automatic evaluation metrics, specifically citation count prediction and review score prediction. We parse all papers of OpenReview and augment each submission with its citation count, reference, and research hypothesis. Our findings reveal that citation count prediction is more viable than review score prediction, and predicting scores is more difficult purely from the research hypothesis than from the full paper. Furthermore, we show that a simple prediction model based solely on title and abstract outperforms LLM-based reviewers, though it still falls short of human-level consistency.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4435.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4435.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-count prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation count prediction as an automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use content-based models to predict a paper's future citation rate (log average citations per month) as a proxy metric for the scientific influence/quality of AI-generated hypotheses or papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Citation count prediction (log average citations per month)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute the logarithm of average citations per month for each paper (total citations divided by months since publication), and train models (embeddings from SPECTER2 with MLP or transformer context encoder) to predict this target. Can be trained either as pairwise comparison (binary cross-entropy on which of two papers has higher score) or direct regression (L1 loss on log-citation score). Inputs tested include Title+Abstract, individual sections (Introduction, Methods, Experiments & Results, Related Work, Conclusion) and an extracted research hypothesis. Context can include titles/abstracts of references or other paper sections; long inputs are chunked into sentence embeddings and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Proxy for influence/impact measured as log average citations per month; implicitly captures aspects like empirical strength of results (result-oriented phrases), novelty/impact signal present in title/abstract, and topical popularity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (OpenReview and ACL-OCL datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Predictive proxy for research-paper influence (applied to research hypotheses and full papers)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Citation prediction is more tractable than review-score prediction. On ACL-OCL, a Title+Abstract model (SPECTER2 embeddings + MLP/no-context) achieved pairwise comparison accuracy 0.665 (±0.010) and Spearman ρs = 0.481 (±0.026); using only Hypothesis yielded accuracy 0.615 and ρs = 0.339. Contexts containing result-related information (Experiments & Results) increased performance. Marginal gains from adding full paper or reference-title context beyond Title+Abstract were observed. Overall correlations with human review scores are weakly positive (Pearson correlations by venue generally small; see paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (model-based prediction) validated by comparing predicted citation scores to ground-truth citation records (Semantic Scholar) and by correlating with human review scores (Pearson correlations). Models trained and evaluated automatically; some qualitative human analyses (Shapley) interpret features.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with ground-truth citation counts from Semantic Scholar; Spearman rank and Pearson correlations reported; pairwise comparison accuracy measured; additionally model interpretability via approximate Shapley/Owen values to check which phrases drive predictions. Temporal generalization is considered by training on older papers and testing on newer subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Citation counts are an imperfect proxy (influenced by author network, field popularity, topicality); citation distributions are highly skewed requiring log transform; content-only models lack author/metadata signals; dataset size limits model capacity and generalisation; excluding figures/tables may reduce signal; topical heterogeneity across venues affects predictive power.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>OpenReview (curated and unified by authors, with Semantic Scholar citation augmentation) and ACL-OCL (updated with citation counts and hypothesis annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4435.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4435.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Review-score prediction (RSP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Review score prediction as an automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train models to predict peer-review scores (average overall score and impact) from paper text (Title+Abstract, sections, or extracted hypothesis) to approximate human review judgments as a proxy for scientific quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Review score prediction (pairwise comparison and direct regression)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Map heterogeneous review fields across venues to a unified review schema (overall, impact, clarity, novelty, correctness, confidence, etc.), then predict average overall/impact scores. Two objective formulations: (1) pairwise comparison trained with binary cross-entropy on which of two papers has higher score (using model output differences with sigmoid), and (2) direct regression trained with L1 loss to predict numerical score. Inputs include Title+Abstract, section-based representations, and research hypothesis; embeddings computed with SPECTER2 and optionally combined with context via a one-layer Transformer encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average overall review score and impact score (normalized across venues); underlying review dimensions (clarity, correctness, novelty, impact, confidence) are used as explanatory axes though primary targets are overall/impact averages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (OpenReview, ICLR, NeurIPS subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Assessment of research hypotheses and full papers via predicted peer-review quality scores</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Review-score prediction is more difficult than citation prediction. Conference-agnostic models on full OpenReview performed at chance (no better than random). Venue-specific models (NeurIPS, ICLR) reached ~60% pairwise comparison accuracy. On an ICLR subset, the trained review-score model (Title+Abstract) had mean Pearson correlation with human mean review scores 0.330 ± 0.030, outperforming the Sakana LLM reviewer (0.161) but below human single-reviewer correlation baseline (0.412 ± 0.044). Models using Title+Abstract outperform models using hypothesis-only for review prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated predictive models compared quantitatively to human reviews (correlations, pairwise comparison accuracy) and to LLM-based reviewers; human reviewer consistency computed by leave-one-out correlation (single-review vs mean of other reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation (Pearson) between predicted and ground-truth mean review scores; pairwise comparison accuracy against actual review-score ordering; human reviewer consistency (leave-one-out) used as an upper baseline. Comparison to LLM reviewers (Sakana, Si et al.) as additional baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Heterogeneous review formats across venues complicate modeling and normalization; differing reviewing standards and topic variability across venues reduce cross-venue generalization; limited labeled review data for some venues; models overfit larger architectures; review scores are imperfect proxies (influenced by reviewer-field fit, author popularity).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>OpenReview (unified review schema, per-venue mapping) and ACL-OCL (updated), with reviews mapped to a normalized schema by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4435.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4435.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise-ranking framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise comparison / learning-to-rank framework for evaluating generated items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate generated theories/hypotheses by training models to predict which of two items has higher quality score, enabling ranking by repeated pairwise comparisons (round-robin or Swiss tournament).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pairwise comparison ranking (binary pairwise classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Train a model f_theta that takes two embedded papers (or representations) and predicts the probability that paper A has higher target score than paper B using sigmoid(f(A)-f(B)); loss is binary cross-entropy on pairwise labels derived from true scores. Can be applied repeatedly to construct a ranking of generated items (round-robin all-pairs or tournament-style).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise comparison accuracy (fraction of correctly ordered pairs) and derived ranking quality (Spearman/Pearson correlations between predicted and true rank order).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (applied to research-paper/hypothesis evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Relative ranking of predictive/hypothesis outputs (applied to both citation and review-score targets)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Pairwise models achieve higher practical performance for citation prediction (e.g., Title+Abstract pairwise accuracy 0.665 on ACL-OCL) than for review-score prediction (venue-agnostic near chance; venue-specific ~60%). Pairwise framework is recommended as it directly supports ranking generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated; validated by comparing pairwise accuracy and rank correlations to ground-truth citation/review scores and by comparing automated rankings to human reviewer orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Pairwise accuracy and Spearman rank correlation against ground-truth scores; comparisons to LLM reviewer pairwise outputs and human pairwise consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires sufficient labeled pairs to train; sensitive to how scores are normalized across venues; pairwise approach may still reflect biases in target metric (citations/reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>OpenReview and ACL-OCL subsets used to construct training pairs and evaluation pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4435.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4435.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based reviewers (comparators)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based paper reviewers (e.g., Sakana reviewer, Si et al. reviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large language model systems that take papers (or proposals) as input and produce review-like judgments or pairwise comparisons; used here as baselines for automated evaluation of generated research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based review (pairwise comparison or textual review generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Systems (e.g., Sakana from Lu et al., 2024; an LLM-based pairwise reviewer from Si et al., 2024) are applied to pairs or individual papers to predict which paper should receive a higher human review score or to generate textual reviews. Performance is measured by pairwise accuracy and correlation between LLM scores and human mean review scores. The paper applies the Si et al. pairwise LLM reviewer to ICLR/NeurIPS subsets and compares it to the trained score-prediction models and to human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise comparison accuracy; Pearson correlation between LLM-predicted scores and mean human review scores; qualitative assessment of review content richness (specificity vs generic strengths/weaknesses).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (applied to peer-review tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation judgments / generated review text (applied to research papers/proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM-based reviewers performed worse than the simple Title+Abstract prediction model on pairwise comparison accuracy in ICLR and NeurIPS subsamples. Quantitatively, the Sakana reviewer showed Pearson correlation 0.161 with mean human review scores on an ICLR subset, substantially below the trained review-score model (0.330 ± 0.030) and human single-reviewer baseline (0.412 ± 0.044). Qualitatively, LLM reviews were more generic and lacked deep semantic critique compared to human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM outputs compared directly to human reviews (correlations, pairwise accuracy) and to learned score-prediction models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct numeric correlation (Pearson) between LLM-predicted scores and ground-truth mean human scores; qualitative side-by-side example comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM reviewers often produce generic strengths/weaknesses and may not base scores on deep semantic understanding; prior studies show LLM reviewers sometimes perform no better than random for novel-hypothesis evaluation. Model internals and sizes often unspecified; reliability varies by task and input format (proposals vs full papers).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Subsamples of OpenReview test sets (ICLR-2024, NeurIPS-2024) used for direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4435.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4435.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unified review schema</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified review data model (normalized review dimensions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized schema mapping heterogeneous venue-specific review fields to a common set of normalized attributes (overall score, clarity, impact, novelty, correctness, confidence, reproducibility, ethics) to enable cross-venue modeling and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Unified review schema / normalized review dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Manually map per-venue review fields (which vary in naming and granularity) to a consistent set of attributes represented as normalized floats in [0,1] (except ethics text). The schema includes overall rating, confidence, novelty, correctness, clarity, impact, reproducibility, and concatenated review text. This unified representation is used as prediction targets and for exploratory correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Normalized numeric review dimensions (overall, impact, clarity, novelty, correctness, confidence, reproducibility) used as evaluation axes for models and for correlational analysis with citation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning peer review</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Normative criteria for paper quality (applied to review-score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using the unified schema, the authors found that almost all review dimensions positively correlate with final overall score (except reviewer confidence which is uncorrelated). Impact and clarity had the strongest correlations with final score. Correlation between mean overall review score and log average citations per month was weakly positive across most venues (with exceptions like NeurIPS).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: human reviews were aggregated and normalized to create targets; automated models predicted these normalized dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical correlation analyses (Pearson) between schema dimensions and overall scores; manual mapping and inspection documented in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Mapping is manual and imperfect; venues differ in what fields they report and in scoring scales, making normalization lossy; heterogeneous formats hinder cross-venue prediction; normalization may not fully remove venue-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>OpenReview submissions with reviews mapped to the unified schema; mapping tables documented in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4435.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4435.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shapley/Owen explainability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature-attribution via approximate Shapley / Owen values</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use approximate Shapley (PartitionExplainer/Owen values) to attribute model predictions (citation or review-score) to input textual features/phrases, to interpret what signals models use when evaluating scientific content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Approximate Shapley / Owen value feature attribution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute approximate Shapley (Owen) values for models trained on ICLR subset to identify which tokens/phrases (from Title+Abstract or Hypothesis) contribute positively or negatively to predicted citation/review scores. Visualize per-example attributions and aggregate to understand model focus (e.g., result-oriented phrases boost citation predictions; method names influence hypothesis-based models).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Importance of input phrases/features for model outputs; directional effects (positive/negative contribution) used as interpretability criteria to validate that model leverages semantically meaningful signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (model interpretability for evaluation models)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Interpretability framework for predictive evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Shapley analyses showed Title+Abstract models focus on result-oriented phrases (e.g., 'superhuman performance', 'speedup') for positive contributions; Hypothesis-based models focus more on named methods/topics. Some phrases contributed oppositely between citation- and review-score models (e.g., 'efficient exploration', 'adversarial objectives'), revealing different learned signals.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated interpretability of automated evaluation models; qualitative comparison with human expectations and domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Qualitative inspection of Shapley value visualizations for example inputs and aggregated behavior across samples; supportive evidence that model uses plausible predictive cues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Shapley approximations can be computationally expensive and dependent on feature representation; attribution does not guarantee causal links and can be misleading on correlated phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ICLR subset of OpenReview used for Shapley/Owen value analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4435.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4435.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis annotation + author validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Research-hypothesis extraction via gpt-3.5-turbo with author validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic extraction of each paper's research hypothesis using GPT-3.5-turbo followed by author (first-author) evaluation of annotation quality on correctness, precision and completeness via Likert surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Automated hypothesis annotation and human validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a one-shot prompt to gpt-3.5-turbo to generate a concise research-hypothesis representation (problem + solution). Then survey first authors to rate the extracted hypothesis on 5-point Likert scales for problem correctness/precision/completeness and solution correctness/precision/completeness. Use these annotations as inputs to hypothesis-only predictive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Annotation judged on correctness, precision, completeness for problem and solution components (5-point Likert scales); used to assess whether hypothesis-only inputs are faithful and useful for downstream prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (applied to paper text summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Concise research hypothesis statements (problem + proposed solution)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Author survey (13 first authors rating 32 hypotheses) showed overall satisfactory correctness but lower completeness; mean ratings: Problem Correctness 4.125 (±1.083), Problem Precision 3.688 (±1.158), Problem Completeness 3.563 (±1.144); Solution Correctness 4.125 (±1.293), Solution Precision 3.906 (±1.042), Solution Completeness 3.625 (±1.192). Hypothesis-only models predicted citation/review scores better than random but worse than Title+Abstract models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated extraction by GPT-3.5-turbo, human validation by first authors via survey; used as automated model input for downstream predictive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>First-author Likert ratings aggregated; comparison of hypothesis-only model performance to Title+Abstract baselines and to random.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Annotation quality limited by upstream PDF parsing (GROBID) errors; completeness of extracted hypotheses is often lacking; only a small sample of authors validated annotations limiting generality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers <em>(Rating: 2)</em></li>
                <li>The AI scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>MARG: multi-agent review generation for scientific papers <em>(Rating: 1)</em></li>
                <li>Realistic citation count prediction task for newly published papers <em>(Rating: 2)</em></li>
                <li>A neural citation count prediction model based on peer review text <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4435",
    "paper_id": "paper-276903104",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Citation-count prediction",
            "name_full": "Citation count prediction as an automatic evaluation metric",
            "brief_description": "Use content-based models to predict a paper's future citation rate (log average citations per month) as a proxy metric for the scientific influence/quality of AI-generated hypotheses or papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Citation count prediction (log average citations per month)",
            "evaluation_method_description": "Compute the logarithm of average citations per month for each paper (total citations divided by months since publication), and train models (embeddings from SPECTER2 with MLP or transformer context encoder) to predict this target. Can be trained either as pairwise comparison (binary cross-entropy on which of two papers has higher score) or direct regression (L1 loss on log-citation score). Inputs tested include Title+Abstract, individual sections (Introduction, Methods, Experiments & Results, Related Work, Conclusion) and an extracted research hypothesis. Context can include titles/abstracts of references or other paper sections; long inputs are chunked into sentence embeddings and averaged.",
            "evaluation_criteria": "Proxy for influence/impact measured as log average citations per month; implicitly captures aspects like empirical strength of results (result-oriented phrases), novelty/impact signal present in title/abstract, and topical popularity.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning (OpenReview and ACL-OCL datasets)",
            "theory_type": "Predictive proxy for research-paper influence (applied to research hypotheses and full papers)",
            "human_comparison": true,
            "evaluation_results": "Citation prediction is more tractable than review-score prediction. On ACL-OCL, a Title+Abstract model (SPECTER2 embeddings + MLP/no-context) achieved pairwise comparison accuracy 0.665 (±0.010) and Spearman ρs = 0.481 (±0.026); using only Hypothesis yielded accuracy 0.615 and ρs = 0.339. Contexts containing result-related information (Experiments & Results) increased performance. Marginal gains from adding full paper or reference-title context beyond Title+Abstract were observed. Overall correlations with human review scores are weakly positive (Pearson correlations by venue generally small; see paper Table 1).",
            "automated_vs_human_evaluation": "Automated metric (model-based prediction) validated by comparing predicted citation scores to ground-truth citation records (Semantic Scholar) and by correlating with human review scores (Pearson correlations). Models trained and evaluated automatically; some qualitative human analyses (Shapley) interpret features.",
            "validation_method": "Correlation with ground-truth citation counts from Semantic Scholar; Spearman rank and Pearson correlations reported; pairwise comparison accuracy measured; additionally model interpretability via approximate Shapley/Owen values to check which phrases drive predictions. Temporal generalization is considered by training on older papers and testing on newer subsets.",
            "limitations_challenges": "Citation counts are an imperfect proxy (influenced by author network, field popularity, topicality); citation distributions are highly skewed requiring log transform; content-only models lack author/metadata signals; dataset size limits model capacity and generalisation; excluding figures/tables may reduce signal; topical heterogeneity across venues affects predictive power.",
            "benchmark_dataset": "OpenReview (curated and unified by authors, with Semantic Scholar citation augmentation) and ACL-OCL (updated with citation counts and hypothesis annotations).",
            "uuid": "e4435.0",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Review-score prediction (RSP)",
            "name_full": "Review score prediction as an automatic evaluation metric",
            "brief_description": "Train models to predict peer-review scores (average overall score and impact) from paper text (Title+Abstract, sections, or extracted hypothesis) to approximate human review judgments as a proxy for scientific quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Review score prediction (pairwise comparison and direct regression)",
            "evaluation_method_description": "Map heterogeneous review fields across venues to a unified review schema (overall, impact, clarity, novelty, correctness, confidence, etc.), then predict average overall/impact scores. Two objective formulations: (1) pairwise comparison trained with binary cross-entropy on which of two papers has higher score (using model output differences with sigmoid), and (2) direct regression trained with L1 loss to predict numerical score. Inputs include Title+Abstract, section-based representations, and research hypothesis; embeddings computed with SPECTER2 and optionally combined with context via a one-layer Transformer encoder.",
            "evaluation_criteria": "Average overall review score and impact score (normalized across venues); underlying review dimensions (clarity, correctness, novelty, impact, confidence) are used as explanatory axes though primary targets are overall/impact averages.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning (OpenReview, ICLR, NeurIPS subsets)",
            "theory_type": "Assessment of research hypotheses and full papers via predicted peer-review quality scores",
            "human_comparison": true,
            "evaluation_results": "Review-score prediction is more difficult than citation prediction. Conference-agnostic models on full OpenReview performed at chance (no better than random). Venue-specific models (NeurIPS, ICLR) reached ~60% pairwise comparison accuracy. On an ICLR subset, the trained review-score model (Title+Abstract) had mean Pearson correlation with human mean review scores 0.330 ± 0.030, outperforming the Sakana LLM reviewer (0.161) but below human single-reviewer correlation baseline (0.412 ± 0.044). Models using Title+Abstract outperform models using hypothesis-only for review prediction.",
            "automated_vs_human_evaluation": "Hybrid: automated predictive models compared quantitatively to human reviews (correlations, pairwise comparison accuracy) and to LLM-based reviewers; human reviewer consistency computed by leave-one-out correlation (single-review vs mean of other reviews).",
            "validation_method": "Correlation (Pearson) between predicted and ground-truth mean review scores; pairwise comparison accuracy against actual review-score ordering; human reviewer consistency (leave-one-out) used as an upper baseline. Comparison to LLM reviewers (Sakana, Si et al.) as additional baselines.",
            "limitations_challenges": "Heterogeneous review formats across venues complicate modeling and normalization; differing reviewing standards and topic variability across venues reduce cross-venue generalization; limited labeled review data for some venues; models overfit larger architectures; review scores are imperfect proxies (influenced by reviewer-field fit, author popularity).",
            "benchmark_dataset": "OpenReview (unified review schema, per-venue mapping) and ACL-OCL (updated), with reviews mapped to a normalized schema by the authors.",
            "uuid": "e4435.1",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Pairwise-ranking framework",
            "name_full": "Pairwise comparison / learning-to-rank framework for evaluating generated items",
            "brief_description": "Evaluate generated theories/hypotheses by training models to predict which of two items has higher quality score, enabling ranking by repeated pairwise comparisons (round-robin or Swiss tournament).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Pairwise comparison ranking (binary pairwise classifier)",
            "evaluation_method_description": "Train a model f_theta that takes two embedded papers (or representations) and predicts the probability that paper A has higher target score than paper B using sigmoid(f(A)-f(B)); loss is binary cross-entropy on pairwise labels derived from true scores. Can be applied repeatedly to construct a ranking of generated items (round-robin all-pairs or tournament-style).",
            "evaluation_criteria": "Pairwise comparison accuracy (fraction of correctly ordered pairs) and derived ranking quality (Spearman/Pearson correlations between predicted and true rank order).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning (applied to research-paper/hypothesis evaluation)",
            "theory_type": "Relative ranking of predictive/hypothesis outputs (applied to both citation and review-score targets)",
            "human_comparison": true,
            "evaluation_results": "Pairwise models achieve higher practical performance for citation prediction (e.g., Title+Abstract pairwise accuracy 0.665 on ACL-OCL) than for review-score prediction (venue-agnostic near chance; venue-specific ~60%). Pairwise framework is recommended as it directly supports ranking generated outputs.",
            "automated_vs_human_evaluation": "Automated; validated by comparing pairwise accuracy and rank correlations to ground-truth citation/review scores and by comparing automated rankings to human reviewer orderings.",
            "validation_method": "Pairwise accuracy and Spearman rank correlation against ground-truth scores; comparisons to LLM reviewer pairwise outputs and human pairwise consistency.",
            "limitations_challenges": "Requires sufficient labeled pairs to train; sensitive to how scores are normalized across venues; pairwise approach may still reflect biases in target metric (citations/reviews).",
            "benchmark_dataset": "OpenReview and ACL-OCL subsets used to construct training pairs and evaluation pairs.",
            "uuid": "e4435.2",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLM-based reviewers (comparators)",
            "name_full": "LLM-based paper reviewers (e.g., Sakana reviewer, Si et al. reviewer)",
            "brief_description": "Large language model systems that take papers (or proposals) as input and produce review-like judgments or pairwise comparisons; used here as baselines for automated evaluation of generated research.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-based review (pairwise comparison or textual review generation)",
            "evaluation_method_description": "Systems (e.g., Sakana from Lu et al., 2024; an LLM-based pairwise reviewer from Si et al., 2024) are applied to pairs or individual papers to predict which paper should receive a higher human review score or to generate textual reviews. Performance is measured by pairwise accuracy and correlation between LLM scores and human mean review scores. The paper applies the Si et al. pairwise LLM reviewer to ICLR/NeurIPS subsets and compares it to the trained score-prediction models and to human reviewers.",
            "evaluation_criteria": "Pairwise comparison accuracy; Pearson correlation between LLM-predicted scores and mean human review scores; qualitative assessment of review content richness (specificity vs generic strengths/weaknesses).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning (applied to peer-review tasks)",
            "theory_type": "Evaluation judgments / generated review text (applied to research papers/proposals)",
            "human_comparison": true,
            "evaluation_results": "LLM-based reviewers performed worse than the simple Title+Abstract prediction model on pairwise comparison accuracy in ICLR and NeurIPS subsamples. Quantitatively, the Sakana reviewer showed Pearson correlation 0.161 with mean human review scores on an ICLR subset, substantially below the trained review-score model (0.330 ± 0.030) and human single-reviewer baseline (0.412 ± 0.044). Qualitatively, LLM reviews were more generic and lacked deep semantic critique compared to human reviews.",
            "automated_vs_human_evaluation": "Automated LLM outputs compared directly to human reviews (correlations, pairwise accuracy) and to learned score-prediction models.",
            "validation_method": "Direct numeric correlation (Pearson) between LLM-predicted scores and ground-truth mean human scores; qualitative side-by-side example comparisons.",
            "limitations_challenges": "LLM reviewers often produce generic strengths/weaknesses and may not base scores on deep semantic understanding; prior studies show LLM reviewers sometimes perform no better than random for novel-hypothesis evaluation. Model internals and sizes often unspecified; reliability varies by task and input format (proposals vs full papers).",
            "benchmark_dataset": "Subsamples of OpenReview test sets (ICLR-2024, NeurIPS-2024) used for direct comparisons.",
            "uuid": "e4435.3",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Unified review schema",
            "name_full": "Unified review data model (normalized review dimensions)",
            "brief_description": "A standardized schema mapping heterogeneous venue-specific review fields to a common set of normalized attributes (overall score, clarity, impact, novelty, correctness, confidence, reproducibility, ethics) to enable cross-venue modeling and prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Unified review schema / normalized review dimensions",
            "evaluation_method_description": "Manually map per-venue review fields (which vary in naming and granularity) to a consistent set of attributes represented as normalized floats in [0,1] (except ethics text). The schema includes overall rating, confidence, novelty, correctness, clarity, impact, reproducibility, and concatenated review text. This unified representation is used as prediction targets and for exploratory correlation analyses.",
            "evaluation_criteria": "Normalized numeric review dimensions (overall, impact, clarity, novelty, correctness, confidence, reproducibility) used as evaluation axes for models and for correlational analysis with citation counts.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning peer review",
            "theory_type": "Normative criteria for paper quality (applied to review-score prediction)",
            "human_comparison": null,
            "evaluation_results": "Using the unified schema, the authors found that almost all review dimensions positively correlate with final overall score (except reviewer confidence which is uncorrelated). Impact and clarity had the strongest correlations with final score. Correlation between mean overall review score and log average citations per month was weakly positive across most venues (with exceptions like NeurIPS).",
            "automated_vs_human_evaluation": "Hybrid: human reviews were aggregated and normalized to create targets; automated models predicted these normalized dimensions.",
            "validation_method": "Statistical correlation analyses (Pearson) between schema dimensions and overall scores; manual mapping and inspection documented in appendices.",
            "limitations_challenges": "Mapping is manual and imperfect; venues differ in what fields they report and in scoring scales, making normalization lossy; heterogeneous formats hinder cross-venue prediction; normalization may not fully remove venue-specific biases.",
            "benchmark_dataset": "OpenReview submissions with reviews mapped to the unified schema; mapping tables documented in appendices.",
            "uuid": "e4435.4",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Shapley/Owen explainability",
            "name_full": "Feature-attribution via approximate Shapley / Owen values",
            "brief_description": "Use approximate Shapley (PartitionExplainer/Owen values) to attribute model predictions (citation or review-score) to input textual features/phrases, to interpret what signals models use when evaluating scientific content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Approximate Shapley / Owen value feature attribution",
            "evaluation_method_description": "Compute approximate Shapley (Owen) values for models trained on ICLR subset to identify which tokens/phrases (from Title+Abstract or Hypothesis) contribute positively or negatively to predicted citation/review scores. Visualize per-example attributions and aggregate to understand model focus (e.g., result-oriented phrases boost citation predictions; method names influence hypothesis-based models).",
            "evaluation_criteria": "Importance of input phrases/features for model outputs; directional effects (positive/negative contribution) used as interpretability criteria to validate that model leverages semantically meaningful signals.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning (model interpretability for evaluation models)",
            "theory_type": "Interpretability framework for predictive evaluation metrics",
            "human_comparison": null,
            "evaluation_results": "Shapley analyses showed Title+Abstract models focus on result-oriented phrases (e.g., 'superhuman performance', 'speedup') for positive contributions; Hypothesis-based models focus more on named methods/topics. Some phrases contributed oppositely between citation- and review-score models (e.g., 'efficient exploration', 'adversarial objectives'), revealing different learned signals.",
            "automated_vs_human_evaluation": "Automated interpretability of automated evaluation models; qualitative comparison with human expectations and domain knowledge.",
            "validation_method": "Qualitative inspection of Shapley value visualizations for example inputs and aggregated behavior across samples; supportive evidence that model uses plausible predictive cues.",
            "limitations_challenges": "Shapley approximations can be computationally expensive and dependent on feature representation; attribution does not guarantee causal links and can be misleading on correlated phrases.",
            "benchmark_dataset": "ICLR subset of OpenReview used for Shapley/Owen value analyses.",
            "uuid": "e4435.5",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Hypothesis annotation + author validation",
            "name_full": "Research-hypothesis extraction via gpt-3.5-turbo with author validation",
            "brief_description": "Automatic extraction of each paper's research hypothesis using GPT-3.5-turbo followed by author (first-author) evaluation of annotation quality on correctness, precision and completeness via Likert surveys.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Automated hypothesis annotation and human validation",
            "evaluation_method_description": "Use a one-shot prompt to gpt-3.5-turbo to generate a concise research-hypothesis representation (problem + solution). Then survey first authors to rate the extracted hypothesis on 5-point Likert scales for problem correctness/precision/completeness and solution correctness/precision/completeness. Use these annotations as inputs to hypothesis-only predictive experiments.",
            "evaluation_criteria": "Annotation judged on correctness, precision, completeness for problem and solution components (5-point Likert scales); used to assess whether hypothesis-only inputs are faithful and useful for downstream prediction.",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "scientific_domain": "Computer Science / Machine Learning (applied to paper text summarization)",
            "theory_type": "Concise research hypothesis statements (problem + proposed solution)",
            "human_comparison": true,
            "evaluation_results": "Author survey (13 first authors rating 32 hypotheses) showed overall satisfactory correctness but lower completeness; mean ratings: Problem Correctness 4.125 (±1.083), Problem Precision 3.688 (±1.158), Problem Completeness 3.563 (±1.144); Solution Correctness 4.125 (±1.293), Solution Precision 3.906 (±1.042), Solution Completeness 3.625 (±1.192). Hypothesis-only models predicted citation/review scores better than random but worse than Title+Abstract models.",
            "automated_vs_human_evaluation": "Hybrid: automated extraction by GPT-3.5-turbo, human validation by first authors via survey; used as automated model input for downstream predictive evaluation.",
            "validation_method": "First-author Likert ratings aggregated; comparison of hypothesis-only model performance to Title+Abstract baselines and to random.",
            "limitations_challenges": "Annotation quality limited by upstream PDF parsing (GROBID) errors; completeness of extracted hypotheses is often lacking; only a small sample of authors validated annotations limiting generality.",
            "uuid": "e4435.6",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers",
            "rating": 2,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        },
        {
            "paper_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "MARG: multi-agent review generation for scientific papers",
            "rating": 1,
            "sanitized_title": "marg_multiagent_review_generation_for_scientific_papers"
        },
        {
            "paper_title": "Realistic citation count prediction task for newly published papers",
            "rating": 2,
            "sanitized_title": "realistic_citation_count_prediction_task_for_newly_published_papers"
        },
        {
            "paper_title": "A neural citation count prediction model based on peer review text",
            "rating": 2,
            "sanitized_title": "a_neural_citation_count_prediction_model_based_on_peer_review_text"
        }
    ],
    "cost": 0.016372,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Evaluation Metrics for Artificially Generated Scientific Research</p>
<p>Niklas Hoepner n.r.hopner@uva.nl 
University of Amsterdam</p>
<p>Leon Eshuijs 
Vrije Universiteit</p>
<p>Dimitrios Alivanistos 
Vrije Universiteit</p>
<p>Giacomo Zamprogno 
Vrije Universiteit</p>
<p>Ilaria Tiddi 
Vrije Universiteit</p>
<p>Automatic Evaluation Metrics for Artificially Generated Scientific Research
17E6E966FFD021CDF8B02D56EB62D111
Foundation models are increasingly used in scientific research, but evaluating AI-generated scientific work remains challenging.Expert reviews are costly, while large language models (LLMs) as proxy reviewers have proven to be unreliable.To address this, we investigate two automatic evaluation metrics, specifically citation count prediction and review score prediction.We parse all papers of OpenReview and augment each submission with its citation count, reference, and research hypothesis.Our findings reveal that citation count prediction is more viable than review score prediction, and predicting scores is more difficult purely from the research hypothesis than from the full paper.Furthermore, we show that a simple prediction model based solely on title and abstract outperforms LLM-based reviewers, though it still falls short of human-level consistency 12 .</p>
<p>Introduction</p>
<p>Advances in foundation models (Driess et al., 2023;OpenAI, 2023) have increased interest in their potential to enhance various stages of the research process (Boiko et al., 2023;Lee et al., 2022), including research hypothesis generation (Wang et al., 2024;Baek et al., 2024;Si et al., 2024), writing assistance (Funkquist et al., 2023;Gao et al., 2023), and peer review (Liu and Shah, 2023;Yuan et al., 2022).Their application to literature-based generation of scientific content has been particularly notable, producing research hypotheses that domain experts consider more novel than those generated by humans (Si et al., 2024).While much of the focus has been on generating research hypotheses (Si et al., 2024;Baek et al., 2024;Wang et al., 2024), fewer studies have explored the generation of complete scientific papers or multiple steps of the research process (Lu et al., 2024;Li et al., 2024).</p>
<p>A key challenge in this area is evaluating the quality of generated scientific content (Baek et al., 2024;Lu et al., 2024;Si et al., 2024).Most studies use a mix of domain expert assessments and LLMbased reviews (Baek et al., 2024;Wang et al., 2024;Qi et al., 2023).However, expert evaluations are costly and time-consuming, limiting them to a subset of methodologies, often selected based on LLM reviewer results (Qi et al., 2023).While early studies showed promise for LLMs in reviewing papers (Baek et al., 2024;Lu et al., 2024), recent findings reveal their reliability issues, with some cases showing LLM reviewers performing no better than random guessing compared to human evaluations of novel research hypotheses (Si et al., 2024).</p>
<p>An automatic evaluation metric should be efficient to compute, reliably indicate scientific quality and generalise to unseen future work.The field of scholarly document quality prediction (SDQP) (de Buy Wenniger et al., 2023) has studied the problem of predicting review scores and citation counts (Kang et al., 2018;Hirako et al., 2024) as proxy metrics.Domain experts reviews are the gold standard for evaluating the scientific quality of a paper and citation counts measure a paper's influence on the scientific community.However, limited data availability has constrained review score prediction (Staudinger et al., 2024), while citation prediction typically relies on factors beyond paper content such as paper metadata (Zhang and Wu, 2024).Prior work has not studied the challenge of predicting scientific quality based on research hypotheses alone and assessing generalisation to future work.</p>
<p>To study both prediction problems, we propose data models for scientific papers and reviews to parse all OpenReview submissions 3 into a unified format, further annotating submissions with citation counts and research hypotheses.This allows us to analyse the relationship between review and 3 https:/openreview.netarXiv:2503.05712v1[cs.CY] 14 Feb 2025 citation scores, evaluate their predictability, and compare them with LLM-based review systems.</p>
<p>In summary, our contributions are:</p>
<p>• We propose citation count and review score prediction as automatic evaluation metrics for AI-generated scientific content.</p>
<p>• We parse all OpenReview submissions into a unifying format, augmenting them with additional metadata.</p>
<p>• Demonstrate that a simple score prediction model is more consistent with human reviews than LLM-based reviewers.</p>
<p>2 Related Work</p>
<p>AI generated Science</p>
<p>Advancements in instructable generative models (OpenAI, 2023;Esser et al., 2024) have sparked interest in their use for scientific content generation (Wang et al., 2024;Lu et al., 2024;Si et al., 2024;Bran et al., 2024;Funkquist et al., 2023).Large Language Models (LLMs) have been explored for tasks such as research hypothesis generation (Wang et al., 2024;Baek et al., 2024;Si et al., 2024), paper drafting (Funkquist et al., 2023;Gao et al., 2023), and even experimental design (Bran et al., 2024;Li et al., 2024).Most studies focus on specific subproblems.Notable exceptions include Lu et al. (2024), who apply LLMs to the entire research process, and Li et al. (2024), who focus on machine learning research but exclude report writing.</p>
<p>A key challenge in this domain is evaluating generated outputs, such as the novelty or feasibility of a research hypothesis, which often requires timeconsuming, costly reviews by domain experts (Si et al., 2024).To address this, some researchers have used LLM-based evaluations as substitutes for human judgment (Baek et al., 2024;Qi et al., 2023;Yang et al., 2024), with some even replacing human evaluators in certain cases entirely (Lu et al., 2024).However, recent studies question the reliability of LLMs as evaluators compared to human reviewers (Si et al., 2024).We propose citation count and review score prediction as alternative evaluation metrics and compare them to LLM-based systems.</p>
<p>Automated Peer Reviewing</p>
<p>With the increase in scientific paper submissions, interest in automating aspects of the peer review process has grown (Fernandes and de Melo, 2024), spanning tasks like reviewer assignment (Jecmen et al., 2023) and review generation (Yuan et al., 2022;D'Arcy et al., 2024;Zhou et al., 2024).An overview of the tasks and related datasets is provided in Staudinger et al. (2024).</p>
<p>While, automatically generated textual reviews (D'Arcy et al., 2024) can provide valuable feedback to authors, they are unsuitable as standalone review metrics, as it is difficult to determine which method generates more valuable research papers.To develop automatic evaluation metrics, we focus on the task of Review Score Prediction (RSP) (Kang et al., 2018).Another related task, Paper Decision Prediction (DSP) (Bharti et al., 2021;Fernandes and de Melo, 2024), could also contribute to evaluation metrics but offers less granularity.While Kang et al. (2018) explore review score prediction, their work is limited to a small dataset (PeerRead,&lt;1000 submissions) and does not address temporal generalization.Follow up work focuses on improving prediction accuracy by dealing with the limited data by learning from unlabelled data (Muangkammuen et al., 2022) or adding intermediate tasks for finetuning (Muangkammuen et al., 2023).</p>
<p>As noted by Staudinger et al. (2024), most studies create their own datasets, many of which are derived from OpenReview.Alternative sources of reviews, such as F10004 and PeerJ5 cover a more diverse range of topics but lack an anonymous review process (Dycke et al., 2023).The lack of a standardized, updated dataset with rich metadata limits method comparisons and progress in automating reviews.Our work addresses this by providing data models for scientific papers and reviews to unify all OpenReview submissions.</p>
<p>Citation Count Prediction</p>
<p>Citation count prediction approaches differ based on the information used (Zhang and Wu, 2024).Most models combine citation history (Wang et al., 2021b), metadata (e.g., authors, h-index) (Bai et al., 2019), and paper content (van Dongen et al., 2020).For newly generated content, without citation history or metadata, only content-based models are applicable to build evaluation metrics.Early studies used n-gram and term-frequency features from titles and abstracts, with small datasets ( 3,000 papers) and short time spans (Fu and Aliferis, 2008;Ibáñez et al., 2009).Recent efforts scaled dataset sizes ( 30,000-40,000 papers) and incorporated embeddings from large language models to improve prediction performance (van Dongen et al., 2020;Jr. et al., 2024).Some studies show significant benefits from using full papers, while others report only marginal improvements (Jr. et al., 2024).</p>
<p>These conflicting findings may stem from differences in datasets and target metrics employed by each study.Such variations complicate direct comparisons across works, hindering the overall progress of the field.Furthermore, few approaches explicitly account for generalization over time.Notably, many studies use validation sets with publication dates that are only minimally separated from those in the training set, with a maximum gap of one month (Hirako et al., 2023(Hirako et al., , 2024)).</p>
<p>Few studies have explored the relationship between citation counts and peer-review scores.While prior work shows that reviews can improve citation prediction models (Li et al., 2019;Plank and van Dalen, 2019), our focus is on their correlation.A strong correlation would suggest both metrics capture similar aspects of a paper's quality.Wang et al. (2021a) found a weak positive correlation between citation counts and review scores for ICLR submissions from 2017 to 2019, which we extend to the entire OpenReview dataset.</p>
<p>Score Prediction</p>
<p>To address the challenge of evaluating the quality of scientific content generated by LLMs, we study the problem of predicting quality scores of scientific papers.As proxies for quality, we utilize citation counts and review scores.We consider two primary tasks: (1) pairwise ranking, where the objective is to predict which of two papers has a higher quality score, and (2) regression, where the goal is to predict the quality score of a single pa-per.The pairwise ranker can generate a ranking of generated content by comparing items in sequence, using either a round-robin format where each item is compared against every other item, or a Swiss tournament system where items are matched based on their current rankings (Si et al., 2024).</p>
<p>Formally, the dataset is represented as
D = {(ω, c, s)} N i=1
, where ω denotes the representation of a paper, c refers to paper context such as references and s is the associated quality score.In this work, we investigate representing papers via different part of the paper ranging from title and abstract to their research hypothesis.Given a scientific text ω or c, we compute embeddings using the SPECTER2 model6 with the regression adapter (Singh et al., 2023).SPECTER2 is specifically designed to compute embeddings for tasks requiring high-quality representations of scientific text.When the input text exceeds the context length supported by the embedding model, we segment the text into sentences using the NLTK sentence tokenizer (Bird, 2006).For each chunk, we compute its embedding independently and subsequently average the embeddings to obtain a fixed-size representation of the entire text.We freeze the parameters of the embedding model and denote embedded scientific text via subscript e (ω e ,c e ).</p>
<p>Let d i = (ω e,i , c e,i , s i ) denote the input data for a single paper.The score model f θ is trained to predict the mapping from the target score by minimizing two objectives:</p>
<ol>
<li>
<p>Pairwise Comparison: We minimize the binary cross-entropy loss between score differences:
L(d 1 , d 2 ; θ) = x log y − (1 − x) log(1 − y),(1)
where x = 1 s 1 &gt;s 2 is a binary indicator for the score comparison, and
y = σ(f θ (d 1 ) − f θ (d 2 ))
represents the predicted probability that paper d 1 has a higher score than paper d 2 .</p>
</li>
<li>
<p>Direct Score Prediction: We minimize the L1 distance to learn the direct mapping of the input to the score: L(ω e , c e , s; θ) = |f θ (ω e , c e ) − s|.</p>
</li>
</ol>
<p>(2)</p>
<p>For models without contextual information, f is implemented as an MLP with a single hidden layer.When context is included, it is represented as a sequence of embeddings (e.g., SPECTER2 embeddings derived from the title and abstract of the references).The context embeddings c e are combined with the paper representation ω e to form a sequence of embeddings which is then processed through a one-layer Transformer encoder (Vaswani et al., 2017).The resulting paper representation embedding is then passed through a MLP similarly to predict the target score (see Figure 1).We look at two types of contexts.In the first, the paper is represented by its title and abstract, with the remaining sections serving as the contextual information.In the second approach, to incorporate references, the paper is still represented by its title and abstract, while the context is formed by the title and abstract of the references.We choose the following target scores for the each score type.</p>
<p>Citation Count: For the citation count, we use the average citations per month as the prediction metric.The score is calculated by dividing the total citation count at the time of model training by the number of months since the paper was published.Given that the distribution of citation scores is highly skewed-where many papers have only a few citations while others receive a significantly larger number-we predict the logarithm of the average citations per month (van Dongen et al., 2020;de Buy Wenniger et al., 2023).</p>
<p>Review Scores: For the available data on doubleblind peer-reviews, each paper is associated with multiple reviews, which can vary in format depending on the conference the paper is submitted to.Reviews often include an overall decision, a review score, and evaluations of specific aspects of the paper, such as clarity and impact.We map all the review data to our review data model (Figure 7 in Appendix A).From the available scores, we predict the average overall review score and impact score.</p>
<p>More complex models are possible.However, we found larger models to overfit on the training set and not generalize to the more recent papers of the test set despite applying common regularization techniques (Srivastava et al., 2014).Further training details are described in Appendix B.</p>
<p>Datasets</p>
<p>Our work builds on the widely used ACL-OCL dataset (Rohatgi et al., 2023), enriched with updated citation counts, title and abstract of references and annotated research hypotheses.We created to the best of our knowledge largest dataset of OpenReview submissions that exists and aug- mented submissions with the same metadata as the ACL-OCL dataset7 .</p>
<p>OpenReview: OpenReview8 serves as a valuable resource for linking scientific paper submissions with their corresponding peer-review assessments and has been a key source for datasets used in studies of the peer-review process (Staudinger et al., 2024).However, a significant challenge lies in the lack of a unified format of reviews from different venues.For instance, while many workshops include only an overall score, ICLR-2023 includes five distinct fields, such as clarity and novelty (see Table 8 in Appendix C).To address this, we developed a unified data model for reviews and manually mapped the fields from each venue to our standardized schema (see Table 9 in Appendix C).</p>
<p>For each submission, we processed the PDF documents using GROBID9 .To flexibly condition on different parts of an academic paper we train a section classifier that takes a paragraph of an academic text as input and classifies it as introduction, background, methodology, experiments &amp; results or conclusion.More details on the training of the section classifier can be found in Appendix D. Each submission was annotated with its research hypothesis using prompts to GPT-3.5-turbo.To assess annotation quality, we ask first authors to evaluate the annotations for their respective papers.More details on the annotation process and the results from the survey can be found in Appendix E.</p>
<p>Additionally, we retrieved title and abstract of the references of all submissions and citation counts for accepted submissions from Semantic Scholar10 .Summary statistics of the dataset are provided in Table 10 in Appendix C. While all reviews are licensed under CC BY 4.0, not all submissions are accompanied by a license11 .To ensure compliance, we publicly release the full dataset (excluding submissions) and provide access to submissions that have a CC BY 4.0 license.The complete dataset, including submissions, is accessible via a script from Huggingface.Details on the dataset creation process can be found in Appendix C.</p>
<p>ACL-OCL: For the ACL-OCL dataset (Rohatgi et al., 2023), we update the citation scores, retrieve the title and abstract of the references and label the sections of the already provided GROBID parses and label each sample with its corresponding research hypothesis.</p>
<p>Experiments</p>
<p>First we study the relation between citation counts and review scores to understand how interchangeable these metrics are.Then we discuss the results for both score prediction tasks and compare the performance of simple score predictors with LLM based reviewers.Lastly, we provide a qualitative analysis to interpret the insights gained from the score prediction models.</p>
<p>Citation Count vs. Review Scores</p>
<p>We analyse the correlation between different review score dimensions and the correlation between citation counts and review scores.All review dimensions, except reviewer confidence, exhibit a positive correlation with the final score (see Figure 2).This indicates that reviewers, on average, do not exhibit varying levels of confidence when assigning high or low overall scores.Among the dimensions, impact and clarity show the strongest correlations with the final score.These results align partially with the findings of Kang et al. (2018), who analysed ACL 2017 reviews and identified clarity as a key factor, though their study reported a weaker correlation for impact compared to our findings.</p>
<p>The correlation between review scores and the logarithm of average citations per month across different venues is presented in Table 1.We analyze only venues from before 2024, allowing papers to have been published for at least one year as early citation counts exhibit high variance (Zhang and Wu, 2024).Overall, we observe a weak positive correlation for most venues, with the exception of NeurIPS, where the correlation is negligible.For broad-topic conferences like ICLR and NeurIPS, a potential confounding factor is the varying popularity of different fields, which can influence citation counts regardless of the quality of the work.When separating ICLR-2023 papers by their authorprovided field, we find mixed results: the correlation strengthens for some fields but disappears entirely for others (see Table 18  There only seems to be a weak overlap of the factors influencing citation count and review scores.Neither review scores nor citation counts alone can be considered definitive measures of scientific quality.Both metrics are influenced by external factors, such as author popularity or reviewer-field fit, that are unrelated to the intrinsic quality of the work.However, citation counts have the advantage of being easier to collect on a larger scale, making it easier to train models of larger scale which can be leveraged as an evaluation metric.</p>
<p>Score Prediction</p>
<p>Citation Score Prediction: In this section, we investigate the task of predicting the log average citations per month using various paper representations and contexts.Specifically, we conduct experiments to evaluate the predictive performance of different section types as paper representations, comparing them to representations based on Title + Abstract and Hypothesis.The results on the ACL-OCL dataset are presented in  tion scores achieve similar Spearman correlations with the ground-truth data.Contexts containing result-related information demonstrate higher prediction accuracy, with the most notable difference observed when comparing the Title + Abstract context to the Hypothesis context.Citation counts can still be predicted solely based on a paper's research hypothesis better than random.These results are in line with the intuition that empirical work will be cited more often if the results are more impressive.</p>
<p>Next, we train the context models and rerun the title and abstract baseline with an empty context to isolate the performance change caused by the architectural adjustment.The results show marginal benefits from training on complete paper information or adding the titles and abstracts of all references as context (see Figure 3).A likely reason for the limited improvement is that most content-related variability in citation scores is already captured by the title and abstract.Remaining differences may stem from external factors, such as the authors' networks.Additionally, the small dataset size constrains training to simpler models, as larger models tend to overfit and fail to generalize.Expanding datasets and utilizing full-text models could improve comparison accuracy in future studies.</p>
<p>Review Score Prediction: For review score prediction, our findings indicate that conferenceagnostic prediction is challenging.On the full OpenReview dataset, the model's performance is no better than random guessing (see Table 3).Two factors likely contribute to this difficulty.First, reviewing standards differ across conferences, meaning that even after normalization, the same paper could receive a score of 0.6 in one venue and 0.8 in another.Second, the broader variability in topics between conferences, as opposed to within a single conference, makes comparisons more dif- ficult.Notably, venue-specific review score prediction models achieve a comparison accuracy of approximately 60% for NeurIPS and ICLR.</p>
<p>A consistent trend across all dataset subsets is that predicting citation counts is easier than predicting review scores, resulting in higher comparison accuracies.As observed with the ACL-OCL dataset, predictions based on title and abstract perform better than those based on the research hypothesis.The comparison accuracy for the citation score is higher on the ACL-OCL dataset, one potential reason is the increased amount of data from a less broad range of topics.To check whether distinguishing by topics improves prediction performance on the ICLR subset we ran an latent dirichlet allocation (LDA) to separate all ICLR submissions into different topics.More details on the topic model can be found in Appendix G.We trained pairwise comparison score models for the five most frequent topics.From the results in   evident that training separate prediction models per latent topic does not improve comparison accuracy.</p>
<p>Comparisons with LLM and Human Reviews</p>
<p>To evaluate our review score prediction models against both LLM-based and human reviewers, we subsampled 200 papers from the test set of ICLR 2024 and NeurIPS 2024 respectively.We first tested the LLM-based reviewer from Si et al. (2024), which takes two papers as input and predicts which one is more likely to receive a higher review score.Additionally, we apply our trained review score prediction models that take title and abstract as input to the same subsets.As shown in Table 5, our simple prediction mod-els, outperform the pairwise comparison accuracy of the LLM reviewer for both ICLR and NeurIPS.Two key differences exist compared to the original setup in Si et al. (2024): (1) the LLM in the original work evaluated project proposals rather than full papers, and (2) the reviewed papers in the original setup were restricted to LLM-related topics, potentially simplifying comparisons.Despite these differences, the LLM-based reviewer demonstrates comparable performance to the original study.</p>
<p>Further we compare the review score prediction model with the Sakana reviewer (Lu et al., 2024).In Figure 4, the relation between mean review score and predicted review scores are visualized.To compare against human reviewing performance, we exclude for each submission that has at least three reviews one review, and plot the relation between the single review score and the mean of the other review scores.The review score prediction model has a mean Pearson correlation over five random seeds of 0.330 ± 0.030 which is higher than the correlation of the Sakana reviewer of 0.161, but lower than the human reviewers correlation of 0.412 ± 0.044.Since results depend on which reviewer is sampled as a single reviewer, the human reviewer baseline is ran over five random seeds as well.Additionally, we can repeat the human reviewer consistency analysis for the whole ICLR</p>
<p>Reviewer</p>
<p>Pairwise  and OpenReview dataset.The mean correlation is 0.504±0.004and 0.475±0.002respectively, again showing that human reviews show a higher consistency than LLM reviews with human reviews.</p>
<p>In Table 19 (Appendix F), we qualitatively compare a review generated by the Sakana LLM reviewer with one written by a human reviewer for a ICLR-2024 submission.The LLM review highlights only generic strengths and weaknesses, while the human review provides detailed insights into the proposed methodology and its connections to the related work.This comparison illustrates that, despite a weak correlation in assigned scores, the LLM reviewer does not seem to base the scores on a deep semantic understanding of the paper.</p>
<p>Qualitative Analysis</p>
<p>To gain deeper insights into the features used by the citation and review score prediction models, we compute approximate Shapley values (Lundberg and Lee, 2017) for models trained on the ICLR subset of the dataset.Our analysis includes a comparison between review score and citation score prediction models, as well as an evaluation of mod-els that use research hypothesis versus title and abstract as input.Visualizations of Shapley values for example inputs are provided in Appendix H. Comparing models using titles and abstracts to those based on research hypotheses shows that the former leverage result-oriented information.For example, phrases like "sub to superhuman performance" or "a speedup compared to state-of-the-art systems" consistently have high positive Shapley values.Models based on research hypotheses focus more on individual methods (e.g., NoisyNet, GAN) or topics (e.g., exploration in deep reinforcement learning, generative modeling).When comparing citation score and review score models, some phrases (e.g., "efficient exploration," "adversarial objectives") contribute oppositely to each score.</p>
<p>Conclusion</p>
<p>Our study highlights the potential of citation and review score prediction models as automatic evaluation metrics, demonstrating their alignment with human reviews and their improved performance compared to LLM-based review systems.However, human reviews are still more consistent than review score predictions with human review scores.The lack of a standardized review format across venues complicates comparisons across venues.Further, venues often only make submissions partially (NeurIPS) or not at all (ICML) available.Predicting citation scores is more viable than predicting review scores.Since content based citation prediction has not yet been deeply explored there are still questions to address related to scalability via larger datasets and the most effective target scores and input features.Moreover, excluding tables and figures potentially limits the potential of citation score prediction models (Hirako et al., 2024).</p>
<p>Limitations</p>
<p>While our score prediction models outperform LLM-based review systems, they still have limitations.Current models utilize only a small portion of the full paper's information, leaving room for improvement in incorporating full paper information and references.Limited dataset sizes may hinder current models' ability to leverage more information effectively.Furthermore, both citation count and review scores are imperfect proxies for scientific quality as they are influenced by factors such as the author's network, raising the open question of how accurately they can be predicted from paper content alone.</p>
<p>B Training Details</p>
<p>No-context model: The no-context model consists of an MLP with 256 hidden units taking the SPECTER2 embeddings of size 768 as input.</p>
<p>The training parameters are listed in Table 6.We apply dropout of 0.3 to the SPECTER2 embeddings.We conduct a grid search over learning rates [0.0001, 0.001, 0.0005, 0.00005] and dropout rates [0, 0.1, 0.2, 0.3, 0.4, 0.5], selecting the bestperforming combination based on the validation set.</p>
<p>Training is run for 100 epochs, and the checkpoint with the lowest validation loss is then evaluated on the test set.</p>
<p>Context Model:</p>
<p>The context model consists of a transformer encoder layer (Vaswani et al., 2017) with one head, ReLU activation (Nair and Hinton, 2010), dropout of 0.3 and hidden units of size 1024.The paper representation and context is concatenated and processed via the encoder layer.The processed paper representation embedding is then passed to the same MLP as in the no-context model.Hyperparameter search and training are done in the same way as for the no-context model.</p>
<p>C OpenReview Dataset</p>
<p>In the following, we outline the collection process for the OpenReview dataset and present key dataset statistics.The dataset comprises submissions from OpenReview12 for which reviews and decisions are publicly accessible.The extent of accessibility varies by venue.For instance, ICML does not provide reviews or decisions, NeurIPS includes reviews only for accepted papers, and ICLR offers full access to all submissions.As illustrated in Figure 8, a significant proportion of the dataset originates from ICLR, followed by NeurIPS.For each venue, we manually map the review fields to the proposed review schema (see Table 9) and extract both the decision and the decision text.The PDF document associated with each submission is downloaded and parsed using GROBID 13 .The parsed sections are then classified into categories from the paper schema (see Figure 5) using our section classifier (see Appendix D).Additionally, we annotate each submission with a research hypothesis (see Appendix E).For accepted submissions, we retrieve the citation count and influential citation count of the corresponding published papers from Semantic Scholar14 , along with their references.For rejected submissions, references are extracted from the parsed GROBID output and matched to entries in the Semantic Scholar corpus to obtain additional information.A summary of the dataset statistics is provided in Table 10.OpenReview continues to grow in popularity, and we plan to update the dataset regularly to provide a growing dataset of papers with corresponding reviews in a unified format.</p>
<p>Review attribute</p>
<p>D Section Classification</p>
<p>Academic papers typically follow a standardized structure.To enable flexible use of different sections of a paper as input for our prediction models, we aim to map each of them to one of the section types outlined in the Paper Data Model (Figure 5).This requires training a section classifier.</p>
<p>To construct the necessary dataset, we first define synonyms for section types commonly found as headings in academic papers (see Table 12).We then process the OpenReview dataset and ACL-OCL dataset, identifying sections where the heading matches one of the predefined synonyms.These sections are subsequently added to the section classification dataset along with their corresponding labels15 .An overview of the frequency of the different section types in the two datasets can be found in Table 13.The section classifier takes a paragraph of text as input and assigns one of the five section types from the paper data model to it.First, the paragraph is split into sentences with NLTK's sentence tokenizer (Bird, 2006)  obtained via Specter2 by averaging over all token embeddings for the sentence.The sequence of sentence embeddings are then processed via two transformer encoder layers with dropout 0.3, hidden dimension of 1024 and eight heads.The processed embeddings are then averaged and passed through a linear layer to obtain logits.The model is trained by minimizing the cross entropy loss.The dataset is split into a training, validation and test set where the training set makes up 70% of the dataset and the validation and test set 15% each.In Table 11 one can find relevant training parameters.The accuracy over 4 random seeds of the classifier on the test set is 0.921 ± 0.006 for the ACL-OCL dataset and 0.93 ± 0.1 for the OpenReview dataset.</p>
<p>E Research Hypothesis Annotation</p>
<p>To understand whether it is possible to predict citation and review scores based on the research hypothesis of a paper alone, we annotate the papers in the ACL-OCL and OpenReview dataset using gpt-3.5-turbo.Following Baek et al. (2024)  as part of the research hypothesis.The one-shot prompt used to query the LLM can be seen in Table 14.No more advanced LLM is used for annotation to restrict costs, as the context windows are large due to the content of the scientific papers.The number of annotation examples is restricted to 1 to reduce the context size, as the context window of gpt-3.5-turbo is restricted to 16000 tokens.</p>
<p>To verify that our research hypothesis annotation tool does indeed capture the main ideas of the papers, we asked first authors of academic papers to rate the quality of the annotated research hypothesis.The annotation quality is rated on the dimensions of correctness, precision and completeness for the problem as well as the solution.The 5 point Likert scales for each dimension is displayed in Table 15.The figure 9 shows the Google Form used to conduct the survey.The survey was filled out by 13 first authors (PhD students and assistant professors) rating a total of 32 research hypotheses.The results are displayed in Table 16.The results show that the correctness of the annotated research hypotheses is overall satisfactory, however the completeness can be improved.In Table 17, some examples of the research hypothesis and improved versions written by first authors can be found.It is important to note that a potential source of error comes from mistakes in GROBID PDF parsing step that is used to extract the paper text used to condition the paper text.You are a PhD student tasked to annotate research papers with the hypothesis they investigate.You will be provided with infos about the paper.Your task is to extract the research hypothesis from this provided text.Requirements:</p>
<p>-Clarity: Ensure the hypothesis is clearly stated and understandable without additional context.</p>
<p>-Completeness: The hypothesis should be self-contained, including all necessary components such as the variables involved and the expected relationship or outcome.</p>
<p>-Terminology: Use precise and field-specific terminology that a research scientist in the relevant or adjacent field would understand.</p>
<p>-Conciseness: Keep the hypothesis one to two sentences long, avoiding unnecessary details or jargon.</p>
<p>Examples: Paper 1: {example_paper_text} Hypothesis: {example_hypothesis} Format:</p>
<p>-Problem: The problem that the paper is addressing -Solution: The solution that the paper is proposing User Message Annotate the following paper with its hypothesis: {paper_text} The solution statement has significant errors or misconceptions about how the problem is solved, but some aspects of the approach are correct.3: The solution statement is mostly correct but contains a few incorrect or misleading details about the approach or method used to solve the problem.4: The solution statement is almost entirely correct, with only minor inaccuracies or misinterpretations regarding the proposed solution.5: The solution statement is entirely accurate, with no errors or misrepresentations regarding the approach, methods, or steps taken to solve the problem.</p>
<p>S. Precision 1: The solution statement is vague and lacks specificity, making it difficult to understand the proposed approach or how it addresses the problem.2: The solution statement is somewhat clear but lacks key specifics, leaving room for misinterpretation about how the solution addresses the problem.3: The solution statement is fairly clear but could benefit from more specificity regarding key methods, steps, or conditions that explain how the problem is being solved.4: The solution statement is clear and specific, but there are a few areas where it could be refined for greater precision or clarity regarding the approach.5: The solution statement is very specific and leaves no room for ambiguity, precisely defining the method, scope, and key elements of the proposed solution.</p>
<p>Continued on next page... 1: The solution statement is vague, missing core details, and does not explain how the problem is addressed or resolved.2: The solution statement captures some aspects of the approach but omits critical elements such as the specific methods, techniques, or constraints.3: The solution statement explains the general approach but lacks clarity on important technical aspects, such as specific steps, tools, or techniques needed to solve the problem.4: The solution statement is thorough and covers most key aspects of the approach, including methods, scope, and constraints, with only minor details missing.5: The solution statement is fully articulated, covering all relevant aspects, including the method, tools, constraints, and implications, leaving no critical information out.</p>
<p>F Additional Results</p>
<p>Here, we present additional resutls.In Table 18 one can see the correlation between citation count and review score for different topics of the venue ICLR 2023.Problem: The paper aims to investigate the impact of common ground in social dialogues on the resolution of co-reference, specifically focusing on the distinction between "inner circle" and "outer circle" references in conversations.(Precision:4, Correctness:4, Completeness:3) Solution: The hypothesis is that resolving references to well-known "inner circle" individuals is more challenging compared to lesser-known "outer circle" individuals in social dialogues, and that training models on preceding data may not effectively acquire common ground knowledge for inner circle references but can improve performance for outer circle mentions.(Precision:3, Correctness:2, Completeness:3)</p>
<p>Examples</p>
<p>First Author Correction:</p>
<p>Problem: The paper aims to investigate the impact of common ground in social dialogues on co-reference resolution, specifically focusing on the distinction between "inner circle" and "outer circle" references in conversations.Solution: The hypothesis is that resolving references to well-known "inner circle" individuals is more challenging compared to lesser-known "outer circle" individuals in social dialogues, and that training models on preceding data may help in acquiring common ground knowledge for inner circle references.To test this, a data set of social dialogue is analysed on referring expressions for 'inner circle' and 'outer circle', and a co-reference resolution model is trained on preceding data and its performance analysed.</p>
<p>G Topic Model</p>
<p>Citation prediction has been shown to benefit from classifiers trained for specific research fields (Zhang and Wu, 2024) instead of a single classifier for all data.While the ACL-OCL dataset is already restricted to the field of Computational Linguistics, the OpenReview dataset covers a broader set of topics.Submissions often include a Field of Study attribute.However, the Field of Study attribute varies widely, containing broad categories like "Computer Science" to more narrow research fields (see Table 21).An overview of the distribution of values for the field is presented in Figure 10.Since the ACL-OCL dataset already focuses on a narrow domain ("Computational Linguistics") we do not perform additional topic classification.To test whether topic labels can improve comparison accuracy of pairwise score prediction models, we label all ICLR submissions with a topic label.</p>
<p>Various classification systems for scientific papers exist (Zhang and Wu, 2024), and different models have been explored (Mendoza et al., 2022).Scholarly databases like Semantic Scholar and ArXiv often provide coarse-grained classifications.For example, almost all OpenReview submissions are labeled as Computer Science by Semantic Scholar, while ArXiv categorizes them as cs.AI (Artificial Intelligence).Other systems, such as Thomson Reuters' Web of Science and CSRankings 16 , face similar limitations.Therefore, we adopt an unsupervised classification approach using Latent Dirichlet Allocation (LDA) (Blei et al., 2003).</p>
<p>We combine titles and abstracts as a unique string, and we pre-process the obtained texts using a common pipeline employing the gensim ( Řehůřek and Sojka, 2010) and nltk (Bird, 2006) libraries: we tokenize the texts and lemmatize the results.In addition, we also generate bigrams and trigrams, as they are widely used in scientific style.For the number of topics we choose the same number as in ICLR-2023 which is 13.</p>
<p>H Qualitative Analysis</p>
<p>For the qualitative analysis we approximate Shapley values by determining Owen values (Owen, 1977)   using title and abstracts (see Figure 11) and hypotheses (see Figure 12) as well as the citation prediction models using title and abstracts (see Figure 13) and hypotheses (see Figure 14).</p>
<p>Figure 1 :
1
Figure 1: Architecture of the context model in case of the full paper representation.The paper representation is green (TA=title abstract) and the context representation is blue (RW=Related Work, M=Methodology, E&amp;R=Experiments and Results, C=Conclusion).</p>
<p>Figure 2 :
2
Figure 2: Pearson correlation heat map for the different dimensions of our unified review data model.</p>
<p>Figure 3 :
3
Figure 3: Spearman correlations for context-based models applied to both the regression and pairwise comparison tasks.Results are averaged over five seeds, with error bars representing the standard deviations.</p>
<p>Figure 4 :
4
Figure4: Scatter plots of predicted review scores and groundtruth review scores on a subset of the test set of ICLR-2024 for the Sakana reviewer, the review score prediction model and human reviews.For human reviews, we randomly select a review as the predicted score and average over the rest.</p>
<p>Figure 5 :
5
Figure 5: Schematic overview of the scientific Paper object.The Field of Study is a list of keywords that are part of the OpenReview submission, where the potential values depend on the venue.Number of citations and influential citations are retrieved from Semantic Scholar.</p>
<p>Figure 9 :
9
Figure 9: Example of the Google Form used to collect the survey data.Parts of the form is blacked out to guarantee annonymity.</p>
<p>TextsExample 1 LLM (gpt-3.5.-turbo):Problem:The paper aims to investigate how conventions develop in taskbased interactions, specifically focusing on how common ground influences the formation of conventions in the presence of new information, such as outer circle characters.(Precision:4, Correctness:5, Completeness:4) Solution: The paper proposes the SPOTTER framework, a gameplay framework for task-based interaction, to study how referring expressions to known 'inner circle' (InC) referents and unknown 'outer circle' (OutC) referents evolve over time, aiming to understand how conventions arise and how they are influenced by changing contexts.(Precision:5, Correctness:5, Completeness:5)First Author Correction:Problem: The paper aims to investigate how conventions develop in taskbased interactions in Human-Robot Interaction, specifically focusing on how common ground influences the formation of conventions in the presence of new information, such as outer circle characters.Solution: The paper proposes the SPOTTER framework, a gameplay framework for task-based interaction, to study how referring expressions to known 'inner circle' (InC) referents and unknown 'outer circle' (OutC) referents evolve over time, aiming to understand how conventions arise and how they are influenced by changing contexts.Example 2 LLM (gpt-3.5.-turbo):</p>
<p>Figure 10 :
10
Figure 10: Frequency of the 20 most frequent field of studies in OpenReview.</p>
<p>Figure 11 :
11
Figure 11: Illustrative Shapley values for titles and abstracts in the review score prediction model trained on the ICLR subset of OpenReview.</p>
<p>Figure 12 :
12
Figure 12: Illustrative Shapley values for research hypotheses in the review score prediction model trained on the ICLR subset of OpenReview.</p>
<p>Figure 13 :
13
Figure 13: Illustrative Shapley values for titles and abstracts in the citation score prediction model trained on the ICLR subset of OpenReview.</p>
<p>Figure 14 :
14
Figure 14: Illustrative Shapley values for research hypotheses in the citation score prediction model trained on the ICLR subset of OpenReview.</p>
<p>in Appendix F).
Dataset# SamplesρAll150020.193All-ICLR49200.148ICLR -202315070.168ICLR -202210720.175ICLR -20218370.163ICLR -20206740.120ICLR -20194950.190ICLR -20183350.184NeurIPS -202329630.103NeurIPS -202225530.085NeurIPS -202122850.085
Table1: Pearson correlation between the log average citation per month and mean overall review scores for different subsets of the OpenReview dataset for accepted papers with at least one citation.</p>
<p>Table 2 .
2
Both learning to rank and learning to predict exact cita-
ContextPairwise Comparison Accuracy ρ sRegression L1-Distance ρ sTitle + Abstract0.665(0.010) 0.481(0.026) 0.921(0.001) 0.498(0.002)Hypothesis0.615(0.008)0.339(0.024)1.003(0.002)0.366(0.001)Introduction0.655(0.006)0.460(0.016)0.943(0.002)0.452(0.002)Related Work0.631(0.008)0.386(0.022)0.955(0.001)0.393(0.002)Methodology0.634(0.010)0.397(0.027)0.981(0.001)0.399(0.002)Experiments &amp; Results 0.651(0.004)0.446(0.012)0.946(0.001)0.449(0.001)Conclusion0.647(0.005)0.439(0.014)0.944(0.001)0.431(0.002)</p>
<p>Table 2 :
2
Performance of the citation prediction models for the pairwise comparisons and the prediction of the log average number of citation per month.All models are trained over five random seeds and mean results are presented with standard deviation in brackets.The best performing model is displayed in bold.</p>
<p>Table 4
4, it is</p>
<p>Table 3 :
3
Prediction results on the OpenReview dataset for the different scores and subsets that the dataset contains.Models are run over five random seeds and the mean results are presented with standard deviation in brackets (TA=Title and Abstract, H=Hypothesis, CC=Citation Count).The best performing model based on the Spearman rank correlation for each subset of the dataset is indicated in bold.</p>
<h1>SamplesPairwise ComparisonAccuracyρ s50680.580(0.002) 0.236(0.006)55420.576(0.003) 0.224(0.008)16240.599(0.003) 0.281(0.006)9620.565(0.006) 0.182(0.016)7070.597(0.002) 0.285(0.005)</h1>
<p>Table 4 :
4
Pairwise review score comparison accuracy for different topic subsets of the ICLR dataset, with topic labels for individual submissions generated using LDA.</p>
<p>Table 5 :
5
Comparison accuracy and spearman correlation (ρ</p>
<p>s ) for LLM-based and review-score comparison models on subsets of ICLR and NeurIPS test sets.Review score predictions are averaged over 5 random seeds, with results shown as mean (standard deviation).</p>
<p>Table 6 :
6
Training parameters for the no context models for both citation and review score prediction on the OpenReview and ACL-OCL dataset.
ReferenceTitleAbstractArXiv IDSemantic Scholar Corpus IDIntentIsInfluentialFigure 6: Schematic overview of the Reference object.The Intent indicates the section of the scientific paperwhere the reference appears (e.g., introduction, method-ology), while isInfluential is a boolean value specifyingwhether the reference played a significant role in thecreation of the paper. Both information come from Se-mantic Scholar.ReviewTextReviewScoreConfidenceNoveltyCorrectnessClarityImpactRepoducibilityEthicsFigure 7: Schematic overview of the review object. TheTextReview component concatenates all textual elementsof the review, such as the summary, strengths, weak-nesses, and questions. Except for the Ethics text field,all other attributes of the review are represented as nor-malized floats ranging from 0 to 1.ParameterValueLearning rate0.00005Dropout0.3Epochs100Batch Size256OptimizerAdamHardwareNVIDIA GeForce RTX 3080 (10GB)Training Time max. 20min</p>
<p>Table 7 :
7
Training parameters for the context model.</p>
<p>Table 8 :
8
Review fields for the ICLR-2023 venue.
FieldRecommendationConfidenceCorrectnessEmpirical Novelty and SignificanceTechnical Novelty and SignificanceFlag For Ethics Review:Summary Of The Review:Clarity, Quality, Novelty And ReproducibilityStrength And WeaknessesSummary Of The Paper</p>
<p>Table 10 :
10
Overview of the types of submissions (Sub.) and reviews (Rev.) and the availability of additional metadata.
OpenReview review fieldsoverall ratingratingevaluationQ6 Overall scoreOverall scorerecommended decisionOverall Scorescoreoverall evaluation review ratingresultsscorepreliminary ratingrecommendationworkshop ratingcustom ratingoverall evaluationexperience assessmentReviewer expertiseconfidencereview assessment: thoroughness in paper readingconfidencereviewer's confidence reviewer expertiseConfidenceQ8 Confidence in your scorereview confidenceworkshop confidencetechnical novelty and significanceoriginalitynoveltyempirical novelty and significancenoveltyQ2(1) Originality/Noveltycorrectnesssoundnessreview assessment: checking correctness of experimentsQ2(3) Correctness/Technical qualityreview assessment: checking correctness of derivations and theorycorrectnesstechnical rigorQ2(4) Quality of experiments (Optional)technical quality and correctness ratingscholarshiptechnical qualitylitreviewpresentationclarityclarityclarity of presentationQ2(6) Clarity of writingclarity ratingContinued on next page...</p>
<p>. Sentence embeddings are
Number of submissions0 5000 10000 15000 20000ICLR NeurIPS ICML acmmm auai ACM MIDL IEEE colmweb robot-learning AKBC NoDaLiDa icaps-conference thecvf MICCAI KDD AAAI aclweb EMNLP Interspeech CPAL NLDL automl iscaconf WBIR Conference humanrobotinteraction roboticsfoundation ECMLPKDD swsa SEMANTiCS MLSysFigure 8: Number of submissions per conferenceParameterValueLearning Rate 0.0001Batch Size128# Epochs20HardwareNVIDIA GeForce RTX 3080 (10GB)Training Time 25min</p>
<p>Table 11 :
11
Training parameters for the section classifier training.</p>
<p>Table 12 :
12
we model a research hypothesis h = [p, s] as a problem and a methodology/solution to solve that problem represented via natural language.Unlike Baek et al.Introduction Introduction Section synonyms used to collect a training dataset for the section classifier (E&amp;R = Experiments and Results).
BackgroundBackgroundRelated WorkHistorical ReviewMethodologyMethodologyMethod AlgorithmPropertiesExperimentsResultsExperiments and ResultsE&amp;RExperimental Design Empirical EvaluationExperiments and AnalysisAblation StudiesEvaluationConclusionConclusion &amp; DiscussionDiscussion and ConclusionsConclusionConclusion and OutlookFurther WorkDiscussions andFuture DirectionsSections# OpenReview # ACL-OCLIntroduction3948756223Background2052123412Methodology66564302E&amp;R1880128157Conclusion2281128157</p>
<p>Table 13 :
13
Number of samples in the section classification dataset resulting from matching section headings of the GROBID PDF parses to the section type synonyms for the OpenReview and ACL-OCL dataset (E&amp;R=Experiments and Results).</p>
<p>Table 14 :
14
Few-shot prompt template for research hypothesis annotation.The problem statement is fundamentally incorrect or misrepresents the actual problem or context.2:Theproblem statement has significant errors or misconceptions about the nature of the problem but some aspects are correct 3: The problem statement is mostly correct but contains a few incorrect or misleading details about the nature or scope of the problem.4:Theproblem statement is almost entirely correct, with only minor inaccuracies or misinterpretations.5:Theproblem statement is entirely accurate, with no errors or misrepresentations regarding the nature or scope of the problem.PS Precision 1: The problem statement is vague and lacks specificity, making it difficult to understand the exact nature of the problem.2:Theproblem statement is somewhat clear but lacks key specifics, leaving room for misinterpretation.3:Theproblem statement is fairly clear but could benefit from more specificity regarding key variables, constraints, or conditions.4:The problem statement is clear and specific, but there are a few areas where it could be refined for greater precision.5:The problem statement is very specific and leaves no room for ambiguity, precisely defining the scope, constraints, and core elements of the problem.Continued on next page...The problem statement is vague, missing core details, and does not define the central challenge or its scope.2: The problem statement captures some aspects of the challenge but omits critical elements such as specific requirements, constraints, or the broader
Dimension5-point Likert scalePS Correct.1:</p>
<p>Table 15 :
15
The 5-point Likert scales used to evaluate the quality of each component of the annotated research hypothesis (PS = Problem Statement, S.=Solution, Compl.=Completeness,Correct.=Correctness).
CriteriaRatingProblem Correctness4.125(1.083)Problem Precision3.688(1.158)Problem Completeness 3.563(1.144)Solution Correctness4.125(1.293)Solution Precision3.906(1.042)Solution Completeness 3.625(1.192)</p>
<p>Table 16 :
16
Average ratings of the annotated research hypotheses on the dimensions of correctness, precision, and completeness, measured using a 5-point Likert scale.Standard deviations are provided in parentheses.</p>
<p>Table 17 :
17
Examples of annotated research hypotheses, their ratings and the corrected versions by the corresponding first authors.The highlighted text in red describes the parts that is changed by the first author
Field of Study# SamplesρAll15070.168Applications1860.231Deep Learning and representational learning3420.236General Machine Learning570.190Generative models630.210ML for Sciences59-0.135Reinforcement Learning1380.138Social Aspects of ML930.198Theory930.125Unsupervised and Self-supervised learning71-0.009</p>
<p>Table 18 :
18
Pearson correlation between the log average citation per month and the mean overall review score for accepted papers with at least one citation for the ICLR-2023 venue for the different fields of study.Fields of study with less than 50 entries are left out.
Review FieldSakana ReviewHuman ReviewSummaryThe paper introduces a FairThis paper addresses human-related bias in text-to-Mapping method to mitigateimage diffusion models, and resolves the issue bybias in text-guided diffusionproposing a novel fair mapping module which out-models, focusing on generat-puts a fair text embedding. Such a module can being human-related images. Thetrained on top of frozen pre-trained text encoder, andmethod is model-agnostic andinserting the module during sampling successfullylightweight, using a linear map-mitigates textual bias. Training the fair module in-ping network to address bi-volves two loss terms: (i) text consistency loss, whichases. A novel fairness evalu-preserves semantic coherence, and (ii) fair distanceation metric is proposed, andpenalty, which brings output embeddings within dif-experiments demonstrate theferent sensitive groups close together. Further, themethod's effectiveness.authors propose a novel evaluation metric, FairScore,which also plans to achieve the conditional indepen-dence of the text prompt and sensitive group infor-mation.WeaknessesModerate originality as it buildsAlthough the paper covers a good amount of rele-on existing concepts.vant previous studies, the paper lacks baseline ex-Experimental evaluation is lim-periments. For example, despite [1] focus on fair-ited and lacks comprehensiveguidance while this work focus on pluggable map-comparison with state-of-the-ping module, the authors can calculate FairScore andart methods.compare w.r.t. training time, overhead memory, etc.Insufficient analysis of compu-While the unfairness is largely resolved through thetational complexity and scala-proposed mapping module, such a result may notbility.come at a surprise since FairScore and the employedCertain sections, particularlyfairness loss term are quite similar.the methodology, could beThe authors note that a detector network is employedclearer.to identify predefined sensitive keywords in the inputprompts. There is no additional detailed explanationabout the detector network.This method explicitly needs a labeled dataset tomitigate the demographic bias in diffusion models.However, in real-world scenarios, it may be chal-lenging to identify and address all potential typesof bias comprehensively. Further, there are remain-ing questions regarding whether it is feasible to (i)simultaneously eliminate multiple types of bias or(ii) sequentially address multiple biases without neg-atively impacting performance. If such challengescannot be properly addressed, it would incur a sig-nificant amount of training time to erase all types ofbiases, and heavy memory cost to save all mappingmodules corresponding to each bias type.Continued on next page...</p>
<p>Table 19 :
19
Comparison of human review and review produced by Sakana on ICLR-2024 submission (S.=Soundness,P.=Presentation,C.=Correctness,CF.=Confidence).</p>
<p>via Partition Shap 17 on the training set.We visualise the approximate Shapley values for three examples for the review score prediction models 16 https://csrankings.org/index?all&amp;us 17 https://shap.readthedocs.io/en/latest/generated/shap.PartitionExplainer.html
co m pu te r sc ie nc e m at he m at ic s re in fo rc em en t le ar ni ng de ep le ar ni ng la rg e la ng ua ge m od el s de ep le ar ni ng an d re pr es e re pr es en ta tio n le ar ni ng gr ap h ne ur al ne tw or ks di ff us io n m od el s ge ne ra tiv e m od el s fe de ra te d le ar ni ng se lf-su pe rv is ed le ar ni ng la rg e la ng ua ge m od el tr an sf or m er ge ne ra liz at io n m ac hi ne le ar ni ng in te rp re ta bi lit y op tim iz at io n ro bu ap pl ic at io ns st ne ss (e g,</p>
<h1>Samples Words 6244 datum, distribution, model, sample, training, generalization, use, show, method, prediction 5934 adversarial, model, attack, training, robustness, learning, datum, robust, privacy, client 2753 network, neural, function, deep, show, gradient, linear, learn, use, matrix 2271 graph, model, transformer, attention, task, performance, training, node, propose, layer 1888 causal, graph, variable, fairness, algorithm, game, tree, structure, effect, decision</h1>
<p>Table 20 :
20
The top 10 most important words for the five most frequent topic resulting from the LDA performed on ICLR submissions. .control theory, learning theory, algorithmic game theory) Social Aspects of Machine Learning (eg.AI safety, fairness, privacy, interpretability, human-AI interaction, ethics) Reinforcement Learning (eg.robotics, planning, hierarchical RL,decision and control) Probabilistic Methods (eg.variational inference, causal inference, Gaussian processes) Optimization (eg.convex and non-convex optimization) Neuroscience and Cognitive Science (e.g., neural coding, brain-computer interfaces) Machine Learning for Sciences (eg. biology, physics, health sciences, social sciences, climate sustainability) Infrastructure (eg.datasets, competitions, implementations, libraries) Generative models General Machine Learning (ie.none of the above) Deep Learning and representational learning Applications (eg.speech processing, computer vision, NLP)
Venue Field of StudyUnsupervised and Self-Supervised LearningTheory (egICLR2023</p>
<p>Table 21 :
21
Research fields available for authors to self-select when categorizing their submissions for ICLR 2023.</p>
<p>https://github.com/NikeHop/automatic_ scientific_quality_metrics
https://github.com/NikeHop/OpenReviewParser
https://f1000research.com/
https://peerj.com/
https://huggingface.co/allenai/specter2_base
https://huggingface.co/datasets/nhop/ scientific-quality-score-prediction
https://openreview.net/
https://github.com/kermitt2/grobid
https://www.semanticscholar.org/
https://openreview.net/legal/terms
https://openreview.net/
https://github.com/kermitt2/grobid
https://www.semanticscholar.org/product/api
https://huggingface.co/datasets/nhop/ academic-section-classification
AcknowledgementsThis research was (partially) funded by the Hybrid Intelligence Center, a 10-year programme funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research, https://hybrid-intelligencecentre.nl.This work used the Dutch national einfrastructure with the support of the SURF Cooperative using grant no.EINF-9756.A Data ModelIn the following we present the full data model for scientific papers, reviews and references.
Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, 10.48550/ARXIV.2404.07738CoRR, abs/2404.077382024</p>
<p>Predicting the citations of scholarly paper. Xiaomei Bai, Fuli Zhang, Ivan Lee, 10.1016/J.JOI.2019.01.010J. Informetrics. 1312019</p>
<p>Peerassist: Leveraging on paper-review interactions to predict peer review decisions. Prabhat Kumar Bharti, Shashi Ranjan, Tirthankar Ghosal, 10.1007/978-3-030-91669-5_33Towards Open and Trustworthy Digital Societies -23rd International Conference on Asia-Pacific Digital Libraries, ICADL 2021, Virtual Event. Lecture Notes in Computer Science. Springer2021. December 1-3, 202113133Mayank Agrawal, and Asif Ekbal</p>
<p>NLTK: the natural language toolkit. Steven Bird, 10.3115/1225403.1225421ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Sydney, AustraliaThe Association for Computer Linguistics2006. July 2006</p>
<p>Latent dirichlet allocation. David M Blei, Andrew Y Ng, Michael I Jordan, J. Mach. Learn. Res. 32003</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, 10.1038/S41586-023-06792-0Nat. 62479922023</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, 10.1038/S42256-024-00832-8Nat. Mac. Intell. 652024</p>
<p>MARG: multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, 10.48550/ARXIV.2401.04259CoRR, abs/2401.042592024</p>
<p>Multischubert: Effective multimodal fusion for scholarly document quality prediction. Gideon Maillette De Buy Wenniger, Thomas Van Dongen, Lambert Schomaker, 10.48550/ARXIV.2308.07971CoRR, abs/2308.079712023</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202</p>
<p>Nlpeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, 10.18653/V1/2023.ACL-LONG.277Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Scaling rectified flow transformers for high-resolution image synthesis. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024OpenReview.net</p>
<p>Enhancing the examination of obstacles in an automated peer review system. Gustavo , Lúcius Fernandes, Pedro O S Vaz De Melo, 10.1007/S00799-023-00382-1Int. J. Digit. Libr. 2522024</p>
<p>Models for predicting and explaining citation count of biomedical articles. Lawrence D Fu, Constantin F Aliferis, AMIA 2008. Washington, DC, USAAMIA2008. November 8-12, 2008</p>
<p>Citebench: A benchmark for scientific citation text generation. Martin Funkquist, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych, 10.18653/V1/2023.EMNLP-MAIN.455Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, 10.18653/V1/2023.EMNLP-MAIN.398Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Realistic citation count prediction task for newly published papers. Jun Hirako, Ryohei Sasano, Koichi Takeda, 10.18653/V1/2023.FINDINGS-EACL.84Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational Linguistics2023. May 2-6, 2023</p>
<p>Cimate: Citation count prediction effectively leveraging the main text. Jun Hirako, Ryohei Sasano, Koichi Takeda, 10.48550/ARXIV.2410.04404CoRR, abs/2410.044042024</p>
<p>Predicting citation count of Bioinformatics papers within four years of publication. Alfonso Ibáñez, Pedro Larrañaga, Concha Bielza, 10.1093/BIOINFORMATICS/BTP585Bioinform. 25242009</p>
<p>A dataset on malicious paper bidding in peer review. Steven Jecmen, Minji Yoon, Vincent Conitzer, B Nihar, Fei Shah, Fang, 10.1145/3543507.3583424Proceedings of the ACM Web Conference 2023, WWW 2023. the ACM Web Conference 2023, WWW 2023Austin, TX, USAACM2023. 30 April 2023 -4 May 2023</p>
<p>Predicting citation impact of research papers using GPT and other text embeddings. Adilson VitalJr, N Filipi, Osvaldo N Silva, Diego R OliveiraJr, Amancio, 10.48550/ARXIV.2407.19942CoRR, abs/2407.199422024</p>
<p>A dataset of peer reviews (peerread): Collection, insights and NLP applications. Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard H Hovy, Roy Schwartz, 10.18653/V1/N18-1149Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018Louisiana, USAAssociation for Computational Linguistics2018. June 1-6, 20181</p>
<p>Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. Mina Lee, Percy Liang, Qian Yang, 10.1145/3491102.3502030CHI '22: CHI Conference on Human Factors in Computing Systems. New Orleans, LA, USAACM2022. 29 April 2022 -5 May 202238819</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, 10.48550/ARXIV.2408.14033CoRR, abs/2408.140332024</p>
<p>A neural citation count prediction model based on peer review text. Siqing Li, Wayne Xin Zhao, Eddy Jing Yin, Ji-Rong Wen, 10.18653/V1/D19-1497Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, 10.48550/ARXIV.2306.00622CoRR, abs/2306.006222023</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/ARXIV.2408.06292CoRR, abs/2408.062922024</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA2017. 2017. December 4-9, 2017</p>
<p>Benchmark for research theme classification of scholarly documents. E Óscar, Wojciech Mendoza, Alaa Kusa, Ronin El-Ebshihy, David Wu, Petr Pride, Drahomira Knoth, Florina Herrmannova, Gabriella Piroi, Allan Pasi, Hanbury, Proceedings of the Third Workshop on Scholarly Document Processing, SDP@COLING 2022. the Third Workshop on Scholarly Document Processing, SDP@COLING 2022Gyeongju, Republic of KoreaAssociation for Computational Linguistics2022. October 12 -17, 2022</p>
<p>Exploiting labeled and unlabeled data via transformer fine-tuning for peerreview score prediction. Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki, 10.18653/V1/2022.FINDINGS-EMNLP.164Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 2022</p>
<p>Intermediate-task transfer learning for peer review score prediction. Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop2023</p>
<p>Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th International Conference on Machine Learning (ICML-10). the 27th International Conference on Machine Learning (ICML-10)Haifa, IsraelOmnipress2010. June 21-24, 2010</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Values of games with a priori unions. Guilliermo Owen, Mathematical economics and game theory: Essays in honor of Oskar Morgenstern. Springer1977</p>
<p>Citetracked: A longitudinal dataset of peer reviews and citations. Barbara Plank, Reinard Van Dalen, Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019). the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019)Paris, France2019. July 25, 20192414CEUR Workshop Proceedings</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, 10.48550/ARXIV.2311.05965CoRR, abs/2311.059652023</p>
<p>Software Framework for Topic Modelling with Large Corpora. Radim Řehůřek, Petr Sojka, Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. the LREC 2010 Workshop on New Challenges for NLP FrameworksValletta, Malta2010ELRA</p>
<p>The ACL OCL corpus: Advancing open science in computational linguistics. Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan, 10.18653/V1/2023.EMNLP-MAIN.640Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 10.48550/ARXIV.2409.04109CoRR, abs/2409.041092024</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, 10.18653/V1/2023.EMNLP-MAIN.338Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 10.5555/2627435.2670313J. Mach. Learn. Res. 1512014</p>
<p>An analysis of tasks and datasets in peer reviewing. Moritz Staudinger, Wojciech Kusa, Florina Piroi, Allan Hanbury, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)2024</p>
<p>Schubert: Scholarly document chunks with bert-encoding boost citation count prediction. Gideon Thomas Van Dongen, Maillette De Buy, Lambert Wenniger, Schomaker, 10.18653/V1/2020.SDP-1.17Proceedings of the First Workshop on Scholarly Document Processing. the First Workshop on Scholarly Document Processing2020. November 19. 2020SDP@EMNLP 2020. Association for Computational Linguistics</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>What have we learned from openreview?. Gang Wang, Qi Peng, Yanfeng Zhang, Mingyang Zhang, 10.1007/978-3-030-85896-4_6Web and Big Data -5th International Joint Conference, APWeb-WAIM 2021. Lecture Notes in Computer Science. Guangzhou, ChinaSpringer2021a. August 23-25, 2021Proceedings, Part I</p>
<p>Prediction and application of article potential citations based on nonlinear citation-forecasting combined model. Kehan Wang, Wenxuan Shi, Junsong Bai, Xiaoping Zhao, Liying Zhang, 10.1007/S11192-021-04026-6Scientometrics. 12682021b</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/V1/2024.ACL-LONG.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, 10.18653/V1/2024.FINDINGS-ACL.804Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 2024and virtual meeting</p>
<p>Can we automate scientific reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, 10.1613/JAIR.1.12862J. Artif. Intell. Res. 752022</p>
<p>Predicting citation impact of academic papers across research areas using multiple models and early citations. Fang Zhang, Shengli Wu, 10.1007/S11192-024-05086-0Scientometrics. 12972024</p>
<p>Is LLM a reliable reviewer? A comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024. 20-25 May, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
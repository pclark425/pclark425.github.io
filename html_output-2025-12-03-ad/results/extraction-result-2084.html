<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2084 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2084</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2084</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-278789432</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16100v1.pdf" target="_blank">BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research</a></p>
                <p><strong>Paper Abstract:</strong> Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation. In this work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows. Each task includes a structured hypothesis derived from the original study's conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables. While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods. The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science. We propose BioDSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2084.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2084.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen (GPT-4o variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-shot code-generation agent that produces a single Python code block to evaluate a hypothesis and returns a final decision (True / False / Not Verifiable). Implemented and evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeGen (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model agent (code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>executable analysis code, empirical observations, and a hypothesis decision (True/False/Not Verifiable)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (operations are data-driven analyses on provided biomedical tables; not claimed to generate transformational novel scientific claims)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>single-shot LLM code generation from schema+hypothesis (pattern extrapolation and translation of prompt instructions into Python analysis code)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>automated execution of generated code on provided dataset; decision compared to ground-truth label; evidence alignment measured by LLM-as-judge against publication evidence; non-verifiable detection evaluated against curated set</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>CodeGen family executable-code baseline reported at ~76.9% executability (Figure 4; this aggregate refers to CodeGen variants). Generated analysis captures evidence modestly: evidence alignment scores typically 0.20-0.25 across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Hypothesis decision performance (reported as Type I/Type II error rates across publication types): Type I error range 0.077–0.157; Type II error range 0.137–0.168 (values come from Table 2 for CodeGen (gpt-4o) across categories). Evidence alignment ~0.20–0.25 (low).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Equivalent to reported Type I error: between 0.077 and 0.157 depending on publication type (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Equivalent to reported Type II error: between 0.137 and 0.168 depending on publication type (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>The paper reports lower sensitivity (higher Type II) than specificity overall for CodeGen; validation degrades on more complex/information-dense domains (Genomics, Integrative). No explicit continuous curve vs. novelty, but complexity/rarity of evidence correlates with worse validation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation (code output) is often executable but agent-produced observations frequently fail to align with ground-truth evidence (alignment 0.20-0.25). In non-executable cases agents disproportionately default to 'Not Verifiable' (~87%) yet still produce hallucinated True/False decisions in ~13% of non-executable cases — indicating a gap where generated claims are not reliably validated.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantitatively reported; qualitative evidence of miscalibration: occasional hallucinated True/False decisions when code is non-executable (paper notes ~13% such cases), indicating overconfidence in some failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Assessed indirectly via non-verifiable hypotheses: CodeGen (gpt-4o) true-positive rate (TPR) for 'Not Verifiable' = 63% on a 100 strictly non-verifiable-hypothesis set, indicating poor performance on out-of-distribution / missing-data claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — evidence alignment score (|O ∩ E|/|E|) and code executability rate are used as proxies for correctness and practical usability; also non-verifiable detection accuracy/TPR.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not explicitly quantified; paper recommends human-in-the-loop especially because agents can produce plausible but incorrect claims; frequency increases for non-verifiable or complex/novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical data analysis) — validation relies on empirical tests and statistics rather than formal proofs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Structured reasoning/planning (CodeGen-Reasoning) is proposed and evaluated; reasoning augmentation reduces some error rates and improves cautiousness (see CodeGen-R* results).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Low evidence-alignment scores (0.20-0.25) despite often-executable code; hallucinated True/False outcomes in non-executable traces (~13%); lower sensitivity (higher Type II) vs. false positive rates — all indicate generation can outpace faithful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Reasoning-augmented variants (CodeGen-R*) show improved Type I/II rates versus CodeGen base, indicating mitigation is possible and that generation/validation gap is not absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>null</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2084.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2084.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen (o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen (O3-mini variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller/faster variant of the CodeGen one-shot code generation pipeline evaluated in the benchmark (o3-mini).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeGen (o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model agent (code generation, smaller variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>executable analysis code, observations, and hypothesis decision</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (same operational domain as benchmark tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>single-shot LLM code generation (smaller model variant producing Python analysis code)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>automatic code execution and comparison to curated ground-truth labels; evidence alignment via LLM-as-judge; non-verifiable detection measured on curated set</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Aggregated CodeGen executability baseline ~76.9% (paper reports CodeGen family baseline; per-variant exec rate not separately tabulated).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Type I error range for CodeGen (o3-mini) across publication types: 0.098–0.157; Type II error range: 0.118–0.191 (Table 2). Evidence alignment remains modest (~0.20–0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Type I error between 0.098 and 0.157 depending on publication type.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Type II error between 0.118 and 0.191 depending on publication type.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Shows larger Type II errors in more complex categories (e.g., Integrative had Type II up to 0.191), indicating validation degrades in harder/less canonical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Similar pattern to larger CodeGen: code can be generated but alignment to publication evidence is low; tend to be conservative but still commit false verifiable claims on non-verifiable tasks (non-verifiable TPR reported at 57%).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; evidence of over-assertion on non-verifiable inputs (43% incorrect verifiable claims on strictly non-verifiable set).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Non-verifiable TPR = 57% (worse than reasoning-augmented agents), indicating poor OOD handling for missing-data hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses evidence alignment and code executability as proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not quantified; recommended more often for non-verifiable/OOD cases.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Reasoning augmentation (CodeGen-Reasoning) tested and shown to reduce some error rates relative to base CodeGen.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>High Type II and occasional false verifiable claims on non-verifiable inputs; lower non-verifiable detection TPR relative to reasoning-augmented agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Reasoning variants improve detection and reduce errors, suggesting gap is reducible.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>null</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2084.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2084.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct framework implemented with GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative reasoning-and-action agent that alternates between natural-language reasoning steps and executable code actions to refine analyses and conclusions; implemented with GPT-4o in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model agent with ReAct reasoning loop</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>multi-step reasoning text, iterative code snippets, observations, final hypothesis decision</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (iterative reasoning enables exploration beyond single-shot code templates)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>iterative LLM reasoning + action (multi-step code generation and execution guided by intermediate observations)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>execute produced code snippets iteratively, observe intermediate outputs, update plan and final decision; compare final decision to ground truth; evidence alignment measured by LLM-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>ReAct agents show higher code executability than CodeGen baselines: ReAct overall ~84.9% executability (Figure 4). Evidence alignment marginally higher than CodeGen but still modest (≈0.20–0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Lower Type II error than CodeGen in many domains. Table 2 shows ReAct (gpt-4o) Type I range ≈0.066–0.120 and Type II range ≈0.128–0.167 across publication types — i.e., improved sensitivity in several categories.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Type I error between ~0.066 and 0.120 depending on publication type.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Type II error between ~0.128 and 0.167 depending on publication type.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Iterative reasoning yields larger gains in complex/high-baseline-error domains (Genomics, Integrative), implying better handling of more novel/complex analyses compared to one-shot generators.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>ReAct's iterative generation tends to produce more executable and better-aligned observations than one-shot CodeGen; nevertheless, evidence alignment remains low overall, indicating the validation gap persists albeit reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantitatively reported; qualitative improvements in cautiousness when reasoning is used are observed but no calibration metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not explicitly reported for baseline ReAct; reasoning-augmented ReAct (ReAct-R*) shows strong non-verifiable detection (see ReAct-Reasoning entry).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — evidence alignment, executability rate, and Type I/II errors used as primary measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not quantified; recommended, especially for complex/novel findings and non-verifiable cases.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>ReAct iterative loop (reason + act) and structured planning lead to reductions in Type II errors and improved evidence capture; authors test ReAct-R* (reasoning-augmented) as mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Despite iterative reasoning, evidence alignment remains modest (0.20–0.25) and agents still miss findings more often than produce false positives (higher Type II), showing generation does not guarantee faithful reproduction of human evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>ReAct reduces Type II errors relative to CodeGen and ReAct-R* further improves metrics (shows gap can be narrowed).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>null</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2084.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2084.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen-Reasoning (CodeGen-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen with decoupled reasoning/planning (CodeGen-Reasoning / CodeGen-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage agent that first uses a smaller model (O3-mini) to produce a structured analysis plan and then uses GPT-4o to generate and execute code from that plan; designed to separate planning from implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeGen-Reasoning (CodeGen-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-stage LLM agent (planner + code generator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured analysis plans, executable code, observations, and a hypothesis decision</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (structured planning intended to improve reasoning over baseline generation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>two-stage LLM pipeline: planning by a smaller model (o3-mini) followed by code generation by GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>execute generated code and compare observations to ground truth; evidence alignment measured; non-verifiable detection tested</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported executability for CodeGen-Reasoning is lower relative to other methods: CodeGen-Reasoning executability ~58.2% (Figure 4). Evidence alignment remains modest; planning sometimes reduces erroneous variable misuse but increases some other error types.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improved Type I/II rates versus base CodeGen in several categories (Table 2). Across publication types Type I error range ≈0.054–0.141 and Type II error range ≈0.110–0.156 (Table 2 CodeGen-R* rows).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Type I error approximately 0.054–0.141 depending on category.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Type II error approximately 0.110–0.156 depending on category.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Structured planning helps in harder domains (reduces some error rates) but overall evidence alignment still low; the benefit is larger where baseline errors were high.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Decoupling planning improves decision metrics compared to one-shot CodeGen but reduces code executability substantially (58.2%), creating a trade-off between planning quality and executable implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported quantitatively; reasoning variant is described as more conservative in non-verifiable detection (higher TPRs generally), but exact calibration metrics are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not separately tabulated; reasoning variants generally better at non-verifiable detection than one-shot CodeGen (paper reports reasoning agents more conservative).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses executability and evidence alignment plus Type I/II errors.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not quantified; recommended especially given lower executability despite improved decision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Two-stage planning + generation (decoupling planning and implementation) is tested and reduces some Type I/II errors, but lowers executability; trade-offs analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Lower executability (58.2%) coupled with only modest alignment indicates that better planning alone does not fully close the generation-validation gap and may introduce new practical failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>CodeGen-R* yields improved Type I/II performance in many categories versus base CodeGen, indicating reasoning/planning reduces some generation-validation mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>null</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2084.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2084.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct-Reasoning (ReAct-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct with structured planning (ReAct-Reasoning / ReAct-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative ReAct agent augmented with explicit structured planning (O3-mini planner + GPT-4o executor) that performs multi-step reasoning, dynamic plan refinement, and code execution to validate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct-Reasoning (ReAct-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>iterative LLM agent with structured planning (multi-stage ReAct)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured analysis plans, iterative code snippets, observations, and final hypothesis decision</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel; targeted at improving reasoning over complex analyses</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>combines planner model (O3-mini) and iterative ReAct loop using GPT-4o to generate/refine code and observations based on intermediate outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>iterative execution of code with dynamic plan updates; final decision compared to curated ground truth; evidence alignment scored by LLM-as-judge; non-verifiable detection explicitly measured</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Highest reported executability among tested agents: ReAct-Reasoning executability = 86.6% (Figure 4). Evidence alignment marginally higher than other methods but still low overall (~0.20–0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Best overall hypothesis validation metrics in many categories (Table 2). Example ranges across categories: Type I error ≈0.051–0.098; Type II error ≈0.094–0.167. ReAct-R* often attains lowest error in columns reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Type I error between ≈0.051 and 0.098 depending on publication type (Table 2 ReAct-R* rows).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Type II error between ≈0.094 and 0.167 depending on publication type.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>ReAct-R* yields the largest relative improvements in domains with high baseline error (Genomics, Integrative), indicating structured iterative reasoning most helps on complex/novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>ReAct-R* narrows the generation-validation gap: higher executability (86.6%) combined with reduced Type I/II errors and strong non-verifiable detection (TPR = 92%) shows closer alignment between generated outputs and validation mechanisms than other variants.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported with numeric calibration metrics; qualitative evidence indicates better cautiousness (higher TPR for Not Verifiable) and fewer hallucinated verifiable claims.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>On strictly non-verifiable hypothesis set (N=100), ReAct-Reasoning achieves TPR = 92% for 'Not Verifiable' class, indicating substantially better OOD/missing-data behavior than one-shot CodeGen agents.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses evidence alignment, code executability rate, Type I/II error rates, and non-verifiable detection accuracy/TPR.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not quantified; paper emphasizes human oversight is critical and more necessary for novel/out-of-distribution findings but ReAct-R* reduces human burden by improving automatic detection of non-verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Structured planning + iterative ReAct loop; decoupling planning and execution (planner O3-mini + GPT-4o executor). This combination gives best trade-off of executability and validation accuracy in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Even ReAct-R* has modest evidence-alignment (0.20–0.25), so while the generation-validation gap shrinks, some divergence between agent outputs and original publication evidence remains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High executability (86.6%), lower Type I/II errors, and very high non-verifiable TPR (92%) show that appropriate agent design (planning+iterative reasoning) can substantially close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>null</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2084.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2084.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (used for extraction/execution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as LLM backbone for extraction and some agent backends)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used in this work both to extract hypotheses/evidence from publications and as the backend for some agent variants (e.g., CodeGen (gpt-4o), ReAct (gpt-4o)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general-purpose natural language and code generation applied to biomedical data science</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>hypothesis/extraction text, analysis plans, and code</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel depending on prompt/task</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>transformer-based sequence generation conditioned on prompts and dataset captions/schema</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as part of agent pipelines where validation is via code execution and comparison to curated ground truth; GPT-4o itself not used for independent formal validation in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used successfully to extract 1,029 hypotheses and 1,177 analysis plans from 329 publications (paper reports this scale); individual performance metrics of GPT-4o extraction (precision/recall) are not numerically reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not applicable as a standalone validator in reported experiments; validation was operationalized by executing code generated by agents and measuring Type I/II/errors/alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified for extraction tasks; authors used zero-temperature batched calls to encourage deterministic generations for extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>GPT-4o is a generation backbone; the paper shows that generation capability (e.g., producing plausible analysis plans/code) does not guarantee faithful validation (evidence alignment low) when used within agent pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported numerically for extraction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Extraction quality judged by downstream executability and alignment metrics, but explicit extraction-accuracy metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not quantified; human curation used in pipeline design and authors recommend human oversight in deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Use of smaller planner models (o3-mini) for structured plans and integration of ReAct iterative loops when GPT-4o is executor; batching/zero-temperature for extraction to increase determinism.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>The paper's overall results show LLMs (including GPT-4o variants) can produce plausible code/claims but evidence alignment and non-verifiable detection can be imperfect, demonstrating generation > validation in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>When GPT-4o is used within reasoning-augmented ReAct-R*, validation metrics improve substantially (higher executability, lower Type I/II), indicating careful design can reduce the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>null</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards an ai co-scientist <em>(Rating: 2)</em></li>
                <li>Automated hypothesis validation with agentic sequential falsifications <em>(Rating: 2)</em></li>
                <li>BioDiscoveryAgent: An ai agent for designing genetic perturbation experiments <em>(Rating: 2)</em></li>
                <li>Discoverybench: Towards data-driven discovery with large language models <em>(Rating: 1)</em></li>
                <li>Co-Scientist <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2084",
    "paper_id": "paper-278789432",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "CodeGen (gpt-4o)",
            "name_full": "CodeGen (GPT-4o variant)",
            "brief_description": "A one-shot code-generation agent that produces a single Python code block to evaluate a hypothesis and returns a final decision (True / False / Not Verifiable). Implemented and evaluated in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeGen (gpt-4o)",
            "system_type": "large language model agent (code generation)",
            "scientific_domain": "biomedical data science / hypothesis validation",
            "output_type": "executable analysis code, empirical observations, and a hypothesis decision (True/False/Not Verifiable)",
            "novelty_level": "in-distribution to moderately novel (operations are data-driven analyses on provided biomedical tables; not claimed to generate transformational novel scientific claims)",
            "generation_method": "single-shot LLM code generation from schema+hypothesis (pattern extrapolation and translation of prompt instructions into Python analysis code)",
            "validation_method": "automated execution of generated code on provided dataset; decision compared to ground-truth label; evidence alignment measured by LLM-as-judge against publication evidence; non-verifiable detection evaluated against curated set",
            "generation_performance": "CodeGen family executable-code baseline reported at ~76.9% executability (Figure 4; this aggregate refers to CodeGen variants). Generated analysis captures evidence modestly: evidence alignment scores typically 0.20-0.25 across methods.",
            "validation_performance": "Hypothesis decision performance (reported as Type I/Type II error rates across publication types): Type I error range 0.077–0.157; Type II error range 0.137–0.168 (values come from Table 2 for CodeGen (gpt-4o) across categories). Evidence alignment ~0.20–0.25 (low).",
            "false_positive_rate": "Equivalent to reported Type I error: between 0.077 and 0.157 depending on publication type (Table 2).",
            "false_negative_rate": "Equivalent to reported Type II error: between 0.137 and 0.168 depending on publication type (Table 2).",
            "performance_vs_novelty": "The paper reports lower sensitivity (higher Type II) than specificity overall for CodeGen; validation degrades on more complex/information-dense domains (Genomics, Integrative). No explicit continuous curve vs. novelty, but complexity/rarity of evidence correlates with worse validation.",
            "generation_validation_comparison": "Generation (code output) is often executable but agent-produced observations frequently fail to align with ground-truth evidence (alignment 0.20-0.25). In non-executable cases agents disproportionately default to 'Not Verifiable' (~87%) yet still produce hallucinated True/False decisions in ~13% of non-executable cases — indicating a gap where generated claims are not reliably validated.",
            "uncertainty_quantification": "null",
            "calibration_quality": "Not quantitatively reported; qualitative evidence of miscalibration: occasional hallucinated True/False decisions when code is non-executable (paper notes ~13% such cases), indicating overconfidence in some failure modes.",
            "out_of_distribution_performance": "Assessed indirectly via non-verifiable hypotheses: CodeGen (gpt-4o) true-positive rate (TPR) for 'Not Verifiable' = 63% on a 100 strictly non-verifiable-hypothesis set, indicating poor performance on out-of-distribution / missing-data claims.",
            "validation_proxy_metrics": "Yes — evidence alignment score (|O ∩ E|/|E|) and code executability rate are used as proxies for correctness and practical usability; also non-verifiable detection accuracy/TPR.",
            "human_validation_required": true,
            "human_validation_frequency": "Not explicitly quantified; paper recommends human-in-the-loop especially because agents can produce plausible but incorrect claims; frequency increases for non-verifiable or complex/novel outputs.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical data analysis) — validation relies on empirical tests and statistics rather than formal proofs",
            "gap_mitigation_strategies": "Structured reasoning/planning (CodeGen-Reasoning) is proposed and evaluated; reasoning augmentation reduces some error rates and improves cautiousness (see CodeGen-R* results).",
            "evidence_supporting_gap": "Low evidence-alignment scores (0.20-0.25) despite often-executable code; hallucinated True/False outcomes in non-executable traces (~13%); lower sensitivity (higher Type II) vs. false positive rates — all indicate generation can outpace faithful validation.",
            "evidence_contradicting_gap": "Reasoning-augmented variants (CodeGen-R*) show improved Type I/II rates versus CodeGen base, indicating mitigation is possible and that generation/validation gap is not absolute.",
            "computational_cost_ratio": "null",
            "uuid": "e2084.0"
        },
        {
            "name_short": "CodeGen (o3-mini)",
            "name_full": "CodeGen (O3-mini variant)",
            "brief_description": "A smaller/faster variant of the CodeGen one-shot code generation pipeline evaluated in the benchmark (o3-mini).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeGen (o3-mini)",
            "system_type": "large language model agent (code generation, smaller variant)",
            "scientific_domain": "biomedical data science / hypothesis validation",
            "output_type": "executable analysis code, observations, and hypothesis decision",
            "novelty_level": "in-distribution to moderately novel (same operational domain as benchmark tasks)",
            "generation_method": "single-shot LLM code generation (smaller model variant producing Python analysis code)",
            "validation_method": "automatic code execution and comparison to curated ground-truth labels; evidence alignment via LLM-as-judge; non-verifiable detection measured on curated set",
            "generation_performance": "Aggregated CodeGen executability baseline ~76.9% (paper reports CodeGen family baseline; per-variant exec rate not separately tabulated).",
            "validation_performance": "Type I error range for CodeGen (o3-mini) across publication types: 0.098–0.157; Type II error range: 0.118–0.191 (Table 2). Evidence alignment remains modest (~0.20–0.25).",
            "false_positive_rate": "Type I error between 0.098 and 0.157 depending on publication type.",
            "false_negative_rate": "Type II error between 0.118 and 0.191 depending on publication type.",
            "performance_vs_novelty": "Shows larger Type II errors in more complex categories (e.g., Integrative had Type II up to 0.191), indicating validation degrades in harder/less canonical tasks.",
            "generation_validation_comparison": "Similar pattern to larger CodeGen: code can be generated but alignment to publication evidence is low; tend to be conservative but still commit false verifiable claims on non-verifiable tasks (non-verifiable TPR reported at 57%).",
            "uncertainty_quantification": "null",
            "calibration_quality": "Not reported; evidence of over-assertion on non-verifiable inputs (43% incorrect verifiable claims on strictly non-verifiable set).",
            "out_of_distribution_performance": "Non-verifiable TPR = 57% (worse than reasoning-augmented agents), indicating poor OOD handling for missing-data hypotheses.",
            "validation_proxy_metrics": "Yes — uses evidence alignment and code executability as proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Not quantified; recommended more often for non-verifiable/OOD cases.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical)",
            "gap_mitigation_strategies": "Reasoning augmentation (CodeGen-Reasoning) tested and shown to reduce some error rates relative to base CodeGen.",
            "evidence_supporting_gap": "High Type II and occasional false verifiable claims on non-verifiable inputs; lower non-verifiable detection TPR relative to reasoning-augmented agents.",
            "evidence_contradicting_gap": "Reasoning variants improve detection and reduce errors, suggesting gap is reducible.",
            "computational_cost_ratio": "null",
            "uuid": "e2084.1"
        },
        {
            "name_short": "ReAct (gpt-4o)",
            "name_full": "ReAct framework implemented with GPT-4o",
            "brief_description": "An iterative reasoning-and-action agent that alternates between natural-language reasoning steps and executable code actions to refine analyses and conclusions; implemented with GPT-4o in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReAct (gpt-4o)",
            "system_type": "large language model agent with ReAct reasoning loop",
            "scientific_domain": "biomedical data science / hypothesis validation",
            "output_type": "multi-step reasoning text, iterative code snippets, observations, final hypothesis decision",
            "novelty_level": "in-distribution to moderately novel (iterative reasoning enables exploration beyond single-shot code templates)",
            "generation_method": "iterative LLM reasoning + action (multi-step code generation and execution guided by intermediate observations)",
            "validation_method": "execute produced code snippets iteratively, observe intermediate outputs, update plan and final decision; compare final decision to ground truth; evidence alignment measured by LLM-as-judge",
            "generation_performance": "ReAct agents show higher code executability than CodeGen baselines: ReAct overall ~84.9% executability (Figure 4). Evidence alignment marginally higher than CodeGen but still modest (≈0.20–0.25).",
            "validation_performance": "Lower Type II error than CodeGen in many domains. Table 2 shows ReAct (gpt-4o) Type I range ≈0.066–0.120 and Type II range ≈0.128–0.167 across publication types — i.e., improved sensitivity in several categories.",
            "false_positive_rate": "Type I error between ~0.066 and 0.120 depending on publication type.",
            "false_negative_rate": "Type II error between ~0.128 and 0.167 depending on publication type.",
            "performance_vs_novelty": "Iterative reasoning yields larger gains in complex/high-baseline-error domains (Genomics, Integrative), implying better handling of more novel/complex analyses compared to one-shot generators.",
            "generation_validation_comparison": "ReAct's iterative generation tends to produce more executable and better-aligned observations than one-shot CodeGen; nevertheless, evidence alignment remains low overall, indicating the validation gap persists albeit reduced.",
            "uncertainty_quantification": "null",
            "calibration_quality": "Not quantitatively reported; qualitative improvements in cautiousness when reasoning is used are observed but no calibration metrics provided.",
            "out_of_distribution_performance": "Not explicitly reported for baseline ReAct; reasoning-augmented ReAct (ReAct-R*) shows strong non-verifiable detection (see ReAct-Reasoning entry).",
            "validation_proxy_metrics": "Yes — evidence alignment, executability rate, and Type I/II errors used as primary measurements.",
            "human_validation_required": true,
            "human_validation_frequency": "Not quantified; recommended, especially for complex/novel findings and non-verifiable cases.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical)",
            "gap_mitigation_strategies": "ReAct iterative loop (reason + act) and structured planning lead to reductions in Type II errors and improved evidence capture; authors test ReAct-R* (reasoning-augmented) as mitigation.",
            "evidence_supporting_gap": "Despite iterative reasoning, evidence alignment remains modest (0.20–0.25) and agents still miss findings more often than produce false positives (higher Type II), showing generation does not guarantee faithful reproduction of human evidence.",
            "evidence_contradicting_gap": "ReAct reduces Type II errors relative to CodeGen and ReAct-R* further improves metrics (shows gap can be narrowed).",
            "computational_cost_ratio": "null",
            "uuid": "e2084.2"
        },
        {
            "name_short": "CodeGen-Reasoning (CodeGen-R*)",
            "name_full": "CodeGen with decoupled reasoning/planning (CodeGen-Reasoning / CodeGen-R*)",
            "brief_description": "A two-stage agent that first uses a smaller model (O3-mini) to produce a structured analysis plan and then uses GPT-4o to generate and execute code from that plan; designed to separate planning from implementation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeGen-Reasoning (CodeGen-R*)",
            "system_type": "multi-stage LLM agent (planner + code generator)",
            "scientific_domain": "biomedical data science / hypothesis validation",
            "output_type": "structured analysis plans, executable code, observations, and a hypothesis decision",
            "novelty_level": "in-distribution to moderately novel (structured planning intended to improve reasoning over baseline generation)",
            "generation_method": "two-stage LLM pipeline: planning by a smaller model (o3-mini) followed by code generation by GPT-4o",
            "validation_method": "execute generated code and compare observations to ground truth; evidence alignment measured; non-verifiable detection tested",
            "generation_performance": "Reported executability for CodeGen-Reasoning is lower relative to other methods: CodeGen-Reasoning executability ~58.2% (Figure 4). Evidence alignment remains modest; planning sometimes reduces erroneous variable misuse but increases some other error types.",
            "validation_performance": "Improved Type I/II rates versus base CodeGen in several categories (Table 2). Across publication types Type I error range ≈0.054–0.141 and Type II error range ≈0.110–0.156 (Table 2 CodeGen-R* rows).",
            "false_positive_rate": "Type I error approximately 0.054–0.141 depending on category.",
            "false_negative_rate": "Type II error approximately 0.110–0.156 depending on category.",
            "performance_vs_novelty": "Structured planning helps in harder domains (reduces some error rates) but overall evidence alignment still low; the benefit is larger where baseline errors were high.",
            "generation_validation_comparison": "Decoupling planning improves decision metrics compared to one-shot CodeGen but reduces code executability substantially (58.2%), creating a trade-off between planning quality and executable implementation.",
            "uncertainty_quantification": "null",
            "calibration_quality": "Not reported quantitatively; reasoning variant is described as more conservative in non-verifiable detection (higher TPRs generally), but exact calibration metrics are absent.",
            "out_of_distribution_performance": "Not separately tabulated; reasoning variants generally better at non-verifiable detection than one-shot CodeGen (paper reports reasoning agents more conservative).",
            "validation_proxy_metrics": "Yes — uses executability and evidence alignment plus Type I/II errors.",
            "human_validation_required": true,
            "human_validation_frequency": "Not quantified; recommended especially given lower executability despite improved decision metrics.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical)",
            "gap_mitigation_strategies": "Two-stage planning + generation (decoupling planning and implementation) is tested and reduces some Type I/II errors, but lowers executability; trade-offs analyzed.",
            "evidence_supporting_gap": "Lower executability (58.2%) coupled with only modest alignment indicates that better planning alone does not fully close the generation-validation gap and may introduce new practical failure modes.",
            "evidence_contradicting_gap": "CodeGen-R* yields improved Type I/II performance in many categories versus base CodeGen, indicating reasoning/planning reduces some generation-validation mismatches.",
            "computational_cost_ratio": "null",
            "uuid": "e2084.3"
        },
        {
            "name_short": "ReAct-Reasoning (ReAct-R*)",
            "name_full": "ReAct with structured planning (ReAct-Reasoning / ReAct-R*)",
            "brief_description": "An iterative ReAct agent augmented with explicit structured planning (O3-mini planner + GPT-4o executor) that performs multi-step reasoning, dynamic plan refinement, and code execution to validate hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReAct-Reasoning (ReAct-R*)",
            "system_type": "iterative LLM agent with structured planning (multi-stage ReAct)",
            "scientific_domain": "biomedical data science / hypothesis validation",
            "output_type": "structured analysis plans, iterative code snippets, observations, and final hypothesis decision",
            "novelty_level": "in-distribution to moderately novel; targeted at improving reasoning over complex analyses",
            "generation_method": "combines planner model (O3-mini) and iterative ReAct loop using GPT-4o to generate/refine code and observations based on intermediate outputs",
            "validation_method": "iterative execution of code with dynamic plan updates; final decision compared to curated ground truth; evidence alignment scored by LLM-as-judge; non-verifiable detection explicitly measured",
            "generation_performance": "Highest reported executability among tested agents: ReAct-Reasoning executability = 86.6% (Figure 4). Evidence alignment marginally higher than other methods but still low overall (~0.20–0.25).",
            "validation_performance": "Best overall hypothesis validation metrics in many categories (Table 2). Example ranges across categories: Type I error ≈0.051–0.098; Type II error ≈0.094–0.167. ReAct-R* often attains lowest error in columns reported.",
            "false_positive_rate": "Type I error between ≈0.051 and 0.098 depending on publication type (Table 2 ReAct-R* rows).",
            "false_negative_rate": "Type II error between ≈0.094 and 0.167 depending on publication type.",
            "performance_vs_novelty": "ReAct-R* yields the largest relative improvements in domains with high baseline error (Genomics, Integrative), indicating structured iterative reasoning most helps on complex/novel tasks.",
            "generation_validation_comparison": "ReAct-R* narrows the generation-validation gap: higher executability (86.6%) combined with reduced Type I/II errors and strong non-verifiable detection (TPR = 92%) shows closer alignment between generated outputs and validation mechanisms than other variants.",
            "uncertainty_quantification": "null",
            "calibration_quality": "Not reported with numeric calibration metrics; qualitative evidence indicates better cautiousness (higher TPR for Not Verifiable) and fewer hallucinated verifiable claims.",
            "out_of_distribution_performance": "On strictly non-verifiable hypothesis set (N=100), ReAct-Reasoning achieves TPR = 92% for 'Not Verifiable' class, indicating substantially better OOD/missing-data behavior than one-shot CodeGen agents.",
            "validation_proxy_metrics": "Yes — uses evidence alignment, code executability rate, Type I/II error rates, and non-verifiable detection accuracy/TPR.",
            "human_validation_required": true,
            "human_validation_frequency": "Not quantified; paper emphasizes human oversight is critical and more necessary for novel/out-of-distribution findings but ReAct-R* reduces human burden by improving automatic detection of non-verifiability.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical)",
            "gap_mitigation_strategies": "Structured planning + iterative ReAct loop; decoupling planning and execution (planner O3-mini + GPT-4o executor). This combination gives best trade-off of executability and validation accuracy in the benchmark.",
            "evidence_supporting_gap": "Even ReAct-R* has modest evidence-alignment (0.20–0.25), so while the generation-validation gap shrinks, some divergence between agent outputs and original publication evidence remains.",
            "evidence_contradicting_gap": "High executability (86.6%), lower Type I/II errors, and very high non-verifiable TPR (92%) show that appropriate agent design (planning+iterative reasoning) can substantially close the gap.",
            "computational_cost_ratio": "null",
            "uuid": "e2084.4"
        },
        {
            "name_short": "GPT-4o (used for extraction/execution)",
            "name_full": "GPT-4o (used as LLM backbone for extraction and some agent backends)",
            "brief_description": "A large language model used in this work both to extract hypotheses/evidence from publications and as the backend for some agent variants (e.g., CodeGen (gpt-4o), ReAct (gpt-4o)).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPT-4o",
            "system_type": "large language model",
            "scientific_domain": "general-purpose natural language and code generation applied to biomedical data science",
            "output_type": "hypothesis/extraction text, analysis plans, and code",
            "novelty_level": "in-distribution to moderately novel depending on prompt/task",
            "generation_method": "transformer-based sequence generation conditioned on prompts and dataset captions/schema",
            "validation_method": "Used as part of agent pipelines where validation is via code execution and comparison to curated ground truth; GPT-4o itself not used for independent formal validation in the paper.",
            "generation_performance": "Used successfully to extract 1,029 hypotheses and 1,177 analysis plans from 329 publications (paper reports this scale); individual performance metrics of GPT-4o extraction (precision/recall) are not numerically reported in the main text.",
            "validation_performance": "Not applicable as a standalone validator in reported experiments; validation was operationalized by executing code generated by agents and measuring Type I/II/errors/alignment.",
            "false_positive_rate": "null",
            "false_negative_rate": "null",
            "performance_vs_novelty": "Not quantified for extraction tasks; authors used zero-temperature batched calls to encourage deterministic generations for extraction.",
            "generation_validation_comparison": "GPT-4o is a generation backbone; the paper shows that generation capability (e.g., producing plausible analysis plans/code) does not guarantee faithful validation (evidence alignment low) when used within agent pipelines.",
            "uncertainty_quantification": "null",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not reported numerically for extraction tasks",
            "validation_proxy_metrics": "Extraction quality judged by downstream executability and alignment metrics, but explicit extraction-accuracy metrics are not provided.",
            "human_validation_required": true,
            "human_validation_frequency": "Not quantified; human curation used in pipeline design and authors recommend human oversight in deployment.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical)",
            "gap_mitigation_strategies": "Use of smaller planner models (o3-mini) for structured plans and integration of ReAct iterative loops when GPT-4o is executor; batching/zero-temperature for extraction to increase determinism.",
            "evidence_supporting_gap": "The paper's overall results show LLMs (including GPT-4o variants) can produce plausible code/claims but evidence alignment and non-verifiable detection can be imperfect, demonstrating generation &gt; validation in some settings.",
            "evidence_contradicting_gap": "When GPT-4o is used within reasoning-augmented ReAct-R*, validation metrics improve substantially (higher executability, lower Type I/II), indicating careful design can reduce the gap.",
            "computational_cost_ratio": "null",
            "uuid": "e2084.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards an ai co-scientist",
            "rating": 2
        },
        {
            "paper_title": "Automated hypothesis validation with agentic sequential falsifications",
            "rating": 2
        },
        {
            "paper_title": "BioDiscoveryAgent: An ai agent for designing genetic perturbation experiments",
            "rating": 2
        },
        {
            "paper_title": "Discoverybench: Towards data-driven discovery with large language models",
            "rating": 1
        },
        {
            "paper_title": "Co-Scientist",
            "rating": 1
        }
    ],
    "cost": 0.02081025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research
22 May 2025</p>
<p>Zifeng Wang zifengw2@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>Benjamin Danek 
University of Illinois Urbana-Champaign</p>
<p>Jimeng Sun jimeng@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research
22 May 20254CE7245871230092A6609653A6257CC0arXiv:2505.16100v1[cs.AI]
Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation.In this work, we present BIODSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks.BIODSA-1K consists of 1,029 hypothesiscentric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows.Each task includes a structured hypothesis derived from the original study's conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables.While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods.The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code.Importantly, BIODSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science.We propose BIODSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) agents promise to accelerate scientific discovery [1,2], with the emergence of "AI scientists" [3] capable of collaborating with human researchers to perform research tasks such as literature mining and data analysis [4][5][6][7].Large language models (LLMs) [8] can serve as the intelligence backbone for converting natural language to structured outputs such as code and mathematical expressions.As a core task in biomedical research, data science bridges the gap from proposed hypotheses to novel discoveries leveraging biomedical data.For example, a researcher might hypothesize that "Genes involved in histone modification " are frequently mutated in non-Hodgkin lymphoma."Testing such hypotheses often requires close collaboration between biomedical experts and data scientists to design analyses, write code, and interpret results, and thus far has been mainly manual efforts in practice [9].</p>
<p>Recent efforts have demonstrated LLM-based agents capable of designing experiments, generating code, and summarizing results [10][11][12][13].However, existing systems often focus on narrow tasks within biomedical research or are evaluated on limited scenarios.In benchmark dataset that captures the breadth and complexity of data science tasks in biomedical research.The following challenges have not been fully explored: (1) although previous studies leverage publications to create data science tasks [10,14,15], those test cases are drawn from a small number of papers, which may not reflect the full scope of biomedical research; (2) limited task diversity as a consequence of restricted case selection; (3) the involved tasks are performed on relatively simple datasets, such as one or two tables with tens of columns; (4) overlooking the foundational data analysis steps and observed evidence that support or refute a hypothesis, thus, correct hypothesis prediction alone does not guarantee the agent performed the correct analysis; and</p>
<p>(5) the inclusion of non-verifiable hypotheses, where the required data is absent or insufficient to support a conclusive answer, yet such cases are rarely discussed.</p>
<p>In this paper, we introduce BIODSA-1K (Biomedical Data Science Agent Benchmark), a novel framework for evaluating AI agents on biomedical data science research tasks (Figure 1).BIODSA-1K specifies a complete cycle of hypothesis formulation, data analysis, and validation, by curating detailed experimental components extracted from published biomedical studies.Specifically, each instance includes a hypothesis statement, corresponding analysis plans, evidence summaries, and quantitative outcome measures.</p>
<p>As illustrated in Figure 1, BIODSA-1K includes 1,029 scientific hypotheses and the corresponding 1,177 analysis tasks drawn from 329 publications of eight types of publications.The analysis tasks are also comprehensive in terms of common analysis is done in biomedical research.A comparison to other representative data science benchmarks is illustrated in Table 1.</p>
<p>BIODSA-1K: Benchmark data and tasks</p>
<p>BIODSA-1K is constructed from scientific publications and their associated biomedical data.At the core of the benchmark are structured components that mirror the research process: a curated collection of publications and corresponding data tables, extracted hypotheses paired with supporting evidence, and data analysis tasks derived from these elements.This framework supports the development and assessment of AI agents on a wide spectrum of capabilities, from code generation and reasoning to hypothesis testing, grounded in scientific discovery workflows.In the following subsections, we detail the construction of BIODSA-1K, including how publications and data were collected, how hypotheses and supporting evidence were extracted, and how downstream tasks were defined to challenge and benchmark agent performance.</p>
<p>Publication and dataset collection</p>
<p>To construct a benchmark that reflects practical biomedical data science, it is essential to include not only scientific publications but also the corresponding biomedical datasets on which those studies are based.We therefore leverage cBioPortal [16], a comprehensive cancer genomics and clinical data TP53 is an oncogene of prostate cancer tumors.</p>
<p>Supporting evidence</p>
<p>Hypothesis</p>
<p>AI agents Biomedical datasets</p>
<p>Gene Lab Timeline</p>
<p>…</p>
<p>Experiments</p>
<p>Hypothesis decisions</p>
<p>Evidence alignment</p>
<p>Alignment score ∈ 0,1</p>
<p>Non-verifiable decision</p>
<p>Evaluation metrics  portal that maintains structured datasets with direct linkage to peer-reviewed publications.It is under a publicly available Open Database License [17].This ensures that our benchmark captures both the analytical context and the quantitative evidence underlying published findings.In particular, we assume that each publication highlights its primary results within the abstract, often supported by descriptive statistics, statistical testing, and predictive modeling results derived from the associated biomedical tables.Thus, the raw data underlying BIODSA-1K consists of two components: the publication abstracts and their corresponding structured biomedical data tables.</p>
<p>Benchmark curation
Publication Hypothesis Hypothesis Hypothesis
We utilize the cBioPortal API 2 to retrieve all available datasets in bulk.Each dataset includes study metadata that specifies the associated publication(s), including PubMed identifiers (PMIDs).Using these PMIDs, we collect the publication abstracts through the PubMed API 3 .In most cases, there is a one-to-one mapping between a dataset and a publication.However, we exclude ambiguous cases involving multiple papers and datasets when their analytical scope extends beyond the specific dataset.This filtering step avoids introducing non-verifiable hypotheses into the benchmark, thereby maintaining a clear linkage between reported findings and the underlying data.</p>
<p>According to established biomedical literature [18], we categorize the publications in our benchmark by study types.Definitions of these categories are provided in Appendix D. As illustrated in Figure 1, BIODSA-1K spans a diverse array of study types, including genomics, integrative, therapeutics, biomarkers, translational, and molecular studies, along with various analysis methodologies.This distribution highlights the comprehensiveness of BIODSA-1K, capturing both high-level exploratory research and focused hypothesis-driven studies.</p>
<p>Dataset caption</p>
<p>We caption the data tables for benchmarking in data science tasks while preserving privacy.The details of how the captioning works can be found in Appendix B. Specifically, we do not send any patient-level records to LLMs and instead construct a schema-based representation.For each column in a data table, we compute type-specific descriptive statistics, such as the number of unique values, missing value ratio, most frequent entries, and data ranges.In this way, for whatever LLM API provider we use, only the captions of the dataset will be shared.For future research and experiments Biology Hypothesis and analysis 8 Publications --8 BioCoder [12] Biomedical Analysis Github --460 ChatGPT-ADA [24] Biomedical Hypothesis and analysis 4 Publications 1 548 4 AI Co-scientist [3] Biomedical Hypothesis and analysis ---3 BioDiscoveryAgent [11] Biomedical with this benchmark, researchers can download the raw data from cBioPortal and execute the LLMgenerated code on them locally.</p>
<p>Figure 1 shows the scale and diversity of the biomedical tables included in BIODSA-1K.Each point represents a data type, positioned by its typical number of rows and columns, and sized by its prevalence in the dataset.The benchmark encompasses a wide spectrum of commonly used biomedical data types, including clinical data, mutation data, gene expression, copy number alteration, protein expression, structural variation, and patient timelines.These data sources are foundational to modern biomedical research and collectively capture the heterogeneity of real-world biomedical analysis.Moreover, the wide variance in both row and column dimensions, ranging from compact gene panels to large-scale expression matrices, demonstrates the high dimensionality and analytical complexity present in BIODSA-1K.Compared to existing benchmarks (as shown in Table 1), which often involve simpler, smaller, or less diverse datasets, our benchmark presents a significantly more challenging and realistic setting for evaluating AI agents on biomedical data science tasks.</p>
<p>Hypothesis and evidence</p>
<p>All data science challenges in BIODSA-1K are extracted from published biomedical studies using a GPT-4o model.The details of the extraction process can be found in Appendix C. Each challenge is centered around a hypothesis and its corresponding supporting evidence, reflecting how scientific claims are typically articulated in real-world literature.Rather than stating hypotheses solely in null form (e.g., "no difference between groups"), authors of original studies often present claims affirmatively (e.g., "Treatment A improves survival"), while the underlying analyses are grounded in statistical tests against a null hypothesis.To preserve fidelity to real-world practice, our benchmark follows this formulation, presenting hypotheses as definitive statements derived from the study's conclusions.Importantly, our design does not assume these statements are inherently true; instead, we evaluate whether AI agents can reconstruct the reasoning and analysis pipeline leading to such claims, including identifying when the data are insufficient to support them.</p>
<p>Each entry includes (1) a clearly stated hypothesis that is supported or rejected in the original publication, and (2) a plausible counter-hypothesis designed to test the agent's ability to reason discriminatively.An example is provided in Supplementary Figure 1.To support hypothesis validation, we extract one or more evidence entries per hypothesis, each corresponding to a distinct data analysis performed in the study.Each evidence entry is annotated with the following fields:</p>
<p>• Analysis plan: a concise description of the statistical or computational procedure used (e.g., frequency analysis, correlation test, clustering).</p>
<p>• Evidence: a textual summary of the result as reported in the publication.</p>
<p>• Variables: input variables used in the analysis and the result variable serving as the output to support or refute the hypothesis.</p>
<p>To mitigate bias toward Type I error, our benchmark includes a significant fraction of non-verifiable cases where the available data are insufficient to reach a definitive conclusion.This design encourages agents not merely to "prove" hypotheses, but to assess them critically in the context of the available evidence, akin to a real-world research setting.</p>
<p>Tasks and evaluation</p>
<p>The primary task in BIODSA-1K is hypothesis validation using structured biomedical data.Given a hypothesis extracted from a publication and the corresponding dataset, an AI agent is required to generate executable code to analyze the data and produce empirical observations.Based on these observations, the agent must decide whether the hypothesis is True, False, or Non-verifiable.To distinguish between the latter two, we define a hypothesis as False if the agent can identify relevant variables in the dataset and derive contradicting evidence through analysis.Conversely, a hypothesis is considered Non-verifiable if no relevant features or data tables exist in the dataset to support or reject the claim.For example, the hypothesis "Prostate cancer brain metastases (PCBM) have a higher mutational burden compared to non-brain metastases" is labeled as Non-verifiable if the dataset lacks mutational burden variables or comparative group labels.</p>
<p>We evaluate agent performance across multiple dimensions.On the hypothesis decision level, we compute both Type I and Type II error rates.Let H ∈ True, False denote the ground truth label of a hypothesis, and Ĥ be the label predicted by the agent.The Type I error (false positive rate) is defined as:
Type I Error = I[H = False ∧ Ĥ = True] I[H = False] ,(1)
and the Type II error (false negative rate) is given by:
Type II Error = I[H = True ∧ Ĥ = False] I[H = True] ,(2)
where I[•] is the indicator function.</p>
<p>In addition to correctness at the decision level, we assess how well the generated observations align with the supporting evidence reported in the original publication.Let E denote the set of ground truth supporting evidences and O the set of observations generated by the agent.We use a large language model (LLM)-as-a-judge [25] approach to measure the evidence alignment score:
Alignment Score = |O ∩ E| |E| .(3)
This metric quantifies the proportion of reported evidence that is successfully captured by the agent's analysis pipeline.</p>
<p>Furthermore, we evaluate the technical quality of the generated code.For each hypothesis, let C denote the total number of code cells generated and C exec the number of those that are executable without error.The code executability rate is defined as: Executability Rate = Cexec C .For ReAct-style agents that explore through multi-step reasoning, this metric is computed over all code snippets generated during the interaction trace.Lastly, we systematically assess agents on their ability to reject non-verifiable hypotheses.These hypotheses are curated by taking claims from other publications that reference unrelated datasets.An ideal agent should classify such hypotheses as Non-verifiable due to the absence of relevant data.Let H = Non-verifiable be the ground truth and Ĥ be the predicted label.We report the non-verifiable detection accuracy as:   We implement four agent-based methods to evaluate performance on BIODSA-1K.CodeGen directly generates a single executable Python code block based on the input hypothesis and dataset schema, and returns a final decision, True, False, or Non-verifiable, based on the produced observations, without explicit intermediate reasoning [26].We evaluate two variants of CodeGen: one powered by GPT-4o and the other by O3mini.ReAct follows the ReAct framework [27], in which the agent alternates between reasoning steps ("thoughts") and code execution ("actions"), allowing iterative refinement of analysis and conclusions.This version is implemented using GPT-4o.
Non-verifiable Accuracy = I[H = Non-verifiable ∧ Ĥ = Non-verifiable] I[H = Non-verifiable] .(4)
To enable more structured reasoning, we also introduce two reasoning-augmented agents aligned with recent developments in data analysis agents [6,13], which decouple experiment planning from execution.CodeGen-Reasoning first prompts O3-mini to generate a structured analysis plan detailing key reasoning and statistical steps, and then passes this plan to GPT-4o for code generation and execution, allowing division of labor between planning and implementation.ReAct-Reasoning extends ReAct with structured planning and uses O3-mini as the backend agent.It supports iterative reasoning and dynamic plan refinement based on intermediate observations across multiple steps.</p>
<p>Hypothesis validation</p>
<p>Table 2 shows that AI agents tend to be conservative in hypothesis validation across all tested publication types.In nearly every setting, the Type II error rate (E II ), which measures the frequency of missed relevant findings, is consistently higher than the Type I error rate (E I ), which reflects the incidence of false positives.For example, in the Biomarkers category, CodeGen (gpt-4o) exhibits a Type II error of 0.164 compared to a Type I error of 0.090.Figure 3 shows that ReAct-based methods consistently outperform CodeGen models, particularly when reasoning is applied.Even without reasoning, ReAct (gpt-4o) achieves lower Type II errors in challenging categories such as Integrative (0.148 vs. 0.153) and Pan-Cancer (0.128 vs. 0.167) compared to CodeGen (gpt-4o).When reasoning is incorporated, ReAct-R<em> outperforms CodeGen-R</em> in most domains, for example, in Translational, ReAct-R<em> reports a Type II error of 0.112 compared to 0.110 for CodeGen-R</em>, while maintaining a lower Type I error (0.098 vs. 0.060).</p>
<p>Finally, Figure 3 suggests that reasoning brings the greatest improvements in domains with higher baseline error rates.This trend is evident in the Genomics and Integrative categories, where non-reasoning methods exhibit relatively high Type II errors: up to 0.191 for CodeGen (o3-mini) in Integrative.In contrast, ReAct-R* reduces the same error to 0.107.This implies that reasoning is particularly valuable in more complex or information-dense publication types, helping agents better navigate and resolve ambiguous or detailed hypotheses.</p>
<p>Analysis quality</p>
<p>Making a correct hypothesis decision does not necessarily imply that the AI agent followed a valid or faithful analytical process, which is a limitation largely overlooked in prior evaluations.To address this, our benchmark explicitly assesses the evidence alignment score, which measures how well the agent-generated analysis captures the ground-truth evidence reported in the original studies.We also examine the executability of the analysis code produced by the agents as a proxy for code quality and practical usability.</p>
<p>As shown in Figure 5, the evidence alignment scores remain modest across all methods, typically ranging from 0.20 to 0.25, regardless of whether the hypothesis being validated is ultimately True or False.Among the evaluated methods, ReAct-based agents exhibit marginally higher alignment scores compared to code generation baselines.However, the consistently low scores across the board suggest  that AI agents often diverge from the evidence used in human-authored analyses, possibly reflecting a lack of domain knowledge or contextual understanding required for appropriate methodological choices.</p>
<p>Figure 6 further breaks down alignment scores by analysis type.We observe that simpler analytical tasks, such as frequency counts, are more reliably handled by AI agents.In contrast, more complex tasks, including clustering and survival analysis, pose significant challenges, with notably lower alignment scores across all models.These findings highlight the need for improved reasoning strategies and domain-specific modeling capabilities in AI systems aimed at biomedical data analysis.</p>
<p>Figure 4 presents the executability of the code generated by different AI agents and categorizes the types of errors found in non-executable outputs.Overall, ReAct-based agents exhibit the highest code executability rates, with ReAct Reasoning achieving 86.6% and ReAct at 84.9%, outperforming both CodeGen (76.9%) and CodeGen Reasoning (58.2%).Among the error types, variable or object misuse is the most common failure mode, especially prominent in CodeGen Reasoning (27.7%) and ReAct (32.1%).Logic and mathematical errors, as well as import or module-related issues, occur less frequently but still contribute to code failure across methods.7 presents the distribution of hypothesis decisions (True, False, Not Verifiable) based on whether the generated code was executable or not.The results reveal a marked difference in decision patterns between the executable and non-executable cases.In the non-executable setting, all three CodeGen variants default to deciding the hypothesis as Not Verifiable in approximately 87% of instances, but still decide around 8% as False and 5% as True.These 13% cases indicate that the AI agents sometimes turn out to hallucinate the findings.By contrast, in cases where the generated code is executable, the proportion of Not Verifiable decisions significantly drops, while the rates of True and False decisions increase substantially.</p>
<p>Non-verifiable hypothesis</p>
<p>We further investigated whether AI agents can act cautiously when faced with non-verifiable hypotheses.As shown in Figure 8, we constructed a set of 100 hypotheses that are strictly non-verifiable, meaning the associated dataset lacks the information needed to either accept or reject them.In this setting, the correct model behavior is to respond with "Not Verifiable"; any decision of "True" or "False" reflects overconfidence or hallucination.The ability to correctly identify these cases, quantified as the true positive rate (TPR) for the "Not Verifiable" class, varies substantially across agents.One-round code generation methods, such as CodeGen gpt-4o and CodeGen o3-mini, achieve only 63% and 57% TPR, respectively, often making incorrect verifiable claims in 37% and 43% of the cases.In contrast, reasoning-augmented agents like CodeGen Reasoning, ReAct, and particularly ReAct Reasoning perform more conservatively, with ReAct Reasoning achieving a TPR of 92%.</p>
<p>Related work</p>
<p>Benchmarks Recent efforts have introduced benchmark datasets to evaluate AI agents in scientific discovery and data science tasks.General-purpose scientific discovery benchmarks such as DiscoveryBench [14], ScienceAgentBench [15], and SpiderV2 [28] focus on a broad range of tasks but often overlook specialized biomedical reasoning challenges.In parallel, several benchmarks specifically target the core task of code generation in scientific domains, including SciCode [23], Blade [22], and DSBench [21].Within biomedicine, BioCoder [12] and CliniDSBench [10] address coding tasks related to biomedical data analysis.However, these benchmarks primarily emphasize code generation, while our work focuses on the broader hypothesis validation process derived directly from published scientific studies.Moreover, BIODSA-1K offers a significantly larger and more diverse evaluation scale, encompassing over three hundred publications, substantially exceeding the coverage of previous benchmarks.</p>
<p>Agents A growing body of work explores the use of AI agents for data science and scientific research.Several systems target general data science tasks, including machine learning modeling and analysis on structured datasets such as Kaggle competitions [29][30][31]20].Gao et al. [7] emphasizes the potential of developing agents specifically tailored for biomedical research.In the biomedical domain, Co-Scientist [3] and BioDiscoveryAgent [11] focus on a niche area: automating the design and execution of genetic perturbation experiments.Other agent frameworks have applied LLMs for bioinformatics programming, biomedical question answering [32], and the development of predictive models for biological outcomes [24].Closest to our work is the line of research on hypothesis validation agents [13], which investigates how agents can reason over structured data to accept or refute scientific claims.Our work builds on these foundations but uniquely grounds the validation tasks in hypotheses and evidence derived from real-world publications, enabling broader and more rigorous evaluation of biomedical data science agents.</p>
<p>Discussion and conclusion</p>
<p>While our benchmark draws from over 300 biomedical studies, it does not fully capture the diversity of the biomedical research landscape.The dataset naturally overrepresents well-established topics with high publication volume, potentially underrepresenting emerging areas or those with limited available data.This skew may influence model performance and generalizability, highlighting the need to continuously expand and rebalance the benchmark to reflect a wider spectrum of scientific inquiry.More broadly, as AI agents become increasingly capable of performing end-to-end data science tasks, they also introduce the risk of generating plausible but incorrect scientific claims.Without proper oversight, such systems could accelerate the propagation of false findings under the guise of data-driven analysis.Ensuring transparency, interpretability, and human-in-the-loop validation will be critical to responsibly deploying these tools in high-stakes scientific domains.</p>
<p>In this work, we present BIODSA-1K, a benchmark designed to evaluate AI agents on realistic biomedical data science tasks.By extracting over a thousand hypotheses and corresponding analysis plans from hundreds of published studies, BIODSA-1K captures the diversity and complexity inherent in real-world biomedical research.Unlike prior benchmarks, it encompasses not only hypothesis validation tasks with sufficient evidence, but also non-verifiable cases where the available data are inconclusive: a frequent yet underrepresented scenario in scientific reasoning.The benchmark enables comprehensive evaluation across multiple dimensions, including decision accuracy, evidence grounding, reasoning validity, and analysis code executability.We envision BIODSA-1K as a foundation for developing more robust, transparent, and trustworthy AI agents for scientific discovery.</p>
<p>[31] Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, et al.Autokaggle: A multi-agent framework for autonomous data science competitions.arXiv preprint arXiv:2410.20424,2024.</p>
<p>[32] Nikita Mehandru, Amanda K Hall, Olesya Melnichenko, Yulia Dubinina, Daniel Tsirulnikov, David Bamman, Ahmed Alaa, Scott Saponas, and Venkat S Malladi.BioAgents: Democratizing bioinformatics analysis with multi-agent systems.arXiv preprint arXiv:2501.06314,2025.</p>
<p>Justification: This paper does not involve theoretical results.Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [Yes] Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and research with human subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [NA]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional review board (IRB) approvals or equivalent for research with human subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [NA]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.In one analysis, the study performed a correlation analysis between MGMT promoter methylation status and mutation rates, reporting a statistically significant positive correlation (r = 0.65, p &lt; 0.01), suggesting that higher methylation is linked to increased mutation burden.A second analysis compared mutation rates between tumors with methylated versus unmethylated MGMT promoters, showing that methylated tumors had significantly higher mutation rates (mean difference = 15 mutations per sample, p &lt; 0.05).These analyses collectively support the hypothesis.</p>
<p>Example 2: PIK3R1 Mutation Frequency.This hypothesis posits that the PIK3R1 gene is frequently mutated in glioblastoma samples, with the null hypothesis stating that it is not.In the first analysis, the study reported a mutation frequency of 25% for PIK3R1 across glioblastoma samples, indicating a notable prevalence.A second analysis ranked gene mutation frequencies and found that PIK3R1 was among the top 10% of all mutated genes, further supporting the claim of frequent alteration.</p>
<p>These examples demonstrate how BIODSA-1K captures both the semantic structure of biomedical hypotheses and the analytical reasoning used to evaluate them, providing a grounded framework for assessing the capabilities of AI agents in data-driven scientific inference.</p>
<p>B Captions of Biomedical Data Tables</p>
<p>To support automated hypothesis validation and dataset reasoning tasks, we systematically generated structured captions for biomedical data tables from the cBioPortal repository.Each caption describes the content and structure of a tabular dataset, including its schema, value distributions, and metadata annotations.We developed a modular pipeline to process every dataset directory and extract metadata from text-based tables, primarily those with filenames beginning with data_.</p>
<p>For each dataset, we extracted high-level metadata including dataset ID, cancer type, and description.Within each dataset directory, we identified all data tables and parsed their contents while ignoring comment lines (those beginning with "#").The first non-comment line was interpreted as the column header.Subsequent lines were parsed as tab-delimited rows, with short rows padded and long rows truncated to maintain schema alignment.</p>
<p>To ensure consistency and facilitate downstream usage, we cleaned column names by removing punctuation and replacing whitespace with underscores.We then inferred the data type of each column using a custom heuristic function and computed column-wise statistics depending on the inferred type:</p>
<p>• Binary and categorical columns: Top value counts and number of unique values were reported, along with missing value rate.• Integer-valued columns: We computed quantiles (1%, 20%, 40%, 60%, 80%, 99%) as well as minimum and maximum values.• Continuous columns: Descriptive statistics were generated, including count, mean, standard deviation, and range, rounded to four decimal places.</p>
<p>Each table's caption includes its name, number of rows and columns, column-level statistics, and preserved comment rows if present.The final structured metadata was saved as JSON files under a centralized metadata directory, one per dataset.These captions serve as machine-readable documentation for real-world biomedical tables and are critical for enabling dataset-aware reasoning by AI agents.</p>
<p>An example metadata structure is as follows:</p>
<p>{ "dataset_id": "example_ds", "type_of_cancer": "glioblastoma", "description": "Clinical and genomic data for GBM samples", "tables": [ { 2. All evidence must include specific, measurable quantities or statistical relationships 3. Result values must be numerical (e.g., percentages, counts, p-values, correlation coefficients) or categorical with clear classifications 4. Analysis variables must be specific data columns or features that exist in the dataset Return your answer as a JSON object in the following format: '''json { "hypotheses": [ { "hypothesis": a specific, binary hypothesis that can be tested statistically, from the abstract, the one which is considered to be true from the study, "wrong_hypothesis": make a random perturbation of the hypothesis so that it is a wrong hypothesis, "supporting_evidences": [ // the evidences that support the alternative hypothesis { "analysis_plan": a brief analysis plan that can yield this evidence, "evidence": specific statistical finding or measurement, "analysis_variables": list of exact variables/features needed for analysis, "result_variable": the specific metric or statistical measure used, "result_variable_value": numerical value, statistical measure, or categorical outcome }, ... ] }, ... ] } """</p>
<p>To increase throughput and reliability, we used batched LLM calls with zero temperature to ensure deterministic completions.Each LLM output was parsed using a custom function that handled both valid JSON and malformed output formats via regular expression matching.The output structure follows a fixed schema that includes a hypothesis, a wrong_hypothesis (a small perturbation to simulate a plausible counterfactual), and a list of supporting_evidences, each containing fields such as analysis_plan, evidence, analysis_variables, result_variable, and result_variable_value.</p>
<p>Each extracted hypothesis is linked to its source publication (via PMID) and the relevant datasets (via dataset ID) so that the claim can later be validated against real-world biomedical tables.The full outputs were stored as structured JSON files, one per publication.This corpus forms the foundation of BIODSA-1K, enabling AI agents to reason over realistic, evidence-backed scientific claims.</p>
<p>D Categorization of biomedical publications</p>
<p>Genomics Publications in this category focus on large-scale genomic profiling of tumors.These studies utilize high-throughput sequencing to catalog somatic mutations, copy-number variations, and other genetic alterations across cancer samples.</p>
<p>Molecular This class covers research that investigates molecular characteristics beyond DNA mutations.It includes analyses of transcriptomic, proteomic, and epigenomic data, often derived from both patient samples and established cancer cell lines.</p>
<p>Pan-Cancer Pan-Cancer studies undertake comparative analyses across multiple types of cancers.They aim to identify common molecular patterns and differences, thereby deepening our understanding of shared and unique cancer pathways.</p>
<p>Therapeutics These publications explore the relationship between genomic alterations and drug responses.The focus is on identifying potential therapeutic targets and advancing personalized treatment strategies based on genetic and molecular data.</p>
<p>Biomarkers Research in this class is dedicated to discovering and validating diagnostic and prognostic markers.These biomarkers help in predicting disease outcomes, guiding treatment decisions, and supporting early detection.</p>
<p>Methods Publications categorized as Methods introduce new computational tools, algorithms, or experimental techniques that facilitate the analysis and interpretation of complex biomedical data.</p>
<p>Integrative Integrative studies combine data from multiple omics layers-such as genomics, transcriptomics, and proteomics-to provide a comprehensive view of tumor biology.They aim to interconnect disparate data types into coherent biological insights.</p>
<p>Translational: This class emphasizes bridging the gap between research and clinical application.Translational studies apply genomic and molecular findings to improve diagnostic methods, prognostic assessments, and treatment strategies in clinical practice.</p>
<p>E Categorization of analysis tasks</p>
<p>Correlation analysis (Correlation) Tasks that focus on statistically relating two or more variables.Examples include correlating gene methylation status with mutation rates or associating mutational profiles with clinical factors such as smoking status.</p>
<p>Comparative analysis (Comparison) Tasks that directly contrast groups or conditions.These include comparing mutation frequencies between groups (e.g., methylated vs. unmethylated promoters) or contrasting profiles across different cancer subtypes.</p>
<p>Frequency analysis (Frequency) Tasks that measure the occurrence or rate of specific genomic events.Typical examples are calculating the mutation frequency for a given gene or determining the prevalence of a particular genetic alteration.</p>
<p>Clustering and classification (Clustering) Tasks that involve grouping data based on similarities.These studies might use cluster analysis to categorize samples by genomic features or mutational signatures.</p>
<p>Survival and prognostic analysis (Survival) Tasks that associate molecular or genomic features with patient outcomes, such as survival curve comparisons or prognostic evaluations.</p>
<p>Functional and experimental analysis (Functional) Tasks that explore gene function or cellular behavior through experimental approaches.This includes RNA interference experiments or assays measuring the effects of gene knockdown on cell proliferation.</p>
<p>Genomic structural analysis (Structural)</p>
<p>Tasks that analyze genomic architecture or structural variants.Examples include the evaluation of copy-number alterations, genomic rearrangements, or spatial mutation distributions.</p>
<p>Pathway and integrative analysis (Pathway) Tasks that integrate multiple data types to elucidate biological pathways and networks.These include integrative pathway analyses, enrichment studies, or assessments of driver mutations in signaling cascades.</p>
<p>F Categorization of code errors</p>
<p>To enable systematic analysis of common failure modes in AI-generated code, we group low-level Python error types into broader categories reflecting common code quality issues.This many-to-one mapping provides a more interpretable summary of model behaviors and facilitates downstream visualization and comparison.The categorization is defined as follows:</p>
<p>• Variable/Object Misuse: Errors such as KeyError, AttributeError, NameError, and IndexError that arise from referencing undefined variables, missing dictionary keys, or invalid object attributes.</p>
<p>• Math/Logic Error: Includes errors like ZeroDivisionError, ValueError, and numpy.linalg.LinAlgError, which typically result from invalid arithmetic operations, numerical instability, or logical violations.</p>
<p>• Import/Module Error: Consists of ImportError and ModuleNotFoundError, indicating missing dependencies or incorrect import paths.</p>
<p>• File/I-O Error: Captures input/output-related issues such as FileNotFoundError and OSError, often caused by referencing unavailable files or malformed I/O operations.</p>
<p>• Pandas/Data Error: Includes errors from data processing libraries, such as pandas.errors.ParserError, MergeError, and IndexingError, typically caused by invalid parsing, merging, or indexing operations.</p>
<p>• General Exception: Encompasses generic or runtime-specific errors such as Exception and RuntimeError, which represent critical failures not captured by more specific categories.</p>
<p>G Agent prompts G.4 ReAct-Reasoning AGENT_MODEL_PROMPT_TEMPLATE = """ You are a scientific agent who can plan and execute python code iteratively to evaluate a scientific hypothesis.</p>
<p>Note:</p>
<p>-You must execute and refine the given analysis plan iteratively until you have enough evidence to support the hypothesis.-You must always write a single Python code block that can be executed directly based on the analysis plan.-Use 'print()' statements in your code to get the observations.""" PLANNING_PROMPT_TEMPLATE = """ # TASK Generate an analysis plan to evaluate the user's scientific hypothesis using the datasets provided.</p>
<p>The plan should consist of clear, actionable psudo codesteps that can be <strong>easily converted to python code</strong> without needing any additional information.</p>
<h1>REQUIREMENTS -Use only table and column names from the schema: do not invent or guess names.</h1>
<p>-Ensure every step is unambiguous and directly executable.</p>
<p>-Use consistent naming for all variables (e.g., tables, columns) throughout the plan.-Be as concise as possible while maintaining full clarity and precision.# DATASET PATHS {dataset_paths} # DATASET SCHEMA {dataset_schema} """</p>
<p>Figure 1 :
1
Figure 1: Benchmark statistics.(left) BIODSA-1K includes diverse types of biomedical research and data analysis tasks created from 329 publications; the x-axis indicates the publication types.; (Right) Bubble plot illustrating the diverse range of biomedical data tables in BIODSA-1K, showing each data table's number of rows (x-axis, log-scale) versus number of columns (y-axis, log-scale).</p>
<p>Figure 2 :
2
Figure 2: Overview of BIODSA-1K.a, Benchmark curation: Scientific publications linked to biomedical datasets are parsed to extract hypotheses and their corresponding supporting evidence, forming the core reasoning challenges.b, Experiments: AI agents are tasked with validating hypotheses by planning analysis steps, generating executable code, observing results, and making decisions based on structured biomedical datasets.c Evaluation metrics: Agent performance is evaluated based on hypothesis decision accuracy (Type I and Type II errors), evidence alignment with publication findings, non-verifiable hypothesis detection (precision and recall), and code executability rate.</p>
<p>Figure 3 :
3
Figure 3: Comparison of Type I and Type II error rates across publication types and agent variants.Each point denotes an agent's performance on a specific publication type.</p>
<p>Figure 3 andFigure 5 :
35
Figure3and Table 2 also demonstrate that reasoning augmentation improves both sensitivity and specificity.Reasoning-enhanced agents (denoted with an asterisk, e.g., CodeGen-R<em> and ReAct-R</em>) consistently outperform their base counterparts in terms of lower error rates.For instance, ReAct-R*</p>
<p>Figure 4 :
4
Figure 4: Code excitability analysis and the breakdown of error types in non-executable code across the selected AI agents.</p>
<p>Figure 7 :
7
Figure 7: Hypothesis validation results' distribution by the code excitability for CodeGen methods.</p>
<p>Table 1 :
1
Comparison of BIODSA-1K with representative benchmarks in general and biomedical domains."Avg.# Tables" denotes the average number of tables per task; "Avg.# Columns" refers to the average columns per table."-" indicates missing or non-tabular data."# Tasks" shows the number of unique data science tasks."<em>" indicates the biology-related portions of the benchmarks.
BenchmarkDomainTask LevelsTask SourcesAvg. # Tables Avg. # Columns</em> # TasksDS-1000 [19]GeneralAnalysisStackoverflow1-1000MLAgentBench [20]GeneralAnalysisPublications14713DSBench [21]GeneralAnalysisKaggle--466BLADE [22]GeneralHypothesis and analysis 31 Publications11312ScienceAgentBench [15]GeneralHypothesis and analysis 44 Publications--102DiscoveryBench-Bio<em> [14] BiologyHypothesis and analysis 2 Publications22616SciCode-Bio</em> [23]</p>
<p>Table 2 :
2
Performance of hypothesis validation across publication types.Each cell reports the Type I error rate (E I , false positive rate) and Type II error rate (E II , false negative rate), with lower values indicating better performance."R<em>" is short for "Reasoning" version of CodeGen and ReAct, respectively.Bold values highlight the best performance in each column.
BiomarkersGenomicsIntegrativeMolecularPan-CancerTherapeuticsTranslationalMethods(n=244) E I E II(n=662) E I E II(n=392) E I E II(n=108) E I E II(n=78) E I E II(n=344) E I E II(n=224) E I E IICodeGen (gpt-4o)0.090 0.164 0.077 0.168 0.095 0.153 0.157 0.157 0.077 0.167 0.087 0.137 0.094 0.147CodeGen (o3-mini) 0.107 0.145 0.128 0.187 0.122 0.191 0.098 0.118 0.103 0.179 0.157 0.181 0.143 0.138ReAct (gpt-4o)0.102 0.148 0.069 0.159 0.066 0.148 0.120 0.167 0.115 0.128 0.090 0.155 0.089 0.161CodeGen-R</em>0.082 0.156 0.054 0.139 0.082 0.125 0.111 0.139 0.141 0.154 0.083 0.148 0.060 0.110ReAct-R*0.090 0.094 0.060 0.125 0.074 0.107 0.074 0.093 0.051 0.167 0.087 0.122 0.098 0.112This provides insight into how well agents can discern dataset limitations and avoid over-assertiveconclusions.3 Experiment3.1 Implemented methods</p>
<p>Figure 6: Evidence alignment scores by types of analyses.reducesthe Type I and II errors in the Genomics category to 0.060 and 0.125, respectively, compared to 0.069 and 0.159 for the base ReAct model.Similarly, CodeGen-R* achieves a Type I error of 0.082 and Type II error of 0.156 on Biomarkers, outperforming the original CodeGen (gpt-4o) with errors of 0.090 and 0.164.These results indicate that structured reasoning enhances the agent's ability to identify relevant evidence while reducing false positives.
0.35300.27 (0.26-0.28)0.26 (0.24-0.28)Alignment Score0.23 (0.22-0.24)0.21 (0.19-0.23)0.20 (0.19-0.22)0.19 (0.18-0.20)0.19 (0.17-0.21)0.16 (0.14-0.17)1007.1%9.0%8012.0%32.1%Percentage (%)40 6076.9%58.2% 27.7%64.9%86.6%200CodeGen CodeGen ReasoningReActReasoning ReActExecutableMath/Logic ErrorOther ErrorsVariable/Object MisuseImport/Module Error</p>
<dl>
<dt>While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</dt>
<dd>[Yes]Justification: [NA]Guidelines:• The answer NA means that paper does not include experiments requiring code.• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• understand the results? Answer: [Yes] Justification: [NA] Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: [NA] Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer "Yes" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: [NA] Guidelines: • Answer: [Yes] Justification: [NA] Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid-eration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] • 8. Experiments compute resources Justification: [NA]
6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to</dd>
</dl>
<p>• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.•We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.16.Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)for what should or should not be described.CodeGen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26 G.2 ReAct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 G.3 CodeGen-Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 G.4 ReAct-Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28 Example 1: MGMT Methylation and Hypermutation.The hypothesis states that MGMT promoter methylation is positively associated with a hypermutator phenotype in treated glioblastomas.The corresponding null hypothesis asserts no such association.
Justification: [NA] Guidelines: • Contents of Appendix A Example hypothesis and supporting evidence21B Captions of Biomedical Data Tables22C Extracting hypothesis and evidence from publications23D Categorization of biomedical publications24E Categorization of analysis tasks25F Categorization of code errors26G Agent prompts26G.1Answer: [NA]</p>
<p>-provided scientific hypothesis, you <strong>Must</strong> write {language} code to help the user evaluate the hypothesis.# IMPORTANT: CODE OUTPUT REQUIREMENTS You must import all the necessary libraries at the beginning of your code.You must use explicit print() statements for ALL outputs you want to see or analyze.Simply writing expressions like 'df.head()' will NOT show results in the execution log.Always use: -print(df.head())-print(analysis_result) -print(statistical_test_output) Every intermediate result and final output must be wrapped in a print() statement to be visible in the execution log.TASK Given the user-provided analysis plan for the user's scientific hypothesis, you <strong>Must</strong> write {language} code to fulfill the plan so that user can execute the code later to evaluate the hypothesis.# IMPORTANT: CODE OUTPUT REQUIREMENTS You must import all the necessary libraries at the beginning of your code.You must use explicit print() statements for ALL outputs you want to see or analyze.Simply writing expressions like 'df.head()' will NOT show results in the execution log.Always use: -print(df.head())-print(analysis_result) -print(statistical_test_output) Every intermediate result and final output must be wrapped in a print() statement to be visible in the execution log.
CODE_GENERATION_PROMPT_TEMPLATE = """# # DATASET PATHS{dataset_paths}## OuptutYour output should be in Markdown format and you should wrap the generated code in'''{language} ''' tags."""G.1 CodeGen# Prompts for CodeGen methods:"""# TASKGiven the user# DATASET PATHS{dataset_paths}# DATASET SCHEMA{dataset_schema}## OuptutYour output should be in Markdown format and you should wrap the generated code in'''{language} ''' tags."""
https://github.com/cbioportal/cbioportal/
https://www.ncbi.nlm.nih.gov/home/develop/api/
NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer:[Yes]Justification:[NA]Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes]Justification:[NA]Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory assumptions and proofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]A Example hypothesis and supporting evidence "result_variable": "correlation_coefficient", "result_variable_value": "r = 0.65, p &lt; 0.01" }, { "analysis_plan": "Compare mutation rates between glioblastomas with methylated and unmethylated MGMT promoters using a t-test.","evidence": "Tumors with methylated MGMT promoters exhibited significantly higher mutation rates than those without methylation (mean difference = 15 mutations/sample, p &lt; 0.05), supporting the association.", "analysis_variables": [ "MGMT_promoter_methylation_status", ""result_variable": "mean_mutation_rate_difference", "result_variable_value": "15 mutations/sample, p &lt; 0. "result_variable": "mutation_frequency", "result_variable_value": "25%" }, { "analysis_plan": "Rank gene mutation frequencies to determine whether PIK3R1 is among the most frequently mutated genes in glioblastoma.","evidence": "PIK3R1 mutation frequency ranked within the top 10% among all genes analyzed, suggesting it is frequently altered in glioblastoma.","analysis_variables": [ "gene_mutation_status" ],"result_variable": "relative_mutation_frequency", "result_variable_value": "Top 10% among all genes" } ] } ] } Supplementary Figure1: Examples of the hypothesis, counter-hypothesis, and supporting evidence extracted from biomedical publications.To illustrate the structure of entries in BIODSA-1K, we present two representative examples derived from glioblastoma studies.Each example consists of a hypothesis formulated from the original study's conclusions, a corresponding null hypothesis, and multiple supporting analyses that provide evidence for or against the claim."name": "data_clinical.txt","n_rows": 287, "n_columns": 12, "n_comment_rows": 4, "columns": [ { "name": "age_at_diagnosis", "data_type": "integer", "n_unique": 45, "missing_rate": 0.03, "statistics": { "min": 22, "0.2": 45, "0.8": 73, "max": 88, "statistics_type": "quantiles" } }, ... ] } ] }This structured captioning process enables interpretability, reusability, and intelligent query capabilities across diverse biomedical datasets in BIODSA-1K.C Extracting hypothesis and evidence from publicationsTo construct a benchmark of data-driven scientific claims, we developed a large-scale pipeline for extracting testable hypotheses and their supporting evidence from biomedical publications.Each extracted instance consists of a binary hypothesis derived from the abstract, a plausible counterhypothesis, and one or more structured evidence entries grounded in quantitative findings.We began with a curated metadata file from cBioPortal, which includes over 1,000 publications indexed by PubMed ID (PMID), their associated dataset identifiers, and accompanying titles, abstracts, and result summaries.After deduplication and filtering, we paired each publication with its corresponding abstract and dataset identifiers.For each entry, we concatenated the title and abstract to form a unified context and sent it to a large language model (GPT-4o) via a structured prompt.The prompt was designed to elicit hypotheses that are:• Binary and testable using statistical or machine learning methods,• Grounded in measurable outcomes, with clear references to statistical relationships or effect sizes,• Accompanied by structured supporting evidence, including analysis plans, involved variables, statistical measures, and result values.# The prompt: """The following is the abstract of a publication: {abstract} Task:Given the abstract of a publication, your task is to extract binary hypotheses and their supporting evidences that can be tested through data analysis.Requirements for hypotheses and evidences: 1.Each hypothesis must be testable using statistical analysis or machine learning methodsG.2 ReActG.3 CodeGen-Reasoning# CodeGen-Reasoning prompts ANALYSIS_PLAN_PROMPT_TEMPLATE = """ # TASK Generate an analysis plan to evaluate the user's scientific hypothesis using the datasets provided.The plan should consist of clear, actionable steps that can be <strong>easily converted to {language} code</strong> without needing any additional information.# REQUIREMENTS -Use only table and column names from the schema: do not invent or guess names.-Ensure every step is unambiguous and directly executable.-Use consistent naming for all variables (e.g., tables, columns) throughout the plan.-Be as concise as possible while maintaining full clarity and precision.# DATASET PATHS {dataset_paths}# DATASET SCHEMA {dataset_schema} # OUTPUT FORMAT Wrap the analysis plan in <analysis_plan> </analysis_plan> tags.So an example output would be ''' <analysis_plan> 1. load the dataset 2. print hello world </analysis_plan> """
The rise and potential of large language model based agents: A survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Science China Information Sciences. 6821211012025</p>
<p>Large language model based multi-agents: a survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>A foundation model for human-ai collaboration in medical literature mining. Zifeng Wang, Lang Cao, Qiao Jin, Joey Chan, Nicholas Wan, Behdad Afzali, Hyun-Jin Cho, Chang-In Choi, Mehdi Emamverdi, K Manjot, Gill, arXiv:2501.162552025arXiv preprint</p>
<p>Position: data-driven discovery with large generative models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Sanchaita Agarwal, Ashish Hazra, Peter Sabharwal, Clark, Forty-first International Conference on Machine Learning. 2024</p>
<p>Empowering biomedical discovery with ai agents. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik, Cell. 187222024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Healthcare data scientist qualifications, skills, and job focus: a content analysis of job postings. Melanie A Meyer, Journal of the American Medical Informatics Association. 2652019</p>
<p>Can large language models replace data scientists in biomedical research. Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, Jimeng Sun, arXiv:2410.215912024arXiv preprint</p>
<p>BioDiscoveryAgent: An ai agent for designing genetic perturbation experiments. Jian Yusuf H Roohani, Qian Vora, Percy Huang, Jure Liang, Leskovec, ICLR 2024 Workshop on Machine Learning for Genomics Explorations. 2024</p>
<p>BioCoder: a benchmark for bioinformatics code generation with large language models. Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark B Gerstein, Bioinformatics. 40Supplement_12024</p>
<p>Automated hypothesis validation with agentic sequential falsifications. Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candès, Jure Leskovec, arXiv:2502.098582025arXiv preprint</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, The Thirteenth International Conference on Learning Representations. 2024</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, arXiv:2410.05080Toward rigorous assessment of language agents for data-driven scientific discovery. 2024arXiv preprint</p>
<p>Integrative analysis of complex cancer genomics and clinical profiles using the cbioportal. Jianjiong Gao, Ugur Bülent Arman Aksoy, Gideon Dogrusoz, Benjamin Dresdner, S Onur Gross, Yichao Sumer, Anders Sun, Rileen Jacobsen, Erik Sinha, Ethan Larsson, Chris Cerami, Nikolaus Sander, Schultz, 10.1126/scisignal.2004088Science Signaling. 62692013</p>
<p>Open data commons open database license (odbl). v1.0</p>
<p>. Bert Vogelstein, Nickolas Papadopoulos, E Victor, Shibin Velculescu, Luis A Zhou, Kenneth W DiazJr, Kinzler, Cancer genome landscapes. Science. 33961272013</p>
<p>DS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wentau Yih, Daniel Fried, Sida Wang, Tao Yu, International Conference on Machine Learning. PMLR2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>DSBench: How far are data science agents from becoming data science experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>BLADE: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Scicode: A research coding benchmark curated by scientists. Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Advances in Neural Information Processing Systems. 202437</p>
<p>Large language models streamline automated machine learning for clinical studies. Tianyu Soroosh Tayebi Arasteh, Mahshad Han, Christiane Lotfinia, Jakob Nikolas Kuhl, Daniel Kather, Sven Truhn, Nebelung, Nature Communications. 15116032024</p>
<p>GPTScore: Evaluate as you desire. Jinlan Fu, See Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Code generation with alphacodium: From prompt engineering to flow engineering. Tal Ridnik, Dedy Kredo, Itamar Friedman, arXiv:2401.085002024arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Spider2-v: How far are multimodal agents from automating data science and engineering workflows?. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Wenjing Hu, Yuchen Mao, Advances in Neural Information Processing Systems. 202437</p>
<p>DS-Agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, International Conference on Machine Learning. PMLR2024</p>
<p>Large language models orchestrating structured reasoning achieve kaggle grandmaster level. Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath , Shahul Hameed, Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, arXiv:2411.035622024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
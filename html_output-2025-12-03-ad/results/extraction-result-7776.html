<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7776 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7776</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7776</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-275119013</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.18719v1.pdf" target="_blank">Using Large Language Models for Automated Grading of Student Writing about Science</a></p>
                <p><strong>Paper Abstract:</strong> Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7776.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7776.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large language model used to grade student writing, generate rubrics, and generate rubric-based grades under three prompting conditions; central LLM evaluated for agreement with human graders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / Astronomy (assessment of science writing)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>grading/evaluation (automated assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based grading under controlled prompt conditions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>GPT-4 was prompted in three ways (Prompt 1: instructor model answer only; Prompt 2: instructor model answer + instructor rubric; Prompt 3: instructor model answer + AI-generated rubric) to assign rubric-part scores and total grades to de-identified student responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with instructor grades measured via statistical tests, RMS differences, and ICC</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics include paired statistical test p-values, RMS difference (raw score units), and Intraclass Correlation Coefficient (ICC; scale 0–1 with higher = better agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MOOC student writing dataset: 120 learners' responses to 12 questions (10 responses per assignment) drawn from three Coursera courses</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Instructor blinded grader graded each sample using instructor rubrics; peer grades from Coursera (median of 3–4 peer graders per assignment) were available for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>When prompted with instructor answer + rubric (Prompt 2) or instructor answer + AI-generated rubric (Prompt 3) GPT-4 grades were not statistically different from instructor grades (p = 1.000). RMS differences with instructor were lowest for Prompt 2. Overall ICC among graders = 0.92 (95% CI: 0.89–0.94).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>GPT-4 grades (with rubric) approximately matched instructor grades and exceeded reliability of peer grading; GPT-4 without rubric behaved similarly to peer graders and differed significantly from instructor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Model version-sizes not stated; potential data contamination in LLMs not fully controlled; graders/instructor fallibility assumed; sample purposive and limited (120 responses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7776.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOC writing dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOC student writing dataset (Coursera: Astronomy, Astrobiology, History & Philosophy of Astronomy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>De-identified sample of 120 student written responses (10 per selected assignment) across 12 writing prompts from three Coursera MOOCs, together with peer grades and instructor grades used to evaluate LLM grading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / Astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>empirical dataset for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human vs LLM grading comparison on MOOC writing corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The dataset provided the ground truth instructor grades, peer median grades, and student submissions; these were used to compute agreement metrics (ICC, RMS, p-values) comparing LLM-assigned grades under different prompt conditions to instructor grades.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Normalized percentage scores, RMS differences, p-values from Friedman/Conover tests, ICC, bootstrap-derived standard deviations</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Scores were normalized to percentages; RMS difference reported in score units (raw points normalized), ICC reported 0–1, bootstrap standard deviations from 10,000 resamples.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>120 de-identified responses drawn from three Coursera MOOCs (Astronomy, Astrobiology, History & Philosophy of Astronomy)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Instructor graded blinded responses using rubrics; peer grading consisted of 3–4 peer graders per assignment with Coursera median used as peer grade.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset sampling was purposive to span peer-grade range; descriptive statistics and resampling (10,000 bootstrap iterations) were used throughout.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used as basis for comparing LLM grades to instructor and peer grades; LLM with rubric matched instructor more closely than peer median did.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Purposeful (non-random) sampling to increase score variance may introduce selection bias; MOOC population not fully representative of broader student populations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7776.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instructor rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instructor-provided grading rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analytical, part-scored rubrics created by course instructors and provided to both peer-graders and GPT-4 (Prompt 2); detailed breakdowns intended to aid consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>grading criteria/framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rubric-guided grading</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Rubrics specified part scores and qualitative criteria (organization, clarity, correctness, use of evidence) and were used by instructor, peers, and GPT-4 to assign part scores that sum to total grades.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Rubric part scores aggregated to totals and normalized to percentages; used to compute agreement with instructor</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Part scores on course-defined scales (e.g., 0–3, 0–4, 0–10) aggregated to assignment totals then normalized to percentages</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Rubrics attached to the 12 studied assignments (provided in Appendix A)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Rubrics designed for non-expert peer graders; instructor used same rubrics for blind grading; GPT-4 was given rubrics in Prompt 2 and used AI-generated rubrics in Prompt 3.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Use of instructor rubric in prompts reduced RMS difference vs instructor and removed statistically significant differences (p = 1.000) between GPT-4 and instructor grades.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>GPT-4 + instructor rubric ≈ instructor grading; GPT-4 with only model answer diverged (similar to peers).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Rubric interpretation may vary across graders and GPT-4; rubrics were originally designed for human peers, not LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7776.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-generated rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated grading rubrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rubrics produced by GPT-4 (Prompt 3) from instructor model answers and course context, then used by GPT-4 to grade student responses; tested as an alternative when instructor rubrics are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>grading criteria/framework generated by LLM</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-created rubric application</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>GPT-4 generated rubrics based on instructor model answers and course metadata, and those rubrics were used (by the LLM) to score student responses and compared to instructor- rubric-based grading.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with instructor grades (p-values, RMS differences), descriptive statistics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same aggregation: part scores aggregated to totals normalized to percentages; RMS and bootstrap sd used to quantify agreement</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AI-generated rubrics provided in Appendix B and applied across the 12 assignments dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human validation of AI rubrics beyond comparing resulting grades to instructor grades; instructor model answers provided as input.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Grades produced using AI-generated rubrics (Prompt 3) were not statistically different from instructor grades (p = 1.000) and improved over Prompt 1; RMS results similar to instructor-rubric condition in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-generated rubrics + instructor answers produced grading concordant with instructor rubric results, suggesting utility when human rubrics are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Quality of AI rubrics depends on model prompt and model knowledge; no independent human validation of rubric construct validity reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7776.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Friedman Test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Friedman's Test (non-parametric rank test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-parametric test used to assess differences among three graders (Instructor, Peer, LLM) across matched samples when assumptions of ANOVA were violated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (non-parametric inference)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical hypothesis test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Friedman test for differences among graders</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Rank-based test comparing repeated measures across three grading conditions; followed by Conover post-hoc pairwise comparisons with Bonferroni correction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Chi-square statistic and p-values from Friedman test; post-hoc pairwise p-values (Conover + Bonferroni)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>p-value significance thresholds (e.g., p < 0.05); pairwise corrected p-values reported</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Matched grades for 120 student responses across three graders/conditions</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Applied to normalized percentage grades per response; sample size limited per individual course so some per-course tests lacked power.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Friedman detected significant differences overall; post-hoc Conover tests showed instructor vs peer (p < 0.001) and instructor vs GPT-4 (Prompt 1) (p < 0.001) significant, but instructor vs GPT-4 when rubric included not significant (p = 1.000 after correction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Statistical test established that GPT-4 without rubric differed from instructor similarly to peer graders, while GPT-4 with rubric did not differ.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small per-course sample sizes reduced power for course-level Friedman tests; reliance on non-parametric ranks does not provide effect size in original score units.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7776.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conover + Bonferroni</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conover's post-hoc test with Bonferroni correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pairwise post-hoc comparisons used after the Friedman test with Bonferroni correction to control family-wise error when assessing differences between grader pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (multiple comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>post-hoc pairwise comparison procedure</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Conover post-hoc test with Bonferroni correction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Conover's rank-based pairwise comparisons performed after Friedman test; Bonferroni multiplicity correction applied to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Corrected p-values for pairwise comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Bonferroni-corrected p-values with significance threshold adjusted by number of comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Matched grader scores across sample of student responses</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Pairwise contrasts included Instructor vs Peer, Instructor vs GPT-4 (Prompts 1–3), and Peer vs GPT-4 conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Significant pairwise differences found for instructor vs peer and instructor vs GPT-4 (Prompt 1) (p < 0.001); no significant difference when rubric included (p = 1.000 after correction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Established specific conditions under which LLM grading differed or matched instructor grading.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Bonferroni correction is conservative and may reduce power; multiple testing decisions affect interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7776.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shapiro-Wilk & Levene</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shapiro-Wilk test (normality) & Levene's test (homogeneity of variances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assumption checks used to determine that parametric ANOVA assumptions were violated, motivating use of non-parametric tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (assumption testing)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>assumption validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Shapiro-Wilk for normality; Levene's for equal variances</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Shapiro-Wilk assessed distribution normality (p = 0.003 indicated non-normality); Levene's test assessed homogeneity of variances (p = 0.021 indicated heteroscedasticity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>p-values from Shapiro-Wilk and Levene's tests</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>p < 0.05 indicates violation of test assumption</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Normalized grade distributions across graders</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Tests run on full aggregated grade distributions prior to choosing Friedman/non-parametric approach.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Shapiro-Wilk p = 0.003 (reject normality); Levene's p = 0.021 (reject homogeneity), leading to use of non-parametric Friedman tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Assumption tests depend on sample size; non-normality and heteroscedasticity were handled by switching to rank-based tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7776.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrap resampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap resampling (Efron 1979)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-parametric resampling (10,000 iterations) used to estimate one-sigma standard deviations for mean scores and to compute p-values for small-sample comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics (resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bootstrap resampling (10,000 iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>10,000 bootstrap samples were drawn to estimate one-sigma standard deviations of mean scores and to compute p-values for differences in small samples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Bootstrap-derived standard errors and p-values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>One-sigma standard deviations derived from bootstrap distribution; p-values empirically estimated from resamples</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Per-question grade samples of 10 student responses (bootstrap used to increase inferential stability)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used to mimic MOOC setting and to provide uncertainty estimates despite small sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>One-sigma standard deviations for per-question averages computed via 10,000 bootstrap iterations; p-values for per-question comparisons all > 0.05 (not statistically significant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Bootstrap confirmed no significant differences in per-question average grades between GPT-4 (with rubric) and instructor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Bootstrap on small samples (n=10 per question) may still have limitations; purposeful sampling reduces representativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7776.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intraclass Correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intraclass Correlation Coefficient (ICC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measure of inter-rater reliability used to quantify agreement among graders (instructor, peer, LLM); reported as ICC = 0.92 indicating excellent agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics / Psychometrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>inter-rater reliability metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Intraclass Correlation Coefficient (Shrout & Fleiss, 1979)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ICC computed on the set of grades to assess consistency among graders; 95% confidence interval also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ICC with 95% confidence interval</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ICC ranges 0–1, with 0.75–0.90 good, >0.90 excellent; reported ICC = 0.92 (95% CI: 0.89–0.94).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>All grader scores across sampled assignments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>ICC encompassed instructor, peer median, and GPT-4 grades to quantify overall inter-rater reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ICC = 0.92 (95% CI: 0.89–0.94) indicating excellent agreement among graders overall.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>High ICC supports conclusion that GPT-4 grading (with rubric) is consistent with instructor grading and exceeds peer grading reliability in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>ICC reflects aggregate reliability; does not pinpoint systematic biases (e.g., LLM leniency) and assumes appropriate ICC model choice which was not exhaustively described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7776.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMS difference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Root Mean Square (RMS) difference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agreement metric computing RMS differences between instructor grades and GPT-4 grades across assignments to quantify typical deviation in score units.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics / Evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>agreement metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Root mean square (RMS) difference between instructor and LLM grades</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute RMS of (instructor grade - LLM grade) across items; lower RMS indicates closer agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>RMS values reported per course and per prompt condition</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>RMS in score units; reported examples: Prompt 1 RMS (Astronomy 0.54, Astrobiology 0.82, HPA 0.76); Prompt 2 RMS (0.38, 0.50, 0.48); Prompt 3 RMS (0.49, 0.37, 0.49).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Grades for assignments across three courses</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>RMS computed after normalizing scores to comparable scales; used to compare prompt conditions' closeness to instructor.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Smallest RMS differences observed when GPT-4 was given instructor answer + rubric (Prompt 2), indicating best agreement with instructor grading.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Quantitatively shows Prompt 2 yields closer scores to instructor than Prompt 1; Prompt 3 often similar to Prompt 2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>RMS conflates bias and variance; reported values aggregated across disparate assignment point scales normalized to percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7776.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mean absolute deviation (dispersion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Absolute Deviation of grader scores (per-question dispersion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used to quantify dispersion of peer grades and instructor grades per question; averages of per-question mean absolute deviations provided as error bars in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistics / Descriptive</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dispersion metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mean absolute deviation (MAD) across graders' scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each question, compute absolute deviations from the question mean and average them; then average these per-question deviations across questions to produce dispersion ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean absolute deviation expressed as percent points (e.g., peer dispersion ranges 10.8%–28.7% across courses)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>MAD in percentage points: peer dispersion reported (intro course 10.8%–19.6%, astrobiology 13.3%–28.7%, HPA 9%–21%); instructor dispersion ranges also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Per-question grade sets for sampled students</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Peer dispersion computed from medians and constituent peer scores; instructor dispersion from repeated instructor scores where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Peer graders generally showed larger dispersion than instructor and GPT-4 in many cases; exact ranges provided per course.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>MAD shows GPT-4 has smaller dispersion than peer grading and is closer to instructor dispersion for many assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MAD offers no directionality; small sample sizes per question (n=10) limit precision of dispersion estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7776.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt conditions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting conditions (Prompt 1, Prompt 2, Prompt 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three distinct LLM prompting frameworks used to evaluate how context affects GPT-4 grading: (1) model answer only, (2) model answer + instructor rubric, (3) model answer + AI-generated rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / NLP prompting methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>experimental prompting framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompt-based evaluation conditions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare GPT-4 grading behavior across three input conditions to determine the effect of rubric/context on agreement with instructor grades.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Statistical significance (Friedman & Conover), RMS, per-question means and CIs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Differences among conditions measured by p-values, RMS differences and bootstrap-derived SDs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same MOOC writing dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Prompts implemented as shown in Appendix C; GPT-4 produced grades and rubrics under these conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Prompt 2 (instructor answer + rubric) produced the best alignment with instructor grades; Prompt 1 matched peer graders and differed significantly from instructor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Explicit rubric in prompt crucial to match instructor grading; AI-generated rubric + model answer also effective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Prompt content and wording likely influence outcomes; prompts provided in appendices but reproducibility requires exact prompt texts and model versioning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7776.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claim–Evidence–Reasoning (CER)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claim–Evidence–Reasoning framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pedagogical framework proposed in discussion as a structured feedback format for LLM-generated reasoning and feedback for student science writing, not directly used in experiments but suggested for future human–LLM assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / Science pedagogy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feedback framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Claim–Evidence–Reasoning feedback structure</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Framework structures feedback as (1) the claim, (2) supporting evidence, and (3) reasoning linking evidence to claim; proposed to improve interpretability of LLM feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not applied in current study; suggested use for future LLM feedback assessments</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Suggested that LLMs might provide reasoning in CER form for formative feedback; no human evaluation details reported for CER in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Framework not tested here; effectiveness for LLM feedback remains to be empirically evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7776.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7776.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bloom's taxonomy levels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom's taxonomy (revised) levels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used in discussion to characterize which cognitive levels are amenable to LLM grading (Remember, Understand, Apply, Analyze, Evaluate) versus more challenging (Create).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Education / Cognitive taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation suitability framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mapping assignment cognitive level to LLM grading suitability</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Authors map assignment types to Bloom levels, suggesting LLMs can reliably assess up to Evaluate but struggle with Create-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative recommendation: LLMs suitable for first five levels of Bloom's taxonomy; Create-level tasks more challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Qualitative claim not empirically tested in this study; needs targeted experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models as MOOCs Graders <em>(Rating: 2)</em></li>
                <li>Time Travel in LLMs: Tracing Data Contamination in Large Language Models <em>(Rating: 2)</em></li>
                <li>Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models <em>(Rating: 2)</em></li>
                <li>Intraclass correlations: uses in assessing rater reliability <em>(Rating: 2)</em></li>
                <li>Bootstrap Methods: Another Look at the Jackknife <em>(Rating: 2)</em></li>
                <li>Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric Scores Determined? <em>(Rating: 2)</em></li>
                <li>C-rater: Automated scoring of short-answer questions <em>(Rating: 1)</em></li>
                <li>Practical nonparametric statistics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7776",
    "paper_id": "paper-275119013",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "The large language model used to grade student writing, generate rubrics, and generate rubric-based grades under three prompting conditions; central LLM evaluated for agreement with human graders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Education / Astronomy (assessment of science writing)",
            "theory_type": "grading/evaluation (automated assessment)",
            "evaluation_method_name": "LLM-based grading under controlled prompt conditions",
            "evaluation_method_description": "GPT-4 was prompted in three ways (Prompt 1: instructor model answer only; Prompt 2: instructor model answer + instructor rubric; Prompt 3: instructor model answer + AI-generated rubric) to assign rubric-part scores and total grades to de-identified student responses.",
            "evaluation_metric": "Agreement with instructor grades measured via statistical tests, RMS differences, and ICC",
            "metric_definition": "Metrics include paired statistical test p-values, RMS difference (raw score units), and Intraclass Correlation Coefficient (ICC; scale 0–1 with higher = better agreement)",
            "dataset_or_benchmark": "MOOC student writing dataset: 120 learners' responses to 12 questions (10 responses per assignment) drawn from three Coursera courses",
            "human_evaluation_details": "Instructor blinded grader graded each sample using instructor rubrics; peer grades from Coursera (median of 3–4 peer graders per assignment) were available for comparison.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "When prompted with instructor answer + rubric (Prompt 2) or instructor answer + AI-generated rubric (Prompt 3) GPT-4 grades were not statistically different from instructor grades (p = 1.000). RMS differences with instructor were lowest for Prompt 2. Overall ICC among graders = 0.92 (95% CI: 0.89–0.94).",
            "comparison_to_human_generated": true,
            "comparison_results": "GPT-4 grades (with rubric) approximately matched instructor grades and exceeded reliability of peer grading; GPT-4 without rubric behaved similarly to peer graders and differed significantly from instructor.",
            "limitations_noted": "Model version-sizes not stated; potential data contamination in LLMs not fully controlled; graders/instructor fallibility assumed; sample purposive and limited (120 responses).",
            "uuid": "e7776.0",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MOOC writing dataset",
            "name_full": "MOOC student writing dataset (Coursera: Astronomy, Astrobiology, History & Philosophy of Astronomy)",
            "brief_description": "De-identified sample of 120 student written responses (10 per selected assignment) across 12 writing prompts from three Coursera MOOCs, together with peer grades and instructor grades used to evaluate LLM grading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Education / Astronomy",
            "theory_type": "empirical dataset for evaluation",
            "evaluation_method_name": "Human vs LLM grading comparison on MOOC writing corpus",
            "evaluation_method_description": "The dataset provided the ground truth instructor grades, peer median grades, and student submissions; these were used to compute agreement metrics (ICC, RMS, p-values) comparing LLM-assigned grades under different prompt conditions to instructor grades.",
            "evaluation_metric": "Normalized percentage scores, RMS differences, p-values from Friedman/Conover tests, ICC, bootstrap-derived standard deviations",
            "metric_definition": "Scores were normalized to percentages; RMS difference reported in score units (raw points normalized), ICC reported 0–1, bootstrap standard deviations from 10,000 resamples.",
            "dataset_or_benchmark": "120 de-identified responses drawn from three Coursera MOOCs (Astronomy, Astrobiology, History & Philosophy of Astronomy)",
            "human_evaluation_details": "Instructor graded blinded responses using rubrics; peer grading consisted of 3–4 peer graders per assignment with Coursera median used as peer grade.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Dataset sampling was purposive to span peer-grade range; descriptive statistics and resampling (10,000 bootstrap iterations) were used throughout.",
            "comparison_to_human_generated": true,
            "comparison_results": "Used as basis for comparing LLM grades to instructor and peer grades; LLM with rubric matched instructor more closely than peer median did.",
            "limitations_noted": "Purposeful (non-random) sampling to increase score variance may introduce selection bias; MOOC population not fully representative of broader student populations.",
            "uuid": "e7776.1",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Instructor rubrics",
            "name_full": "Instructor-provided grading rubrics",
            "brief_description": "Analytical, part-scored rubrics created by course instructors and provided to both peer-graders and GPT-4 (Prompt 2); detailed breakdowns intended to aid consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Education / Assessment",
            "theory_type": "grading criteria/framework",
            "evaluation_method_name": "Rubric-guided grading",
            "evaluation_method_description": "Rubrics specified part scores and qualitative criteria (organization, clarity, correctness, use of evidence) and were used by instructor, peers, and GPT-4 to assign part scores that sum to total grades.",
            "evaluation_metric": "Rubric part scores aggregated to totals and normalized to percentages; used to compute agreement with instructor",
            "metric_definition": "Part scores on course-defined scales (e.g., 0–3, 0–4, 0–10) aggregated to assignment totals then normalized to percentages",
            "dataset_or_benchmark": "Rubrics attached to the 12 studied assignments (provided in Appendix A)",
            "human_evaluation_details": "Rubrics designed for non-expert peer graders; instructor used same rubrics for blind grading; GPT-4 was given rubrics in Prompt 2 and used AI-generated rubrics in Prompt 3.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Use of instructor rubric in prompts reduced RMS difference vs instructor and removed statistically significant differences (p = 1.000) between GPT-4 and instructor grades.",
            "comparison_to_human_generated": true,
            "comparison_results": "GPT-4 + instructor rubric ≈ instructor grading; GPT-4 with only model answer diverged (similar to peers).",
            "limitations_noted": "Rubric interpretation may vary across graders and GPT-4; rubrics were originally designed for human peers, not LLMs.",
            "uuid": "e7776.2",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "AI-generated rubrics",
            "name_full": "LLM-generated grading rubrics",
            "brief_description": "Rubrics produced by GPT-4 (Prompt 3) from instructor model answers and course context, then used by GPT-4 to grade student responses; tested as an alternative when instructor rubrics are unavailable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Education / Assessment",
            "theory_type": "grading criteria/framework generated by LLM",
            "evaluation_method_name": "LLM-created rubric application",
            "evaluation_method_description": "GPT-4 generated rubrics based on instructor model answers and course metadata, and those rubrics were used (by the LLM) to score student responses and compared to instructor- rubric-based grading.",
            "evaluation_metric": "Agreement with instructor grades (p-values, RMS differences), descriptive statistics",
            "metric_definition": "Same aggregation: part scores aggregated to totals normalized to percentages; RMS and bootstrap sd used to quantify agreement",
            "dataset_or_benchmark": "AI-generated rubrics provided in Appendix B and applied across the 12 assignments dataset",
            "human_evaluation_details": "No human validation of AI rubrics beyond comparing resulting grades to instructor grades; instructor model answers provided as input.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Grades produced using AI-generated rubrics (Prompt 3) were not statistically different from instructor grades (p = 1.000) and improved over Prompt 1; RMS results similar to instructor-rubric condition in many cases.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-generated rubrics + instructor answers produced grading concordant with instructor rubric results, suggesting utility when human rubrics are absent.",
            "limitations_noted": "Quality of AI rubrics depends on model prompt and model knowledge; no independent human validation of rubric construct validity reported.",
            "uuid": "e7776.3",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Friedman Test",
            "name_full": "Friedman's Test (non-parametric rank test)",
            "brief_description": "Non-parametric test used to assess differences among three graders (Instructor, Peer, LLM) across matched samples when assumptions of ANOVA were violated.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics (non-parametric inference)",
            "theory_type": "statistical hypothesis test",
            "evaluation_method_name": "Friedman test for differences among graders",
            "evaluation_method_description": "Rank-based test comparing repeated measures across three grading conditions; followed by Conover post-hoc pairwise comparisons with Bonferroni correction.",
            "evaluation_metric": "Chi-square statistic and p-values from Friedman test; post-hoc pairwise p-values (Conover + Bonferroni)",
            "metric_definition": "p-value significance thresholds (e.g., p &lt; 0.05); pairwise corrected p-values reported",
            "dataset_or_benchmark": "Matched grades for 120 student responses across three graders/conditions",
            "human_evaluation_details": "Applied to normalized percentage grades per response; sample size limited per individual course so some per-course tests lacked power.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Friedman detected significant differences overall; post-hoc Conover tests showed instructor vs peer (p &lt; 0.001) and instructor vs GPT-4 (Prompt 1) (p &lt; 0.001) significant, but instructor vs GPT-4 when rubric included not significant (p = 1.000 after correction).",
            "comparison_to_human_generated": true,
            "comparison_results": "Statistical test established that GPT-4 without rubric differed from instructor similarly to peer graders, while GPT-4 with rubric did not differ.",
            "limitations_noted": "Small per-course sample sizes reduced power for course-level Friedman tests; reliance on non-parametric ranks does not provide effect size in original score units.",
            "uuid": "e7776.4",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Conover + Bonferroni",
            "name_full": "Conover's post-hoc test with Bonferroni correction",
            "brief_description": "Pairwise post-hoc comparisons used after the Friedman test with Bonferroni correction to control family-wise error when assessing differences between grader pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics (multiple comparisons)",
            "theory_type": "post-hoc pairwise comparison procedure",
            "evaluation_method_name": "Conover post-hoc test with Bonferroni correction",
            "evaluation_method_description": "Conover's rank-based pairwise comparisons performed after Friedman test; Bonferroni multiplicity correction applied to reduce false positives.",
            "evaluation_metric": "Corrected p-values for pairwise comparisons",
            "metric_definition": "Bonferroni-corrected p-values with significance threshold adjusted by number of comparisons",
            "dataset_or_benchmark": "Matched grader scores across sample of student responses",
            "human_evaluation_details": "Pairwise contrasts included Instructor vs Peer, Instructor vs GPT-4 (Prompts 1–3), and Peer vs GPT-4 conditions.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Significant pairwise differences found for instructor vs peer and instructor vs GPT-4 (Prompt 1) (p &lt; 0.001); no significant difference when rubric included (p = 1.000 after correction).",
            "comparison_to_human_generated": true,
            "comparison_results": "Established specific conditions under which LLM grading differed or matched instructor grading.",
            "limitations_noted": "Bonferroni correction is conservative and may reduce power; multiple testing decisions affect interpretability.",
            "uuid": "e7776.5",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Shapiro-Wilk & Levene",
            "name_full": "Shapiro-Wilk test (normality) & Levene's test (homogeneity of variances)",
            "brief_description": "Assumption checks used to determine that parametric ANOVA assumptions were violated, motivating use of non-parametric tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics (assumption testing)",
            "theory_type": "assumption validation",
            "evaluation_method_name": "Shapiro-Wilk for normality; Levene's for equal variances",
            "evaluation_method_description": "Shapiro-Wilk assessed distribution normality (p = 0.003 indicated non-normality); Levene's test assessed homogeneity of variances (p = 0.021 indicated heteroscedasticity).",
            "evaluation_metric": "p-values from Shapiro-Wilk and Levene's tests",
            "metric_definition": "p &lt; 0.05 indicates violation of test assumption",
            "dataset_or_benchmark": "Normalized grade distributions across graders",
            "human_evaluation_details": "Tests run on full aggregated grade distributions prior to choosing Friedman/non-parametric approach.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Shapiro-Wilk p = 0.003 (reject normality); Levene's p = 0.021 (reject homogeneity), leading to use of non-parametric Friedman tests.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Assumption tests depend on sample size; non-normality and heteroscedasticity were handled by switching to rank-based tests.",
            "uuid": "e7776.6",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Bootstrap resampling",
            "name_full": "Bootstrap resampling (Efron 1979)",
            "brief_description": "Non-parametric resampling (10,000 iterations) used to estimate one-sigma standard deviations for mean scores and to compute p-values for small-sample comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics (resampling)",
            "theory_type": "uncertainty estimation",
            "evaluation_method_name": "Bootstrap resampling (10,000 iterations)",
            "evaluation_method_description": "10,000 bootstrap samples were drawn to estimate one-sigma standard deviations of mean scores and to compute p-values for differences in small samples.",
            "evaluation_metric": "Bootstrap-derived standard errors and p-values",
            "metric_definition": "One-sigma standard deviations derived from bootstrap distribution; p-values empirically estimated from resamples",
            "dataset_or_benchmark": "Per-question grade samples of 10 student responses (bootstrap used to increase inferential stability)",
            "human_evaluation_details": "Used to mimic MOOC setting and to provide uncertainty estimates despite small sample sizes.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "One-sigma standard deviations for per-question averages computed via 10,000 bootstrap iterations; p-values for per-question comparisons all &gt; 0.05 (not statistically significant).",
            "comparison_to_human_generated": true,
            "comparison_results": "Bootstrap confirmed no significant differences in per-question average grades between GPT-4 (with rubric) and instructor.",
            "limitations_noted": "Bootstrap on small samples (n=10 per question) may still have limitations; purposeful sampling reduces representativeness.",
            "uuid": "e7776.7",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Intraclass Correlation",
            "name_full": "Intraclass Correlation Coefficient (ICC)",
            "brief_description": "Measure of inter-rater reliability used to quantify agreement among graders (instructor, peer, LLM); reported as ICC = 0.92 indicating excellent agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics / Psychometrics",
            "theory_type": "inter-rater reliability metric",
            "evaluation_method_name": "Intraclass Correlation Coefficient (Shrout & Fleiss, 1979)",
            "evaluation_method_description": "ICC computed on the set of grades to assess consistency among graders; 95% confidence interval also reported.",
            "evaluation_metric": "ICC with 95% confidence interval",
            "metric_definition": "ICC ranges 0–1, with 0.75–0.90 good, &gt;0.90 excellent; reported ICC = 0.92 (95% CI: 0.89–0.94).",
            "dataset_or_benchmark": "All grader scores across sampled assignments",
            "human_evaluation_details": "ICC encompassed instructor, peer median, and GPT-4 grades to quantify overall inter-rater reliability.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ICC = 0.92 (95% CI: 0.89–0.94) indicating excellent agreement among graders overall.",
            "comparison_to_human_generated": true,
            "comparison_results": "High ICC supports conclusion that GPT-4 grading (with rubric) is consistent with instructor grading and exceeds peer grading reliability in aggregate.",
            "limitations_noted": "ICC reflects aggregate reliability; does not pinpoint systematic biases (e.g., LLM leniency) and assumes appropriate ICC model choice which was not exhaustively described.",
            "uuid": "e7776.8",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RMS difference",
            "name_full": "Root Mean Square (RMS) difference",
            "brief_description": "Agreement metric computing RMS differences between instructor grades and GPT-4 grades across assignments to quantify typical deviation in score units.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics / Evaluation metrics",
            "theory_type": "agreement metric",
            "evaluation_method_name": "Root mean square (RMS) difference between instructor and LLM grades",
            "evaluation_method_description": "Compute RMS of (instructor grade - LLM grade) across items; lower RMS indicates closer agreement.",
            "evaluation_metric": "RMS values reported per course and per prompt condition",
            "metric_definition": "RMS in score units; reported examples: Prompt 1 RMS (Astronomy 0.54, Astrobiology 0.82, HPA 0.76); Prompt 2 RMS (0.38, 0.50, 0.48); Prompt 3 RMS (0.49, 0.37, 0.49).",
            "dataset_or_benchmark": "Grades for assignments across three courses",
            "human_evaluation_details": "RMS computed after normalizing scores to comparable scales; used to compare prompt conditions' closeness to instructor.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Smallest RMS differences observed when GPT-4 was given instructor answer + rubric (Prompt 2), indicating best agreement with instructor grading.",
            "comparison_to_human_generated": true,
            "comparison_results": "Quantitatively shows Prompt 2 yields closer scores to instructor than Prompt 1; Prompt 3 often similar to Prompt 2.",
            "limitations_noted": "RMS conflates bias and variance; reported values aggregated across disparate assignment point scales normalized to percentages.",
            "uuid": "e7776.9",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Mean absolute deviation (dispersion)",
            "name_full": "Mean Absolute Deviation of grader scores (per-question dispersion)",
            "brief_description": "Used to quantify dispersion of peer grades and instructor grades per question; averages of per-question mean absolute deviations provided as error bars in figures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Statistics / Descriptive",
            "theory_type": "dispersion metric",
            "evaluation_method_name": "Mean absolute deviation (MAD) across graders' scores",
            "evaluation_method_description": "For each question, compute absolute deviations from the question mean and average them; then average these per-question deviations across questions to produce dispersion ranges.",
            "evaluation_metric": "Mean absolute deviation expressed as percent points (e.g., peer dispersion ranges 10.8%–28.7% across courses)",
            "metric_definition": "MAD in percentage points: peer dispersion reported (intro course 10.8%–19.6%, astrobiology 13.3%–28.7%, HPA 9%–21%); instructor dispersion ranges also reported.",
            "dataset_or_benchmark": "Per-question grade sets for sampled students",
            "human_evaluation_details": "Peer dispersion computed from medians and constituent peer scores; instructor dispersion from repeated instructor scores where applicable.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Peer graders generally showed larger dispersion than instructor and GPT-4 in many cases; exact ranges provided per course.",
            "comparison_to_human_generated": true,
            "comparison_results": "MAD shows GPT-4 has smaller dispersion than peer grading and is closer to instructor dispersion for many assignments.",
            "limitations_noted": "MAD offers no directionality; small sample sizes per question (n=10) limit precision of dispersion estimates.",
            "uuid": "e7776.10",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Prompt conditions",
            "name_full": "Prompting conditions (Prompt 1, Prompt 2, Prompt 3)",
            "brief_description": "Three distinct LLM prompting frameworks used to evaluate how context affects GPT-4 grading: (1) model answer only, (2) model answer + instructor rubric, (3) model answer + AI-generated rubric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "scientific_domain": "Education / NLP prompting methodology",
            "theory_type": "experimental prompting framework",
            "evaluation_method_name": "Prompt-based evaluation conditions",
            "evaluation_method_description": "Compare GPT-4 grading behavior across three input conditions to determine the effect of rubric/context on agreement with instructor grades.",
            "evaluation_metric": "Statistical significance (Friedman & Conover), RMS, per-question means and CIs",
            "metric_definition": "Differences among conditions measured by p-values, RMS differences and bootstrap-derived SDs",
            "dataset_or_benchmark": "Same MOOC writing dataset",
            "human_evaluation_details": "Prompts implemented as shown in Appendix C; GPT-4 produced grades and rubrics under these conditions.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Prompt 2 (instructor answer + rubric) produced the best alignment with instructor grades; Prompt 1 matched peer graders and differed significantly from instructor.",
            "comparison_to_human_generated": true,
            "comparison_results": "Explicit rubric in prompt crucial to match instructor grading; AI-generated rubric + model answer also effective.",
            "limitations_noted": "Prompt content and wording likely influence outcomes; prompts provided in appendices but reproducibility requires exact prompt texts and model versioning.",
            "uuid": "e7776.11",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Claim–Evidence–Reasoning (CER)",
            "name_full": "Claim–Evidence–Reasoning framework",
            "brief_description": "Pedagogical framework proposed in discussion as a structured feedback format for LLM-generated reasoning and feedback for student science writing, not directly used in experiments but suggested for future human–LLM assessments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Education / Science pedagogy",
            "theory_type": "feedback framework",
            "evaluation_method_name": "Claim–Evidence–Reasoning feedback structure",
            "evaluation_method_description": "Framework structures feedback as (1) the claim, (2) supporting evidence, and (3) reasoning linking evidence to claim; proposed to improve interpretability of LLM feedback.",
            "evaluation_metric": "Not applied in current study; suggested use for future LLM feedback assessments",
            "metric_definition": null,
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Suggested that LLMs might provide reasoning in CER form for formative feedback; no human evaluation details reported for CER in this study.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Framework not tested here; effectiveness for LLM feedback remains to be empirically evaluated.",
            "uuid": "e7776.12",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Bloom's taxonomy levels",
            "name_full": "Bloom's taxonomy (revised) levels",
            "brief_description": "Used in discussion to characterize which cognitive levels are amenable to LLM grading (Remember, Understand, Apply, Analyze, Evaluate) versus more challenging (Create).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Education / Cognitive taxonomy",
            "theory_type": "evaluation suitability framework",
            "evaluation_method_name": "Mapping assignment cognitive level to LLM grading suitability",
            "evaluation_method_description": "Authors map assignment types to Bloom levels, suggesting LLMs can reliably assess up to Evaluate but struggle with Create-level tasks.",
            "evaluation_metric": null,
            "metric_definition": null,
            "dataset_or_benchmark": null,
            "human_evaluation_details": null,
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Qualitative recommendation: LLMs suitable for first five levels of Bloom's taxonomy; Create-level tasks more challenging.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Qualitative claim not empirically tested in this study; needs targeted experiments.",
            "uuid": "e7776.13",
            "source_info": {
                "paper_title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models as MOOCs Graders",
            "rating": 2,
            "sanitized_title": "large_language_models_as_moocs_graders"
        },
        {
            "paper_title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
            "rating": 2,
            "sanitized_title": "time_travel_in_llms_tracing_data_contamination_in_large_language_models"
        },
        {
            "paper_title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "rating": 2,
            "sanitized_title": "data_contamination_quiz_a_tool_to_detect_and_estimate_contamination_in_large_language_models"
        },
        {
            "paper_title": "Intraclass correlations: uses in assessing rater reliability",
            "rating": 2,
            "sanitized_title": "intraclass_correlations_uses_in_assessing_rater_reliability"
        },
        {
            "paper_title": "Bootstrap Methods: Another Look at the Jackknife",
            "rating": 2,
            "sanitized_title": "bootstrap_methods_another_look_at_the_jackknife"
        },
        {
            "paper_title": "Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric Scores Determined?",
            "rating": 2,
            "sanitized_title": "automated_essay_scoring_and_the_deep_learning_black_box_how_are_rubric_scores_determined"
        },
        {
            "paper_title": "C-rater: Automated scoring of short-answer questions",
            "rating": 1,
            "sanitized_title": "crater_automated_scoring_of_shortanswer_questions"
        },
        {
            "paper_title": "Practical nonparametric statistics",
            "rating": 1,
            "sanitized_title": "practical_nonparametric_statistics"
        }
    ],
    "cost": 0.02232125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Using Large Language Models for Automated Grading of Student Writing about Science</p>
<p>Chris Impey cimpey@as.arizona.edu 
Department of Astronomy
University of Arizona
85721TucsonAZUnited States</p>
<p>Matthew Wenger 
Department of Astronomy
University of Arizona
85721TucsonAZUnited States</p>
<p>Nikhil Garuda 
Department of Astronomy
University of Arizona
85721TucsonAZUnited States</p>
<p>Shahriar Golchin 
Department of Computer Science
University of Arizona
85721TucsonAZUnited States</p>
<p>Sarah Stamer 
Department of Astronomy
University of Arizona
85721TucsonAZUnited States</p>
<p>Using Large Language Models for Automated Grading of Student Writing about Science
0BCAE3CD1A9B1468A099A74762A9A840student writingscience classesonline educationassessmentmachine learninglarge language models
Assessing writing in large classes for formal or informal learners presents a significant challenge.Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer.The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing.An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy.The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera.One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy.The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar.The data comprised answers from 120 students to 12 questions across the three courses.GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses.In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics.Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses.The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) has had a profound effect on diverse fields (Ryan, 2023;Shamshiri et al., 2024;Thiranavukasaru et al., 2023).AI has also impacted education at every level (Zhang &amp; Aslan, 2021).College leaders see both promise and peril in this disruptive technology.In one survey of college leaders' opinions about Generative AI, 78% agreed that the tools offer an opportunity to improve how colleges educate, operate, and conduct research, but 57% also thought that the same tools pose a threat to how colleges educate, operate, and conduct research (Anft, 2023).Recently, large language models (LLMs) and tools such as ChatGPT (Ouyang et al., 2022) have shown great potential to help students learn but have also led to concerns about plagiarism and a degradation in the ability of students to write and synthesize information (Grassini, 2023).A complex typology of AI's capabilities affects every aspect of education, from tutoring and assessment to the way institutions admit students and identify those who are at risk (Holmes &amp; Tuomi, 2022).The literature discussing LLMs in the classroom has mostly focused on instructors and students using them to generate educational content (Kasneci et al., 2023).In 2023, Khan Academy and OpenAI announced a partnership using GPT-4 as a learning assistant tool to facilitate student learning (OpenAI, 2023;Khan, 2023).Students will be able to ask questions about content as they would an instructor.More sophisticated AI assistants for students (and instructors) are anticipated in the future.</p>
<p>We embarked on a project to see if LLMs could be useful in massive open online courses, or MOOCs.MOOCs are typically free and are open to anyone in the world who has access to a computer and the Internet.In their first ten years, MOOCs have grown to nearly 20,000 courses, offered by 950 universities, and serving 220 million students worldwide (Shah, 2021).MOOCs are of interest to researchers because they are an informal learning environment where people can learn about science without enrolling in a university class, particularly adult learners (Falk and Needham, 2013).Although MOOCs resemble formal classes, with video lectures, quizzes, and activities, the learning is in a selfdirected environment guided by individual needs and interests (Oakley &amp; Sejnowski, 2019).Unlike the college setting, learners do not get grades or transferable credit and classes do not typically contribute to a degree program.MOOCs have an international audience that encompasses many developing countries, so they can play an important role in the democratization of education (Impey, 2020).The current study builds on prior work examining peer assessment in MOOCs.Although Formanek et al. (2017) showed that participation in peer grading is correlated with student engagement and course completion, it has also been found that peer grading can be inconsistent and there are problems with reliability and validity (Formanek et al., 2019;Gamage, Staubitz, &amp; Whiting, 2021;Usher &amp; Barak, 2018;Yousef, &amp; Sumner, 2021).</p>
<p>This study investigates whether an existing top performing LLM can equal or surpass peer grading in reliability relative to an instructor (Bojic, Kovacevic, &amp; Caparkapa, 2023).Instructor model answers are one input, but another is an instructor-generated rubric, since predicting rubric scores has been found to be essential to automated essay grading (Kumar &amp; Boulanger, 2020).We also investigate whether LLMs can be effective at generating grading rubrics.This can be useful for pre-existing writing assignments where no rubric is provided.If LLMs can approach instructor reliability, they can potentially be used in low-stakes MOOCs with tens of thousands of learners, where grading by peers is a burden and grading by a human instructor is essentially impossible.</p>
<p>Among the three courses included in this study, writing assignments for introductory astronomy and astrobiology are the easiest for a human or a machine to evaluate because the assignments are content based.The course on the history and philosophy of astronomy is more challenging because answers to some of the questions depend on speculation or hypothetical situations, where the judgement can be subjective (Golchin et al. 2024, Impey, 2023).</p>
<p>We address the following research questions: can the LLMs (1) generate a grade comparable to that of an instructor, (2) match or exceed the reliability of peer grading, and (3) create a grading rubric that will produce LLM grades comparable to that of an instructor?This work is intended to apply to writing assignments on a variety of topics and act as a proof-of-concept to explore whether LLMs can reliably evaluate science writing and whether automated grading can be scaled to many thousands of students in online classes.</p>
<p>Previous Work</p>
<p>Keeping online learners motivated and involved is far more difficult than in a face-to-face class (Martin &amp; Borup, 2022).MOOCs present a particular challenge since there is little opportunity for direct, real-time interaction with other students or the instructor.Prior research shows that MOOC completion rates are persistently low (Jung &amp; Lee 2018;Onah, Sinclair, &amp; Boyatt, 2014;Reich &amp; Ruipérez-Valiente, 2019;Wang &amp; Baker, 2015).Course design and pedagogy can play a role in improving these results.Active learning through lab activities and writing assignments has been shown to increase MOOC engagement and completion rates (Formanek et al., 2019;Shah et al., 2022).In addition to being connected with course engagement and completion, writing assignments promote learning.Suitably designed writing assignments can enhance science literacy (Hand et al., 1999) and can be an effective way to improve student reasoning skills and critical thinking (Graham, Kiuhara, &amp; MacKay, 2020;Libarkin &amp; Ording, 2012;Quitadamo &amp; Kurtz, 2007).Unfortunately, assessing, grading, and providing feedback on writing assignments in large classes is notoriously difficult.At the scale of MOOCs, it is impractical for instructors to grade thousands of writing assignments.</p>
<p>The current solution to the problem of scale is peer-grading.In the Coursera system, every writing assignment is graded by multiple other randomly selected students in the same course, using an instructor-provided rubric.Students are allowed to grade peer assignments only after they have submitted their own, and final grades are determined by taking the median of the peer grades.Although peergrading does have some positive effects, such as improving learner engagement and motivation, as a grading methodology it often has limited reliability and validity (Formanek et al., 2017;Formanek et al., 2019;Gamage et al., 2021;Usher &amp; Barak, 2018;Yousef, &amp; Sumner, 2021).A previous study analyzed peer grading for the astronomy course used here (Formanek et al., 2017).In that study, using data from 2015, peer review for 300 assignments out of 4 points had a mean score of 3.39, slightly lower than the instructor mean score, with a standard deviation of 0.78.In terms of reliability, unsurprisingly, instructor grades were the most reliable, followed by trained undergraduate graders, followed by peer graders.The correlation between the median instructor grades and the median peer grades was moderate (r = 0.49).</p>
<p>Automatic grading of writing assignments using machine learning is another possible solution to the problem of grading at scale.There is evidence that MOOCs using automated grading systems have higher completion rates (Kruchinin, 2019).AI techniques are beginning to be used in MOOCs since they can readily be scaled for many thousands of learners.The use of machine learning to grade or assist with the grading of short answers has been attempted before using a variety of methods (Borad &amp; Netak, 2021;Leacock &amp; Chodorow, 2003;Weegar &amp; Idestam-Almquist, 2024).A literature review in 2020 found twenty papers using AI for the assessment of students (Sanchez-Prieto et al., 2020).Four analyzed student behaviors, six investigated student feelings or sentiments, and ten assessed student achievement through AI-based methods.Among the ten, the focus was grading multiple-choice tests, lab exercises, concept maps, and short-answer questions.None investigated longer writing assignments, as we do in this work.Recently, GPT-3.5 (Ye et al., 2023) has been shown to have an accuracy of 65-95% for grading multiplechoice tests across ten different science topics (Alseddiqi et al., 2023).This variability in performance can be attributed to potential data contamination issues in LLMs or their inherent probabilistic behavior during text generation (Golchin &amp; Surdeanu, 2023a, b).One study has used LLMs to validate peerassigned essay scores in a Coursera MOOC (Morris et al., 2023).In a recent study, the BERT language model was effective in evaluating writing based on grammar, semantics, coherence, and prompt relevance (Vanga et al., 2023).Another study using the RoBERTa language model shows that this language model could outperform human raters (Beseiso et al., 2021).</p>
<p>Methods</p>
<p>Research Context</p>
<p>This research uses data gathered from three MOOCs.Our education research group has been offering MOOCs through Coursera since 2013 (Impey, Wenger, &amp; Austin, 2015).The classes utilized in this study are on the topics of astronomy (Impey et al., 2016), astrobiology (Impey, Wenger, &amp; Riabokin, 2023), and the history and philosophy of astronomy (Impey, 2023).Respectively, they are titled "Astronomy: Exploring Time and Space," "Astrobiology: Exploring Other Worlds," and "Knowing the Universe: History and Philosophy of Astronomy."Together, they have enrolled nearly 225,000 learners in 190 countries.All three courses share a similar format: weekly modules that contain both video lecture content and supplemental documents including lecture slides.The assessments include automatically graded multiple-choice quizzes using the quiz system provided by Coursera as well as peer-graded writing assignments.These classes are available for free and are aimed at learners who have no background in science.They are designed for a high-school or introductory college-level audience.</p>
<p>The first course, "Astronomy: Exploring Time and Space," was launched in 2013.It is an introductorylevel survey of astronomy that covers topics including history, the night sky, telescopes, light, the Solar System, stars, galaxies, and life in the universe.It is the longest course with 11 modules that contain 18 hours of video lectures and lecture slides, as well as optional supplementary materials such as articles about relevant topics and podcast episodes.Assessments include quizzes, five peer-graded writing assignments at the end of modules 2, 4, 7, 8, and 10, and a project at the end of week 9.There are currently 194,032 learners enrolled, and 9,655 have completed the course.</p>
<p>The second course, "Astrobiology: Exploring Other Worlds," was launched in 2019 and is an introductory-level course that covers topics including planet formation, exoplanet detection, habitability, life, biology, and the search for extraterrestrial intelligence.It has six modules that contain 10 hours of video lectures and lecture slides.Assessments include short quizzes after each lecture video, three peergraded writing assignments at the end of modules 2, 3, and 5, and a final written and peer-graded project at the end of week 6.There are currently 26,337 learners enrolled, and 1,302 have completed the course.</p>
<p>The third course, "Knowing the Universe: History and Philosophy of Astronomy," was launched in 2022.It is an introductory-level course that covers topics including prehistory, the advent of science, world cultures and the development of observational astronomy, the development of modern science, modern physics and relativity, cosmology, and life beyond Earth.It has eight modules that contain 18 hours of video lectures and lecture slides.Assessments include quizzes and seven short peer-graded writing assignments at the end of modules 2, 3, 4, 5, 6, 7, and 8.Each writing assignment has three question options from which students must select one.There are currently 3,916 learners enrolled, and 181 have completed the course.</p>
<p>The writing assignments in each class were designed independently and emphasized slightly different criteria.The questions for Introductory Astronomy and Astrobiology focused on applying and analyzing course materials and responding with accurate scientific answers.The questions for the History and Philosophy course are more open-ended and allow students more flexibility in how to answer.Writing assignments for all three courses asked students to organize their thoughts, state their ideas clearly, and to use supporting evidence.Students are provided with instructions that include a writing prompt and the criteria that will be used to grade the assignments.These criteria were then used in the grading rubrics for scoring peer assignments.Rubrics were designed with non-expert peer-graders in mind and included several features which have been found to improve the consistency of scoring (Jonsson &amp; Svingby, 2007).The rubrics for these courses are generally analytical and the Introductory Astronomy and Astrobiology questions are topic specific while those for the History and Philosophy class are more general to account for a variety of topics and responses.For more subjective criteria (organization, clarity) the scale was kept small in an attempt to produce more consistent scores.Often these criteria are scored as present or not, with little requirement for graders to judge the level of each criterion.To the extent possible, a detailed breakdown for how points should be allocated was provided in the rubric.The text for all writing assignment questions and grading rubrics used in this research study are provided in Appendix A. All three courses use the same built-in Coursera peer-grading system.This research was overseen by the Institutional Review Board at our university.Students in these courses completed an optional survey at the beginning of each class that includes questions about demographics as well as their consent to participate, or not, in our research.Data was gathered only from students who both completed the survey and agreed to participate in our research study.All answers were de-identified prior to instructor and LLM grading.While attempts were made to reduce bias in the sampling process, we acknowledge that these results are likely not to be representative of the global population.They are, however, representative of Coursera learners.Most learners on Coursera are between the ages of 24 and 44 and a majority of students in our classes are male.The largest difference in gender was in the History and Philosophy course (34% female vs. 65% male) and the smallest difference was in the Astrobiology course (45% female, 54% male).Most of the students in our courses live in either the United States or India.Since all data are de-identified, sample selection was minimally influenced by these characteristics.</p>
<p>Research Data</p>
<p>In order to answer the research questions in this study, we needed a sample of student writing assignments and three sets of grades for those assignments: 1) Instructor grades, 2) Peer Grades, and 3) GPT-4 Grades.The first step was to select the assignments.To have a sample large enough to conduct a statistical analysis, yet possible for a single instructor to grade in a reasonable timeframe and with available LLM resources, we aimed for a sample size of around 100 assignments.Although analyzing grades for a single question might increase statistical power, it does not show the generalizability of using an LLM across different types of courses and questions.It also opens up the possibility of cherry-picking an assignment that would produce an unrepresentative result, either because of the structure of the question, or the construction of the grading rubric.By selecting responses to multiple different questions from different courses, these risks are reduced, the data selection process is more fair, the data is more representative of the variety of questions and rubrics used across these courses, and the generalizability of the result is improved.With this in mind, we gathered a subset of answers from 120 learners to each of 12 questions across the three courses, with 10 answers for each assignment.Answers were chosen from five questions in the Introductory Astronomy class, three questions from the Astrobiology class, and four questions from the History and Philosophy of Astronomy class.The assignments in the Introductory Astronomy class were worth 9 points each, except for the first assignment which was worth 6 points.Astrobiology class assignments were worth 10 points each, and the assignments in the History and Philosophy of Astronomy class were worth 4 points each.For the introductory astronomy and astrobiology courses, the students were asked to write a response of 250 to 750 words, and for the history and philosophy class, the instructions asked for 250 to 300 words.These word limits were not strictly enforced, so there is some variation.</p>
<p>The de-identified assignments were purposefully sampled (Ramos Rojas et al., 2017) to have a spread of peer grades that represent a full range of possible scores.Answers were chosen blindly without examining the content.This selection method was chosen because a majority of students in each class received full marks on the assignments and initial attempts to select assignments randomly resulted in a highly skewed dataset with mostly high scores, which would have provided no dynamic range.Another data selection challenge arose from the fact that the history and philosophy course was launched most recently and has a lower enrollment.As a result, fewer assignments were available.Because each writing assignment had three question options for students to choose from, the assignments were first examined to determine the topic covered and which questions had the most responses.The questions with the most responses were selected for analysis.As with the other two courses, a sample of assignments was selected to represent as much of the range of scores as possible, given the limited available answers.</p>
<p>To answer the first research question, one of the instructors graded the student assignments using the rubrics he had created (Stevens and Levi, 2012;Pisano et al., 2021).These are the same rubrics that are available to students and peer-graders in each course.The grading was made blind by having one of the authors provide writing assignments to the instructor without grade information.The instructor then graded the assignments with part scores according to the rubric.</p>
<p>A full set of the questions and grading rubrics used in this study have been provided in Appendix A. These same rubrics were provided to the LLMs for this experiment.Example or model answers (also referred to as instructor-provided answers) were written by the instructor to represent an ideal version of an acceptable correct answer for each assignment.Model answers are available upon request by contacting the authors.</p>
<p>Grades from GPT-4 were generated by providing information to each model using the template prompts shown in Figures 4, 5, and 6 in Appendix C. The three conditions used in the prompts included: Prompt 1: instructor-provided answer only; Prompt 2: instructor-provided answer plus instructor rubric; and Prompt 3: LLM (AI) generated rubric plus instructor answers.Example rubrics generated by GPT-4 can be found in Appendix B.</p>
<p>For the second research question, peer grades were downloaded from Coursera.Learners in these courses are required to grade assignments of their peers to complete the course.They are supposed to review three or four assignments, or their final grade for the assignment is reduced by 20%.The final writing assignment grade is the median of the scores received from the peer graders.</p>
<p>To answer research question 3, LLM rubrics were generated by providing information about the category of the course, the audience, and the total grades and breakdown along with the instructor model answer.Example GPT-4 generated rubrics are provided in Appendix B. These LLM rubrics were then provided to GPT-4 in a prompt to grade student answers.Example prompts are provided in Appendix C.</p>
<p>Results</p>
<p>In this study, we addressed the following research questions: can the LLMs (1) generate a grade comparable to that of an instructor, (2) match or exceed the reliability of peer grading, and (3) create a grading rubric that will produce LLM grades comparable to that of an instructor?</p>
<p>We normalized the scores by converting them to percentages and conducted a paired-test analysis of the data to determine whether we could identify differences between the instructor grades, peer grades, and GPT-4 grades.Due to the skewed distributions of the scores we obtained (shown below in Figure 1), we wanted to check the assumptions of normality and homogeneity of variances.Due to violations of normality as indicated by Shapiro-Wilk (p=0.003)(Shapiro &amp; Wilk, 1965;Razali &amp; Wah, 2011) and homogeneity of variances indicated by Levene's tests (p=0.021)(Levene, 1960;Brown &amp; Forsythe, 1974), non-parametric statistical methods were employed.Specifically, the Friedman Test (Friedman, 1937;Friedman, 1940) was used to assess differences among graders, and Conover's post-hoc tests (Conover, &amp; Iman 1979;Conover, 1999) were conducted for pairwise comparisons.A Friedman Test was conducted to compare the grades among the Instructor, Peer, and LLM graders.The results from a post-hoc pairwise comparison using Conover's test with Bonferroni correction (Pereira, Afonso, &amp; Medeiros, 2015), to reduce false positives, are presented below in Table 1.For the LLM, Table 1 includes three lines.The first line is the average score determined by the LLM when provided with the instructor's model answer (Prompt 1).The second line is the average score from the LLM when provided with the instructor's model answer and the rubric written by the instructor (Prompt 2).The third line is the average score using a rubric created by the LLM based on the instructor's model answer (Prompt 3).The results indicate significant differences between the instructor's grades and both peer grades (p &lt; 0.001) and the LLM when GPT-4 is only provided with an example answer (p &lt; 0.001).No significant differences were found between the instructor's grades and LLM grades when the GPT-4 is prompted with both an example answer and a rubric, whether it is instructor provided, or LLM generated.(p = 1.000).LLM grades with only an example answer are not statistically different than peer grades.</p>
<p>Because the types of questions and rubrics are very different between courses, we wanted to look more closely to see if there were differences in the way that GPT-4 graded the questions.We ran a similar paired-test analysis for each class, but the number of assignments was too small and none of the differences between graders was significant.</p>
<p>Instead, we used descriptive statistics to examine the results for individual questions.The results of this analysis are summarized in Table 2, which shows the average scores for each question in each course.One-sigma standard deviations for each average were determined from bootstrap resampling of the scores (Efron, 1979).To mimic the situation of a MOOC setting, 10,000 bootstrap samples were taken in each case.The p-values for these results are presented below in Table 3.All are above 0.05, implying that there is no statistically significant difference in average grades.Table 3 This table shows the p-values for the differences in the average scores of 10 students' writing assignments for each question.In Table 2, there is no row for instructor grades since that acts as the baseline for the p-values, but we instead show the p-values obtained from the peer grades.These p-values were computed through bootstrap resampling (Efron, 1979) with 10,000 iterations.(Adapted from Golchin et al., 2024) We also calculated root mean square (RMS) values to examine the agreement of instructor and LLM scores.The RMS results after averaging over all questions in each class are shown in Table 3.  4 This table shows the root mean square (RMS) differences between instructor grades and the grades assigned by GPT-4 across three courses: Astronomy, Astrobiology, and History and Philosophy of Astronomy.The results indicate that GPT-4 generally achieves close alignment with instructor grades, particularly when an instructor-provided rubric is used.The smallest RMS differences for GPT-4 occur when using the instructor answer plus rubric, suggesting that this combination provides the most accurate grading relative to the instructor.</p>
<p>Large Language Model vs. Instructor</p>
<p>For research question 1: Can the LLM generate grades comparable to those of an instructor?The answer is yes, when given appropriate information, GPT-4 was able to produce grades that are not statistically different than the instructor.These results, however, depend on how the LLM prompt is constructed, and what information is provided.Based on the results in Table 1, the LLM produces grades that are significantly different from the instructor when only given an example answer.The best performance is observed for GPT-4 when the prompt includes both an example answer and a grading rubric, whether it is instructor-provided, or LLM generated, the LLM produced grades that are not significantly different from the Instructor's grades, suggesting that the LLM can effectively replicate instructor grading when it is provided with sufficient information.</p>
<p>This result seems reasonable since the 'Instructor Answer + Rubric' prompt aligns closely with what the instructor uses when grading.Incorporation of an AI-generated rubric has the potential to create more robust rubrics by leveraging correlations from instructor answers.It's important to note, however, that relying solely on correct answers alone may not be sufficient, and the use of a grading rubric is crucial for ensuring greater reliability in the grading process.</p>
<p>Individual Courses</p>
<p>Although the paired-test results failed to detect significant differences between graders by class due to the small numbers and lack of statistical power, some variations in the descriptive statistics are notable.For the class "Astronomy: Exploring Time and Space," The LLM consistently graded higher than the instructor across all prompts, Notably, the LLM struggled to align with the instructor's grades in the first question due to its open-endedness.On the second question, there is improved agreement with the instructor, as both LLMs tend to grade higher.On questions three and four GPT-4 produced grades similar to the instructor.The fifth question has the lowest instructor scores, GPT-4 graded substantially higher than the instructor.It is noteworthy that the results remain consistent whether the instructor's answer is paired with an AI-generated rubric or with the instructor's rubric.</p>
<p>In "Astrobiology: Exploring Other Worlds," the questions allow students to give both open-ended answers, allowing for a thorough exploration of the topics, and well-structured answers, presenting information in a logical and organized fashion.On the first question, GPT-4 consistently scores higher than the instructor for all prompts.The best results were obtained when the LLM was provided with an instructor's model answer and rubric.On the second question, the pattern is similar, with GPT-4 being more lenient in its scoring.On the third question, GPT-4 had excellent agreement with the instructor.</p>
<p>The "Knowing the Universe: History and Philosophy of Astronomy" course presents the greatest challenge for either an instructor or for an LLM because the subject matter is very broad and involves conceptually challenging material.An answer might be based as much on plausible speculation as on a consensus among experts.Students are therefore challenged to use higher-order thinking skills rather than mere fact retrieval.Instructor's scores were lower as a percentage than for the other two courses.On the first question, the LLM scored lower than the instructor by a substantial amount, with the best agreement for a prompt with the AI-generated rubric.On the second question, the agreement is generally better.On the third question, the agreement is poor, with three of the prompts several standard deviations away from instructor scores.On the fourth question, the agreement is again poor, with the best result obtained using the instructor model answer along with an AI-generated rubric.</p>
<p>Reliability of Large Language Models and Peer Grading</p>
<p>For research question 2: Can the LLM match or exceed the reliability of peer/instructor grading?The answer is yes, The LLM's reliability matches or exceeds that of peer grading.In line with our previous research (Formanek et al., 2017), this study confirmed that peer grades differed significantly from the instructor's grades, indicating variability in peer assessments.GPT-4 had a similar problem with reliability when provided with only an example answer.Table 1 shows that GPT-4 produced grades that differ significantly from the instructor when only prompted with an example answer, yet these results were not statistically different from peer graders.This difference disappears when GPT-4 was prompted with both a rubric and an example answer.This highlights the importance of prompt design in LLM grading performance and demonstrates that GPT-4 is capable of providing grades that are more reliable than peer grades and more closely aligned with instructor grades.The Intraclass Correlation Coefficient (ICC) was calculated to assess inter-rater reliability (Shrout &amp; Fleiss, 1979).An ICC of 0.92 (95% CI: 0.89 -0.94) was obtained, indicating excellent reliability among the graders.The high ICC value (0.92) demonstrates that the LLM's grading is consistent and reliable, matching or exceeding the reliability of peer grading.</p>
<p>These results are also visible in the mean scores for each question in Table 2. Peer graders generally give higher scores and are more lenient than the instructor in almost every case.This means that LLMs are as capable of accurately grading student writing assignments as the peer-graders currently used in these MOOCs.LLMs clearly have the potential to act as proxies for the instructor, avoiding some of the pitfalls and limitations of using novices (other students in the class) to grade student assignments</p>
<p>Language Model Rubrics</p>
<p>Research question 3 was addressed by prompting GPT-4 to create a rubric based on the question text and the example answer provided by the instructor.These rubrics were evaluated by having the LLM assign grades using the AI-generated rubric in combination with the instructor-provided answer, and then comparing that with the results of the instructor rubric and instructor-provided answer.The differences between the resulting scores were not statistically significant, indicating that LLM-generated rubrics are of similar utility to the instructor-provided rubrics in terms of using them for this automated grading procedure.They also improve the results relative to providing the instructor answer alone, bringing grade agreement in line with using the instructor rubric.</p>
<p>Comparisons for Individual Students</p>
<p>The comparisons just described are averages across all ten students sampled.However, a student cares about the accuracy of their own grade more than the class average.To address this, we made a direct comparison of instructor, GPT-4 (Prompt 2: Instructor Provided Answer + Rubric), and peer-graded scores on all twelve assignments for each of the ten students individually.Prompt 2 was chosen for the GPT-4 scores because it is the most direct comparison, as it uses the same information available to instructors and peers.Additionally, we investigated the dispersion among the peer grades, where there were four for each assignment in the introductory course, and three for each assignment in the astrobiology and history and philosophy courses.Figure 2 is a scatterplot of the difference in grades between the instructor and the median for the peer grades on the x-axis and the difference in grades between the instructor and GPT-4 on the y-axis.Histograms are also shown.For the introductory course, the mean instructor minus GPT-4 score is -0.01 ± 0.15, compared to the mean instructor minus peer grading score of -0.07 ± 0.26.The LLM grade is very close to the instructor grade, with a smaller dispersion than the peer grade.For the astrobiology course, the mean instructor minus GPT-4 score is -0.02 ± 0.17, compared to the mean instructor minus peer grading score of -0.09 ± 0.20.The LLM grade is very close to the instructor grade, with a slightly smaller dispersion than the peer grade.Lastly, for the history and philosophy course, the mean instructor minus GPT-4 score is -0.08 ± 0.22, compared to the mean instructor minus peer grading score of -0.22 ± 0.27.The LLM grade is much closer to the instructor grade and has a slightly smaller dispersion than the peer grade.</p>
<p>In one notable anomaly from the astrobiology course, both peer grades and GPT-4 gave substantially higher scores than the instructor.Upon closer examination, this answer was exceptionally long and contained rambling passages full of extraneous information.GPT-4 clearly struggled with this passage, and peer graders gave full credit even though the instructor recognized that the answer was largely incorrect.</p>
<p>Overall, LLM grades and peer grades are always more lenient than instructor grades, with a few examples in the astrobiology class where peer graders were considerably more lenient than either the instructor or GPT-4.Overall, GPT-4 performs better than peer grading.</p>
<p>To further understand why there was a higher discrepancy among peer grades and how that discrepancy compared to the instructor, we retrieved from Coursera the median peer grades for each student to see the dispersion of peer grades for each question.We calculated a representative score by taking the average of the 10 median student grades and converting them into percentages.Then, we calculated the mean absolute deviation for each of the peer grades and used the average of those deviations for the error bars on each score.For the instructor scores, we used the average of the instructor-given scores for the 10 students, converted into percentages, and the average of the mean absolute deviation for each score as the error bars.The results are shown below in Figure 3.The two panels show the mean scores as points, and the shaded regions (using the same color scheme as Figure 2) denote the average magnitudes of deviations of data from a question's mean.These scores are normalized to show percentages.</p>
<p>The peer dispersion ranges from 10.8% to 19.6%, and the instructor dispersion ranges from 10.7% to 24.4% for the introductory course.Peer dispersion ranges from 13.3% to 28.7%, and the instructor dispersion ranges from 23.4% to 30.4% for the astrobiology course.Peer dispersion ranges from 9% to 21%, and the instructor dispersion ranges from 12% to 20% for the history and philosophy course.The instructor dispersion is smaller than peer dispersion for the history and philosophy course and the astrobiology course, whereas the peer dispersion is smaller than the instructor dispersion for the introductory course.This analysis of grades for individual students affirms that GPT-4 is superior to Coursera's peer grading mechanism, and it comes very close to matching the grades assigned by the instructor.</p>
<p>Discussion and Conclusions</p>
<p>The promise and peril of AI for education cannot be fully elucidated in a simple pilot study like this.However, the results of using an LLM to grade student writing assignments in these MOOCs are promising.</p>
<p>• For research question 1: Can the LLM generate grades comparable to those of an instructor?The answer is yes.GPT-4 was able to produce comparable grades to an instructor when prompted with appropriate information which, in this case, included an example answer and a rubric.• For research question 2: Can the LLM match or exceed the reliability of peer/instructor grading?</p>
<p>The answer is yes, GPT-4's grading is consistent and reliable, matching or exceeding the reliability of peer grading.• For research question 3: Can the LLM create a grading rubric that will produce LLM grades comparable to that of an instructor?The answer is yes, GPT-4 was able to produce comparable grades to an instructor with LLM generated rubrics, indicating the LLM-generated rubrics were of similar utility to the instructor-provided rubrics in terms of using them for this automated grading procedure.</p>
<p>The performance of the LLM in matching the instructor grades is better for the astronomy and astrobiology courses than it is for the history and philosophy class, where questions are more open-ended and it is challenging even for an instructor to create a concrete rubric.In an astronomy MOOC where a direct comparison can be made, GPT-4 performed as well as peer grading in matching the instructor grades with a small dispersion.When the LLM is used to create a rubric, that prompt alone does not give good results, but it does in combination with an instructor's model answer.Lastly, these AI methods can easily be scaled to evaluate the science writing of thousands or tens of thousands of online learners in real-time.In this analysis, we have treated instructor grading as the "gold standard" and shown that GPT-4 comes close to the instructor score in three different MOOCs.Equipped with a rubric and a detailed explanation of the instructional goals for the assignment, it is likely that a future LLM could match the performance and reliability of a human instructor.In fact, since we have assumed instructors to be perfect, when in fact they are fallible, it's plausible that an LLM could one day eclipse the reliability of an instructor.</p>
<p>Results from this initial study have provided insights into the strengths and weaknesses of an LLM grading system.The writing assignments analyzed for this project were gathered from an existing course that was developed without any plan to grade assignments using an LLM.Our future work will explore the opportunities and challenges of applying these computer-aided systems to assessing student writing assignments.In particular, we want to examine and compare the reasoning that instructors and LLM's use to explain their grading decisions.Progress could be made in using LLMs to generate model answers and rubrics for open-ended questions and questions asking for speculation.Another approach involves the development of writing assignments and grading systems specifically designed to play to the strengths of the LLM.From an educational perspective, the kinds of student assignments that are best suited to LLM grading occupy the first five levels of Bloom's taxonomy (Krathwohl, 2002).They are: Remember, Understand, Apply, Analyze, and Evaluate.These levels all require factual knowledge, conceptual knowledge, and procedural knowledge, which can be drawn from an existing base of information on which the LLM can be or may have already been trained.Any assignments that fall in the sixth level (Create) will be more challenging for the LLM to assess, as was seen in the results from the Astrobiology class assignments.Current LLMs fall short of being able to evaluate creative assignments and any writing that requires metacognitive thinking.</p>
<p>Limitations and Future Work</p>
<p>Although these results are promising, several limitations are worth mentioning.Even though the authors took steps to reduce selection bias by de-identifying and blinding the data, with the relatively small sample size and the use of purposeful instead of random sampling, it is possible that systematic error could have been introduced unintentionally into the results through the selection of writing assignments that are not representative of student work in some way.The grading rubrics are also a potential limitation.Although the instructors attempted to develop rubrics that produced reliable results, it is possible that the interpretation of the grading system differed between the instructor, peer-graders, and GPT-4.It is also important to note that prior researchers have found that it is still possible to fool automated systems (Filighera et al. 2020), and this study does not address these concerns.Finally, this study was conducted using data from MOOC learners.Although the content of the class is adapted from undergraduate science classes, the motivations of MOOC and undergraduate students are different, as is the necessity of accurate grading.In a free, non-credit MOOC the stakes are low.It is more important to consider how to deal with potentially inaccurate grades in a high-stakes environment (Azad et al. 2020).</p>
<p>This study dealt with science writing by lifelong, adult learners in MOOCs.A natural next step for this research is to apply it in the college classroom.College instructors agree that writing is an important tool for helping undergraduates learn science and apply the principles of scientific thinking (Moon, Gear, &amp; Schultz, 2018).However, in the large introductory classes where most students get their only experience of science, the grading burden of evaluating student writing is severe.Peer grading can be used, but as in the MOOCs described here, undergraduates are not always reliable graders (Biango-Daniels &amp; Sarvary, 2020).We plan to use LLMs to help instructors grade student writing in large General Education classes that satisfy the science requirement for non-science majors to graduate.Initially, it would provide feedback for formative assessment, where the LLM delivers a grade based on the instructor's model answer and rubric, as in this study.Beyond that, the LLM will provide reasoning for the grade and this feedback could use the claim, evidence, reasoning framework that is widely used in middle and high school science classrooms, and recently at the college level (Eden, 2023).LLMs have recently been used for fact-checking and for identifying claim-evidence pairs in scientific content (Koneru, Wu, &amp; Rajtmajer, 2023;Wang et al., 2023;Zeng &amp; Zubiaga, 2024).The hope is that LLMs could provide instructors and their students with assessments of the scientific validity of student writing, aiming for the "gold standard" of conceptual learning (Gere et al., 2019).</p>
<p>Rubric 1: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?Correct answers should include a discussion of the observational rather than the experimental nature of astronomy as well as the challenges of gathering data meaning that most data is gathered from a distance or from indirect evidence.</p>
<p>3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples.Correct answers should include a discussion of the observational rather than the experimental nature of astronomy as well as the challenges of gathering data meaning that most data is gathered from a distance or from indirect evidence.2 points: The writer answers the question correctly, but does not explain it well, use sufficient evidence, or include both of the above components.0 points: The writer does not answer the question correctly Part 2: Ancient cultures built some impressive structures that incorporated astronomical functions and information (Stonehenge, Chichen Itza, the Great Pyramid).A friend or acquaintance of yours tries to argue that some of these structures and artifacts are evidence of "ancient astronauts" or visits by intelligent aliens.How would you rebut or argue against this idea?</p>
<p>Rubric 2: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?Correct answers should include a discussion of how ancient technology was well within the capability of humans at the time as well as a discussion of the difficulty of interstellar travel or a lack of evidence for alien or extraterrestrial visitation.</p>
<p>3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples.Correct answers should include a discussion of how ancient technology was well within the capability of humans at the time as well as a discussion of the difficulty of interstellar travel or a lack of evidence for alien or extraterrestrial visitation. 2 points: The writer answers the question correctly, but does not explain it well, use sufficient evidence, or include both of the above components.Rubric Part 2: 4 points: The writer correctly identifies both detection methods; clear explanation of how each detection method works 3 points: The writer correctly identifies both detection methods; explanation for one or both methods is unclear or incomplete 2 points: The writer correctly identifies and explains one detection method; does not identify other detection method, no explanation 1 point: The writer correctly identifies one detection method; explanation of detection method unclear or incomplete 0 points: The writer does not identify or explain either detection method Part 3: Clearly identify one exoplanet as Earth-like.</p>
<p>Rubric Part 3: 1 point: Writer clearly identifies one exoplanet as Earth-like 0 points: Writer does not clearly identify one exoplanet as Earth-like</p>
<p>Overall Rubric Question 1: Student writes with sufficient clarity and detail to communicate their points effectively 2 points: Writer presents arguments in clear, logical way that demonstrates understanding of concepts; correct use of scientific terms/language.Connections between concepts are well developed.1 point: Arguments attempt to address key concepts, some gaps in logic or comprehension.Some use of scientific terms, majority used correctly.0 points: Arguments are difficult to follow, multiple flaws in logic.Incorrect or no use of key terms.Connections between concepts not present.</p>
<p>The student uses data from the graphs to support their explanations.1 point: Yes, the writer uses data to support their explanation.0 points: No, the writer does not use sufficient data to support their explanation.</p>
<p>ABIO Question 2:</p>
<p>Part 1: Discuss how habitable zone range and spectral type are related.</p>
<p>Rubric Part 1:</p>
<p>A well-written answer will define habitable zone and discuss how habitable zone range and star spectral type are related.2 points: The writer defines habitable zone and discusses how habitable zone range and spectral type are related.1 point: The writer defines habitable zone or the writer discusses how habitable zone range and spectral type are related 0 points: The writer does not define habitable zone or discuss how habitable zone range and spectral type are related</p>
<p>Part 2: Address Yousef's statement that all three planets likely have liquid surface water because they all orbit at 1 AU.Since the Earth orbits at 1 AU from the sun, and we know Earth has liquid surface water, then these exoplanets should as well.Clearly state whether you agree or disagree with the conclusion.Explain your answer with evidence and use data to support your answer.</p>
<p>Rubric Part 2:</p>
<p>The writer addresses Yousef's statement that "all three planets likely have liquid surface water because they all orbit at 1 AU.Since the Earth orbits at 1 AU from the sun, and we know Earth has liquid surface water, then these exoplanets should as well." 3 points: The writer clearly states whether they agree or disagree with Yousef's conclusion and explains answer with evidence and uses data to support answer. 2 points: The writer states whether they agree or disagree with Yousef's conclusion and attempts explanation, however, the explanation is incomplete or poorly supported by data. 1 point: The writer states whether they agree or disagree, they do not provide explanation, and do not use data to support their conclusion.0 points: The writer does not address Yousef's statement.</p>
<p>Part 3: Clearly state whether you agree or disagree with Lora's conclusion that both exoplanets 2 &amp; 3 will have liquid water, but not exoplanet-1.The star for exoplanet-1 is spectral type A, which is too big and hot and would evaporate water on exoplanets.But exoplanet-2 and exoplanet-3 orbit around favorable spectral types G and M, therefore they likely have liquid surface water.Explain your answer with evidence and use data to support your answer.</p>
<p>Rubric Part 3: The writer addresses Lora's statement that "both exoplanets 2 &amp; 3 will have liquid water, but not exoplanet-1.The star for exoplanet-1 is spectral type A, which is too big and hot and would evaporate water on exoplanets.But exoplanet-2 and exoplanet-3 orbit around favorable spectral types G and M, therefore they likely have liquid surface water." 3 points: The writer clearly states whether they agree or disagree with Lora's conclusion and explains answer with evidence and they use data to support the answer.2 points: The writer states whether they agree or disagree with Lora's conclusion and attempts an explanation, however, the explanation is incomplete or poorly supported by data. 1 point: The writer states whether they agree or disagree, they do not provide explanation, and do not use data to support their conclusion.0 points: The writer does not address Lora's statement.</p>
<p>Overall Rubric Question 2:</p>
<p>The answer is written with clarity and good communication.Overall Rubric Question 3: Student writes with sufficient clarity and detail to communicate their points effectively 2 points: Presents arguments in clear, logical way that demonstrates understanding of concepts; correct use of scientific terms/language.Connections between concepts are well developed.1 point: Arguments attempt to address key concepts, some gaps in logic or comprehension.Some use of scientific terms, majority used correctly.The author clearly responds to the prompt, makes a compelling argument, and uses supporting evidence and/or information from the class materials.0 points: The author did not address the prompt at all and did not use supporting evidence from the course. 1 point: The author attempted to answer respond to the prompt at a basic level but it is confusing or incomplete and does not reference course information to support the claims.2 points: The author does a reasonable job of responding to the prompt, but it suffers from one (and only one) of the following problems: 1) the response is incomplete or unclear 2) it does not reference course information to support the claims.</p>
<p>3 points: The author answers the question thoroughly and thoughtfully and references information from the course to support their perspective or claims.</p>
<p>HPA Question 2:</p>
<p>According to the current understanding of the universe, the cosmos has a definite beginning but an infinite future.What are the philosophical problems and implications of this?</p>
<p>Rubric Part 1: The author clearly states the prompt they have chosen for the assignment.0 points: No 1 point: Yes Rubric Part 2: The author clearly responds to the prompt, makes a compelling argument, and uses supporting evidence and/or information from the class materials.0 points: The author did not address the prompt at all and did not use supporting evidence from the course. 1 point: The author attempted to answer respond to the prompt at a basic level but it is confusing or incomplete and does not reference course information to support the claims.2 points: The author does a reasonable job of responding to the prompt, but it suffers from one (and only one) of the following problems: 1) the response is incomplete or unclear 2) it does not reference course information to support the claims.</p>
<p>3 points: The author answers the question thoroughly and thoughtfully and references information from the course to support their perspective or claims.</p>
<p>HPA Question 3: Fig. 5 The second prompt builds on the first approach by adding the instructor-provided rubric for each of the questions in addition to the correct answer.This approach aligns more closely with how an instructor actually grades using these criteria.This ensures that the scoring and deductions match the instructor's standards.</p>
<p>Fig. 6 In the third prompt, we approach the LLM differently by asking it to create a rubric.Here, we provide the information of the course and give it the supporting instructor answers and total scores to create a new LLM-made rubric.The reasoning behind this is that LLMs are trained on extensive amounts of data from across the Internet and thus possess comprehensive interdisciplinary knowledge.This rubric is then used instead of the instructor's rubric from prompt 2 to get the scores from the LLM.</p>
<p>Fig. 1
1
Fig. 1 Histograms of normalized grades from instructor, peers, and LLM prompts for all responses showing skewed distribution of scores.</p>
<p>Fig. 2
2
Fig. 2 Scatter plot showing the difference between the grades by the instructor and the median of the peer scores received by Coursera for each student on the x-axis, and the difference between the grades by the instructor and the LLM Model (GPT-4 prompt 2) on the y-axis.Symbols for the three courses are yellow triangles (Introductory Astronomy), blue circles (Astrobiology), and green squares (History and Philosophy of Astronomy).The two histograms show the distribution of the data points for these differences along each axis.Dashed lines in the histograms indicate the means of the three classes for the two measures of grade difference.</p>
<p>Fig. 3
3
Fig.3Dispersion of grades as a function of the questions in each of the three courses.The two panels show the mean scores as points, and the shaded regions (using the same color scheme as Figure2) denote the average magnitudes of deviations of data from a question's mean.These scores are normalized to show percentages.</p>
<p>0 points: The writer does not answer the question correctly ETS Question 2: Part 1: What are the advantages of large telescopes?Provide at least one.Rubric 1: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly Part 2: Why do astronomers want telescopes in space when putting them there is expensive?Rubric Part 2: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly Part 3: What are some examples of wavelength regions beyond the spectrum of visible light where astronomers can learn about the universe?Provide at least two.Rubric Part 3: Does the writer answer the question correctly AND the writer expresses their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.The write only includes one wavelength instead of two.0 points: The writer does not answer the question correctly ETS Question 3: Part 1: What are the two main, indirect methods for finding exoplanets?Rubric Part 1: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well, use sufficient evidence, or include both of the above components.0 points: The writer does not answer the question correctly Part 2: Why is it so difficult to see exoplanets directly in an image?Rubric Part 2: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly Part 3: What are some similarities or differences between our Solar System and new, distant planet systems?Provide at least one similarity and/or difference.Rubric Part 3: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer correctly answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly ETS Question 4: Part 1: What is the source or cause of the Sun's light, and how do all the elements in the periodic table get produced?Rubric Part 1: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.The write only includes one answer instead of two.0 points: The writer does not answer the question correctly Part 2: What is the general process by which a large diffuse cloud of gas turns into a star and surrounding planets?Rubric Part 2: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples.Writer must mention gravity to receive full credit.2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly Part 3: Name of the two end states of stars much more massive than the Sun and describe their physical properties?Rubric Part 3: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly ETS Question 5: Part 1: Why do astronomers often say that large telescopes are like time machines, or equivalently, why is distant light old light?Rubric Part 1: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples 2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly Part 2: What is the evidence that the universe began in a hot, dense state 13.8 billion years ago?Rubric 2: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples.2 points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly Part 3: The atoms in our bodies and in all the stars in all 100 galaxies form a small percentage of the contents of the universe.What are the two dominant ingredients of the universe and why are astronomers so unsure of their physical nature?Rubric Part 3: Does the writer answer the question correctly AND express their thoughts clearly using supporting facts and relevant examples?3 points: The writer answers the question correctly AND the writer expresses their thoughts clearly, and uses supporting facts and relevant examples.2points: The writer answers the question correctly, but does not explain it well.0 points: The writer does not answer the question correctly identify the detection methods used to gather data for each exoplanet.Briefly explain how each detection method works.Correctly identify both detection methods.Clearly explain how each detection method works.Rubric Part 1: 2 points: The writer clearly identifies physical characteristics for both exoplanets 1 point: The writer identifies physical characteristics of only one exoplanet 0 points: The writer does not clearly identify physical characteristics of either exoplanet Part 2: Correctly identify which physical characteristics can be learned from each set of data, and explain why.Clearly identify physical characteristics for both exoplanets.</p>
<p>0</p>
<p>points: Arguments are difficult to follow, multiple flaws in logic.Incorrect or no use of key terms.Connections between concepts not present.Knowing the Universe: History and Philosophy of Astronomy (HPA) HPA Question 1: Describe the practical and philosophical importance of astronomy for humans living a nomadic lifestyle in 20,000 B.C. Rubric Part 1: The author clearly states the prompt they have chosen for the assignment.0 points: No 1 point: Yes Rubric Part 2:</p>
<p>Table 1 .
1
Post-Hoc Analysis of Friedman Test
Model</p>
<p>Table 1
1
Results of a post-hoc analysis for the Friedman chi-square analysis using Conover's test with Bonferroni correction.An asterisk * indicates results that are significant at p &lt; 0.05.</p>
<p>Table 2 .
2
Instructor grades, Peer Grades, and LLM Grades
CoursesModelPromptAstronomyAstrobiologyHistory and PhilosophyQ1Q2Q3Q4Q5Q1Q2Q3Q1Q2Q3Q4Instructor Grades3.90 ± 0.548.2 ± 0.377.51 ± 0.927.41 ± 0.865.51 ± 0.946.8 ± 1.096.7 ± 0.837.89 ± 0.853.50 ± 0.212.39 ± 0.292.70 ± 0.202.40 ± 0.15Peer Grades5.15 ± 0.277.55 ± 0.757.41 ± 0.827.45 ± 0.467.40 ± 0.837.50 ± 0.797.46 ± 1.029.04 ± 0.523.60 ± 0.213.69 ± 0.203.40 ± 0.383.80 ± 0.19Prompt 1: Instructor Provided Answer4.75 ± 0.418.65 ± 0.207.61± 0.877.51 ± 0.776.21 ± 0.907.50 ± 0.707.91 ± 0.688.10 ± 0.333.50 ± 0.213.25 ± 0.233.65 ± 0.143.2 ± 0.24Prompt 2:Instructor4.40 ±8.30±7.31 ±6.91 ±5.91 ±7.11 ±7.41 ±7.50 ±3.20 ±3.10 ±3.20 ±2.70 ±GPT-4Provided Answer0.410.280.860.901.060.950.830.410.310.170.190.28+ RubricPrompt 3:AI Rubric +4.40 ±8.50 ±7.61 ±7.06 ±6.36 ±7.11 ±7.11 ±7.50 ±3.20 ±2.95 ±3.20 ±2.95 ±Instructor0.480.270.870.891.100.850.980.290.220.230.180.23Answers</p>
<p>Table 2
2
Average grades from 10 students' writing assignments for each question in each course.Results from GPT-4 are displayed for each prompt.One-sigma standard deviations (shown below the averages) for each average were determined from bootstrap resampling of the scores.
CoursesModelPromptAstronomyAstrobiologyHistory and PhilosophyQ1Q2Q3Q4Q5Q1Q2Q3Q1Q2Q3Q4Peer Grades0.480.520.950.980.500.660.650.500.830.500.580.61Prompt 1;Instructor0.490.490.970.960.660.650.530.871.000.520.550.51Provided AnswerPrompt 2:GPT-4Instructor Provided Answer0.520.780.890.740.790.870.640.710.610.520.550.55+ RubricPrompt 3:AI Rubric + Instructor0.550.600.950.790.640.860.800.720.570.510.550.51Answers
Table 3. p-values for results in Table 1</p>
<p>Table 4 .
4
RMS Differences between Instructor and LLM Grades
ModelPromptAstronomyAstrobiologyHistory and PhilosophyPrompt 1: Instructor Answer0.540.820.76GPT-4Prompt 2: Instructor Answer + Rubric0.380.500.48Prompt 3: AI Rubric + Instructor Answer0.490.370.49Table</p>
<p>2 points: Presents arguments in clear, logical way that demonstrates understanding of concepts and correctly use scientific terms and language.Connections between concepts are well developed.1 point: Arguments attempt to address key concepts, some gaps in logic or comprehension.Some use of scientific terms, majority used correctly.0points:Argumentsaredifficulttofollow, multiple flaws in logic.Incorrect or no use of key terms.Connections between concepts are not present.Discuss why/whether it is plausible that exobiology exists at all, in terms of exoplanet type, spectral type and orbital distance.Use data from the table and the generalized geologic timeline to support your reasoning.Rubric Part 1: Discuss why or whether it is plausible that exobiology exists at all, in terms of exoplanet type, spectral type and orbital distance.Use data from the table and the generalized geologic timeline to support your reasoning.4points:Cleardiscussion of plausibility of exobiology, describes habitability in terms of spectral type and orbital distance; uses data tabletocharacterize exoplanet type; discussion supported by examples from class and data from table. 3 points: Clear discussion of plausibility of exobiology; describes habitability but leaves out mention of ONE of: spectral type, orbital distance, exoplanet type; (if exoplanet type is identified) uses data table to characterize exoplanet type; discussion supported by examples from class and data from table. 2 points: Discusses plausibility of exobiology, states exoplanet type but does not give evidence to support characterization; mentions relationship between spectral type and habitability; little use of evidence to support discussion. 1 point: Vague discussion of habitability.0points:Nodiscussion.Part 2: Clearly state the geologic eon and make a strong argument for the state of exobiology, given their choice of geologic eon.Discusses whether the exobiology is unicellular, multicellular, intelligent etc. Use geologic eon or age, exoplanet type and examples from class to support argument.Present arguments in a clear, logical way that demonstrates understanding of concepts.Correctly use scientific terms or language.Connections between concepts should be well developed.Rubric Part 2:Discuss whether the exobiology is unicellular, multicellular, intelligent etc. and explain your reasoning.Clearly state the geologic eon that corresponds to the age of the exoplanet system.Use data from the table and the generalized geologic timeline to support your reasoning.4 points: Clearly states geologic eon and makes a strong argument for the state of exobiology, given their choice of geologic eon.Discusses whether the exobiology is unicellular, multicellular, intelligent etc.; uses geologic eon/age, exoplanet type and examples from class to support argument.3 points: Provides some reasoning for the state of exobiology, given their choice of geologic eon.States geologic eon.Discusses whether the exobiology is unicellular, multicellular, intelligent etc.; uses geologic eon/age, exoplanet type and examples from class to support argument.2 points: Attempts to explain state of evolution in connection with geologic age; discusses whether the exobiology is unicellular, multicellular, intelligent etc.; mentions geologic eon but does not clearly identify/choose one for the exoplanet; use of some data/examples to support argument. 1 point: States whether the exobiology is unicellular, multicellular, intelligent etc.; does not use evidence to support conclusion; mentions geologic eon or does not clearly identify which geologic eon they have chosen; does not use data to support argument.0 points: No discussion of exobiology or geologic eon.
Part 1:ABIO Question 3
AcknowledgementsWe acknowledge fruitful conversations with Sanlyn Buxner about the statistical analysis and the educational implications of this research, and with Alexander Danehy on the computational basis for the analysis and on the strengths and weaknesses of the current generation of large language models.This work was supported in part by a grant from the National Science Foundation, award DUE-2020784.AcknowledgementsWe acknowledge many fruitful conversations with Sanlyn Buxner about the statistical analysis and educational implications of this research, and with Alexander Danehy on the computational basis for the analysis and on the strengths and weaknesses of the current generation of large language models.This work was supported in part by a grant from the National Science Foundation, award DUE-2020784.Appendix B. Example Rubrics Generated for Knowing the Universe: History and Philosophy of Astronomy (HPA) by GPT-4Rubric for HPA Question 1:-Understanding of the topic (1 point): The student demonstrates a clear understanding of the practical and philosophical importance of astronomy for humans living a nomadic lifestyle in 20,000 B.C. -Use of course topics and sources (1 point): The student effectively uses course topics and sources to support their claims.-Clarity and completeness of the answer (1 point): The student's answer is clear, concise, and fully addresses the prompt.-Originality and creativity (1 point): The student's answer is original and demonstrates creative thinking.Rubric for HPA Question 2: -Understanding of the topic (1 point): The student demonstrates a clear understanding of the philosophical problems and implications of the universe having a definite beginning but an infinite future.-Use of course topics and sources (1 point): The student effectively uses course topics and sources to support their claims.-Clarity and completeness of the answer (1 point): The student's answer is clear, concise, and fully addresses the prompt.-Originality and creativity (1 point): The student's answer is original and demonstrates creative thinking.Rubric for HPA Question 3: -Understanding of the topic (1 point): The student demonstrates a clear understanding of why liberty and personal rights might be connected to the pursuit of science according to Enlightenment philosophy.-Use of course topics and sources (1 point): The student effectively uses course topics and sources to support their claims.-Clarity and completeness of the answer (1 point): The student's answer is clear, concise, and fully addresses the prompt.-Originality and creativity (1 point): The student's answer is original and demonstrates creative thinking.Rubric for HPA Question 4: -Understanding of the topic (1 point): The student demonstrates a clear understanding of the philosophical and scientific implications of the universe being eternal and unchanging as proposed by the steady state model in the 1950's.-Use of course topics and sources (1 point): The student effectively uses course topics and sources to support their claims.-Clarity and completeness of the answer (1 point): The student's answer is clear, concise, and fully addresses the prompt.-Originality and creativity (1 point): The student's answer is original and demonstrates creative thinking.FundingThis work was supported in part by a grant from the National Science Foundation, award DUE-2020784.Author ContributionsAll authors contributed to the design and conception of the study.Evaluation material for the MOOCs was created by Matthew Wenger, who also acted as the instructor grader for this project.Implementation of the LLMs for grading student writing was done by Nikhil Garuda and Shahriar Golchin.The statistical analysis was done by Nikhil Garuda.The gathering of peer grading information was carried out by Nikhil Garuda and Sarah Stamer.The first draft of the manuscript was written entirely by Chris Impey.AllStatements and DeclarationsCompeting InterestsThe authors have no relevant financial or non-financial interests to disclose.Author ContributionsAll authors contributed to the design and conception of the study.Evaluation material for the MOOCs was created by Matthew Wenger, who also acted as the model instructor for this project.Implementation of the LLMs for grading student writing was done by Nikhil Garuda and Shahriar Golchin.Gathering of the peer grading information was carried out by Nikhil Garuda and Sarah Stamer.The first draft of the manuscript was written entirely by Chris Impey.All authors commented on successive versions of the manuscript and all authors read and approved the final manuscript.Appendix Appendix A. Questions and grading rubricsAstronomy: Exploring Time and Space (ETS)ETS Question 1:Part 1: In terms of the scientific method, how does astronomy differ from a lab science like chemistry or biology?How can astronomers be confident of their understanding of objects that are remote from the Earth?According to Enlightenment philosophy, why might liberty and personal rights be connected to the pursuit of science?Rubric Part 1: The author clearly states the prompt they have chosen for the assignment.0 points: No 1 point: Yes Rubric Part 2: The author clearly responds to the prompt, makes a compelling argument, and uses supporting evidence and/or information from the class materials.0 points: The author did not address the prompt at all and did not use supporting evidence from the course. 1 point: The author attempted to answer respond to the prompt at a basic level but it is confusing or incomplete and does not reference course information to support the claims.2 points: The author does a reasonable job of responding to the prompt, but it suffers from one (and only one) of the following problems: 1) the response is incomplete or unclear 2) it does not reference course information to support the claims.The author clearly responds to the prompt, makes a compelling argument, and uses supporting evidence and/or information from the class materials.0 points: The author did not address the prompt at all and did not use supporting evidence from the course. 1 point: The author attempted to answer respond to the prompt at a basic level but it is confusing or incomplete and does not reference course information to support the claims.2 points: The author does a reasonable job of responding to the prompt, but it suffers from one (and only one) of the following problems: 1) the response is incomplete or unclear 2) it does not reference course information to support the claims.3 points: The author answers the question thoroughly and thoughtfully and references information from the course to support their perspective or claims.Appendix C. Example LLM PromptsFig.4The first prompt uses the correct answers supplied by the course instructor, here the instructor of the MOOC.This is then used to generate the final grades for the assignments.The prompt also embeds the students' response into the input.Here, the LLM has no guidance or context on how the grading should be done.It is only provided with the total points for each question.
Revolutionizing Online Learning: The Potential of ChatGPT in Massive Open Online Courses. M Alseddiqi, A Al-Mofleh, L Albalooshi, O Najam, 10.24018/ejedu.2023.4.4.686European Journal of Education and Pedagogy. 442023</p>
<p>Perspectives on Generative AI: College Leaders Assess the Promise and the Threat of a Game-Changing Tool. M Anft, The Chronicle of Higher Education. 2023</p>
<p>Strategies for Deploying Unreliable AI Graders in High-transparency High-stakes Exams. S Azad, B Chen, M Fowler, M West, C Zilles, International Conference on Artificial Intelligence in Education. Springer2020</p>
<p>A Novel Automated Essay Scoring Approach for Reliable Higher Education Assessments. M Beseiso, O A Alzubi, H Rashaideh, M Biango-Daniels, M Sarvary, A Challenge in Teaching Scientific Communication: Academic Experience Does Not Improve Undergraduates' Ability to Assess Their or Their Peers' Writing. Assessment and Evaluation in Higher Education. 2021. 202033</p>
<p>GPT-4 Surpassing Human Performance in Linguistic Pragmatics. L Bojic, P Kovacevic, M Cabarkapa, 2023</p>
<p>Intelligent Human Computer Interaction. J G Borad, L D Netak, M Singh, D K Kang, J H Lee, U S Tiwary, D Singh, W Y Chung, 10.1007/978-3-030-68449-5_25Lecture Notes in Computer Science. 20202021SpringerAutomated Grading of Essays: A Review</p>
<p>Robust tests for the equality of variances. M B Brown, A B Forsythe, Journal of the American statistical association. 693461974</p>
<p>Multiple-comparisons procedures. W J Conover, R L Iman, Informal report. 1979No. LA-7677-MS). Los Alamos National Lab.(LANL)</p>
<p>Practical nonparametric statistics. W J Conover, 1999John Wiley &amp; Sons350</p>
<p>A Modified Claim, Evidence, Reasoning Organizer to Support Writing in the Science Classroom. A Eden, The American Biology Teacher. 8552023</p>
<p>B Efron, Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics. 19797</p>
<p>Factors Contributing to Adult Knowledge of Science and Technology. J H Falk, M D Needham, Journal of Research in Science Teaching. 5042013</p>
<p>Fooling Automatic Short Answer Grading Systems. A Filighera, T Steuer, C Rensing, International conference on artificial intelligence in education. ChamSpringer International Publishing2020. June</p>
<p>Insights About Large-Scale Online Peer Assessment from an Analysis of an. M Formanek, M Wenger, S Buxner, C D Impey, T Sonam, Astronomy MOOC. Computers and Education. 1132432017</p>
<p>Relationship between Learners' Motivation and Course Engagement in an Astronomy Massive Open Online Course. M Formanek, S Buxner, C Impey, M Wenger, Physical Review Physics Education Research. 15201402019</p>
<p>The use of ranks to avoid the assumption of normality implicit in the analysis of variance. M Friedman, Journal of the american statistical association. 321937. 200</p>
<p>A comparison of alternative tests of significance for the problem of m rankings. The annals of mathematical statistics. M Friedman, 194011</p>
<p>D Gamage, T Staubitz, M Whiting, 10.1080/01587919.2021.1911626Peer Assessment in MOOCs; Systematic Literature Review. Distance Education. 202142</p>
<p>Writing and Conceptual Learning in Science: An Analysis of Assignments. A R Gere, N Limlamai, E Wilson, K M Saylor, R Pugh, 2019Written Communication36</p>
<p>Large Language Models as MOOCs Graders. S Golchin, N Garuda, C Impey, M Wenger, 2024</p>
<p>S Golchin, M Surdeanu, Time Travel in LLMs: Tracing Data Contamination in Large Language Models. 2023a</p>
<p>Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. S Golchin, M Surdeanu, 2023b</p>
<p>S Graham, S A Kiuhara, M Mackay, The Effects of Writing on Learning in Science, Social Studies, and Mathematics: A Meta-Analysis. 202090</p>
<p>Shaping the Future of Education: Exploring the Potential and Consequences of AI and ChatGPT in Educational Settings. S Grassini, Education Sciences. 1376922023</p>
<p>A Writing in Science Framework Designed to Enhance Science Literacy. B Hand, C Lawrence, L D Yore, International Journal of Science Education. 21101999</p>
<p>State of the Art and Practice in AI in Education. W Holmes, I Tuomi, European Journal of Education. 5742022</p>
<p>C D Impey, M C Wenger, C L Austin, Astronomy for Astronomical Numbers: A Worldwide Massive Open Online Class. The International Review of Research in Open and Distributed Learning. 201516</p>
<p>Bringing the Universe to the World: Lessons Learned from a Massive Open Online Class on Astronomy. C D Impey, M Wenger, M Formanek, S Buxner, Communicating Astronomy with the Public Journal. 212016</p>
<p>Higher Education Online and the Developing World. C D Impey, Journal of Education and Human Development. 922020</p>
<p>Knowing the Universe: Teaching the History and Philosophy of Astronomy. C D Impey, 10.32374/AEJ.2023.3.1.058aepAstronomy Education Journal. 2023</p>
<p>The Design and Delivery of an Astrobiology Massive Open Online Course. C D Impey, M Wenger, X Riabokin, Astrobiology. 2342023</p>
<p>The use of scoring rubrics: Reliability, validity and educational consequences. A Jonsson, G Svingby, Educational Research Review. 222007</p>
<p>Learning engagement and persistence in massive open online courses (MOOCS). Y Jung, J Lee, Computers &amp; Education. 1222018</p>
<p>ChatGPT for good? On Opportunities and Challenges of Large Language Models for Education. E Kasneci, K Seßler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, M Stadler, J Weller, J Kuhn, G Kasneci, 10.1016/j.lindif.2023.102274Learning and Individual Differences. 1032023. 102274</p>
<p>Harnessing GPT-4 so that all students benefit. A nonprofit approach for equal access. S Khan, 2023. 7 Feb. 2024</p>
<p>Can Large Language Models Discern Evidence for Scientific Hypotheses?. S Koneru, J Wu, S Rajtmajer, Case Studies in the Social Sciences. 2023</p>
<p>A Revision of Bloom's Taxonomy: An Overview. D R Krathwohl, Theory into Practice. 4142002</p>
<p>An investigation into the attraction and completion rates of MOOCs. S Kruchinin, 2019Knowledge Management &amp; E-Learning11</p>
<p>Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric Scores Determined?. V S Kumar, D Boulanger, International Journal of Artificial Intelligence in Education. 312020</p>
<p>C-rater: Automated scoring of short-answer questions. C Leacock, M Chodorow, Computers and the Humanities. 372003</p>
<p>Contributions to probability and statistics: Essays in honor of Harold Hotelling. H Levene, I. Olkin1960Stanford University Press</p>
<p>The Utility of Writing Assignments in Undergraduate Bioscience. J Libarkin, G Ording, CBE-Life Sciences Education. 1112012</p>
<p>Online Learner Engagement: Conceptual Definitions, Research Themes, and Supportive Practices. F Martin, J Borup, Educational Psychologist. 5732022</p>
<p>A Moon, A R Gear, G V Schultz, Writing in the STEM Classroom: Faculty Conceptions of Writing and its Role in the Undergraduate Classroom. 2018102</p>
<p>Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs). W Morris, S A Crossley, L Holmes, A Trumbore, 10.1145/3576050.3576098LAK23: 13 th International Learning Analytics and Knowledge Conference. 2023</p>
<p>What We Learned from Creating One of the World's Most Popular MOOCs. B A Oakley, T J Sejnowski, 10.1038/s41539-019-0046-0MJP Science Learning. 2019</p>
<p>Dropout Rates of Massive Open Online courses: Behavioural Patterns. EDULEARN14 proceedings. D F Onah, J Sinclair, R Boyatt, 2014</p>
<p>Openai, OpenAI Customer Stories: Khan Academy. 2023. 7 Feb. 2024</p>
<p>Training Language Models to Follow Instructions with Human Feedback. L Ouyang, 2022</p>
<p>Overview of Friedman's test and post-hoc analysis. D G Pereira, A Afonso, F M Medeiros, Communications in Statistics-Simulation and Computation. 44102015</p>
<p>Development and Validation of a Universal Science Writing Rubric that is Applicable to Diverse Genres of Science Writing. A Pisano, A Crawford, H Huffman, B Graham, N Kelp, 10.1128/jmbe.00189-21Journal of Microbiology and Biology Education. 222021</p>
<p>Learning to Improve: Using Writing to Increase Critical Thinking Performance in General Education Biology. I J Quitadamo, M J Kurtz, CBE-Life Sciences Education. 62007</p>
<p>J Reich, J A Ruipérez-Valiente, The MOOC pivot. 2019363</p>
<p>Sampling techniques to improve big data exploration. J A Ramos Rojas, M Beth Kery, S Rosenthal, A Dey, 10.1109/LDAV.2017.8231848IEEE 7th Symposium on Large Data Analysis and Visualization (LDAV). 2017. 2017</p>
<p>Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests. N M Razali, Y B Wah, Journal of statistical modeling and analytics. 212011</p>
<p>M Ryan, The Societal and Ethical Impacts of Artificial Intelligence in Agriculture: Mapping Agricultural AI Literature. AI and Society. 202338</p>
<p>AI-Driven Assessment of Students: Current Uses and Research Trends. J C Sánchez-Prieto, A Gamazo, J Cruz-Benito, R Therón, F J García-Peñalvo, 10.1007/978-3-030-50513-4_22Learning and Collaboration Technologies. Design, Experiences. 7th International Conference. P Zaphiris, A Ioannou, Copenhagen, DenmarkSpringer Nature20202020</p>
<p>By the Numbers: MOOCs in 2021. D Shah, Class Central. 2021</p>
<p>Learner-centric MOOC Model: A Pedagogical Design Model Towards Active Learner Participation and Higher Completion Rates. V Shah, S Murthy, J Warriem, S Saharasbudhe, G Banergee, S Iyer, Educational Technology Research and Development. 702022</p>
<p>Text Mining and Natural Language Processing in Construction. Automation in Construction. A Shamshiri, K R Ryu, J Y Park, 10.1016/j.autcon.2023.1052002024158</p>
<p>An analysis of variance test for normality (complete samples). S S Shapiro, M B Wilk, Biometrika. 523-41965</p>
<p>P E Shrout, J L Fleiss, Intraclass correlations: uses in assessing rater reliability. 197986420</p>
<p>D D Stevens, A J Levi, 10.4324/9781003445432Introduction to Rubrics: An Assessment Tool to Save Grading Time, Convey Effective Feedback, and Promote Student Learning. 2012Routledge</p>
<p>Large Language Models in Medicine. A J Thirunavukasaru, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature Medicine. 292023</p>
<p>Peer Assessment in a Project-Based Engineering Course: Comparing Between On-Campus and Online Learning Environments. M Usher, M Barak, Assessment &amp; Evaluation in Higher Education. 4352018</p>
<p>Autograder: A Feature-Based Quantitative Essay Grading System Using BERT. R R Vanga, C Sindhu, M S Bharath, T C Reddy, M Kanneganti, ICT Infrastructure and Computing. ICT4SD 2023. Lecture Notes in Networks and Systems. M Tuba, S Akashe, A Joshi, 2023</p>
<p>. Singapore Springer, 10.1007/978-981-99-4932-8_8</p>
<p>Content or platform: Why do students complete MOOCs. Y Wang, R Baker, MERLOT Journal of Online Learning and Teaching. 1112015</p>
<p>Y Wang, R G Reddy, Z M Mujahid, A Arora, A Rubashevskii, J Geng, O M Afzal, L Pan, N Borenstein, A Pillai, I Augenstein, Y Gurevych, P Nakov, Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output. 2023</p>
<p>Reducing Workload in Short Answer Grading Using Machine Learning. R Weegar, P Idestam-Almquist, International Journal of Artificial Intelligence in Education. 3422024</p>
<p>J Ye, X Chen, N Xu, C Zu, Z Shao, S Liu, Y Cui, Z Zhao, C Gong, Y Shen, J Zhou, S Chen, T Gui, Q Zhang, X Huang, A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models. 2023</p>
<p>Reflections on the Last Decade of MOOC Research. A M F Yousef, T Sumner, Computer Applications in Engineering Education. 2942021</p>
<p>MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification. X Zeng, A Zubiaga, 2024</p>
<p>AI Technologies for Education: Recent Research and Future Directions. K Zhang, A B Aslan, Computers and Education: Artificial Intelligence. 22021. 100025</p>            </div>
        </div>

    </div>
</body>
</html>
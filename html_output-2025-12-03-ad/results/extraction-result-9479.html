<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9479 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9479</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9479</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-6f702f88f0d74a24f1ff06211975b30839ac2c58</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6f702f88f0d74a24f1ff06211975b30839ac2c58" target="_blank">Thousands of AI Authors on the Future of AI</a></p>
                <p><strong>Paper Venue:</strong> Journal of Artificial Intelligence Research</p>
                <p><strong>Paper TL;DR:</strong> There was broad agreement that research aimed at minimizing risks from AI systems ought to be more prioritized and research aimed at minimizing risks from AI systems ought to be more prioritized.</p>
                <p><strong>Paper Abstract:</strong> In October 2023, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace, nature and impacts of AI progress. Significant steps were taken to minimize and evaluate bias. In evaluations of participation bias, we found that most groups responded at similar rates. The participants estimated that several milestones had at least a 50% chance of being feasible for AI by 2028, including constructing a payment processing site and fine-tuning an LLM. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027 and 50% by 2047—13 years earlier than in our 2022 survey (N = 738). The chance of all occupations becoming fully automatable, however, was not expected to reach 10% until 2037, and 50% until 2116 (compared to 2164 in the 2022 survey. 
Most respondents expressed substantial uncertainty about long-term impacts: While 68% in 2023 thought good outcomes from high-level machine intelligence AI were more likely than bad ones, 48% of these net optimists gave at least a 5% chance of extremely bad outcomes. Conversely, 59% of net pessimists gave 5% or more to extremely good outcomes. Depending on how we asked, between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that “substantial” or “extreme” concern is warranted about AI increasing misinformation, boosting authoritarian control, worsening inequality, and other scenarios. There was broad agreement that research aimed at minimizing risks from AI systems ought to be more prioritized.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9479",
    "paper_id": "paper-6f702f88f0d74a24f1ff06211975b30839ac2c58",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0039025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>THOUSANDS OF AI AUTHORS ON THE Future OF AI</h1>
<h2>PREPRINT</h2>
<p>Katja Grace ${ }^{* \dagger \ddagger}$ AI Impacts Berkeley, California United States katja@aiimpacts.org</p>
<h2>Ben Weinstein-Raun</h2>
<p>Independent
Berkeley, California
United States</p>
<p>Harlan Stewart ${ }^{\dagger}$ AI Impacts Berkeley, California United States</p>
<p>Julia Fabienne Sandkühler ${ }^{\dagger}$ Department of Psychology University of Bonn Germany</p>
<p>Stephen Thomas ${ }^{\dagger}$ AI Impacts Berkeley, California United States</p>
<p>Jan Brauner ${ }^{\ddagger}$<br>Department of Computer Science<br>University of Oxford<br>United Kingdom</p>
<p>Richard C. Korzekwa ${ }^{\dagger \ddagger}$<br>AI Impacts<br>Berkeley, California<br>United States</p>
<p>January 2024</p>
<h2>ABSTRACT</h2>
<p>In the largest survey of its kind, we surveyed 2,778 researchers who had published in top-tier artificial intelligence (AI) venues, asking for their predictions on the pace of AI progress and the nature and impacts of advanced AI systems. The aggregate forecasts give at least a $50 \%$ chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at $10 \%$ by 2027 , and $50 \%$ by 2047 . The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach $10 \%$ by 2037 , and $50 \%$ as late as 2116 (compared to 2164 in the 2022 survey).
Most respondents expressed substantial uncertainty about the long-term value of AI progress: While $68.3 \%$ thought good outcomes from superhuman AI are more likely than bad, of these net optimists $48 \%$ gave at least a $5 \%$ chance of extremely bad outcomes such as human extinction, and $59 \%$ of net pessimists gave $5 \%$ or more to extremely good outcomes. Between $37.8 \%$ and $51.4 \%$ of respondents gave at least a $10 \%$ chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that "substantial" or "extreme" concern is warranted about six different AI-related scenarios, including spread of false information, authoritarian population control, and worsened inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Artificial intelligence appears poised to reshape society. Decision-makers are working to address opportunities and threats due to AI in the private sector [OpenAI, 2023], academia [Center for Human-compatible Artificial Intelligence, 2023], and government at the state, national, and international levels [Newsom, 2023, AI.gov, 2023, Inter-Agency Working Group on Artificial Intelligence, 2022].
Navigating this situation requires judgments about how the progress and impact of AI are likely to unfold. However, there is a lack of apparent consensus among AI experts on the future of AI [Korzekwa and Stewart, 2023]. These judgments are difficult, and there are no established methods of making them well. Thus we must combine various noisy methods, such as extrapolating progress trends [Villalobos, 2023]; reasoning about reference classes of similar events [Grace et al., 2021]; analyzing the nature of agents [Omohundro, 2008]; probing qualities of current AI systems and techniques [Park et al., 2023]; applying economic models to AI scenarios [Jones, 2023, Trammell and Korinek, 2023]; and relying on forecasting aggregation systems such as markets, professional forecasters, and the judgments of various subject matter experts.
One important source of evidence comes from the predictions of AI researchers. Their familiarity with the technology and the dynamics of its past progress puts them in a good position to make educated guesses about the future of AI. However, they are experts in AI research, not AI forecasting and might thus lack generic forecasting skills and experience, or expertise in non-technical factors that influence the trajectory of AI. While AI experts' predictions should not be seen as a reliable guide to objective truth, they can provide one important piece of the puzzle.
We conducted a survey of 2,778 AI researchers who had published peer-reviewed research in the prior year in six top AI venues (NeurIPS, ICML, ICLR, AAAI, IJCAI, JMLR). This to our knowledge constitutes the largest survey of AI researchers to date. The survey took place in the fall of 2023, after an eventful year of broad progress (including the launch of ChatGPT and GPT-4, Google's Bard, Bing AI Chat, Anthropic's Claude 1 and 2), shift in public awareness of AI issues (including two widely signed and publicized AI safety letters [Future of Life Institute, 2023, Center for AI Safety, 2023]), and governments beginning to address questions of AI regulation in the US, UK, and EU [Biden, 2023, Amodei, 2023, gov.uk, 2023, European Parliament, 2023]. The survey included questions about the speed and dynamics of AI progress, and the social consequences of more advanced AI.</p>
<h2>2 The Survey</h2>
<p>This survey, the "2023 Expert Survey on Progress in AI," or ESPAI, is the third in a series of very similar surveys. The first two were conducted in 2016 [Grace et al., 2018a] and 2022 [Grace et al., 2022]. The 2023 survey included around four times as many participants as the 2022 survey by expanding from two publication venues (NeurIPS and ICML) to six. It also includes several new questions to probe the nature of future AI systems and diverse potential risks. The survey complements a collection of recent work gathering views on similar questions from the public [Stein-Perlman, 2023] and corporate leadership [Chui et al., 2023]. The full set of questions is available from AI Impacts [2023a].
Most questions solicited responses in one of three ways: on a Likert scale (multiple choice along a single axis); a probability estimate; or an estimate of a future year. A smaller number of questions asked for write-in responses or numerical estimates.
The way a question is framed can greatly influence the response, and this seems more likely for complex questions [Tversky and Kahneman, 1981]. To assess and mitigate framing effects, we often posed different variations of questions on the same topic to different random subsets of respondents. For example, all questions about how soon a milestone would be reached were framed in two ways: fixed-years and fixed-probabilities. Half of respondents were asked to estimate the probability that a milestone would be reached by a given year ("fixed-years framing"), while the other half were asked to estimate the year by which the milestone would be feasible with a given probability ("fixed-probabilities framing"). To minimize confusion, each participant received one framing throughout the survey.
In several parts of the survey, each participant randomly received questions on only one of several topics, to keep the survey brief. We allocated these questions to differently sized subsets of participants based on factors like the importance of the question and the relative value of a larger sample size. This means that most questions were not assigned to all 2,778 participants (see Section 6: Methods).
Several questions asked participants to estimate how many years until a milestone will be feasible. In these questions, we asked participants to provide three year-probability pairs (either via the fixed-years framing or fixed-probabilities framing described above), which we used to approximate a probability distribution for that participant by fitting a gamma cumulative distribution function to these points.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Most milestones are predicted to have better than even odds of happening within the next ten years, though with a wide range of plausible dates. The figure shows aggregate distributions over when selected milestones are expected, including 39 tasks, four occupations, and two measures of general human-level performance (see Section 3.2), shown as solid circles, open circles, and solid squares respectively. Circles/squares represent the year where the aggregate distribution gives a milestone a $50 \%$ chance of being met, and intervals represent the range of years between $25 \%$ and $75 \%$ probability. Note that these intervals represent an aggregate of uncertainty expressed by participants, not estimation uncertainty. The displayed milestone descriptions are summaries; for full descriptions, see Appendix C.</p>
<h1>3 Results on AI Progress</h1>
<h3>3.1 How soon will 39 tasks be feasible for AI?</h3>
<p>The survey asked about when each of 39 tasks would become feasible, where "feasible" was described as meaning "one of the best resourced labs could implement it in less than a year if they chose to. Ignore the question of whether they would choose to." Each respondent was asked about four tasks, so that each task received around 250 estimates. Each respondent gave three probability-year pairs per task.
To aggregate the responses, we first fit a gamma distribution to each participant's three probability-year pairs, and then computed the mean across the participants' individual gamma distributions.
All but four of the 39 tasks were predicted to have at least a $50 \%$ chance of being feasible within the next ten years (Figure 1). This includes several economically very valuable tasks-such as coding an entire payment processing site from scratch and writing new songs indistinguishable from real ones by hit artists such as Taylor Swift. It also includes tasks that imply substantial progress in sample-efficiency (e.g. 'Beat novices in $50 \%$ of Atari games after 20 minutes of play'), AI-driven AI progress (e.g. autonomously fine-tuning an open-source LLM), and robotics (e.g. folding laundry).
The six tasks expected to take longer than ten years were: "After spending time in a virtual world, output the differential equations governing that world in symbolic form" (12 years), "Physically install the electrical wiring in a new home" (17 years), "Research and write" (19 years) or "Replicate" (12 years) "a high-quality ML paper," "Prove mathematical theorems that are publishable in top mathematics journals today" (22 years), and solving "long-standing unsolved problems in mathematics" such as a Millennium Prize problem (27 years).</p>
<h3>3.1.1 Comparison with 2022</h3>
<p>32 AI task questions were identical to those in the 2022 survey, as well as the 2016 survey [Grace et al., 2018a], with the exception of minor edits to task descriptions for updated accuracy between 2016 and 2022 [AI Impacts, 2023b]. All tasks from Grace et al. [2018a] were included, regardless of whether the authors would judge them to be achieved. The survey population included more conferences in 2023, but this did not appear to have a notable effect on opinion (See 5.2.3). Figure 2 shows how expected dates for reaching these milestones shifted from 2022 to 2023.
Between 2022 and 2023, aggregate predictions for 21 out of 32 tasks moved earlier. The aggregate predictions for 11 tasks moved later.
On average, for the 32 tasks included in both the 2022 and 2023 surveys, the $50^{\text {th }}$ percentile year they were expected to become feasible shifted 1.0 years earlier $(\mathrm{SD}=2.0, \mathrm{SE}=0.18)$.</p>
<h3>3.2 How soon will human-level performance on all tasks or occupations be feasible?</h3>
<p>We asked how soon participants expected AI systems to outperform humans across all activities, which we framed in two ways: as either tasks, in the question about "High-Level Machine Intelligence" (HLMI), or occupations, in the question about "Full Automation of Labor" (FAOL).</p>
<h3>3.2.1 How soon will 'High-Level Machine Intelligence' be feasible?</h3>
<p>We defined High-Level Machine Intelligence (HLMI) thus:
High-level machine intelligence (HLMI) is achieved when unaided machines can accomplish every task better and more cheaply than human workers. Ignore aspects of tasks for which being a human is intrinsically advantageous, e.g. being accepted as a jury member. Think feasibility, not adoption.</p>
<p>We asked for predictions, assuming "human scientific activity continues without major negative disruption." We aggregated the results $(\mathrm{n}=1,714)$ by fitting gamma distributions, as with individual task predictions in 3.1.
In both 2022 and 2023, respondents gave a wide range of predictions for how soon HLMI will be feasible (Figure 3). The aggregate 2023 forecast predicted a $50 \%$ chance of HLMI by 2047, down thirteen years from 2060 in the 2022 survey. For comparison, in the six years between the 2016 and 2022 surveys, the expected date moved only one year earlier, from 2061 to $2060^{1}$.
The aggregate 2023 forecast predicted a $10 \%$ chance of HLMI by 2027, down two years from 2029 in the 2022 survey. See Appendix B for a table comparing this with Full Automation of Labor, which is discussed below.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Expected feasibility of many AI milestones moved substantially earlier in the course of one year (between 2022 and 2023). The milestones are sorted (within each scale-adjusted chart) by size of drop from 2022 forecast to 2023 forecast, with the largest change first. The year when the aggregate distribution gives a milestone a $50 \%$ chance of being met is represented by solid circles, open circles, and solid squares for tasks, occupations, and general human-level performance respectively. The three groups of questions have different formats that may also influence answers. For full descriptions of the summarized milestones, see Appendix C.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Aggregate forecast for $50^{\text {th }}$ percentile arrival time of High-Level Machine intelligence (HLMI) dropped by 13 years between 2022 and 2023. The forecast for $50^{\text {th }}$ percentile arrival time of Full Automation of Labor (FAOL) dropped by 48 years in the same period. However, there is still a lot of uncertainty. "Aggregate Forecast" is the mean distribution over all individual cumulative distribution functions. For comparison, we included the 2022 Aggregate Forecast. To give a sense of the range of responses, we included random subsets of individual 2023 and 2022 forecasts. Note that the thinner 'confidence interval' in 2023 (compared to 2022) is due to our increased confidence about the average respondents' views due to a larger sample size, not respondents' predictions converging.</p>
<h1>3.2.2 How soon will 'Full Automation of Labor' be feasible?</h1>
<p>The other framing of the question about how soon AI systems would outperform humans across all activities was about "Full Automation of Labor," or FAOL. We defined FAOL thus:</p>
<p>Say an occupation becomes fully automatable when unaided machines can accomplish it better and more cheaply than human workers. Ignore aspects of occupations for which being a human is intrinsically advantageous, e.g. being accepted as a jury member. Think feasibility, not adoption. [...]
Say we have reached 'full automation of labor' when all occupations are fully automatable. That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers.</p>
<p>Before the participants ( $\mathrm{n}=774$ ) were asked about the full automation of labor, respondents were asked when four specific occupations would become fully automatable: "Truck driver," "Surgeon," "Retail salesperson," and "AI researcher" (Figure 1). They were also asked to think of an existing human occupation that they thought would be among the final ones to be fully automatable. They were then asked when 'full automation of labor' (FAOL) would be achieved.
The aggregate 2023 forecast predicted a $50 \%$ chance of FAOL by 2116, down 48 years from 2164 in the 2022 survey (Figure 3). We checked if this difference was significant for participants who received the question in the fixedprobabilities framing, and found that it was ( $\mathrm{p}=.0052$, Yuen's test (bootstrap version); see Appendix F). There is about a 70-year difference between the mean $50 \%$ prediction for HLMI and the mean $50 \%$ prediction for FAOL. We discuss this surprising finding in the next section, "Framing effect of HLMI vs FAOL."
Compared to 2016, 2023 has earlier 50\% estimates but later 10\% estimates. (See Appendix B)
The answers to the write-in question about an existing occupation likely to be among the last automatable were categorized according to O<em>NET's All Job Family Occupations categories [O</em>NET, 2023]. The top five mostsuggested categories were: "Computer and Mathematical" ( 91 write-in answers in this category), "Life, Physical, and Social Science" ( 77 answers), "Healthcare Practitioners and Technical" (56), "Management" (49), and "Arts, Design, Entertainment, Sports, and Media" (39).
The answers to the write-in question about an existing occupation likely to be among the last automatable were categorized according to $\mathrm{O} * \mathrm{NET}^{\prime}$ 's categories [O*NET, 2023]. The top five most-suggested categories were: "Computer and Mathematical" ( 91 write-in answers in this category), "Life, Physical, and Social Science" ( 77 answers), "Healthcare Practitioners and Technical" (56), "Management" (49), and "Arts, Design, Entertainment, Sports, and Media" (39).</p>
<h3>3.2.3 Differences between HLMI and FAOL</h3>
<p>Predictions for a $50 \%$ chance of the arrival of FAOL are consistently more than sixty years later than those for a $50 \%$ chance of the arrival of HLMI. This was seen in the results from the surveys of 2023, 2022, and 2016. This is surprising because HLMI and FAOL are quite similar: FAOL asks about the automation of all occupations; HLMI asks about the feasible automation of all tasks. Since occupations might naturally be understood either as complex tasks, composed of tasks, or closely connected with one of these, achieving HLMI seems to either imply having already achieved FAOL, or suggest being close.
We do not know what accounts for this gap in forecasts. Insofar as HLMI and FAOL refer to the same event, the difference in predictions about the time of their arrival would seem to be a framing effect.
However, the relationship between "tasks" and "occupations" is debatable. And the question sets do differ beyond definitions: only the HLMI questions are preceded by the instruction to "assume that human scientific activity continues without major negative disruption," and the FAOL block asks a sequence of questions about the automation of specific occupations before asking about full automation of labor. So conceivably this wide difference could be caused by respondents expecting major disruption to scientific progress, or by the act of thinking through specific examples shifting overall anticipations. From our experience with question testing, it also seems possible that the difference is due to other differences in interpretation of the questions, such as thinking of automating occupations but not tasks as including physical manipulation, or interpreting FAOL to require adoption of AI in automating occupations, not mere feasibility (contrary to the question wording).</p>
<h1>3.2.4 Demographic differences</h1>
<p>Geographical background was correlated with expectations about the timing of human-level performance: respondents whose undergraduate education was in Asia anticipated an 11 year earlier arrival of HLMI than participants from Europe, North America, or other regions combined. See Appendix A for more demographic comparisons.</p>
<h3>3.2.5 Do participants think they agree on timing of HLMI?</h3>
<p>We asked respondents a set of "meta" questions about their views on others' views ( $\mathrm{n}=671$ ). One meta question asked to what extent they thought that they disagreed with the typical AI researcher about when HLMI would exist. $44 \%$ said "Not much," $46 \%$ said "A moderate amount," and $10 \%$ said "A lot."</p>
<h3>3.3 Framing effect of fixed-years vs fixed-probabilities</h3>
<p>All questions about how soon a milestone would be reached were framed in two ways: fixed-years and fixed-probabilities. In either framing, we ask for three year-probability pairs, but in one we fix a set of probabilities ( $10 \%, 50 \%, 90 \%$ ) and ask how many years until the participant would assign each probability to the milestone being met, whereas in the other framing we fix a set of future years (usually 10 years, 20 years, 50 years) and ask about the probability of the milestone occurring by that year.
The fixed-years framing has been previously observed to produce systematically later predictions [Grace et al., 2018a, 2022], but we do not know if one framing is more accurate than the other. Here we have used both and combined them with equal weight.
The previously-observed framing effect was again observed in this survey. For example, the year with a $50 \%$ chance of HLMI from participants answering in the fixed-year frame ( 34 years) was twice as far into the future as that for participants answering in the fixed-probability frame ( 17 years). However, it's notable that even the larger of these two is shorter than 2022's combined forecast ( 37 years), demonstrating a substantial shift of predictions closer to the present (Fig 18 in Appendix B).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Most respondents indicated that the pace of progress in their area of AI increased between the first and second half of their time in a field. Participants were asked whether the second half of the time they had spent working in their area of AI saw more progress than the first half. The median time working in the area was 5 years.</p>
<h3>3.4 Change in observed rates of progress</h3>
<p>We asked respondents which AI area they had worked in for the longest and whether progress in the second half was faster than the first (Figure 4).</p>
<h1>3.5 What causes AI progress?</h1>
<p>We asked about the sensitivity of progress in AI capabilities to changes in five inputs: 1) researcher effort, 2) decline in cost of computation, 3) effort put into increasing the size and availability of training datasets, 4) funding, and 5) progress in AI algorithms. We asked respondents to imagine that only half as much of each input had been available over the past decade, ${ }^{2}$ and the effect they would expect this to have had on the rate of AI progress. The results are shown in Figure 5.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Estimated reduction in AI progress if inputs had been halved over the past decade. Red dots represent means. Boxes contain the $25^{\text {th }}$ to $75^{\text {th }}$ percentile range; middle lines are medians. Whiskers are the least and greatest values that are not more than 1.5 times the interquartile range from the median. Participants estimated that halving the drop in costs of computing would have had the greatest effect on AI progress over the last decade, while halving 'researcher effort' and 'progress in AI algorithms' would have had the least effect. Overall, all the included inputs were seen as having contributed substantially to AI progress.</p>
<p>There was a wide range of views about each input, implying a large degree of uncertainty. Additionally, the relatively even distribution of predictions cuts against a common narrative that progress in cheap computing is the dominant driver of AI progress. Across all inputs, we observe many more answers of " $0 \%$ " (no difference) and " $100 \%$ " (all AI progress lost) than we would expect, which suggests to us possible misunderstandings of the question.</p>
<h3>3.6 Will there be an intelligence explosion?</h3>
<p>We asked respondents about the possibility, after HLMI is hypothetically achieved, of an 'intelligence explosion,' as explained in this question:</p>
<p>Some people have argued the following:
If AI systems do nearly all research and development, improvements in AI will accelerate the pace of technological progress, including further progress in AI.
Over a short period (less than 5 years), this feedback loop could cause technological progress to become more than an order of magnitude faster.
How likely do you find this argument to be broadly correct?
The results to this first question are shown in Figure 6.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Since 2016 a majority of respondents have thought that it's either "quite likely," "likely," or an "about even chance" that technological progress becomes more than an order of magnitude faster within 5 years of HLMI being achieved.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Median probability $(\mathrm{N})$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2016</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">2023</td>
</tr>
<tr>
<td style="text-align: center;">Explosive progress 2 years after HLMI</td>
<td style="text-align: center;">$20 \%(225)$</td>
<td style="text-align: center;">$20 \%(339)$</td>
<td style="text-align: center;">$20 \%(298)$</td>
</tr>
<tr>
<td style="text-align: center;">Explosive progress 30 years after HLMI</td>
<td style="text-align: center;">$80 \%(225)$</td>
<td style="text-align: center;">$80 \%(339)$</td>
<td style="text-align: center;">$80 \%(297)$</td>
</tr>
<tr>
<td style="text-align: center;">Intelligence explosion argument is broadly correct</td>
<td style="text-align: center;">"41-60\%" (232)</td>
<td style="text-align: center;">"41-60\%" (386)</td>
<td style="text-align: center;">"41-60\%" (299)</td>
</tr>
<tr>
<td style="text-align: center;">AI is vastly better than humans 2 years after HLMI</td>
<td style="text-align: center;">$10 \%(213)$</td>
<td style="text-align: center;">$10 \%(371)$</td>
<td style="text-align: center;">$10 \%(281)$</td>
</tr>
<tr>
<td style="text-align: center;">AI is vastly better than humans 30 years after HLMI</td>
<td style="text-align: center;">$50 \%(214)$</td>
<td style="text-align: center;">$60 \%(371)$</td>
<td style="text-align: center;">$60 \%(282)$</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of three questions regarding a hypothetical intelligence explosion have remained remarkably stable since 2016.</p>
<p>The other two 'intelligence explosion' questions were about the likelihood of a dramatically increased rate of global technological advancement 2 and 30 years post-HLMI, and AI that can outperform humans across all professions 2 and 20 years post-HLMI. Results are shown in Table 1.</p>
<p>In sum, across these three questions, the median participant did not overall expect something like a rapid acceleration of progress from an 'intelligence explosion,' but did give substantial credence to it.</p>
<p>Further figures related to intelligence explosion questions are in Appendix B.</p>
<h1>3.7 What will AI systems in 2043 be like?</h1>
<p>Concerns about risks from future AI systems are often linked to specific traits related to alignment, trustworthiness, predictability, self-directedness, capabilities, and jailbreakability. We asked respondents how likely it was that at least some state-of-the-art AI systems in 2043 would have each of eleven such traits ( $n \in[649,667]^{3}$ ).
All 11 traits were considered to have a relatively high chance of existing in AI systems in 2043, though with much uncertainty. Only one trait had a median answer below 'even chance': "Take actions to attain power." While there was no consensus even on this trait, it's notable that it was deemed least likely, because it is arguably the most sinister, being key to an argument for extinction-level danger from AI [Carlsmith, 2022].</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Respondents' estimates of the likelihood that at least some AI systems in 2043 will have each of these traits; organized from least to most likely.</p>
<p>Answers reflected substantial uncertainty and disagreement among participants. No trait attracted near-unanimity on any probability, and no more than $55 \%$ of respondents answered "very likely" or "very unlikely" about any trait. (Figure 7)
There were areas of agreement, however. For instance, a large majority of participants thought state-of-the-art AI systems in twenty years would be likely or very likely to:</p>
<ol>
<li>Find unexpected ways to achieve goals ( $82.3 \%$ of respondents),</li>
<li>Be able to talk like a human expert on most topics ( $81.4 \%$ of respondents), and</li>
<li>Frequently behave in ways that are surprising to humans ( $69.1 \%$ of respondents)</li>
</ol>
<h1>3.8 Will AI in 2028 truthfully and intelligibly explain its decisions?</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Most respondents considered it unlikely that users of AI systems in 2028 will be able to know the true reasons for the AI systems' choices, with only $20 \%$ giving it better than even odds. (n=912)</p>
<p>Uninterpretable reasoning in AI systems is often considered an AI risk factor, potentially leading to outcomes ranging from unjust biases in treatment of people to active pursuit of harm hidden by capable agents. We thus asked about the interpretability of AI systems in five years (Figure 8):</p>
<p>For typical state-of-the-art AI systems in 2028, do you think it will be possible for users to know the true reasons for systems making a particular choice? By "true reasons" we mean the AI correctly explains its internal decision-making process in a way humans can understand. By "true reasons" we do not mean the decision itself is correct.</p>
<p>This is related to the question in Section 3.7, which asked how likely it was that at least some state-of-the-art AI systems in 2043 (fifteen years later), "can be trusted to accurately explain their actions." The median answer there was "even chance" (40-60\% likely), compared to "unlikely" (10-40\%) on this question.</p>
<h1>4 Results on Social Impacts of AI</h1>
<h3>4.1 How concerning are 11 future AI-related scenarios?</h3>
<p>We asked participants ( $\mathrm{n}=1,345$ ) about eleven potentially concerning AI scenarios, such as AI-enabled misinformation, worsened economic inequality, and biased AI systems worsening injustice. We asked how much concern each deserved in the next thirty years (Figure 9).
Each scenario was considered worthy of either substantial or extreme concern by more than $30 \%$ of respondents. As measured by the percentage of respondents who thought a scenario constituted either a "substantial" or "extreme" concern, the scenarios worthy of most concern were: spread of false information e.g. deepfakes ( $86 \%$ ), manipulation of large-scale public opinion trends (79\%), AI letting dangerous groups make powerful tools (e.g. engineered viruses) (73\%), authoritarian rulers using AI to control their populations ( $73 \%$ ), and AI systems worsening economic inequality by disproportionately benefiting certain individuals ( $71 \%$ ).
There is some ambiguity about the reason why a scenario might be considered concerning: it might be considered especially disastrous, or especially likely, or both. From our results, there's no way to disambiguate these considerations.</p>
<h3>4.2 How good or bad for humans will High-Level Machine Intelligence be?</h3>
<p>We asked participants to assume that, at some point, "high-level machine intelligence" (HLMI) will exist, as defined in Section 3.2. Given this assumption for the sake of the question, we asked how good or bad they expect the overall impact of this to be "in the long run" for humanity.
Respondents exhibited diverse views on the future impact of advanced AI (Figure 10), highlighting how respondents' views are more complex than can be represented by an 'optimism vs pessimism' axis. Many people who have high probabilities of bad outcomes also have high probabilities of good outcomes. A majority spread their credence across the entire spectrum of outcomes, with $64 \%$ assigning non-zero probabilities to both extremely good and extremely bad scenarios. $68.3 \%$ of participants found good outcomes more likely than bad outcomes, while $57.8 \%$ considered extremely bad outcomes (e.g. human extinction) a nontrivial possibility ( $\geq 5 \%$ likely). Even among net optimists, nearly half ( $48.4 \%$ ) gave at least $5 \%$ credence to extremely bad outcomes, and among net pessimists, more than half ( $58.6 \%$ ) gave at least $5 \%$ to extremely good outcomes. The broad variance in credence in catastrophic scenarios shows there isn't strong evidence understood by all experts that this kind of outcome is certain or implausible.
The median prediction for extremely bad outcomes, such as human extinction, was 5\% (mean 9\%). Over a third of participants ( $38 \%$ ) put at least a $10 \%$ chance on extremely bad outcomes. This is comparable to, but somewhat lower than, rates of assigning at least $10 \%$ to extinction-level outcomes in answers to other questions more directly about extinction, between $41 \%$ and $51 \%$ (see Section 4.3). On the very pessimistic end, one in ten participants put at least a $25 \%$ chance on outcomes in the range of human extinction).
Since 2022, mean overall probability on extreme outcomes (good or bad) has fallen slightly (Figure 11). The proportion of people who put at least a $10 \%$ chance on extremely bad outcomes (e.g. human extinction) has fallen from $48 \%$ in 2022 in 2023, and the mean prediction for this type of outcome is down from $14 \%$ to $9.0 \%$.
Appendix A contains comparisons between results from different demographics on this question, and Appendix B contains another relevant figure.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Amount of concern potential scenarios deserve, organized from most to least extreme concern.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Respondents exhibited diverse views on the expected goodness/badness of High Level Machine Intelligence (HLMI). We asked participants to assume, for the sake of the question, that HLMI will be built at some point. The figure shows a random selection of 800 responses on the positivity or negativity of long-run impacts of HLMI on humanity. Each vertical bar represents one participant and the bars are sorted left to right by a weighted sum of probabilities corresponding to overall optimism. Responses range from extremely optimistic to extremely pessimistic. Over a third of participants ( $38 \%$ ) put at least a $10 \%$ chance on extremely bad outcomes (e.g. human extinction).</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Mean but not median predictions in 2023 ( $\mathbf{n = 2 7 0 4}$ ) about the consequences of HLMI have shifted slightly away from extreme outcomes compared to 2022 ( $\mathbf{n = 5 5 9}$ ). Error bars indicate the standard error.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Mean and median predictions to three questions on human extinction. Error bars indicate the standard error. (Question $1 \mathrm{n}=149$ in 2022 and 1321 in 2023. Question $2 \mathrm{n}=162$ in 2022 and 661 in 2023. Question 3 was asked only in 2023, $\mathrm{n}=655$ ).</p>
<h1>4.3 How likely is AI to cause human extinction?</h1>
<p>To further clarify views on the "extremely bad (e.g. human extinction)" scenarios in the question on overall impacts, participants were given one of three similar questions about human extinction. Their differences were intended to help isolate exactly how concerning different scenarios are, what respondents expect to happen, and how much difference working makes.</p>
<p>Answers to these questions were mostly consistent, with medians of $5 \%$ or $10 \%$. These are also close to answers to the question on general value of long-run impact, ${ }^{5}$, which might suggest the bulk of the "extremely bad (e.g. human extinction)" answers to that question is from human extinction or similarly permanent and severe disempowerment of the human species, as opposed to other outcomes that respondents to that question may have had in mind but that would have been less severe.</p>
<p>Depending on how we asked, between $41.2 \%$ and $51.4 \%$ of respondents estimated a greater than $10 \%$ chance of human extinction or severe disempowerment (see Figure 13). This is comparable to, but somewhat higher than, the proportion of respondents- $38 \%$-who assigned at least $10 \%$ to "extremely bad" outcomes "(e.g. human extinction)" in the question asking "How good or bad for humans will High-Level Machine Intelligence be?" (See Section 4.2)</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Statistics</th>
<th style="text-align: center;">2022 Result</th>
<th style="text-align: center;">2023 Result</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">What probability do you put on future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species?</td>
<td style="text-align: center;">N; Mean (SD); Median (IQR)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 149 ; 15.7 \%(22.1 \%) ; \ &amp; 5 \%(19 \%) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1321 ; 16.2 \%(23 \%) ; \ &amp; 5 \%(19 \%) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">What probability do you put on human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species?</td>
<td style="text-align: center;">N; Mean (SD); Median (IQR)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 162 ; 20.5 \%(26.2 \%) ; \ &amp; 10 \%(29 \%) \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 661 ; 19.4 \%(26 \%) ; \ &amp; 10 \%(29 \%) \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">What probability do you put on future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species within the next $\mathbf{1 0 0}$ years? ${ }^{4}$</td>
<td style="text-align: center;">N; Mean (SD); Median (IQR)</td>
<td style="text-align: center;">Not asked</td>
<td style="text-align: center;">$\begin{aligned} &amp; 655 ; 14.4 \%(22.2 \%) ; \ &amp; 5 \%(19.9 \%) \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Respondents' estimates in 2022 and 2023 for the probability that AI causes human extinction. For each of the two questions that were asked in both years, the results are very similar.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Percent of participants who gave a probability of $\mathbf{1 0 \%}$ or higher to an extremely bad outcome of HLMI (e.g. human extinction) (see Section 4.2) or to the three questions specifically about human extinction or disempowerment.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Portion of respondents</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Much slower</td>
<td style="text-align: center;">$4.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Somewhat slower</td>
<td style="text-align: center;">$29.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Current speed</td>
<td style="text-align: center;">$26.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Somewhat faster</td>
<td style="text-align: center;">$22.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Much faster</td>
<td style="text-align: center;">$15.6 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: There was disagreement about whether faster or slower global AI progress over the next five years would be best for humanity's future.</p>
<p>Appendix A contains comparisons between results from different demographics on this question.</p>
<h1>4.4 Views on others' concerns about AI</h1>
<p>Respondents were asked a set of "meta" questions about their views on others' views ( $\mathrm{n}=671$ ). One question was, "To what extent do you think people's concerns about future risks from AI are due to misunderstandings of AI research?" $10.7 \%$ said "Almost entirely." $44 \%$ said "To a large extent." $29.1 \%$ said "Somewhat." $14.6 \%$ said "Not much." $1.6 \%$ said "Hardly at all." This may reflect a view amongst AI researchers that the general public misunderstands AI.</p>
<h3>4.5 What rate of AI progress would produce the most optimism?</h3>
<p>We asked participants "What rate of global AI progress over the next five years would make you feel most optimistic for humanity's future? Assume any change in speed affects all projects equally." There was disagreement on whether faster or slower progress would be preferable, though large divergence from the current speed was less popular (Table 3).</p>
<h3>4.6 How much should AI safety research be prioritized?</h3>
<p>We asked some respondents one version of a question about AI research prioritization that matched previous surveys, and we asked other respondents a near-identical question that had been slightly updated. In the old version of the question, we defined AI safety research as follows:</p>
<p>Let 'AI safety research' include any AI-related research that, rather than being primarily aimed at improving the capabilities of AI systems, is instead primarily aimed at minimizing potential risks of AI systems (beyond what is already accomplished for those goals by increasing AI system capabilities).
Examples of AI safety research might include:</p>
<ul>
<li>Improving the human-interpretability of machine learning algorithms for the purpose of improving the safety and robustness of AI systems, not focused on improving AI capabilities</li>
<li>Research on long-term existential risks from AI systems</li>
<li>AI-specific formal verification research</li>
<li>Policy research about how to maximize the public benefits of AI</li>
</ul>
<p>The updated question is identical except for the inclusion of this example:</p>
<ul>
<li>Developing methodologies to identify, measure, and mitigate biases in AI models to ensure fair and ethical decision-making</li>
</ul>
<p>In both variations, we asked, "How much should society prioritize AI safety research, relative to how much it is currently prioritized?" A Welch t-test found that the difference between the two framings was not significant $(t(1327)=-0.58, p=0.564, d=-0.03)$, so the results were combined $(\mathrm{n}=1329)$. A large majority of respondents thought that AI safety research should be prioritized more than it currently is. The percentage of researchers who thought so increased compared to earlier surveys, but only slightly since 2022. (Figure 14)</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: $\mathbf{7 0 \%}$ of respondents thought AI safety research should be prioritized more than it currently is. Developments since the 2022 survey have not substantially changed the proportion of participants who think AI safety should be prioritized "more" ore "much more".</p>
<h1>4.7 How worthy and hard is the alignment problem?</h1>
<p>A second set of AI safety questions was based on Stuart Russell's formulation of the alignment problem [Russell, 2014]. This set of questions began with a summary of Russell's argument-which claims that with advanced AI, "you get exactly what you ask for, not what you want"-then asked:</p>
<ol>
<li>Do you think this argument points at an important problem?</li>
<li>How valuable is it to work on this problem today, compared to other problems in AI?</li>
<li>How hard do you think this problem is, compared to other problems in AI?</li>
</ol>
<p>The majority of respondents said that the alignment problem is either a "very important problem" (41\%) or "among the most important problems in the field" (13\%), and the majority said the it is "harder" (36\%) or "much harder" (21\%) than other problems in AI. However, respondents did not generally think that it is more valuable to work on the alignment problem today than other problems. (Figure 15)</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Attitudes towards Stuart Russell's formulation of the alignment problem. Participants viewed the alignment problem as important and difficult, but not more valuable to work on than other problems.</p>
<h1>5 Discussion</h1>
<h3>5.1 Summary of results</h3>
<p>Participants expressed a wide range of views on almost every question: some of the biggest areas of consensus are on how wide-open possibilities for the future appear to be. This uncertainty is striking, but several patterns of opinion are particularly informative.
While the range of views on how long it will take for milestones to be feasible can be broad, this year's survey saw a general shift towards earlier expectations. Over the fourteen months since the last survey [Grace et al., 2022], a similar participant pool expected human-level performance 13 to 48 years sooner on average (depending on how the question was phrased), and 21 out of 32 shorter term milestones are now expected earlier.
Another striking pattern is widespread assignment of credence to extremely bad outcomes from AI. As in 2022, a majority of participants considered AI to pose at least a $5 \%$ chance of causing human extinction or similarly permanent and severe disempowerment of the human species, and this result was consistent across four different questions, two assigned to each participant. Across these same questions, between $38 \%$ and $51 \%$ placed at least $10 \%$ chance on advanced AI bringing these extinction-level outcomes (see Figure 13).
In general, there were a wide range of views about expected social consequences of advanced AI, and most people put some weight on both extremely good outcomes and extremely bad outcomes. While the optimistic scenarios reflect AI's potential to revolutionize various aspects of work and life, the pessimistic predictions-particularly those involving extinction-level risks-serve as a stark reminder of the high stakes involved in AI development and deployment.
Concerns were expressed over many topics beyond human extinction: over half of eleven potentially concerning AI scenarios were deemed either "substantially" or "extremely" concerning by over half of respondents.</p>
<h3>5.2 Caveats and limitations</h3>
<h3>5.2.1 Forecasting is hard, even for experts</h3>
<p>Forecasting is difficult in general, and subject-matter experts have been observed to perform poorly [Tetlock, 2005, Savage et al., 2021]. Our participants' expertise is in AI, and they do not, to our knowledge, have any unusual skill at forecasting in general.
There are signs in this research and past surveys that these experts are not accurate forecasters across the range of questions we ask. For one thing, on many questions different respondents give very different answers, which limits the number of them who can be close to the truth. Nonetheless, in other contexts, averages from a large set of noisy predictions can still be relatively accurate [Surowiecki, 2004], so a question remains as to how informative these aggregate forecasts are.
Another piece of evidence against the accuracy of forecasts is the observation of substantial framing effects (See Sections 3.2 and 3.3). If seemingly unimportant changes in question framing lead to large changes in responses, this suggests that even aggregate answers to any particular question are not an accurate guide to the answer. In an extreme example in a closely related study, Karger et al. [2023] found college graduates gave answers nearly six orders of magnitude apart when asked in different ways to estimate the size of existential risks from AI: When given example odds of low-probability events, estimates were much lower. A similar effect might apply at some scale to our participants, though their expertise and quantitative training might mitigate it. Participants who had thought more in the past about AI risks seem to give higher numbers, suggesting they are unlikely to give radically lower numbers with further examples of risks (see Table 4).
Despite these limitations, AI researchers are well-positioned to contribute to the accuracy of our collective guesses about the future. While unreliable, educated guesses are what we must all rely on, and theirs are informed by expertise in the relevant field. These forecasts should be part of a broader set of evidence from sources such as trends in computer hardware, advancements in AI capabilities, economic analyses, and insights from forecasting experts. However, AI researchers' familiarity with the relevant technology, and experience with the dynamics of its progress, make them among the best-positioned to make informative educated guesses.</p>
<h3>5.2.2 Participation</h3>
<p>The survey was taken by $15 \%$ of those we contacted. This appears to be within the typical range for a large expert survey. Based on an analysis by Hamilton [2003], the median response rate across 199 surveys was $26 \%$, and larger invitation lists tend to yield lower response rates: surveys sent to over 20,000 people, like ours, are expected to have</p>
<p>a response rate in the range of $10 \%$. In specialized samples, such as scientists, lower response rates are common due to sampling challenges, as exemplified by on Assessing Fundamental Attitudes of Life Scientists as a Basis for Biosecurity Education [2009], which achieved a $15.7 \%$ response rate in a survey of life scientists.</p>
<p>As with any survey, our results could be skewed by participation bias, if participants had systematically different opinions than those who chose not to participate. We sought to minimize this effect by aiming to maximize response rate, and by limiting cues about the survey content available before opting to take the survey. We looked for evidence of response bias at the survey level and question level for some questions, and did not find any that would affect the results to a large extent (see Appendix D for more detail).</p>
<h1>5.2.3 Change in sample from 2022</h1>
<p>The two past editions of ESPAI have only surveyed researchers at NeurIPS and ICML, whereas this year we also contacted researchers who published in ICLR, AAAI, IJCAI, and JMLR. This could make comparison between survey years less meaningful, if the populations have different opinions. However where we checked, the subset of 2023 participants who published in NeurIPS or ICML specifically appeared to have very similar opinions to the full sample.</p>
<h2>6 Methods</h2>
<p>In October 2023, we distributed a survey of perspectives about the future of AI to people who had recently published at one of six top-tier AI venues. The questions focused on topics such as the timing of AI progress, the future impact of AI, and AI safety. The survey and its implementation were approved by the Ethics Committee of the University of Bonn (248/23-EP). The survey and its analysis were preregistered [Sandkühler et al., 2023].</p>
<h3>6.1 Survey questions</h3>
<p>Most questions were identical to those asked previously in surveys conducted in 2016 [Grace et al., 2018a] and 2022 [Grace et al., 2022]. For each of two questions, we added a new version with modified phrasing and randomly assigned participants either the version of the question asked in Grace et al. [2022] or the new version. We also added several new questions which were not based on questions from Grace et al. [2018a] or Grace et al. [2022]. While designing new questions, we tested them in a series of interviews with AI researchers and students. The full survey is available on the AI Impacts Wiki (Link in Appendix E).
As in Grace et al. [2018a] and Grace et al. [2022], most questions were randomly assigned to only a subset of participants, in order to keep the number of questions for each participant low. The survey was hosted on the Qualtrics survey platform.
For a full description of the survey's flow, see the diagram in Figure 16.</p>
<h3>6.2 Randomization</h3>
<p>Some questions were available in the two types of framing we call the "fixed-years framing" and the "fixed-probabilities framing." In the "fixed-probabilities framing," we asked respondents how many years until they thought each AI task would be feasible with a small chance ( $10 \%$ ), an even chance ( $50 \%$ ), and a high chance ( $90 \%$ ). In the "fixed-years framing," we asked respondents how likely they thought it was that each AI task would be feasible within the next 10 years, 20 years and 50 years (or 40 years $^{6}$ ). The questions available in these two framings were</p>
<ul>
<li>those asking when narrow AI tasks would become feasible</li>
<li>that asking when human-level machine intelligence (HLMI) would become feasible.</li>
<li>those asking when occupations would be automated</li>
</ul>
<p>Respondents were randomly allocated to either the "fixed-years framing" or the "fixed-probabilities framing" (allocation ratio: $1: 1$ ) and then received all the questions above using the same framing.</p>
<h3>6.3 Recruitment</h3>
<p>We recruited participants who published in 2022 at any of six top-tier AI venues: the Conference on Neural Information Processing Systems (NeurIPS), the International Conference on Machine Learning (ICML), the International Conference</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Individual components of the survey and their randomization. Horizontal divisions represent participants being randomly split between questions (parenthetical percentages also give the fraction of participants receiving each question block). There was further randomization (not shown), within some question blocks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The HLMI-framing used 40 years instead of 50 years. This was done to keep the survey consistent with the previous surveys, where this discrepancy was introduced by mistake.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
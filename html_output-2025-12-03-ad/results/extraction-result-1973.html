<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-279410599</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.14507v1.pdf" target="_blank">Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1973.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1973.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SigLIP-BC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frozen SigLIP embeddings with Behavioral Cloning policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimalist navigation pipeline that trains a feedforward behavioral cloning (BC) policy to map frozen SigLIP joint image-text embeddings to wheel velocity commands, using additive L2-normalized fusion of image and text features and no mapping, memory, or VLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frozen SigLIP + Behavioral Cloning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SigLIP's pretrained image and text encoders produce 1152-d L2-normalized embeddings; image and text vectors are summed and normalized to form a joint embedding o that is provided as input to a feedforward BC policy predicting 2D wheel velocities. The SigLIP VLM is kept frozen; the BC policy is trained on demonstrations collected from a privileged PPO expert that has full state access. No explicit spatial modules, mapping, or memory are used — the policy is reactive based solely on single-frame joint embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>SigLIP image encoder and text encoder (pretrained), producing 1152-dimensional joint embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Referred in paper as SigLIP (So400M-Patch14-384); exact pretraining dataset details not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Additive joint embedding grounding: L2-normalize image and text encodings, sum them, and L2-normalize again to create a joint image-text vector; the BC policy learns to map this joint embedding to actions, thereby grounding language to visual observations via embedding-space alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Global single-frame image-level joint representation (single-frame, whole-image embedding fused with text embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>None explicit — spatial information is implicit via language-relative cues and SigLIP's inherent spatial sensitivity; no explicit 3D coordinates, depth maps, mapping, or episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation (instruction-conditioned mobile navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Custom Jetbot simulation: language-specified target navigation among five colored spheres in a 3m x 3m arena (simulation environment described in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric RGB images in photorealistic simulation (NVIDIA Isaac Sim / Isaac Lab)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (reach correct target within 0.1m), number of timesteps to completion, cumulative reward, trajectory efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>74.0% success rate; average 369.4 timesteps on successful episodes (compared to expert 100% success and 114.0 timesteps average)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper compares three frozen VLMs using identical BC training: SigLIP (1152-d) achieves 74.0% success, CLIP (ViT-B/32, 512-d) 62.0%, ViLT (768-d, VQA-finetuned) 40.0%. Richer embedding dimensionality and stronger spatial sensitivity (as measured by cosine-distance spatial test) aligned with better navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Identified as a major limitation: frozen single-frame embeddings provide semantic grounding but insufficient spatial reasoning, planning, and temporal memory, causing inefficient trajectories (3.2× more steps), inconsistent rewards, and failures such as timeouts and circling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Observed failure modes include timeout (not reaching target within 1000 timesteps), repeated circling behavior, and target confusion between visually similar colored spheres; overall failure rate is 26% (100 test episodes, 74 successes), worst-case timesteps up to 828 vs expert max 154; per-failure-type breakdowns not fully quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>None reported — experiments conducted entirely in simulation with no domain adaptation or fine-tuning to real-world observations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>This work uses only frozen VLMs and does not perform or report fine-tuning; the paper discusses fine-tuning (e.g., RT-2, Robotic-CLIP) as likely to improve performance but provides no empirical frozen-vs-finetuned comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late additive fusion: L2-normalize image and text embeddings, sum them, then L2-normalize the result (vector addition in embedding space).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training dataset ≈ 500 demonstration trajectories (≈100 per target), totaling tens of thousands of state-action pairs; BC loss converged quickly, but no direct comparison of sample efficiency versus alternatives is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Frozen pretrained VLM embeddings (SigLIP) provide effective semantic grounding sufficient for majority-success language-guided navigation (74% SR) without fine-tuning, but lack explicit spatial memory and planning leading to large efficiency gaps and failure modes; richer embeddings and inherent spatial sensitivity improve grounding utility even if BC training loss is higher.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1973.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-BC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frozen CLIP (ViT-B/32) embeddings with Behavioral Cloning policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Behavioral cloning policy identical to the SigLIP pipeline but using frozen CLIP (ViT-B/32) embeddings; provides weaker spatial sensitivity and lower navigation success than SigLIP under identical training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frozen CLIP (ViT-B/32) + Behavioral Cloning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same BC setup as main pipeline: CLIP image and text encoders produce embeddings (paper uses CLIP ViT-B/32); image and text features are L2-normalized, summed, normalized to a joint vector, and fed to a feedforward BC policy trained on expert demonstrations. CLIP weights are frozen; no mapping or memory modules used.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP ViT-B/32 (frozen) as referenced in paper</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Referenced as CLIP (ViT-B/32) [18] in paper; specific pretraining dataset details are not enumerated within this paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Additive joint embedding: L2-normalize CLIP image and text encodings, sum, normalize to create joint embedding; BC policy maps embedding to actions, grounding language via embedding alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Global image-level joint representation (single-frame)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit only — relies on CLIP's inherent spatial sensitivity and relative spatial cues in the language prompt; no explicit spatial map or memory.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Same custom Jetbot navigation task (5 colored spheres in 3m x 3m simulated arena)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric RGB images in simulation (Isaac Sim)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate; timesteps to completion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>62.0% success rate; average 417.6 timesteps on successful episodes (paper reports CLIP slower and less successful than SigLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>CLIP (62.0% SR) underperforms SigLIP (74.0%) but outperforms ViLT (40.0%). CLIP's embedding dimensionality (512-d) and weaker measured spatial sensitivity correlated with lower navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>CLIP shows weaker spatial sensitivity relative to SigLIP (paper's spatial-distance test: CLIP 13.0% difference vs SigLIP 26.5%), contributing to less efficient navigation and higher failure rate; limitations similar to SigLIP but more pronounced.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper reports higher average timesteps and lower success relative to SigLIP, implying more frequent inefficiency and failures; specific per-mode frequencies not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Only frozen CLIP was evaluated; fine-tuning not performed in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Additive fusion (L2-normalized image + text, sum, normalize)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained with same demonstration dataset (~500 trajectories); no separate sample-efficiency claims.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>When used as frozen embeddings in a BC pipeline, CLIP provides semantic grounding but lower spatial sensitivity and navigation performance than SigLIP; embedding dimension and spatial sensitivity correlate with downstream navigation success.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1973.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViLT-BC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frozen ViLT (VQA-finetuned) embeddings with Behavioral Cloning policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the BC pipeline using a VQA-finetuned ViLT model; performs worst among compared VLMs in the navigation task, suggesting task-specific fine-tuning for non-embodied tasks does not necessarily transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frozen ViLT (VQA-finetuned) + Behavioral Cloning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ViLT image-text encoder (VQA-finetuned) produces embeddings that are fused with language via the same additive L2-normalized scheme and fed to a BC policy; ViLT weights are kept frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>ViLT (VQA-finetuned) as referenced in paper</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Referenced as ViLT (VQA-finetuned) [9]; specific pretraining dataset details are not provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Additive joint embedding of ViLT image and text features (L2-normalize, sum, normalize), with BC mapping to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Global single-frame image-level joint representation</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit only; limited spatial sensitivity reported</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Same Jetbot 5-sphere navigation task in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric RGB simulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate; average timesteps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>40.0% success rate; average 472.0 timesteps for successful episodes (worst among the three compared VLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>ViLT (40.0% SR) performed worse than CLIP and SigLIP despite being VQA-finetuned, suggesting VQA fine-tuning does not transfer well to continuous navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper interprets ViLT's poor performance as misaligned fine-tuning (VQA) that specialized representations away from continuous spatial reasoning needed for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Higher inefficiency and failures relative to CLIP and SigLIP; specific counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper uses frozen ViLT (already VQA-finetuned) and does not attempt navigation-specific fine-tuning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Additive fusion (L2-normalized image + text, sum, normalize)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained on the same demonstration dataset; no separate sample-efficiency claim.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Task-specific fine-tuning for non-embodied tasks (e.g., VQA) may not produce representations effective for embodied spatial tasks; general-purpose representations with stronger spatial sensitivity worked better.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1973.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Privileged-Expert (PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Privileged state-aware expert policy trained with Proximal Policy Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A privileged expert trained with PPO that has full access to state information (target identity and precise position) and generates near-optimal demonstrations used to train the BC student policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Privileged PPO expert</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Expert policy π_β uses full state s and goal g (one-hot target identity and precise goal position) as input and is trained with PPO to produce wheel velocity commands; functions as a perfect GPS-like navigator to collect demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (expert uses full state, not VLM encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>N/A — expert has direct access to goal identity/position rather than grounding via language-vision embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>State-level privileged representation (precise positions and identity)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit — uses exact coordinates of target and state for precise navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>vision-language navigation (used to generate demonstrations for BC)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Same Jetbot navigation environment</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>N/A for expert (state-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate; timesteps to completion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>100% success rate; average 114.0 timesteps to reach goal; consistently reaches within 0.1m of target</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Expert showed no failures in the evaluated episodes; functions as near-perfect oracle baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Expert collected ≈500 trajectories (used as BC training data); expert training details (PPO training steps) shown but not exhaustively quantified for sample efficiency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Serves as privileged oracle demonstrating the performance ceiling; comparison highlights student BC on frozen VLM embeddings achieves majority success but substantial efficiency gap vs. state-aware expert.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1973.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a recent approach that fine-tunes vision-language models on action data to directly generate robot commands, representing a more complex adaptation than frozen embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2 (vision-language-action transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in paper as an approach that fine-tunes pretrained VLMs on action datasets so they can directly output robot commands from visual and language inputs; contrasted with the paper's frozen-embedding minimalist approach.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Fine-tuned VLM (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified here (referenced externally as RT-2 work)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Fine-tuning VLMs on action data to align visual-language representations with motor outputs (not detailed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Presumably global joint embeddings fine-tuned for control (paper only mentions at high-level)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Not specified in this text; RT-2 is presented as a more end-to-end fine-tuned approach likely to capture embodied priors</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Robotic control / vision-language-action tasks (mentioned broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Mentioned as a contrasting approach that requires more complexity and fine-tuning than frozen embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Mentioned as fine-tuned approach (contrasted with frozen embeddings); no empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Cited as representative of architectures that adapt VLMs via fine-tuning to generate robot commands, illustrating a performance-complexity tradeoff versus frozen-embedding baselines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1973.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robotic-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic-CLIP (CLIP fine-tuned on action data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an approach that fine-tunes CLIP on action data for robotic applications, representing a supervised adaptation path contrasted with frozen embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Robotic-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tunes CLIP on robot action/demonstration datasets to align visual-language features with control outputs; presented as more complex and compute-intensive compared to frozen embedding usage.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Fine-tuned CLIP (as described in cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>CLIP pretraining referenced externally; specific fine-tuning dataset not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Task-specific fine-tuning of VLM to improve mapping from joint embeddings to actions (high-level description only)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Global joint embeddings adapted for control (paper does not detail internals)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Not described in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Robotic control / navigation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Cited to illustrate approaches that require adaptation vs. frozen use.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Mentioned as fine-tuned approach; no empirical frozen-vs-finetuned comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Provided as a contrasting example that fine-tuning can improve embodied performance at the cost of additional complexity and data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1973.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-Nav / VLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP-Nav / Visual Language Maps (VLMaps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned classes of approaches that combine VLMs with specialized navigation architectures such as mapping modules or spatial awareness components to support spatial reasoning and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP-Nav / VLMaps (VLM + mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as systems that integrate vision-language models with explicit mapping/spatial modules (e.g., building a visual-language map) to provide spatial memory and planning capabilities beyond single-frame embeddings; contrasted with the paper's reactive single-frame BC approach.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>VLMs (e.g., CLIP) combined with mapping modules (external literature)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Grounding via VLM embeddings integrated into spatial maps and planning pipelines (region-level or scene-level mapping), rather than pure single-frame embedding matching.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Scene-level / map-level representations (mapping + VLM-derived features)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit spatial maps (VLMaps) enabling memory and planning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Referenced work on visual language maps and CLIP-based navigation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper suggests such mapping approaches address the spatial reasoning and memory shortcomings observed with frozen single-frame embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Typically map-based integration of VLM features (paper only mentions at high level)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>The paper contrasts mapping + VLM approaches (e.g., VLMaps) as likely to close the efficiency/performance gap by adding explicit spatial memory and planning capabilities that frozen single-frame embeddings lack.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1973.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLM (vision-language models with spatial reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as an approach that augments VLMs with explicit spatial reasoning capabilities to better handle embodied spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as endowing vision-language models with spatial reasoning mechanisms (paper cites SpatialVLM as an example) — contrasted with the authors' deliberate omission of spatial augmentation to isolate the VLM embedding's raw capability.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>VLM with added spatial modules (as per cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Explicit spatial reasoning components stacked on VLM representations (paper-level description only)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level / spatial-aware representations</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit spatial reasoning tokens or modules (as per SpatialVLM literature)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-language navigation / spatial tasks (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper suggests SpatialVLM-type approaches address spatial reasoning deficits found in frozen-embedding reactive policies.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Spatial augmentation of VLMs is a proposed remedy for the planning/memory shortcomings observed with frozen single-frame embeddings in navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1973.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZSON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZSON (Zero-shot object-goal navigation using multimodal goal embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as prior work leveraging multimodal goal embeddings for object-goal navigation; cited as related work rather than used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ZSON</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses multimodal goal embeddings to enable zero-shot object-goal navigation (paper references ZSON in related work), presented as an alternate way to ground language/object goals for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Multimodal embeddings (paper-level mention only)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Embedding-based goal specification enabling matching between observation and goal embedding (overview only in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Goal-level / multimodal embedding</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object-goal navigation (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ZSON (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Mentioned as a multimodal-embedding approach in prior work; paper cites it among methods that augment embedding-based grounding with additional mechanisms for navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1973.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NaVILA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NaVILA (Legged robot vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a recent vision-language-action model for navigation (2025 work), cited to represent specialized architectures that likely outperform frozen-embedding reactive policies but demand more complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NaVILA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referential mention only: a legged robot vision-language-action model for navigation and embodied control; used as an example of more complex specialized systems compared to the paper's minimalist approach.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Not described here; cited as alternate architecture that integrates vision-language features into action-conditioned models.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Navigation / vision-language-action (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Cited as an example of specialized vision-language-action architectures that trade complexity for improved embodied performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1973.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models / Latent Planning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models and Latent-space Planning approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as promising directions to integrate perception with spatial reasoning and planning, potentially alleviating the limitations of frozen single-frame embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models / Latent Planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-level mention recommending world-model-style or latent-dynamics planning methods to combine perceptual grounding from VLMs with temporal and spatial structure needed for efficient navigation; not implemented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Not applicable here (conceptual suggestion)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Integrate VLM-derived perceptual features into learned latent dynamics/world models to support planning and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-step latent representations / world states</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicitly learned latent spatial dynamics or explicit world-state representations depending on the cited method</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Navigation, planning, and control (proposed hybrid direction)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>true</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper suggests world models and latent planning could remedy perception-grounding bottlenecks by providing memory and structured planning beyond single-frame embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Integrating frozen VLM perceptual signals into latent planning/world-model frameworks is recommended to close the gap between semantic grounding and spatial-temporal reasoning needed for efficient embodied navigation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1973.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1973.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R3M (Representation for robot manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as an approach for embodiment-aware representation learning that could complement pretrained VLMs to better encode navigation-relevant distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a task-specific representation learning method (for manipulation) that could inspire embodiment-aware representations for navigation; discussed as a potential complementary direction rather than evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Not described in this paper; referenced as embodiment-aware representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Not specified</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Manipulation (original work) — referenced for representation ideas relevant to navigation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R3M (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper suggests representation learning approaches like R3M could reduce confusion between visually similar targets by encoding navigation-relevant distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Embodiment-aware representation learning (e.g., R3M) is suggested as a future direction to complement general-purpose VLM embeddings for embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. <em>(Rating: 2)</em></li>
                <li>Visual language maps for robot navigation. <em>(Rating: 2)</em></li>
                <li>RT-2: Visionlanguage-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Robotic-clip: Fine-tuning clip on action data for robotic applications. <em>(Rating: 2)</em></li>
                <li>Clip-nav: Using clip for zeroshot vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings. <em>(Rating: 2)</em></li>
                <li>Navila: Legged robot vision-language-action model for navigation. <em>(Rating: 2)</em></li>
                <li>R3m: A universal visual representation for robot manipulation. <em>(Rating: 1)</em></li>
                <li>Learning latent dynamics for planning from pixels. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1973",
    "paper_id": "paper-279410599",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "SigLIP-BC",
            "name_full": "Frozen SigLIP embeddings with Behavioral Cloning policy",
            "brief_description": "A minimalist navigation pipeline that trains a feedforward behavioral cloning (BC) policy to map frozen SigLIP joint image-text embeddings to wheel velocity commands, using additive L2-normalized fusion of image and text features and no mapping, memory, or VLM fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Frozen SigLIP + Behavioral Cloning",
            "model_description": "SigLIP's pretrained image and text encoders produce 1152-d L2-normalized embeddings; image and text vectors are summed and normalized to form a joint embedding o that is provided as input to a feedforward BC policy predicting 2D wheel velocities. The SigLIP VLM is kept frozen; the BC policy is trained on demonstrations collected from a privileged PPO expert that has full state access. No explicit spatial modules, mapping, or memory are used — the policy is reactive based solely on single-frame joint embeddings.",
            "visual_encoder_type": "SigLIP image encoder and text encoder (pretrained), producing 1152-dimensional joint embeddings",
            "visual_encoder_pretraining": "Referred in paper as SigLIP (So400M-Patch14-384); exact pretraining dataset details not specified in this paper",
            "grounding_mechanism": "Additive joint embedding grounding: L2-normalize image and text encodings, sum them, and L2-normalize again to create a joint image-text vector; the BC policy learns to map this joint embedding to actions, thereby grounding language to visual observations via embedding-space alignment.",
            "representation_level": "Global single-frame image-level joint representation (single-frame, whole-image embedding fused with text embedding)",
            "spatial_representation": "None explicit — spatial information is implicit via language-relative cues and SigLIP's inherent spatial sensitivity; no explicit 3D coordinates, depth maps, mapping, or episodic memory",
            "embodied_task_type": "vision-language navigation (instruction-conditioned mobile navigation)",
            "embodied_task_name": "Custom Jetbot simulation: language-specified target navigation among five colored spheres in a 3m x 3m arena (simulation environment described in paper)",
            "visual_domain": "Egocentric RGB images in photorealistic simulation (NVIDIA Isaac Sim / Isaac Lab)",
            "performance_metric": "Success rate (reach correct target within 0.1m), number of timesteps to completion, cumulative reward, trajectory efficiency",
            "performance_value": "74.0% success rate; average 369.4 timesteps on successful episodes (compared to expert 100% success and 114.0 timesteps average)",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper compares three frozen VLMs using identical BC training: SigLIP (1152-d) achieves 74.0% success, CLIP (ViT-B/32, 512-d) 62.0%, ViLT (768-d, VQA-finetuned) 40.0%. Richer embedding dimensionality and stronger spatial sensitivity (as measured by cosine-distance spatial test) aligned with better navigation performance.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Identified as a major limitation: frozen single-frame embeddings provide semantic grounding but insufficient spatial reasoning, planning, and temporal memory, causing inefficient trajectories (3.2× more steps), inconsistent rewards, and failures such as timeouts and circling.",
            "failure_mode_analysis": "Observed failure modes include timeout (not reaching target within 1000 timesteps), repeated circling behavior, and target confusion between visually similar colored spheres; overall failure rate is 26% (100 test episodes, 74 successes), worst-case timesteps up to 828 vs expert max 154; per-failure-type breakdowns not fully quantified in the paper.",
            "domain_shift_handling": "None reported — experiments conducted entirely in simulation with no domain adaptation or fine-tuning to real-world observations.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "This work uses only frozen VLMs and does not perform or report fine-tuning; the paper discusses fine-tuning (e.g., RT-2, Robotic-CLIP) as likely to improve performance but provides no empirical frozen-vs-finetuned comparison.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Late additive fusion: L2-normalize image and text embeddings, sum them, then L2-normalize the result (vector addition in embedding space).",
            "sample_efficiency": "Training dataset ≈ 500 demonstration trajectories (≈100 per target), totaling tens of thousands of state-action pairs; BC loss converged quickly, but no direct comparison of sample efficiency versus alternatives is provided.",
            "key_findings_grounding": "Frozen pretrained VLM embeddings (SigLIP) provide effective semantic grounding sufficient for majority-success language-guided navigation (74% SR) without fine-tuning, but lack explicit spatial memory and planning leading to large efficiency gaps and failure modes; richer embeddings and inherent spatial sensitivity improve grounding utility even if BC training loss is higher.",
            "uuid": "e1973.0"
        },
        {
            "name_short": "CLIP-BC",
            "name_full": "Frozen CLIP (ViT-B/32) embeddings with Behavioral Cloning policy",
            "brief_description": "Behavioral cloning policy identical to the SigLIP pipeline but using frozen CLIP (ViT-B/32) embeddings; provides weaker spatial sensitivity and lower navigation success than SigLIP under identical training and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Frozen CLIP (ViT-B/32) + Behavioral Cloning",
            "model_description": "Same BC setup as main pipeline: CLIP image and text encoders produce embeddings (paper uses CLIP ViT-B/32); image and text features are L2-normalized, summed, normalized to a joint vector, and fed to a feedforward BC policy trained on expert demonstrations. CLIP weights are frozen; no mapping or memory modules used.",
            "visual_encoder_type": "CLIP ViT-B/32 (frozen) as referenced in paper",
            "visual_encoder_pretraining": "Referenced as CLIP (ViT-B/32) [18] in paper; specific pretraining dataset details are not enumerated within this paper",
            "grounding_mechanism": "Additive joint embedding: L2-normalize CLIP image and text encodings, sum, normalize to create joint embedding; BC policy maps embedding to actions, grounding language via embedding alignment.",
            "representation_level": "Global image-level joint representation (single-frame)",
            "spatial_representation": "Implicit only — relies on CLIP's inherent spatial sensitivity and relative spatial cues in the language prompt; no explicit spatial map or memory.",
            "embodied_task_type": "vision-language navigation",
            "embodied_task_name": "Same custom Jetbot navigation task (5 colored spheres in 3m x 3m simulated arena)",
            "visual_domain": "Egocentric RGB images in simulation (Isaac Sim)",
            "performance_metric": "Success rate; timesteps to completion",
            "performance_value": "62.0% success rate; average 417.6 timesteps on successful episodes (paper reports CLIP slower and less successful than SigLIP)",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": true,
            "encoder_comparison_results": "CLIP (62.0% SR) underperforms SigLIP (74.0%) but outperforms ViLT (40.0%). CLIP's embedding dimensionality (512-d) and weaker measured spatial sensitivity correlated with lower navigation performance.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "CLIP shows weaker spatial sensitivity relative to SigLIP (paper's spatial-distance test: CLIP 13.0% difference vs SigLIP 26.5%), contributing to less efficient navigation and higher failure rate; limitations similar to SigLIP but more pronounced.",
            "failure_mode_analysis": "Paper reports higher average timesteps and lower success relative to SigLIP, implying more frequent inefficiency and failures; specific per-mode frequencies not enumerated.",
            "domain_shift_handling": "None reported",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Only frozen CLIP was evaluated; fine-tuning not performed in these experiments.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Additive fusion (L2-normalized image + text, sum, normalize)",
            "sample_efficiency": "Trained with same demonstration dataset (~500 trajectories); no separate sample-efficiency claims.",
            "key_findings_grounding": "When used as frozen embeddings in a BC pipeline, CLIP provides semantic grounding but lower spatial sensitivity and navigation performance than SigLIP; embedding dimension and spatial sensitivity correlate with downstream navigation success.",
            "uuid": "e1973.1"
        },
        {
            "name_short": "ViLT-BC",
            "name_full": "Frozen ViLT (VQA-finetuned) embeddings with Behavioral Cloning policy",
            "brief_description": "Application of the BC pipeline using a VQA-finetuned ViLT model; performs worst among compared VLMs in the navigation task, suggesting task-specific fine-tuning for non-embodied tasks does not necessarily transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Frozen ViLT (VQA-finetuned) + Behavioral Cloning",
            "model_description": "ViLT image-text encoder (VQA-finetuned) produces embeddings that are fused with language via the same additive L2-normalized scheme and fed to a BC policy; ViLT weights are kept frozen.",
            "visual_encoder_type": "ViLT (VQA-finetuned) as referenced in paper",
            "visual_encoder_pretraining": "Referenced as ViLT (VQA-finetuned) [9]; specific pretraining dataset details are not provided in this paper",
            "grounding_mechanism": "Additive joint embedding of ViLT image and text features (L2-normalize, sum, normalize), with BC mapping to actions.",
            "representation_level": "Global single-frame image-level joint representation",
            "spatial_representation": "Implicit only; limited spatial sensitivity reported",
            "embodied_task_type": "vision-language navigation",
            "embodied_task_name": "Same Jetbot 5-sphere navigation task in simulation",
            "visual_domain": "Egocentric RGB simulation",
            "performance_metric": "Success rate; average timesteps",
            "performance_value": "40.0% success rate; average 472.0 timesteps for successful episodes (worst among the three compared VLMs)",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": true,
            "encoder_comparison_results": "ViLT (40.0% SR) performed worse than CLIP and SigLIP despite being VQA-finetuned, suggesting VQA fine-tuning does not transfer well to continuous navigation tasks.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper interprets ViLT's poor performance as misaligned fine-tuning (VQA) that specialized representations away from continuous spatial reasoning needed for navigation.",
            "failure_mode_analysis": "Higher inefficiency and failures relative to CLIP and SigLIP; specific counts not provided.",
            "domain_shift_handling": "None reported",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Paper uses frozen ViLT (already VQA-finetuned) and does not attempt navigation-specific fine-tuning in experiments.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Additive fusion (L2-normalized image + text, sum, normalize)",
            "sample_efficiency": "Trained on the same demonstration dataset; no separate sample-efficiency claim.",
            "key_findings_grounding": "Task-specific fine-tuning for non-embodied tasks (e.g., VQA) may not produce representations effective for embodied spatial tasks; general-purpose representations with stronger spatial sensitivity worked better.",
            "uuid": "e1973.2"
        },
        {
            "name_short": "Privileged-Expert (PPO)",
            "name_full": "Privileged state-aware expert policy trained with Proximal Policy Optimization",
            "brief_description": "A privileged expert trained with PPO that has full access to state information (target identity and precise position) and generates near-optimal demonstrations used to train the BC student policies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Privileged PPO expert",
            "model_description": "Expert policy π_β uses full state s and goal g (one-hot target identity and precise goal position) as input and is trained with PPO to produce wheel velocity commands; functions as a perfect GPS-like navigator to collect demonstrations.",
            "visual_encoder_type": "N/A (expert uses full state, not VLM encoders)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "N/A — expert has direct access to goal identity/position rather than grounding via language-vision embeddings",
            "representation_level": "State-level privileged representation (precise positions and identity)",
            "spatial_representation": "Explicit — uses exact coordinates of target and state for precise navigation",
            "embodied_task_type": "vision-language navigation (used to generate demonstrations for BC)",
            "embodied_task_name": "Same Jetbot navigation environment",
            "visual_domain": "N/A for expert (state-based)",
            "performance_metric": "Success rate; timesteps to completion",
            "performance_value": "100% success rate; average 114.0 timesteps to reach goal; consistently reaches within 0.1m of target",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Expert showed no failures in the evaluated episodes; functions as near-perfect oracle baseline.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": "Expert collected ≈500 trajectories (used as BC training data); expert training details (PPO training steps) shown but not exhaustively quantified for sample efficiency comparisons.",
            "key_findings_grounding": "Serves as privileged oracle demonstrating the performance ceiling; comparison highlights student BC on frozen VLM embeddings achieves majority success but substantial efficiency gap vs. state-aware expert.",
            "uuid": "e1973.3"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (vision-language-action model)",
            "brief_description": "Mentioned as a recent approach that fine-tunes vision-language models on action data to directly generate robot commands, representing a more complex adaptation than frozen embeddings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RT-2 (vision-language-action transfer)",
            "model_description": "Described in paper as an approach that fine-tunes pretrained VLMs on action datasets so they can directly output robot commands from visual and language inputs; contrasted with the paper's frozen-embedding minimalist approach.",
            "visual_encoder_type": "Fine-tuned VLM (unspecified in this paper)",
            "visual_encoder_pretraining": "Not specified here (referenced externally as RT-2 work)",
            "grounding_mechanism": "Fine-tuning VLMs on action data to align visual-language representations with motor outputs (not detailed in this paper)",
            "representation_level": "Presumably global joint embeddings fine-tuned for control (paper only mentions at high-level)",
            "spatial_representation": "Not specified in this text; RT-2 is presented as a more end-to-end fine-tuned approach likely to capture embodied priors",
            "embodied_task_type": "Robotic control / vision-language-action tasks (mentioned broadly)",
            "embodied_task_name": null,
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Mentioned as a contrasting approach that requires more complexity and fine-tuning than frozen embeddings.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Mentioned as fine-tuned approach (contrasted with frozen embeddings); no empirical comparison in this paper.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Cited as representative of architectures that adapt VLMs via fine-tuning to generate robot commands, illustrating a performance-complexity tradeoff versus frozen-embedding baselines.",
            "uuid": "e1973.4"
        },
        {
            "name_short": "Robotic-CLIP",
            "name_full": "Robotic-CLIP (CLIP fine-tuned on action data)",
            "brief_description": "Mentioned as an approach that fine-tunes CLIP on action data for robotic applications, representing a supervised adaptation path contrasted with frozen embeddings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Robotic-CLIP",
            "model_description": "Fine-tunes CLIP on robot action/demonstration datasets to align visual-language features with control outputs; presented as more complex and compute-intensive compared to frozen embedding usage.",
            "visual_encoder_type": "Fine-tuned CLIP (as described in cited literature)",
            "visual_encoder_pretraining": "CLIP pretraining referenced externally; specific fine-tuning dataset not specified in this paper",
            "grounding_mechanism": "Task-specific fine-tuning of VLM to improve mapping from joint embeddings to actions (high-level description only)",
            "representation_level": "Global joint embeddings adapted for control (paper does not detail internals)",
            "spatial_representation": "Not described in this paper",
            "embodied_task_type": "Robotic control / navigation (mentioned)",
            "embodied_task_name": null,
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Cited to illustrate approaches that require adaptation vs. frozen use.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Mentioned as fine-tuned approach; no empirical frozen-vs-finetuned comparison in this paper.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Provided as a contrasting example that fine-tuning can improve embodied performance at the cost of additional complexity and data.",
            "uuid": "e1973.5"
        },
        {
            "name_short": "CLIP-Nav / VLMaps",
            "name_full": "CLIP-Nav / Visual Language Maps (VLMaps)",
            "brief_description": "Mentioned classes of approaches that combine VLMs with specialized navigation architectures such as mapping modules or spatial awareness components to support spatial reasoning and memory.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CLIP-Nav / VLMaps (VLM + mapping)",
            "model_description": "Described as systems that integrate vision-language models with explicit mapping/spatial modules (e.g., building a visual-language map) to provide spatial memory and planning capabilities beyond single-frame embeddings; contrasted with the paper's reactive single-frame BC approach.",
            "visual_encoder_type": "VLMs (e.g., CLIP) combined with mapping modules (external literature)",
            "visual_encoder_pretraining": "Not specified in this paper",
            "grounding_mechanism": "Grounding via VLM embeddings integrated into spatial maps and planning pipelines (region-level or scene-level mapping), rather than pure single-frame embedding matching.",
            "representation_level": "Scene-level / map-level representations (mapping + VLM-derived features)",
            "spatial_representation": "Explicit spatial maps (VLMaps) enabling memory and planning",
            "embodied_task_type": "Vision-language navigation",
            "embodied_task_name": "Referenced work on visual language maps and CLIP-based navigation",
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Paper suggests such mapping approaches address the spatial reasoning and memory shortcomings observed with frozen single-frame embeddings.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Typically map-based integration of VLM features (paper only mentions at high level)",
            "sample_efficiency": null,
            "key_findings_grounding": "The paper contrasts mapping + VLM approaches (e.g., VLMaps) as likely to close the efficiency/performance gap by adding explicit spatial memory and planning capabilities that frozen single-frame embeddings lack.",
            "uuid": "e1973.6"
        },
        {
            "name_short": "SpatialVLM",
            "name_full": "SpatialVLM (vision-language models with spatial reasoning)",
            "brief_description": "Cited as an approach that augments VLMs with explicit spatial reasoning capabilities to better handle embodied spatial tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SpatialVLM",
            "model_description": "Described as endowing vision-language models with spatial reasoning mechanisms (paper cites SpatialVLM as an example) — contrasted with the authors' deliberate omission of spatial augmentation to isolate the VLM embedding's raw capability.",
            "visual_encoder_type": "VLM with added spatial modules (as per cited work)",
            "visual_encoder_pretraining": "Not specified here",
            "grounding_mechanism": "Explicit spatial reasoning components stacked on VLM representations (paper-level description only)",
            "representation_level": "Region-level / spatial-aware representations",
            "spatial_representation": "Explicit spatial reasoning tokens or modules (as per SpatialVLM literature)",
            "embodied_task_type": "Vision-language navigation / spatial tasks (mentioned)",
            "embodied_task_name": null,
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Paper suggests SpatialVLM-type approaches address spatial reasoning deficits found in frozen-embedding reactive policies.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Spatial augmentation of VLMs is a proposed remedy for the planning/memory shortcomings observed with frozen single-frame embeddings in navigation.",
            "uuid": "e1973.7"
        },
        {
            "name_short": "ZSON",
            "name_full": "ZSON (Zero-shot object-goal navigation using multimodal goal embeddings)",
            "brief_description": "Referenced as prior work leveraging multimodal goal embeddings for object-goal navigation; cited as related work rather than used in experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ZSON",
            "model_description": "Uses multimodal goal embeddings to enable zero-shot object-goal navigation (paper references ZSON in related work), presented as an alternate way to ground language/object goals for navigation.",
            "visual_encoder_type": "Multimodal embeddings (paper-level mention only)",
            "visual_encoder_pretraining": "Not specified in this paper",
            "grounding_mechanism": "Embedding-based goal specification enabling matching between observation and goal embedding (overview only in this paper)",
            "representation_level": "Goal-level / multimodal embedding",
            "spatial_representation": "Not specified here",
            "embodied_task_type": "Object-goal navigation (zero-shot)",
            "embodied_task_name": "ZSON (referenced)",
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Mentioned as a multimodal-embedding approach in prior work; paper cites it among methods that augment embedding-based grounding with additional mechanisms for navigation.",
            "uuid": "e1973.8"
        },
        {
            "name_short": "NaVILA",
            "name_full": "NaVILA (Legged robot vision-language-action model)",
            "brief_description": "Mentioned as a recent vision-language-action model for navigation (2025 work), cited to represent specialized architectures that likely outperform frozen-embedding reactive policies but demand more complexity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "NaVILA",
            "model_description": "Referential mention only: a legged robot vision-language-action model for navigation and embodied control; used as an example of more complex specialized systems compared to the paper's minimalist approach.",
            "visual_encoder_type": "Not specified in this paper",
            "visual_encoder_pretraining": "Not specified",
            "grounding_mechanism": "Not described here; cited as alternate architecture that integrates vision-language features into action-conditioned models.",
            "representation_level": null,
            "spatial_representation": null,
            "embodied_task_type": "Navigation / vision-language-action (mentioned)",
            "embodied_task_name": null,
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Cited as an example of specialized vision-language-action architectures that trade complexity for improved embodied performance.",
            "uuid": "e1973.9"
        },
        {
            "name_short": "World Models / Latent Planning",
            "name_full": "World Models and Latent-space Planning approaches",
            "brief_description": "Cited as promising directions to integrate perception with spatial reasoning and planning, potentially alleviating the limitations of frozen single-frame embeddings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "World Models / Latent Planning",
            "model_description": "High-level mention recommending world-model-style or latent-dynamics planning methods to combine perceptual grounding from VLMs with temporal and spatial structure needed for efficient navigation; not implemented in this paper.",
            "visual_encoder_type": "Not applicable here (conceptual suggestion)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Integrate VLM-derived perceptual features into learned latent dynamics/world models to support planning and memory.",
            "representation_level": "Multi-step latent representations / world states",
            "spatial_representation": "Implicitly learned latent spatial dynamics or explicit world-state representations depending on the cited method",
            "embodied_task_type": "Navigation, planning, and control (proposed hybrid direction)",
            "embodied_task_name": null,
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": "true",
            "perception_bottleneck_details": "Paper suggests world models and latent planning could remedy perception-grounding bottlenecks by providing memory and structured planning beyond single-frame embeddings.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Integrating frozen VLM perceptual signals into latent planning/world-model frameworks is recommended to close the gap between semantic grounding and spatial-temporal reasoning needed for efficient embodied navigation.",
            "uuid": "e1973.10"
        },
        {
            "name_short": "R3M",
            "name_full": "R3M (Representation for robot manipulation)",
            "brief_description": "Cited as an approach for embodiment-aware representation learning that could complement pretrained VLMs to better encode navigation-relevant distinctions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "R3M",
            "model_description": "Referenced as a task-specific representation learning method (for manipulation) that could inspire embodiment-aware representations for navigation; discussed as a potential complementary direction rather than evaluated here.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Not described in this paper; referenced as embodiment-aware representation learning",
            "representation_level": "Not specified",
            "spatial_representation": null,
            "embodied_task_type": "Manipulation (original work) — referenced for representation ideas relevant to navigation",
            "embodied_task_name": "R3M (referenced)",
            "visual_domain": null,
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": "Paper suggests representation learning approaches like R3M could reduce confusion between visually similar targets by encoding navigation-relevant distinctions.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Embodiment-aware representation learning (e.g., R3M) is suggested as a future direction to complement general-purpose VLM embeddings for embodied tasks.",
            "uuid": "e1973.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities.",
            "rating": 2
        },
        {
            "paper_title": "Visual language maps for robot navigation.",
            "rating": 2
        },
        {
            "paper_title": "RT-2: Visionlanguage-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Robotic-clip: Fine-tuning clip on action data for robotic applications.",
            "rating": 2
        },
        {
            "paper_title": "Clip-nav: Using clip for zeroshot vision-and-language navigation.",
            "rating": 2
        },
        {
            "paper_title": "ZSON: Zero-shot object-goal navigation using multimodal goal embeddings.",
            "rating": 2
        },
        {
            "paper_title": "Navila: Legged robot vision-language-action model for navigation.",
            "rating": 2
        },
        {
            "paper_title": "R3m: A universal visual representation for robot manipulation.",
            "rating": 1
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels.",
            "rating": 1
        }
    ],
    "cost": 0.0238895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?
17 Jun 2025</p>
<p>Nitesh Subedi 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Adam Haroon 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Shreyan Ganguly 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Samuel T K Tetteh 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Prajwal Koirala 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Cody Fleming 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Soumik Sarkar 
Department of Mechanical Engineering
Iowa State University Ames
50011IowaUSA</p>
<p>Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?
17 Jun 202533DDBF4F1B36C2CBE77673BD304488EEarXiv:2506.14507v1[cs.RO]Accepted to Robotics: Science and Systems (RSS) 2025 Workshop on RobotVision-Language ModelsBehavior CloningLanguage-Guided NavigationFrozen EmbeddingsFoundation ModelsRobot Learning
Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training.While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional finetuning or specialized modules?We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert.Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average.This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning.By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios.Our code is available at https://github.com/oadamharoon/text2nav</p>
<p>I. INTRODUCTION</p>
<p>The ability to follow natural language instructions is crucial for robots to operate effectively in human environments.Instructions like "go to the red ball" simultaneously specify both a goal object and the desired action, creating a flexible interface between humans and robots [14].However, bridging the gap between language understanding and physical action presents significant challenges, as the rich semantic knowledge embedded in natural language far exceeds what robots can learn from limited task-specific experience [22].</p>
<p>Traditional approaches to language-guided robot navigation typically require extensive task-specific training data and specialized architectures to ground language in visual observations.These approaches often suffer from limited generalization to novel commands or environments, as they can only recognize objects and follow instructions within predefined categories [18].The emergence of large-scale vision-language models (VLMs) like CLIP [18] and SigLIP [23] has opened new possibilities for robotics by providing rich semantic representations trained on diverse internet-scale data.</p>
<p>Recent work has demonstrated the benefits of integrating these foundation models into robotic systems.For example, approaches like RT-2 [24] and Robotic-CLIP [16] fine-tune VLMs on action data to directly generate robot commands from visual and language inputs.Other systems like CLIP-Nav [4] and VLM-Social-Nav [21] combine VLMs with specialized navigation architectures such as mapping modules or spatial awareness components.While these approaches show impressive capabilities, they often involve complex architectures, fine-tuning procedures, or additional perception modules tailored specifically for navigation.</p>
<p>This complexity raises a fundamental question for robotics researchers: are the rich semantic representations from pretrained VLMs already sufficient for basic navigation tasks, or are specialized architectures and fine-tuning essential?Answering this question is critical for several reasons:</p>
<p>• Resource efficiency: Complex architectures and fine-tuning require substantial computational resources that may be unnecessary if simpler approaches can achieve acceptable performance • Transfer learning: Understanding the intrinsic capabilities of frozen embeddings provides insights into their transferability across different embodied tasks • System design: Knowing the limitations of pretrained embeddings can guide more informed decisions about when to use specialized modules versus leveraging general-purpose representations</p>
<p>To address this question, we propose a minimalist approach that decouples the perception component (a Fig. 1: Training and deployment pipeline.A privileged expert policy π β collects demonstrations in simulation using full state observations.These are converted into joint vision-language embeddings using a pretrained VLM, forming the dataset D ′ .A policy π is then trained via behavioral cloning.At deployment, π receives only partial observations (camera image and instruction), which are encoded by the same VLM to select actions.frozen VLM) from the policy learning process.Rather than designing a complex end-to-end architecture, we investigate whether a simple behavior cloning policy trained on VLM embeddings can successfully navigate to language-specified targets.Our framework consists of two phases (Fig. 1): (1) training a privileged expert with full state access, and (2) distilling this expert's knowledge into a policy that operates solely on frozen vision-language embeddings.This minimalist approach differs fundamentally from previous work in three key technical aspects.First, unlike RT-2 [24] or Robotic-CLIP [16], we do not fine-tune the VLM on task-specific data, avoiding the computational cost and data requirements of adaptation.Second, unlike SpatialVLM [2], we do not augment the model with specialized spatial reasoning capabilities.Third, in contrast to approaches like VLMaps [7] or ZSON [12], we intentionally omit mapping or exploration modules that would provide spatial memory.Instead, we treat the VLM strictly as a frozen feature extractor, learning a reactive policy that must implicitly encode all navigationrelevant information from single-frame embeddings.</p>
<p>Our empirical evaluation reveals both the promise and limitations of this approach.The policy achieves a 74% success rate in navigating to language-specified targets, demonstrating that pretrained embeddings can indeed support basic grounding of language to visual targets.However, the policy takes significantly longer paths (3.2× on average) compared to the expert, indicating limitations in spatial reasoning, planning, and memory.These findings provide valuable insights for re- searchers seeking to balance simplicity and performance in language-guided robot navigation systems.</p>
<p>Our contributions include:</p>
<p>• A minimalist framework for language-guided navigation that uses frozen vision-language embeddings as the sole representation for policy learning • An empirical evaluation demonstrating that pretrained embeddings alone can achieve a 74% success rate in semantic target navigation • Analysis of the performance gap between our VLMguided policy and the privileged expert, revealing the strengths and limitations of pretrained embeddings for navigation • Insights into future directions for effectively leveraging foundation models in embodied robotics</p>
<p>II. METHODOLOGY</p>
<p>Our approach consists of two phases: (1) an expert demonstration phase using privileged information, and (2) a learning phase where a policy is trained on the expert data using vision-language representations.After training, the learned policy is deployed using only its onboard sensors and the language instruction.</p>
<p>A. Phase 1: Privileged Expert Policy</p>
<p>In Phase 1, we develop an expert behavioral policy π β (s, g) that can reliably navigate to a goal, where s denotes the full state and g denotes the goal specification.We represent g as a one-hot indicator of the target sphere's identity (among five possibilities) along with its position in the environment.The expert knows both which object is the target and its precise location, making navigation a straightforward control problem.We implement π β using Proximal Policy Optimization (PPO) [20].</p>
<p>The expert policy effectively functions as a perfect GPS-based navigator.In our preliminary evaluations, it achieves a 100% success rate, consistently reaching the correct sphere and stopping within 0.1m of the target.Importantly, π β receives the target identity directly rather than processing language instructions-this is acceptable since its role is solely to generate demonstrations, not to be deployed at test time.</p>
<p>Using π β , we collect a dataset of navigation trajectories.Each trajectory begins with the robot at a random location and orientation, with one of the five colored spheres randomly chosen as the goal.We generate natural language commands that include both semantic color identification and spatial positioning relative to the robot's current orientation (e.g., "The target is the red ball which is to your left.Move toward the ball."or "The target is the blue ball which is straight ahead.Move toward the ball.")for each target.These relative spatial cues are determined based on the angular position of the target relative to the robot's heading.The expert controls the robot until it reaches the goal, and we record:</p>
<p>• Camera images from the robot's perspective • Language instructions for the target • Expert actions (wheel velocity commands)</p>
<p>• Reward signals based on distance to goal The dataset D consists of approximately 500 trajectories (100 per target object), averaging 5-8 seconds of navigation each, resulting in tens of thousands of stateaction pairs for training.</p>
<p>B. Vision-Language Model Selection</p>
<p>To guide our choice of vision-language model, we first assessed whether pretrained VLMs encode not only semantic object information but also spatial relationships relevant to navigation tasks.We conducted an empirical comparison of three prominent models: ViLT (VQA-finetuned) [9], CLIP (ViT-B/32) [18], and SigLIP (So400M-Patch14-384) [1].</p>
<p>We designed a spatial distance test based on joint embeddings of images and language prompts.For each reference image containing a colored sphere, we generated a natural language instruction.Each 256×256 image was divided into an even 3×3 spatial grid, creating nine distinct regions for localizing objects.For each reference image, we selected two additional images containing the same colored object-one where the object appeared in the same spatial grid cell (same spatial semantics), and one where it appeared in a different cell (different spatial semantics).We controlled for background and lighting conditions to isolate the effect of spatial positioning.</p>
<p>We computed the cosine distance between the joint image-text embeddings of the reference image and each comparison image across x different configurations.Lower cosine distance indicates greater semantic and spatial similarity.If a vision-language model encodes spatial semantics, we expect lower distances when spatial Get expert action a t = π β (s t , g)
Image embedding v t = L2Norm(VLM img (I t )) 18: Text embedding u t = L2Norm(VLM text (l)) 19: Joint embedding: o t = L2Norm(v t + u t ) 20:
Add (o t , a t ) to D ′ 21: end for 22: Train policy π to minimize loss:
L(θ) = E (o,a)∼D ′ ∥π θ (o) − a∥ 2
23: return trained policy π positioning is consistent, even when the same object and instruction are present in both images.</p>
<p>As shown in Table I, all three models exhibited sensitivity to spatial positioning, with lower cosine distances for image pairs sharing the same spatial cell.Notably, SigLIP demonstrated the most pronounced distinction between same and different spatial contexts (26.5% difference), suggesting stronger spatial sensitivity in its joint embedding space compared to CLIP (13.0%difference) and ViLT (16.3% difference).</p>
<p>While this test does not fully assess complex spatial reasoning capabilities required for navigation planning, it confirms that pretrained VLMs inherently encode some degree of spatial information in their joint embeddings without explicit spatial training.This finding, combined with SigLIP's efficient architecture and strong performance on downstream tasks with fewer parameters [1], led us to select it as our vision-language backbone.</p>
<p>C. Vision-Language Embedding with SigLIP</p>
<p>Based on our comparative analysis, we leverage SigLIP [23] to bridge the gap between high-dimensional sensory inputs and the learning algorithm.SigLIP consists of an image encoder and a text encoder that project images and text into a shared 1152-dimensional latent space where semantically related concepts have high cosine similarity.The model's sigmoid-based contrastive loss function offers improved stability compared to CLIP's [18] softmax-based approach, making it particularly suitable for robotic applications.</p>
<p>For each timestep in our dataset, we compute a joint embedding that fuses visual and language information:</p>
<p>1) The image I and instruction L are processed through SigLIP's pretrained encoders 2) The resulting features are L2-normalized 3) The normalized features are summed and normalized again to produce a joint embedding vector o This vector o serves as a compact representation of both what the robot sees, what it has been instructed to do, and the spatial relationship between the robot and target.The inclusion of relative spatial descriptors in the language instructions enables the joint embedding to capture both semantic object identity and directional guidance.Critically, the SigLIP model remains completely frozen throughout our pipeline-we make no updates to its weights during training.</p>
<p>D. Behavioral Cloning Policy Learning</p>
<p>With the processed dataset D ′ = {(o t , a t )}, we train a policy π(a|o) via behavioral cloning [11].The policy is implemented as a feedforward neural network that takes the joint embedding vector o as input and outputs a two-dimensional action a = (ω 1 , ω 2 ) representing normalized wheel velocity commands.</p>
<p>We train the policy to minimize the mean squared error between its predictions and the expert's actions: During evaluation, the robot must navigate to a specific colored sphere based on language instructions that include both semantic color identification and relative spatial cues using only RGB camera input and pretrained vision-language embeddings.
L(θ) = E (o,a)∼D ′ ∥π θ (o) − a∥ 2
We optimize this loss using Adam with an initial learning rate of 1 × 10 −3 , decaying over time.The network converges quickly due to the large quantity of near-optimal demonstrations.</p>
<p>An important consideration is that the expert's actions are conditioned on privileged information that the student policy lacks.The student policy must learn behaviors like "turn towards the red object when seeing it" and "explore when the target is not visible" purely from the demonstration data, without ever receiving explicit state information.At deployment, the policy takes as input the current camera image and language instruction, processes them through SigLIP to obtain the joint embedding o, and outputs action commands in a closed-loop manner.</p>
<p>III. EXPERIMENTAL SETUP a) Simulation Environment:</p>
<p>We conduct all experiments in NVIDIA Isaac Sim [17] and NVIDIA Isaac Lab [13], a high-fidelity robotics simulation framework.Our code is available at https://github.com/oadamharoon/text2nav. The environment consists of a flat 3m × 3m arena with five colored spheres (red, green, blue, yellow, and pink) of 10cm diameter positioned at fixed locations for each episode.The robot starts at a random location and orientation.No obstacles are present, focusing the challenge on identifying the correct target and navigating efficiently toward it among visually similar distractors.</p>
<p>b) Robot Platform: The agent is modeled after the NVIDIA Jetbot, a small differential-drive robot with a forward-facing RGB camera.The camera provides a first-person view at 256×256 resolution.The robot's action space consists of two-dimensional angular velocity commands that directly control the wheels.Each action is applied for a short timestep (1/60 seconds) before the next observation is captured.c) Task Definition: At the start of each episode, one of the five colored balls is randomly chosen as the target.A natural language instruction is generated that includes both the target color and its relative spatial position based on the angular offset between the robot's heading and target location: "The target is the [color] ball which is to your [left/right/straight ahead].Move toward the ball."The robot must navigate to the correct target using only its egocentric RGB images and the language instruction.Success is defined as reaching within a small distance (0.1m) of the correct target within a maximum of 1000 timesteps.</p>
<p>d) Evaluation Protocol: We evaluate both the expert policy π β and the learned policy π on 100 test episodes with randomized starting positions.For each episode, we record:</p>
<p>• Success rate (reaching the correct target) • Number of timesteps to completion • Cumulative reward • Trajectory efficiency (compared to optimal paths) IV.RESULTS Our evaluation reveals a substantial performance gap between the expert policy with privileged state access and the vision-language embedding-based policy (Table II).The expert policy π β achieves a perfect 100% success rate, while the learned policy π succeeds in 74% of test episodes.The learned policy π using only vision-language embeddings achieves 74% success but requires significantly more timesteps than the privileged expert π β , even in successful cases.This highlights both the potential and limitations of using pretrained embeddings alone for navigation.</p>
<p>a) Success Rate Analysis: The 74% success rate of the vision-language policy is notable, as it demonstrates that pretrained embeddings alone can guide a robot to the correct target in a majority of cases without any architecture specifically designed for navigation or spatial reasoning.The policy correctly grounds language instructions in visual observations and generates appropriate actions to reach the target-all through the lens of a frozen vision-language model and a simple feedforward network.</p>
<p>b) Efficiency Gap: Despite its reasonable success rate, the learned policy exhibits significantly lower efficiency compared to the expert.When successful, π takes on average 369.4 timesteps to reach the goal-3.2×more than the expert's average of 114.0 timesteps.Even the best-case performance of π (216 timesteps) is approximately triple the expert's minimum (73 timesteps), while the worst case requires 828 timesteps compared to the expert's maximum of 154.</p>
<p>c) Reward and Consistency: The reward comparison (Fig. 4) further illustrates the performance gap.The learned policy's reward curve shows considerable episode-to-episode variation, with rewards ranging from approximately -8 to -13, indicating inconsistent performance across different starting configurations.In contrast, the expert maintains stable, high rewards across all test episodes.This variability suggests that the VLMbased policy struggles more with certain spatial arrangements or viewing angles than others.</p>
<p>d) Learning Dynamics: The behavioral cloning loss for SigLIP (Fig. 6) shows rapid initial convergence followed by more gradual improvement, eventually stabilizing around 0.06.This suggests that while the policy learns to approximate the expert's actions in many situations, there remains a persistent gap-likely corresponding to scenarios where the expert relied on privileged information not captured in the visual-linguistic embeddings.</p>
<p>A. Baseline Evaluation: Multi-Model Comparison</p>
<p>74.0%</p>
<p>Fig. 5: Multi-model performance comparison.Success rates across three vision-language models using identical BC training procedures (n=100 episodes each).SigLIP demonstrates superior performance, achieving 74.0% success rate compared to CLIP's 62.0% and ViLT's 40.0%.impact of different vision-language architectures, using identical training procedures and evaluation protocols across all three candidate models.We trained separate behavioral cloning policies using embeddings from ViLT (VQA-finetuned), CLIP (ViT-B/32), and SigLIP, evaluating each on 100 episodes for fair statistical comparison.</p>
<p>The results reveal a clear performance hierarchy: SigLIP (74.0%success) &gt; CLIP (62.0%) &gt; ViLT (40.0%), as shown in Figure 5. Remarkably, SigLIP achieved both the highest success rate and superior efficiency, requiring an average of only 369.4 timesteps for successful episodes compared to CLIP's 417.6 and ViLT's 472.0 timesteps.This dual superiority in both success and efficiency strongly validates our methodological choice of SigLIP as the vision-language backbone.</p>
<p>a) Embedding Dimensionality and Performance: The performance ranking aligns closely with embedding dimensionality: SigLIP's 1152-dimensional representations significantly outperform ViLT's 768-dimensional and CLIP's 512-dimensional embeddings.This suggests that richer representational capacity translates directly to better navigation performance, even when using frozen embeddings without task-specific adaptation.</p>
<p>b) BC Loss vs. Navigation Performance: Interestingly, the navigation performance ranking inversely correlates with behavioral cloning loss (Fig. 6).ViLT and CLIP achieved lower BC training losses, while SigLIP exhibited higher BC loss but superior actual navigation performance.This confirms our earlier hypothesis that BC loss measures imitation fidelity rather than task performance-SigLIP's richer 1152-dimensional embeddings are harder for the BC network to compress into action predictions, but they contain more navigationrelevant information that enables better spatial reasoning.c) Spatial Sensitivity Validation: These results validate our spatial sensitivity analysis from Section II-B, where SigLIP demonstrated the most pronounced spatial distinction (26.5% cosine distance difference between same/different spatial positions) compared to CLIP (13.0%) and ViLT (16.3%).The navigation performance hierarchy directly mirrors the spatial sensitivity ranking, confirming that inherent spatial understanding in visionlanguage embeddings translates to practical navigation capabilities.</p>
<p>d) Statistical Significance: The 95% confidence intervals reveal statistically significant differences between models: SigLIP (74.0%[65.4% -82.6%]),CLIP (62.0%[52.5% -71.5%]), and ViLT (40.0%[30.4% -49.6%]).The non-overlapping confidence intervals between SigLIP and ViLT, and minimal overlap between SigLIP and CLIP, demonstrate that the performance differences are statistically robust.</p>
<p>e) VQA Fine-tuning Limitations: ViLT's poor performance (40.0%success), despite being fine-tuned for visual question answering, suggests that task-specific fine-tuning on non-embodied datasets may not transfer effectively to robotic navigation.The VQA fine-tuning appears to have specialized ViLT for discrete questionanswering rather than the continuous spatial reasoning required for navigation, highlighting the importance of representation generality for embodied tasks.</p>
<p>These findings provide several key insights for using VLMs in navigation: (1) larger embedding dimensions generally improve performance when sufficient training data is available, (2) models with stronger inherent spatial sensitivity achieve better navigation results, (3) BC loss is not predictive of downstream task performance, and (4) general-purpose vision-language models may outperform task-specific fine-tuned variants for embodied applications requiring spatial reasoning.</p>
<p>V. DISCUSSION AND CONCLUSION</p>
<p>Our work addresses a fundamental question in embodied AI: can pretrained vision-language embeddings alone serve as a sufficient representation for navigation policies?The results provide a nuanced answer: while these embeddings enable basic language-guided navigation (74% success rate), they show clear limitations in efficiency and consistency compared to stateaware approaches.This performance gap offers valuable insights into both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks.SigLIP achieves superior navigation performance despite higher BC loss than ViLT and CLIP, suggesting that BC loss does not predict downstream task performance for embodied applications.</p>
<p>A. Semantic Grounding vs. Spatial Reasoning</p>
<p>Our results demonstrate that pretrained visionlanguage embeddings excel at semantic grounding-connecting language descriptions to visual observations.The policy successfully differentiates between colored targets based solely on the joint embeddings of images and instructions.This capability aligns with the core strength of models like SigLIP [23] and CLIP [18], which are trained specifically to align visual and linguistic concepts.</p>
<p>However, the efficiency gap reveals that these embeddings struggle with more complex spatial reasoning and planning.While our VLM selection study showed that SigLIP encodes basic spatial sensitivity (distinguishing same vs.different object positions), this level of spatial understanding appears insufficient for efficient navigation.The 3.2× longer paths taken by our policy suggest that while it can identify targets, it lacks the temporal memory and higher-order spatial reasoning needed for direct navigation.Unlike specialized approaches that incorporate mapping modules [7] or explicit spatial reasoning components [2], our minimalist policy must infer spatial relationships purely from a sequence of independent embeddings without explicit spatial representation.</p>
<p>This distinction between basic spatial sensitivity and complex spatial reasoning highlights an important consideration for robotics researchers: while foundation models offer powerful semantic representations and some inherent spatial understanding out-of-the-box, they may need to be complemented with dedicated spatial reasoning mechanisms for tasks requiring efficient path planning and exploration.Recent work in latent space planning [5,19] offers promising directions for bridging this gap by learning structured representations that support both semantic understanding and planning.</p>
<p>B. Prompt Engineering Sensitivity</p>
<p>An important consideration in our approach is the sensitivity to prompt design.Our initial experiments with simple color-only instructions (e.g., "Go to the red ball") yielded significantly lower performance.The inclusion of relative spatial cues ("The target is red ball which is to your left.Move toward the ball.")proved crucial for enabling the VLM embeddings to ground both semantic and spatial information effectively.This highlights the importance of prompt engineering when leveraging foundation models for embodied tasks, as the linguistic framing directly impacts the model's ability to extract navigation-relevant features from the joint embedding space.</p>
<p>C. Performance-Complexity Tradeoffs</p>
<p>Our minimalist approach highlights an important tradeoff in robotics: simplicity versus performance.More complex approaches like RT-2 [24] or NaVILA [3] can achieve higher performance through fine-tuning or specialized architectures but require substantial computational resources and engineering effort.In contrast, our approach using frozen embeddings and simple behavioral cloning demonstrates that reasonable performance can be achieved with minimal complexity.</p>
<p>This tradeoff is particularly relevant for resourceconstrained applications where training compute or specialized hardware may be limited.Our results suggest that for basic navigation tasks with clear visual targets, pretrained embeddings alone may provide sufficient capabilities without the need for extensive fine-tuning or complex architectures.However, for tasks requiring efficient navigation or complex reasoning, the additional complexity of specialized approaches may be justified.</p>
<p>Recent work in student-teacher distillation [10] and zero-shot policy transfer [8] offers promising directions for better balancing this tradeoff.By more effectively distilling the privileged expert's knowledge into a realizable student policy, it may be possible to achieve performance closer to the expert while maintaining the simplicity of our approach.</p>
<p>D. Implications for Future Research</p>
<p>Our findings suggest several promising directions for future research at the intersection of foundation models and robotics:</p>
<p>• Hybrid architectures: To address the repeated circling and target confusion failures we observed, combining the semantic richness of pretrained embeddings with explicit spatial memory could enable the policy to maintain consistent object identification across viewpoints.World models [6] offer a promising framework for integrating perception with spatial reasoning.• Task-specific representation learning: The confusion between visually similar targets suggests that generic vision-language embeddings may not optimally encode the distinctions most relevant for navigation.Approaches like R3M [15] could provide embodiment-aware representations that better capture navigation-relevant features while building on the spatial sensitivity already present in models like SigLIP.• Data-efficient adaptation: The timeout failures indicate that frozen embeddings struggle with systematic exploration.Lightweight adaptation techniques could align pretrained representations with navigation-specific requirements without the full computational cost of fine-tuning.</p>
<p>E. Conclusion</p>
<p>Our work provides an important empirical baseline for understanding the capabilities and limitations of pretrained vision-language embeddings in embodied navigation tasks.By demonstrating that these embeddings alone can achieve a 74% success rate in following language instructions, we highlight their potential as lightweight representations for robotics.However, the significant efficiency gap compared to privileged experts reveals the need for additional inductive biases or specialized components to achieve expert-level performance.</p>
<p>This minimalist approach provides robotics researchers with a practical baseline for evaluating when to use foundation models as-is, versus when to invest in specialized components.As foundation models continue to advance in capability and efficiency, understanding their intrinsic strengths and limitations becomes increasingly important for effective system design.By isolating and quantifying the specific contribution of vision-language embeddings to navigation performance, our work enables more informed architectural decisions at the intersection of foundation models and embodied robotics.</p>
<p>VI. ACKNOWLEDGMENT</p>
<p>This work is funded from NSF-USDA COALESCE grant #2021-67021-34418.</p>
<p>Fig. 2 :
2
Fig. 2: Expert policy training reward.Reward plot for the expert policy π β , which quickly learns to successfully navigate to specified targets using privileged state information.</p>
<p>Algorithm 1 3 :
13
Training Language-Conditioned Navigation Policy via Behavioral Cloning Require: Simulator S, expert policy π β , pretrained VLM (SigLIP), number of episodes N Ensure: Learned policy π that maps VLM joint embeddings to robot actions 1: Initialize dataset D ← ∅ 2: for i = 1 to N do Sample initial state s 0 in S</p>
<p>Fig. 3 :
3
Fig. 3: Simulation environment.Simulation environment showing the Jetbot with multiple colored spheres.During evaluation, the robot must navigate to a specific colored sphere based on language instructions that include both semantic color identification and relative spatial cues using only RGB camera input and pretrained vision-language embeddings.</p>
<p>Fig. 4 :
4
Fig. 4: Reward comparison.Cumulative rewards comparison between the expert (π β , blue) and VLM-based policy (π, orange).The learned policy shows greater variability and consistently lower rewards due to inefficient navigation and occasional failures.</p>
<p>While our main results demonstrate SigLIP's effectiveness, we conducted a controlled comparison to validate our VLM selection methodology and understand the</p>
<p>Fig. 6 :
6
Fig.6: Behavioral cloning loss across VLM architectures.SigLIP achieves superior navigation performance despite higher BC loss than ViLT and CLIP, suggesting that BC loss does not predict downstream task performance for embodied applications.</p>
<p>TABLE I :
I
VLM spatial sensitivity analysis.Cosine distance between joint image-text embeddings across spatial configurations.Lower values indicate greater embedding similarity given a task.All models show smaller distances for same spatial positions, but SigLIP shows the most pronounced separation.</p>
<p>TABLE II :
II
Performance comparison between policies.</p>
<p>Getting vit in shape: Scaling laws for compute-optimal model design. Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, Lucas Beyer, 2024</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia, 2024</p>
<p>Navila: Legged robot vision-language-action model for navigation. An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang, 2025</p>
<p>Clip-nav: Using clip for zeroshot vision-and-language navigation. Gunnar Vishnu Sashank Dorbala, Robinson Sigurdsson, Jesse Piramuthu, Gaurav S Thomason, Sukhatme, 2022</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, 2019</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, 2024</p>
<p>Visual language maps for robot navigation. Chenguang Huang, Oier Mees, Andy Zeng, Wolfram Burgard, 2023</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, 2022</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, 2021</p>
<p>Distilling realizable students from unrealizable teachers. Yujin Kim, Nathaniel Chin, Arnav Vasudev, Sanjiban Choudhury, 2025</p>
<p>Constrained behavior cloning for robotic learning. Wensheng Liang, Jun Xie, Zhicheng Wang, Jianwei Tan, Xiaoguang Ma, 2024</p>
<p>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings. Arjun Majumdar, Gunjan Aggarwal, Bhavika Suresh Devnani, Judy Hoffman, Dhruv Batra, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Orbit: A unified simulation framework for interactive robot learning environments. Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, Animesh Garg, 10.1109/LRA.2023.3270034IEEE Robotics and Automation Letters. 862023</p>
<p>Language-conditioned offline rl for multi-robot navigation. Steven Morad, Ajay Shankar, Jan Blumenkamp, Amanda Prorok, 2024</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, 2022</p>
<p>Robotic-clip: Fine-tuning clip on action data for robotic applications. Nghia Nguyen, Minh Nhat Vu, Tung D Ta, Baoru Huang, Thieu Vo, Ngan Le, Anh Nguyen, 2024</p>
<p>. Nvidia Isaac Sim, 2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021</p>
<p>Latent plans for task-agnostic offline reinforcement learning. Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, Wolfram Burgard, 2022</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, 2017</p>
<p>Vlm-social-nav: Socially aware robot navigation through scoring using vision-language models. Daeun Song, Jing Liang, Amirreza Payandeh, Xuesu Amir Hossain Raj, Dinesh Xiao, Manocha, 2024</p>
<p>Openworld object manipulation using pre-trained visionlanguage models. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, Karol Hausman, 2023</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, 2023</p>
<p>RT-2: Visionlanguage-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Grecia Pannag R Sanketi, Salazar, Krista Michael S Ryoo, Kanishka Reymann, Karl Rao, Igor Pertsch, Henryk Mordatch, Yao Michalewski, Sergey Lu, Lisa Levine, Tsang-Wei Edward Lee, Isabel Lee, Yuheng Leal, Dmitry Kuang, Ryan Kalashnikov, Julian, J Nikhil, Alex Joshi, Jasmine Irpan, Alexander Hsu, Karol Herzog, Keerthana Hausman, Chuyuan Gopalakrishnan, Pete Fu, Chelsea Florence, Finn, Avinava Kumar, Danny Dubey, Tianli Driess, Ding, Montserrat Gonzalez Arenas, and Kehang Han. Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan20237th Annual Conference on Robot Learning</p>            </div>
        </div>

    </div>
</body>
</html>
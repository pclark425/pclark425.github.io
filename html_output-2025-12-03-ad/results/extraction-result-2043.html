<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2043 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2043</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2043</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-281394802</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.14380v2.pdf" target="_blank">CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a"coach"for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2043.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2043.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRAFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coaching Reinforcement learning Autonomously using Foundation models for Multi-Robot Coordination Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses foundation models (LLMs and VLMs) as a 'coach' to automatically decompose long-horizon multi-robot coordination tasks into curricula of subtasks, generate executable reward functions, iteratively refine rewards with VLM feedback, and sequentially train MARL policies to achieve complex coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Given a natural-language description of the target task and environment, a curriculum LLM generates multiple candidate curricula (lists of subtasks described in natural language grounded in environment state variables). The LLM refines candidates into a final staged curriculum (up to 5 subtasks) that specify subgoals and suggested reward components. For each subtask the reward-generation LLM produces an executable Python reward function split into named reward components; policies are trained sequentially per subtask with CTDE MARL (MAPPO). After training a subtask, a VLM evaluates rollouts (snapshot images + state trajectories) to decide success/failure; on failure an advice VLM produces free-form advice based on reward-component learning-curve images, and a refine LLM rewrites the reward (up to J refinements, J=3 used). When moving between subtasks, actor weights are carried over while exploration std and value network are reset to avoid loss of plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Multi-quadruped navigation and bimanual manipulation (Quadruped Gate, Quadruped Seesaw, Two Arm Lift)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Robotics domains with high-dimensional continuous action spaces, decentralized partially-observable control (Dec-POMDP), physical interaction between agents and objects, long-horizon tasks requiring staged coordination (reach/grasp/lift or coordinated passage), multiple reward components, and nonstationarity from decentralized agents.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Curriculum LLM and reward LLM are provided environment description and explicit state variables (e.g., Euclidean distances from end-effector to handle, orientation differences, gripper states, pot elevation, pot tilt as cosine, Frobenius norm for orientation differences, distances between robots/objects). VLM evaluation is given snapshot images and state-variable trajectories and task-relevant metrics extracted from state for judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>VLM-based policy evaluation and advice; separate refine LLM; MAPPO MARL training (CTDE); helper functions for reward generation; reward-component logging and learning-curve images; exploration resets and value-network reinitialization between subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>CRAFT (LLM-generated curricula + VLM refinement + sequential training) achieved peak success rates across environments: Quadruped Gate up to 90%, Quadruped Seesaw up to 60%, Two Arm Lift up to 100% (simulation top runs). Zero-shot sim-to-real transfer on Quadruped Gate yielded 65% success over 20 hardware runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>Not reported as a direct manual-curated curricula baseline; comparison made to 'no_curriculum' (training with final CRAFT-generated reward but without staged curriculum) — CRAFT outperformed no_curriculum in Quadruped Gate and Two Arm Lift (no_curriculum could not match staged training; Quadruped Seesaw had comparable performance to CRAFT). Exact numeric success rates for manual curricula not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>No explicit heuristic-based curriculum baseline reported. A 'no_refinement' baseline (resampling reward LLM without VLM-guided refinement) shows lower effective curricula ratio and lower average performance though its maximum-success runs can be comparable when successful.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Baselines trained without staged curricula: env_reward (environment-provided dense rewards) achieved ~0% success in most experiments; example_reward (example used to seed reward LLM) achieved <10% success in all environments. CRAFT substantially outperformed these.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Zero-shot transfer of Quadruped Gate policy from sim to real robots (Unitree Go2 and Go1) achieved 65% success over 20 runs without further fine-tuning, demonstrating some sim-to-real generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Stochasticity/variability of foundation-model outputs causes trial-to-trial variability and occasional failed curricula (authors recommend multiple attempts in sim). VLMs lack native 3D motion reasoning — mitigated by providing task-relevant metrics and snapshot images. Reward-generation can produce poorly-shaped rewards requiring iterative refinement (authors used up to J=3 refinements). Carrying actor weights forward can cause loss of plasticity, mitigated by resetting exploration std and reinitializing the value network. Effective curricula generation can sometimes collapse to zero success if poorly structured.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Curriculum staging produced large benefits for long-horizon coordination: CRAFT succeeded where vanilla MARL with env or example rewards failed (env_reward often 0%); staged training was particularly critical in Quadruped Gate and Two Arm Lift, enabling high success rates in long-horizon, high-dimensional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>In robotics tasks requiring precise manipulation and physical balancing (Two Arm Lift) and coordinated navigation (Quadruped Gate/Seesaw), the LLM-generated curricula plus VLM-refinement enabled learning policies that were infeasible for vanilla MARL without hand-crafted curricula or expertly tuned rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Comparisons included: env_reward (environment reward), example_reward (seed example reward), no_curriculum (train with final reward but without staged curriculum), and no_refinement (remove VLM-guided refinement and instead resample LLM rewards). Key ablation observations: removing curriculum (no_curriculum) reduces success in several tasks; removing refinement (no_refinement) lowers effective curricula ratio and average performance; env_reward and example_reward baselines largely fail (0% or <10%). No per-component numeric ablation percentages beyond these reported outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated curricula coupled with executable LLM-generated reward functions and a VLM-guided reward-refinement loop substantially improve learning of long-horizon multi-agent coordination in challenging robotics domains, outperforming training on environment-provided or example rewards and training without staged curricula. The VLM-guided refinement increases reliability of curricula generation; main limitations are stochasticity of foundation models and occasional unreliable curricula, which can be mitigated by multiple simulation trials and iterative refinement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2043.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2043.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum LLM (module)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Generation Large Language Model module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The module that uses an LLM (prompted with environment description, state variables, and candidate curricula) to produce multiple curriculum candidates and then refine them into a final sequence of subtasks with grounded state-variable descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Generates multiple curriculum candidates (up to I candidates) from a natural-language environment/task description; then refines candidates by selecting and merging stronger task definitions into a coherent final curriculum (max 5 subtasks). Each subtask description is grounded in permitted environment state variables to improve reward design and VLM evaluation. The LLM is also provided earlier subtask reward code for context to encourage curricula that exploit previously learned skills.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Same as CRAFT: multi-quadruped navigation and bimanual manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Long-horizon, compositional tasks requiring staged skill acquisition and coordination among agents under partial observability and continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Curriculum prompts include environment description, a list of allowed state variables (e.g., distances, orientations, pot elevation/tilt, gripper state), and previously learned tasks/rewards when available.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used alongside reward-generation LLM, VLM evaluation, and reward-refinement loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>As part of CRAFT, curricula from this module contributed to final policies reaching up to 90% (Quadruped Gate), 60% (Quadruped Seesaw), and 100% (Two Arm Lift) in top runs; multiple candidate generation + refinement helped avoid unstable zero-shot outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Zero-shot curriculum generation can be unstable due to LLM stochasticity and can produce overly simple, overly long, or misaligned curricula; authors mitigate by generating and refining multiple candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Designed specifically to break long-horizon goals into manageable subtasks, improving learning in long-horizon coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Grounding subtask descriptions in environment state variables helps produce semantically useful subtasks for robotics domains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Indirectly evaluated via no_curriculum comparison which shows staged curricula produced by this module are necessary in several tasks (Gate and Two Arm Lift) for high success.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Generating multiple curriculum candidates and asking the LLM to refine them (rather than relying on a single zero-shot output) improves curriculum quality and downstream policy learning; grounding tasks in explicit state variables increases alignment with reward design and VLM evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2043.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2043.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-guided reward-refinement loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language Model guided reward-refinement loop (advice VLM + refine LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-part loop where a VLM evaluates trained policy rollouts and produces failure reasons and reward-component learning-curve advice; a refine LLM then rewrites the reward function in executable code using that advice, iterating up to a fixed number of refinements to produce improved rewards for the subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>hybrid LLM+VLM (LLM generates rewards; VLM evaluates and advises)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-4o-2024-08-06 (used for reward generation and refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>After training on a subtask with an LLM-generated base reward, a VLM evaluates snapshot images and state trajectories to decide success/failure and provide a textual failure reason. The VLM also inspects plotted reward-component learning curves (images) and issues targeted advice (≤3 recommendations) for changing reward structure. A refine LLM receives the previous reward code + VLM advice and produces a refined reward function in executable Python; this loop repeats up to J iterations (J=3 in experiments) or until success.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Multi-robot coordination tasks used in CRAFT (Quadruped Gate, Quadruped Seesaw, Two Arm Lift)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Requires visual and temporal judgement about coordination and stability; reward-shaping must balance multiple competing objectives (e.g., lift vs balance).</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Advice VLM uses failure reason, subtask description, previous reward code, and images of per-component reward learning curves; VLM evaluation uses snapshot images and state trajectories with task-relevant metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Operates with reward-generation LLM, MAPPO training, and curriculum LLM; produces refined executable reward code to be retrained.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>VLM-guided refinement increased the 'effective curricula ratio' (fraction of curricula that achieved nonzero success) versus the no_refinement baseline and improved average reliability. Where no_refinement occasionally achieved comparable maximum success for lucky runs, its average performance and consistency were worse.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>VLMs can struggle with 3D motion reasoning; advice quality depends on the clarity of reward-component curves and VLM capability. The loop is limited by the maximum refinement iterations (J) and by the scope of allowed helper functions and state variables defined in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Crucial for shaping rewards that enable progress on long-horizon subtasks (e.g., converting a vanishing lift term into a continuous elevation shaping reward enabled successful lifting).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Particularly effective in manipulation tasks where multiple reward components (grasp/proximity/lift/stability) must be balanced; VLM advice identified weak components and suggested concrete changes (e.g., remove square on elevation, increase lift weight, reduce balance weight) that produced success.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Comparison to 'no_refinement' (resample reward LLM without VLM advice) shows reduced reliable curriculum generation and lower average performance; authors report higher effective curricula ratio with the VLM-guided loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>A closed-loop where visual evaluation and reward-component diagnostics (via VLM) inform targeted reward refinements by an LLM yields more reliable and effective reward functions for subtasks than naive resampling of LLM rewards; this increases the consistency of successful curricula.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curricullm: Automatic task curricula design for learning complex robot skills using large language models <em>(Rating: 2)</em></li>
                <li>Environment curriculum generation via large language models <em>(Rating: 2)</em></li>
                <li>Eureka: Human-level reward design via coding large language models <em>(Rating: 2)</em></li>
                <li>Vision-language models are zero-shot reward models for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Progprompt: Generating situated robot task plans using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2043",
    "paper_id": "paper-281394802",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "CRAFT",
            "name_full": "Coaching Reinforcement learning Autonomously using Foundation models for Multi-Robot Coordination Tasks",
            "brief_description": "A framework that uses foundation models (LLMs and VLMs) as a 'coach' to automatically decompose long-horizon multi-robot coordination tasks into curricula of subtasks, generate executable reward functions, iteratively refine rewards with VLM feedback, and sequentially train MARL policies to achieve complex coordination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "gpt-4o-2024-08-06",
            "llm_model_size": null,
            "curriculum_description": "Given a natural-language description of the target task and environment, a curriculum LLM generates multiple candidate curricula (lists of subtasks described in natural language grounded in environment state variables). The LLM refines candidates into a final staged curriculum (up to 5 subtasks) that specify subgoals and suggested reward components. For each subtask the reward-generation LLM produces an executable Python reward function split into named reward components; policies are trained sequentially per subtask with CTDE MARL (MAPPO). After training a subtask, a VLM evaluates rollouts (snapshot images + state trajectories) to decide success/failure; on failure an advice VLM produces free-form advice based on reward-component learning-curve images, and a refine LLM rewrites the reward (up to J refinements, J=3 used). When moving between subtasks, actor weights are carried over while exploration std and value network are reset to avoid loss of plasticity.",
            "domain_name": "Multi-quadruped navigation and bimanual manipulation (Quadruped Gate, Quadruped Seesaw, Two Arm Lift)",
            "domain_characteristics": "Robotics domains with high-dimensional continuous action spaces, decentralized partially-observable control (Dec-POMDP), physical interaction between agents and objects, long-horizon tasks requiring staged coordination (reach/grasp/lift or coordinated passage), multiple reward components, and nonstationarity from decentralized agents.",
            "state_conditioning": true,
            "state_conditioning_details": "Curriculum LLM and reward LLM are provided environment description and explicit state variables (e.g., Euclidean distances from end-effector to handle, orientation differences, gripper states, pot elevation, pot tilt as cosine, Frobenius norm for orientation differences, distances between robots/objects). VLM evaluation is given snapshot images and state-variable trajectories and task-relevant metrics extracted from state for judgment.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "VLM-based policy evaluation and advice; separate refine LLM; MAPPO MARL training (CTDE); helper functions for reward generation; reward-component logging and learning-curve images; exploration resets and value-network reinitialization between subtasks.",
            "performance_llm_curriculum": "CRAFT (LLM-generated curricula + VLM refinement + sequential training) achieved peak success rates across environments: Quadruped Gate up to 90%, Quadruped Seesaw up to 60%, Two Arm Lift up to 100% (simulation top runs). Zero-shot sim-to-real transfer on Quadruped Gate yielded 65% success over 20 hardware runs.",
            "performance_manual_curriculum": "Not reported as a direct manual-curated curricula baseline; comparison made to 'no_curriculum' (training with final CRAFT-generated reward but without staged curriculum) — CRAFT outperformed no_curriculum in Quadruped Gate and Two Arm Lift (no_curriculum could not match staged training; Quadruped Seesaw had comparable performance to CRAFT). Exact numeric success rates for manual curricula not provided.",
            "performance_heuristic_curriculum": "No explicit heuristic-based curriculum baseline reported. A 'no_refinement' baseline (resampling reward LLM without VLM-guided refinement) shows lower effective curricula ratio and lower average performance though its maximum-success runs can be comparable when successful.",
            "performance_no_curriculum": "Baselines trained without staged curricula: env_reward (environment-provided dense rewards) achieved ~0% success in most experiments; example_reward (example used to seed reward LLM) achieved &lt;10% success in all environments. CRAFT substantially outperformed these.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "Zero-shot transfer of Quadruped Gate policy from sim to real robots (Unitree Go2 and Go1) achieved 65% success over 20 runs without further fine-tuning, demonstrating some sim-to-real generalization.",
            "computational_cost": null,
            "failure_modes_limitations": "Stochasticity/variability of foundation-model outputs causes trial-to-trial variability and occasional failed curricula (authors recommend multiple attempts in sim). VLMs lack native 3D motion reasoning — mitigated by providing task-relevant metrics and snapshot images. Reward-generation can produce poorly-shaped rewards requiring iterative refinement (authors used up to J=3 refinements). Carrying actor weights forward can cause loss of plasticity, mitigated by resetting exploration std and reinitializing the value network. Effective curricula generation can sometimes collapse to zero success if poorly structured.",
            "long_horizon_performance": "Curriculum staging produced large benefits for long-horizon coordination: CRAFT succeeded where vanilla MARL with env or example rewards failed (env_reward often 0%); staged training was particularly critical in Quadruped Gate and Two Arm Lift, enabling high success rates in long-horizon, high-dimensional tasks.",
            "specialized_domain_performance": "In robotics tasks requiring precise manipulation and physical balancing (Two Arm Lift) and coordinated navigation (Quadruped Gate/Seesaw), the LLM-generated curricula plus VLM-refinement enabled learning policies that were infeasible for vanilla MARL without hand-crafted curricula or expertly tuned rewards.",
            "ablation_studies": "Comparisons included: env_reward (environment reward), example_reward (seed example reward), no_curriculum (train with final reward but without staged curriculum), and no_refinement (remove VLM-guided refinement and instead resample LLM rewards). Key ablation observations: removing curriculum (no_curriculum) reduces success in several tasks; removing refinement (no_refinement) lowers effective curricula ratio and average performance; env_reward and example_reward baselines largely fail (0% or &lt;10%). No per-component numeric ablation percentages beyond these reported outcomes.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-generated curricula coupled with executable LLM-generated reward functions and a VLM-guided reward-refinement loop substantially improve learning of long-horizon multi-agent coordination in challenging robotics domains, outperforming training on environment-provided or example rewards and training without staged curricula. The VLM-guided refinement increases reliability of curricula generation; main limitations are stochasticity of foundation models and occasional unreliable curricula, which can be mitigated by multiple simulation trials and iterative refinement.",
            "uuid": "e2043.0"
        },
        {
            "name_short": "Curriculum LLM (module)",
            "name_full": "Curriculum Generation Large Language Model module",
            "brief_description": "The module that uses an LLM (prompted with environment description, state variables, and candidate curricula) to produce multiple curriculum candidates and then refine them into a final sequence of subtasks with grounded state-variable descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "gpt-4o-2024-08-06",
            "llm_model_size": null,
            "curriculum_description": "Generates multiple curriculum candidates (up to I candidates) from a natural-language environment/task description; then refines candidates by selecting and merging stronger task definitions into a coherent final curriculum (max 5 subtasks). Each subtask description is grounded in permitted environment state variables to improve reward design and VLM evaluation. The LLM is also provided earlier subtask reward code for context to encourage curricula that exploit previously learned skills.",
            "domain_name": "Same as CRAFT: multi-quadruped navigation and bimanual manipulation",
            "domain_characteristics": "Long-horizon, compositional tasks requiring staged skill acquisition and coordination among agents under partial observability and continuous control.",
            "state_conditioning": true,
            "state_conditioning_details": "Curriculum prompts include environment description, a list of allowed state variables (e.g., distances, orientations, pot elevation/tilt, gripper state), and previously learned tasks/rewards when available.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Used alongside reward-generation LLM, VLM evaluation, and reward-refinement loop.",
            "performance_llm_curriculum": "As part of CRAFT, curricula from this module contributed to final policies reaching up to 90% (Quadruped Gate), 60% (Quadruped Seesaw), and 100% (Two Arm Lift) in top runs; multiple candidate generation + refinement helped avoid unstable zero-shot outputs.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Zero-shot curriculum generation can be unstable due to LLM stochasticity and can produce overly simple, overly long, or misaligned curricula; authors mitigate by generating and refining multiple candidates.",
            "long_horizon_performance": "Designed specifically to break long-horizon goals into manageable subtasks, improving learning in long-horizon coordination.",
            "specialized_domain_performance": "Grounding subtask descriptions in environment state variables helps produce semantically useful subtasks for robotics domains.",
            "ablation_studies": "Indirectly evaluated via no_curriculum comparison which shows staged curricula produced by this module are necessary in several tasks (Gate and Two Arm Lift) for high success.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Generating multiple curriculum candidates and asking the LLM to refine them (rather than relying on a single zero-shot output) improves curriculum quality and downstream policy learning; grounding tasks in explicit state variables increases alignment with reward design and VLM evaluation.",
            "uuid": "e2043.1"
        },
        {
            "name_short": "VLM-guided reward-refinement loop",
            "name_full": "Vision-Language Model guided reward-refinement loop (advice VLM + refine LLM)",
            "brief_description": "A two-part loop where a VLM evaluates trained policy rollouts and produces failure reasons and reward-component learning-curve advice; a refine LLM then rewrites the reward function in executable code using that advice, iterating up to a fixed number of refinements to produce improved rewards for the subtask.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "hybrid LLM+VLM (LLM generates rewards; VLM evaluates and advises)",
            "llm_model_name": "gpt-4o-2024-08-06 (used for reward generation and refinement)",
            "llm_model_size": null,
            "curriculum_description": "After training on a subtask with an LLM-generated base reward, a VLM evaluates snapshot images and state trajectories to decide success/failure and provide a textual failure reason. The VLM also inspects plotted reward-component learning curves (images) and issues targeted advice (≤3 recommendations) for changing reward structure. A refine LLM receives the previous reward code + VLM advice and produces a refined reward function in executable Python; this loop repeats up to J iterations (J=3 in experiments) or until success.",
            "domain_name": "Multi-robot coordination tasks used in CRAFT (Quadruped Gate, Quadruped Seesaw, Two Arm Lift)",
            "domain_characteristics": "Requires visual and temporal judgement about coordination and stability; reward-shaping must balance multiple competing objectives (e.g., lift vs balance).",
            "state_conditioning": true,
            "state_conditioning_details": "Advice VLM uses failure reason, subtask description, previous reward code, and images of per-component reward learning curves; VLM evaluation uses snapshot images and state trajectories with task-relevant metrics.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Operates with reward-generation LLM, MAPPO training, and curriculum LLM; produces refined executable reward code to be retrained.",
            "performance_llm_curriculum": "VLM-guided refinement increased the 'effective curricula ratio' (fraction of curricula that achieved nonzero success) versus the no_refinement baseline and improved average reliability. Where no_refinement occasionally achieved comparable maximum success for lucky runs, its average performance and consistency were worse.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "VLMs can struggle with 3D motion reasoning; advice quality depends on the clarity of reward-component curves and VLM capability. The loop is limited by the maximum refinement iterations (J) and by the scope of allowed helper functions and state variables defined in prompts.",
            "long_horizon_performance": "Crucial for shaping rewards that enable progress on long-horizon subtasks (e.g., converting a vanishing lift term into a continuous elevation shaping reward enabled successful lifting).",
            "specialized_domain_performance": "Particularly effective in manipulation tasks where multiple reward components (grasp/proximity/lift/stability) must be balanced; VLM advice identified weak components and suggested concrete changes (e.g., remove square on elevation, increase lift weight, reduce balance weight) that produced success.",
            "ablation_studies": "Comparison to 'no_refinement' (resample reward LLM without VLM advice) shows reduced reliable curriculum generation and lower average performance; authors report higher effective curricula ratio with the VLM-guided loop.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "A closed-loop where visual evaluation and reward-component diagnostics (via VLM) inform targeted reward refinements by an LLM yields more reliable and effective reward functions for subtasks than naive resampling of LLM rewards; this increases the consistency of successful curricula.",
            "uuid": "e2043.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curricullm: Automatic task curricula design for learning complex robot skills using large language models",
            "rating": 2
        },
        {
            "paper_title": "Environment curriculum generation via large language models",
            "rating": 2
        },
        {
            "paper_title": "Eureka: Human-level reward design via coding large language models",
            "rating": 2
        },
        {
            "paper_title": "Vision-language models are zero-shot reward models for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Progprompt: Generating situated robot task plans using large language models",
            "rating": 1
        }
    ],
    "cost": 0.013971,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</p>
<p>Seoyeon Choi seoyeon99@berkeley.edu 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>Kanghyun Ryu kanghyun.ryu@berkeley.edu 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>Jonghoon Ock 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>Negar Mehr 
Seoyeon Choi
Kanghyun Ryu, Jonghoon Ock</p>
<p>Negar Mehr is with Mechanical Engineering
University of California Berkeley</p>
<p>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks
13BC51CC84355BA56511AFF3327C671FReward Function Generation Curriculum Generation
Refine curriculum candidates for bimanual manipulation task : lift and balance the potCandidateProvide reward component learning curve imagesReward GenerationTrain policy and collect rollout information Decision: Failure Pot's elevation never exceeds 0.009 m, well below 0.05 m …Evaluation Reward GenerationTrain policy and collect rollout information Decision: Success Pot's elevation recached 0.05 and also maintained a cosine of pot tilt degree consistently above 0.97…</p>
<p>For each subtask M k , a reward LLM generates a reward function R j k and a decentralized policy π j k , shared among agents, is trained via MARL.A VLM evaluates this policy using state variable trajectories and snapshot images.If the policy fails, an advice VLM provides advice from reward component learning curves, plots of individual reward terms during training.The reward LLM incorporates this advice to generate a revised reward.Successful policy advances to the next subtask and this iterative process continues until the final task is reached.(Right) Demonstration of the Quadruped Gate task, where two quadrupeds coordinate to pass through a narrow gate without collision: the blue robot passes first while the orange robot waits; then, the orange robot proceeds once the gate is clear.This policy was learned in simulation and transferred to real-world.</p>
<p>Abstract-Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems.However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and nonstationary transitions inherent to decentralized settings.On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills.Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination.CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs).In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop.We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors.In addition, we validate the multi-quadruped navigation policy in real hardware experiments.Project website is https://iconlab.negarmehr.com/CRAFT/</p>
<p>I. INTRODUCTION</p>
<p>Learning coordinated behaviors is essential for enabling robots to work together in real-world tasks, yet achieving such coordination remains a significant challenge.Multi-Agent Reinforcement Learning (MARL) offers a promising framework, with notable successes in domains such as realtime strategy games [1], [2] and simulated sports [3], and it is increasingly being explored for multi-robot coordination [4], [5].However, applying MARL to robotics is notoriously difficult due to the high-dimensional action spaces and sparse rewards inherent in robotic tasks alongside the nonstationarity introduced by decentralized decision-making.To coordinate effectively, robots need to reason about each other's actions and navigate complex credit assignment problems [6].These challenges become even more pronounced in multi-robot domains that involve physical interaction, longhorizon planning, and intricate cooperation.In this paper, we consider the problem of teaching robots to coordinate with one another by developing an automatic curriculum generation and learning pipeline that enables MARL policies to acquire effective coordination strategies in complex multirobot tasks.</p>
<p>In the physical world, humans learn complex coordination progressively.For example, a soccer team begins with simple arXiv:2509.14380v2[cs.RO] 1 Oct 2025 cooperative drills, such as short passes, before advancing to elaborate team strategies.As such, curriculum generation has been studied as an effective approach for learning coordination and strategies in MARL domains by structuring the training into stages of increasing complexity [7].A curriculum breaks a complex task into a sequence of subtasks where early subtasks help agents learn fundamental skills.As training progresses, long-horizon objectives are decomposed into manageable subtasks that build on these fundamentals, allowing agents to gradually master complicated coordination strategies.Through this staged progression, curriculum generation enables the emergence of complex long-horizon collaborative behaviors that are difficult to achieve when training agents directly on the target task [5], [8], [9].</p>
<p>Despite its effectiveness, curriculum design is often nontrivial as it requires domain knowledge to identify key steps in long-horizon tasks and reasoning ability to monitor the learning progress [10].Meanwhile, recent advances in foundation models, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), have introduced humanlike reasoning capabilities to robot learning.These reasoning capabilities have been leveraged for task decomposition [11], [12], reward shaping [13], [14], and policy evaluation [15], [16].We argue, however, that these models can go beyond such individual applications and instead serve as a coach, i.e. entity that teaches agents how to coordinate.Coaching naturally integrates all these skills: a good coach breaks down the task, defines success criteria for each subtask, and provides actionable feedback to guide improvement.</p>
<p>In this paper, we introduce CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks.CRAFT first decomposes a complex cooperative task into sequences of subtasks that facilitate the learning of intricate multi-agent coordination policies.Then, each subtask is paired with an LLM-generated reward function in executable code, while a VLM provides feedback on trained policies to iteratively refine rewards through a VLM-guided reward-refinement loop.Through this process, CRAFT produces curricula for long-horizon coordination tasks, designs semantically rich reward functions, and evaluates task-policy alignment using visual information.Through experiments in simulation and on physical robots, we demonstrate that CRAFT enables learning coordination behaviors in multi-quadruped navigation and bimanual manipulation tasks that standard learning methods fail to solve.</p>
<p>In summary, our contributions are the following: 1) We propose CRAFT, a framework that automatically designs curricula for long-horizon multi-robot coordination tasks by leveraging the reasoning capabilities of foundation models.2) We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, which require long-horizon coordination.CRAFT learns complex coordination policies that were infeasible without human-designed curricula or hand-calibrated rewards.3) We validate the multi-quadruped navigation policies learned with CRAFT in hardware experiments, demon-strating their real-world transferability.</p>
<p>II. RELATED WORKS</p>
<p>A. Curriculum Learning and Foundation Models</p>
<p>Curriculum learning accelerates RL by structuring training as a sequence of subtasks with increasing difficulty [8].It has been shown to improve sample efficiency [17], enable the learning of challenging tasks [8], and support multitask learning [18].While many prior works on curriculum design have relied on trial-and-error of human experts [9], [19], more recent efforts have explored leveraging the reasoning capabilities of LLMs for automated curriculum design [20], [21], [22].In this work, we extend curricula design with foundation models to multi-agent settings, where reasoning about agent interactions and long-horizon task decomposition is even more critical.</p>
<p>B. LLM/VLM for Reward Design</p>
<p>Recent works have shown the effectiveness of reasoning and general knowledge of foundation models for reward design.Several works leverage language models to generate reward functions in executable code and further refine them through evolutionary search [13], [23].Others use VLM's ability to evaluate alignment between visual observations and language descriptions, directly querying a VLM for the reward [14] or providing other feedback to the policy, such as success or failure [15], [16], [24].Building on these works, we employ the reward generation capabilities of LLMs to produce reward functions for each subtask in a curriculum, and use a VLM to evaluate trained policies and provide feedback for refining reward functions.</p>
<p>C. Multi-robot Coordination</p>
<p>Several works have explored MARL for multi-robot coordination.For a team of quadrupeds, MARL has been employed in hierarchical structure where it learns highlevel coordination behavior for object manipulation [25], [26] or soccer [4], combined with low-level locomotion policy.However, these methods rely on either carefully designed reward functions [25] or manually specified curricula [26].MARL also has been used for multi-robot navigation with collision avoidance in aerial [27] or mobile robots [28].Notably, curriculum learning frameworks have been actively employed in MARL training [4], [26], [27], demonstrating their effectiveness in learning complex coordination behaviors.Based on these successful curriculum learning experiences in MARL, our aim is to automate curriculum design using foundation models, thereby enabling learning complex coordination behaviors while reducing the need for extensive human intervention in MARL.</p>
<p>III. PROBLEM FORMULATION</p>
<p>In this work, we aim to learn a multi-agent coordination problem where a team of agents learn how to coordinate to achieve a target task M target .We model the target task as a decentralized partially observable Markov decision process (Dec-POMDP [29]), defined by the tuple     M target = ⟨N, S, A, O, P, R target , γ⟩, where N is the number of agents, S is the global state space, and A is the joint action space of the agents.We denote the local observation of each agent i at the global state s as o i = O(s; i).The joint action of all N agents is denoted as a = (a 1 , . . ., a N ), where a i is the action of agent i.The transition probability from state s to s ′ when taking the joint action a is denoted as P (s ′ |s, a).We assume that there exists a shared reward function R target (s, a, s ′ ) that agents need to collectively optimize, and γ is the discount factor.In this work, we train a MARL policy using a centralized training and decentralized execution (CTDE) framework [29], [30], where all agents share the same policy π θ (a i |o i ) that observes each agent's local observation o i and outputs its action a i .Our goal is to learn the shared policy π θ (a i |o i ) such that it maximizes the expected discounted reward
J(θ) = E at,st [ ∞ t γ t R target (s t , a t , s t+1 )]. We define a curriculum C as a sequence of K subtasks C = [M 1 , M 2 , . . . , M K ],
where M k denotes the k-th subtask.Each subtask M k is constructed by replacing the reward R target in M target to its own unique reward R k .Then, a curriculum can equivalently be described as a sequence of reward functions [R 1 , R 2 , . . ., R K ].Therefore, we aim to optimize a sequence of reward functions [R 1 , R 2 , . . ., R K ] so that the final learned policy can accomplish M target .</p>
<p>IV. METHOD</p>
<p>Motivated by how human coaches train teams for coordination tasks, our method leverages LLMs and VLMs as a coach for teaching robots on how to coordinate.In training a team for complex coordination, a coach would first decompose the target task into a sequence of subtasks that specify the details of the coordination strategy.The coach then trains the team on each subtask by defining its objectives (rewards).Once training is complete, the coach evaluates the team's performance, deciding whether to progress to the next subtask or revisit the current one.Inspired by this process, our framework follows five key stages as shown in Fig. 1: 1) Curriculum generation module -A curriculum LLM decomposes the long-horizon coordination task into a sequence of subtasks described in natural language.2) Reward function generation module -A reward generation LLM generates a reward function in executable python code based on the natural language descriptions of the subtasks, providing dense rewards that clearly specify the desired behavior for each subtask.3) Policy evaluation module -An evaluation VLM evaluates the success or failure of the policy trained with the LLM-generated reward based on the visual and quantitative rollouts of the policy.4) Reward refinement module -If the policy fails to achieve the desired behavior, an advice VLM provides advice on how to change the reward based on the rollout information and learning curve.Then, an LLM takes the advice and refines the reward function.5) Sequential training of subtasks -Throughout the training, we initialize each subtask with the policy learned from the previous one while motivating exploration to learn the new subtask.</p>
<p>In the following, we discuss each of these modules in detail.</p>
<p>A. Curriculum Generation Module</p>
<p>As a first step, we break down a coordination task into a sequence of subtasks.Given a natural language description of the target task and environment, a curriculum LLM outputs a sequence of subtasks in natural language.However, relying solely on zero-shot curriculum generation can result in unstable outputs due to the stochastic nature of LLM responses, ranging from overly simple to unnecessarily long or misaligned with the target task.To mitigate this, we first generate multiple candidate curricula C i , i ∈ {1, 2, . . ., I}.We then prompt the LLM with the environment description, the target task M target , and the candidate curricula, asking it to refine these candidates into a more coherent and effective curriculum.An example is shown in Fig. 2. -The shaped "lift_reward" is vanishingly small because you square a tiny number.Remove the square on elevation so that small gains in height yield a larger gradient … -Or simply increase the lift multiplier K from 80 up to a few hundred so that lift_reward becomes competitive with balance_reward.</p>
<p>Re-balance lift vs. balance shaping.</p>
<p>-Reduce the balance term (e.g. from 2.0 to 0.5 or 1.0) so the policy isn't "over-paid" to stay flat.Through this process, we obtain the final curriculum C = [l 1 , ..., l K ], where l k is a natural language description of each subtask M k .The description l k is grounded in the state variables of the environment to improve reward design and VLM evaluation.For example, in Task 1: Approach Handles (Fig. 2), instead of a vague description such as "each robot arm should get closer to its handle," the LLM-generated subtask explicitly uses the state variable Euclidean distance to its respective handle, producing the description "each arm minimizes its Euclidean distance to its target handle."</p>
<p>B. Reward Function Generation Module</p>
<p>Based on natural language description l k , we generate a subtask-specific reward R k for subtask M k in executable code.Compared to describing the curriculum through environment configurations [21] or PDDL [31], specifying the task as a reward provides greater flexibility in representing diverse subtasks across different domains.Furthermore, translating the reward into executable code enables seamless integration with existing environments and algorithms.In this subsection, we describe the generation of the initial base reward R 0 k .Here, the superscript denotes the refinement index, where R 0 k is the base reward for subtask k, while R j k denote the reward obtained after the j-th refinement that will be discussed in Section IV-D.</p>
<p>To generate a base reward function R 0 k , we prompt the LLM with the language description of the environment, current subtask description l k , and an example reward function.Since writing reward functions from scratch for highdimensional multi-agent systems can be challenging, we provide example reward functions, which are simple, structured variations of the environment-provided rewards, along with helper functions for state processing.For instance, in the bimanual manipulation task shown in Fig. 3, a helper function self._get_tilt_degree_cosine() is provided to compute the cosine value of the pot's tilt angle.Since curriculum learning should exploit knowledge acquired in previous subtasks, we also include the reward functions from earlier subtasks as additional context in the prompt.</p>
<p>When writing a reward function, we prompt the LLM to separate the reward into reward components (e.g.lift_ reward, balance_reward in Fig. 3) during training time.These logs will later be provided as reward component learning curves for reward refinements in Section IV-D.After reward generation, we train a policy π 0 k with the base reward function R 0 k via MARL, such as MAPPO [29].We note that as we assume using CTDE MARL framework, we are designing a reward for a team, rather than specific agents.</p>
<p>C. Policy Evaluation Module</p>
<p>Designing an effective reward function from natural language description is challenging, and one-shot LLMgenerated rewards often fail to produce the desired behavior.Therefore, we need a verification step to evaluate the trained policy π 0 k and determine whether to advance to the next subtask or retry the current one.</p>
<p>When evaluating a robot policy, humans rely on visual trajectory rollouts to analyze the collaboration between robots and the progress in the subtask.Motivated by this, we use VLMs for evaluating the success or failure of π 0 k by providing a sequence of snapshot images.Moreover, we also provide state variable trajectory data to make it easier for VLMs to conclude the subtask success.This design choice allows our method to be easily applied to various multi-robot platforms and arbitrary LLM-designed subtasks.However, even state-of-the-art LLMs and VLMs lack a native understanding of 3D motion [32], [33].This limitation is particularly pronounced in multi-robot environments, where the dimensionality of the state space scales with the number of agents, making it difficult for VLMs to reason about coordination behaviors directly from raw 3D state variables.Therefore, we extract and provide task-relevant metrics, such as Frobenius norm for orientation differences or distance between objects that are easier for LLMs/VLMs to interpret.When the VLM judges the policy to be successful, we proceed to the next subtask M k+1 .If it concludes the policy as a failure, policy evaluation VLM provides failure reasons which are used for refining the current reward function R 0 k to be more aligned with the desired behavior.</p>
<p>D. Reward Refinement Module</p>
<p>When generated reward R 0 k fails to induce the desired behavior, we should design a new reward function and retry the training process.A naive approach to this reward refinement is resampling from the reward generation module with the same prompt [20], or with minimal additional information, such as appending the previously attempted reward function [34].However, such naive resampling methods require extensive trial and error and are not scalable to high-dimensional multi-agent systems.To overcome this, we draw inspiration from prior works that have suggested that a team of specialized models can be more effective than a single model for complex tasks [35], [36].Following this, we introduce two additional models: an advice VLM and a refine LLM, which together enable more meaningful refinement.</p>
<p>First, we prompt the advice VLM to generate unrestricted, free-form advice on how to refine the current reward function based on the current subtask description l k , the previously attempted reward function R j k , and the failure reason obtained from policy evaluation module in Section IV-C.In addition, we provide the learning curves of individual per-step reward components, which indicate how each component evolved during training.Since these values often oscillate and are difficult to represent as long numerical sequences in text, we instead convert them into graphical images, offering the VLM a compact and interpretable representation.</p>
<p>Second, in the refine LLM, we provide the previously attempted reward function R  VLM-guided reward-refinement loop.We allow a maximum of J refinement iterations.If the policy still fails after J iterations, the LLM selects the best policy among π 0 k , ..., π J k using rollout information and proceeds to the next subtask.</p>
<p>In our experiments, we found that J = 3 iterations were sufficient to generate a successful reward.An example of this refinement process is shown in Fig. 3.</p>
<p>E. Sequential Training of Subtasks</p>
<p>When moving from one subtask to the next, the challenge lies in balancing exploitation and exploration: the policy should retain useful behaviors learned in the previous subtask, while adapting to the new reward function.To exploit prior knowledge, we initialize the weights of policy π k+1 for subtask M k+1 with the weights of policy π k from subtask M k .The initialized policy π k+1 is then trained with the reward function R k+1 .However, because the reward generation LLM is provided with the history of rewards, parts of R k+1 often resemble R k .This can cause a loss of plasticity [37], [38] leading the policy π k+1 to converge to a local optimum similar to policy π k , preventing further learning under the new reward function R k+1 .</p>
<p>To avoid loss of plasticity and encourage progress on the new subtask, we employ two techniques.First, we reset the exploration parameter of the policy network to reintroduce exploration at the beginning of each subtask training.In our case, we reset the policy standard deviation layer of MAPPO at the beginning of subtask training.This helps the policy to explore other actions while maintaining current behavior and escape the local-optima of the earlier reward function.Furthermore, we reset the value network with random weights [26].By resetting only the value network while maintaining the actor network weights, the actor can exploit the previous policy while the critic is reinitialized to estimate the value under the new reward function.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we evaluate our framework in collaborative multi-robot scenarios.We consider a bimanual manipulation task and multi-quadruped navigation tasks that require complex long-horizon coordination.We show that CRAFT can learn coordination that is challenging to achieve with vanilla MARL even with human-designed reward functions.We also validate our quadruped navigation task in hardware to show our policy transfers to real-world.CRAFT achieves highest success rate on every environment, demonstrating its ability to learn complex coordination tasks that are challenging to learn without curriculum or well-crafted reward functions.</p>
<p>A. Experiment Setup 1) Simulation Environments: We evaluate CRAFT in the following multi-agent coordination environments.The environments are shown in Fig. 4.</p>
<p>• Quadruped Gate: Two quadrupeds should coordinate to pass the narrow gate without collision [39].We define a task as success when both agents have passed the gate.Each quadruped has a continuous action space of xand y-axis velocities, and yaw velocity.• Quadruped Seesaw: Two quadrupeds should coordinate on the seesaw so that one agent climbs up to the platform at the top of the seesaw while the other balances it [39].We define a task as success when one of the agents reaches the target platform.The action space is identical to Gate environment.• Two Arm Lift: Two robot arms should coordinate in lifting the pot together while keeping the pot level [40].We define a task as success when the pot is lifted above 0.1m and no more tilted than 30 degrees.Each arm has a continuous action space, which consists of six end-effector delta pose, position and orientation changes relative to the current pose, and one gripper state.These environments are exceptionally challenging for MARL as they demand long-horizon coordination and decentralized control under partial observability.To our knowledge, no prior RL methods without expert demonstrations have succeeded in solving them reliably.We use MAPPO [29], which employs decentralized execution with only local observations, to evaluate CRAFT's ability to learn decentralized coordination.MAPPO is implemented with OpenRL library [41] for the multi-quadruped navigation, and skrl library [42] for the bimanual manipulation task.We utilize gpt-4o-2024-08-06 as our LLM models and o4-mini-2025-04-16 as our VLM models.</p>
<p>2) Baselines &amp; Ablation Study: We design our experiments such that we answer the following questions.Q1 Can our framework learn MARL policies that require complex coordination between multiple robots?Q2 Is curriculum learning framework necessary for learning effective coordination policies?Q3 To what extent is the reward refinement module necessary for learning successful policies?</p>
<p>To answer Q1, we compare CRAFT with env_reward and example_reward baselines where env_reward uses the environment-provided reward, while example_reward uses the example reward provided to reward generation module of CRAFT.Both baselines are trained without any staged curricula.To answer Q2, we compare CRAFT with no_curriculum, where we use the final reward functions generated by CRAFT that achieved top-three highest success rate.However, we train the policy without a curriculum and fix the reward function during training.This comparison tests whether a well-designed reward function alone is sufficient for successful policy training, or whether staged curriculum is also necessary.Finally, for Q3, we compare CRAFT with a no_refinement baseline, which removes the VLM-guided reward refinement module (Section IV-D).Instead, no_refinement resamples new reward functions from the reward module, relying on the stochasticity of the LLM to generate effective reward functions.To evaluate the consistency of generated curricula and reward functions, we report the effective curricula ratio.In our experiments, many trials collapse to zero success when the curriculum is poorly structured.Therefore, we define effective curricula as curricula that achieves any nonzero success rate.</p>
<p>B. Simulation Results</p>
<p>Q1: CRAFT can learn successful coordination policies.We provide the success rate curves for policies with top three success rates among 5 experiments in Fig. 6.In all three environments, CRAFT achieves the highest success rate.We emphasize that CRAFT achieves successful policies without expert demonstrations or hand-crafted curricula, reaching maximum success rates of up to 90% in Quadruped Gate, 60% in Quadruped Seesaw, and 100% in Two Arm Lift.In contrast, env_reward achieves a 0% success rate in most experiments, despite having a dense reward function designed by the environment developers.These results underscore both the difficulty of the benchmarks and the effectiveness of our framework.In addition, while exam-ple_reward that was provided to CRAFT also shows less than 10% success rate in every environments, CRAFT designs effective curricula from it and achieves much higher success rate.This demonstrates that curriculum training and reward  searching process of CRAFT enables learning a coordination policy that was challenging for vanilla MARL training.The performance gap is especially evident in the Two Arm Lift task, which requires both long-horizon planning and control over high-dimensional action spaces, highlighting the strength of CRAFT in complex multi-robot settings.</p>
<p>Qualitatively, we observe that policies trained with env_reward and example_reward are susceptible to falling into local optima, resulting in suboptimal behaviors.As shown in Fig. 5, a policy trained with env_reward converges to suboptimal strategies, such as grasping the pot but failing to lift it.This arises because long-horizon tasks require a mixture of multiple reward components for different subtasks, such as reaching the pot, grasping, and lifting it.In such cases, it is difficult to design a reward whose global optimum aligns with the desired behavior.Furthermore, even with a well-designed reward function, the presence of multiple local optima can still lead to suboptimal policies.</p>
<p>Q2: Sequential training with curriculum can achieve success rate beyond vanilla training, though its impact varies by task.Following up to Q1, we aim to answer whether a good reward function alone is sufficient for long-horizon coordination, or whether sequential training of the curriculum is also necessary.Overall, CRAFT achieves higher success rates than no_curriculum across all three environments.The gap is especially large in Quadruped Gate and Two Arm Lift, where staged training yields policies with success rates that no_curriculum cannot match even with the same reward functions.In Quadruped Seesaw, no_curriculum achieves a comparable success rate to CRAFT, but we note that it benefits from the well-designed reward function produced by CRAFT.By contrast, env_reward and exam-ple_reward fail to learn effective coordination in any environment, underscoring the role of CRAFT's reward-refinement loop in producing well-shaped rewards that extend beyond those provided by the environment.</p>
<p>Q3: The VLM-guided reward-refinement loop in CRAFT improves curricula reliability.We report the effective curricula ratio and maximum success rate from multiple runs in Table I.With the VLM-guided reward- refinement, CRAFT achieves higher effective curricula ratio than the no_refinement baseline.This highlights that VLMguided reward-refinement loop contributes to reliable subtask training, leading to consistent generation of effective curricula and corresponding reward functions.We note that if no_refinement generates successful curricula, it also learns a comparable policy, as demonstrated in the maximum success rate in Table I.However, its unreliable curriculum generation strongly degrades its average performance shown in Fig. 6.</p>
<p>C. Hardware Validation</p>
<p>We validate our method by transferring the learned policy to real robots on the Quadruped Gate task.The policy is first trained in simulation using CRAFT and then deployed on hardware.We use one Unitree Go2 and one Unitree Go1 robot.Since the embodiments and low-level MPC controllers differ from in simulation, we applied action component-wise scaling to the policy outputs before deployment.</p>
<p>As shown in Fig. 1 and Fig. 7, our method successfully transfers to the real world.The two quadrupeds exhibit coordinated behavior: the blue robot passes through the gate first while the orange robot waits, then proceeds once the gate is clear.Without any additional fine-tuning, our zeroshot transferred policy achieves a success rate of 65% over 20 runs, serving as a solid baseline given that no further learning is performed after simulation training.</p>
<p>VI. CONCLUSIONS</p>
<p>In this paper, we proposed CRAFT, a framework to train long-horizon multi-robot coordination tasks.We leveraged the reasoning capabilities of foundation models, with LLMs and VLMs acting as a "coach" to decompose complex behaviors into curricula of subtasks, each paired with an LLM-generated reward function.A VLM-guided rewardrefinement loop further improves these rewards, producing semantically rich reward signals that enable successful learning in long-horizon coordination tasks.Through our experiments in multi-quadruped navigation and bimanual manipulation, we demonstrated that our method is effective and also transferable to real hardware.</p>
<p>A key limitation of CRAFT is the variability across trials, as the stochasticity of foundation models can cause occasional failures.However, this variability is not a major obstacle, since we can run multiple training attempts in simulation and deploy only successful policies in real-world.We also view this stochasticity as an advantage since diverse curricula and refinements often yield more creative and effective solutions.Future work can focus on improving reliability in the trials and balancing creativity with stability in foundation-model-driven robot learning.</p>
<p>APPENDIX</p>
<p>A. Prompts</p>
<p>In this section, we provide prompts used in CRAFT.While the details may vary slightly across tasks, the structure remains the same.Full prompts are available at github_link.</p>
<p>1) Curriculum Generation Module (Section IV-A):</p>
<p>We use two prompts which are given to the curriculum LLM.One for generating candidate curricula, and another for refining the final curriculum from the candidates.Prompt 1: LLM prompt for generating curriculum candidates.</p>
<p>You are a curriculum generator trying to generate a curriculum to solve multi-agent reinforcement learning tasks as effectively as possible.Your goal is to write a list of subtasks and corresponding reward function that will help agents to efficiently coordinate for target task.Some helpful tips for generating a curriculum: (1) Do not explore the world by doing random actions and do not try moving to manually defined position.</p>
<p>(2) Try to make the curriculum simple.Do not generate more than 5 tasks.</p>
<p>(3) The last task of your curriculum should align with the final goal of the environment.(4) In curriculum, you cannot chage the environment or the terminating condition.Focus on the task description.</p>
<p>(5) You are learning a centralized policy that have access to every agents.</p>
<p>You will be given several variables you can use to describe each subtask in curriculum.Each task should be described using the given variables and do not introduce new variables.Prompt 2: LLM prompt for refining curriculum candidates.</p>
<p>You are a curriculum generator trying to generate a curriculum to solve multi-agent reinforcement learning tasks as effectively as possible.Your goal is to write a list of subtasks and corresponding reward function that will help agents to efficiently coordinate for target task.</p>
<p>You will be given three candidates of curriculums that were generated by other generators.Pick the best version and refine it if needed.Note that the maximum possible curriculum is 5.</p>
<p>You will be given several variables you can use to describe each subtask in curriculum.Each task should be described using the given variables and do not introduce new variables.You should write reward function for given task using useful variables from the environment.Your reward function is part of the curriculum learning which consists of learning sequence of different tasks.You will be given description of past tasks, reward code for past tasks, and current task descriptiion.</p>
<p>Your gpt_reward(self) function's output should consist of three items (1) the total reward, an integer (2) a dictionary of each individual reward component where keys are names of reward component and items are each rewards (3) the maximum possible reward that is used to normalize the reward, an integer Some helpful tips for writing a reward function code:</p>
<p>(1) Use numpy or scipy functions to write the reward function.numpy, scipy is already imported as np and scipy.Do not import additional library.</p>
<p>(2) You may put higher weight on the reward for current task, but also include rewards from past tasks to avoid forgetting.3) Policy Evaluation Module (Section IV-C): Below is the prompt given to the evaluation VLM for evaluating a trained policy.</p>
<p>Prompt 4: VLM prompt for policy evaluation.</p>
<p>You are a multi-agent reinforcement learning engineer asked to evaluate a rollout from a task in a curriculum.</p>
<p>You will be provided with: -Snapshot images from the rollout -Trajectory data (state sequences) -Previous and current task descriptions Your job is to determine: 1.Whether the agents achieved the current goal.Current task is: &lt;<Current task>&gt; Former tasks that RL succeeded to learn is: &lt;<Former tasks>&gt; Given images are snapshots in chronological order of the rollout.</p>
<p>Trajectory data: &lt;<Trajectory>&gt;</p>
<p>Note that the distance and orientation is considered as "very close" if the value is smaller than 0.03 (This is NOT a threshold value, this is given to have a sense of how close they are) Note that the trajectory data is sequential data for a whole episode.</p>
<p>Make your decision on both the trajectory data and the image.</p>
<p>4) Reward Refinement Module (Section IV-D):</p>
<p>We use two prompts.One for generating advice on how to refine the reward and one for refining the reward function given the advice (LLM).</p>
<p>Prompt 5: VLM prompt for generating reward refinement advice.</p>
<p>You are a multi agent reinforcement learning engineer trying to refine a reward function for a given task.You will be given: the reward function that was tried and failed, the experimental results, a description of why it failed, and the reward-component curve images.</p>
<p>First, analyze the reason why the reward function failed to learn the task.Then, provide advice on how to improve or modify the reward function to achieve the task.</p>
<p>These are some tips in giving advice: (1) Do not give more than three advice.</p>
<p>(2) You cannot add a penalty in the reward function, e.g.no reward can be a negative value.</p>
<p>These are the only functions that are allowed to used.&lt;<Helper_Functions>&gt; Information that cannot be obtained from the above functions cannot be used in the reward function.</p>
<p>Current task is: &lt;<Task_Name>&gt; &lt;<Task_Description>&gt; &lt;<Task_Reason>&gt; Reward function that was tried: &lt;<Reward_Function>&gt; Failure reason: &lt;<Failure_Reason>&gt; Do not give an advice on the curriculum of the reward.Just consider how to improve the reward function given the current task.Do not provide the whole code of the reward function, just give advice on how to improve it.</p>
<p>Prompt 6: LLM prompt for refining a reward function using advice.</p>
<p>You are a multi agent reinforcement learning engineer trying to refine a reward function for the given task.The reward function is part of the curriculum learning which consists of learning sequence of different tasks.You will be given description of past tasks, a reward function that was tried and failed, and an advice on how to improve it.Follow the advice and provide a new reward function that should be able to achieve the given task.Do not repeat the advice or the old reward function in your answer.</p>
<p>Your gpt_reward(self) function's output should consist of two items (1) the total reward, an integer (2) a dictionary of each individual reward component where keys are names of reward component and items are each rewards (3) the maximum possible reward that is used to normalize the reward, an integer Some helpful tips for writing a reward function code:</p>
<p>(1) Use numpy or scipy functions to write the reward function.numpy, scipy is already imported as np and scipy.Do not import additional library.</p>
<p>(2) You may put higher weight on the reward for current task, but also include rewards from past tasks to avoid forgetting.So not all the advice might be applicable.</p>
<p>5) Task-Specific Substitutions for Two Arm Lift: Below are task-specific substitutions (e.g., environment details and task descriptions) that are inserted into the general prompts above.</p>
<p>Fig. 1 :
1
Fig. 1: (left) Overview of CRAFT.Given a natural language description of the target task, CRAFT generates a curriculum C = [M1, . . ., MK ] of subtasks that progressively build coordination towards the target task.For each subtask M k , a reward LLM generates a reward function R jk and a decentralized policy π j k , shared among agents, is trained via MARL.A VLM evaluates this policy using state variable trajectories and snapshot images.If the policy fails, an advice VLM provides advice from reward component learning curves, plots of individual reward terms during training.The reward LLM incorporates this advice to generate a revised reward.Successful policy advances to the next subtask and this iterative process continues until the final task is reached.(Right) Demonstration of the Quadruped Gate task, where two quadrupeds coordinate to pass through a narrow gate without collision: the blue robot passes first while the orange robot waits; then, the orange robot proceeds once the gate is clear.This policy was learned in simulation and transferred to real-world.</p>
<p>Task 1 :Task 3 :
13
Position and AlignEach robot arm should minimize the Euclidean distance to its respective handle and orient its endeffector to match the handle's orientation accurately.Task 2: Synchronize Grip … Coordinated Lift and LevelingBoth robot arms must collaboratively lift the pot to a minimum height of 0.1m, maintaining the pot's tilt within an acceptable range (cosine of tilt angle ≥ cos(30 degrees)).</p>
<p>Task 1 :Task 3 :Task 4 :
134
Approach HandlesEach robot arm minimizes its Euclidean distance to its respective target handle.Task 2: Grasp Handles … Initial Lift and BalanceCollaboratively elevate the pot to a height above 0.05m while maintaining minimal pot tilt (cosine of tilt angle close to 1).Final Lift and Stabilize Elevate the pot beyond 0.1m while ensuring the pot remains within the specified tilt limit (no more tilted than 30 degrees).</p>
<p>Task 1 :Task 3 :
13
Position and AlignEach robot arm should minimize the Euclidean distance to its respective handle and orient its endeffector to match the handle's orientation accurately.Task 2: Synchronize Grip … Initial Lift and StabilizeCollaboratively elevate the pot to a height above 0.05m while keeping the pot tilt within the acceptable range (cosine ≥ cos(30°)).</p>
<p>Task 4 :
4
Coordinated Final LiftElevate the pot beyond 0.1m while ensuring the pot remains within the specified tilt limit (no more tilted than 30 degrees).</p>
<p>Fig. 2 :
2
Fig. 2: Example of curriculum refinement for task lift and balance the pot.Three different candidate curricula C 1 to C 3 , generated by the curriculum LLM, are re-provided to the LLM for refinement.In C 1 , Task 1 focuses only on minimizing distance, while Task 1 in C 3 is defined as minimizing distance and matching orientation.In contrast, Task 3 and Task 4 in C 1 break down the lifting into two stages of first lifting halfway and then to a full height, whereas C 3 represents lifting as a single task.The curriculum LLM merges these candidates into a final curriculum C by selecting the stronger tasks definitions from each candidate.</p>
<p>Evaluation Refine reward function … 1 .
1
Boost the relative weight or reshape the lift signal.</p>
<p>Advice Curriculum 1 .
1
Fig. 3: Example of reward refinement of subtask Coordinate Preliminary Lift.Through the first reward-refinement loop, R 1k=3 was produced and the evaluation VLM marked the policy as a failure since the pot never reached the required elevation of 0.05 m.The reward component learning curves were then passed to the advice VLM, which identified that lift_reward was too weak compared to balance_reward.It recommended removing the square on elevation, increasing the lift weight, and decreasing the balance weight.The revised reward R 2 k=3 reflects these changes: the square on elevation was removed, the lift weight increased from 80 to 200, and the balance weight decreased from 2 to 1.With this reward, the policy successfully achieved the 0.05 m elevation and satisfied the subtask.</p>
<p>Fig. 4 :
4
Fig. 4: Illustrative snapshot showing successful execution of multiagent coordination tasks by a CRAFT-trained policy.</p>
<p>j k and advice generated by the VLM.The refine LLM then produces a new refined reward R j+1 k .Once a new refined reward R j+1 k is generated, we train a corresponding policy π j+1 k and iterate through the</p>
<p>Fig. 5 :
5
Fig. 5: Illustrative snapshot of policies trained with env_reward without curriculum.The policy shows suboptimal behaviors, such as only one agent passing the gate or only managed to grasp the pot, rather than lifting it.</p>
<p>Fig. 6 :
6
Fig.6: Success Rate of the top-three curricula from each environment.Each environment is evaluated by 100 random initial conditions.CRAFT achieves highest success rate on every environment, demonstrating its ability to learn complex coordination tasks that are challenging to learn without curriculum or well-crafted reward functions.</p>
<p>Fig. 7 :
7
Fig. 7: Trajectory of quadrupeds in our hardware experiments for Quadruped Gate policy trained with CRAFT.Two quadrupeds exhibit coordination behaviors in real world and manage to pass the gate sequentially without collision.</p>
<p>names, descriptions, and reasons in the format.&lt;<Environment_Description>&gt; Varaibles you can use for curriculum descriptions are &lt;<State_Variables>&gt; These are the rules that you have to consider when writing a curriculum (1) You cannot change start position of robots and objects.(2) You cannot change the number of robots and objects.(3) You should consider all robots in the environment.</p>
<p>2 )
2
out the names, descriptions, and reasons in the format.&lt;<Environment_Description>&gt; Varaibles you can use for curriculum descriptions are &lt;<State_Variables>&gt; These are the rules that you have to consider when writing a curriculum (1) You cannot change start position of robots and objects.(2) You cannot change the number of robots and objects.(3) You should consider all robots in the environment.Three candidates of curriculums are: Reward Function Generation Module (Section IV-B): Below is the prompt given to the reward generation LLM for generating a base reward function.Prompt 3: LLM prompt for generating base reward function.</p>
<p>( 3 )
3
Total reward must stay within [0, 1].Always, scale the reward to have maximum 1. (4) Only use provided variables.(5) Do not change other predefined parts in the code.&lt;<Environment_Description>&gt; You can use help functions to calculate reward.&lt;<Helper_Functions>&gt; Example reward function for the task is &lt;<Example_Reward>&gt; You should re-implement reward function to assign a proper reward function for given task.You can change the reward components that aligns well with the task.Do not include any new inputs in the functions.Note that you are designing a reward function for team, not a single agent.Generate a reward function code and command for Task Name</p>
<p>2 .Fill
2
Whether they have forgotten earlier subtasks.out the [] brackets in the given format.(Don't keep the [] brackets)</p>
<p>( 3 )
3
Total reward must stay within [0, 1].Always, scale the reward to have maximum 1. (4) Only use provided variables.(5) Do not change other predefined parts in the code.You can use help functions to calculate reward.&lt;<Helper_Functions>&gt; Current task is: &lt;<Current_Task>&gt; Former tasks that RL succeeded to learn is: &lt;<Former_Tasks>&gt; Reward function that was tried: &lt;<Reward_Function>&gt; Advice on how to improve the reward function: &lt;<Advice>&gt; Do not deviate from the reward structure given in the example.Note that the advice might not acount for what information it can use in the reward function.</p>
<p>TABLE I :
I
Effective curricula ratio indicates how many curricula achieved a nonzero success rate across 5 experiments.Maximum success rate refers to the success rate of the best policy obtained among the 5 experiments.CRAFT generates more effective curricula than the no_refinement baseline, demonstrating its ability to consistently generate successful curricula.Moreover, the higher maximum success rate shows that the reward refinement loop further contributes to generating better curricula.</p>
<p>ACKNOWLEDGMENTThe authors are grateful to Yuman Gao, Prof. Koushil Sreenath and the Hybrid Robotics Lab for their support on the hardware experiments.Prompt 7: «Environment_Description» for Two Arm Lift.A large pot with two handles is placed on a table top.The pot is rectangular shaped with U-shaped handles on the opposite side of the pot rim.The grip of the handle is level, parallel to the tabletop.Two robot arms are placed on opposite ends of the table.The two robot arms must each grab a handle and lift the pot together, above a certain height (0.1m), while keeping the pot level (no more tilted than 30 degrees).The pot location is randomized at the beginning of each episode.Prompt 8: «State_Variables» for Two Arm Lift.-Euclidean distance from each robot's end-effector and its target handle -Orientation difference between each robot's end-effector and its target handle -Gripper state of each robot (whether it grasped the handle or not) -Pot elevation above the table -Pot tilt as cosine of the angle between its normal and the vertical Prompt 9: «Helper_Functions» for Two Arm Lift.self._get_check_grasps(): returns two boolean value for each arm whether or not it's grasping the handle or not.self._get_pot_elevation():returns the distance (in meters) from pot's bottom to the table top self._get_tilt_degree():returns the cosine value of the tilted degree of the pot.The tilted degree is the difference between the z axis and pot's normal vector self._get_gripper_to_handle_distance():returns two float value of distance from each robot's end-effector to each handle self._get_gripper_to_handle_rotation():returns two rotation difference (calcualated with R1.T @ R2) between each robot's end-effector to each handle with shape : (3,3) each self._check_success():returns True if the pot is higher than 0.1B. CRAFT ExamplesThis section presents example outputs from a successful trial of the Two Arm Lift task, including curricula, generated rewards, policy evaluations, advice, and refined rewards.Output 1: Example final curriculum generated for Two Arm LiftTask 1 Name: Arm Alignment Task Description: The task focuses on minimizing the Euclidean distance from each robot's end-effector to its respective target handle.The reward will be based on the decreasing distance between each robot's end-effector and its assigned handle.Reason: Establishing an initial alignment of the robot arms with the handles will set the foundation for successful grasping.This step helps the robots learn coordinated movement towards the handle, which is essential for synchronized actions in subsequent tasks.Task 2 Name: Grasp Coordination Task Description: The goal is for both robot arms to match the orientation of their end-effectors with the handles and change the gripper state to the grasped condition.The reward will increase when both arms have reduced orientation differences and both successfully grasp their handles.Reason: By focusing on grasp coordination, this task trains the robots to orient themselves correctly and clamp securely onto the pot's handles.Achieving a secure grasp is crucial for preventing slippage and ensuring stable lifting in future tasks.Task 3 Name: Initial Lifting Task Description: Once both robot arms have securely grasped the handles, they must elevate the pot slightly above the table (e.g., 0.05m) without exceeding a 30degree tilt as measured by the pot tilt variable.Rewards will be given for maintaining a level pot during the lift.Reason: This task introduces the concept of collaboration in lifting, emphasizing control over the elevation while managing pot stability.It sets the precedence for maintaining balance and coordination when further lifting in subsequent tasks.Task 4 Name: Controlled Elevation Task Description: In this task, the robot arms must lift the pot above the required height of 0.1m while ensuring that the pot's tilt remains within the 30-degree threshold.The reward structure will be based on the pot elevation variable while maintaining minimal tilt.Reason: Taking the learning from the previous tasks, this task requires the robots to execute coordinated actions for a complete lift, focusing on both height and tilt.This task aligns with the target goal, ensuring that the robots can perform the full lift in a controlled manner.Task 5 Name: Synchronized Full Lift Task Description: The task requires the robots to synthesize all prior skills to lift the pot above 0.1m in a seamless motion, keeping the tilt within the desired limits and demonstrating a complete coordinated effort.The reward is given for achieving all aspects of the task simultaneously.Reason: This final task ensures that the robots can integrate all learned experiences to accomplish the target task in a real-world scenario, demonstrating full competence in synchronized lifting operations."proximity_reward": proximity_reward, "alignment_reward": alignment_reward, "grasp_reward": grasp_reward, "controlled_elevation_reward": controlled_elevation_reward, "stability_reward": stability_reward, "cooperation_reward": cooperation_reward, } return self.gpt_rew,self.gpt_rew_dict,max_reward Output 3: Example policy evaluation for Two Arm Lift.The policy was trained with reward R 0 k=4 given above.Decision: FailureReason:-The pot elevation peaked at only ~0.016 m, well below the required 0.1 m lift height.-Although pot tilt remained within the 30 degree threshold, the current task's primary elevation goal was not met.-Gripper1 never achieved a stable grasp on its handle, and gripper0 only grasped sporadically (34/790 steps), indicating a failure to retain reliable grasp coordination.-Both arms drifted away from their handles after initial approach (distances never fell below 0.03 m), showing degradation in arm alignment learned in earlier tasks.Output 4: Example advice for Two Arm Lift.The advice was generated for the reward R 0 k=4 given above.Analysis of failure 1. Sparse elevation feedback: Controlled elevation and stability rewards only kick in once elevation &gt;= 0.1 m and tilt within threshold.Because the agents rarely reach that threshold early on, they get almost zero signal to lift beyond ~0.016 m. 2. Binary tilt gating: direction_coef zeroes out any elevation reward if tilt is even slightly over 30 degree, so there is no gradient to correct small tilt errors during the lift.3. Incentives decoupled from lift: Proximity, alignment, grasp and cooperation rewards are largely satisfied independently of successful coordinated lifting.Once the handles are touched or grasped, there is little extra push to actually raise the pot.Three targeted improvements 1. Continuous elevation shaping -Replace the hard step for elevation with a smooth, monotonically increasing reward -for example r_elev = min(elevation / threshold, 1.0) or a quadratic/ sigmoid on elevation.This gives gradient toward any increase in height, not just surpassing 0.1 m. 2. Tilt-aware bonus rather than gate -Instead of direction_coef={0,1}, use a continuous tilt bonus such as r_tilt = (cos_z -cos_max) / (1 -cos_max), clipped to [0,1], and multiply it with the elevation reward.This still rewards low tilt but provides a gradient that gently pushes the pot back toward upright whenever it begins to tilt.3. Tighten coupling of grasp, proximity, and lift -Multiply (or weight more heavily) your proximity/alignment rewards by the current elevation factor (e.g.r_prox x r_elev).That way, the agents only obtain full proximity/alignment credit if they hold on while actually lifting.It discourages "touch and release" and encourages maintaining contact through the elevation phase.Code 3: Example refined reward code for Two Arm Lift.Reward R 1 k=4 was refined from R 0 k=4 using the advice above."proximity_reward": proximity_reward, "alignment_reward": alignment_reward, "grasp_reward": grasp_reward, "controlled_elevation_reward": controlled_elevation_reward, "stability_reward": stability_reward, "cooperation_reward": cooperation_reward, } return self.gpt_rew,self.gpt_rew_dict,max_reward
Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, nature. 57577822019</p>
<p>Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. B Ellis, J Cook, S Moalla, M Samvelyan, M Sun, A Mahajan, J Foerster, S Whiteson, Advances in Neural Information Processing Systems. 202336</p>
<p>Google research football: A novel reinforcement learning environment. K Kurach, A Raichuk, P Stańczyk, M Zaj Ąc, O Bachem, L Espeholt, C Riquelme, D Vincent, M Michalski, O Bousquet, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Toward real-world cooperative and competitive soccer with quadrupedal robot teams. Z Su, Y Gao, E Lukas, Y Li, J Cai, F Tulbah, F Gao, C Yu, arXiv:2505.138342025arXiv preprint</p>
<p>Marladona-towards cooperative team play using multi-agent reinforcement learning. Z Li, F Bjelonic, V Klemm, M Hutter, 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE2025</p>
<p>Leveraging large language models for effective and explainable multi-agent credit assignment. K Nagpal, D Dong, N Mehr, Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems. the 24th International Conference on Autonomous Agents and Multiagent Systems2025</p>
<p>Variational automatic curriculum learning for sparse-reward cooperative multi-agent problems. J Chen, Y Zhang, Y Xu, H Ma, H Yang, J Song, Y Wang, Y Wu, Advances in Neural Information Processing Systems. 202134</p>
<p>Automatic curriculum learning for deep rl: a short survey. R Portelas, C Colas, L Weng, K Hofmann, P.-Y Oudeyer, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Cooperative multiagent control using deep reinforcement learning. J K Gupta, M Egorov, M Kochenderfer, International conference on autonomous agents and multiagent systems. Springer2017</p>
<p>A survey on curriculum learning. X Wang, Y Chen, W Zhu, IEEE transactions on pattern analysis and machine intelligence. 202144</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, Transactions on Machine Learning Research. 2024</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Eureka: Human-level reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Vision-language models are zero-shot reward models for reinforcement learning. J Rocamonde, V Montesinos, E Nava, E Perez, D Lindner, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Autoeval: Autonomous evaluation of generalist robot manipulation policies in the real world. Z Zhou, P Atreya, Y L Tan, K Pertsch, S Levine, arXiv:2503.242782025arXiv preprint</p>
<p>Aha: A visionlanguage-model for detecting and reasoning over failures in robotic manipulation. J Duan, W Pumacay, N Kumar, Y R Wang, S Tian, W Yuan, R Krishna, D Fox, A Mandlekar, Y Guo, arXiv:2410.003712024arXiv preprint</p>
<p>Reverse forward curriculum learning for extreme sample and demo efficiency. S Tao, A Shukla, T Chan, H Su, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Unsupervised curricula for visual meta-reinforcement learning. A Jabri, K Hsu, A Gupta, B Eysenbach, S Levine, C Finn, Advances in Neural Information Processing Systems. 201932</p>
<p>Tizero: Mastering multi-agent football with curriculum learning and self-play. F Lin, S Huang, T Pearce, W Chen, W.-W Tu, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. the 2023 International Conference on Autonomous Agents and Multiagent Systems2023</p>
<p>Curricullm: Automatic task curricula design for learning complex robot skills using large language models. K Ryu, Q Liao, Z Li, P Delgosha, K Sreenath, N Mehr, 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE2025</p>
<p>Environment curriculum generation via large language models. W Liang, S Wang, H.-J Wang, O Bastani, D Jayaraman, Y J Ma, 8th Annual Conference on Robot Learning. 2024</p>
<p>Aura: Agentic upskilling via reinforced abstractions. A Zhu, Y Tanaka, D Hong, arXiv:2506.025072025arXiv preprint</p>
<p>Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics. J Song, Z Zhou, J Liu, C Fang, Z Shu, L Ma, arXiv:2309.066872023arXiv preprint</p>
<p>Learning a high-quality robotic wiping policy using systematic reward analysis and visual-language model based curriculum. Y Liu, D Kang, S Ha, arXiv:2502.125992025arXiv preprint</p>
<p>Learning multi-agent loco-manipulation for long-horizon quadrupedal pushing. Y Feng, C Hong, Y Niu, S Liu, Y Yang, D Zhao, 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE2025</p>
<p>Decentralized navigation of a cable-towed load using quadrupedal robot team via marl. W.-T Chen, M Nguyen, Z Li, G N Sue, K Sreenath, arXiv:2503.182212025arXiv preprint</p>
<p>Resolving conflicting constraints in multi-agent reinforcement learning with layered safety. J J Choi, J J Aloor, J Li, M G Mendoza, H Balakrishnan, C J Tomlin, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsLos Angeles, USAJune 2025</p>
<p>Learning differentiable and safe multi-robot control for generalization to novel environments using control barrier functions. V Sharma, N Mehr, N Hovakimyan, 2024 IEEE 63rd Conference on Decision and Control (CDC). IEEE2024</p>
<p>The surprising effectiveness of ppo in cooperative multi-agent games. C Yu, A Velu, E Vinitsky, J Gao, Y Wang, A Bayen, Y Wu, Advances in neural information processing systems. 202235</p>
<p>Monotonic value function factorisation for deep multiagent reinforcement learning. T Rashid, M Samvelyan, C S De Witt, G Farquhar, J Foerster, S Whiteson, Journal of Machine Learning Research. 211782020</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Sadigh, L Guibas, F Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>3d-vla: A 3d vision-language-action generative world model. H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, International Conference on Machine Learning. PMLR2024</p>
<p>Synthesizing interpretable control policies through large language model guided search. C Bosio, M W Mueller, 2025 American Control Conference (ACC). IEEE2025</p>
<p>Large language model based multi-agents: a survey of progress and challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent llm agents. Y Talebirad, A Nadiri, arXiv:2306.033142023arXiv preprint</p>
<p>Loss of plasticity in continual deep reinforcement learning. Z Abbas, R Zhao, J Modayil, A White, M C Machado, Conference on lifelong learning agents. PMLR2023</p>
<p>Loss of plasticity in deep continual learning. S Dohare, J F Hernandez-Garcia, Q Lan, P Rahman, A R Mahmood, R S Sutton, Nature. 63280262024</p>
<p>Mqe: Unleashing the power of interaction with multi-agent quadruped environment. Z Xiong, B Chen, S Huang, W.-W Tu, Z He, Y Gao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, A Joshi, S Nasiriany, Y Zhu, K Lin, arXiv:2009.122932020</p>
<p>Openrl: A unified reinforcement learning framework. S Huang, W Chen, Y Sun, F Bie, W.-W Tu, arXiv:2312.161892023arXiv preprint</p>
<p>skrl: Modular and flexible library for reinforcement learning. A Serrano-Muñoz, D Chrysostomou, S Bøgh, N Arana-Arexolaleiba, Journal of Machine Learning Research. 242542023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7434 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7434</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7434</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-265019477</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.01767v2.pdf" target="_blank">PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion</a></p>
                <p><strong>Paper Abstract:</strong> Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6\% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at \url{https://github.com/gydpku/PPTC}.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7434.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7434.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>turn_vs_session</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turn-based versus Session-based Prompt/Problem Presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two different evaluation/prompt formats: (1) turn-based where each turn is evaluated assuming previous turns were correctly completed (previous turns' correct API sequences and label PPT content provided), and (2) session-based where the model must sequentially generate APIs and the resulting predicted PPT from prior turns is used as context; errors accumulate. The presentation format strongly affects measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (reported); evaluated across multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive/transformer conversational LLM (GPT-4) accessed via Azure OpenAI chat completion API; other evaluated models include ChatGPT, Text-Davinci-003, LLaMa-2 variants, WizardLM, Vicuna, Baichuan.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC PowerPoint Task Completion (creating new PPT / editing PPT template)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate sequences of PowerPoint API calls to create or edit PPT files to satisfy user instructions in multi-turn dialogues; evaluated both per-turn (assuming prior turns correct) and across full sessions (using model outputs as prior context).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Turn-based prompt (previous turns' correct API sequences and label PPT used) vs Session-based prompt (previous turns' LLM-generated APIs and predicted PPT used).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / conversation context</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Turn-based: input includes current instruction, dialogue history with feasible API sequences, and parsed label PPT content from previous turns (assumes perfect prior execution). Session-based: same prompt structure but previous turns' outputs are the LLM's own API sequences and the parsed prediction PPT file (so errors accumulate); both use the same inference prompt template, API reference file, and parsed PPT content. Temperature = 0, max_tokens = 2048 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based accuracy and session-based accuracy (proportion of correctly completed turns / sessions according to PPTX-Match evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4: creating-new task: 75.1% turn-based accuracy vs 22.7% session-based accuracy; editing-template task: 38.1% turn-based accuracy vs 6.0% session-based accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>For turn-based evaluation the contrasting format is session-based evaluation (and vice versa): creating-new baseline (session) = 22.7% accuracy; editing baseline (session) = 6.0% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Creating-new: -52.4 percentage points absolute (75.1% -> 22.7%); Editing-template: -32.1 percentage points absolute (38.1% -> 6.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same inference prompt template used; turn-based evaluation provided feasible API sequences and label PPT content for previous turns; session-based used model outputs and predicted PPTs. Temperature=0, max_tokens=2048.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7434.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7434.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>history_ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dialogue History Presence versus Removal (Ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation testing of including previous-turn dialogue history (previous instructions with feasible API sequences) in the prompt versus removing it; history functions similarly to few-shot demonstrations and affects performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 accessed via chat completion API, temperature 0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC Turn-based evaluation (creating new PPT / editing PPT template)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate API sequence for current turn given prompt inputs; ablation removes dialogue history from prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction prompt with or without dialogue history (previous turns + feasible API sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>With history: prompt includes previous turns and their feasible API sequences and parsed PPT content (acts as few-shot examples). Without history: these elements removed. All other prompt elements unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4: creating-new task drops from 75.1% to 73.1% (absolute -2.0 pp) when history removed; editing-template task decreases by 6.2% (absolute; reported decrease).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>With history: 75.1% (creating-new), 38.1% (editing-template).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Creating-new: -2.0 percentage points absolute; Editing-template: -6.2 percentage points absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Turn-based evaluation; removal of dialogue history only; other prompt components (API file, PPT content, task instruction) retained. Temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7434.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7434.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cot_prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (Zero-shot-CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Inserting the phrase 'Let's think step by step' (zero-shot CoT) into the prompt to elicit intermediate reasoning steps, evaluated as a prompt-style modification to improve planning for multi-step API generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 chat model; prompted with CoT cue in addition to standard inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC Turn-based / Session-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate API sequences for PPT operations; CoT prompt elicits intermediate decomposition/step-by-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-thought style prompt modification (natural-language cue 'Let's think step by step').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / reasoning prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot-CoT: append 'Let's think step by step' after dialogue history to encourage the model to produce intermediate steps before final API sequence. Otherwise same inference prompt. No additional demonstration examples used for CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based accuracy (and session-based accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4: creating-new task: baseline 75.1% -> CoT 77.0% (approx +1.9 pp); editing-template task: baseline 38.1% -> CoT 40.6% (approx +2.5 pp). Session-based improvements small (~+0.4 pp for creating).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline GPT-4 (no CoT): 75.1% (creating), 38.1% (editing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Creating-new: +1.9 percentage points absolute; Editing-template: +2.5 percentage points absolute; session improvements minor.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CoT implemented by adding the sentence 'Let's think step by step' after dialogue history in the prompt. Temperature=0, max_tokens=2048.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7434.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7434.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>tot_prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought (ToT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying the Tree-of-Thought algorithm to allow tree-like branching of intermediate reasoning states (ToT) as a more complex planning prompt strategy; compared to zero-shot CoT and baseline prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 with ToT control loop (official ToT code) to generate and evaluate multiple reasoning 'thoughts' per step.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC Turn-based / Session-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate API sequences for PPT tasks using Tree-of-Thought planning to explore multiple reasoning branches.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Tree-of-Thought algorithmic prompting (multiple candidate thought branches, voting/evaluation), integrated with the standard inference prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / search-based planning</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>ToT implemented following official code; it generates, evaluates, and selects among multiple thought branches. ToT dramatically increases token cost (reported 5–10x) compared to Zero-shot-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based accuracy (and token cost)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4: creating-new task: baseline 75.1% -> ToT 76.5% (approx +1.4 pp); editing-template task: baseline 38.1% -> ToT ~40.6% (approx +2.5 pp). ToT did not outperform Zero-shot-CoT and used significantly more tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline GPT-4 (no ToT): 75.1% creating; 38.1% editing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Creating-new: +1.4 percentage points absolute vs baseline; ToT was worse or similar to CoT despite 5–10x token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ToT run using official implementation; token cost 5–10x higher than CoT. Temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7434.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7434.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>content_selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Content Selection (Prompt Filtering of PPT Content)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-time selection algorithm where the LLM (with three demonstration examples) is first prompted to select only relevant shapes from the parsed PPT file; the generated API sequence prompt then contains only the selected shapes instead of the whole PPT content, reducing irrelevant context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used twice in pipeline: once to select relevant PPT shapes (with 3 demonstration examples), then to generate the API sequence using the filtered PPT content.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC Turn-based / Session-based evaluation (primarily turn-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate API sequences to accomplish PPT instructions; content selection reduces prompt size and filters irrelevant objects prior to API generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two-stage prompting: (1) selection prompt with 3 demonstrations to pick relevant PPT shapes; (2) main API-generation prompt that includes only the selected shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt engineering (context filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Content Selection prompt includes three demonstration examples guiding selection; replaces full parsed PPT content in the main prompt with only the selected shapes. This can increase or decrease token usage depending on selection behavior (reported tokens: creating task input tokens increased in one run due to selection representation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based accuracy; avg token cost</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4: creating-new task: baseline 75.1% -> content selection 77.5% (+2.4 pp); editing-template task: baseline 38.1% -> content selection 43.1% (+5.0 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline GPT-4 without content selection: 75.1% creating; 38.1% editing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Creating-new: +2.4 percentage points absolute; Editing-template: +5.0 percentage points absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Content selection used 3 demonstration examples in the selection prompt; selection replaces PPT content in main prompt. Token counts reported (creating: avg tokens rose to 3.4k from 2.9k in this experiment; editing: token usage and accuracy improved).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7434.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7434.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>api_selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>API Selection (Embedding-based Top-k API Inclusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Select the most relevant APIs to include in the prompt by computing cosine similarity between instruction and API description embeddings and keeping the top-k (k=15) APIs, reducing prompt verbosity and API hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used to generate API sequences after the reference API file is pruned by embedding-similarity selection (text-embedding-ada-002 used to compute embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC Turn-based / Session-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate API sequences to perform PPT operations; only top-k (k=15) APIs by embedding similarity are included in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt content modification: replace full API reference list with top-k APIs ranked by embedding cosine similarity to current instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt content / API context filtering</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Embeddings from text-embedding-ada-002; cosine similarity ranking; k set to 15; reduces number of APIs in prompt and average token count.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based accuracy; avg token cost</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4: creating-new task: baseline 75.1% -> API selection 76.4% (+1.3 pp). API selection also reduced average token cost (reported avg tokens decreased from ~2.9k to ~1.5k in one experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline GPT-4 with full API list: 75.1% (creating).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Creating-new: +1.3 percentage points absolute; token cost reduction substantial in reported measures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Embedding similarity using text-embedding-ada-002; top k=15 selected. Temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7434.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7434.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>long_input_token_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long PPT Template / Token Limit and Truncation Effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The format/length of the PPT environment (long templates) and model token limits affect LLM performance; long or complex PPT content leads to lower accuracy, and token truncation for models with small context windows causes information loss and degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and open-source LLMs (LLaMa-2 variants, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 with large token limit (used higher token input), open-source models typically had 2–4k token limits leading to truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PPTC Editing PPT Template task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Editing operations on long, complex PPT templates requiring understanding of extensive slide content; prompts include parsed PPT file content which can be lengthy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Long parsed PPT content included in prompt; when input exceeds token limit, PPT content is truncated before prompting the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt length</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>When token number exceeds model limits, authors cut PPT file content to reduce prompt size. GPT-4 had higher token expense when editing templates (allowed more content) but still performed poorly; open-source models' token limits (2–4K) often required truncation, causing information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>turn-based and session-based accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4 editing-template turn-based accuracy: 38.1% (baseline). With content selection (to reduce irrelevant content) accuracy rose to 43.1%. Session-based accuracy remained near zero for editing-task across models (GPT-4 session-based 6.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline (no content selection, long PPT content included): GPT-4 38.1% turn-based accuracy on editing task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Using content selection to reduce prompt length: +5.0 percentage points absolute (38.1% -> 43.1%) on editing task for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>When prompt tokens exceed limit, authors truncate PPT content; content selection used as a mitigation. Temperature=0; max tokens reported 2048 used for completion calls and larger contexts used where allowed by model.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Gorilla: Large language model connected with massive apis <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating large language models to master 16000+ real-world apis <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7434",
    "paper_id": "paper-265019477",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "turn_vs_session",
            "name_full": "Turn-based versus Session-based Prompt/Problem Presentation",
            "brief_description": "Two different evaluation/prompt formats: (1) turn-based where each turn is evaluated assuming previous turns were correctly completed (previous turns' correct API sequences and label PPT content provided), and (2) session-based where the model must sequentially generate APIs and the resulting predicted PPT from prior turns is used as context; errors accumulate. The presentation format strongly affects measured performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (reported); evaluated across multiple LLMs",
            "model_description": "Large autoregressive/transformer conversational LLM (GPT-4) accessed via Azure OpenAI chat completion API; other evaluated models include ChatGPT, Text-Davinci-003, LLaMa-2 variants, WizardLM, Vicuna, Baichuan.",
            "model_size": null,
            "task_name": "PPTC PowerPoint Task Completion (creating new PPT / editing PPT template)",
            "task_description": "Generate sequences of PowerPoint API calls to create or edit PPT files to satisfy user instructions in multi-turn dialogues; evaluated both per-turn (assuming prior turns correct) and across full sessions (using model outputs as prior context).",
            "problem_format": "Turn-based prompt (previous turns' correct API sequences and label PPT used) vs Session-based prompt (previous turns' LLM-generated APIs and predicted PPT used).",
            "format_category": "prompt style / conversation context",
            "format_details": "Turn-based: input includes current instruction, dialogue history with feasible API sequences, and parsed label PPT content from previous turns (assumes perfect prior execution). Session-based: same prompt structure but previous turns' outputs are the LLM's own API sequences and the parsed prediction PPT file (so errors accumulate); both use the same inference prompt template, API reference file, and parsed PPT content. Temperature = 0, max_tokens = 2048 in experiments.",
            "performance_metric": "turn-based accuracy and session-based accuracy (proportion of correctly completed turns / sessions according to PPTX-Match evaluation)",
            "performance_value": "GPT-4: creating-new task: 75.1% turn-based accuracy vs 22.7% session-based accuracy; editing-template task: 38.1% turn-based accuracy vs 6.0% session-based accuracy.",
            "baseline_performance": "For turn-based evaluation the contrasting format is session-based evaluation (and vice versa): creating-new baseline (session) = 22.7% accuracy; editing baseline (session) = 6.0% accuracy.",
            "performance_change": "Creating-new: -52.4 percentage points absolute (75.1% -&gt; 22.7%); Editing-template: -32.1 percentage points absolute (38.1% -&gt; 6.0%).",
            "experimental_setting": "Same inference prompt template used; turn-based evaluation provided feasible API sequences and label PPT content for previous turns; session-based used model outputs and predicted PPTs. Temperature=0, max_tokens=2048.",
            "statistical_significance": null,
            "uuid": "e7434.0",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "history_ablation",
            "name_full": "Dialogue History Presence versus Removal (Ablation)",
            "brief_description": "Ablation testing of including previous-turn dialogue history (previous instructions with feasible API sequences) in the prompt versus removing it; history functions similarly to few-shot demonstrations and affects performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 accessed via chat completion API, temperature 0.",
            "model_size": null,
            "task_name": "PPTC Turn-based evaluation (creating new PPT / editing PPT template)",
            "task_description": "Generate API sequence for current turn given prompt inputs; ablation removes dialogue history from prompt.",
            "problem_format": "Natural-language instruction prompt with or without dialogue history (previous turns + feasible API sequences).",
            "format_category": "prompt style / few-shot demonstration",
            "format_details": "With history: prompt includes previous turns and their feasible API sequences and parsed PPT content (acts as few-shot examples). Without history: these elements removed. All other prompt elements unchanged.",
            "performance_metric": "turn-based accuracy",
            "performance_value": "GPT-4: creating-new task drops from 75.1% to 73.1% (absolute -2.0 pp) when history removed; editing-template task decreases by 6.2% (absolute; reported decrease).",
            "baseline_performance": "With history: 75.1% (creating-new), 38.1% (editing-template).",
            "performance_change": "Creating-new: -2.0 percentage points absolute; Editing-template: -6.2 percentage points absolute.",
            "experimental_setting": "Turn-based evaluation; removal of dialogue history only; other prompt components (API file, PPT content, task instruction) retained. Temperature=0.",
            "statistical_significance": null,
            "uuid": "e7434.1",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "cot_prompting",
            "name_full": "Chain-of-Thought (Zero-shot-CoT) Prompting",
            "brief_description": "Inserting the phrase 'Let's think step by step' (zero-shot CoT) into the prompt to elicit intermediate reasoning steps, evaluated as a prompt-style modification to improve planning for multi-step API generation.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 chat model; prompted with CoT cue in addition to standard inputs.",
            "model_size": null,
            "task_name": "PPTC Turn-based / Session-based evaluation",
            "task_description": "Generate API sequences for PPT operations; CoT prompt elicits intermediate decomposition/step-by-step reasoning.",
            "problem_format": "Chain-of-thought style prompt modification (natural-language cue 'Let's think step by step').",
            "format_category": "prompt style / reasoning prompt",
            "format_details": "Zero-shot-CoT: append 'Let's think step by step' after dialogue history to encourage the model to produce intermediate steps before final API sequence. Otherwise same inference prompt. No additional demonstration examples used for CoT.",
            "performance_metric": "turn-based accuracy (and session-based accuracy)",
            "performance_value": "GPT-4: creating-new task: baseline 75.1% -&gt; CoT 77.0% (approx +1.9 pp); editing-template task: baseline 38.1% -&gt; CoT 40.6% (approx +2.5 pp). Session-based improvements small (~+0.4 pp for creating).",
            "baseline_performance": "Baseline GPT-4 (no CoT): 75.1% (creating), 38.1% (editing).",
            "performance_change": "Creating-new: +1.9 percentage points absolute; Editing-template: +2.5 percentage points absolute; session improvements minor.",
            "experimental_setting": "CoT implemented by adding the sentence 'Let's think step by step' after dialogue history in the prompt. Temperature=0, max_tokens=2048.",
            "statistical_significance": null,
            "uuid": "e7434.2",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "tot_prompting",
            "name_full": "Tree-of-Thought (ToT) Prompting",
            "brief_description": "Applying the Tree-of-Thought algorithm to allow tree-like branching of intermediate reasoning states (ToT) as a more complex planning prompt strategy; compared to zero-shot CoT and baseline prompts.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 with ToT control loop (official ToT code) to generate and evaluate multiple reasoning 'thoughts' per step.",
            "model_size": null,
            "task_name": "PPTC Turn-based / Session-based evaluation",
            "task_description": "Generate API sequences for PPT tasks using Tree-of-Thought planning to explore multiple reasoning branches.",
            "problem_format": "Tree-of-Thought algorithmic prompting (multiple candidate thought branches, voting/evaluation), integrated with the standard inference prompt.",
            "format_category": "prompt style / search-based planning",
            "format_details": "ToT implemented following official code; it generates, evaluates, and selects among multiple thought branches. ToT dramatically increases token cost (reported 5–10x) compared to Zero-shot-CoT.",
            "performance_metric": "turn-based accuracy (and token cost)",
            "performance_value": "GPT-4: creating-new task: baseline 75.1% -&gt; ToT 76.5% (approx +1.4 pp); editing-template task: baseline 38.1% -&gt; ToT ~40.6% (approx +2.5 pp). ToT did not outperform Zero-shot-CoT and used significantly more tokens.",
            "baseline_performance": "Baseline GPT-4 (no ToT): 75.1% creating; 38.1% editing.",
            "performance_change": "Creating-new: +1.4 percentage points absolute vs baseline; ToT was worse or similar to CoT despite 5–10x token cost.",
            "experimental_setting": "ToT run using official implementation; token cost 5–10x higher than CoT. Temperature=0.",
            "statistical_significance": null,
            "uuid": "e7434.3",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "content_selection",
            "name_full": "Content Selection (Prompt Filtering of PPT Content)",
            "brief_description": "A prompt-time selection algorithm where the LLM (with three demonstration examples) is first prompted to select only relevant shapes from the parsed PPT file; the generated API sequence prompt then contains only the selected shapes instead of the whole PPT content, reducing irrelevant context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used twice in pipeline: once to select relevant PPT shapes (with 3 demonstration examples), then to generate the API sequence using the filtered PPT content.",
            "model_size": null,
            "task_name": "PPTC Turn-based / Session-based evaluation (primarily turn-based)",
            "task_description": "Generate API sequences to accomplish PPT instructions; content selection reduces prompt size and filters irrelevant objects prior to API generation.",
            "problem_format": "Two-stage prompting: (1) selection prompt with 3 demonstrations to pick relevant PPT shapes; (2) main API-generation prompt that includes only the selected shapes.",
            "format_category": "input modality / prompt engineering (context filtering)",
            "format_details": "Content Selection prompt includes three demonstration examples guiding selection; replaces full parsed PPT content in the main prompt with only the selected shapes. This can increase or decrease token usage depending on selection behavior (reported tokens: creating task input tokens increased in one run due to selection representation).",
            "performance_metric": "turn-based accuracy; avg token cost",
            "performance_value": "GPT-4: creating-new task: baseline 75.1% -&gt; content selection 77.5% (+2.4 pp); editing-template task: baseline 38.1% -&gt; content selection 43.1% (+5.0 pp).",
            "baseline_performance": "Baseline GPT-4 without content selection: 75.1% creating; 38.1% editing.",
            "performance_change": "Creating-new: +2.4 percentage points absolute; Editing-template: +5.0 percentage points absolute.",
            "experimental_setting": "Content selection used 3 demonstration examples in the selection prompt; selection replaces PPT content in main prompt. Token counts reported (creating: avg tokens rose to 3.4k from 2.9k in this experiment; editing: token usage and accuracy improved).",
            "statistical_significance": null,
            "uuid": "e7434.4",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "api_selection",
            "name_full": "API Selection (Embedding-based Top-k API Inclusion)",
            "brief_description": "Select the most relevant APIs to include in the prompt by computing cosine similarity between instruction and API description embeddings and keeping the top-k (k=15) APIs, reducing prompt verbosity and API hallucination risk.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used to generate API sequences after the reference API file is pruned by embedding-similarity selection (text-embedding-ada-002 used to compute embeddings).",
            "model_size": null,
            "task_name": "PPTC Turn-based / Session-based evaluation",
            "task_description": "Generate API sequences to perform PPT operations; only top-k (k=15) APIs by embedding similarity are included in prompt.",
            "problem_format": "Prompt content modification: replace full API reference list with top-k APIs ranked by embedding cosine similarity to current instruction.",
            "format_category": "prompt content / API context filtering",
            "format_details": "Embeddings from text-embedding-ada-002; cosine similarity ranking; k set to 15; reduces number of APIs in prompt and average token count.",
            "performance_metric": "turn-based accuracy; avg token cost",
            "performance_value": "GPT-4: creating-new task: baseline 75.1% -&gt; API selection 76.4% (+1.3 pp). API selection also reduced average token cost (reported avg tokens decreased from ~2.9k to ~1.5k in one experiment).",
            "baseline_performance": "Baseline GPT-4 with full API list: 75.1% (creating).",
            "performance_change": "Creating-new: +1.3 percentage points absolute; token cost reduction substantial in reported measures.",
            "experimental_setting": "Embedding similarity using text-embedding-ada-002; top k=15 selected. Temperature=0.",
            "statistical_significance": null,
            "uuid": "e7434.5",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "long_input_token_effect",
            "name_full": "Long PPT Template / Token Limit and Truncation Effects",
            "brief_description": "The format/length of the PPT environment (long templates) and model token limits affect LLM performance; long or complex PPT content leads to lower accuracy, and token truncation for models with small context windows causes information loss and degraded performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 and open-source LLMs (LLaMa-2 variants, etc.)",
            "model_description": "GPT-4 with large token limit (used higher token input), open-source models typically had 2–4k token limits leading to truncation.",
            "model_size": null,
            "task_name": "PPTC Editing PPT Template task",
            "task_description": "Editing operations on long, complex PPT templates requiring understanding of extensive slide content; prompts include parsed PPT file content which can be lengthy.",
            "problem_format": "Long parsed PPT content included in prompt; when input exceeds token limit, PPT content is truncated before prompting the LLM.",
            "format_category": "input modality / prompt length",
            "format_details": "When token number exceeds model limits, authors cut PPT file content to reduce prompt size. GPT-4 had higher token expense when editing templates (allowed more content) but still performed poorly; open-source models' token limits (2–4K) often required truncation, causing information loss.",
            "performance_metric": "turn-based and session-based accuracy",
            "performance_value": "GPT-4 editing-template turn-based accuracy: 38.1% (baseline). With content selection (to reduce irrelevant content) accuracy rose to 43.1%. Session-based accuracy remained near zero for editing-task across models (GPT-4 session-based 6.0%).",
            "baseline_performance": "Baseline (no content selection, long PPT content included): GPT-4 38.1% turn-based accuracy on editing task.",
            "performance_change": "Using content selection to reduce prompt length: +5.0 percentage points absolute (38.1% -&gt; 43.1%) on editing task for GPT-4.",
            "experimental_setting": "When prompt tokens exceed limit, authors truncate PPT content; content selection used as a mitigation. Temperature=0; max tokens reported 2048 used for completion calls and larger contexts used where allowed by model.",
            "statistical_significance": null,
            "uuid": "e7434.6",
            "source_info": {
                "paper_title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Gorilla: Large language model connected with massive apis",
            "rating": 2,
            "sanitized_title": "gorilla_large_language_model_connected_with_massive_apis"
        },
        {
            "paper_title": "ToolLLM: Facilitating large language models to master 16000+ real-world apis",
            "rating": 2,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.01732725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion</p>
<p>Yiduo Guo 
Peking University</p>
<p>Zekai Zhang 
Peking University</p>
<p>Yaobo Liang yaobo.liang@microsoft.com 
Microsoft Research Asia</p>
<p>Dongyan Zhao zhaody@pku.edu.cn 
Peking University</p>
<p>Nan Duan nanduan@microsoft.com 
Microsoft Research Asia</p>
<p>PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion
6C813BDF0E9DD3B7E3D927C6D0F19C2E
Recent evaluations of Large Language Models (LLMs) have centered around testing their zeroshot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs.However, the evaluation of LLMs utilizing complex tools to finish multiturn, multi-modal instructions in a complex multi-modal environment has not been investigated.To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions.It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations.We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences.We measure 3 closed LLMs and 6 open-source LLMs.The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy.We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception.These pose great challenges for future LLM and agent systems.We release the data, code, and evaluation system of PPTC at https://github.com/gydpku/PPTC.</p>
<p>Introduction</p>
<p>Recent evaluation works for Large Language Models (e.g.ChatGPT and GPT-4 (OpenAI, 2023)) focus on their zero-shot/few-shot abilities on basic natural language tasks (Jiao et al., 2023;Zhong et al., 2023;Wang et al., 2023b;Qin et al., 2023a) and their tool-use ability to generate APIs for solving user instructions, such as basic APIs like a calculator in tool transformer (Schick et al., 2023), * Equal contribution RapidAPIs in ToolLLM (Qin et al., 2023c), and hugggingface APIs in Gorilla (Patil et al., 2023).However, these tool-use works emphasize the translation of natural language instructions into APIs and ignore the challenge of using APIs in the observation of complex multi-modal environments to finish user instructions.Also, their evaluation approach focuses on comparing the generated APIs with the label API sequence, assuming there's only one unique solution.This approach becomes impracticable in situations with multiple/unlimited correct solutions.To address these challenges, we introduce Power-Point Task Completion (PPTC), a benchmark that measures LLMs' performance in creating and editing PPT file tasks based on user instructions.We choose PowerPoint as it includes various elements like textbox, table, and image and supports a wider range of APIs than Word and Excel.</p>
<p>Our benchmark has three distinctive features arXiv:2311.01767v2[cs.CL] 7 Nov 2023 from other task completion benchmarks: (1) Multiturn dialogue with varying difficulty.Our PPTC benchmark simulates the multi-turn dialogue session between the user and the LLM (see Figure 1) and contains 279 multi-turn sessions.Each multiturn session in our benchmark includes 2 to 17 turns.Each turn consists of a user instruction that describes the user's needs, a feasible solution that provides the correct solution, and the resulting label output file.Some turns can be easily addressed using a single API, while over half of the instructions require multiple APIs for completion.We provide the LLM with a reference API file that contains all feasible APIs for selection.</p>
<p>(2) Multi-modality.</p>
<p>Finishing the instruction of our benchmark requires understanding the multi-modal PPT file content and using multi-modal API operations (e.g., PPTC has 268 image operation-related instructions).</p>
<p>(3) Evaluation based on the final status: We propose the PPTX-Match Evaluation system to evaluate the LLM's outcome.To identify if the LLM completes the instruction, it checks the PPT file produced by executing the LLM-generated APIs rather than the LLM-generated APIs, thus all API sequences that lead to the correct final status are acceptable.</p>
<p>To finish the instruction, we use the current instruction, past turns' instructions (dialogue history), the PPT file content (specific environment information), and the reference API file as the input to prompt the LLM to generate an API sequence as the solution (See Figure 2 (A)).Then we use the API executor to execute the API sequence and return the user the resulting PPT file (See Figure 2 (B)).We name the resulting PPT file as the prediction file.In the evaluation step (See Figure 2 (C)), the PPTX-Match Evaluation system first uses the Python-PPTX library to extract all attributes from the prediction PPT file and the label output file.Then it uses the position relation checker to check if objects' positions conform to the label relation and the attribute content checker to check if the attribute's content is matched with the corresponding label attribute's content.The LLM correctly completes the current turn's instruction if all attributes of the file pass these tests.Evaluation metrics include turn-based accuracy which is the ratio of correctly completed turns to the total number of turns and session-based accuracy which is the ratio of correctly completed sessions to the overall session count.</p>
<p>We measure the performance of three closedsource LLMs ChatGPT, and six open-source LLMs (e.g., LLaMa-2) in our benchmark.We further test planning (e.g., CoT (Wei et al., 2022)) and content selection algorithms' performance based on GPT-4.Experiment results show that GPT-4 is the strongest LLM among all LLMs but still encounters challenges when completing entire multi-turn sessions.For example, although GPT-4 achieves 75.1% turn-based accuracy in the creating new PPT file task, it only achieves 22.7% session-based accuracy as errors made in previous turns.GPT-4 and other LLMs also struggle to process long PPT templates (complex file environment).For example, GPT-4 only achieves 38.1% turn-based accuracy in the editing task.We further find that GPT-4 struggles to finish instructions involving non-text modality operations, especially for position-related operations, such as 'Put object A on the top of the slide'.It only achieves 24% accuracy in these instructions.</p>
<p>In summary, this paper has the following contributions:</p>
<p>(1) We propose the PowerPoint Task Completion benchmark to measure LLM's task completion performance within the PowerPoint official software.This benchmark contains 279 multi-turn sessions with hundreds of multi-modal instructions in the complex multi-modal environment.</p>
<p>(2) We propose the PPTX-evaluation system to automatically measure LLMs' performance in our benchmark.We test 3 closed-source LLMs and 6 open-source LLMs and find that GPT-4 is the strongest LLM among all LLMs.</p>
<p>(3) We further analyze LLMs in our benchmarks and find three key error factors: error accumulation in the session, long PPT template processing, and multi-modality perception.These findings pose significant challenges for future LLMs and LLMbased systems.</p>
<p>PPTC Benchmark</p>
<p>In this section, we introduce our Power-Point Task Completion (PPTC) benchmark, including the overview of our benchmark, its collection and validation process, and the PPTX-Match Evaluation System for evaluation.We further analyze the statistics information of our benchmark.</p>
<p>Benchmark Overview</p>
<p>Benchmark components Our benchmark focuses on two basic tasks within PowerPoint: creating the new PPT file and editing the existing long PPT template for measuring long PPT Content understanding.We have gathered 229 multi-turn dialogue sessions for creating the new PPT file and 50 sessions for editing existing templates.Each multi-turn session includes 2 to 17 turns.Each turn comprises three parts: (1) the user instruction (2) the label output file as the ground truth (3) one feasible API sequence for finishing the instruction.Our benchmark also contains an API reference file that includes 49 feasible APIs for various operations and can complete all instructions in our benchmark.For each API, we describe its functionality and arguments and provide usage guidelines.For complex APIs, we also offer example cases.We list the details of all APIs in Appendix A.</p>
<p>Task description To complete the instruction in one turn, in general, the AI assistant must comprehend the user's current and prior instructions for context.It should also analyze the content of the PPT file to identify relevant objects mentioned in the instruction.Additionally, it needs to select appropriate APIs from a reference API file to achieve the user's goals.So we use these as the input of the AI assistant and it should output an API sequence as the solution.Then, it executes this API sequence and provides the user with the resulting PPT file as its response (See the whole process in Figure 2).</p>
<p>Addressing LLM limitations in our benchmark Compared to the general AI assistant, LLMs still have two limitations for completing the task in our benchmarks: (1) LLMs can not directly process the PPT file.So we provide a PPT reader function that extracts all shapes and their information from the PPT file and transforms them into the text format as the PPT file content.Then LLMs can understand and process the PPT file content.(2) LLMs cannot directly use PPT software through a keyboard and mouse.Therefore, we have defined PPT APIs based on the operational logic within the PPT software.and provide an implementation for these APIs in Python that can swiftly generate PPT files.In future work, it may be possible to explore the use of large multimodal models to understand on-screen content and implement APIs using a keyboard and mouse.</p>
<p>Benchmark Collection</p>
<p>Design Principles We follow these principles to design our benchmark: (1) Multi-turn instructions: One session in our benchmark should contain multiturn instructions to finish the user's complex need.</p>
<p>(2) Instructions of varying difficulty: Some instructions can be achieved with a single API, while others necessitate a sequence of APIs for successful completion.(3) Diverse multimodal operations: User instructions should cover a wide range of operations on PPT, such as text-related, image-related, and position-related APIs.(4) Topic Consistency:</p>
<p>The dialogue in a session should center around the session topic.Each user instruction in a session aligns closely with the previous instructions (the context), ensuring a coherent and contextually relevant dialogue flow.(5) Practicability First: The session topic and specific instructions should simulate the user's need in real world</p>
<p>Benchmark Collection and Validation To collect user instructions, we engage 6 skilled crowd workers who craft instructions in accordance with the principles we've outlined.Our crowd workers comprise professional data science engineers wellversed in PowerPoint.To achieve practicability first, we request crowd workers to write instructions based on their actual PowerPoint experience.On each session, the workers are asked to first find and list a practicable session topic.For the editing PPT template task, the topic must based on the template file background and is practicable to the template * .To achieve multi-instructions and topic consistency, the workers write instructions step by step and make them consistent with the topic.To achieve diverse multi-operations, we ask them not to write session that only involves a single modality operation.Each worker takes on a specific role in the instructional writing work and is encouraged to write instructions in his/her own words.Workers were asked to spend at least 20 minutes on every session.We delete repeated sessions and short sessions that have no more than 50 tokens.</p>
<p>Then we ask the seventh worker to write the feasible API sequence with minimal API usage for each instruction.Next, the workers create the PPT label file by using the provided API sequence.</p>
<p>During the whole process, the principal engineer reviews and refines the instructions and API sequences written by the above 7 workers for initial quality assurance.</p>
<p>To ensure the data quality of this benchmark, the three authors of this paper further undertake the following validation steps: (1) Assessing Instruction Clarity and Relevance: They examine whether the instructions are clear, contextually related to the session topic, and align with the ongoing conversation.( 2) API Sequence Execution: The authors execute the provided API sequences to identify and rectify coding errors.(3) Goal Achievement Check: They verify if the instructions' intended goals are successfully completed in the label files.</p>
<p>In the event that errors are identified during this validation process, the authors promptly report them to the respective workers for revision.The three authors are computer science senior students and researchers.</p>
<p>PPTX-Match Evaluation System</p>
<p>We design the PPTX-Match Evaluation System to evaluate LLMs' performance on the PPTC benchmark.Specifically, our PPTX-Match Evaluation System first uses a Python-PPTX Content Reader Module to iterate over all shapes in the prediction PPT file produced with the LLM and the label output file.A shape in the PPTX library typically refers to an individual object, such as a text box or table.Then our system extracts attributes like text, style, and position of the shapes using the PPTX library.Next, we check all attributes from the prediction PPT file.For non-position attributes (e.g., text content), we first convert it and the corresponding attribute in the label PPT file into two strings, and then we use the Exact Match method to examine if the two strings are the same.If they are different or we do not find the corresponding attribute in the label file, then we find an incorrect match.For the position attribute (e.g., location information), we focus on checking if the objects in the prediction PPT file satisfy the correct position relation <A, B, REL>, where A and B are objects that should satisfy the relation REL.In the benchmark collection process, we ask crowd workers to label the position relation that objects should satisfy to finish the instruction.During the evaluation If there are no incorrect matches for all nonposition attributes and no rule violations for all position-related attributes, we consider the LLM has successfully completed the user instruction.</p>
<p>Benchmark Statistics Analysis</p>
<p>To understand the properties of PPTC, we analyze the instructions and APIs in the benchmark.Specifically, we explore (i) the number of turns in a session, (ii) the difficulty of the instruction in terms of the number of APIs required to finish it, and (iii) the number of multi-modality instructions.We report statistics about the PPTC benchmark in Figure 3.</p>
<p>The number of turns in a session The session turn number distribution (Figure 3 (a)), measured as the number of turns in a session, shows that all sessions in our benchmark have at least two turns and almost all sessions have at least 3 turns (between 3 and 13 turns for the 5th to 95th percentile, respectively).The longest session has 17 turns, which is very challenging as the errors made in previous turns can influence the completion of the current instruction.</p>
<p>Diffculty varies in APIs number The number of APIs in a sequence falls between 1 and 5 for the 5th to 95th percentile (Figure 3 (b)), respectively, shows that our instructions' difficulty varies from a simple sentence that can be finished by one API to a complex instruction that requires the LLM to generate multiple APIs.The longest API sequence consists of 29 APIs.Generating long API sequences is very challenging as the LLM needs to understand sub-goals in the complex instruction, select appropriate APIs from the file, and generate APIs in a reliable order.</p>
<p>Rich multi-modal instructions Our benchmark has hundreds of instructions that involve multimodalities content (Figure 3 (c)).The "chart" modality has the fewest instructions, with 120, while the "position" modality has the most, with 292 instructions.To finish these instructions, LLMs need to employ related-modal APIs based on the understanding of multi-modal file content.</p>
<p>Algorithms</p>
<p>In this section, we introduce the algorithms we considered to enhance the LLM's performance in our benchmark.These algorithms can be categorized into two approaches: planning algorithms that help the LLM in decomposing the user instruction and solving it step by step and selection algorithms that assist the LLM in choosing important environmental information or APIs.</p>
<p>Planning Algorithms</p>
<p>Complex user instructions often require multiple intermediate steps to complete.We mainly consider two planning algorithms:</p>
<p>Zero-shot-CoT (Kojima et al., 2022) enables LLMs to autonomously generate intermediate reasoning processes for complex instruction by prompting LLMs to "Let's think step by step".</p>
<p>Tree of Thoughts (ToT) (Yao et al., 2023) enables LLMs to follow tree-like reasoning paths, where each tree node represents a thinking state.It leverages LLMs to generate evaluations or votes on different thoughts.</p>
<p>Selection Algorithms</p>
<p>Combining the whole PPT file and the whole API file into the LLM's input can result in an overwhelming amount of redundant information, such as irrelevant file content and unhelpful APIs.Filtering the redundant information would improve the efficiency of the LLM.In this context, we primarily focus on two algorithms for selecting the PPT file content and APIs, respectively:</p>
<p>Content Selection algorithm Firstly, we extract all shapes of the PPT file by Python-PPTX.Next, we prompt the LLM to select the shapes for completing the user's instruction.We show the prompt in Figure 8, in which we add three demonstration examples to guide the LLM to do selection.In this algorithm, we replace the whole PPT file with the selected shapes when prompting the LLM to generate the API sequence.</p>
<p>API Selection algorithm The API selection algorithm is based on the embedding similarity to select the most relevant APIs for user instructions.Specifically, we use the text embedding API to get the embeddings of all API descriptions and the current user instruction.Next, we compute the cosine similarity between the instruction embedding and each API description's embedding and rank them based on the similarity score.In this algorithm, we replace the whole reference API file with the top k APIs when prompting the LLM to generate the API sequence.</p>
<p>Experiments</p>
<p>Large Language Models Selected for Evaluation</p>
<p>Here, we assess different cutting-edge large language models using our benchmark.These chosen models showcase a wide array of capabilities and are highly regarded in the field.The evaluated large language models include 3 closed-source LLMs and 6 open-source LLMs:</p>
<p>• GPT-4 (OpenAI, 2023): The latest LLM in the GPT series.GPT-4 is a cutting-edge, large-scale generative pre-trained transformer model.It offers improved performance and a wider knowledge base compared to its predecessors.It showcases human-level proficiency in several scenarios.</p>
<p>• ChatGPT: ChatGPT is a conversational AI model crafted for dynamic interactions.It's learned from extensive instruction data and fine-tuned through reinforcement learning with human feedback (RLHF).This empowers it to deliver responses that align with human expectations, maintaining context and coherence in conversations.</p>
<p>• Text-Davinci-003 (Brown et al., 2020): GPT-3.5 sits between GPT-3 and GPT-4, enhancing performance via additional instruction tuning.</p>
<p>It acts as a link between these models, facilitating comparison.We've chosen the Text-Davinci-003 variant from the GPT-3.5 series for our evaluation.</p>
<p>Experimental Setup</p>
<p>In this section, we provide an overview of the experimental setup utilized to assess the performance of LLMs on our PPTC benchmark.</p>
<p>Turn-Based and Session-Based Evaluations</p>
<p>We consider two performance evaluation approaches in our benchmark: turn-based and sessionbased evaluations.For the turn-based evaluation, we measure the LLM's ability to finish a single turn.Specifically, in this evaluation, we assume that the previous turns have been correctly finished, and we prompt the LLM to generate the API sequence to finish the current turn's user instruction.The prompt consists of the task instruction for finishing the current user instruction, the API file containing feasible APIs, the parsed PPT file content from the PPT file, and dialogue history consisting of instructions of previous turns with their feasible API sequences (see the left of Figure 4).For the session-based evaluation, we measure the LLM's ability to finish a session containing multiple turns.For all turns in a session, we prompt the LLM to finish them sequentially.The prompt in this evaluation has two differences: the API solutions for previous turns in dialogue history are the outputs of the LLM instead of the correct API sequences.</p>
<p>(2) The PPT content is parsed from the PPT file obtained by executing the previous outputs of the LLM (see the right of Figure 4).That means the error made by LLMs in previous turns would influence subsequent turns.Metrics For turn-based evaluation, we report the turn-based accuracy as the ratio of the number of successfully finished turns to the total number of turns.We also report the average token number of the input of one turn and the average API number for finishing one turn as the cost measurement.For session-based evaluation, we report the session-based accuracy as the ratio of the number of successfully finished sessions to the total number of sessions.We also report the average value of the token number of all inputs in one session and the average API number required to complete one session as the cost measurement.</p>
<p>Inference prompt in PPTC</p>
<p>(Task instruction) You are an AI assistant to help the user to operate PowerPoint and edit the contents.Give you the user instruction:<Current user instruc-tion>, you can complete it based on the following APIs and PPT file content.Current you are at page <Page id>.Please finish the user instruction with the functions you have.Don't generate instructions beyond what the user has instructed.Don't guess what the user may instruct in the next step and generete API for them.Don't use python loop to call API.You can only call API once in one line.If the user does not specify the page to be modified, you can directly start using the APIs without having to navigate to other pages.You need to generate code which can finish the user instruction.The multiple lines of code should be surrounded by <code> and </code> such as: <code> API(); API(); </code> For example, if the user instruction is "create a slide", then the answer should be: <code> create_slide(); </code> (API file) Now, you have access to a list of PowerPoint APIs with the following functions: <APIs and their descriptions> (e.g.,API(name="set_width", parameters="(width)", description="This API sets the width of the selected object.",parameter_description="It takes one parameter 'width', the width of an object in centimeters as float.",composition_instruction="You should first choose an object before you can change the width of it.",api_desc="width of picture and shapes") ) Figure 4: The inference prompt we used in both turnbased and session-based evaluation settings.In the turnbased evaluation, we assess the LLM's performance for the current turn and assume the LLM has correctly finished previous turns.We then use feasible API sequences of previous turns as the AI response in the dialogue history and parse the label file of previous turns as the PPT file content.In the session-based evaluation, we evaluate the completion of the entire session and do not assume the LLM has correctly finished previous turns.We use the LLM's generated API sequences as the response and parsed the LLM prediction file as the PPT file content.</p>
<p>Implementation Details</p>
<p>All experiments were conducted using the respective language models' API provided by Azure Ope-nAI Service † .Azure OpenAI services offer two API types: completion and chat completion.Completion API generates text from prompts, while chat completion API responds based on conversation history and new input.We use the completion API for Text-Davinci-003 and the chat completion API for ChatGPT and GPT-4.We set a temperature of zero for deterministic output and a max token limit of 2048.The frequency penalty and top p are kept at their default values of zero and 1, respectively.We use the text-embedding-ada-002 API as the embedding API in the API selection algorithm and set k as 15.For open-source LLMs, we choose the chat version of LLaMa-2, the v1.2 version of WizardLM, and the chat version of Baichuan as our open-source LLMs.We choose the 13 billion parameters model of the three LLMs.</p>
<p>For the zero-shot CoT method, we add the sentence 'Let's think step by step' after the dialogue history of the prompt.For the ToT method, we follow the official code to run it ‡ .We run the four algorithm methods based on the GPT-4 model.</p>
<p>If the token number of the input prompt is beyond the token limit, we cut the PPT file content to reduce the token number of the prompt.</p>
<p>Main results</p>
<p>We report the results of LLMs in both turn-based and session-based evaluations in Table 1 and 2. From the results, we highlight the following key findings.</p>
<p>(1) Superior Performance of GPT-4: GPT-4 consistently outperforms other closed-source and open-source LLMs in both two tasks.Impressively, GPT-4 achieves 75.1% turn-based accuracy in the creating new PPT file task, demonstrating its strong capability to finish one turn of the user instruction.GPT-4 also has a lower API cost compared to other closed-source LLMs since its precise API usage.GPT-4 incurs the highest token expense when editing PPT templates.That is because its higher token limit than other LLMs allows us to input more PPT template content.† https://azure.microsoft.com/en-us/products/cognitive-services/ openai-service ‡ ToT:https://github.com/princeton-nlp/tree-of-thought-llm</p>
<p>(2) Code continual pre-training and further instruction finetuning can boost open-source LLMs' performance.: Based on Table 1, it's evident that current open-source LLMs struggle to match the performance of closed-source LLMs.For example, LLaMa-2-chat only achieves 16.2% turn-based accuracy in the creating new PPT file task, which is far from the performance achieved by closed-source LLMs.We further find that code continual pretraining (Code-LLaMa) and instruction fine-tuning based on LLaMa-2 (WizardLM and Vicuna) can further improve LLaMa-2 performance obviously.For example, Code-LLaMa improves LLaMa-2's turn-based accuracy in the creating new PPT file task by 20.4 %.This observation suggests that there's untapped potential in open-source LLMs when it comes to our benchmark, and this potential can be unlocked further by code pre-training and enhancing instruction following ability.</p>
<p>(3) Planning and selection algorithms can improve LLMs' turn-based performance From Table 2, we observe that the planning algorithms (CoT and ToT) can further improve the turn-based performance of GPT-4 by 1∼2 percent.However, we surprisingly find that the more complex ToT algorithm does not outperform the zero-shot CoT algorithm with a 5∼10 times token cost.Content and API selection algorithms can further improve the turn-based performance of GPT-4 by 1∼ 5 percent.That is because they reduce the task difficulty by filtering irrelevant PPT content/APIs in the input prompt.The API selection algorithm also reduces the average token cost by reducing the number of APIs listed in the prompt.However, for the challenging session-based evaluation, these algorithms can not improve GPT-4's performance or improve it slightly.</p>
<p>Three challenges in our PPTC benchmark</p>
<p>From the result Table 1 and Figure 5. we highlight the following three key challenges.</p>
<p>(1) Error accumulation makes LLMs performance poor in finishing the entire multi-turn session.:The performance of all LLMs in handling sessions consisting of multiple turns is notably poor.Even GPT-4, which performs well in turn-based evaluation, achieves only a 22.7% session-based accuracy for the "creating new PPT file" task and a mere 6.0% session-based accuracy for the "editing PPT template" task.Current planning algorithms usually fail to improve session-based accuracy.In some cases, they can even make the performance worse.The session-based evaluation is challenging since errors made in previous turns make the LLM fail to finish the session and also influence the completion of the current turn.Also, we need more advanced planning algorithms to complete the multi-turn session.</p>
<p>(2) LLMs perform badly in processing long PPT template: Current LLMs' performance in the editing PPT temples task is pretty poor.For example, the strongest GPT-4 only achieves 38.1% turnbased accuracy and 6.0% session-based accuracy in this task.Other LLMs' performance is even poorer.The content selection algorithm can partially solve this challenge by filtering out irrelevant file content, but GPT-4 with it still only achieves 43.1% turnbased accuracy.That means current LLMs (e.g., GPT-4) still struggle to handle complex and lengthy PPT templates.For open-source LLMs, there's a risk of information loss due to token limitations (typically 2∼4K tokens limit), which often require truncating lengthy PPT content.When it comes to session-based performance, the accuracy remains nearly zero.This implies that current LLMs are still far from being ideal PPT agents capable of effectively assisting users in editing PPT templates during a multi-turn dialogue session.</p>
<p>(3) Multi-modal instructions increase the LLM's failure rate significantly.To assess LLMs' task completion performance for instructions involving multi-modal operations (Table, Chart, Pic-ture, Position, and text), we calculate the average accuracy of GPT-4 for instructions involving each modality, respectively.This is done by dividing the number of correctly completed instructions within each modality by the total number of instructions involving that modality's operation.The results are presented in Figure 5 (a).From the figure, we observe that GPT-4 performs exceptionally well in the text modality, achieving an accuracy of 85.6%.Its performance becomes poorer when processing structured data (Chart and Table ), with 12.4% and 16.2% lower accuracy.Instructions involving picture-related operation pose an even greater challenge for GPT-4, as it achieves a 56.8% turnbased accuracy in this modality.GPT-4 exhibits its weakest performance in instructions involving position-related (spatial) operations, with only 24% accuracy.This underscores GPT-4's limitations in spatial perception ability.</p>
<p>Analysis</p>
<p>In this section, we analyze the reasons for GPT-4's errors.We further analyze the influence of model size and dialogue history.</p>
<p>Error Analysis of GPT-4 in our benchmark</p>
<p>To analyze the error made by GPT-4, in our benchmark, we gather 50 wrong samples for each of the two tasks in our benchmark in the turn-based evaluation.We find that these wrong samples fall into four error types and visualize the distribution of these four main error types in Figure 5 (b): (1) Po- sition errors: These occur when GPT-4 struggles with instructions involving position adjustments.For example, when asked to move the shape to the bottom of the slide, GPT-4 wrongly calls the "set_top" API.position error is the main error in the creating new PPT file task as this task contains many instructions involving position operation.(2) Calling unavailable APIs: GPT-4 sometimes generates APIs that don't actually exist in the reference API file, resulting in what we call the "API hallucination problem."(3) Misunderstanding PPT file content: GPT-4's comprehension of the PPT content can be flawed, leading to the generation of incorrect API sequences.For example, when instructed to make the font size of the current slide's title consistent with previous slides, GPT-4 set a font size that is different from what was used in previous slides' titles.In the editing template task, misunderstanding the PPT content becomes the main error since this task needs to understand the complex PPT template.(4) Unfollowing Powerpoint task rules: Completing Powerpoint tasks demands a deep understanding of various task rules.For instance, writing a new slide title requires first deleting the original text and then inserting the new title into the text box.However, GPT-4 may directly insert the new content.For the session-based evaluation, we also collect 50 wrong examples.We find that the main reason for the poor session-based performance is the LLM fails to finish the session once it makes an error in one turn of the session.The reasons for errors made in a single turn are similar to those in the turn-based evaluation.One unique phenomenon in this evaluation is that the LLM would repeat previous errors (e.g., employing infeasible APIs) in subsequent turns.</p>
<p>Does bigger LLM work better on PPTC?</p>
<p>To investigate how the model size impacts the LLM's performance in our benchmark, we conduct tests using LLaMa-2-chat LLM with 7, 13, and 70 billion parameters and plot the results in Figure 5 (c).We observe that larger LLM consistently achieve higher turn-based accuracy for both the creating new PPT and editing PPT template tasks.For example, in the creating new PPT file task, we find that the turn-based accuracy increases from 13.2 (7B) to 30.1 (70B).However, we do not observe a clear positive correlation between model size and session-based performance.One possible explanation is that although the 70B LLM can correctly finish more intermediate steps, it still falls short of completing the entire session.To improve the session-based performance, a larger LLM may be necessary.</p>
<p>Does dialogue history help LLMs to generate the API sequence?</p>
<p>To investigate the influence of dialogue history in our prompt (see Figure 4), we make an ablation experiment for the dialogue history component of our turn-based evaluation prompt § .In this evaluation, the dialogue history contains previous turns along with their feasible API sequences.When we removed the dialogue history from the prompt, we observed a decline in GPT-4's performance.Specifically, GPT-4 drops its performance from 75.1 % to 73.1 % in the creating new PPT file task and decreases its performance by 6.2 % in the editing template task.This experiment shows the positive effect of the dialogue history, as it helps the LLM to both understand the dialogue background and instruct the LLM to correctly use the APIs, similar to few-shot demonstration examples.</p>
<p>Related Works</p>
<p>Large Language Models like ChatGPT, GPT-4 (Bubeck et al., 2023;OpenAI, 2023), and Bard have billions of parameters and have been trained on the Internet corpus with trillions of tokens.They can write code (Liu et al., 2023a), prove mathematical theorems (Jiang et al., 2022), pass the professional exam (Zhong et al., 2023;Gilson et al., 2023;Katz et al., 2023), and also perform well on other basic natural language tasks (Kim et al., 2023;Jiao et al., 2023;Zhong et al., 2023;Wang et al., 2023b).That raises the hope of achieving artificial general intelligence (AGI).</p>
<p>To further boost LLM's performance on the specific task, one approach involves prompting engineerings, such as the chain of thought prompting (Wei et al., 2022;Shi et al., 2022;Yao et al., 2023), self-consistency (Wang et al., 2022) and the least to most prompting (Zhou et al., 2022).Another approach aims to use feedback to improve performance.The self-refine method (Madaan et al., 2023) refines the output through iterative feedback and refinement Provided by LLM itself.The Reflexion (Shinn et al., 2023) method generates and stores the reflection based on the sparse reward signal and then uses the reflection to induce better decisions in subsequent trials.The learning to program method (Guo et al., 2023) learns the task program by inducing the general solutions from the errors (feedback) iteratively and uses the program to guide the inference.</p>
<p>Task completion benchmarks for measuring large language models.To measure LLM's task completion performance, Saycan (Brohan et al., 2023) and VirtualHome (Puig et al., 2018) benchmarks ask LLM to generate the correct action sequence for controlling the robot to finish user instruction.WebShop (Yao et al., 2022) and Android in the wild (Rawles et al., 2023) ask LLM to navigate websites and conduct actions to meet the user requirement.APIBench (Patil et al., 2023) and ToolBench (Xu et al., 2023b;Qin et al., 2023b) involve selecting and using APIs to complete the task instruction.Agentbench (Liu et al., 2023b)assesses LLM as autonomous agents in 8 environments and WebArena (Zhou et al., 2023) considers task completion in web-based interactions.</p>
<p>AI assistant system for complex task completion For more complex tasks that involve using tools and utilizing environmental information, there are many strong AI systems (e.g., TaskMatrix (Liang et al., 2023)) that help the user finish the complex task.One approach involves connecting massive models and tools with the Large Language Model for task completion.Examples include Visual ChatGPT (Wu et al., 2023) and Hugging-GPT (Shen et al., 2023) which use LLM to deploy task-specific models to finish the user instruction based on the observation of task information (e.g., visual information), Voyager (Wang et al., 2023a) that uses the fixed LLM to continually learn skills (tools) based on the observation of the Minecraft environment.Another approach involves training an end-to-end LLM to finish the user instruction.Examples include Gorilla (Patil et al., 2023) for generating API calls to finish the user query by using the API bench to fine-tune the pre-trained LLaMA (Touvron et al., 2023) model.PaLM-E (Driess et al., 2023) for various robot reasoning tasks by fine-tuning the pretrained PaLM (Chowdhery et al., 2022) model using features from sensor modalities.Different from the above systems, we focus on the research topic of the AI assistant system in office software.</p>
<p>Conclusion</p>
<p>We introduce the PowerPoint Task Completion benchmark to measure LLMs' ability to complete user instructions within the context of the Power-Point software.It contains hundreds of multi-turn sessions with different topics and thousands of instructions with varying levels of difficulty.We further propose the PPTX-evaluation system to access and compare the performance of different LLMs.Results show that GPT-4 is the strongest LLM but still performs poorly in finishing entire sessions.We further analyze the behavior of LLMs and find three main error factors that limit their performance.Our benchmark and findings can help the research community design better AI task completion assistants.</p>
<p>Our benchmark does not consider instructions that involve subjective evaluation.For example, the user may want to make the slide more beautiful.However, it's hard to automatically evaluate if the generated file (the model output) is more beautiful.Another limitation is that we do not consider the instructions that need non-API operations.For example, the user may want to draw a cat on the slide.That instruction needs the AI-assistant system to draw the cat by dragging the mouse and is still infeasible for LLMs and LLM-based systems.We only consider instructions that can be completed by directly executing the API sequence.</p>
<p>Figure 1 :
1
Figure 1: Within our benchmark, we simulate this multiturn dialogue scenario between humans and LLMs to evaluate LLMs' PPT task completion performance.</p>
<p>Figure 2 :
2
Figure 2: We illustrate how LLMs complete one turn in a session.(A) To prompt the LLM, we provide it with the current instruction, previous instructions (dialogue history), PPT file content, and the API reference file.'PPT reader' is a function that transforms the PPT file into the text-based format as the PPT file content.(B) The LLM then generates the API sequence and executes it to obtain the prediction PPT file.(C) We evaluate attributes and position relations in the prediction file.</p>
<p>Figure 3 :
3
Figure 3: Statistics for PPTC.a) Session turn number distribution.b) Instruction API number distribution (tokens).c) Distribution of instructions involving Chart, Table, Picture, and Position.Instructions involving 'Position' need the system to conduct position-related operations based on the understanding of spatial information.Note that one instruction may involve multiple different modalities.phase, we extract the position attributes of objects A and B and use predefined functions to verify if the objects' position attributes satisfy the position relation.If there are no incorrect matches for all nonposition attributes and no rule violations for all position-related attributes, we consider the LLM has successfully completed the user instruction.</p>
<p>(</p>
<p>PPT file content) All the PPT contents are: <Begin of PPT> Turn-based: <Parsed PPT file content of the label PPT file of the previous turns> Session-based: <Parsed PPT file content of the LLM prediction file of the previous turns> <End of PPT> (Dialogue history) ¬User¬: Hello! ¬AI¬: Hi there!How can I help you?¬User¬: <the first instruction> ¬AI¬: Turn-based: <the correct feasible API sequence>, Session-based: <the LLM-generated API sequence> ... ¬User¬: <Current user instruction>.Surrounding your answer with <code> and </code>.¬AI¬:</p>
<p>Figure 5 :
5
Figure5: We illustrate the analysis results of the creating new PPT file task (task 1) and the editing PPT template task (task 2).In sub-figure (a), we report the average turn-based accuracy for instructions involving chart, table, picture, position, and pure text.We don't draw the accuracy of task 2 as no chart instruction in this task.In sub-figure (b), we report the ratio of four common errors made by GPT-4.In sub-figure (c), we report the accuracy with the model size.We don't plot the session-based accuracy of task 2 as it is zero.</p>
<p>Table 1 :
1
We report the results of LLMs in this table.'TD-003'isthe Text-Davinci-003 model.We directly use the prompts in Figure4to prompt LLMs to generate the API sequence.
Creating new PPTEditing PPT templateModels and MethodsTurn-basedSession-basedTurn-basedSession-basedAccuracy Avg token Avg API Accuracy Avg token Avg API Accuracy Avg token Avg API Accuracy Avg token Avg APITD-00372.62.8k3.012.720.8k23.924.42.9k8.14.013.2k26.6ChatGPT70.62.9k3.212.720.0k23.426.34.1k7.92.09.2k22.9GPT-475.12.9k2.922.720.8k22.438.17.5k7.86.024.1k24.7LLaMa-216.42.8k3.93.421.6k24.78.72.2k7.20.09.5k15.6Code-LLaMa36.82.8k3.40.020.7k32.118.73k7.32.09.6k22.6WizardLM23.91.3k3.34.312.5k22.410.01.3k5.70.04.3k16.5Vicuna-v1.524.31.3k3.92.211.0k33.76.81.3k6.70.04.3k22.7Baichuan15.51.3k9.80.010.9k44.74.41.3k9.60.04.3k24.3Baichuan-216.31.3k9.13.611.6k48.98.71.3k9.20.04.2k22.3Creating new PPT fileEditing PPT templateModels and MethodsTurn-basedSession-basedTurn-basedSession-basedAccuracy Avg token Avg API Accuracy Avg token Avg API Accuracy Avg token Avg API Accuracy Avg token Avg APIGPT-475.12.9k2.922.720.8k22.438.17.5k7.86.024.1k24.7GPT-4+CoT77.02.9k3.123.120.8k22.740.67.5k8.06.024.1k25.2GPT-4+ToT76.520.8k3.021.8146.4k22.640.681k7.64.0256.8k24.0GPT-4+Content selection77.53.4k3.021.824.5k22.043.15.8k8.04.018.7k25.2GPT-4+API selection76.41.5k2.918.810.6k21.338.17k8.010.022.4k25.8</p>
<p>Table 2 :
2
We report the results of GPT-4 and algorithms based on the GPT-4 model.'CoT' and 'ToT' are the chain of thought and tree of thought algorithms.</p>
<ul>
<li>We collect 50 PPT templates from the SlidesCarnival website (https://www.slidescarnival.com/). SlidesCarnival is a free and open-source PPT template website. Each session in the editing task has a unique template. We encourage topic diversity in our templates. We remove templates that are too short (2∼5 slides) and have repeated topics.
§  The task instruction, current user instruction, API file, PPT content in the prompt are necessary parts for generating the API sequence. So we don't conduct ablation studies on them.
A The API Reference FileWe list all APIs and their descriptions in Figures6  and 7. We provide 49 feasible APIs.B The Prompt for Content Selection AlgorithmWe put the prompt of content selection algorithm in Figure8.API reference fileSlide
Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR2023</li>
</ul>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. April 202314</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment. Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash, JMIR Medical Education. 91e453122023</p>
<p>Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, Nan Duan, arXiv:2304.10464Learning to program with natural language. 2023arXiv preprint</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Sean Albert Q Jiang, Jin Peng Welleck, Wenda Zhou, Jiacheng Li, Mateja Liu, Timothée Jamnik, Yuhuai Lacroix, Guillaume Wu, Lample, arXiv:2210.122832022arXiv preprint</p>
<p>Is chatgpt a good translator? a preliminary study. Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Zhaopeng Tu, arXiv:2301.087452023arXiv preprint</p>
<p>. Martin Daniel, Michael James Katz, Shang Bommarito, Pablo Gao, Arredondo, 2023Gpt-4 passes the bar exam. Available at SSRN 4389233</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, arXiv:2303.16434Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. 2023arXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, arXiv:2305.012102023aarXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023barXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, arXiv:2302.06476Is chatgpt a general-purpose natural language processing task solver? arXiv preprint. 2023a</p>
<p>. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023b. Tool learning with foundation models</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023carXiv preprint</p>
<p>Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap, arXiv:2307.10088Android in the wild: A large-scale dataset for android device control. 2023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, arXiv:2210.030572022arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Is chatgpt a good sentiment analyzer? a preliminary study. Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, Rui Xia, arXiv:2304.043392023barXiv preprint</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671Visual chatgpt: Talking, drawing and editing with visual foundation models. 2023arXiv preprint</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023aarXiv preprint</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, arXiv:2305.165042023barXiv preprint</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Dian Da Pan, Dong Wang, Fan Yan, Yang, arXiv:2309.10305Open large-scale language models. 20232arXiv preprint</p>
<p>Webshop: Towards scalable realworld web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, arXiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
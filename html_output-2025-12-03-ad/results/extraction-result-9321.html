<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-265157726</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07599v1.pdf" target="_blank">Testing LLMs on Code Generation with Varying Levels of Prompt Specificity</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated unparalleled prowess in mimicking human-like text generation and processing. Among the myriad of applications that benefit from LLMs, automated code generation is increasingly promising. The potential to transform natural language prompts into executable code promises a major shift in software development practices and paves the way for significant reductions in manual coding efforts and the likelihood of human-induced errors. This paper reports the results of a study that evaluates the performance of various LLMs, such as Bard, ChatGPT-3.5, ChatGPT-4, and Claude-2, in generating Python for coding problems. We focus on how levels of prompt specificity impact the accuracy, time efficiency, and space efficiency of the generated code. A benchmark of 104 coding problems, each with four types of prompts with varying degrees of tests and specificity, was employed to examine these aspects comprehensively. Our results indicate significant variations in performance across different LLMs and prompt types, and its key contribution is to reveal the ideal prompting strategy for creating accurate Python functions. This study lays the groundwork for further research in LLM capabilities and suggests practical implications for utilizing LLMs in automated code generation tasks and test-driven development.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9321.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9321.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AllModels_PromptFormats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate performance of all evaluated LLMs by prompt presentation format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated comparison across all evaluated LLMs (GPT-3.5, GPT-4, Claude-2, Bard, text-davinci-002) showing how four prompt presentation formats affect code-generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>aggregate (GPT-3.5, GPT-4, Claude-2, Bard, text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Python code generation on 104 coding problems (4 prompt types)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python functions for a benchmark of coding problems; correctness evaluated via additional unseen test cases (pass/fail counts).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Four prompt presentation formats were evaluated: (1) Prompt Only: problem statement only; (2) Prompt with Tests: problem statement + example test cases; (3) Prompt Tests Only (Tests Only): only example test cases (no problem statement); (4) Prompt Generic Tests: tests with masked function names (increased ambiguity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>The three alternative formats compared were Prompt Only, Prompt with Tests, and Prompt Generic Tests (when analyzing Prompt Tests Only), and vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prompt Tests Only: 231/315 (73.3%); Prompt with Tests: 228/315 (72.4%); Prompt Only: 222/315 (70.5%); Prompt Generic Tests: 200/315 (63.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See performance field: Tests Only slightly outperformed Prompt with Tests and Prompt Only; Prompt Generic Tests (masked function names) performed worst.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Best vs worst: +31/315 (~+9.8 percentage points) (Prompt Tests Only 73.3% vs Prompt Generic Tests 63.5%); Tests Only vs Prompt with Tests: +3/315 (~+1.0 pp); Tests Only vs Prompt Only: +9/315 (~+2.9 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that providing tests (even without a problem statement) supports LLM deductive reasoning and specification inference, improving code generation; masked function names (Prompt Generic Tests) increase ambiguity and reduce performance by making requirement inference harder. Tests may act analogously to chain-of-thought / worked-examples, guiding reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Benchmark: 104 coding problems (26 generated by LLMs; each problem presented in 4 prompt styles → 104 prompts). Evaluation aggregated across 5 models for each format (denominators reported as 315 = 63 tasks × 5 models). Each problem evaluated with additional unseen test cases; scoring counted passes/fails. Each problem run in its own LLM session to avoid cross-problem leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing LLMs on Code Generation with Varying Levels of Prompt Specificity', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9321.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9321.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_overall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (evaluated model) performance across prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was evaluated on the coding benchmark and showed high overall performance and strong adaptation to tests-only prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Python code generation on the paper's benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python functions for benchmark problems; correctness measured by unseen test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>All four formats used (Prompt Only; Prompt with Tests; Prompt Tests Only; Prompt Generic Tests). The paper reports per-format scores for GPT-3.5 where available.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across the three other formats (Prompt Only, Prompt with Tests, Prompt Generic Tests).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall: 210/252 (83.3%). Per-format where reported: Prompt Tests Only: 55/63 (87.3%); Prompt with Tests: 55/63 (87.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prompt Tests Only (55/63) ≈ Prompt with Tests (55/63); other per-format scores not explicitly enumerated for GPT-3.5 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Prompt Tests Only vs overall worst-format (not directly provided for GPT-3.5); from reported per-format values, Tests-only vs Generic (if Generic were similar to aggregate) would be expected to drop several percentage points. Using reported per-format values: Tests-only (87.3%) vs aggregate worst-format for all models (Prompt Generic Tests 63.5%) implies a potential model-format sensitivity but exact per-model generic-test score not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>GPT-3.5 benefits from test-case driven prompts; tests allow the model to deduce functional requirements. Authors note GPT-3.5 shows strong deductive reasoning on tests-only format.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-format denominator appears to be 63 examples (55/63 reported for two formats). Overall denominator 252 (4 formats × 63). Problems presented in separate sessions; additional unseen tests used to evaluate correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing LLMs on Code Generation with Varying Levels of Prompt Specificity', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9321.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9321.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_overall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (evaluated model) performance across prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 performed similarly to GPT-3.5 overall and showed robustness across prompt styles, with some sensitivity to masked-function (generic) tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Python code generation on the paper's benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python functions for benchmark problems evaluated by unseen test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>All four formats used. Reported per-format: Prompt Generic Tests (masked function names), Prompt Tests Only, Prompt with Tests, Prompt Only.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Prompt Only, Prompt with Tests, and Prompt Generic Tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall: 209/252 (82.9%). Per-format where reported: Prompt Tests Only: 55/63 (87.3%); Prompt Generic Tests: 51/63 (81.0%); Prompt with Tests: 55/63 (87.3%) (paper implies GPT-4 scored similarly to GPT-3.5 on Tests-only and with-tests).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prompt Tests Only and Prompt with Tests: 55/63 (both ~87.3%); Prompt Generic Tests lower at 51/63 (~81.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Prompt Tests Only (55/63, 87.3%) vs Prompt Generic Tests (51/63, 81.0%) = +4/63 (~+6.35 percentage points) advantage for clearer test information over masked/generic tests.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>GPT-4 can leverage tests to infer requirements; masked function names (generic tests) reduce available explicit cues and thus hurt performance. Authors suggest GPT-4 adapts well across formats but is negatively impacted by ambiguity introduced by masked names.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-format denominator reported as 63 (e.g., 51/63 for Prompt Generic Tests). Overall 252 examples per model across 4 formats. Problems created by multiple models; tests included unseen test cases; each problem evaluated in its own session.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing LLMs on Code Generation with Varying Levels of Prompt Specificity', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9321.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9321.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-2_overall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-2 (evaluated model) performance across prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-2 performs well overall but appears to benefit more from clearer, more specific prompts (Prompt Only and Prompt with Tests).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Python code generation on the paper's benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python functions for benchmark problems; correctness judged by additional unseen test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>All four formats used. Reported per-format: Prompt Only, Prompt with Tests, others included but fewer per-format specifics shown.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Prompt Only, Prompt with Tests, Prompt Tests Only, Prompt Generic Tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall: 204/252 (81.0%). Per-format where reported: Prompt Only: 54/63 (85.7%); Prompt with Tests: 56/63 (88.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Claude scored higher on more specific prompts (Prompt Only 54/63 and Prompt with Tests 56/63) compared to the aggregate low for Prompt Generic Tests.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Prompt with Tests (56/63, 88.9%) vs Prompt Only (54/63, 85.7%) = +2/63 (~+3.2 pp). Compared to aggregate worst-format (Prompt Generic Tests 63.5%), Claude's best (56/63) is ~+25.4 pp, though per-model generic-score not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors conclude Claude benefits from clearer, explicit instructions and contextual examples; higher prompt specificity yields better results for Claude-2 than ambiguous prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-format denominators appear to be 63 (per-format scores reported). Problems evaluated with additional unseen test cases; each problem executed in its own session; 26 base problems × 4 prompt styles = 104 prompts total; results aggregated per model over 252 total evaluations (4×63).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing LLMs on Code Generation with Varying Levels of Prompt Specificity', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9321.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9321.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard_overview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (evaluated model) performance across prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bard showed the weakest performance among the modern models in the study, with consistent but lower scores across all prompt styles and per-format ranges reported instead of exact per-format totals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Python code generation on the paper's benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python functions for benchmark problems evaluated by additional unseen test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>All four formats used; the paper reports Bard's per-format scores as a range rather than exact numbers: scores ranged between 36/63 and 46/63 depending on prompt type.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to GPT-3.5, GPT-4, Claude-2, and text-davinci-002 across the same four prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall per-format range reported: 36/63 (57.1%) to 46/63 (73.0%) across prompt types; exact per-format mapping not fully enumerated in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Bard consistently underperformed compared to GPT-3.5, GPT-4, and Claude-2; no detailed per-format comparisons beyond the range were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors interpret Bard's performance as 'balanced adaptability' but overall weaker capability compared to state-of-the-art models; no detailed causal hypotheses beyond general observations about relative model strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-format denominators appear to be 63; exact per-format Bard scores not fully listed. All problems evaluated with unseen tests, each in its own session.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing LLMs on Code Generation with Varying Levels of Prompt Specificity', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9321.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9321.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextDavinci002_overview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-davinci-002 (evaluated older model) performance across prompt formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>text-davinci-002 performed substantially worse than the other evaluated models on this code-generation benchmark, indicating the rapid progress in LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Python code generation on the paper's benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python functions for benchmark problems; correctness judged by additional unseen test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>All four prompt formats were used in the study; explicit per-format breakdown for this model was not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to GPT-3.5, GPT-4, Claude-2, and Bard across the four prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall: 86/252 (34.1%). Per-format breakdown not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Substantially lower than the other models across formats; no per-format numbers provided to calculate effect sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute poor performance to the model being older/less advanced and possibly tuned differently; suggests such older models may need different prompting strategies to improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Overall denominator 252 for the model (4 formats × 63); per-format details not reported. Problems evaluated with unseen test cases; each in its own session.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing LLMs on Code Generation with Varying Levels of Prompt Specificity', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating the performance of code generation models for solving parsons problems with small prompt variations <em>(Rating: 2)</em></li>
                <li>Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language <em>(Rating: 2)</em></li>
                <li>DS-1000: A natural and reliable benchmark for data science code generation <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9321",
    "paper_id": "paper-265157726",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "AllModels_PromptFormats",
            "name_full": "Aggregate performance of all evaluated LLMs by prompt presentation format",
            "brief_description": "Aggregated comparison across all evaluated LLMs (GPT-3.5, GPT-4, Claude-2, Bard, text-davinci-002) showing how four prompt presentation formats affect code-generation performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "aggregate (GPT-3.5, GPT-4, Claude-2, Bard, text-davinci-002)",
            "model_size": null,
            "task_name": "Python code generation on 104 coding problems (4 prompt types)",
            "task_description": "Generate Python functions for a benchmark of coding problems; correctness evaluated via additional unseen test cases (pass/fail counts).",
            "presentation_format": "Four prompt presentation formats were evaluated: (1) Prompt Only: problem statement only; (2) Prompt with Tests: problem statement + example test cases; (3) Prompt Tests Only (Tests Only): only example test cases (no problem statement); (4) Prompt Generic Tests: tests with masked function names (increased ambiguity).",
            "comparison_format": "The three alternative formats compared were Prompt Only, Prompt with Tests, and Prompt Generic Tests (when analyzing Prompt Tests Only), and vice versa.",
            "performance": "Prompt Tests Only: 231/315 (73.3%); Prompt with Tests: 228/315 (72.4%); Prompt Only: 222/315 (70.5%); Prompt Generic Tests: 200/315 (63.5%).",
            "performance_comparison": "See performance field: Tests Only slightly outperformed Prompt with Tests and Prompt Only; Prompt Generic Tests (masked function names) performed worst.",
            "format_effect_size": "Best vs worst: +31/315 (~+9.8 percentage points) (Prompt Tests Only 73.3% vs Prompt Generic Tests 63.5%); Tests Only vs Prompt with Tests: +3/315 (~+1.0 pp); Tests Only vs Prompt Only: +9/315 (~+2.9 pp).",
            "explanation_or_hypothesis": "Authors hypothesize that providing tests (even without a problem statement) supports LLM deductive reasoning and specification inference, improving code generation; masked function names (Prompt Generic Tests) increase ambiguity and reduce performance by making requirement inference harder. Tests may act analogously to chain-of-thought / worked-examples, guiding reasoning.",
            "null_or_negative_result": false,
            "experimental_details": "Benchmark: 104 coding problems (26 generated by LLMs; each problem presented in 4 prompt styles → 104 prompts). Evaluation aggregated across 5 models for each format (denominators reported as 315 = 63 tasks × 5 models). Each problem evaluated with additional unseen test cases; scoring counted passes/fails. Each problem run in its own LLM session to avoid cross-problem leakage.",
            "uuid": "e9321.0",
            "source_info": {
                "paper_title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5_overall",
            "name_full": "OpenAI GPT-3.5 (evaluated model) performance across prompt formats",
            "brief_description": "GPT-3.5 was evaluated on the coding benchmark and showed high overall performance and strong adaptation to tests-only prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Python code generation on the paper's benchmark",
            "task_description": "Generate Python functions for benchmark problems; correctness measured by unseen test cases.",
            "presentation_format": "All four formats used (Prompt Only; Prompt with Tests; Prompt Tests Only; Prompt Generic Tests). The paper reports per-format scores for GPT-3.5 where available.",
            "comparison_format": "Compared across the three other formats (Prompt Only, Prompt with Tests, Prompt Generic Tests).",
            "performance": "Overall: 210/252 (83.3%). Per-format where reported: Prompt Tests Only: 55/63 (87.3%); Prompt with Tests: 55/63 (87.3%).",
            "performance_comparison": "Prompt Tests Only (55/63) ≈ Prompt with Tests (55/63); other per-format scores not explicitly enumerated for GPT-3.5 in the paper.",
            "format_effect_size": "Prompt Tests Only vs overall worst-format (not directly provided for GPT-3.5); from reported per-format values, Tests-only vs Generic (if Generic were similar to aggregate) would be expected to drop several percentage points. Using reported per-format values: Tests-only (87.3%) vs aggregate worst-format for all models (Prompt Generic Tests 63.5%) implies a potential model-format sensitivity but exact per-model generic-test score not reported.",
            "explanation_or_hypothesis": "GPT-3.5 benefits from test-case driven prompts; tests allow the model to deduce functional requirements. Authors note GPT-3.5 shows strong deductive reasoning on tests-only format.",
            "null_or_negative_result": false,
            "experimental_details": "Per-format denominator appears to be 63 examples (55/63 reported for two formats). Overall denominator 252 (4 formats × 63). Problems presented in separate sessions; additional unseen tests used to evaluate correctness.",
            "uuid": "e9321.1",
            "source_info": {
                "paper_title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4_overall",
            "name_full": "OpenAI GPT-4 (evaluated model) performance across prompt formats",
            "brief_description": "GPT-4 performed similarly to GPT-3.5 overall and showed robustness across prompt styles, with some sensitivity to masked-function (generic) tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Python code generation on the paper's benchmark",
            "task_description": "Generate Python functions for benchmark problems evaluated by unseen test cases.",
            "presentation_format": "All four formats used. Reported per-format: Prompt Generic Tests (masked function names), Prompt Tests Only, Prompt with Tests, Prompt Only.",
            "comparison_format": "Compared to Prompt Only, Prompt with Tests, and Prompt Generic Tests.",
            "performance": "Overall: 209/252 (82.9%). Per-format where reported: Prompt Tests Only: 55/63 (87.3%); Prompt Generic Tests: 51/63 (81.0%); Prompt with Tests: 55/63 (87.3%) (paper implies GPT-4 scored similarly to GPT-3.5 on Tests-only and with-tests).",
            "performance_comparison": "Prompt Tests Only and Prompt with Tests: 55/63 (both ~87.3%); Prompt Generic Tests lower at 51/63 (~81.0%).",
            "format_effect_size": "Prompt Tests Only (55/63, 87.3%) vs Prompt Generic Tests (51/63, 81.0%) = +4/63 (~+6.35 percentage points) advantage for clearer test information over masked/generic tests.",
            "explanation_or_hypothesis": "GPT-4 can leverage tests to infer requirements; masked function names (generic tests) reduce available explicit cues and thus hurt performance. Authors suggest GPT-4 adapts well across formats but is negatively impacted by ambiguity introduced by masked names.",
            "null_or_negative_result": false,
            "experimental_details": "Per-format denominator reported as 63 (e.g., 51/63 for Prompt Generic Tests). Overall 252 examples per model across 4 formats. Problems created by multiple models; tests included unseen test cases; each problem evaluated in its own session.",
            "uuid": "e9321.2",
            "source_info": {
                "paper_title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Claude-2_overall",
            "name_full": "Anthropic Claude-2 (evaluated model) performance across prompt formats",
            "brief_description": "Claude-2 performs well overall but appears to benefit more from clearer, more specific prompts (Prompt Only and Prompt with Tests).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-2",
            "model_size": null,
            "task_name": "Python code generation on the paper's benchmark",
            "task_description": "Generate Python functions for benchmark problems; correctness judged by additional unseen test cases.",
            "presentation_format": "All four formats used. Reported per-format: Prompt Only, Prompt with Tests, others included but fewer per-format specifics shown.",
            "comparison_format": "Compared to Prompt Only, Prompt with Tests, Prompt Tests Only, Prompt Generic Tests.",
            "performance": "Overall: 204/252 (81.0%). Per-format where reported: Prompt Only: 54/63 (85.7%); Prompt with Tests: 56/63 (88.9%).",
            "performance_comparison": "Claude scored higher on more specific prompts (Prompt Only 54/63 and Prompt with Tests 56/63) compared to the aggregate low for Prompt Generic Tests.",
            "format_effect_size": "Prompt with Tests (56/63, 88.9%) vs Prompt Only (54/63, 85.7%) = +2/63 (~+3.2 pp). Compared to aggregate worst-format (Prompt Generic Tests 63.5%), Claude's best (56/63) is ~+25.4 pp, though per-model generic-score not reported.",
            "explanation_or_hypothesis": "Authors conclude Claude benefits from clearer, explicit instructions and contextual examples; higher prompt specificity yields better results for Claude-2 than ambiguous prompts.",
            "null_or_negative_result": false,
            "experimental_details": "Per-format denominators appear to be 63 (per-format scores reported). Problems evaluated with additional unseen test cases; each problem executed in its own session; 26 base problems × 4 prompt styles = 104 prompts total; results aggregated per model over 252 total evaluations (4×63).",
            "uuid": "e9321.3",
            "source_info": {
                "paper_title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Bard_overview",
            "name_full": "Google Bard (evaluated model) performance across prompt formats",
            "brief_description": "Bard showed the weakest performance among the modern models in the study, with consistent but lower scores across all prompt styles and per-format ranges reported instead of exact per-format totals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_size": null,
            "task_name": "Python code generation on the paper's benchmark",
            "task_description": "Generate Python functions for benchmark problems evaluated by additional unseen test cases.",
            "presentation_format": "All four formats used; the paper reports Bard's per-format scores as a range rather than exact numbers: scores ranged between 36/63 and 46/63 depending on prompt type.",
            "comparison_format": "Compared to GPT-3.5, GPT-4, Claude-2, and text-davinci-002 across the same four prompt formats.",
            "performance": "Overall per-format range reported: 36/63 (57.1%) to 46/63 (73.0%) across prompt types; exact per-format mapping not fully enumerated in paper.",
            "performance_comparison": "Bard consistently underperformed compared to GPT-3.5, GPT-4, and Claude-2; no detailed per-format comparisons beyond the range were provided.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors interpret Bard's performance as 'balanced adaptability' but overall weaker capability compared to state-of-the-art models; no detailed causal hypotheses beyond general observations about relative model strengths.",
            "null_or_negative_result": false,
            "experimental_details": "Per-format denominators appear to be 63; exact per-format Bard scores not fully listed. All problems evaluated with unseen tests, each in its own session.",
            "uuid": "e9321.4",
            "source_info": {
                "paper_title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "TextDavinci002_overview",
            "name_full": "OpenAI text-davinci-002 (evaluated older model) performance across prompt formats",
            "brief_description": "text-davinci-002 performed substantially worse than the other evaluated models on this code-generation benchmark, indicating the rapid progress in LLM capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_size": null,
            "task_name": "Python code generation on the paper's benchmark",
            "task_description": "Generate Python functions for benchmark problems; correctness judged by additional unseen test cases.",
            "presentation_format": "All four prompt formats were used in the study; explicit per-format breakdown for this model was not provided in the paper.",
            "comparison_format": "Compared to GPT-3.5, GPT-4, Claude-2, and Bard across the four prompt formats.",
            "performance": "Overall: 86/252 (34.1%). Per-format breakdown not provided.",
            "performance_comparison": "Substantially lower than the other models across formats; no per-format numbers provided to calculate effect sizes.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors attribute poor performance to the model being older/less advanced and possibly tuned differently; suggests such older models may need different prompting strategies to improve results.",
            "null_or_negative_result": false,
            "experimental_details": "Overall denominator 252 for the model (4 formats × 63); per-format details not reported. Problems evaluated with unseen test cases; each in its own session.",
            "uuid": "e9321.5",
            "source_info": {
                "paper_title": "Testing LLMs on Code Generation with Varying Levels of Prompt Specificity",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating the performance of code generation models for solving parsons problems with small prompt variations",
            "rating": 2,
            "sanitized_title": "evaluating_the_performance_of_code_generation_models_for_solving_parsons_problems_with_small_prompt_variations"
        },
        {
            "paper_title": "Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language",
            "rating": 2,
            "sanitized_title": "conversing_with_copilot_exploring_prompt_engineering_for_solving_cs1_problems_using_natural_language"
        },
        {
            "paper_title": "DS-1000: A natural and reliable benchmark for data science code generation",
            "rating": 2,
            "sanitized_title": "ds1000_a_natural_and_reliable_benchmark_for_data_science_code_generation"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01391925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Testing LLMs on Code Generation with Varying Levels of Prompt Specificity
10 Nov 2023</p>
<p>Lincoln Murr lincoln.d.murr@vanderbilt.edu 
Department of Computer Science
Vanderbilt University
Tennessee, NashvilleTNUSA</p>
<p>Morgan Grainger morgan.j.grainger@vanderbilt.edu 
Department of Computer Science
Vanderbilt University
Tennessee, NashvilleTNUSA</p>
<p>David Gao david.gao@vanderbilt.edu 
Department of Computer Science
Vanderbilt University
Tennessee, NashvilleTNUSA</p>
<p>Testing LLMs on Code Generation with Varying Levels of Prompt Specificity
10 Nov 20238A6D8E2EF6A1F0D5DE0CBB1666A43013arXiv:2311.07599v1[cs.SE]Prompt EngineeringArtificial IntelligenceLarge Language Models
Large language models (LLMs) have demonstrated unparalleled prowess in mimicking human-like text generation and processing.Among the myriad of applications that benefit from LLMs, automated code generation is increasingly promising.The potential to transform natural language prompts into executable code promises a major shift in software development practices and paves the way for significant reductions in manual coding efforts and the likelihood of human-induced errors.This paper reports the results of a study that evaluates the performance of various LLMs, such as Bard, ChatGPT-3.5,ChatGPT-4, and Claude-2, in generating Python for coding problems.We focus on how levels of prompt specificity impact the accuracy, time efficiency, and space efficiency of the generated code.A benchmark of 104 coding problems, each with four types of prompts with varying degrees of tests and specificity, was employed to examine these aspects comprehensively.Our results indicate significant variations in performance across different LLMs and prompt types, and its key contribution is to reveal the ideal prompting strategy for creating accurate Python functions.This study lays the groundwork for further research in LLM capabilities and suggests practical implications for utilizing LLMs in automated code generation tasks and test-driven development.</p>
<p>I. INTRODUCTION</p>
<p>Emerging trends and challenges.The advent of large language models (LLMs) are transforming computational science and technology, offering unprecedented capabilities in processing and generating human-like text.LLMs hold promise in applications ranging from natural language understanding to automated code generation.Automated code generation is a field that explores the potential of LLMs to convert natural language prompts into executable computer code, potentially transforming software development by significantly reducing manual coding effort and minimizing human error.For example, 40% of the code generated by GitHub Copilot is being checked in with no modifications [3].Evaluating the effectiveness and versatility of LLMs in code generation tasks is complicated, however, necessitating meticulous exploration and comprehensive testing methods.</p>
<p>The evolution of generative AI has been marked by the release of LLMs, such as Bard, ChatGPT-3.5,ChatGPT-4, and Claude-2, each with unique architectures and capabilities.These LLMs are driving innovations in natural language processing (NLP) and machine learning research in diverse domains.Applying these LLMs to generate code automatically is enabling more intuitive and efficient software development processes, allowing developers to focus on higher-level designs and logic.However, the inherent complexity and variability of natural language pose significant challenges in harnessing the full potential of LLMs in this domain.</p>
<p>Overview of prompt engineering.A key element in leveraging LLMs for code generation is prompt engineering [2], which involves crafting prompts with varying degrees of specificity to elicit desired responses from LLMs.A prompt is typically a statement, question, or instruction given to an LLM to invoke a specific response or action.It guides the LLM in understanding the user's requirements and generating coherent and contextually appropriate responses.For instance, in code generation tasks, a prompt could range from a high-level problem description to a detailed input, output, and functional requirements specification.Understanding the intricacies of prompts and prompt engineering is crucial to enabling the customization of user interactions with LLMs, allowing them to optimize results in accordance with the requirements of a given task.</p>
<p>Prompt specificity can significantly impact the accuracy and efficiency of the output generated by an LLM [10].This specificity refers to the level of detail and clarity provided in the information given to an LLM.A high-specificity prompt may include explicit instructions, detailed requirements, and clear expectations, leaving little room for interpretation or ambiguity.Conversely, a low-specificity prompt is more general, open-ended, and leaves more to the interpretation by the LLM.</p>
<p>The level of specificity in a prompt can significantly impact the output generated by LLMs.A highly specific prompt that provides an LLM with unambiguous directions may yield outputs that are precise, accurate, and closely aligned with user expectations.However, it may also constrain an LLM's creative and generative capabilities, potentially leading to overly restrictive outputs and a lack of innovation or adaptability.</p>
<p>Conversely, a prompt with lower specificity allows an LLM more freedom to interpret user intentions and generate more informative responses.This flexibility can result in more creative, diverse, and adaptable outputs in different contexts.However, it also poses the risk of an LLM generating offtarget, irrelevant, or incorrect outputs due to the lack of clear guidance and constraints.</p>
<p>Overview of test-driven development Test-driven development (TDD) [7] is a widely adopted practice in software engineering where developers write tests before writing the actual code.It is based on developing and refining software by first writing tests to define the desired behavior of the code and then writing code to pass those tests.Integrating TDD principles in prompt engineering can be particularly helpful in shaping LLM responses.</p>
<p>When LLMs are provided with tests as part of the prompts, they can serve as a definitive specification of what the code should accomplish, guiding the model to generate code that meets the stipulated requirements and passes the provided tests.Incorporating these tests adds a layer of specificity and constraint to the prompts, potentially aiding in generating more precise and robust code while also offering insights into LLMs' adaptability and comprehension skills in conforming to established software development practices.Test-enabled prompts can significantly impact the accuracy, efficiency, and reliability of the code generated by LLMs, ensuring that the generated code is syntactically correct, functionally sound, and adherent to the specified behavior.</p>
<p>Summary of research objectives, questions, and contributions.The main objective of our research is to determine the effectiveness of including tests in prompts to LLMs to guide their generation of code.In particular, we assess the performance of popular LLMs, such as ChatGPT-3.5,ChatGPT-4, Bard, and Claude-2, on a diverse set of coding problems, each presented with varying levels of prompt specificity.A secondary objective of this study is to explore the performance of each LLM on these coding problems to evaluate their capabilities.</p>
<p>Central to this study are the following research questions: 1) How do varying levels of prompt specificity influence the quality, accuracy, and efficiency of the code generated by LLMs? 2) Is there a discernible difference in performance across various LLMs in code generation tasks? 3) Does employing a test-driven prompting approach lead to different-possibly more optimized-solutions compared to traditional prompting techniques?Our research focuses on exploring how different levels of prompt specificity impact the performance of LLMs in generating Python code.By examining a diverse range of coding problems and employing varying degrees of prompt specificity, this study evaluates the reasoning capabilities and limitations of leading LLMs such as ChatGPT-3.5,ChatGPT-4, Bard, and Claude-2.Understanding how specificity in prompts influences generated code provides insights into these LLMs' adaptability and comprehension skills, revealing their ability to interpret and respond to varying levels of information detail.</p>
<p>A key contribution of this paper is the illumination of the inherent reasoning capabilities of current LLMs.By interacting with these LLMs using prompts of varying specificity and analyzing the resultant code, we aim to discern how these models can comprehend, interpret, and generate logical and functional Python code.This exploration sheds light on the depth of understanding and logical reasoning these LLMs possess, offering a glimpse into their capabilities in code generation and prompt interpretation.</p>
<p>This paper also provides a well-founded position on the optimal manner of prompting LLMs for generating Python code.By evaluating the accuracy, efficiency, and reliability of the code generated under different prompting strategies, we codify which approach yields the most desirable outcomes.This insight is crucial as it informs best practices in leveraging LLMs for automated code generation, allowing for more effective and reliable utilization of these LLMs in practical software development scenarios.</p>
<p>The findings of this study have implications for both practical applications and future research.By determining the most effective prompting strategies, developers and researchers can optimize the use of LLMs in software development, enhancing productivity and reducing the likelihood of errors.Moreover, this research lays a foundation for further exploration into the capabilities of LLMs, spurring further investigations into how these LLMs can be refined and improved to understand better and generate complex logical programming constructs.</p>
<p>Paper organization.This paper is organized into the following sections:</p>
<p>• introduction (Section I): Provides background on LLMs for code generation and prompt engineering, summarizes the research objectives, questions, and contributions, and gives an overview of the paper structure.</p>
<p>II. RELATED WORK</p>
<p>This section summarizes recent work on prompt engineering and the use of generative AI models for software engineering.</p>
<p>Applying LLMs for software engineering has garnered increasing attention as these transformer-based models have surged in popularity for general usage.Evaluation of different models spans a range of programming tasks such as Parsons Problems [8], CS1 Problems [1], Data Science topics [5], and competitive programming problems [6], among others.</p>
<p>Much research, akin to that by Li et al., delves into training or fine-tuning new models specifically for programming tasks.Preliminary results suggest that these models might even outperform humans in certain programming tasks [6].As models enhance these capabilities, we aspire that general-purpose large language models readily accessible to the masses-like GPT-4, Bard, and Claude-2-can be assessed.This would enable developers to harness their power without the necessity of training bespoke models.</p>
<p>A significant consideration is the application to distinct programming tasks, which consequently alters the evaluation metrics across studies.While several research initiatives have employed existing programming problem evaluation platforms like Codeforces [6] and Leetcode [9], this methodology constrains the variety of problems that can be assessed.It potentially narrows our understanding of the authentic coding proficiencies of large language models.Consequently, Lai et al. instituted a benchmark that evaluates code generated by these models beyond mere functional correctness [5].This approach, among other benchmarks, is meticulously dissected in the survey by Zan et al., which critiques 17 benchmarks and their limitations [13].</p>
<p>The domain of prompt engineering has also piqued interest.This discipline optimizes user input structures to generative models to refine the ensuing output.The rapid evolution of this field is attributed to the realization that outputs from large language models are profoundly influenced by their input sequences and can fluctuate based on various prompting techniques [12].Specifically, Reeves et al. discerned negligible performance variations when altering prompts for programming tasks within the Parson's problem formats [8].In contrast, a separate study by Denny, Kumar, and Giacaman, which primarily fixated on the prompt engineering dimension, revealed that prompt categorizations yielded varied results yet proffered a novel perspective into the potential diversities in prompting [1].</p>
<p>In our pursuit, we focus on the ramifications of prompt specificity by adopting a more systematic categorization approach.Moreover, we intend to assess code generation capabilities across a spectrum of publicly accessible, generalpurpose large language models.Through our efforts, we aspire to deliver a tangible application that benefits a wider audience, building upon the rich tapestry of preceding research endeavors.</p>
<p>III. TESTING METHODOLOGY</p>
<p>This section provides a detailed overview of the testing methodology, including the process for generating coding problems, crafting varied prompt types to assess LLMs under different conditions, the approach to testing and evaluating the LLMs, and the metrics used to quantify performance.</p>
<p>A. Generation of Coding Problems</p>
<p>A standardized approach was employed to conduct a meticulous and comprehensive examination of the adaptability and comprehension skills of leading LLMs-GPT-4, Claude-2, and Bard.Each model was presented with the same prompt, included in the appendix, tasking them to generate coding problems.The objective was to understand how well these models could conceptualize and formulate coding problems when provided with identical instructions.Twenty-six problems, each with four prompts, were created -11 from GPT-4, 10 from Claude-2, and five from Bard.This variety, with the proportion loosely based on the popularity of each model, captures a broader scope of coding questions than one model and provides some protection against an LLM being biased toward its own problems.</p>
<p>B. Creation of Varied Prompts</p>
<p>In addition to creating coding problems, each model was also instructed to generate four distinct prompts for each problem they created.This was done to present each problem in four unique styles, aiming to assess the adaptability and comprehension skills of the LLMs under varying levels of instruction specificity and clarity.The four styles of prompts created, along with an example of the prompts used for the problem of sorting the even integers in an array, were as follows:</p>
<p>1) Prompt Only: This included just the problem statement, providing the minimum amount of information, and evaluated how well the LLMs could interpret and respond to straightforward, unembellished instructions.</p>
<p>Example: Implement a function that takes an array of integers and sorts the even numbers in ascending order while leaving the odd numbers in their original positions.2) Prompt with Tests: This encompassed a problem statement supplemented with example test cases.This style assessed the LLMs' ability to interpret and utilize additional contextual information in the form of test cases to generate code.</p>
<p>Example: Implement a function that takes an array of integers and sorts the even numbers in ascending order while leaving the odd numbers in their original positions.</p>
<p>C. Testing and Evaluation</p>
<p>Once the prompts and their corresponding problems were generated, the LLMs were subjected to a comprehensive series of tests to gauge their performance and correctness accurately.These tests included additional test cases that were not initially part of the original prompts, ensuring the assessment of extensibility.Each problem was generated in its own LLM session to ensure that prior information did not bias the results or provide more insight into the type of problem the generic testing prompts were evaluating.The tests were designed to measure the number of passes and fails for each prompt type, offering insights into the reliability and accuracy of the generated code under different prompting conditions.The data derived from the tests was categorized based on the prompt and LLM solver, enabling a detailed comparative analysis of the performance of each LLM across different prompt types and problem scenarios.</p>
<p>IV. ANALYSIS OF RESULTS</p>
<p>Section IV analyzes the aggregated test results, examining the performance of the LLMs overall and by model and prompt type.It also draws comparisons between the models and discusses key implications of the findings.</p>
<p>A. Analysis of Overall Model Performance by Prompt Style</p>
<p>The aggregated results across all models for the various prompt styles offer intriguing insights into the general adaptability and proficiency of LLMs in code generation tasks:</p>
<p>1) Prompt Tests Only (231/315): This style emerged as the most effective, garnering the highest total score.The fact that the models performed best when provided solely with clearly defined test cases suggests a strong deductive reasoning capability among LLMs.It implies that the models are adept at inferring the requirements of a problem based purely on the expected outcomes and can generate code that aligns with these inferred requirements.It may also mean that prompting with tests is analogous to the "chain-of-thought" prompt engineering format, in which an LLM is given a set of example cases or questions to refine its reasoning abilities further [11].</p>
<p>B. Overall Performance by Model</p>
<p>1) GPT-3.5 stands out as the top performer with a total score of 210/252, closely followed by GPT-4 with 209/252.Claude is not far behind with a total of 204/252.This indicates that these models have a relatively similar and high level of adaptability and comprehension when tasked with code generation under varying prompting styles.This result could be attributed to the relative simplicity of the Python coding problems, as GPT-4 generally outperforms GPT-3.5 in most benchmarks [4].2) Text-davinci-002 clearly lags behind the other models with a considerably lower score of 86/252, suggesting that it might either struggle with the coding tasks presented or require different prompting strategies to optimize its performance.Given that this model is the least advanced, this result was expected.</p>
<p>C. Analysis by Prompt Type</p>
<p>1) Prompt Generic Tests: GPT-4 performs the best in this category with 51/63, indicating its strong capability to adapt to ambiguity and infer requirements even when function names are masked.Given that GPT-4 is one of the state-of-the-art large language models capable of advanced reasoning, it was expected to emerge as the leader.2) Prompt Only: Claude, with a score of 54/63, showcases its proficiency in interpreting and generating code from straightforward, unembellished instructions.3) Prompt Tests Only: Both GPT-3.5 and GPT-4 share the top spot in this category, each scoring 55/63.This suggests that these models possess strong deductive reasoning skills, able to infer problem requirements solely based on test cases and their names.4) Prompt with Tests: Claude, GPT-4, and GPT-3.5 all perform similarly well in this category with scores of 56/63 and 55/63 respectively, indicating their ability to effectively utilize additional contextual information in the form of test cases.</p>
<p>D. Comparing Models</p>
<p>1) Bard showcases consistent performance across all prompt types, with scores ranging from 36/63 to 46/63.Its performance indicates a balanced adaptability to different prompting styles and overall poor performance compared to other state-of-the-art models.2) Claude excels particularly when provided with more specific prompts such as "Prompt Only" and "Prompt with Tests," suggesting that it benefits from clearer and more detailed instructions.3) GPT-4 and GPT-3.5 display remarkable adaptability across all prompt styles, indicating their robustness and versatility in code generation tasks.4) Text-davinci-002 demonstrates its age, as its performance is significantly below the other models across all prompting styles.</p>
<p>E. Implications</p>
<p>The results imply that while all models have demonstrated the capability to generate code based on varying prompt styles, there are clear distinctions in their adaptability and proficiency.It suggests that while models like GPT-3.5 and GPT-4 are highly versatile and can handle a wide range of prompting styles effectively, models like Claude might benefit more from explicit and detailed instructions.Text-davinci-002's performance emphasizes the rapid development speed in LLMs and the previous importance placed on optimizing for either code or text-based tasks.</p>
<p>V. FUTURE WORK</p>
<p>The findings reported in this paper assess how varying levels of prompt specificity influence the performance of leading LLMs in generating Python code.While our research has provided valuable insights into these models' adaptability and comprehension skills, several avenues remain unexplored, offering potential for further research and deeper understanding.Here are some promising directions for future work:</p>
<p>A. Exploration of Different LLMs</p>
<p>While this study primarily focused on models like GPT-3.5, GPT-4, Claude-2, and Bard, the landscape of Large Language Models is vast and constantly evolving.Future research could explore the performance of other LLMs, including Llama 2 and Google's yet-to-be-released Gemini, to provide a more comprehensive perspective on the capabilities and limitations of different models in automated code generation tasks.</p>
<p>B. Testing in Different Programming Languages</p>
<p>Our research predominantly concentrated on Python code generation.However, the versatility and adaptability of LLMs could be further tested by exploring code generation in different programming languages.This would elucidate whether the findings of our study are specific to Python or generalizable across various languages, offering insights into the universality of LLM capabilities.Evaluating the impact of prompt specificity on different language architectures, like functional languages, could also provide helpful insight into the reasoning ability of LLMs across programming paradigms.Moreover, looking at less popular languages could prove interesting, as the LLMs' abilities to reason on generic tests may not exist in a language that the LLM has less training data about.</p>
<p>C. Examination of More Complex Problems</p>
<p>The coding problems employed in this study were diverse yet generally simple one-function programs.There is potential to delve into more intricate and complex problems in future investigations.By challenging LLMs with multifaceted problems that require deeper logic, advanced algorithms, or specialized knowledge, we could gauge the depth of understanding, logical reasoning, and problem-solving skills of these models.</p>
<p>D. Experimentation with Varied Prompt Types</p>
<p>Our study employed four distinct styles of prompts.Future research could experiment with different types of prompts, exploring varying degrees of specificity, ambiguity, or even introducing elements like visual cues, multi-modal inputs, or real-world context.This would allow for a more granular examination of how LLMs interpret and respond to diverse and unconventional prompting strategies.</p>
<p>E. Exploring existing LLM Benchmarks</p>
<p>There are numerous ways to benchmark a coding problem output, and several have been created specifically for LLMs.It would be beneficial to analyze Large Language Model output on various grading criteria, including style, space complexity, and time complexity.</p>
<p>VI. CONCLUDING REMARKS</p>
<p>The following are the lessons we learned from conducting this research: • Vast opportunities exist to build on this research.While our examination was limited to Python and constrained problem scopes, the insights showed existing LLMs' reasoning skills and limitations.There remains ample opportunity for future work to expand the breadth and depth of LLM evaluations through more complex problems, diverse languages, unconventional prompts, and emerging models.Another area to explore more deeply would be including different levels of context within the prompt types, such as other numbers of test cases, and following a more traditional chain-of-thought prompting approach modified for code generation.As LLMs evolve at a remarkable pace, prompt engineering will play an increasingly crucial role in effectively harnessing these models and realizing their full potential across a vast array of real-world applications.• Of the models examined, GPT-3.5 and GPT-4 display remarkable proficiency across all prompt types, showcasing robust adaptability.Meanwhile, Claude performs well when provided with precise prompts, suggesting it benefits from more explicit instructions.The outdated Textdavinci-002 significantly lags behind all other models, confirming the rapid evolution of LLM architectures.• Set realistic expectations when applying LLMs.Despite impressive advances, LLMs may still generate logically unsound code or fail on atypical edge cases.Judicious human oversight is critical, including testing, validation, analyzing generated code quality, and handling exceptions.Use LLMs to accelerate development, but employ proper code reviews and monitoring.This study presented a comprehensive investigation into the performance of popular LLMs, such as Bard, Claude, GPT-3.5, and GPT-4, in generating Python code based on prompts with varying levels of specificity.By carefully designing a benchmark encompassing 104 coding problems and four distinct prompt types, we systematically evaluated the accuracy, efficiency, and reliability of the code generated by these models under different prompting conditions.</p>
<p>VII. ACKNOWLEDGMENTS</p>
<p>This paper used GPT-4 for assistance in generating content, code, and prompts, with original prompts created by Lincoln Murr.</p>
<p>Prompt Generic Tests (200/315): This style, characterized by test cases with masked function names, posed the most challenges for the models, resulting in the lowest overall score.The reduced score in this category underscores the difficulty LLMs face when confronted with ambiguity.Without explicit function names or a clear problem statement, the models are tasked with deducing the problem's requirements and dealing with the added layer of uncertainty introduced by the masked function names.These tests were more intended to test the deductive reasoning ability of LLMs and less as a model for prompt engineers and LLM users to utilize.
contextually relevant code. Given the close score toprompting with tests alone, it is difficult to make aconclusive judgment on which format is better, but itmight be hypothesized that the prompt with the testcould provide conflicting or ambiguous information thatis not present when tests alone are inputted.3) Prompt Only (222/315): With just the problem state-ment provided, the models still showcased strongperformance, albeit slightly lower than when testswere included. This indicates that while LLMs caneffectively interpret and respond to straightforwardproblem descriptions, the additional context of testcases-whether provided alone or alongside the problemstatement-enhances their code generation capabilities.4)2) Prompt with Tests (228/315): Closely following the"Prompt Tests Only" style, this approach combiningproblem statements with example test cases also provedhighly effective. Combining descriptive instructions andpractical examples might offer a balance of guidanceand context, enabling LLMs to generate accurate and</p>
<p>Ambiguity poses challenges for LLMs.Masked function names reduced performance, indicating difficulty adapting to uncertainty about specific requirements.While this was expected, it offers a benchmark by which future models can be evaluated on reasoning and deduction.
• Prompting with tests enhances LLM code generationcapabilities. LLMs can effectively leverage test casesto infer requirements and generate more accurate code,whether tests are provided alone or with problem state-ments. Consequently, software engineers utilizing AI-assisted tools for generating code would benefit fromincluding test cases when prompting LLMs and adheringto a TDD approach throughout the development lifecycle.
•</p>
<p>Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. Paul Denny, Viraj Kumar, Nasser Giacaman, 2022</p>
<p>Prompt engineering for chatgpt: A quick guide to techniques, tips, and best practices. Sabit Ekin, 042023</p>
<p>Scott Guthrie, Morgan stanley tmt conference. 2023</p>
<p>Gpt-4 vs. gpt-3.5: A concise showdown. Anis Koubaa, 032023</p>
<p>DS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, Tao Yu, Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy ; Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Cyprien de Masson. 2022378</p>
<p>Test driven development: A review. Muhammad Usama, Pervez , Laiba Eman, 072022</p>
<p>Evaluating the performance of code generation models for solving parsons problems with small prompt variations. Brent Reeves, Sami Sarsa, James Prather, Paul Denny, Brett A Becker, Arto Hellas, Bailey Kimmel, Garrett Powell, Juho Leinonen, Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1, ITiCSE 2023. the 2023 Conference on Innovation and Technology in Computer Science Education V. 1, ITiCSE 2023New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Extending the frontier of chatgpt: Code generation and debugging. Saadat Fardin Ahsan Sakib, A H M Hasan Khan, Rezaul, Karim, 2023</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang, Prompting gpt-3 to be reliable. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, Douglas C Schmidt, 2023</p>
<p>Large language models meet NL2Code: A survey. Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, Jian-Guang Lou, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231VIII. APPENDIX</p>
<p>The GitHub repository available at github.com/murrlincoln/ SWE-AI-Mini-Research contains the test cases, prompts, and other relevant information. presented in this paper</p>            </div>
        </div>

    </div>
</body>
</html>
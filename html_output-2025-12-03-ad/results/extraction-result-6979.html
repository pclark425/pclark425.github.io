<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6979 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6979</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6979</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269032862</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.06911v1.pdf" target="_blank">GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism</a></p>
                <p><strong>Paper Abstract:</strong> Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cum-bersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6979.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6979.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized Graph Sequence with Special Tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential text encoding of a knowledge graph obtained by serializing triples into a token sequence augmented with special markers (e.g. <Graph>, <H>, <R>, <T>) so that pretrained language models can be fine-tuned directly on graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized Graph with special tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each KG triple (head, relation, tail) is converted into a text span by prepending entity/relation tokens with special markers: <Graph> to mark graph input, and for each triple <H> head, <R> relation, <T> tail. Triples are concatenated in the dataset's default triple order to form the input sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token‑based (lossy)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Triple-wise serialization / concatenation in the order triples appear in the dataset (dataset-default ordering); no traversal like DFS/BFS is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.1 (unconstrained and constrained splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text (graph-to-text) generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (baseline fine-tuned on linearized graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-large encoder-decoder Transformer used as baseline; in this paper baseline T5-large is reported with ~737M trainable parameters in their comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ROUGE-L, chrF++ (automatic); human eval Fluency and Meaning (Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline T5-large (linearized graph input) on WebNLG unconstrained: BLEU 61.41, METEOR 45.96, ROUGE-L 71.70, chrF++ 75.27. Human eval: Fluency 5.57, Meaning 5.41 (Likert 1-7).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows direct fine-tuning of pretrained LMs without adding graph encoders; straightforward input formatting for PLMs. Enables fluent text generation but is prone to hallucination on short inputs per the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy with respect to explicit graph connectivity (edge relationships and global structure can be obscured), can lead to hallucinations and omission of graph facts, misaligned with PLM pretraining on plain text (authors note potential forgetting), and no canonical ordering is enforced by default.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared in the paper to GNN-based encodings and the proposed hierarchical + GraSAME approach: linearization is simple and effective for fluency but loses explicit connectivity; GraSAME augments linearization with token-level graph structure to preserve connectivity and reduce hallucination while requiring fewer extra trainable parameters than some SOTA pretraining-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6979.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6979.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-level Hierarchy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level Hierarchical Graph Structure (derived from linearized graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-granular hierarchical graph built from the linearized triple sequence: each token is a node and typed, bidirectional edges (five relation types r1..r5) encode hierarchical and local connectivity to preserve graph structure for GNN encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level Hierarchical Graph Structure</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Starting from the linearized text sequence, each token becomes a graph node. Specific relation types define edges: r1 connects a global <Graph> node to special tokens (<H>, <R>, <T>); r2 connects adjacent special tokens within a triple; r3 links special tokens to their entity tokens; r4 links consecutive tokens within entities; r5 links special tokens that share the same entity. Edges are bidirectional to enable two-way message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical, token‑based, graph‑preserving</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Constructed deterministically from the linearized triple sequence by creating the five relation-type edge groups (r1..r5) between tokens as defined; not produced via DFS/BFS traversal but derived from triple structure and token positions.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.1 (unconstrained and constrained splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text (graph-to-text) generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large + GraSAME (GraSAME-SAGE / GraSAME-GAT / GraSAME-RGCN variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-large encoder-decoder (pretrained) with GraSAME: a graph-guided self-attention module that embeds token-level hierarchical graph via a GNN (GraphSAGE/GAT/RGCN) inside encoder self-attention. PLM parameters are frozen; only the GNN/GraSAME components are trained. Reported GraSAME-SAGE configuration has ~151M trainable parameters in the paper's table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ROUGE-L, chrF++; human evaluation (Fluency, Meaning); ablations (BLEU/METEOR); graph-size broken down BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GraSAME-SAGE on WebNLG unconstrained: BLEU 65.55, METEOR 48.38, ROUGE-L 74.55, chrF++ 77.34. Human evaluation: Fluency 5.56, Meaning 5.62 (Likert 1-7), meaning +0.21 vs baseline T5. Ablation: removing bidirectional edges or graph-reconstruction loss reduces BLEU/METEOR (example: -bidirectional edges BLEU 60.74, METEOR 45.13). Graph-size analysis: BLEU improvements over T5 for many graph sizes (e.g., +8.61 BLEU on 7-triple inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables PLMs to incorporate token-level structural information during fine-tuning without concatenating separate graph embeddings; reduces number of trainable parameters compared to training full PLM (authors report >100M parameter savings vs some SOTA). However, training converges slower because the GNN is not pre-trained (paper reports slower convergence even though overall training is fast). Improves factual fidelity (reduces hallucinations) and improves BLEU/METEOR especially on complex graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires extracting a hierarchical token graph from linearized input which may need dataset/task-specific adjustments (not a universal template). Still relies on the linearized graph in the input (so misaligned with pretraining on plain text). GNN component is not pre-trained leading to slower convergence and need for extra fine-tuning epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Designed to synthesize strengths of linearization (direct PLM fine-tuning) and GNN encodings (explicit structural preservation). Compared to pure linearization (T5 baseline) it yields higher BLEU/METEOR and less hallucination; compared to other GNN+PLM approaches or methods that require additional pretraining (e.g., KGPT, JointGT), GraSAME achieves comparable performance while using fewer trainable parameters and not requiring extra pretraining tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>JointGT: Graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>KGPT: Knowledge-grounded pretraining for data-to-text generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6979",
    "paper_id": "paper-269032862",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Linearized Graph",
            "name_full": "Linearized Graph Sequence with Special Tokens",
            "brief_description": "A sequential text encoding of a knowledge graph obtained by serializing triples into a token sequence augmented with special markers (e.g. &lt;Graph&gt;, &lt;H&gt;, &lt;R&gt;, &lt;T&gt;) so that pretrained language models can be fine-tuned directly on graph inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Linearized Graph with special tokens",
            "representation_description": "Each KG triple (head, relation, tail) is converted into a text span by prepending entity/relation tokens with special markers: &lt;Graph&gt; to mark graph input, and for each triple &lt;H&gt; head, &lt;R&gt; relation, &lt;T&gt; tail. Triples are concatenated in the dataset's default triple order to form the input sequence.",
            "representation_type": "sequential, token‑based (lossy)",
            "encoding_method": "Triple-wise serialization / concatenation in the order triples appear in the dataset (dataset-default ordering); no traversal like DFS/BFS is applied.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.1 (unconstrained and constrained splits)",
            "task_name": "KG-to-text (graph-to-text) generation",
            "model_name": "T5-large (baseline fine-tuned on linearized graphs)",
            "model_description": "T5-large encoder-decoder Transformer used as baseline; in this paper baseline T5-large is reported with ~737M trainable parameters in their comparison table.",
            "performance_metric": "BLEU, METEOR, ROUGE-L, chrF++ (automatic); human eval Fluency and Meaning (Likert)",
            "performance_value": "Baseline T5-large (linearized graph input) on WebNLG unconstrained: BLEU 61.41, METEOR 45.96, ROUGE-L 71.70, chrF++ 75.27. Human eval: Fluency 5.57, Meaning 5.41 (Likert 1-7).",
            "impact_on_training": "Allows direct fine-tuning of pretrained LMs without adding graph encoders; straightforward input formatting for PLMs. Enables fluent text generation but is prone to hallucination on short inputs per the paper's analysis.",
            "limitations": "Lossy with respect to explicit graph connectivity (edge relationships and global structure can be obscured), can lead to hallucinations and omission of graph facts, misaligned with PLM pretraining on plain text (authors note potential forgetting), and no canonical ordering is enforced by default.",
            "comparison_with_other": "Compared in the paper to GNN-based encodings and the proposed hierarchical + GraSAME approach: linearization is simple and effective for fluency but loses explicit connectivity; GraSAME augments linearization with token-level graph structure to preserve connectivity and reduce hallucination while requiring fewer extra trainable parameters than some SOTA pretraining-based approaches.",
            "uuid": "e6979.0",
            "source_info": {
                "paper_title": "GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Token-level Hierarchy",
            "name_full": "Token-level Hierarchical Graph Structure (derived from linearized graph)",
            "brief_description": "A token-granular hierarchical graph built from the linearized triple sequence: each token is a node and typed, bidirectional edges (five relation types r1..r5) encode hierarchical and local connectivity to preserve graph structure for GNN encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Token-level Hierarchical Graph Structure",
            "representation_description": "Starting from the linearized text sequence, each token becomes a graph node. Specific relation types define edges: r1 connects a global &lt;Graph&gt; node to special tokens (&lt;H&gt;, &lt;R&gt;, &lt;T&gt;); r2 connects adjacent special tokens within a triple; r3 links special tokens to their entity tokens; r4 links consecutive tokens within entities; r5 links special tokens that share the same entity. Edges are bidirectional to enable two-way message passing.",
            "representation_type": "hierarchical, token‑based, graph‑preserving",
            "encoding_method": "Constructed deterministically from the linearized triple sequence by creating the five relation-type edge groups (r1..r5) between tokens as defined; not produced via DFS/BFS traversal but derived from triple structure and token positions.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.1 (unconstrained and constrained splits)",
            "task_name": "KG-to-text (graph-to-text) generation",
            "model_name": "T5-large + GraSAME (GraSAME-SAGE / GraSAME-GAT / GraSAME-RGCN variants)",
            "model_description": "T5-large encoder-decoder (pretrained) with GraSAME: a graph-guided self-attention module that embeds token-level hierarchical graph via a GNN (GraphSAGE/GAT/RGCN) inside encoder self-attention. PLM parameters are frozen; only the GNN/GraSAME components are trained. Reported GraSAME-SAGE configuration has ~151M trainable parameters in the paper's table.",
            "performance_metric": "BLEU, METEOR, ROUGE-L, chrF++; human evaluation (Fluency, Meaning); ablations (BLEU/METEOR); graph-size broken down BLEU",
            "performance_value": "GraSAME-SAGE on WebNLG unconstrained: BLEU 65.55, METEOR 48.38, ROUGE-L 74.55, chrF++ 77.34. Human evaluation: Fluency 5.56, Meaning 5.62 (Likert 1-7), meaning +0.21 vs baseline T5. Ablation: removing bidirectional edges or graph-reconstruction loss reduces BLEU/METEOR (example: -bidirectional edges BLEU 60.74, METEOR 45.13). Graph-size analysis: BLEU improvements over T5 for many graph sizes (e.g., +8.61 BLEU on 7-triple inputs).",
            "impact_on_training": "Enables PLMs to incorporate token-level structural information during fine-tuning without concatenating separate graph embeddings; reduces number of trainable parameters compared to training full PLM (authors report &gt;100M parameter savings vs some SOTA). However, training converges slower because the GNN is not pre-trained (paper reports slower convergence even though overall training is fast). Improves factual fidelity (reduces hallucinations) and improves BLEU/METEOR especially on complex graphs.",
            "limitations": "Requires extracting a hierarchical token graph from linearized input which may need dataset/task-specific adjustments (not a universal template). Still relies on the linearized graph in the input (so misaligned with pretraining on plain text). GNN component is not pre-trained leading to slower convergence and need for extra fine-tuning epochs.",
            "comparison_with_other": "Designed to synthesize strengths of linearization (direct PLM fine-tuning) and GNN encodings (explicit structural preservation). Compared to pure linearization (T5 baseline) it yields higher BLEU/METEOR and less hallucination; compared to other GNN+PLM approaches or methods that require additional pretraining (e.g., KGPT, JointGT), GraSAME achieves comparable performance while using fewer trainable parameters and not requiring extra pretraining tasks.",
            "uuid": "e6979.1",
            "source_info": {
                "paper_title": "GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity",
            "rating": 2,
            "sanitized_title": "have_your_text_and_use_it_too_endtoend_neural_datatotext_generation_with_semantic_fidelity"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "KGPT: Knowledge-grounded pretraining for data-to-text generation",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 1,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        }
    ],
    "cost": 0.011806749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism
10 Apr 2024</p>
<p>Shuzhou Yuan shuzhou.yuan@kit.edu 
Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) TU Dresden
Karlsruhe Institute of Technology</p>
<p>Michael Färber michael.faerber@kit.edu 
Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) TU Dresden
Karlsruhe Institute of Technology</p>
<p>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism
10 Apr 2024B79292486051DC3D0A2692B886518A49arXiv:2404.06911v1[cs.CL]
Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks.However, bridging the modality gap between graph structures and text remains a significant challenge.Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs.In this work, we propose a novel graphguided self-attention mechanism, GraSAME.GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts.As an end-to-end, lightweight multimodal module, GraSAME follows a multitask learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs.Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets.Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.</p>
<p>Introduction</p>
<p>The paradigm of pre-training and fine-tuning has increasingly become the standard approach for leveraging the inherent knowledge of language models in a wide range of Natural Language Processing (NLP) tasks (Xu et al., 2021).Pretrained Language Models (PLMs) like Transformer (Vaswani et al., 2017), T5 (Raffel et al., 2020), and GPT (Brown et al., 2020), which are trained on extensive text corpora, have demonstrated remarkable performance across various NLP challenges.However, these models primarily focus on textual data, presenting a significant limitation in processing structured information, such as Knowledge Graphs (KGs), molecular graph and social networks.Such graph structures are crucial for storing external knowledge and can significantly enhance PLM's performance on knowledge-driven tasks (Zhang et al., 2019;Peters et al., 2019).This limitation becomes particularly evident in tasks that require a deep understanding of both textual and structural data, such as graph-to-text generation (Gardent et al., 2017), KG-based fact checking (Kim et al., 2023), and translation between molecules and natural language (Edwards et al., 2022).</p>
<p>To address the challenge of processing structural input in PLMs, recent research has explored two main strategies: linearizing graph structures into text sequences (Harkous et al., 2020;Ribeiro et al., 2021a;Schmitt et al., 2021), and encoding structural information using Graph Neural Networks (GNNs) (Yao et al., 2020;Ribeiro et al., 2021b;Li et al., 2021;Zhang et al., 2022).While linearization allows direct fine-tuning of PLMs, studies have shown that it often fails to preserve the inherent structural information and explicit node connectivity (Song et al., 2018;Ribeiro et al., 2019).Conversely, while GNNs effectively encode complex structures, the modality difference between text and graphs complicates the integration with PLMs, requiring additional training for aligning and concatenating embeddings from graph and textual modality (Li et al., 2021;Zhang et al., 2022).</p>
<p>To merge the strengths of PLMs and GNNs, we introduce GraSAME, a novel Graph-guided Self-Attention MEchanism, enhancing PLMs' ability to process graph inputs.As depicted in Figure 1, we construct a token-level hierarchical graph structure from the linearized graph sequence to maintain the input graph's structural integrity.GraSAME is designed to learn the token-level graph information from a GNN and integrate it seamlessly into the text representation for PLM.Based on a standard transformer architecture, we substitute the Figure 1: An example of KG-to-text generation.We visualize the hierarchical graph structure derived from the linearized graph input, tokenized by T5-large tokenizer.Each token from the input text is depicted as a node.Various relation types, each indicated by a unique color, are assigned between nodes to establish the hierarchical structure and ensure effective information flow among neighboring nodes.</p>
<p>self-attention layers in the encoder with GraSAME.As GraSAME effectively encodes the graph structure, we train only its parameters while keeping the PLM's parameters frozen.This approach enables PLMs equipped with GraSAME to simultaneously process structural and textual information dynamically, eliminating the need for complex alignment or concatenation of different modalities.</p>
<p>Applied to KG-to-text generation task WebNLG, we integrate GraSAME into the encoder-decoder model T5 with multi-task fine-tuning.The KG-totext generation is particularly suitable as it necessitates the processing of both graph and text information, providing a clear intuition to assess the effectiveness of GraSAME.Our experiments demonstrate that GraSAME is compatible with various GNN architectures such as GraphSAGE (Hamilton et al., 2017), GAT (Veličković et al., 2018) and RGCN (Schlichtkrull et al., 2018), yielding performance that not only surpasses baseline models but also comparable to state-of-the-art (SOTA) models.Moreover, GraSAME effectively integrates structural information purely during fine-tuning and saves over 100 million trainable parameters.</p>
<p>In summary, our contributions are: i) Introducing a novel graph-guided attention mechanism GraSAME to incorporate explicit structural information into PLMs.This innovation enables PLMs to process both textual and structural inputs smoothly, bridging the modality gap of GNNs and PLMs.With GraSAME, PLMs can dynamically interact with GNNs, effectively interpreting graph inputs, which is crucial for NLP tasks that require structural information.ii) Applying GraSAME to KG-to-text generation on WebNLG datasets, achieving results comparable to SOTA models while saving over 100 million trainable parameters.</p>
<p>Related Work</p>
<p>Structural Information for PLMs.Although PLMs inherit linguistic structure information from pre-training (Nie et al., 2024), external structural information helps PLMs enhance their ability to understand the syntax of natural language (Yang et al., 2022), summarize source code (Choi et al., 2021) and generate better text (Song et al., 2020).Much initial focus of infusing structural information into PLMs has been on modifying pre-training objectives (Peters et al., 2019;Xiong et al., 2020;He et al., 2020).Zhang et al. (2019) utilized both textual corpora and KGs to pre-train an enhanced language representation model.Ke et al. (2021) proposed three new pre-training tasks to explicitly enhance the graph-text alignment.Also, recent efforts increasingly aimed at injecting structural information into PLMs during fine-tuning for various NLP tasks (Yasunaga et al., 2021;Ribeiro et al., 2021b).Wang et al. (2021) proposed K-Adapter to infuse knowledge into PLMs.Zhang et al. (2022) came up with GreaseLM model to utilise KG information for question answering.</p>
<p>KG-to-text Generation.Previous approaches to enabling PLMs to process graph inputs often relied on linearizing the input graph into a text sequence (Harkous et al., 2020;Mager et al., 2020;Ribeiro et al., 2021b;Colas et al., 2022).Ribeiro et al. (2021a) investigated PLMs on graph-to-text generation using linearized graphs and found that this method is effective, yet results in the loss of specific edge connections in graphs (Song et al., 2018;Beck et al., 2018;Ribeiro et al., 2019).Also, Yuan and Faerber (2023) evaluated generative models using linearized graph and uncovered the issues of hallucinations in the models.An alternative approach to encode the graph inputs is leveraging GNNs (Koncel-Kedziorski et al., 2019;Ribeiro et al., 2020).But this method typically entails additional steps such as aligning modalities and concatenating embeddings (Li et al., 2021), which adds complexity to the development of a seamless end-to-end pipeline for integrating GNN with PLM.Diverging from the previous methods, our work synthesizes the strengths of both linearized graph and GNN.Moreover, GraSAME also follows a lightweight fine-tuning avoiding updating the parameters of the whole model, inspired by the adapter and parameter-efficient fine-tuning approaches (Houlsby et al., 2019;Ribeiro et al., 2021b;Wang et al., 2021;Yuan et al., 2024).</p>
<p>Model</p>
<p>In this section, we detail the components of our model.Theoretically, GraSAME is adaptable to any attention-based PLMs.We choose T5 model (Raffel et al., 2020) as our foundation due to its encoder-decoder architecture, which is well-suited for KG-to-text generation.</p>
<p>Encoder-Decoder Model</p>
<p>Encoder-decoder model, such as T5, is a classic Transformer model consisting of encoder and decoder layers.Each encoder layer includes two distinct sublayers: a self-attention mechanism and a position-wise fully connected feed-forward network.The self-attention mechanism utilizes h distinct attention heads.Consider a conditional generation task such as KG-to-text generation, where the input is a sequence of tokens x = (x 1 , . . ., x n ) with each x i ∈ R dx , and the aim is to generate target sequence of tokens y = (y 1 , . . ., y n ).The attention head processes an input sequence, the outputs of all attention heads are merged via concatenation, followed by a parameterized linear transformation to yield the final output of the self-attention sublayer.The computation of each output element z i , with each z i ∈ R dz , involves a weighted sum of linearly transformed input elements, defined as:
z i = n j=1 α ij (x j W V ),(1)
where α ij represents the weight coefficient, calculated using a softmax function:
α ij = softmax( (x i W Q )(x j W K ) T √ d k ).(2)
The matrices W V , W Q , W K ∈ R dx×dz are layerspecific trainable parameters, and are distinct for each attention head.</p>
<p>Graph-guided Self-Attention Mechanism</p>
<p>Self-attention allows for the interaction of token representations by treating each input sequence as a fully-connected graph with tokens as nodes (Yao and Wan, 2020).However, this process does not retain the original structural information and explicit connectivity between the tokens.To address this issue, we introduce GraSAME, a method that integrates text with token-level hierarchical graph representation illustrated in Figure 1.</p>
<p>Architecture of GraSAME</p>
<p>GraSAME involves incorporating a GNN within the self-attention layer of the PLM.This addition enables the direct encoding of the hierarchical graph structure and facilitates the smooth transfer of structural information into the PLM.We visualize the architecture in Figure 2. Graph Neural Network.The graph neural network models structural characteristics of the input graph effectively by using various graph convolutional layers.The primary goal of the GNN is to learn the representations for both individual nodes and the overall graph structure.In most GNN models, the node representation is updated iteratively by aggregating the representations of its neighboring nodes.The representation of node v i at the l-th layer is represented by h (l) , with the initial representation h (0) set to the node's feature vector x i .The process of representation update at the l-th This process generates a graph embedding x , which subsequently induces the Q vector.The Q vector then guides the self-attention mechanism to produce a graphaware representation z.The visualization of GNN is taken from GraphSAGE (Hamilton et al., 2017).layer involves two main operations:
a (l) = AGGREGATE (l) {h (l−1) j : j ∈ N (v i )} , (3) h (l) i = COMBINE (l) h (l−1) i , a (l) ,(4)
where N (v i ) denotes the neighbors of v i .The AGGREGATE function compiles message passing from these neighbors, using techniques like MEAN, MAX, or SUM, varying with the GNN architecture.The COMBINE function then integrates this aggregated information into the node's current representation, thereby updating it.</p>
<p>Incorporating Method.Drawing inspiration from the multimodal self-attention layer by Yao and Wan (2020), we present a graph-guided selfattention architecture designed to simultaneously encode text representation and hierarchical graph structure.In our approach, all tokens in the input text are treated as nodes, with their initial features derived from the token representations in h (l) .As shown in Figure 2, the token representations are aggregated and updated through a GNN layer.This process generates the vector Q, which subsequently guides the self-attention layer in the encoder of the PLM.</p>
<p>Formally, we adapt Equation 2 such that the weight coefficient αij is derived from the node representation xi in the graph modality, and the token representation x j from the text modality:
αij = softmax ( xi W Q )(x j W K ) T √ d k . (5)
The output of the self-attention layer is then calculated as:
zi = n j=1 αij (x j W V ). (6)
This modification ensures that the hidden word representations are influenced by the graph embedding.</p>
<p>In each encoder layer of the model, we incorporate residual connections and layer normalization.The standard self-attention layer in the encoder is replaced with GraSAME, while the decoder retains the standard Transformer implementation.In the encoder's final layer, zi serves as the input to the decoder, which generates the target sequence.</p>
<p>Graph Representation</p>
<p>As PLMs are designed to process textual input only, it becomes necessary to perform certain preprocessing steps when addressing graph-based NLP tasks.</p>
<p>Considering the task of KG-to-text generation, we represent the graph input as a linearized graph following prior studies (Harkous et al., 2020;Ribeiro et al., 2021a), and also extract a token-level hierarchical graph structure to ensure information flow among neighboring nodes.Linearized Graph.In line with Ribeiro et al. (2021a), we linearize the graph into a sequence of text augmented with special tokens.As depicted in Figure 1, a KG triple is composed of a head, relation, and tail entity.Accordingly, we prepend each entity with special tokens: <H>, <R>, <T >.Furthermore, to distinguish between text and graph inputs, we introduce the <Graph> token.1 Previous work (Ribeiro et al., 2021a) suggests that PLMs generate fluent text regardless of the linearization order of the graph.Hence we adhere to the default sequence in which triples appear in the dataset.</p>
<p>Hierarchical Graph Structure.We derive a token-level hierarchical graph structure from the linearized graph, as depicted in Figure 1.The concept of this hierarchy is inspired by the treelike structures observed in functional organizations such as companies, universities, and even insect societies (Pooler, 2017;Anderson et al., 2001), and has been explored in neural network research for knowledge representation as well (Ying et al., 2018;Moutsinas et al., 2021;Chen et al., 2020).In this structure, each token dynamically interacts within its hierarchical tier through message passing, which we believe helps preserve the graph's original explicit connectivity and ensures effective information flow among neighboring nodes.Formally, let's consider a graph G = (V, E, R), where V is the set of nodes and E comprises labeled edges of the form (u, r, v).Here, u and v represent nodes in V, and r from R denotes the relation type.In our structure, each token in the linearized graph is considered as a node, and we define specific relation types r between two nodes to enhance the hierarchy:</p>
<ol>
<li>
<p>r 1 connects the global node <Graph> to special tokens <H>, <R>, <T >.</p>
</li>
<li>
<p>r 2 links adjacent special tokens within a triple.</p>
</li>
<li>
<p>r 3 connects special tokens to their respective entity tokens.</p>
</li>
<li>
<p>r 4 joins consecutive tokens within entities.</p>
</li>
<li>
<p>r 5 associates special tokens sharing the same entity.</p>
</li>
</ol>
<p>An example of such a hierarchical graph is presented in Figure 1.We design the edges to be bidirectional, as this approach of information propagation in multiple directions can enhance the model's performance (Yao et al., 2020).</p>
<p>4 Multi-Task Fine-Tuning</p>
<p>Our training approach employs a multi-task learning strategy with efficient, lightweight fine-tuning.</p>
<p>We initialize the model with pretrained parameters denoted by ϕ.The parameters of the PLM remain frozen, and only the GNN component within GraSAME is updated, given its effective encoding of graph structure.</p>
<p>Training Objectives</p>
<p>We retain the standard language model objective of generating the next token in a sequence while introducing an additional graph reconstruction task.This task is designed to strengthen the relation types between pairs of nodes, enhancing the hierarchy.</p>
<p>Text Generation.The text generation task is implemented by a PLM with a language modeling head on top.Given an input sequence x and a graph representation G, the model aims to generate a target sequence y by minimizing the cross-entropy loss:
L T G = − |y| i=1 log P ϕ (y i |y 1:i−1 , x, G), (7)
where P ϕ is the generative probability from PLM.</p>
<p>Graph Reconstruction.Building on previous work that focused on predicting relationships between entities (Song et al., 2020;Li et al., 2021), our approach reformulates the graph reconstruction task.We aim to predict the relation type r in the triple (u, r, v), where u and v are nodes in the hierarchical graph structure.Node representations h u and h v are derived from the last hidden states of the PLM's encoder.Consequently, the probability of relation r is given by:
p(r|u, v) = softmax(W [h u ; h v ] + b), (8)
where W and b are trainable parameters.The loss for graph reconstruction is computed using crossentropy loss:
L GR = − ⟨u,r,v⟩∈E log p(r|u, v).(9)
We integrate the text generation loss and the graph reconstruction loss to train the PLM.The overall training loss is defined as follows:
L total = L T G + λL GR , (10)
where λ is a weighting coefficient.2</p>
<p>Experiments</p>
<p>In this section, we introduce the details of our experiments on KG-to-text generation task.We modify T5 for conditional generation from Huggingface (Wolf et al., 2019), and implement GraSAME with the GNN layers provided by PyTorch Geometric (Fey and Lenssen, 2019).</p>
<p>Dataset</p>
<p>WebNLG3 (Gardent et al., 2017) is a commonlyused benchmark in KG-to-text generation (Chen et al., 2020;Li et al., 2021;Li and Liang, 2021;Colas et al., 2022;Ke et al., 2021).We employ WebNLG version 2.1 for our experiments, as it represents a refined version of the widely-used version 2.0 (Shimorina and Gardent, 2018;Chen et al., 2020;Ke et al., 2021;Colas et al., 2022).This version offers two distinct splits: unconstrained and constrained.</p>
<p>WebNLG unconstrained.Each WebNLG sample comprises several KG triples and a corresponding descriptive text.The triples are structured as (head, relation, tail) and the model is supposed to generate fluent text to describe the input triples.An illustrative example is presented in Figure 1.In the unconstrained dataset, there is no overlap of input graphs between the train, validation, and test sets.</p>
<p>WebNLG constrained.The data structure of constrained is as same as unconstrained dataset.However, the constrained dataset presents a greater challenge by ensuring that there is no overlap of triples in the input graphs across train, validation and test sets.</p>
<p>Setting</p>
<p>As an enhancement of self-attention mechanism, a foundational model is required for the implementation of GraSAME.For our experiments, T5-large serves as the foundational model.Prior to training, we expand T5's vocabulary to include the special tokens <H>, <R>, <T >, and <Graph>.To ensure fair comparisons, we maintain consistent hyperparameters across both the baseline and our models. 4All models are fine-tuned with the training set.The BLEU score on validation set is employed to identify the best-performing model, which is subsequently evaluated on the test set.</p>
<p>Evaluation Metrics.To evaluate the performance of the models, we use the automatic evaluation metrics BLEU (Papineni et al., 2002), ME-TEOR (Denkowski and Lavie, 2014), ROUGE-L (Lin, 2004) and chrF++ (Popović, 2015) following previous work (Shimorina and Gardent, 2018;Ribeiro et al., 2021a).The evaluations are conducted using the official evaluation script from the WebNLG challenge (Shimorina and Gardent, 2018).</p>
<p>Baseline.T5-large is employed as the baseline model.Previous research (Ribeiro et al., 2021a;Ke et al., 2021) has demonstrated T5's SOTA performance on graph-to-text generation, making it a robust baseline (Clive et al., 2022).This choice allows for a fair comparison with our approach.</p>
<p>GraSAME is integrated into the T5-large model for our experiments.We investigate the efficacy of different GNNs for encoding the hierarchical graph structure and denote them as:</p>
<p>GraSAME-GAT.Graph Attention Network (GAT) (Veličković et al., 2018) is used as the GNN component in GraSAME.GAT utilizes an attention mechanisms to aggregate the information from neighbouring nodes.</p>
<p>GraSAME-RGCN.We also integrate Relational Graph Convolutional Network (RGCN) (Schlichtkrull et al., 2018) with GraSAME.RGCN extends the Graph Convolutional Network (Kipf and Welling, 2016), enabling it to process local graph neighborhoods within large-scale relational data.</p>
<p>GraSAME-SAGE.GraphSAGE (Hamilton et al., 2017) is an inductive framework that efficiently generates node embeddings for unseen nodes by leveraging node feature information.We also combine it with GraSAME.We present the primary results for WebNLG unconstrained in Table 1.Remarkably, our leading model, GraSAME-SAGE, surpasses the baseline T5-large in all metrics, despite having over 500 million fewer trainable parameters.We believe this is due to the powerful encoding ability of Graph-SAGE for unseen nodes (for example, the special tokens we add to the model's vocabulary manually).Although other models don't outperform the baseline T5, they still achieve noteworthy performance, achieving BLEU scores of over 60 with much fewer trainable parameters than baseline.</p>
<p>Evaluation Results</p>
<p>Main Results</p>
<p>Model</p>
<p>For comparison, we also include the results of the SOTA models from related work.Notably, GraSAME-SAGE outperforms GCN and KGPT on BLEU and METEOR scores, and achieves performance comparable to JoinGT(T5).Our ME-TEOR score of 48.38 is even higher than that of Jo-inGT(T5).It is worth mentioning that both KGPT and JointGT are pre-trained with additional tasks to fine-tune KG-to-text generation.Additionally, JointGT(T5) has 114 million more trainable parameters than GraSAME-SAGE.This highlights the efficiency of our approach, demonstrating that GraSAME can enhance PLMs by leveraging tokenlevel structural information for KG-to-text generation.</p>
<p>The results for WebNLG constrained are similar as for WebNLG unconstrained.As illustrated in Table 3, GraSAME-SAGE outperforms the baseline T5, while our other models achieve comparable results to it with higher BLEU scores and fewer trainable parameters.In comparison to Jo-inGT(T5), GraSAME-SAGE maintains over 98% performance with fewer than 114 million trainable parameters, and it doesn't require additional pretraining tasks.This prove the generalization of our method under the constrained condition, where there is no overlap between training and test triples.</p>
<p>Detailed Analysis on Graph Size</p>
<p>We conduct a comprehensive analysis focusing on input graph size, as detailed in Table 2. Notably, GraSAME consistently improves the BLEU scores across various input triple counts, except for sixtriple inputs.The most pronounced improvement occurs with seven-triple inputs, where the BLEU score surpasses the baseline by 8.61.Seven-triple inputs form the most complex graph structure in the   test set, highlighting the efficacy of GraSAME in fortifying the PLM's capacity to process complex graph inputs through token-level structural integration.Interestingly, a significant improvement also emerges in one-triple input graphs, likely due to the baseline model's propensity for generating hallucinations with shorter input graphs.5
Model A P B M R C JointGT(T5) ✓265M</p>
<p>Human Evaluation</p>
<p>Model</p>
<p>Fluency Meaning Gold 5.59 5.71 T5 5.57 5.41 GraSAME 5.56 5.62 To further assess the quality of the generated text, we conduct a human evaluation using the crowdsourcing platform Amazon Mechanical Turk. 6We randomly select 100 texts generated by both baseline and GraSAME-SAGE models, along with their corresponding gold standard references.In line with previous studies (Castro Ferreira et al., 2019;Ribeiro et al., 2021a), we ask three annotators to rate the texts on a 1-7 Likert scale across two dimensions: (i) Fluency: Assessing whether the text is fluent, natural, and easy to read.(ii) Meaning: Evaluating if the text accurately conveys the information from the input graph without including extraneous information (hallucination).We specifically instruct annotators to pay close attention to instances of hallucination, as this issue has gained significant attention in recent PLM research (González Corbelle et al., 2022;Ji et al., 2023;Yuan and Faerber, 2023).</p>
<p>As indicated in Table 4, both the baseline and GraSAME models produce fluent text, scoring only marginally lower than the reference by 0.02 and 0.03, respectively.Regarding the meaningfulness of the generated text, GraSAME surpasses the baseline, achieving a score that is 0.21 points higher.This human evaluation confirms that GraSAME is capable of generating not only fluent text but also text that more accurately encapsulates the input information, while minimizing hallucinations.We conduct an ablation study focusing on two key aspects of the model: the bidirectional edges and the graph reconstruction task.This study is implemented using the top-performing GraSAME-SAGE model on WebNLG unconstrained.</p>
<p>Ablation Study</p>
<p>Bidirectional Edges: We retain a single edge direction in the hierarchical graph structure, specifically from bottom to top tokens.</p>
<p>Graph Reconstruction: We omit the graph construction loss during training, allowing the model to update solely based on the text generation loss.</p>
<p>The outcomes of the ablation study are detailed in Table 5.Both the bidirectional edge and graph reconstruction components significantly enhance the performance of GraSAME.Excluding either element results in a decrease in both BLEU and METEOR scores, with a marginally greater reduction observed upon the removal of bidirectional edges.This suggests that bidirectional edges are crucial for adequate message passing within the hierarchical graph structure.</p>
<p>Model Variations</p>
<p>Considering the method of incorporating GNN into the self-attention layer, we introduce two additional variations of GraSAME, as visualized in Figure 4 of Appendix D. In Variation 1, the GNN generates graph embeddings to influence the vectors K and V , instead of Q.In Variation 2, we insert the GNN layer before the entire self-attention mechanism, which means the vectors K, V and Q are all derived from the graph embedding.</p>
<p>The results for the two variations are presented in Table 6.While Variation 1 outperforms Variation 2, both variations show a decrease in performance compared to the original GraSAME.This demonstrates that our proposed GraSAME is a valid and effective mechanism for encoding structural information.</p>
<p>Error Analysis</p>
<p>We illustrate three examples of generated text in Table 7.The first two examples have relatively short input texts, whereas the third example includes a longer and more complex input graph.</p>
<p>In the first and second examples, the text generated by T5 includes hallucinations not present in the input data.In contrast, GraSAME strictly adheres to facts conveyed by the input graphs.When handling more complex graph inputs, both T5 and GraSAME produce fluent and accurate text without introducing extraneous information.However, T5 makes a minor grammatical error with a preposition.This indicates that GraSAME effectively mitigates the issue of hallucinations, particularly with short and simple inputs.Moreover, for longer and more complex inputs, GraSAME demonstrates a superior understanding of the input structure, resulting in higher-quality text generation.</p>
<p>Conclusion</p>
<p>In this work, we introduce GraSAME, a novel graph-guided self-attention mechanism that enables PLMs to process token-level structural information.With GraSAME, PLMs are capable of  handling text and graph input simultaneously.This approach facilitates seamless information flow between text and graph embeddings, eliminating the need for additional concatenation.Evaluated on the KG-to-text generation task, GraSAME demonstrates performance comparable to SOTA models with significantly fewer trainable parameters.Through a detailed analysis of graph size and human evaluation, GraSAME demonstrates its enhanced ability to process more complex graph inputs and generate more accurate text.Moving forward, we aim to explore GraSAME's potential in encoding specific graph structures, like molecular graph (Edwards et al., 2022), in combination with large language models.</p>
<p>Limitation</p>
<p>Despite the effectiveness of our approach, we acknowledge several limitations in our work.Firstly, our method involves extracting a hierarchical graph structure from a linearized graph.While this structure facilitates efficient information exchange, it requires specific adjustments when applied to dif-ferent datasets or tasks.Our goal is to combine the advantages of PLM and GNN, yet crafting a universal template that addresses all related tasks remains challenging.</p>
<p>Secondly, we observe that the training process of GraSAME is fast, but it tends to converge slower than the baseline model without GraSAME.This slower convergence is attributed to the GNN component of GraSAME not being pre-trained, necessitating additional training epochs for optimal interaction with the PLM.</p>
<p>Thirdly, our approach still incorporates the linearized graph as part of the input, which does not align with the pre-training process of PLMs typically conducted with plain text corpora in natural language.This misalignment could potentially lead to the forgetting of pre-trained knowledge.Addressing these limitations will be the focus of our future work.</p>
<p>Ethics Statement</p>
<p>This research was conducted in accordance with the ACM Code of Ethics.The datasets we used are publicly available (Gardent et al., 2017;Kim et al., 2023), and we only used them to evaluate our models.We are not responsible for any potentially erroneous statements within the datasets.Additionally, care should be taken with the potential issues of hallucination when using the baseline model T5 for text generation.</p>
<p>B Data Statistics</p>
<p>We report the statistics of WebNLG in</p>
<p>C Impact of Graph Reconstruction Loss</p>
<p>To investigate how graph reconstruction loss affect the performance and to determine the optimal value of λ in Equation 10, we visualize the tuning process in Figure 3.We use GraSAME-SAGE and the validation set of the WebNLG unconstrained dataset.The identified best value for λ is 0.08.</p>
<p>D Visualization of the Model Variations</p>
<p>To provide a clearer understanding of the model variations, we visualize the internal structure of the self-attention layer for the two model variations in Figure 4.</p>
<p>the GPU to achieve the desired batch size, which could potentially impact the test set results.Consequently, we conducted the model evaluation on the test set using a single GPU for accuracy.(a) Variation 1: The text embedding x is fed into a GNN along with the edge index derived from the hierarchical graph structure.This process generates a graph embedding x , which subsequently induces the K and V vector.The Q vector then interacts with K and V vector to produce a graph-aware representation z.</p>
<p>(b) Variation 2: The text embedding x is fed into a GNN along with the edge index derived from the hierarchical graph structure.This process generates a graph embedding x , which subsequently induces the Q, K and V vector.</p>
<p>Figure 2 :
2
Figure 2: The architecture of GraSAME.The text embedding x is fed into a GNN along with the edge index derived from the hierarchical graph structure.This process generates a graph embedding x , which subsequently induces the Q vector.The Q vector then guides the self-attention mechanism to produce a graphaware representation z.The visualization of GNN is taken from GraphSAGE(Hamilton et al., 2017).</p>
<p>A denotes if additional pre-training tasks are implemented or not.P = Trainable parameters, B = BLEU, M = METEOR, R = ROUGE, C = chrF++.The results of GCN, KGPT, JointGT(T5) are re-printed from Shimorina and Gardent (2018), Chen et al. (2020) and Ke et al. (2021), respectively.Bold indicates the best score of the models we trained.Underline indicates the best score of SOTA models in previous work.</p>
<p>tional pre-training tasks are implemented or not.P = Trainable parameters, B = BLEU, M = METEOR, R = ROUGE, C = chrF++.The results of JointGT(T5) are re-printed from Ke et al. (2021).Bold indicates the best score of the models we trained.</p>
<p>Figure 3 :
3
Figure 3: The impact of λ, experimented with GraSAME-SAGE on the validation set of WebNLG unconstrained.</p>
<p>Figure 4 :
4
Figure 4: Visualization of two variations of GraSAME.</p>
<p>Table 1 :
1
Results on WebNLG unconstrained.
A PBMRCGCN✗ -60.80 42.76 71.13 -KGPT✓ 177M 64.11 46.30 74.57 -JointGT(T5)✓ 265M 66.14 47.25 75.91 -T5-large✗ 737M 61.41 45.96 71.70 75.27GraSAME-GAT✗ 75.7M 60.44 44.91 70.73 72.49GraSAME-RGCN ✗ 453M 60.26 44.46 70.93 71.88GraSAME-SAGE ✗ 151M 65.55 48.38 74.55 77.34</p>
<p>Table 2 :
2
BLEU scores for different graph sizes in WebNLG unconstrained test set.T5 is the baseline model T5-large, GraSAME is the best performed model GraSAME-SAGE.+, -denote the difference of scores.
Model1 triple 2 triples 3 triples 4 triples 5 triples 6 triples 7 triplesT546.8167.5864.3964.8658.7368.9162.97GraSAME +7.80+1.85+3.05+2.52+3.73-2.83+8.61</p>
<p>Table 3 :
3
Results on WebNLG constrained.A denotes if addi-</p>
<p>Table 4 :
4
Human evaluation on WebNLG unstrained.T5 denotes the baseline model T5-large, GraSAME denotes GraSAME-SAGE.The Fleiss' Kappa κ is 0.42, which indicates moderate agreement.</p>
<p>Table 5 :
5
Ablation study on WebNLG unconstrained.
ModelBLEU METEORGraSAME-SAGE65.5548.38-bidirectional edges60.7445.13-graph reconstruction 61.7545.65</p>
<p>Table 6 :
6
Results of model variations on WebNLG unconstrained.GraSAME = GraSAME-SAGE.
ModelBLEU METEOR ROUGE chrF++Variation 1 60.6246.5771.0374.67Variation 2 60.0746.0170.1774.08GraSAME 65.5548.3874.5577.34</p>
<p>New Jersey Volunteer Infantry Monument is located on the Monocacy National Battlefield, Frederick County, Maryland.The monument was established in 1907-07-11 and is categorised as a historic district in the United States.T5 The 14th New Jersey Volunteer Infantry Monument is located in the Monocacy National Battlefield, Frederick County, Maryland.It was established on 11 July 1907 and is categorised as a historic district in the United States.GraSAME The 14th New Jersey Volunteer Infantry Monument is located on the Monocacy National Battlefield in Frederick County, Maryland, United States.It was established on 11 July 1907 and is categorised as a historic district in the United States.
ModelExampleInputtranslate graph to English: <Graph> <H> Iraq <R> language <T > ArabicReferenceIraq language is Arabic.T5Arabic is a language spoken in the country of Iraq. The country is a member of the UnitedNations.GraSAME Arabic is a language spoken in Iraq.Inputtranslate graph to English: <Graph> <H> Madrid <R> country <T > SpainReferenceMadrid is in the country of Spain.T5Madrid is a city in the country of Spain and is a popular tourist destination there.GraSAME Madrid is a city in the country of Spain.Inputtranslate graph to English: <Graph> <H> Monocacy National Battlefield <R> location <T >Frederick County, Maryland <H> 14th New Jersey Volunteer Infantry Monument <R> estab-lished <T > "1907-07-11" <H> 14th New Jersey Volunteer Infantry Monument <R> country<T > "United States" <H> 14th New Jersey Volunteer Infantry Monument <R> category <T >Historic districts in the United States <H> 14th New Jersey Volunteer Infantry Monument <R>district <T > Monocacy National Battlefield <H> 14th New Jersey Volunteer Infantry Monument<R> state <T > "Maryland"ReferenceThe 14th</p>
<p>Table 7 :
7
Examples of text generated by T5-large and GraSAME-SAGE, with hallucinations highlighted in blue and other incorrect text marked in red.</p>
<p>Table 8 :
8
Hyperparameters.</p>
<p>Table 9, which is the original split in WebNLG version 2.1.
DatasetSize| Train | | Dev | | Test |WebNLG U 1287616191600WebNLG C 1289515941606</p>
<p>Table 9 :
9
The statistics of dataset.WebNLG U = WebNLG unconstrained, WebNLG C = WebNLG constrained.</p>
<p>For the KG-to-text generation task, the prompt "translate graph to English: " is added as a task description to match the input format of T5.
We observed that the value of λ significantly impacts performance. After tuning on the validation set of the WebNLG unconstrained dataset, we set λ to 0.08, which yielded the best BLEU score. Further details are provided in Appendix C.
https://synalp.gitlabpages.inria.fr/ webnlg-challenge/
Details on the hyper-parameters are provided in Appendix A.
A more in-depth error analysis is presented in Section
. 6 https://www.mturk.com
AcknowledgementsWe want to thank the anonymous reviewers for their helpful comments.Also, we would like to thank our colleague Yindong Wang for the inspiration of the idea, and Ercong Nie for his feedback on this work.A HyperparametersWe present the hyperparameters for T5-large and GraSAME in Table8.We train the model until the training process converges.To keep a fair comparison, we keep the hyperparameters as same as possible.The learning rate of GraSAME is set larger than it for T5, because GraSAME converges slower due to fewer trainable parameters.The models are trained with 4 NVIDIA A100-SXM4-40GB GPUs. 77 We noted that when employing Distributed Data Parallel(Li et al., 2020)
The complexity and hierarchical structure of tasks in insect societies. Carl Anderson, Nigel R Franks, Daniel W Mc-Shea, Animal Behaviour. 6242001</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, 10.18653/v1/P18-1026Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Thiago Castro Ferreira, Chris Van Der Lee, Emiel Van Miltenburg, Emiel Krahmer, 10.18653/v1/D19-1052Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>KGPT: Knowledge-grounded pretraining for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang, Wang , 10.18653/v1/2020.emnlp-main.697Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Learning sequential and structural information for source code summarization. Yunseok Choi, Jinyeong Bak, Cheolwon Na, Jee-Hyong Lee, 10.18653/v1/2021.findings-acl.251Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Control prefixes for parameter-efficient text generation. Jordan Clive, Kris Cao, Marek Rei, 10.18653/v1/2022.gem-1.31Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM). the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>GAP: A graph-aware language model framework for knowledge graph-to-text generation. Anthony Colas, Mehrdad Alvandipour, Daisy Zhe Wang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaInternational Committee on Computational Linguistics2022</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, 10.3115/v1/W14-3348Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational Linguistics2014</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, 10.18653/v1/2022.emnlp-main.26Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Fast graph representation learning with PyTorch Geometric. Matthias Fey, Jan E Lenssen, ICLR Workshop on Representation Learning on Graphs and Manifolds. 2019</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational Linguistics2017</p>
<p>Dealing with hallucination and omission in neural natural language generation: A use case on meteorology. Javier González Corbelle, Alberto Bugarín-Diz, Jose Alonso-Moral, Juan Taboada, Proceedings of the 15th International Conference on Natural Language Generation. the 15th International Conference on Natural Language GenerationWaterville, Maine, USAAssociation for Computational Linguistics2022and virtual meeting</p>
<p>Inductive representation learning on large graphs. Advances in neural information processing systems. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730</p>
<p>Have your text and use it too! end-to-end neural datato-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, 10.18653/v1/2020.coling-main.218Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>BERT-MK: Integrating graph contextualized knowledge into pretrained language models. Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Jing Yuan, Tong Xu, 10.18653/v1/2020.findings-emnlp.207Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning2019</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>JointGT: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, 10.18653/v1/2021.findings-acl.223Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>FactKG: Fact verification via reasoning on knowledge graphs. Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, Edward Choi, 10.18653/v1/2023.acl-long.895Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Fewshot knowledge graph-to-text generation with pretrained language models. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen, 10.18653/v1/2021.findings-acl.136Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Pytorch distributed: Experiences on accelerating data parallel training. Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, Proceedings of the VLDB Endowment. the VLDB Endowment202013</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>GPT-too: A language-model-first approach for AMR-to-text generation. Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, Salim Roukos, 10.18653/v1/2020.acl-main.167Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Graph hierarchy: a novel framework to analyse hierarchical structures in complex networks. Giannis Moutsinas, Choudhry Shuaib, Weisi Guo, Stephen Jarvis, Scientific Reports. 111139432021</p>
<p>Decomposed prompting: Unveiling multilingual linguistic structure knowledge in englishcentric large language models. Ercong Nie, Shuzhou Yuan, Bolei Ma, Helmut Schmid, Michael Färber, Frauke Kreuter, Hinrich Schütze, arXiv:2402.183972024arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Knowledge enhanced contextual word representations. Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A Smith, 10.18653/v1/D19-1005Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Hierarchical Organization in Society. James Pooler, 2017Routledge</p>
<p>chrF: character n-gram F-score for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, 10.18653/v1/2021.nlp4convai-1.20Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI. the 3rd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2021aHinrich Schütze, and Iryna Gurevych</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Claire Zhang, Iryna Gardent, Gurevych, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 82020</p>
<p>Structural adapters in pretrained language models for AMR-to-Text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, 10.18653/v1/2021.emnlp-main.351Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021bOnline and Punta Cana</p>
<p>Modeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, The Semantic Web: 15th International Conference. Heraklion, Crete, GreeceSpringer2018. 2018. June 3-7, 201815</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, Leonardo F R Ribeiro, 10.18653/v1/2021.textgraphs-1.2Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15). the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)Mexico City, MexicoAssociation for Computational Linguistics2021Philipp Dufter, Iryna Gurevych, and Hinrich Schütze</p>
<p>Handling rare items in data-to-text generation. Anastasia Shimorina, Claire Gardent, 10.18653/v1/W18-6543Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational Linguistics2018</p>
<p>Structural information preserving for graph-to-text generation. Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu, 10.18653/v1/2020.acl-main.712Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Graph Attention Networks. International Conference on Learning Representations. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, 2018Accepted as poster</p>
<p>K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou, 10.18653/v1/2021.findings-acl.121Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Huggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. Wenhan Xiong, Jingfei Du, William Yang, Wang , Veselin Stoyanov, 2020ICLR</p>
<p>Rethinking network pruning -under the pre-train and fine-tune paradigm. Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, Zhibin Xiao, 10.18653/v1/2021.naacl-main.188Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Learning structural information for syntax-controlled paraphrase generation. Erguang Yang, Chenglin Bai, Deyi Xiong, Yujie Zhang, Yao Meng, Jinan Xu, Yufeng Chen, 10.18653/v1/2022.findings-naacl.160Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Multimodal transformer for multimodal machine translation. Shaowei Yao, Xiaojun Wan, 10.18653/v1/2020.acl-main.400Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Heterogeneous graph transformer for graph-tosequence learning. Shaowei Yao, Tianming Wang, Xiaojun Wan, 10.18653/v1/2020.acl-main.640Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>QA-GNN: Reasoning with language models and knowledge graphs for question answering. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec, 10.18653/v1/2021.naacl-main.45Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Hierarchical graph representation learning with differentiable pooling. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec, Advances in neural information processing systems. 201831</p>
<p>Evaluating generative models for graph-to-text generation. Shuzhou Yuan, Michael Faerber, Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing. the 14th International Conference on Recent Advances in Natural Language ProcessingVarna, Bulgaria; Shoumen, BulgariaINCOMA Ltd2023</p>
<p>Helmut Schmid, and Hinrich Schütze. Shuzhou Yuan, Ercong Nie, Michael Färber, arXiv:2402.11709Gnnavi: Navigating the information flow in large language models by graph neural network. 2024arXiv preprint</p>
<p>Greaselm: Graph reasoning enhanced language models for question answering. Zhang, Bosselut, Yasunaga, Ren, Liang, Manning, Leskovec, International Conference on Representation Learning (ICLR). 2022</p>
<p>ERNIE: Enhanced language representation with informative entities. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, 10.18653/v1/P19-1139Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
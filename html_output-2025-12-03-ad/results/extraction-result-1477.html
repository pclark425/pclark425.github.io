<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1477 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1477</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1477</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-102350928</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1811.04319v3.pdf" target="_blank">Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text</a></p>
                <p><strong>Paper Abstract:</strong> Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1477.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1477.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEXT2QUEST / TEXTLABS curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEXT2QUEST (TEXTLABS instance) curriculum for procedural text learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-style training setup based on synthetic, controllable text-based games (TEXTLABS) where procedural scientific texts (materials synthesis) are converted to quests of increasing difficulty (measured by quest length and number of entities); a text-RL agent (LSTM-DQN) is trained on these levels to learn to map surface text to action-graphs by executing the corresponding action sequences in a simulated text environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN (adapted from Narasimhan 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A basic LSTM-DQN text-based RL agent adapted to the TEXTLABS setting. Uses an LSTM encoder with a DQN head over a combinatorial action space A = W_v × W_o1 × W_o2 (verbs × object1 × object2). To mitigate lack of full action history conditioning the agent concatenates the last four commands and always appends the full quest instructions to each observation; illegal actions are pruned at each state.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TEXTLABS (an instance of TEXT2QUEST built on TextWorld / Inform7)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-based, partially-observable simulated lab environment for materials synthesis procedures. Interactions are symbolic text actions (domain verbs and native TextWorld actions) applied to named entities; actions include relation-creating operations (e.g., input-assign(material, operation), link-descriptor(descriptor,entity)), domain ops (run-op, obtain), and standard text-game actions (take, drop, examine); the environment supports state-tracking via a symbolic interpreter and can be populated by synthetic surfaces paired with ground-truth action-graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>laboratory experiments (materials synthesis procedures / scientific procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Material synthesis steps such as incorporating start materials into operations, running operations (run-op), linking descriptors to materials or apparatus, filtering/splitting mixtures, and creating operation-result entities (implicit outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures are compositional sequences of primitive symbolic actions that produce/transform entities; tasks decompose into ordered operations whose dependencies induce a topological ordering in an action graph, and implicit entities (operation results) may appear as intermediate nodes requiring subsequent actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>difficulty-based (level) curriculum generated from synthetic quests</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Curriculum data is generated synthetically by the Quest and Surface Generators (forward-chaining and heuristic search) and controllable NLG. Tasks are organized into discrete difficulty levels where difficulty is controlled by maximum quest length and the number of entities in the target action-graph. The training procedure for the experiment used multiple levels (increasing difficulty) with 100 games per level for training and 10 games per level for testing. The synthetic generator enables fine-grained control of compositional complexity and implicit entity structure to shape the curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task difficulty (increasing maximum quest length and number of entities / compositional complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Controlled by max quest length and entity count; representative synthetic tasks correspond to short real-world procedures (1–3 sentence procedures) up to longer multi-step quests — exact numeric step ranges are not explicitly reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative report: the adapted LSTM-DQN agent learned to successfully perform required actions only on the easiest curriculum levels; for longer/harder synthetic quests the agent failed to complete tasks. The paper defines a normalized reward metric (avg. normalized reward per game where r_i = +1 for each correct action in K and -1 for incorrect actions; normalized score of 1 indicates exact action-sequence match) but does not provide detailed numerical results in the text (plots in Fig. 4 are referenced but numeric values are not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A synthetic, difficulty-ordered curriculum is feasible and useful to create controllable training data for procedural text grounding; however, in these preliminary experiments curriculum alone did not yield success on moderate-to-hard tasks with the basic LSTM-DQN agent: the agent only succeeded on the simplest levels. Primary bottlenecks identified include insufficient sequence/state conditioning in the agent architecture, irreversible actions in the current simulator increasing difficulty, and the general difficulty of learning initial semantic-parsing policies with RL alone (authors suggest hybrid RL/supervised methods, richer sequence encoders, recurrent memory, attention, and state information to improve results). The paper also highlights the advantage of coupling simulated curricula with controllable NLG to scale annotated training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_contexts</strong></td>
                            <td>["The paper explicitly proposes enabling curriculum learning via synthetic data generation: '...the game format allows applying powerful neural programming methods, with a significantly richer training environment, including advances such as curriculum learning, common-sense and domain-specific constraints, and full state tracking.'", "Preliminary experiments: 'we train a simple text-based RL agent on synthetic games in increasingly difficult environments. Difficulty is measured by maximum quest length, and the number of entities in the target action graph.'", "Evaluation setup: 'We train the agent on 100 games per level and test on 10 games. Evaluation is measured by avg. normalized reward per game... As can be seen in Fig. 4, the agent learns to successfully perform the required actions only for the easiest levels.'"]</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Grounding natural language with autonomous interaction <em>(Rating: 2)</em></li>
                <li>Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning <em>(Rating: 2)</em></li>
                <li>Extracting action sequences from texts based on deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning to follow language instructions with adversarial reward induction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1477",
    "paper_id": "paper-102350928",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "TEXT2QUEST / TEXTLABS curriculum",
            "name_full": "TEXT2QUEST (TEXTLABS instance) curriculum for procedural text learning",
            "brief_description": "A curriculum-style training setup based on synthetic, controllable text-based games (TEXTLABS) where procedural scientific texts (materials synthesis) are converted to quests of increasing difficulty (measured by quest length and number of entities); a text-RL agent (LSTM-DQN) is trained on these levels to learn to map surface text to action-graphs by executing the corresponding action sequences in a simulated text environment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN (adapted from Narasimhan 2017)",
            "agent_description": "A basic LSTM-DQN text-based RL agent adapted to the TEXTLABS setting. Uses an LSTM encoder with a DQN head over a combinatorial action space A = W_v × W_o1 × W_o2 (verbs × object1 × object2). To mitigate lack of full action history conditioning the agent concatenates the last four commands and always appends the full quest instructions to each observation; illegal actions are pruned at each state.",
            "agent_size": null,
            "environment_name": "TEXTLABS (an instance of TEXT2QUEST built on TextWorld / Inform7)",
            "environment_description": "A text-based, partially-observable simulated lab environment for materials synthesis procedures. Interactions are symbolic text actions (domain verbs and native TextWorld actions) applied to named entities; actions include relation-creating operations (e.g., input-assign(material, operation), link-descriptor(descriptor,entity)), domain ops (run-op, obtain), and standard text-game actions (take, drop, examine); the environment supports state-tracking via a symbolic interpreter and can be populated by synthetic surfaces paired with ground-truth action-graphs.",
            "procedure_type": "laboratory experiments (materials synthesis procedures / scientific procedures)",
            "procedure_examples": "Material synthesis steps such as incorporating start materials into operations, running operations (run-op), linking descriptors to materials or apparatus, filtering/splitting mixtures, and creating operation-result entities (implicit outputs).",
            "compositional_structure": "Procedures are compositional sequences of primitive symbolic actions that produce/transform entities; tasks decompose into ordered operations whose dependencies induce a topological ordering in an action graph, and implicit entities (operation results) may appear as intermediate nodes requiring subsequent actions.",
            "uses_curriculum": true,
            "curriculum_name": "difficulty-based (level) curriculum generated from synthetic quests",
            "curriculum_description": "Curriculum data is generated synthetically by the Quest and Surface Generators (forward-chaining and heuristic search) and controllable NLG. Tasks are organized into discrete difficulty levels where difficulty is controlled by maximum quest length and the number of entities in the target action-graph. The training procedure for the experiment used multiple levels (increasing difficulty) with 100 games per level for training and 10 games per level for testing. The synthetic generator enables fine-grained control of compositional complexity and implicit entity structure to shape the curriculum.",
            "curriculum_ordering_principle": "task difficulty (increasing maximum quest length and number of entities / compositional complexity)",
            "task_complexity_range": "Controlled by max quest length and entity count; representative synthetic tasks correspond to short real-world procedures (1–3 sentence procedures) up to longer multi-step quests — exact numeric step ranges are not explicitly reported in the text.",
            "performance_with_curriculum": "Qualitative report: the adapted LSTM-DQN agent learned to successfully perform required actions only on the easiest curriculum levels; for longer/harder synthetic quests the agent failed to complete tasks. The paper defines a normalized reward metric (avg. normalized reward per game where r_i = +1 for each correct action in K and -1 for incorrect actions; normalized score of 1 indicates exact action-sequence match) but does not provide detailed numerical results in the text (plots in Fig. 4 are referenced but numeric values are not reported).",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "A synthetic, difficulty-ordered curriculum is feasible and useful to create controllable training data for procedural text grounding; however, in these preliminary experiments curriculum alone did not yield success on moderate-to-hard tasks with the basic LSTM-DQN agent: the agent only succeeded on the simplest levels. Primary bottlenecks identified include insufficient sequence/state conditioning in the agent architecture, irreversible actions in the current simulator increasing difficulty, and the general difficulty of learning initial semantic-parsing policies with RL alone (authors suggest hybrid RL/supervised methods, richer sequence encoders, recurrent memory, attention, and state information to improve results). The paper also highlights the advantage of coupling simulated curricula with controllable NLG to scale annotated training data.",
            "citation_contexts": [
                "The paper explicitly proposes enabling curriculum learning via synthetic data generation: '...the game format allows applying powerful neural programming methods, with a significantly richer training environment, including advances such as curriculum learning, common-sense and domain-specific constraints, and full state tracking.'",
                "Preliminary experiments: 'we train a simple text-based RL agent on synthetic games in increasingly difficult environments. Difficulty is measured by maximum quest length, and the number of entities in the target action graph.'",
                "Evaluation setup: 'We train the agent on 100 games per level and test on 10 games. Evaluation is measured by avg. normalized reward per game... As can be seen in Fig. 4, the agent learns to successfully perform the required actions only for the easiest levels.'"
            ],
            "uuid": "e1477.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Grounding natural language with autonomous interaction",
            "rating": 2,
            "sanitized_title": "grounding_natural_language_with_autonomous_interaction"
        },
        {
            "paper_title": "Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning",
            "rating": 2,
            "sanitized_title": "deep_dynaq_integrating_planning_for_taskcompletion_dialogue_policy_learning"
        },
        {
            "paper_title": "Extracting action sequences from texts based on deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "extracting_action_sequences_from_texts_based_on_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning to follow language instructions with adversarial reward induction",
            "rating": 1,
            "sanitized_title": "learning_to_follow_language_instructions_with_adversarial_reward_induction"
        }
    ],
    "cost": 0.008158499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text</p>
<p>Ronen Tamari ronent@cs.huji.ac.il 
NAIST / RIKEN-AIP
NAIST / RIKEN-AIP
The Hebrew University of Jerusalem
The Hebrew University of Jerusalem</p>
<p>Hiroyuki Shindo shindo@is.naist.jp 
NAIST / RIKEN-AIP
NAIST / RIKEN-AIP
The Hebrew University of Jerusalem
The Hebrew University of Jerusalem</p>
<p>Dafna Shahaf dshahaf@cs.huji.ac.il 
NAIST / RIKEN-AIP
NAIST / RIKEN-AIP
The Hebrew University of Jerusalem
The Hebrew University of Jerusalem</p>
<p>Yuji Matsumoto 
NAIST / RIKEN-AIP
NAIST / RIKEN-AIP
The Hebrew University of Jerusalem
The Hebrew University of Jerusalem</p>
<p>Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text</p>
<p>Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging realworld problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, TEXT2QUEST, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a textbased simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.</p>
<p>Introduction</p>
<p>Materials science literature includes a vast amount of synthesis procedures described in natural language. The ability to automatically parse these texts into a structured form could allow for datadriven synthesis planning, a key enabler in the design and discovery of novel materials (Kim et al., 2018;Mysore et al., 2017). A particularly useful parsing is action graph extraction, which maps a passage describing a procedure to a symbolic action-graph representation of the core entities, operations and their accompanying arguments, as they unfold throughout the text (Fig. 1).</p>
<p>Procedural text understanding is a highly challenging task for today's learning algorithms (Lucy and Gauthier, 2017;. Synthesis procedures are especially challenging, as they are written in difficult and highly technical language assuming prior knowledge. Some texts are long, many follow a non-linear narrative, or include logical quantifiers ("all synthesis steps were performed in an argon atmosphere..."). Furthermore, annotated data is scarce and expensive to obtain.</p>
<p>Two related research areas are grounded semantic parsing and state-tracking readingcomprehension. Grounded (or executable) semantic parsers map natural language to a symbolic representation which can also be thought of as a sequence of instructions in some pre-defined programming language. Such "neural-programing" architectures offer strong symbolic reasoning capabilities, compositionality modelling, and strong generalization (Reed and de Freitas), but are typically applied to simple texts due to prohibitive annotation costs (Liang et al., 2017). State-tracking models (Bosselut et al., 2017;Das et al., 2018;Bansal et al., 2017) can model complex relations between entities as they unfold, with easier training but less symbolic reasoning abilities. Their applicability to longer texts is hindered as well by the lack of fine-grained annotated data.</p>
<p>In this work we describe an approach, TEXT2QUEST, that attempts to combine the strengths of both methods. Instead of trying to learn from static text, we propose to treat procedural text as instructions for an interactive game (or "quest"). The learning agent interacts with entities defined in the text by executing symbolic actions (Fig. 2). A text-based symbolic interpreter handles execution and tracking of the agent's state and actions. The game is completed by "simulating" the instructions correctly; i.e., mapping instructions to a sequence of actions. Correct simulation thus directly yields the desired action graph.</p>
<p>While there is some engineering overhead required for the simulator, we demonstrate that it is relatively straightforward to convert an annotation schema to a text-based game. We believe that the benefits make it worth pursuing: the game for- Figure 1: Sample surface text (left) and possible corresponding action-graph (right) for typical partial material synthesis procedure. Operation numbers in parentheses are added for clarity. Nodes are entities, edges are relations linking them, equivalent to actions in the text-based game. mat allows applying powerful neural programming methods, with a significantly richer training environment, including advances such as curriculum learning, common-sense and domain-specific constraints, and full state tracking. Such "friendly" environments that assist the learning agent have been shown to be valuable (Liang et al., 2017) and enable learning of patterns that are often hard to learn from surface annotations alone, such as implicit effects of operations (i.e., filtering a mixture splits it into two entities).</p>
<p>Interestingly, understanding by simulation aligns well with models of human cognition; mental simulation, the ability to construct and manipulate an internal world model, is a cornerstone of human intelligence involved in many unique behaviors, including language comprehension (Marblestone et al., 2016;Hamrick, 2019). In this work we take first steps towards this idea. Our contributions are:</p>
<p>• We propose a novel formulation of the problem of procedural text understanding as a textbased game, enabling the use of neural programming and text-based reinforcement learning (RL) methods. • We present and release TEXTLABS 1 , an instance of TEXT2QUEST designed for interaction with synthesis procedure texts. We focus on the material-science setting, but the approach is intended to be more generally applicable. • We propose to address the problem of obtaining full-graph annotations at scale by coupling the simulator with controllable natural language generation (NLG) to generate synthetic data, also enabling curriculum learning. While this work is preliminary in nature, neural programming and text-based reinforcement learning approaches are attracting significant and growing interest, and we expect advances in these areas to directly benefit future versions of the system.</p>
<p>Related Work</p>
<p>Procedure understanding: Many recent works have focused on tracking entities and relations in long texts, such as cooking recipes and scientific processes (Bosselut et al., 2017;Das et al., 2018). However, these methods do not directly extract a full action graph. For action graph extraction, earlier works use sequence tagging methods (Mysore et al., 2017). Feng et al. (2018) have applied deep-RL to the problem of extracting action sequences, but assume explicit procedural instruction texts. In Johnson (2017), a graph is constructed from simple generated stories, using state tracking at each time step as supervision.</p>
<p>Semantic parsing &amp; Neural Programming: Research to-date has focused mainly on shorter and simpler texts which may require complex symbolic reasoning, such as mapping natural language to queries over knowledge graphs (Liang et al., 2017). In the case of narrative parsing, the text itself may be complex while the programs are relatively simple (creating and linking between entities present in the text). Recent work (Lu et al., 2018) frames narrative understanding as neuralprogramming, the learner converts a document into a structured form, using a predefined set of datastructures. This approach is similar to ours, though with simpler texts and without a simulated environment. In our approach, the learning architecture is decoupled from the symbolic interpreter environment, enabling greater architectural flexibility.</p>
<p>Text-RL: Text-based games are used to study language grounding and understanding and RL for combinatorical action spaces (Zahavy et al., 2018;Narasimhan, 2017) but have not yet been applied to real world problems. TextWorld (Côté et al., 2018) is a recently released reinforcement learning sandbox environment for creation of custom textbased games, upon which we base TEXTLABS.</p>
<p>Problem Formulation</p>
<p>Entities, Relations &amp; Rules (E, R, Λ): Assume two vocabularies defining types of entities E = {e 1 , ..., e N } and relations R = {r 1 , ..., r K }. A fact f is a grounded predicate of the form f = r (h, t) , h, t ∈ E, r ∈ R (single or double argument predicate relations are allowed). We define the set of valid world-states S, where a state s ∈ S is a set of facts, and validity is decided by a worldmodel Λ defined using linear logic. Λ is comprised of production rules (or transition rules) over entities and relations governing which new facts can be produced from a given state. Following the schema used in the Synthesis Project 2 (see for example MSP), entity types include materials, operations, and relevant descriptors (like operation conditions, etc.). Relations link between entities (like input(material,operation) or denote single predicate relations (entity properties such as solid(material)). We currently use a simplified version of the schema to ease the learning problem. See appendix A.1 for a mapping of relations and entities. Production rules correspond to the actions available to the learner, in our domain these include for example link-descriptor(descriptor,entity), inputassign(material, operation). While not currently included, actions such as co-reference linking and generation of entities can also be incorporated.</p>
<p>Action-Graph (K): An action sequence is defined to be a sequence of valid actions (or production rules) rooted at some initial state s 0 : K = (s 0 , λ 0 , λ 1 , ..., λ n ) (applying λ i to s i results in s i+1 , intermediate states are left out for brevity). Note that actions may apply to implicit entities not present in the surface text (for example, the result of an operation). Construction of an action graph corresponding to K is straightforward (entities as nodes, actions connecting them as edges), and henceforth we use K to denote either the sequence or the graph. Note that there can be multiple possible action sequences resulting in the same action graph, equivalent w.r.t the topological ordering of operations induced by their dependencies.</p>
<p>Surface (X): A surface is simply a text in natural language describing a process.</p>
<p>Learning Task: Our objective is to learn a mapping Ψ : X → K. As this mapping may be highly complex, we convert the problem to a structured prediction setting. As an intermediate step we map an input X to an enriched text-based-game G representation (details below), where the solution of G is the required action graph K. The game is modelled as a partially observable Markov Decision Process (POMDP) G = (S, A, T, Ω, O, R, γ).</p>
<p>We refer the reader to Côté et al. (2018) for a detailed exposition, and focus here on mapping the game-setting to our approach: S are states, A are actions, T are conditional state transition probabilities, where all are constant per domain and defined by E, R, Λ. Ω are observations, and O are conditional observations probabilities. R : S × A → R is the reward function, γ ∈ [0, 1] is the discount factor. As γ, Ω, O are also preset (with actual observations dependent on agent actions), mapping a surface X to game G boils down to providing a list of entities for initializing s 0 . For training and evaluation, a reward function must also be provided (not necessary for applying a trained model on un-annotated text "in the wild").</p>
<p>If a fully annotated action graph is available (whether synthetic or real), this mapping is simple: the initial game state s 0 is a room where the agent is placed alongside all entities. Each edge corresponds to an action in the game. Given an action sequence K, a reward function R can be automatically computed, giving intermediate rewards and penalizing wrong actions. A quest in TextWorld can be defined via a final goal state, thus allowing For data "in the wild", entities can be identified using named entity recognition (NER) as preprocessing. Future directions include end-to-end learning to reduce cascading initialization errors.</p>
<p>By default, the TextWorld environment is partially observable. The agent observes the surface X at time t = 0 and other textual descriptions upon executing an "examine" action. Unlike classic text-based games where partial observability is part of the challenge, in our case we can adopt the "friendly-environment" perspective and assist the learner with information such as state-tracking or action pruning (Liang et al., 2017;Johnson, 2017).</p>
<p>Proposed Solution Architecture</p>
<p>Our system consists of 6 core modules (Fig. 3): a Knowledge Base defines entity, relation and action vocabularies. This is used by the Surface Generator and Quest Generator modules to generate pairs (X,K) of synthetic surfaces and their corresponding action graphs for training. For un-annotated text, a pre-trained domain specific NER tagger 3 is used to extract an initial game state s 0 by identifying the mentioned entities. A learning agent extracts K from a generated game.</p>
<p>The TEXT2QUEST architecture supports three central modes of operation: (i) Enrich existing real world annotated pairs (X, K) by converting them to game instances for training the game-solving agent. (ii) Produce synthetic training pairs (X,K). (iii) Convert un-annotated texts to game instances for action graph extraction "in the wild".</p>
<p>The current version of TEXTLABS supports mode (ii). We implemented simple prototypes of the domain-specific Knowledge Base, plus Quest and Surface Generators. See Sec. A.1 for details about converting the entity and relation annotation schema into TextWorld. TextWorld is easily extensible and can support a variety of interaction semantics. Aside from adding a domain specific entity type-tree and actions, most of the underlying logic engine and interface is handled automatically. For the game environment, we use Inform7, a programming language and interpreter for text-based games. For quest generation, we currently use simple forward chaining and heuristic search strategies to create plausible quests (for example, all start materials must be incorporated into the synthesis route). Combining these with a simple rule-based Surface Generator already allows for creating simple synthetic training game instances (Fig. 2).</p>
<p>Preliminary Evaluation</p>
<p>As a very preliminary sanity check for the TEXT-LABS environment, we train a simple text-based RL agent on synthetic games in increasingly difficult environments. Difficulty is measured by maximum quest length, and the number of entities in the target action graph. See Sec. A.2 for representative examples. We use the basic LSTM-DQN agent of Narasimhan (2017) adapted to the TEXTLABS setting. The action space is
A = {W v × W o 1 × W o 2 },
where W v consists of 8 action-verbs corresponding to the entity relations tracked and additional native TextWorld actions like take (see Sec. A.1 for details). W o 1 , W o 2 are (identical) sets of potential arguments corresponding to the active entities which can be interacted with in the game (single and double argument actions allowed). As this basic agent is not conditioned on previous actions, we further concatenate the last four commands taken to the current observation. For the same reason, we also append the full quest instructions at every timestep's observation. All illegal actions are pruned at each state to reduce search space size.</p>
<p>We train the agent on 100 games per level and test on 10 games. Evaluation is measured by avg. normalized reward per game: 1 |K| T t=1 r i , where K is the true action sequence, T is the episode length (set to 50) and r i = 1 for each action in K and −1 for otherwise (and 0 for neutral actions like examine). A normalized score of 1 means the agent performed the required actions exactly.</p>
<p>As can be seen in Fig. 4, the agent learns to successfully perform the required actions only for the easiest levels. Examining longer games the agent did not complete, we note that the lack of conditioning on previous states is a serious limitation. Equipping agents with better sequence encoding (e.g., attention), recurrent memory, and utilizing state information is expected to significantly improve performance. Furthermore, due to technical limitations of the current implementation, some actions cannot be reversed. This adds to the difficulty of the task, and will be addressed in future versions. Finally, learning good initial policies for semantic parsers is known to be a hard problem with RL alone, and related approaches commonly use hybrid RL/supervised training methods (Liang et al., 2017;Jiang et al., 2012).</p>
<p>Discussion</p>
<p>Our approach faces tough challenges. However, we are encouraged by the significant recent advances towards these challenges in related areas, and plan to leverage this progress for our framework.</p>
<p>Programming semantics and rewards for instruction-following agents is known to be notoriously difficult (Winograd, 1972) as language and environments grow increasingly complex. Research on learned instruction-conditional reward models (Bahdanau et al., 2018) is a promising approach towards reducing the amount of "environment engineering" required.</p>
<p>Another critical open question in our framework is whether the surface generator will be able to generate surfaces representative enough to allow for generalization to real examples. Current NLG systems are increasingly capable of structured text generation (Marcheggiani and Perez-Beltrachini, 2018), and though they produce relatively short surfaces, we believe that coupling them with the generated action graphs is a promising approach to scaling up to longer sequences while maintaining coherence. Such systems can use sentence-level semantic parses as training data, meaning they can leverage existing weakly-supervised shallow parsing techniques. Encouraging for our modelling paradigm, recent work (Peng et al., 2018) extending the Dyna-Q (DQ) framework (Sutton, 1990) demonstrates a real-world application of structured NLG with a simulated RL training environment.</p>
<p>Given sufficient text generation capabilities, one may question the added utility of the game environment (as opposed to learning a direct mapping X → K). Recent research suggests that for stronger generalization, data alone may not be enough, and symbolic reasoning capabilities are necessary (Khashabi et al., 2018;Yi et al., 2018). Given the compositional complexity and difficulty of the language involved, we believe they will prove necessary in our setting as well.</p>
<p>Conclusions</p>
<p>There is a growing need for combining neurosymbolic reasoning with advanced language representation methods. In the case of procedural text understanding, key obstacles are suitable training environments, as well as the lack of fully annotated action graphs. Motivated by this, we proposed TEXT2QUEST, an approach intended to enhance learning by turning raw text inputs into a structured text-based game environment, as well as augmenting data with synthetic fully annotated action graphs. To encourage further research in this direction, we publicly release TEXTLABS, an instance of TEXT2QUEST for the materials synthesis task. We implemented prototype modules for basic game generation and solving. Future work will focus on designing learning agents to solve the games, as well as improving text generation capabilities. We hope that the proposed approach will lead to developing useful systems for action graph extraction as well as other language understanding tasks.</p>
<p>A Appendices</p>
<p>A.1 Entity &amp; Relation Types</p>
<p>We have claimed that converting an annotation schema to a game for TEXTLABS was relatively straightforward. In this section, we provide details of the mapping between the Synthesis Project annotation schema of (denoted with "SP" in the tables) to the TEXTLABS implementation (denoted "TL"). A mapping between the central entity types is presented in Figure 5, as well as the TEXTLABS actions and representative corresponding relations in the schema. All current TEXTLABS entities and actions are shown here, though not all of the original entities and relations are listed. For the full mapping, refer to the project source repository.</p>
<p>A.2 Synthetic Action-Graphs</p>
<p>Figure 6 displays sample representative generated quests for the various difficulty levels evaluated in Sec. 5, demonstrating the controllable complexity. As can be seen by comparison with the real text in Fig. 7 (which is only one sentence), these graphs correspond to short real-world surfaces, where even the hardest could by covered by a 2-3 sentence-long procedure.</p>
<p>A.3 Action-Graphs from Real Annotated Graphs</p>
<p>We now provide further details on how the original Synthesis Project (SP) annotated graphs can be converted to a TEXTLABS action graph K. There are some minor differences between the formats, primarily in the handling of the SP "next-operation" relation. Rather than use a "next-operation" relation, we currently opt to explicitly model inputs/outputs to operations, as can be seen in Fig.  7. This is a natural abstraction away from the surface text enabled by the grounded environment, and helps in tracking which materials participated in each operation, which is useful information for later analysis. Also, as noted, we currently use a simplified mapping (for example, many descriptor annotations such "Amount-Unit", "Property-Misc", etc. are chunked together as generic descriptors).</p>
<p>In Fig. 7 we show K both in action graph and action sequence form to demonstrate the equivalence. Also, we note that the "next-operation" annotations in MSP are currently just placeholders and not the true labels. For the purpose of demonstration, in Fig. 7 we manually add the correct annotation to our example (center and bottom).    </p>
<p>Figure 2 :
2Excerpt from an actual "material synthesis quest" generated by our system with example input/outputs.</p>
<p>Figure 3 :
3Proposed solution architecture of TEXT2QUEST. (i) Flow for training agent on games from real annotated data. (ii) Flow for training agent on synthetic games. (iii) Extracting action graph from un-annotated real data. multiple possible winning action sequences. See appendices A.2, A.3 for examples.</p>
<p>Figure 4 :
4Preliminary evaluation results for a basic LSTM-DQN text-RL agent on synthetic quests. Dotted line shows average generated quest lengths.</p>
<p>Figure 5 :
5Central entity/relation types from the Synthesis Project schema ("SP"), and the corresponding TEXT-LABS version ("TL").</p>
<p>Figure 6 :
6Sample representative generated quests for various difficulty levels (listed in parentheses by each graph). Each edge corresponds to an action in the text-based game.</p>
<p>Figure 7 :
7Comparisons of the equivalent action graph representations. Top: Action graph section from Synthesis Project (MSP). Center: TEXTLABS, showing same section with K in graph form. Dashed borders indicate operation result entities which may be implicit in the text. Bottom: TEXTLABS with same K as list of actions from initial state s 0 .</p>
<p>Material Null Currently ignored, not part of synthesis Brand Descriptor Apparatus-Descriptor Synthesis-Apparatus-Descriptor -Mixture Internal entity, represents a mixtureEntity Type (SP) </p>
<p>Entity Type (TL) 
Notes 
Material 
Material 
Number 
Descriptor 
Operation 
Operation 
Amount-Unit 
Descriptor 
Condition-Unit 
Operation-Descriptor 
Material-Descriptor 
Material-Descriptor 
Condition-Misc 
Operation-Descriptor 
Synthesis-Apparatus 
Synthesis-Apparatus 
Nonrecipe-Relation Type (SP) 
Action (TL) 
Participant-Material 
input-assign 
Apparatus-of 
locate 
Recipe-Target 
obtain 
Descriptor-of 
link-descriptor 
-
run-op 
Internal, used for simulating actions 
-
take/drop/examine 
Native TextWorld actions on entities </p>
<ul>
<li>Work was begun while author was an intern at RIKEN and continued at the Hebrew University.
Code and experiments available at https://github. com/ronentk/TextLabs
https://www.synthesisproject.org/
For the materials synthesis domain we use the tagger available at https://github.com/olivettigroup/ materials-synthesis-generative-models</li>
</ul>
<p>Learning to follow language instructions with adversarial reward induction. Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, Edward Grefenstette, abs/1806.01946CoRRDzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefen- stette. 2018. Learning to follow language instruc- tions with adversarial reward induction. CoRR, abs/1806.01946.</p>
<p>Relnet: End-to-end modeling of entities &amp; relations. CoRR. Trapit Bansal, Arvind Neelakantan, Andrew Mc-Callum, abs/1706.07179Trapit Bansal, Arvind Neelakantan, and Andrew Mc- Callum. 2017. Relnet: End-to-end modeling of enti- ties &amp; relations. CoRR, abs/1706.07179.</p>
<p>Simulating action dynamics with neural process networks. Antoine Bosselut, Omer Levy, Ari Holtzman, Corin Ennis, Dieter Fox, Yejin Choi, abs/1711.05313CoRRAntoine Bosselut, Omer Levy, Ari Holtzman, Corin Ennis, Dieter Fox, and Yejin Choi. 2017. Simulat- ing action dynamics with neural process networks. CoRR, abs/1711.05313.</p>
<p>Textworld: A learning environment for text-based games. Ákos Marc-Alexandre Côté, Xingdi Kádár, Ben Yuan, Tavian Kybartas, Emery Barnes, James Fine, Matthew Moore, Layla El Hausknecht, Mahmoud Asri, Wendy Adada, Adam Tay, Trischler, CoRR, abs/1806.11532Marc-Alexandre Côté,Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.</p>
<p>Building dynamic knowledge graphs from text using machine reading comprehension. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew Mccallum, abs/1810.05682CoRRRajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, and Andrew McCallum. 2018. Building dynamic knowledge graphs from text using machine reading comprehension. CoRR, abs/1810.05682.</p>
<p>Extracting action sequences from texts based on deep reinforcement learning. Wenfeng Feng, Subbarao Hankz Hankui Zhuo, Kambhampati, abs/1803.02632CoRRWenfeng Feng, Hankz Hankui Zhuo, and Subbarao Kambhampati. 2018. Extracting action sequences from texts based on deep reinforcement learning. CoRR, abs/1803.02632.</p>
<p>Analogues of mental simulation and imagination in deep learning. Current Opinion in Behavioral Sciences. B Jessica, Hamrick, Jessica B Hamrick. 2019. Analogues of mental sim- ulation and imagination in deep learning. Current Opinion in Behavioral Sciences.</p>
<p>Learned prioritization for trading off accuracy and speed. Jiarong Jiang, Adam R Teichert, Hal Daumé, Jason Eisner, NIPS. Jiarong Jiang, Adam R. Teichert, Hal Daumé, and Ja- son Eisner. 2012. Learned prioritization for trading off accuracy and speed. In NIPS.</p>
<p>Learning graphical state transitions. Daniel D Johnson, Daniel D. Johnson. 2017. Learning graphical state tran- sitions. In ICLR 2017.</p>
<p>Question answering as global reasoning over semantic abstractions. Daniel Khashabi, Tushar Khot, Ashutosh Sabharwal, Dan Roth, AAAI. Daniel Khashabi, Tushar Khot, Ashutosh Sabharwal, and Dan Roth. 2018. Question answering as global reasoning over semantic abstractions. In AAAI.</p>
<p>Edward Kim, Zach Jensen, Alexander Van Grootel, Kevin Huang, Matthew Staib, Sheshera Mysore, Haw-Shiuan Chang, Emma Strubell, Andrew Mc-Callum, Stefanie Jegelka, arXiv:1901.00032Inorganic materials synthesis planning with literature-trained neural networks. arXiv preprintEdward Kim, Zach Jensen, Alexander van Grootel, Kevin Huang, Matthew Staib, Sheshera Mysore, Haw-Shiuan Chang, Emma Strubell, Andrew Mc- Callum, Stefanie Jegelka, et al. 2018. Inorganic ma- terials synthesis planning with literature-trained neu- ral networks. arXiv preprint arXiv:1901.00032.</p>
<p>Zero-shot relation extraction via reading comprehension. Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer, abs/1706.04115CoRROmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. CoRR, abs/1706.04115.</p>
<p>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, Ni Lao, 10.18653/v1/P17-1003Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. 2017. Neural symbolic ma- chines: Learning semantic parsers on freebase with weak supervision. pages 23-33.</p>
<p>Object-oriented neural programming (oonp) for document understanding. Zhengdong Lu, Haotian Cui, Xianggen Liu, Yukun Yan, Daqi Zheng, ACL. Zhengdong Lu, Haotian Cui, Xianggen Liu, Yukun Yan, and Daqi Zheng. 2018. Object-oriented neu- ral programming (oonp) for document understand- ing. In ACL.</p>
<p>Are distributional representations ready for the real world? evaluating word vectors for grounded perceptual meaning. Li Lucy, Jon Gauthier, abs/1705.11168CoRRLi Lucy and Jon Gauthier. 2017. Are distributional representations ready for the real world? evaluat- ing word vectors for grounded perceptual meaning. CoRR, abs/1705.11168.</p>
<p>Toward an integration of deep learning and neuroscience. Adam H Marblestone, Gregory Wayne, Konrad P Körding, Front. Comput. Neurosci. Adam H. Marblestone, Gregory Wayne, and Konrad P. Körding. 2016. Toward an integration of deep learn- ing and neuroscience. In Front. Comput. Neurosci.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, abs/1810.09995CoRRDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. CoRR, abs/1810.09995.</p>
<p>Automatically extracting action graphs from materials science synthesis procedures. Sheshera Mysore, Edward Kim, Emma Strubell, Ao Liu, Haw-Shiuan Chang, Srikrishna Kompella, Kevin Huang, Andrew Mccallum, Elsa Olivetti, abs/1711.06872CoRRSheshera Mysore, Edward Kim, Emma Strubell, Ao Liu, Haw-Shiuan Chang, Srikrishna Kompella, Kevin Huang, Andrew McCallum, and Elsa Olivetti. 2017. Automatically extracting action graphs from materials science synthesis procedures. CoRR, abs/1711.06872.</p>
<p>Grounding natural language with autonomous interaction. Karthik Narasimhan, Cambridge, USAMassachusetts Institute of TechnologyPh.D. thesisKarthik Narasimhan. 2017. Grounding natural lan- guage with autonomous interaction. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, USA.</p>
<p>Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, Shang-Yu Su, Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning. Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su. 2018. Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning.</p>
<p>. Scott Reed, Nando De Freitas, Neural Programmer-Interpreters. Scott Reed and Nando de Freitas. Neural Programmer- Interpreters. pages 1-13.</p>
<p>Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. Richard S Sutton, Machine Learning Proceedings. Richard S. Sutton. 1990. Integrated Architectures for Learning, Planning, and Reacting Based on Approx- imating Dynamic Programming. In Machine Learn- ing Proceedings 1990.</p>
<p>Understanding natural language. Terry Winograd, Cognitive Psychology. 31Terry Winograd. 1972. Understanding natural lan- guage. Cognitive Psychology, 3(1):1 -191.</p>
<p>Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Joshua B Tenenbaum, Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. (NeurIPS). Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Tor- ralba, Pushmeet Kohli, and Joshua B. Tenenbaum. 2018. Neural-Symbolic VQA: Disentangling Rea- soning from Vision and Language Understanding. (NeurIPS).</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel Jaymin Mankowitz, Shie Mannor, NeurIPS. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel Jaymin Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In NeurIPS.</p>            </div>
        </div>

    </div>
</body>
</html>
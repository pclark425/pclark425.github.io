<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3980 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3980</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3980</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-bf8491bef353df126e2306ad2fe4b898697b906a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bf8491bef353df126e2306ad2fe4b898697b906a" target="_blank">A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is found that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks and is better at understanding non-Latin script languages than generating them.</p>
                <p><strong>Paper Abstract:</strong> This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn"prompt engineering"fashion. We also release codebase for evaluation set extraction.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3980.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3980.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multitask Zero-shot Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multitask, Multilingual, Multimodal Zero-shot Benchmarking of ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation paradigm used in the paper that measures ChatGPT performance across many standard NLP tasks in a zero-shot setting using public test sets (summarization, MT, sentiment, QA, TOD, misinformation detection, knowledge-grounded dialogue, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific performance (task accuracy/quality) using standard task criteria such as ROUGE for summarization, ChrF++ for machine translation, Macro F1 for sentiment, BLEU/JGA/Inform Rate for task-oriented dialogue, AUC for some QA tasks, BLEU/ROUGE/BLEU-1/ROUGE-L for open-domain generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Run ChatGPT in a zero-shot manner on sampled test subsets (30–200 examples per task), compute automatic metrics against references, and compare to prior zero-shot LLMs and fully fine-tuned SOTA models; include qualitative human judgment for some tasks (e.g., human judgment for knowledge-grounded dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>A suite of public datasets sampled for each task (examples: SAMSum and CNN/DM for summarization, FLORES-200 for MT, NusaX for sentiment/LID, MultiWOZ2.2 for TO D, OpenDialKG for knowledge-grounded dialogue, various QA sets). Each dataset is a standard benchmark for its respective NLP task.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>ROUGE-1/ROUGE-2 (summarization), ChrF++ (MT), Macro F1 (sentiment), Accuracy (QA, misinformation), BLEU/JGA/Inform Rate (TOD), BLEU/ROUGE/BLEU-1/ROUGE-L (open-domain KGD), AUC (Pep-3k), plus numeric comparisons to fine-tuned SOTA and zero-shot SOTA baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual sampling and manual validation in selected cases (e.g., native-speaker validation for translation correctness, human judgment on dialogue quality); comparisons to previously reported human-evaluated SOTA where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited sample sizes (30–200 samples per task) due to use of UI; zero-shot evaluation may not reflect tuned or few-shot performance; automatic metrics may poorly correlate with human preference for long or stylistic outputs; ChatGPT's internal pretraining knowledge can confound reference-based automatic evaluation (responses longer or containing non-ground-truth knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT outperforms prior zero-shot LLMs on 9/13 reported datasets and sometimes exceeds fine-tuned models on a few tasks, but underperforms fully fine-tuned task-specific models on many dialogue and knowledge-grounded tasks; specific reported numbers include e.g., summarization ROUGE-1 ~35.29 on sampled subsets, MT ChrF++ variable by language group (e.g., 58.64 HRL → Eng), sentiment Macro F1 up to 83.24 for English (sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3980.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3980.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning Evaluation (Fine-grained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained Reasoning Evaluation across Logical, Commonsense, Mathematical, Temporal, Spatial, Multi-hop and Other Categories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted evaluation that decomposes 'reasoning' into multiple categories (deductive, inductive, abductive, analogical, causal, multi-hop, mathematical, temporal, spatial, commonsense) and tests ChatGPT on representative QA datasets for each.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness/accuracy of answers; correctness of generated rationales and explanations when provided; category-specific reasoning success (e.g., induction vs. deduction).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Select QA datasets that predominantly require one reasoning type, sample ~30 instances per dataset, prompt ChatGPT, manually check accuracy of answers and verify rationales/explanations; also conduct ablation/prompting variants (e.g., explicitly ask for inference) to test sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Representative benchmarks used include: bAbI (inductive/deductive tasks), EntailmentBank (deductive), CLUTRR (inductive), αNLI (abductive), MATH (mathematical reasoning), TIMEDIAL (temporal), SPARTQA and StepGame (spatial), CommonsenseQA/PIQA/Pep-3k (commonsense), HotpotQA (multi-hop), Letter string analogy (analogical).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-dataset accuracy counts (e.g., number correct out of 30) reported in Table 5; an aggregate average reported in abstract (63.41% accurate on average across 10 reasoning categories). Per-category numeric results: Deductive ~28/30, Inductive ~13/30 (CLUTRR) or 20/30 with prompting, Mathematical 13/30, Temporal 26/30, Spatial varied (e.g., SpartQA 8/32 hard/basic combined), Multi-hop (HotpotQA) 8/30, Analogical 30/30, Commonsense ~25-28/30 across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual human verification of correctness and rationales; prompts and evaluation decisions made by human experimenters; dataset ground-truth from human-annotated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small sample sizes; reasoning categories can overlap making pure categorization difficult; ChatGPT is a 'lazy reasoner' (per authors) and performance is sensitive to prompt phrasing (explicitly requesting inference improved induction); inconsistent across reasoning types (poor at mathematical and spatial reasoning, multi-hop reasoning, better at analogical and many commonsense tasks). The evaluation calls for more comprehensive benchmarks for multi-ability tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Findings: ChatGPT is unreliable as a reasoner: relatively strong on deductive, analogical and many commonsense tasks, weak on inductive (unless prompted), poor at mathematical and spatial reasoning and multi-hop tasks; specific counts include e.g., MATH 13/30, HotpotQA 8/30, EntailmentBank 28/30.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3980.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3980.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factuality & Hallucination Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factuality and Hallucination Evaluation using Fact-checking and QA Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of ChatGPT's propensity to generate nonfactual or unverifiable statements (hallucinations) and its ability to detect misinformation using COVID-related claim datasets and TruthfulQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy in veracity classification (detecting misinformation), refusal rates on social claims, degree/type of hallucination (extrinsic vs. intrinsic), and propensity to produce imitative falsehoods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Run ChatGPT on fact-checking and QA test sets (COVID scientific/social claim sets, TruthfulQA), measure accuracy in veracity judgments (manual checking), analyze generated responses for hallucination examples, classify hallucination types (extrinsic/intrinsic), and report refusal behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>COVID-Scientific and COVID-Social claim sets (Lee et al., 2021) for misinformation detection; TruthfulQA (Lin et al., 2022) to test tendency to mimic human falsehoods; other QA sets where hallucinations observed (e.g., machine translation, QA outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy on COVID datasets: 92% (46/50) on covid-scientific and 73.33% (22/30, excluding verification-refusing cases) on covid-social; TruthfulQA failure rate: ChatGPT fails to answer truthfully 35.38% of the time on a 66-sample subset. Qualitative examples and categorizations of hallucinations provided (extrinsic hallucinations dominant).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual adjudication of veracity and identification of hallucinations; human reviewers determine whether ChatGPT's claims are verifiable from sources and classify hallucination types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ChatGPT tends to produce extrinsic hallucinations (factual claims not supported by source), may refuse societal claims, and shows memorization/imitative falsehoods; manual evaluation required for fine-grained hallucination detection; need for scalable automatic detectors and external knowledge grounding to mitigate hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT performs strongly on scientific COVID claims (92%) but less well on social claims and exhibits a substantial rate of untruthful responses on TruthfulQA (~35% failure), demonstrating hallucination and imitation-of-falsehood problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3980.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3980.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interactivity / Multi-turn Prompting Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation of Interactivity (Multi-turn Dialogues) for Performance Improvement and Post-editing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessing how ChatGPT's conversational multi-turn interface and feedback loops (prompt engineering) can be used to refine outputs and improve task metrics (summarization, MT post-editing, multimodal image/code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in task metrics after iterative prompts or post-editing (e.g., ROUGE, ChrF++), qualitative improvements in generated content (shorter summaries, corrected translations, corrected SVG drawings), and measured error reduction in image/code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Two- or three-turn iterative prompting where users ask for refinements (e.g., 'Please make the summary shorter') or provide postediting instructions; evaluate metric changes between turns; conduct ablation (InstructGPT vs ChatGPT) to show the effect of conversational capability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SAMSum subset (50 docs) for summarization multi-turn; FLORES-200 sampled pairs for MT + automatic post-editing experiment; flag-drawing dataset for multimodal iterative post-editing.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Summarization: +7.99 ROUGE-1 and +1.64 ROUGE-2 from a single length-control follow-up prompt (reported as ~8% ROUGE-1 gain in paper summary). MT: up to ~2% ChrF++ improvement reported for low-resource MT through iterative post-editing. Flag drawing: per-turn error-rate improvements (34% of samples improved from turn1→turn2 and 36% from turn2→turn3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human experimenters craft follow-up prompts and judge whether iterative changes improved output; native speakers manually validate corrected translations; human evaluation used for qualitative judgments in image generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Interactivity sometimes fails to correct errors (model may retain wrong answers across turns), improvements depend on prompt quality and multi-turn state maintenance; evaluation is limited by sample sizes and by comparing against non-conversational variants (InstructGPT), and ground-truth metrics may not capture human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Multi-turn interaction can improve quality: notable gains reported include ~8% ROUGE-1 on summarization and ~2% ChrF++ on MT; iterative SVG post-editing reduces errors in many flag examples but some error types (shape/size) remain frequent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3980.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3980.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodality (Flag Drawing) Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-mediated Multimodal Evaluation via Flag Drawing using SVG Code Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dedicated multimodal evaluation that uses code generation (SVG, HTML Canvas, Python Turtle) as an intermediate to test ChatGPT's ability to convert textual descriptions into images and iteratively correct them via multi-turn interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Image correctness measured by four error types (layout, color, missing components, shape/size) and an aggregate 5-grade quality scale (A–E corresponding to 0–4+ errors). Improvement across turns is measured.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Design a national-flag drawing task: (1) ask ChatGPT to describe the flag, (2) ask it to generate SVG code from the description, (3) request iterative fixes if errors remain; evaluate rendered SVGs against ground-truth flags and code by counting error types and assigning grades; ablation removing description step to test chain-of-thought benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>A 50-national-flag dataset constructed by the authors spanning continents; flag ground truth used for evaluation of generated SVG images.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Grade distribution per turn (Table 4); error-type frequencies (shape/size errors 68% of time, layout 34%, color 20%, missing components 18%); per-turn improvement rates (34% improved from turn1→turn2, 36% from turn2→turn3); ablation shows dramatic performance drop without pre-generated description.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators evaluate the rendered SVG images vs ground truth, assign grades, and categorize errors; prompts and interactions scripted by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ChatGPT's multimodal output is elementary compared to native vision-language models because it never directly observes visual data; predominant failures are in fine shape/size rendering (e.g., complex emblem shapes like maple leaf), and quality depends on disentangling description-then-generation (chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT can produce plausible flags via code; performance improves with self-generated textual descriptions prior to code generation; majority of residual errors are shape/size related; iterative post-editing reduces errors in a substantive fraction of cases but does not fully close the gap to visual models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3980.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3980.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multilingual Evaluation (Understanding & Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilingual Evaluation of Language Understanding and Generation in High- and Low-resource Languages</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation that measures ChatGPT's performance on language understanding tasks (sentiment analysis, language ID) and generation tasks (machine translation) across language-resource categories (HRL, MRL, LRL, X-LRL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy for classification tasks (e.g., Macro F1 for sentiment, accuracy for LID) and correctness of translations (manual native-speaker validation) plus automatic metrics (ChrF++ for MT).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Sample sentences from multilingual benchmarks (NusaX for sentiment/LID, FLORES-200 for MT), run ChatGPT zero-shot on these samples, compute automatic metrics where applicable and perform native-speaker manual validation for translation correctness; analyze performance by language resource group.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>NusaX (multilingual sentiment and language ID across Indonesian local languages) and FLORES-200 (multilingual translation benchmark sampling HRL and LRL languages).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Macro F1 for sentiment per language (e.g., English 83.24 Macro F1 on sampled subset), LID accuracy (English 100%, Javanese 0% in some cases), ChrF++ scores for MT reported per language group (e.g., ChatGPT 58.64 for some HRL → Eng cases, much lower for LRL). Manual counts of correct translations per 30-sample subset shown in Table 3 (e.g., French 29/30 XXX→Eng, Sundanese 0/30 Eng→XXX).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Native speaker manual validation of translations for correctness; humans also annotate and analyze failure modes (e.g., translating into a related but wrong language).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reduced performance on low-resource and extremely low-resource languages, especially in generation (Eng→XXX) and for non-Latin scripts; tendency to hallucinate in low-resource translations; evaluation limited by small sample sizes and reliance on manual validation for final correctness judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT shows strong understanding and generation for high-resource languages but degrades on low-resource languages; it understands some low-resource languages (Javanese) but may fail to identify them; generation to low-resource targets is notably weak (several languages had few or zero correct Eng→XXX translations in 30-sample tests).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>No Language Left Behind: Scaling human-centered machine translation <em>(Rating: 2)</em></li>
                <li>EntailmentBank <em>(Rating: 2)</em></li>
                <li>Towards AI-complete question answering: bAbI tasks <em>(Rating: 2)</em></li>
                <li>MATH: analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>FLORES-200 evaluation benchmark for low-resource and multilingual machine translation <em>(Rating: 2)</em></li>
                <li>SAMSum: A human-annotated dialogue dataset for abstractive summarization <em>(Rating: 1)</em></li>
                <li>OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3980",
    "paper_id": "paper-bf8491bef353df126e2306ad2fe4b898697b906a",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "Multitask Zero-shot Benchmarking",
            "name_full": "Multitask, Multilingual, Multimodal Zero-shot Benchmarking of ChatGPT",
            "brief_description": "Evaluation paradigm used in the paper that measures ChatGPT performance across many standard NLP tasks in a zero-shot setting using public test sets (summarization, MT, sentiment, QA, TOD, misinformation detection, knowledge-grounded dialogue, etc.).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Task-specific performance (task accuracy/quality) using standard task criteria such as ROUGE for summarization, ChrF++ for machine translation, Macro F1 for sentiment, BLEU/JGA/Inform Rate for task-oriented dialogue, AUC for some QA tasks, BLEU/ROUGE/BLEU-1/ROUGE-L for open-domain generation.",
            "evaluation_methods": "Run ChatGPT in a zero-shot manner on sampled test subsets (30–200 examples per task), compute automatic metrics against references, and compare to prior zero-shot LLMs and fully fine-tuned SOTA models; include qualitative human judgment for some tasks (e.g., human judgment for knowledge-grounded dialogue).",
            "benchmark_or_dataset": "A suite of public datasets sampled for each task (examples: SAMSum and CNN/DM for summarization, FLORES-200 for MT, NusaX for sentiment/LID, MultiWOZ2.2 for TO D, OpenDialKG for knowledge-grounded dialogue, various QA sets). Each dataset is a standard benchmark for its respective NLP task.",
            "metrics_reported": "ROUGE-1/ROUGE-2 (summarization), ChrF++ (MT), Macro F1 (sentiment), Accuracy (QA, misinformation), BLEU/JGA/Inform Rate (TOD), BLEU/ROUGE/BLEU-1/ROUGE-L (open-domain KGD), AUC (Pep-3k), plus numeric comparisons to fine-tuned SOTA and zero-shot SOTA baselines.",
            "human_involvement": "Manual sampling and manual validation in selected cases (e.g., native-speaker validation for translation correctness, human judgment on dialogue quality); comparisons to previously reported human-evaluated SOTA where applicable.",
            "limitations_or_challenges": "Limited sample sizes (30–200 samples per task) due to use of UI; zero-shot evaluation may not reflect tuned or few-shot performance; automatic metrics may poorly correlate with human preference for long or stylistic outputs; ChatGPT's internal pretraining knowledge can confound reference-based automatic evaluation (responses longer or containing non-ground-truth knowledge).",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT outperforms prior zero-shot LLMs on 9/13 reported datasets and sometimes exceeds fine-tuned models on a few tasks, but underperforms fully fine-tuned task-specific models on many dialogue and knowledge-grounded tasks; specific reported numbers include e.g., summarization ROUGE-1 ~35.29 on sampled subsets, MT ChrF++ variable by language group (e.g., 58.64 HRL → Eng), sentiment Macro F1 up to 83.24 for English (sampled).",
            "uuid": "e3980.0",
            "source_info": {
                "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Reasoning Evaluation (Fine-grained)",
            "name_full": "Fine-grained Reasoning Evaluation across Logical, Commonsense, Mathematical, Temporal, Spatial, Multi-hop and Other Categories",
            "brief_description": "A targeted evaluation that decomposes 'reasoning' into multiple categories (deductive, inductive, abductive, analogical, causal, multi-hop, mathematical, temporal, spatial, commonsense) and tests ChatGPT on representative QA datasets for each.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness/accuracy of answers; correctness of generated rationales and explanations when provided; category-specific reasoning success (e.g., induction vs. deduction).",
            "evaluation_methods": "Select QA datasets that predominantly require one reasoning type, sample ~30 instances per dataset, prompt ChatGPT, manually check accuracy of answers and verify rationales/explanations; also conduct ablation/prompting variants (e.g., explicitly ask for inference) to test sensitivity.",
            "benchmark_or_dataset": "Representative benchmarks used include: bAbI (inductive/deductive tasks), EntailmentBank (deductive), CLUTRR (inductive), αNLI (abductive), MATH (mathematical reasoning), TIMEDIAL (temporal), SPARTQA and StepGame (spatial), CommonsenseQA/PIQA/Pep-3k (commonsense), HotpotQA (multi-hop), Letter string analogy (analogical).",
            "metrics_reported": "Per-dataset accuracy counts (e.g., number correct out of 30) reported in Table 5; an aggregate average reported in abstract (63.41% accurate on average across 10 reasoning categories). Per-category numeric results: Deductive ~28/30, Inductive ~13/30 (CLUTRR) or 20/30 with prompting, Mathematical 13/30, Temporal 26/30, Spatial varied (e.g., SpartQA 8/32 hard/basic combined), Multi-hop (HotpotQA) 8/30, Analogical 30/30, Commonsense ~25-28/30 across datasets.",
            "human_involvement": "Manual human verification of correctness and rationales; prompts and evaluation decisions made by human experimenters; dataset ground-truth from human-annotated benchmarks.",
            "limitations_or_challenges": "Small sample sizes; reasoning categories can overlap making pure categorization difficult; ChatGPT is a 'lazy reasoner' (per authors) and performance is sensitive to prompt phrasing (explicitly requesting inference improved induction); inconsistent across reasoning types (poor at mathematical and spatial reasoning, multi-hop reasoning, better at analogical and many commonsense tasks). The evaluation calls for more comprehensive benchmarks for multi-ability tasks.",
            "llm_theory_example": null,
            "evaluation_results": "Findings: ChatGPT is unreliable as a reasoner: relatively strong on deductive, analogical and many commonsense tasks, weak on inductive (unless prompted), poor at mathematical and spatial reasoning and multi-hop tasks; specific counts include e.g., MATH 13/30, HotpotQA 8/30, EntailmentBank 28/30.",
            "uuid": "e3980.1",
            "source_info": {
                "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Factuality & Hallucination Evaluation",
            "name_full": "Factuality and Hallucination Evaluation using Fact-checking and QA Datasets",
            "brief_description": "Evaluation of ChatGPT's propensity to generate nonfactual or unverifiable statements (hallucinations) and its ability to detect misinformation using COVID-related claim datasets and TruthfulQA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Accuracy in veracity classification (detecting misinformation), refusal rates on social claims, degree/type of hallucination (extrinsic vs. intrinsic), and propensity to produce imitative falsehoods.",
            "evaluation_methods": "Run ChatGPT on fact-checking and QA test sets (COVID scientific/social claim sets, TruthfulQA), measure accuracy in veracity judgments (manual checking), analyze generated responses for hallucination examples, classify hallucination types (extrinsic/intrinsic), and report refusal behavior.",
            "benchmark_or_dataset": "COVID-Scientific and COVID-Social claim sets (Lee et al., 2021) for misinformation detection; TruthfulQA (Lin et al., 2022) to test tendency to mimic human falsehoods; other QA sets where hallucinations observed (e.g., machine translation, QA outputs).",
            "metrics_reported": "Accuracy on COVID datasets: 92% (46/50) on covid-scientific and 73.33% (22/30, excluding verification-refusing cases) on covid-social; TruthfulQA failure rate: ChatGPT fails to answer truthfully 35.38% of the time on a 66-sample subset. Qualitative examples and categorizations of hallucinations provided (extrinsic hallucinations dominant).",
            "human_involvement": "Manual adjudication of veracity and identification of hallucinations; human reviewers determine whether ChatGPT's claims are verifiable from sources and classify hallucination types.",
            "limitations_or_challenges": "ChatGPT tends to produce extrinsic hallucinations (factual claims not supported by source), may refuse societal claims, and shows memorization/imitative falsehoods; manual evaluation required for fine-grained hallucination detection; need for scalable automatic detectors and external knowledge grounding to mitigate hallucination.",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT performs strongly on scientific COVID claims (92%) but less well on social claims and exhibits a substantial rate of untruthful responses on TruthfulQA (~35% failure), demonstrating hallucination and imitation-of-falsehood problems.",
            "uuid": "e3980.2",
            "source_info": {
                "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Interactivity / Multi-turn Prompting Evaluation",
            "name_full": "Evaluation of Interactivity (Multi-turn Dialogues) for Performance Improvement and Post-editing",
            "brief_description": "Assessing how ChatGPT's conversational multi-turn interface and feedback loops (prompt engineering) can be used to refine outputs and improve task metrics (summarization, MT post-editing, multimodal image/code generation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Improvement in task metrics after iterative prompts or post-editing (e.g., ROUGE, ChrF++), qualitative improvements in generated content (shorter summaries, corrected translations, corrected SVG drawings), and measured error reduction in image/code generation.",
            "evaluation_methods": "Two- or three-turn iterative prompting where users ask for refinements (e.g., 'Please make the summary shorter') or provide postediting instructions; evaluate metric changes between turns; conduct ablation (InstructGPT vs ChatGPT) to show the effect of conversational capability.",
            "benchmark_or_dataset": "SAMSum subset (50 docs) for summarization multi-turn; FLORES-200 sampled pairs for MT + automatic post-editing experiment; flag-drawing dataset for multimodal iterative post-editing.",
            "metrics_reported": "Summarization: +7.99 ROUGE-1 and +1.64 ROUGE-2 from a single length-control follow-up prompt (reported as ~8% ROUGE-1 gain in paper summary). MT: up to ~2% ChrF++ improvement reported for low-resource MT through iterative post-editing. Flag drawing: per-turn error-rate improvements (34% of samples improved from turn1→turn2 and 36% from turn2→turn3).",
            "human_involvement": "Human experimenters craft follow-up prompts and judge whether iterative changes improved output; native speakers manually validate corrected translations; human evaluation used for qualitative judgments in image generation.",
            "limitations_or_challenges": "Interactivity sometimes fails to correct errors (model may retain wrong answers across turns), improvements depend on prompt quality and multi-turn state maintenance; evaluation is limited by sample sizes and by comparing against non-conversational variants (InstructGPT), and ground-truth metrics may not capture human preference.",
            "llm_theory_example": null,
            "evaluation_results": "Multi-turn interaction can improve quality: notable gains reported include ~8% ROUGE-1 on summarization and ~2% ChrF++ on MT; iterative SVG post-editing reduces errors in many flag examples but some error types (shape/size) remain frequent.",
            "uuid": "e3980.3",
            "source_info": {
                "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Multimodality (Flag Drawing) Evaluation",
            "name_full": "Code-mediated Multimodal Evaluation via Flag Drawing using SVG Code Generation",
            "brief_description": "A dedicated multimodal evaluation that uses code generation (SVG, HTML Canvas, Python Turtle) as an intermediate to test ChatGPT's ability to convert textual descriptions into images and iteratively correct them via multi-turn interaction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Image correctness measured by four error types (layout, color, missing components, shape/size) and an aggregate 5-grade quality scale (A–E corresponding to 0–4+ errors). Improvement across turns is measured.",
            "evaluation_methods": "Design a national-flag drawing task: (1) ask ChatGPT to describe the flag, (2) ask it to generate SVG code from the description, (3) request iterative fixes if errors remain; evaluate rendered SVGs against ground-truth flags and code by counting error types and assigning grades; ablation removing description step to test chain-of-thought benefit.",
            "benchmark_or_dataset": "A 50-national-flag dataset constructed by the authors spanning continents; flag ground truth used for evaluation of generated SVG images.",
            "metrics_reported": "Grade distribution per turn (Table 4); error-type frequencies (shape/size errors 68% of time, layout 34%, color 20%, missing components 18%); per-turn improvement rates (34% improved from turn1→turn2, 36% from turn2→turn3); ablation shows dramatic performance drop without pre-generated description.",
            "human_involvement": "Human annotators evaluate the rendered SVG images vs ground truth, assign grades, and categorize errors; prompts and interactions scripted by humans.",
            "limitations_or_challenges": "ChatGPT's multimodal output is elementary compared to native vision-language models because it never directly observes visual data; predominant failures are in fine shape/size rendering (e.g., complex emblem shapes like maple leaf), and quality depends on disentangling description-then-generation (chain-of-thought).",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT can produce plausible flags via code; performance improves with self-generated textual descriptions prior to code generation; majority of residual errors are shape/size related; iterative post-editing reduces errors in a substantive fraction of cases but does not fully close the gap to visual models.",
            "uuid": "e3980.4",
            "source_info": {
                "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Multilingual Evaluation (Understanding & Generation)",
            "name_full": "Multilingual Evaluation of Language Understanding and Generation in High- and Low-resource Languages",
            "brief_description": "Evaluation that measures ChatGPT's performance on language understanding tasks (sentiment analysis, language ID) and generation tasks (machine translation) across language-resource categories (HRL, MRL, LRL, X-LRL).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Accuracy for classification tasks (e.g., Macro F1 for sentiment, accuracy for LID) and correctness of translations (manual native-speaker validation) plus automatic metrics (ChrF++ for MT).",
            "evaluation_methods": "Sample sentences from multilingual benchmarks (NusaX for sentiment/LID, FLORES-200 for MT), run ChatGPT zero-shot on these samples, compute automatic metrics where applicable and perform native-speaker manual validation for translation correctness; analyze performance by language resource group.",
            "benchmark_or_dataset": "NusaX (multilingual sentiment and language ID across Indonesian local languages) and FLORES-200 (multilingual translation benchmark sampling HRL and LRL languages).",
            "metrics_reported": "Macro F1 for sentiment per language (e.g., English 83.24 Macro F1 on sampled subset), LID accuracy (English 100%, Javanese 0% in some cases), ChrF++ scores for MT reported per language group (e.g., ChatGPT 58.64 for some HRL → Eng cases, much lower for LRL). Manual counts of correct translations per 30-sample subset shown in Table 3 (e.g., French 29/30 XXX→Eng, Sundanese 0/30 Eng→XXX).",
            "human_involvement": "Native speaker manual validation of translations for correctness; humans also annotate and analyze failure modes (e.g., translating into a related but wrong language).",
            "limitations_or_challenges": "Reduced performance on low-resource and extremely low-resource languages, especially in generation (Eng→XXX) and for non-Latin scripts; tendency to hallucinate in low-resource translations; evaluation limited by small sample sizes and reliance on manual validation for final correctness judgment.",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT shows strong understanding and generation for high-resource languages but degrades on low-resource languages; it understands some low-resource languages (Javanese) but may fail to identify them; generation to low-resource targets is notably weak (several languages had few or zero correct Eng→XXX translations in 30-sample tests).",
            "uuid": "e3980.5",
            "source_info": {
                "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 2
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "No Language Left Behind: Scaling human-centered machine translation",
            "rating": 2
        },
        {
            "paper_title": "EntailmentBank",
            "rating": 2
        },
        {
            "paper_title": "Towards AI-complete question answering: bAbI tasks",
            "rating": 2
        },
        {
            "paper_title": "MATH: analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "FLORES-200 evaluation benchmark for low-resource and multilingual machine translation",
            "rating": 2
        },
        {
            "paper_title": "SAMSum: A human-annotated dialogue dataset for abstractive summarization",
            "rating": 1
        },
        {
            "paper_title": "OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs",
            "rating": 1
        }
    ],
    "cost": 0.017877,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity</h1>
<p>Yejin Bang<em> Samuel Cahyawijaya Nayeon Lee Wenliang Dai Dan Su Bryan Wilie Holy Lovenia Ziwei Ji Tiezheng Yu Willy Chung Quyet V. Do Yan Xu Pascale Fung</em><br>Centre for Artificial Intelligence Research (CAiRE)<br>The Hong Kong University of Science and Technology<br>yjbang@connect.ust.hk, pascale@ece.ust.hk</p>
<h4>Abstract</h4>
<p>This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets, using 23 data sets covering 8 different common NLP application tasks. We extensively evaluate the multitask, multilingual, and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zeroshot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts via an intermediate code generation step. Moreover, we find that ChatGPT is $63.41 \%$ accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. ChatGPT suffers from hallucination problems like other LLMs. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e., $\mathbf{8 \%}$ ROUGE-1 on summarization and $\mathbf{2 \%} \mathbf{C h r F + +}$ on machine translation, in a multi-turn "prompt engineering" fashion. We release a code for evaluation set extraction. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>ChatGPT is a successor of the large language model (LLM) InstructGPT (Ouyang et al., 2022) with a dialog interface that is fine-tuned using the Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2017) approach. ChatGPT has gathered 100 million monthly active users in such a short period of time (Hu, 2023) and is being used by businesses and consumers alike for a myriad of mostly textual tasks. One reason for its unprecedented popularity is that ChatGPT, through</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>its scale and via RLHF, has shown impressive abilities in many areas of NLP as well as emergent abilities. Another reason is that its dialog interface allows users to interact with the underlying LLM more effectively and efficiently via interactive chats that are akin to multi-turn prompting.</p>
<p>However, despite its powerful abilities, anecdotal reports on ChatGPT consistently showed remaining challenges - for example, it fails in some elementary mathematical (Gilson et al., 2022; Goldberg, 2023; Frieder et al., 2023; Choi et al., 2023; Davis, 2023b) and commonsense reasoning tasks (Guo et al., 2023; Davis, 2023b); it hallucinates with human-like fluency and eloquence on things that are not based on truth (Shen et al., 2023; Thorp, 2023; Smith, 2023); and as a generalpurpose language model trained from everything on the web, its language coverage is questionable ( Lu et al., 2022; Jiao et al., 2023). Consequently, it is not clear what people can or cannot use ChatGPT for despite its popularity.</p>
<p>Since OpenAI never published any benchmarking results on ChatGPT at the time, seeing this need, in February 2023, we proposed a comprehensive framework for quantitatively evaluating interactive LLMs such as ChatGPT through standard public test sets on major NLP tasks such as question answering, reasoning, summarization, machine translation, sentiment analysis, language identification, task-oriented dialogue, and misinformation detection. We evaluate its multilingual performance as well as vision-language multimodal abilities. With additional experiments, we also quantitatively evaluated its primary limitations in reasoning and hallucination. In addition, we conducted experiments to test its multi-turn interactivity as a means for better prompt engineering. We aimed to provide insights to users of ChatGPT on the strengths mentioned above and limitations, as well as how they can improve outcomes with interactivity. To the best of our knowledge, this is the first published</p>
<p>benchmark of ChatGPT from a third party. More recently, the GPT-4 technical report (OpenAI, 2023) published a number of human task benchmarks.</p>
<p>The true scope of all emergent capabilities of generative models, including ChatGPT, is still unclear. Thus, any benchmarking exercise cannot be $100 \%$ "comprehensive" in the scientific sense. We aim to show not just researchers but also users what ChatGPT can and cannot do by presenting interpretable benchmarking results in a zero-shot setting without access to APIs so that the general audience can replicate our evaluation with the test sets we have provided in a zero-shot setting. This version of ChatGPT is 15 December 2022.</p>
<p>The following are the major insights we have gained from the evaluations:</p>
<p>Multitask, Multimodal, and Multilingual: For 9/13 NLP datasets, ChatGPT outperforms previous LLMs with zero-shot learning. It even outperforms fully fine-tuned task-specific LMs on 4 different tasks. In other cases, ChatGPT is on par or slightly lower than fully fine-tuned for specific NLP tasks; ChatGPT fails to generalize to lowresource and extremely low-resource languages (e.g., Marathi, Sundanese, and Buginese). There is an overall performance degradation in low-resource languages, especially in non-Latin scripts in the case of translation; its weakness lies in generation rather than understanding part of the translation process; ChatGPT enables a code intermediate medium to bridge vision and language, even though the multi-modality ability is still elementary compared to vision-language models.</p>
<p>Reasoning: We tested 10 different reasoning categories with 634 samples in total. Based on our experiments, ChatGPT shows more weakness in inductive reasoning than in deductive or abductive reasoning. ChatGPT also lacks spatial and mathematical reasoning while showing better temporal reasoning. Further, we found that ChatGPT is relatively better at commonsense reasoning than nontextual semantic reasoning. Finally, while ChatGPT shows acceptable performance in causal and analogical reasoning, it is bad at multi-hop reasoning capability, similar to other LLMs' weakness (Ott et al., 2023).</p>
<p>Hallucination: Similar to other LLMs (Radford et al., 2019; Muennighoff et al., 2022; Workshop et al., 2022), ChatGPT suffers from the hallucination problem. It generates more extrinsic hallucinations - factual statements that cannot be
verified from the source.
Interactivity: One of the primary differentiating factors of ChatGPT from its predecessors is its multi-turn dialog interactivity. This enables ChatGPT to perform multiple tasks within a dialog session. There is also significant performance improvement ( $8 \%$ ROUGE-1 on summarization and $2 \% \mathrm{ChrF}++$ on low-resource machine translation) via multi-turn interactivity in various standard NLP tasks. This process is akin to prompt engineering with feedback from the system.</p>
<h2>2 Multitask, Multilingual, and Multimodal Evaluations of ChatGPT</h2>
<h3>2.1 Multitask Ability of ChatGPT</h3>
<p>ChatGPT has become very well-known in such a short period of time to general public users, not just those who are in AI, machine learning, and NLP communities who might be more familiar with LLMs. One of the main reasons is that, in addition to media reports, innumerable use cases of ChatGPT are shared by both non-academic and academic users online (Marr, 2022; Gordon, 2023; Shankland, 2023). There have been debates and panels on whether ChatGPT is approaching Artificial General Intelligence, as it seems to be able to carry out a multitude of tasks without specific fine-tuning (Desk, 2023; Johnson, 2023; Kingson, 2023). On the other hand, there has also been as much sharing of its failures in simple tasks (Gilson et al., 2022; Choi et al., 2023; Shen et al., 2023).</p>
<p>Instead of relying on anecdotal examples, we first evaluate ChatGPT's performance in various standard NLP tasks in a zero-shot manner to obtain a basic/better understanding of its multi-task ability. We compile results from the existing literature on ChatGPT and compare them with the state-of-the-art fully-fine-tuned and zero-shot models across multiple tasks. We evaluate ChatGPT performances on 21 datasets covering 8 tasks, i.e., summarization, machine translation, sentiment analysis, question answering, task-oriented dialogue, open-domain knowledge-grounded dialogue, and misinformation detection tasks. We sample testing cases from existing standard test sets for each task with a sample size ranging from 30 to 200 samples.</p>
<p>Multitask Generalization of ChatGPT The result of the multitask evaluation is shown in Table 1. ChatGPT is shown to achieve remarkable zero-shot performances on multiple tasks, surpassing pre-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Reference</th>
<th style="text-align: center;">Fine-Tuned <br> SOTA</th>
<th style="text-align: center;">Zero-Shot <br> SOTA</th>
<th style="text-align: center;">ChatGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Summarization</td>
<td style="text-align: center;">CNN/DM <br> SAMSum</td>
<td style="text-align: center;">ROUGE-1 <br> ROUGE-1</td>
<td style="text-align: center;">Lewis et al. (2020a) <br> Lewis et al. (2020a)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 44.47 \ &amp; 47.28 \end{aligned}$</td>
<td style="text-align: center;">$35.27^{\circ}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 35.29 \ &amp; 35.29 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MT <br> (XXX $\rightarrow$ Eng)</td>
<td style="text-align: center;">FLoRes-200 (HRL) <br> FLoRes-200 (LRL)</td>
<td style="text-align: center;">$\mathrm{ChrF++}$ <br> $\mathrm{ChrF++}$</td>
<td style="text-align: center;">Team et al. (2022) <br> Team et al. (2022)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 63.5 \ &amp; 54.9 \end{aligned}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{aligned} &amp; 58.64 \ &amp; 27.75 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MT <br> (Eng $\rightarrow$ XXX)</td>
<td style="text-align: center;">FLoRes-200 (HRL) <br> FLoRes-200 (LRL)</td>
<td style="text-align: center;">$\mathrm{ChrF++}$ <br> $\mathrm{ChrF++}$</td>
<td style="text-align: center;">Team et al. (2022) <br> Team et al. (2022)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 54.4 \ &amp; 41.9 \end{aligned}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{aligned} &amp; 51.12 \ &amp; 21.57 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Sentiment <br> Analysis</td>
<td style="text-align: center;">NusaX - Eng <br> NusaX - Ind <br> NusaX - Jav <br> NusaX - Bug</td>
<td style="text-align: center;">Macro F1 <br> Macro F1 <br> Macro F1 <br> Macro F1</td>
<td style="text-align: center;">Winata et al. (2022) <br> Winata et al. (2022) <br> Winata et al. (2022) <br> Winata et al. (2022)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 92.6 \ &amp; 91.6 \ &amp; 84.2 \ &amp; 70.0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 61.5 \ &amp; 59.3 \ &amp; 55.7 \ &amp; 55.9 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 83.24 \ &amp; 82.13 \ &amp; 79.64 \ &amp; 55.84 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Question <br> Answering</td>
<td style="text-align: center;">bAbI task (15116) <br> EntailmentBank <br> CLUTRR <br> StepGame (k=9 1 k=1) <br> Pep-3k</td>
<td style="text-align: center;">Accuracy <br> Accuracy <br> Accuracy <br> Accuracy <br> AUC</td>
<td style="text-align: center;">Weston et al. (2016a) <br> Clark et al. (2018) <br> Minervini et al. (2020) <br> Mirzaee and Kordjamshidi (2022) <br> Porada et al. (2021)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1001100 \ &amp; 86.5 \ &amp; 95.0 \ &amp; 48.4198 .7 \ &amp; 67.0 \end{aligned}$</td>
<td style="text-align: center;">- <br> 78.58 <br> 28.6 <br> 48.4198 .7 <br> 67.0</td>
<td style="text-align: center;">$\begin{gathered} 93.3166 .7 \ 93.3 \ 43.3 \ 23.3163 .3 \ 93.3 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Misinformation Detection</td>
<td style="text-align: center;">COVID-Social <br> COVID-Scientific</td>
<td style="text-align: center;">Accuracy <br> Accuracy</td>
<td style="text-align: center;">Lee et al. (2021) <br> Lee et al. (2021)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 77.7 \ &amp; 74.7 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 50.0 \ &amp; 71.1 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 73.3 \ &amp; 92.0 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Task-Oriented Dialogue</td>
<td style="text-align: center;">MultiWOZ2.2 <br> MultiWOZ2.2 <br> MultiWOZ2.2</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { JGA } \ &amp; \text { BLEU } \ &amp; \text { Inform Rate } \end{aligned}$</td>
<td style="text-align: center;">Zhao et al. (2022) <br> Nekvinda and Dulek (2021) <br> Yang et al. (2021)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 60.6 \ &amp; 19.1 \ &amp; 95.7 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 46.7 \ &amp; - \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 24.4 \ &amp; 5.65 \ &amp; 71.1 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Open-Domain KGD</td>
<td style="text-align: center;">OpenDialKG <br> OpenDialKG</td>
<td style="text-align: center;">BLEU 1 ROUGE-L <br> FeQA</td>
<td style="text-align: center;">Ji et al. (2022b) <br> Ji et al. (2022b)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 20.8140 .0 \ &amp; 48.0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 3.1129 .5 \ &amp; 23.0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 4.1118 .6 \ &amp; 15.0 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of ChatGPT compared to state-of-the-art fully-fine-tuned models (Fine-Tuned SOTA) and LLM in zero-shot settings (Zero-Shot SOTA). The referenced performances are evaluation results on full test sets, while the ChatGPT performances are computed on subsets of the corresponding dataset using 30 to 200 data samples for each task. For Machine Translation (MT) tasks, we follow the definitions of high-resource language (HRL) and low-resource language (LRL) from NLLB (Team et al., 2022) and take subsets of languages to represent each group. JGA denotes joint goal accuracy. Average of performances for CNN and DM from Goyal et al. (2022). LMs in zero-shot settings are as follows. Summarization: InstructGPT, MT: NLLB-200, Sentiment Analysis: XLM-R LARGE, QA: ST-MoE-32B, ZeroQA, GPT-3, Misinformation Detection: GPT-2, Task-Oriented Dialogue: D3ST, Open-Domain KGD: GPT-Jurassic-6B.
vious state-of-the-art zero-shot models on 9 out of 13 evaluation datasets with reported zero-shot LLMs' performances. In most tasks, especially task-oriented and knowledge-grounded dialogue tasks, task-specific fully-fine-tuned models outperform ChatGPT. Compared to the latter, ChatGPT yields lower performance in most tasks while still surpassing the performance on 4 datasets.</p>
<p>Furthermore, from the evaluation results, we also observe several limitations of ChatGPT: 1) limited language understanding and generation capabilities on low-resource languages, 2) lacking reasoning ability as shown from the results in QA, and 3) performing task-oriented and knowledge-grounded dialogue tasks. More detailed experimental setup and analysis for each task are shared in Appendix $\S \mathrm{C}$. We also provide the complete list of all the datasets used in our evaluation in Appendix I.</p>
<p>ChatGPT on Dialogue Tasks Given that ChatGPT has the ability to generate conversation-like responses, we test it on conventional dialogue tasks: 1) knowledge-grounded open-domain dialogue and 2) Task-oriented dialogue. Task setups are ex-
plained in Appendix C.6.</p>
<h2>Knowledge-Grounded Open-Domain Dialogue</h2>
<p>To quantitatively measure ChatGPT's performance on knowledge-grounded dialogue, we utilize 50 samples from the test set of OpenDialKG (Moon et al., 2019), which contains open-ended dialogues grounded on a knowledge path. According to human judgment, the responses from ChatGPT are of high quality with fluent response generation and incorporating the provided knowledge in the response. However, the automatic evaluation results are relatively low compared with fine-tuned GPT2. We postulate this is because ChatGPT responses are longer than the golden answers and include content from its parametrized knowledge injected during pre-training.</p>
<p>Task-Oriented Dialogue We investigate and discuss how ChatGPT's emergent abilities and interactivity could potentially be leveraged for ToD as well in two setups. Firstly, A) modular approach: testing dialogue state tracking (DST) and response generation using oracle actions. DST is mediocre</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">SA Acc.</th>
<th style="text-align: center;">LID Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">English</td>
<td style="text-align: center;">HRL</td>
<td style="text-align: center;">$84 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Indonesian</td>
<td style="text-align: center;">MRL</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Javanese</td>
<td style="text-align: center;">LRL</td>
<td style="text-align: center;">$78 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Buginese</td>
<td style="text-align: center;">X-LRL</td>
<td style="text-align: center;">$56 \%$</td>
<td style="text-align: center;">$12 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy of ChatGPT on Sentiment Analysis (SA) and Language Identification (LID) tasks.
while ChatGPT successfully leverages all information provided while answering the questions with a $71.1 \%$ inform rate and 5.65 BLEU score. Next, B) Unified approach: a direct approach to simulate the ToD interaction while leveraging information in a structured database. We observed the limitations of ChatGPT: 1) ChatGPT cannot keep the belief state across multiple turns within the interaction, 2) ChatGPT's response tends to be wrong if the query introduces a basic level of reasoning 3) ChatGPT tends to generate hallucinated information beyond the given knowledge, which is not desirable for ToD. We provide details and examples of the modular and unified approaches in Appendix C.6.2.</p>
<h3>2.2 Evaluating Multilinguality of ChatGPT</h3>
<p>Training data size affects language understanding and generation ability of LMs (Raffel et al., 2022; Cahyawijaya et al., 2021; Rae et al., 2021; Workshop et al., 2022; Chowdhery et al., 2022; Hoffmann et al., 2022). As an LLM, the same premise also applies to ChatGPT, but the question is to what extent. We investigate this question through a series of experiments by analyzing 1) the language understanding capability through sentiment analysis (SA) and language identification (LID) tasks, and 2) the language generation capability through machine translation using English as the pivot language. Based on the size proportion in CommonCrawl (i.e., the primary source of language pre-training data used in various LLMs), we group languages into 4 language resource categories, i.e., high-resource language (HRL) ( $&lt;&gt;\mathbb{1} \%$ ), medium-resource language (MRL) ( $\geq 0.01 \%$ ), low-resource language (LRL) ( $\geq 0.0001 \%$ ), and extremely low-resource language (X-LRL) ( $&lt;0.0001 \%$ ). The statistics of the languages are shown in Table 9 and other details are described in Appendix D.</p>
<h3>2.2.1 Language Understanding</h3>
<p>We investigate the language understanding ability of ChatGPT on 4 languages from different language categories in NusaX (Winata et al., 2022), i.e. English, Indonesian, Javanese, and Buginese,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">XXX $\rightarrow$ Eng</th>
<th style="text-align: center;">Eng $\rightarrow$ XXX</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Chinese</td>
<td style="text-align: center;">HRL</td>
<td style="text-align: center;">24/30</td>
<td style="text-align: center;">14/30</td>
</tr>
<tr>
<td style="text-align: center;">French</td>
<td style="text-align: center;">HRL</td>
<td style="text-align: center;">29/30</td>
<td style="text-align: center;">25/30</td>
</tr>
<tr>
<td style="text-align: center;">Indonesian</td>
<td style="text-align: center;">MRL</td>
<td style="text-align: center;">28/30</td>
<td style="text-align: center;">19/30</td>
</tr>
<tr>
<td style="text-align: center;">Korean</td>
<td style="text-align: center;">MRL</td>
<td style="text-align: center;">22/30</td>
<td style="text-align: center;">12/30</td>
</tr>
<tr>
<td style="text-align: center;">Javanese</td>
<td style="text-align: center;">LRL</td>
<td style="text-align: center;">7/30</td>
<td style="text-align: center;">6/30</td>
</tr>
<tr>
<td style="text-align: center;">Sundanese</td>
<td style="text-align: center;">LRL</td>
<td style="text-align: center;">9/30</td>
<td style="text-align: center;">0/30</td>
</tr>
</tbody>
</table>
<p>Table 3: #Correct translations of ChatGPT. XXX denotes the target language listed in the first column.
through sentiment analysis and language identification tasks. ChatGPT fails to generalize to extremely low-resource languages. As shown in Table 2, there is a clear correlation between ChatGPT performance with the language resource category. This result aligns with the findings from prior works (Chowdhery et al., 2022; Workshop et al., 2022; Muennighoff et al., 2022), where LLMs, including ChatGPT, yield a lower performance for lower resource languages. Interestingly, the performance gap between English, Indonesian, and Javanese is considered marginal compared to the performance gap with Buginese. This suggests that ChatGPT has a limitation in generalizing toward extremely low-resource languages. Furthermore, we also find that ChatGPT can understand low-resource languages, such as Javanese, without having the knowledge to identify the language itself. Moreover, ChatGPT displays better humanpreferred responses when it has no knowledge about the language. For instance, as illustrated in 8, ChatGPT lets the user know that its prediction is uncertain when it does not completely understand the language and also provides broader information regarding the language.</p>
<h3>2.2.2 Language Generation</h3>
<p>We assess the multilingual language generation ability of ChatGPT through machine translation. We experiment with 6 languages: French, Chinese, Indonesian, Korean, Javanese, and Sundanese from the FLORES-200 dataset (Team et al., 2022; Goyal et al., 2021). For each language, we sample 30 English-XXX parallel sentences and perform two directions of translation using English as the pivot language. The correctness of the translation results is manually validated by a native speaker of the corresponding language.</p>
<p>Based on our evaluation results (Table 3), similar to other LLMs (Workshop et al., 2022; Muennighoff et al., 2022), ChatGPT produces better English translation quality from high-resource languages, such as French and Chinese. While for</p>
<p>low-resource languages, such as Javanese and Sundanese, ChatGPT tends to generate several mistranslated words/phrases and sometimes even hallucinate some objects. Moreover, we also observe that sometimes ChatGPT translates the English sentence into a different but related language other than the requested target language (see §H.2). This fact suggests that the generalization of LLMs, including ChatGPT, to low-resource languages, remains an open challenge. Moreover, we also find that ChatGPT can handle Latin script languages better than non-Latin script languages, especially in generating sentences using those scripts.</p>
<h3>2.3 Evaluating Multimodality of ChatGPT</h3>
<p>Since ChatGPT is a purely text-prompted language model, it is unlikely to explore its multimodal capabilities with visual inputs like contemporary visionlanguage works (Rombach et al., 2022; Ramesh et al., 2021; Yu et al., 2021a; Radford et al., 2021; Dai et al., 2022; Lovenia et al., 2022; Dai et al., 2023a). However, thanks to its code understanding and generation abilities, programming codes can serve as the intermediate medium to bridge vision and language (Rasheed, 2020; Shiryaev, 2022). Given textual prompts, ChatGPT can generate code representations of visual images using the SVG (Scalable Vector Graphics) format or APIs (e.g., HTML Canvas element, Python Turtle graphics). For example, as shown in Figure 1, ChatGPT can generate a well-formed and suitable intermediate representation in code format to synthesize images given the dialogue context and user prompts.</p>
<p>In this way, even though the generated images are symbolic and their quality is not comparable to the ones generated by modern text-to-image models (Ramesh et al., 2021; Rombach et al., 2022), it is worth exploring due to three reasons. Firstly, it helps us investigate the visual understanding and reasoning abilities of ChatGPT, which can be seen as an emergent skill after the very large-scale pretraining on text and code data. Furthermore, representing images with code is a more explainable way to understand the model's behaviors and rationales in text-to-image generation. Third, it is a natural way to evaluate ChatGPT's ability on multi-turn interaction by asking for post-editing and corrections of the generated images.</p>
<p>To systematically evaluate the image generation ability of ChatGPT through code generation, we designed a national flag drawing task. This task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Grade <br> (# of Errors)</th>
<th style="text-align: center;">Turn 1 <br> (w/o desc)</th>
<th style="text-align: center;">Turn 1</th>
<th style="text-align: center;">Turn 2</th>
<th style="text-align: center;">Turn 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A (0)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$\mathbf{2 4}$</td>
</tr>
<tr>
<td style="text-align: center;">B (1)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$\mathbf{2 4}$</td>
</tr>
<tr>
<td style="text-align: center;">C (2)</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">D (3)</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$\mathbf{2 6}$</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">E $(\geq 4)$</td>
<td style="text-align: center;">$\mathbf{6 2}$</td>
<td style="text-align: center;">$\mathbf{3 2}$</td>
<td style="text-align: center;">$\mathbf{2 6}$</td>
<td style="text-align: center;">22</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of the portion (\%) of generated flags evaluated into five grades, $\mathrm{A} \sim \mathrm{E}$. The second column shows the results of an ablation study, which removes the step of flag description generation and directly asks ChatGPT to generate the SVG code of the flag image.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generate a code snippet to represent</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">German flag in SVG format.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Turn 1 (w/o pre-generated desc.)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Describe how German flag looks like.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The flag of Germany consists of three equal horizontal bands of black, red, and gold.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generate a code snippet to represent that flag in SVG format.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Turn 1 (with pre-generated desc.)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 1: An example of a German flag drawn by ChatGPT using SVG format: (top) without and (bottom) with a self-retrieved textual description of the flag. A rendered image is shown in place of the generated SVG format for the sake of simplicity.
tests how ChatGPT's textually described knowledge (language) converts into the drawing (vision) through the SVG (code), using multi-turn conversations. The task contains three steps. Firstly, we ask ChatGPT to illustrate the appearance of the flag. Next, based on the description, we ask ChatGPT to generate the SVG code of that flag. Finally, if the generated image contains errors, we iteratively ask ChatGPT to fix them. There are four types of errors: 1) layout, 2) color, 3) missing components, 4) shape/size. We uniformly collect 50 national flags from different continents and conduct the flagdrawing task on ChatGPT. The prompts and full results are shown in Appendix E. The generated flag images are evaluated by the aforementioned four error types as criteria. We further assess the image quality with five grades, $\mathrm{A} \sim \mathrm{E}$, which indicate zero to four (or above) errors. An overview of the result evaluation is provided in Table 4.</p>
<p>We share our major two findings from the task:</p>
<p>1) ChatGPT is capable of drawing, yet better</p>
<p>with a self-generated textual description. As demonstrated in Table 4 and Appendix E, by following the task formulation, ChatGPT can generate plausible national flags using the SVG format. To better understand the behavior of ChatGPT, we perform an ablation study by removing the description generation step. As illustrated by Figure 1, the performance drops dramatically without first prompting the textual flag description, which is generated by ChatGPT itself. Explicitly describing the appearance of the flag and then drawing disentangles the image generation process, which can be considered as a chain-of-thought reasoning. 2) ChatGPT is an elementary illustrator. Among the four error types, the majority lies in the shape/size error, which happens $68 \%$ of the time. For the other three error types (layout, color, missing components), they appear $34 \%, 20 \%$, and $18 \%$ of the time, respectively. For instance, ChatGPT cannot generate the exact shape of the maple leaf in the Canadian flag while it gets the layout and color correctly (Figure 3). This is a natural defect of text-only language models as they never see actual visual data and textual data is usually conceptual.</p>
<h2>3 Reasoning Evaluations of ChatGPT</h2>
<p>Reasoning is one of the most actively discussed and debated abilities of LLMs as scaling the model parameter size also increases the implicit knowledge in LLMs (Wei et al., 2022a; Wang et al., 2022; Huang and Chang, 2022). Mahowald et al. eloquently argues that "language ability does not equal to thinking" or "reasoning" in LLMs, and that LLMs have poor reasoning skills despite possessing human-level language skills.</p>
<p>In the NLP literature, evaluating a model's reasoning often means evaluating its various skills in arithmetic, commonsense, and symbolic reasoning in different NLP tasks that require such skills (Talmor et al., 2020; Zelikman et al., 2022; Wei et al., 2022b). However, the reasoning itself is a much broader concept thus it is hard to conclude whether a model can "reason" or not based on those aforementioned, and current works on reasoning are scattered. This is in line with the anecdotal experience of users with ChatGPT - some of the examples demonstrate surprisingly good "reasoning" abilities compared to previously introduced LLMs but at the same time ChatGPT fails in very simple reasoning problems (the, 2023; Venuto, 2023; Qiao et al., 2022; Cookup.ai, 2022; Labs, 2022).</p>
<p>Thus, we investigate the reasoning ability of ChatGPT in a more fine-grained manner, which includes deductive, inductive, abductive, analogical, causal, multi-hop, mathematical, temporal, and spatial reasoning, via question-answering tasks. We categorize available QA tasks into each category by avoiding overlap (i.e., choosing testsets that require mainly one specific category of reasoning). Composed results and corresponding datasets for each category are shown in Table 5. For evaluation, we manually check the accuracy of the answer as well as verify the rationales and explanations generated by ChatGPT. A detailed explanation of task setup is explained in Appendix F.</p>
<p>Logical Reasoning Inductive, deductive, and abductive reasoning are common forms of logical reasoning, a process of deriving a conclusion or judgment based on given evidence or past experience and observations (Rogers et al., 2022; Wason and Johnson-Laird, 1972; Huang and Chang, 2022). We first investigate basic reasoning skills with bAbI tasks (Weston et al., 2016b), 30 examples each from task 15 (inductive) and task 16 (deductive). One major investigation is that ChatGPT is a lazy reasoner that suffers more from induction. Interestingly, when ChatGPT was asked to answer a question given premises without any prompt engineering, it performed poorly in induction ( 0 out of 30 ) while it achieved much better performance in deduction (19 out of 30). However, when ChatGPT is explicitly asked for reasonable inference inductive reasoning increases to 20 out of 30 . Yet, it is still not as good as in deduction. When we repeat the analysis on advanced tasks, specifically on CLUTRR (Sinha et al., 2019) for induction and EntailmentBank for deduction (Dalvi et al., 2021), the same conclusion holds based on our experiment.</p>
<p>Non-textual semantic reasoning It is often investigated in public sharing about ChatGPT errors cases that it lacks the reasoning ability that requires non-text semantic understanding such as mathematical, temporal, and spatial reasoning. Not surprisingly, it could only score $23.33 \%(7 / 30)$ for the MATH dataset (Saxton et al., 2019), which tests mathematical reasoning. Overall, ChatGPT correctly answers $86.67 \%$ of the time (26/30), suggesting that it has a decent temporal reasoning ability. ChatGPT falls short of the spatial reasoning tasks, with success rates of $43.33 \%$ for StepGame and $43.75 \%$ for SpartQA. We investigate the errors that</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Categories</th>
<th style="text-align: center;">Testset</th>
<th style="text-align: center;">Result</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Deductive</td>
<td style="text-align: center;">EntailmentBank <br> bAbl (task 15)</td>
<td style="text-align: center;">$\begin{gathered} 28 / 30 \ 28 / 30 \text { (as-is: 19/30) } \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Inductive</td>
<td style="text-align: center;">CLUTRR <br> bAbl (task16)</td>
<td style="text-align: center;">$\begin{gathered} 13 / 30 \ 20 / 30 \text { (as-is: } 0 / 30) \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Abductive</td>
<td style="text-align: center;">$\alpha$ NLI</td>
<td style="text-align: center;">26/30</td>
</tr>
<tr>
<td style="text-align: center;">Mathematical</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">$13 / 30$</td>
</tr>
<tr>
<td style="text-align: center;">Temporal</td>
<td style="text-align: center;">Timedial</td>
<td style="text-align: center;">26/30</td>
</tr>
<tr>
<td style="text-align: center;">Spatial</td>
<td style="text-align: center;">SpartQA (hard । basic) <br> StepGame (hard । basic) <br> StepGame (cardinal) <br> StepGame (diagonal) <br> StepGame (clock)</td>
<td style="text-align: center;">$\begin{gathered} 8 / 32120 / 32 \ 7 / 30119 / 30 \ 17 / 20 \ 11 / 20 \ 5 / 20 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">CommonsenseQA <br> PIQA <br> Pep-3k (Hard)</td>
<td style="text-align: center;">$\begin{gathered} 27 / 30 \ 25 / 30 \ 28 / 30 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Causal</td>
<td style="text-align: center;">E-Care</td>
<td style="text-align: center;">24/30</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop</td>
<td style="text-align: center;">hotpotQA</td>
<td style="text-align: center;">$8 / 30$</td>
</tr>
<tr>
<td style="text-align: center;">Analogical</td>
<td style="text-align: center;">Letter string analogy</td>
<td style="text-align: center;">30/30</td>
</tr>
</tbody>
</table>
<p>Table 5: Composed results for all reasoning tasks.
it often fails to understand clock direction (e.g., "W is at K's 3 o'clock") and diagonal spatial relations.</p>
<p>Commonsense Reasoning It is understanding and reasoning about everyday concepts and knowledge that most people are familiar with, to make judgments and predictions about new situations (Storks et al., 2019). Recent works show that LLMs perform impressively well on commonsense reasoning benchmarks (Qiao et al., 2022; Huang and Chang, 2022; Bhargava and Ng, 2022). Based on our evaluation with CommonsenseQA (Talmor et al., 2018), PiQA (Bisk et al., 2020) and Pep3k (Wang et al., 2018b), ChatGPT shows surprisingly good commonsense reasoning capability, perhaps due to its large parametric memory.</p>
<h2>4 Factuality and Hallucination</h2>
<p>LLMs are known to be susceptible to generating nonfactual, untruthful information, which is referred to as hallucination (Lee et al., 2022; Ji et al., 2022a,b; Su et al., 2022; Dai et al., 2023b; Xu et al., 2023). Many anecdotal witnesses show ChatGPT also seems to suffer from the same problem as other LLMs. To evaluate this aspect of ChatGPT, we first explore existing fact-checking and QA test sets and also illustrate the challenge of hallucination in ChatGPT by sharing hallucination examples.</p>
<p>Factuality We evaluate ChatGPT with test sets that consist of scientific and social claims related to COVID-19 (Lee et al., 2021). ChatGPT is able to detect misinformation $92 \%$ (46/50) and $73.33 \%$ (22/30, excluding verification-refusing cases) accuracy on covid-scientific and covid-social respec- tively. In comparison to its previously reported performance, ChatGPT's performance on covidscientific is impressive. Interestingly, for more societal-related claims, ChatGPT often refuses to make verification. However, it cannot avoid the criticism that parameterized knowledge is obtained by better memorization as it still shows worse performance in questions designed to cause imitative falsehoods. We test on 66 test samples from TruthfulQA (Lin et al., 2022), which tests the extent of LLMs to mimic human falsehood and $35.38 \%$ of the time ChatGPT fails to answer truthfully.</p>
<p>Hallucination From various tasks, we often find extrinsic hallucinations, including both untruthful and factual ones, across various tasks such as Machine Translation and question answering, which causes degradation in performance. The intrinsic hallucinations are barely found as discussed in tasks about summarization and knowledgegrounded open-domain dialogue. We share examples of these hallucination types detected from different task explorations in Table 19.</p>
<h2>5 Evaluating Interactivity in ChatGPT</h2>
<p>ChatGPT has a built-in interactive ability thanks to conversational data fine-tuning and RLHF. We further delve into the benefit of exploiting this interactive ability of ChatGPT in three NLP tasks, summarization, machine translation, and multimodal generation. Our experiments demonstrate the potential of employing multi-turn interaction to refine the quality of the generated responses and improve the task performance of ChatGPT.</p>
<p>Interactivity on Summarization Summarization models aim to extract essential information from documents and to generate short, concise, and readable text (Yu et al., 2021b; Su et al., 2021). In real-world applications, people may want to improve the summary based on the previously generated summary. We ran experiments with 50 documents from SAMSum (Gliwa et al., 2019) and conducted a two-turn iterative prompt approach. ChatGPT usually generates an overly long summary. By adding a follow-up prompt after the first summary, "Please make the summary shorter", ChatGPT could provide a much shorter summary than the first response. Experimental results show that with the second length control prompt, the refined summaries achieve 7.99 and 1.64 gains on ROUGE-1 and ROUGE-2 respectively.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Result of the multi-turn MT-APE experiment. #Correct MT denotes the number of correct translations. #Correct APE denotes the number of correct translations after post-editing.</p>
<p>Interactivity on Machine Translation One of the capabilities of ChatGPT is to perform text translation from one language to another. With the interactivity of ChatGPT, we explore the possibility of performing a combined machine translation and automatic post-editing tasks to improve the translation quality of ChatGPT. For the experiment, we adapt the dataset used in §2.2.2. As shown in Figure 2, despite the translation and post-editing being done using a single ChatGPT model, the multi-turn approach method helps to improve the correctness of the translation by making partial corrections or even full corrections in some cases. We provide experimental setup details and examples of the postediting in Appendix J.</p>
<p>Interactivity on Multimodal Generation The multi-turn interaction ability of ChatGPT enables the refinement of text-to-image generation. It is one of the most natural ways for humans to create artwork or product designs by requesting an AI tool iteratively. Through interaction with ChatGPT over multiple turns, a process of creating an interesting painting can be achieved (Figure 7).</p>
<p>To quantitatively study how this ability impacts image generation, we conduct at most three rounds of post-editing for the flag-drawing task. As shown in Figure 4, in the first round of generation, ChatGPT rarely generates errorless SVG images except for some simple flags (e.g., Nigerian and German). We observe that $34 \%$ and $36 \%$ of samples experience improvement (i.e., fewer errors) from turn 1 to 2 and from turn 2 to 3 , respectively. We also tested with the InstructGPT, which has the same backbone model as ChatGPT but lacks conversation ability. InstructGPT cannot achieve salient improvements by directly putting the intermediate results in the input context (Appendix H.3).</p>
<h2>6 Conclusion and Discussion</h2>
<p>Multitask, Multilingual, Multimodal ChatGPT outperforms SOTA LLMs in a zero-shot manner on
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Changes in ChatGPT's drawing of the Canadian flag over three turns. Layout, color, completion, and shape/size are marked as $\checkmark$ if they align with those of the ground truth, and $\boldsymbol{X}$ otherwise.
various tasks and even surpasses fine-tuned models on some tasks. However, there are still some failure cases (§2.1) and it produces responses with altered nuance and meaning. Therefore, dealing with these special cases is a complex but important task. In terms of multilinguality, ChatGPT achieves strong performance in many high-resource and mediumresource languages. Nevertheless, ChatGPT still lacks the ability to understand and generate sentences in low-resource languages (§2.2), which is also supported by Lai et al. (2023). Additionally, ChatGPT lacks the generation ability of non-Latin script languages (§2.2.2), despite the languages being high-resource. These raise the concern of language diversity and inclusivity in ChatGPT (Joshi et al., 2020; Aji et al., 2022). Regarding multimodality, our flag drawing experiments showed the potential of ChatGPT's multimodal ability. It would be an interesting research direction to further explore ChatGPT's multimodal ability to answer "can textual models like ChatGPT switch to a multimodal backbone?"</p>
<p>Reasoning The impressive performance of ChatGPT has sparked interest in expanding its usage beyond traditional NLP tasks into more complex domains requiring sophisticated reasoning such as problem-solving, decision-making, and planning. Our evaluation of its reasoning abilities shows that they are not reliable. Specifically, our findings indicate that ChatGPT exhibits a tendency to be a lazy reasoner and that its capabilities are inconsistent across various reasoning abilities; To support the further expansion of its use cases, it is necessary to</p>
<p>prioritize the development of systems with robust complex reasoning capabilities, which should also be facilitated by the creation of more comprehensive benchmarks for assessing these abilities, such as works by Laskar et al. (2023b); Qin et al. (2023); Davis (2023a), particularly when multiple abilities are required to complete the tasks.</p>
<p>Factuality\&amp;Hallucinations ChatGPT, like other LLMs, still makes things up (Ji et al., 2022a). To ensure factuality, it is possible to build LLMs with an interface to an external knowledge source, like Blenderbot 3.0 (Shuster et al., 2022) and LaMDa (Thoppilan et al., 2022). Meanwhile, there are many forms of hallucinations that are not necessarily counterfactual but still undesirable. The RLHF process of ChatGPT can ensure human feedback to mitigate undesirable responses. However, researchers need to work on coming up with more automatic and scalable methods to detect and mitigate hallucinations and other undesirable artifacts.</p>
<p>Interactivity Compared with the previous LLMs, the interactive ability of ChatGPT has made a leap according to both qualitative and quantitative measures. Through interactivity, ChatGPT can recite from its own description, which is a very important ability. A similar exploration of this ability in LLMs has also been explored in other research works (Sun et al., 2022; Wang et al., 2023). However, sometimes ChatGPT retains the wrong answer even after receiving multiple rounds of prompts from the user. Improving the ability of ChatGPT to handle multiple rounds of user feedback is also an important challenge.</p>
<h2>Limitation</h2>
<p>The experiments are done with the UI of ChatGPT provided by OpenAI (15 December 2019 version), before the ChatGPT API was released, thus, the number of samples for evaluation is limited (30-200). However, tasks of evaluation should not be affected much because most of the recent updates/releases of ChatGPT are related to safety concerns. Moreover, It is possible to augment our benchmarks with other technical benchmarks for research purposes, especially now that the ChatGPT APIs are available. There has been recent automatic or human-in-the-loop evaluations such as (Laskar et al., 2023a) Nevertheless, many of the benchmarks are not necessarily interpretable to laypeople for general purposes, such as named
entity recognition and etc. Our paper provides an easier-to-follow guideline.</p>
<p>Due to the page limit, many parts of the experimental setup details are added to the Appendix while the overall structure of evaluation and major insights stay in the main content. This may cause the reader inconvenience to follow the experiments. However, we publicly release the codebase that can help the community replicate the exact same evaluation either on ChatGPT or other LLMs easily.</p>
<h2>Ethics Statement</h2>
<p>Responsible Generative AI Previous works have discussed the ethical implications or concerns associated with ChatGPT (and other LLMs) (Jabotinsky and Sarel, 2022; Susnjak, 2022; Blanco-Gonzalez et al., 2022; Aydın and Karaarslan, 2022; Jeblick et al., 2022). Agreeing with the previous literature, the responsible design and usage of LLMs including ChatGPT is an important and pressing challenge today. There are common issues with these models, such as fairness, toxicity, demographic bias, and safety, which need to be addressed. In the case of ChatGPT, OpenAI constructs safety layers and uses RLHF and potentially other means to filter out undesirable system responses. However, this is still not perfect and requires future research to further improve the robustness of the safety layer. This process is resource-intensive and opaque to the public. We hope to see a more open discussion and sharing of responsible design of LLMs from various organizations including OpenAI in the future.</p>
<p>Use of Scientific Artifacts/Data This paper conducts an evaluation of ChatGPT for academic purposes only. We comply with the terms and conditions of ChatGPT stated in https://openai.com /policies/terms-of-use. Moreover, we comply with all the licenses of all the data (i.e., test sets/benchmarks) that are used in this evaluation.</p>
<h2>Acknowledgments</h2>
<p>This work has been partially funded by MRP/055/18 of the Innovation Technology Commission, Hong Kong SAR Government; the Hong Kong Fellowship Scheme by the Hong Kong Research Grants Council, and PF20-43679 Hong Kong PhD Fellowship Scheme, Hong Kong Research Grants Council.</p>
<h2>References</h2>
<ol>
<li>Chatgpt vs satya nadella over biryani: The chatbot is learning from its mistakes.</li>
</ol>
<p>Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. 2022. One country, 700+ languages: NLP challenges for underrepresented languages and dialects in Indonesia. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7226-7249, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Ömer Aydın and Enis Karaarslan. 2022. Openai chatgpt generated literature review: Digital twin in healthcare. Available at SSRN 4308687.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Paul Bartha. 2013. Analogy and analogical reasoning.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In International Conference on Learning Representations.</p>
<p>Prajjwal Bhargava and Vincent Ng. 2022. Commonsense knowledge reasoning and generation with pretrained language models: a survey. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12317-12325.</p>
<p>Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the wmt 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation, pages 109-117, Abu Dhabi.</p>
<p>David G.W. Birch. 2022. Chatgpt is a window into the real future of financial services.</p>
<p>Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439.</p>
<p>Alexandre Blanco-Gonzalez, Alfonso Cabezon, Alejandro Seco-Gonzalez, Daniel Conde-Torres, Paula Antelo-Riveiro, Angel Pineiro, and Rebeca GarciaFandino. 2022. The role of ai in drug discovery: Challenges, opportunities, and strategies. arXiv preprint arXiv:2212.08104.</p>
<p>Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Fajri Koto, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Ivan Halim Parmonangan, Ika Alfina, Muhammad Satrio Wicaksono, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Akbar Septiandri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Farid Adilazuarda, Ryan Ignatius, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Cuk Tho, Ichwanul Muslim Karo Karo, Tirana Noor Fatyanosa, Ziwei Ji, Pascale Fung, Graham Neubig, Timothy Baldwin, Sebastian Ruder, Herry Sujaini, Sakriani Sakti, and Ayu Purwarianti. 2022. Nusacrowd: Open source initiative for indonesian nlp resources.</p>
<p>Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG: Benchmark and resources for evaluating Indonesian natural language generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8875-8898, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ethan C. Chau and Noah A. Smith. 2021. Specializing multilingual language models: An empirical study. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 51-61, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. 2023. Chatgpt goes to law school. Available at SSRN.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge.</p>
<p>Cookup.ai. 2022. Chatgpt - where it lacks.
Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022. Enabling multimodal generation on CLIP via vision-language knowledge distillation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2383-2395, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023a. Instructblip: Towards general-purpose vision-language models with instruction tuning.</p>
<p>Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. 2023b. Plausible may not be faithful: Probing object hallucination in vision-language pre-training. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2136-2148, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370.</p>
<p>Ernest Davis. 2023a. Benchmarks for automated commonsense reasoning: A survey. arXiv preprint arXiv:2302.04752.</p>
<p>Ernest Davis. 2023b. Mathematics, word problems, common sense, and artificial intelligence. arXiv preprint arXiv:2301.09723.</p>
<p>Web Desk. 2023. Colombian judge uses chatgpt in ruling, triggers debate.</p>
<p>Igor Douven. 2017. Abduction.
Michael Dowling and Brian Lucey. 2023. Chatgpt for (finance) research: The bananarama conjecture. Finance Research Letters, page 103662.</p>
<p>Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin. 2022. e-CARE: a new dataset for exploring explainable causal reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 432-446, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. 2023. Mathematical capabilities of chatgpt.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation.</p>
<p>Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, and David Chartash. 2022. How well does chatgpt do when taking the medical licensing exams? the implications of large language models for medical education and knowledge assessment. medRxiv, pages 2022-12.</p>
<p>Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. EMNLP-IJCNLP 2019, page 70.</p>
<p>Yoav Goldberg. 2023. Some remarks on large language models.</p>
<p>Cindy Gordon. 2023. Chatgpt is the fastest growing app in the history of web applications.</p>
<p>Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2021. The flores-101 evaluation benchmark for low-resource and multilingual machine translation.</p>
<p>Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.</p>
<p>Roberto Gozalo-Brizuela and Eduardo C GarridoMerchan. 2023. Chatgpt is not all you need. a state of the art review of large generative ai models. arXiv preprint arXiv:2301.04655.</p>
<p>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597.</p>
<p>James Hawthorne. 2021. Inductive Logic. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Spring 2021 edition. Metaphysics Research Lab, Stanford University.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.</p>
<p>Krystal Hu. 2023. Chatgpt sets record for fastestgrowing user base - analyst note.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.</p>
<p>Arfinda Ilmania, Abdurrahman, Samuel Cahyawijaya, and Ayu Purwarianti. 2018. Aspect detection and sentiment classification using deep neural network for indonesian aspect-based sentiment analysis. In 2018 International Conference on Asian Language Processing (IALP), pages 62-67.</p>
<p>Hadar Yoana Jabotinsky and Roee Sarel. 2022. Coauthoring with an ai? ethical dilemmas and artificial intelligence. Ethical Dilemmas and Artificial Intelligence (December 15, 2022).</p>
<p>Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, et al. 2022. Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports. arXiv preprint arXiv:2212.14882.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022a. Survey of hallucination in natural language generation. ACM Comput. Surv. Just Accepted.</p>
<p>Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and Pascale Fung. 2022b. Rho $(\rho)$ : Reducing hallucination in open-domain dialogues with knowledge grounding. arXiv preprint arXiv:2212.01588.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study.</p>
<p>Arianna Johnson. 2023. Is chatgpt partisan? poems about trump and biden raise questions about the ai bot's bias-here's what experts think.</p>
<p>Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282-6293, Online. Association for Computational Linguistics.</p>
<p>Jennifer A. Kingson. 2023. Friend or foe? teachers debate chatgpt.</p>
<p>Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. 2023. Chatgpt: Jack of all trades, master of none. Information Fusion, page 101861.</p>
<p>Escape Velocity Labs. 2022. Chatgpt imitates logical reasoning surprisingly well.</p>
<p>Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. arXiv preprint arXiv:2304.05613.</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023a. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Findings of the Association for Computational Linguistics: ACL 2023, pages 431-469, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023b. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486.</p>
<p>Anton E Lawson. 2005. What is the role of induction and deduction in reasoning and scientific inquiry? Journal of Research in Science Teaching, 42(6):716740 .</p>
<p>Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021. Towards few-shot fact-checking via perplexity. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1971-1981, Online. Association for Computational Linguistics.</p>
<p>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. In Advances in Neural Information Processing Systems.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020b. Bart: Denoising sequence-to-sequence pre-training</p>
<p>for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages $7871-7880$.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Zhenpeng Zhou, Paul A Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, et al. 2021. Zero-shot dialogue state tracking via cross-task transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7890-7900.</p>
<p>Holy Lovenia, Bryan Wilie, Romain Barraud, Samuel Cahyawijaya, Willy Chung, and Pascale Fung. 2022. Every picture tells a story: Image-grounded controllable stylistic story generation. In Proceedings of the 6th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 40-52, Gyeongju, Republic of Korea. International Conference on Computational Linguistics.</p>
<p>Hongyuan Lu, Haoyang Huang, Shuming Ma, Dongdong Zhang, Wai Lam, and Furu Wei. 2022. Trip: Triangular document-level pre-training for multilingual language models. arXiv preprint arXiv:2212.07752.</p>
<p>Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, and Pascale Fung. 2021. Few-shot bot: Promptbased learning for dialogue systems. arXiv preprint arXiv:2110.08118.</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627.</p>
<p>Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, and Erik Cambria. 2023. Gpteval: A survey on assessments of chatgpt and gpt-4. arXiv preprint arXiv:2308.12488.</p>
<p>Bernard Marr. 2022. What does chatgpt really mean for businesses?</p>
<p>Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140.</p>
<p>Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, and Tim Rocktäschel. 2020. Learning reasoning strategies in end-to-end differentiable proving. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org.</p>
<p>Roshanak Mirzaee and Parisa Kordjamshidi. 2022. Transfer learning with synthetic corpora for spatial role labeling and reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6148-6165, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: A textual question answering benchmark for spatial reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4582-4598, Online. Association for Computational Linguistics.</p>
<p>Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 845-854.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.</p>
<p>Benjamin Muller, Antonios Anastasopoulos, Benoît Sagot, and Djamé Seddah. 2021. When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 448-462, Online. Association for Computational Linguistics.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence</p>
<p>RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 280-290, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Tomáš Nekvinda and Ondřej Dušek. 2021. Shades of BLEU, flavours of success: The case of MultiWOZ. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 34-46, Online. Association for Computational Linguistics.</p>
<p>Oded Nov, Nina Singh, and Devin M Mann. 2023. Putting chatgpt's medical advice to the (turing) test. medRxiv, pages 2023-01.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Simon Ott, Konstantin Hebenstreit, Valentin Liévin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, and Matthias Samwald. 2023. Thoughtsource: A central hub for large language model reasoning data. arXiv preprint arXiv:2301.11596.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Ian Porada, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. 2021. Modeling event plausibility with consistent conceptual abstraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1732-1743, Online. Association for Computational Linguistics.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey. arXiv preprint arXiv:2212.09597.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476.</p>
<p>Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui. 2021. TIMEDIAL: Temporal commonsense reasoning in dialog. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7066-7076, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis and insights from training gopher.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2022. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR.</p>
<p>Fabin Rasheed. 2020. Gpt3 sees.</p>
<p>Partha Pratim Ray. 2023. Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems.</p>
<p>Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2022. Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension. ACM Comput. Surv. Just Accepted.</p>
<p>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $10684-10695$.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations.</p>
<p>Stephen Shankland. 2023. Why the chatgpt ai chatbot is blowing everyone's mind.</p>
<p>Yiqiu Shen, Laura Heacock, Jonathan Elias, Keith D Hentel, Beatriu Reig, George Shih, and Linda Moy. 2023. Chatgpt and other large language models are double-edged swords.</p>
<p>Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022a. Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):1132111329 .</p>
<p>Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022b. StepGame: A new benchmark for robust multi-hop spatial reasoning in texts. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):1132111329 .</p>
<p>Denis Shiryaev. 2022. Drawing mona lisa with chatgpt.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. 2019. Clutrr: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4506-4515.</p>
<p>Noah Smith. 2023. Why does chatgpt constantly lie?
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,</p>
<p>Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Shane Storks, Qiaozi Gao, and Joyce Y Chai. 2019. Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches. arXiv preprint arXiv:1904.01172, pages $1-60$.</p>
<p>Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022. Read before generate! faithful long form question answering with machine reading. In Findings of the Association for Computational Linguistics: ACL 2022, pages 744756 .</p>
<p>Dan Su, Tiezheng Yu, and Pascale Fung. 2021. Improve query focused abstractive summarization by incorporating answer relevance. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3124-3131.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Teo Susnjak. 2022. Chatgpt: The end of online exam integrity? arXiv preprint arXiv:2212.09292.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937.</p>
<p>NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling humancentered machine translation.</p>
<p>Richmond Thomason. 2018. Logic and artificial intelligence.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>H Holden Thorp. 2023. Chatgpt is fun, but not an author.</p>
<p>Giuseppe Venuto. 2023. Giuven95/chatgpt-failures: Chatgpt failure archive.</p>
<p>Douglas Walton. 2014. Abductive reasoning. University of Alabama Press.</p>
<p>Ada Wan. 2022. Fairness in representation for multilingual NLP: Insights from controlled experiments on conditional language modeling. In International Conference on Learning Representations.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Su Wang, Greg Durrett, and Katrin Erk. 2018b. Modeling semantic plausibility by injecting world knowledge. arXiv preprint arXiv:1804.00619.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Peter Cathcart Wason and Philip Nicholas JohnsonLaird. 1972. Psychology of reasoning: Structure and content, volume 86. Harvard University Press.</p>
<p>Taylor Webb, Keith J. Holyoak, and Hongjing Lu. 2022a. Emergent analogical reasoning in large language models.</p>
<p>Taylor Webb, Keith J Holyoak, and Hongjing Lu. 2022b. Emergent analogical reasoning in large language models. arXiv preprint arXiv:2212.09196.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. 2016a. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, and Tomas Mikolov. 2016b. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Representations, ICLR 2016.</p>
<p>Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, et al. 2020. Indonlu: Benchmark and resources for evaluating indonesian natural language understanding. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages $843-857$.</p>
<p>Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. 2022. Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages.</p>
<p>Cameron R. Wolfe. 2023. Specialized llms: Chatgpt, lamda, galactica, codex, sparrow, and more.</p>
<p>BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanueva del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško,</p>
<p>Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, ZhengXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, JanChristoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman,</p>
<p>Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio MirandaEscalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirug Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model.</p>
<p>Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zihan Liu, Genta Indra Winata, Andrea Madotto, Dan Su, and Pascale Fung. 2022. Retrieval-free knowledgegrounded dialogue response generation with adapters. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 93-107.</p>
<p>Yan Xu, Deqian Kong, Dehong Xu, Ziwei Ji, Bo Pang, Pascale Fung, and Ying Nian Wu. 2023. Diverse and faithful knowledge-grounded dialogue generation via sequential posterior inference. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 38518-38534. PMLR.</p>
<p>Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2021. Ubar: Towards fully end-to-end task-oriented dialog system with gpt-2. Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14230-14238.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for</p>
<p>diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380.</p>
<p>Tiezheng Yu, Wenliang Dai, Zihan Liu, and Pascale Fung. 2021a. Vision guided generative pre-trained language models for multimodal abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3995-4007, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Tiezheng Yu, Zihan Liu, and Pascale Fung. 2021b. Adaptsum: Towards low-resource domain adaptation for abstractive summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5892-5904.</p>
<p>Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines. ACL 2020, page 109.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Jeffrey Zhao, Raghav Gupta, Yuanbin Cao, Dian Yu, Mingqiu Wang, Harrison Lee, Abhinav Rastogi, Izhak Shafran, and Yonghui Wu. 2022. Descriptiondriven task-oriented dialog modeling. ArXiv, abs/2201.08904.</p>
<p>Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3377-3390.</p>
<p>Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023a. Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867.</p>
<p>Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023b. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity.</p>
<h2>Appendix</h2>
<p>The appendix consists the following content:</p>
<ul>
<li>A: Background and Related Work</li>
<li>B: General Experimental Details</li>
<li>C: Details for Multitask Evaluation</li>
<li>D: Details for Multilinguality Evaluation</li>
<li>E: Multimodality - Flag Drawing Task</li>
<li>F: Details for Reasoning Evaluation</li>
<li>G: Details for Hallucination Evaluations</li>
<li>H: Details for Interactivity Evaluation</li>
<li>I: List of Evaluation Datasets</li>
<li>J: Examples from Machine Translation and Post-Editing</li>
</ul>
<h2>A Background and Related Work</h2>
<h2>A. 1 ChatGPT</h2>
<p>Compared to existing LLMs, ChatGPT has unique characteristics. First, it has the ability to interact with users in a conversation-like manner, while retaining its accumulated knowledge and generalization ability gained from pre-training. This is achieved by pre-training ChatGPT on a large-scale conversational-style dataset, that is constructed by transforming a large-scale instruction-tuning corpus used for building InstructGPT into a conversational format, then fine-tuning the model based on a reward model to further improve the generation quality and align the generation with human</p>
<p>Second, ChatGPT is trained with a better humanaligned objective function via Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017). Conventional natural language generation models, including dialogue models, are trained with maximum likelihood estimation (MLE) and might not be aligned with human preferences. For instance, for dialogue systems, humanness, engagement, and groundedness are some examples of essential criteria for success. Such discrepancy between training objectives and evaluation metrics becomes a bottleneck to performance improvement. By using RLHF, ChatGPT aligns more closely with human preferences in generating text than by using MLE.</p>
<h2>Discussion on its Capability</h2>
<p>As ChatGPT has become available to public users through an easily accessible UI, there have been many discussions from a wide range of communities, not just from AI or NLP, but also from
other disciplines. A line of discussion is the specific emergent ability and strength of ChatGPT in more technical perspectives. Guo et al. (2023) conducts linguistic analyses of ChatGPT's writing against human experts and found that ChatGPT responses are strictly focused on the given question, more formal, objective, and less emotional. Nov et al. (2023) also studies ChatGPT's generated medical advice if it passes the Turing test. Frieder et al. (2023) show that "significantly below those of an average mathematics graduate student." There are many investigations of ChatGPT's understanding and potential applications in different fields such as law (Choi et al., 2023), medical domain (BlancoGonzalez et al., 2022; Jeblick et al., 2022) and finance (Birch, 2022; Dowling and Lucey, 2023). Jeblick et al. (2022) conduct a case study of the application of ChatGPT on simplified radiology reports. Another important line of discussion is the ethical concerns over the use of ChatGPT. The most active discussion is over the use of academic writing and exam integrity (Jabotinsky and Sarel, 2022; Susnjak, 2022). OpenAI also discusses the misuse of LM for disinformation and remedies. ${ }^{2}$ Zhuo et al. study AI ethics of ChatGPT in criteria of bias, reliability, robustness, and toxicity.</p>
<h2>A. 2 LLM benchmark and evaluation</h2>
<p>With the advancement of LLMs' generalization ability, there have been efforts to understand their capabilities, limitations, and risks. Recently, several benchmarks with a collection of a large number of NLP datasets, such as BIG-Bench (Srivastava et al., 2022) and AI LM Harness (Gao et al., 2021), have been introduced. Moreover, HELM (Liang et al., 2022) is proposed to conduct a holistic evaluation of LLMs that considers scenarios and metrics with a top-down approach. In this work, we instead focus on specific limitations and unique findings of ChatGPT that had not been discussed with previous LLMs.</p>
<p>There are also other works that discuss LLMs' emergent abilities through thorough surveys or case studies. Mahowald et al. (2023) thoroughly studies LLMs capabilities by distinguishing formal and functional linguistic competence with reference to cognitive science, psychology, and NLP to clarify the discourse surrounding LLMs' potential. Other works focus on more specific abilities such as mathematical skills (Davis, 2023b), reasoning (Webb</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>et al., 2022a; Qiao et al., 2022). Also, there have been overviews of existing LLMs (Gozalo-Brizuela and Garrido-Merchan, 2023; Wolfe, 2023)</p>
<h2>A. 3 ChatGPT Evaluation</h2>
<p>To the best of our knowledge, this benchmarking exercise is the first of its kind. Since the introduction of ChatGPT with its advancement, there has been a huge amount of assessments of ChatGPT to understand its limits. Mao et al. (2023) provides a survey of recent assessments of ChatGPT in broad categories of 1) Language and Reasoning Ability, 2) Scientific Knowledge, and 3) Ethical Considerations. Laskar et al. provide extensive automatic or human-in-the-loop evaluations on 140 tasks. Qin et al. mainly evaluated the reasoning abilities of ChatGPT while Zhuo et al.; Ray focus on other important aspects such as ethics, robustness, reliability, limitations, and future scope of ChatGPT. Kocoń et al. examined whether the high quality of the LLM can indicate a tool's usefulness to society by evaluating ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans. After the introduction of ChatGPT, GPT-4 has been introduced by OpenAI. However, OpenAI is not disclosing any internal benchmarking of ChatGPT. Even in their GPT-4 technical report (OpenAI, 2023), they have shown the performance of GPT4 in terms of human-level exams. So, it is important that there are 3rd party evaluations of generative models.</p>
<h2>B General Experimental Details</h2>
<p>The experiments were done with the UI (15 December 2019 version) of ChatGPT provided by OpenAI, before the ChatGPT API was released. The number of samples for evaluation is 30-200. We've prioritized sample diversity, hand-picking tasks that encapsulate the abroad spectrum of scenarios a language model is likely to encounter, thus creating a representative snapshot of potential realworld applications. All experiments are single-run.</p>
<h2>C Multitask Evaluation of ChatGPT</h2>
<h2>C. 1 Summarization</h2>
<p>We test on 100 samples from two common summarization datasets: half from SAMSum (Gliwa et al., 2019), a dialogue summarization dataset, and another half from CNN/DM (Hermann et al., 2015; Nallapati et al., 2016), news summarization datasets. The large version of Bart (Lewis et al.,
2020b) model fine-tuned on both datasets is conducted for comparison. Moreover, OpenAI's text-davinci-002 is used as the previous SOTA zero-shot model. We calculate ROUGE-1 scores for evaluating the generated summary. According to the evaluation, ChatGPT achieves a similar zero-shot performance with text-davinci-002, which is expected since they evolved from the same GPT3 pretrained checkpoint. However, the fine-tuned Bart still outperforms zero-shot ChatGPT by a large margin.</p>
<h2>C. 2 Machine Translation</h2>
<p>We evaluate the machine translation ability of ChatGPT on both high-resource and low-resource languages using the ChrF++ metric (Popović, 2015). Specifically, we incorporate 8 high-resource languages, i.e., French (fra), Spanish (spa), Chinese (zho), Arabic (ara), Japanese (jpn), Indonesian (ind), Korean (kor), and Vietnamese (vie), and 4 low-resource languages, i.e., Javanese (jav), Sundanese (sun), Marathi (mar), and Buginese (bug) for our evaluation. ${ }^{3}$ For each language pair, we sample 30 Eng $\leftrightarrow$ XXX parallel sentences from the FLORES-200 dataset (Team et al., 2022; Goyal et al., 2021). The result of our experiment suggests that ChatGPT can well perform XXX $\rightarrow$ Eng translation, but it still lacks the ability to perform Eng $\rightarrow$ XXX translation.</p>
<h2>C. 3 Sentiment Analysis</h2>
<p>Sentiment analysis has been widely explored for both high-resource and low-resource languages (Wang et al., 2018a; Wilie et al., 2020; Ilmania et al., 2018).</p>
<p>We explore the sentiment analysis ability of ChatGPT through 4 languages with diverse amounts of resources in NusaX (Winata et al., 2022): English (eng), Indonesian (ind), Javanese (jav), and Buginese (bug). For each language, we sample 50 sentences from the corresponding dataset for our experiment and measure the macro F1 score as the evaluation metric. We compare the results with two baselines, i.e., supervised state-of-the-art performance from Winata et al. (2022) and zero-shot multilingual LLM from Cahyawijaya et al. (2022). ChatGPT outperforms the previous state-of-the-art zero-shot model by a large margin except for the Buginese, where it performs on par.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This shows that ChatGPT still has a limited understanding of extremely low-resource languages.</p>
<h2>C. 4 Question Answering</h2>
<p>Since Question Answering (QA) is a broad topic, we classify QA datasets into different categories based on the knowledge/reasoning type required to do the task, e.g commonsense reasoning, spatial reasoning, temporal reasoning, etc., to have a clearer analysis on ChatGPT's abilities. For each category, we select several datasets, and for each dataset, we sample 30 instances and test ChatGPT on the subset. Based on our experiment results, ChatGPT outperforms the existing zero-shot and some of the fine-tuned state-of-the-art performance on question answering. Furthermore, ChatGPT achieves near-perfect scores on three tasks, i.e., bAbI task 15, EntailmentBank, and Pep-3k.</p>
<h2>C. 5 Misinformation Detection</h2>
<p>We test ChatGPT's ability to detect misinformation with the test sets that consist of scientific and social claims related to COVID-19 (Lee et al., 2021) with 100 samples. We take half from scientific (covid-scientific) and another half from social (covid-social) sets. We evaluate the accuracy of the veracity by manually checking the generated text. ChatGPT could detect misinformation $92 \%$ (46/50) and $73.33 \%$ (22/30, excluding verificationrefusing cases) accuracy on covid-scientific and covid-social respectively.</p>
<h2>C. 6 ChatGPT on Dialogue Tasks</h2>
<h2>C.6.1 Knowledge-Grounded Open-Domain Dialogue</h2>
<p>Open-domain dialogue systems interact with humans with generated responses automatically and aim to provide users with an engaging experience. To boost informativeness, these systems leverage external knowledge, including structured knowledge such as knowledge graphs (Zhao et al., 2020; Ji et al., 2022b) and unstructured knowledge such as free text (Xu et al., 2022, 2023).</p>
<p>Prompt used for experiment: "Can we try dialogue generation? I will give you turns, and you can generate the next turn, but only one. \n \n You can also consider the knowledge of XXX for your reference in the dialogue."</p>
<h2>C.6.2 Task-Oriented Dialogue Experimental Setups</h2>
<p>Setup A: Modular Approach We investigate ChatGPT's ability for both dialogue state tracking and response generation in 50 dialogue turn samples taken from MultiWOZ2.2 (Zang et al., 2020). In detail, we ask the model to provide the belief state as domain-intent: [slot1, value1], ... in the prompt following previous zero-shot (Lin et al., 2021) and few-shot (Madotto et al., 2021) approaches, and provide an exhaustive list of domain-intent-slot-value for the given dialogue. For the response generation, we provide only the oracle dialogue actions (e.g. 'Hotel-Inform':['area', 'centre']), and ask ChatGPT to generate a TOD response given the dialogue history. We assess DST with joint goal accuracy (JGA), the ratio of dialogue turns where the predicted dialogue state is exactly the ground truth, and response generation with BLEU and inform rate(\%)</p>
<p>Setup B: Unified Approach We explore ChatGPT's ability to simulate a TOD interaction in an end-to-end manner by providing nothing more than a structured database and giving the instruction: "Use the following knowledge base to complete the task of recommending a restaurant as a task-oriented dialogue system".</p>
<p>Result Analysis: We could investigate whether ChatGPT is able to complete basic retrieval queries and respond to users' requests such as "Give me some restaurants that serve Italian food" or "I would prefer cheap options please". However, there are several limitations that we could investigate as follow.</p>
<ul>
<li>Long-term Multi-turn Dependency: ChatGPT cannot keep the belief state across multiple turns within the interaction. For instance, asking for Italian food will overwrite the previous turn's belief state by asking for restaurants with a rating of 3 or higher. However, if the user explicitly asks to recall the earlier preferences, ChatGPT is able to correct the retrieved information and incorporate the previous belief state. This is interesting as it shows that the information previously given in multi-turn is still usable, but needs to be called explicitly.</li>
<li>Basic Reasoning Failure: ChatGPT's response tends to be wrong if the query introduces a basic level of reasoning such as when</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ For a fairer comparison in our multitask experiment, we strictly follow the definition of high-resource and low-resource languages from NLLB (Team et al., 2022).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
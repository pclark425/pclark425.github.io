<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5595 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5595</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5595</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-cb29cf52f0f7d2e4324c68690a55b22890f2212d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cb29cf52f0f7d2e4324c68690a55b22890f2212d" target="_blank">How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas, and builds three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios.</p>
                <p><strong>Paper Abstract:</strong> The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5595.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5595.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (fine-tuned GPT-3.5 with RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (GPT-3.5 series, fine-tuned with Reinforcement Learning from Human Feedback) was used in this work as a text-based simulator of human experts by generating domain-specific answers (finance, medicine, law, psychology, computer-science, open QA) to the same questions answered by humans, enabling direct human-vs-model comparison, linguistic analyses, and downstream detection experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5 series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described by the authors as fine-tuned from the GPT-3.5 series using Reinforcement Learning from Human Feedback (RLHF); pretrained on large-scale web-crawled text, books, and code. Used via OpenAI's ChatGPT preview interface, where outputs can vary with chat history and decoding randomness was mitigated by refreshing threads per question.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multiple applied subdomains: medicine (medical QA/dialog), finance (investment/financial QA), law (legal QA), psychology (psychological QA/advice), computer science/concepts (wiki_csai), open-domain QA/ELI5</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate expert-style, text-based answers to user questions in the listed subdomains (i.e., simulate responses that a domain expert or high-quality web answer would provide); the paper treats ChatGPT as a simulator of human expert answers for comparative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Human evaluation metrics (Turing test detection accuracy: pair-expert, single-expert, single-amateur), Helpfulness proportion (fraction of cases where ChatGPT judged more helpful), automated detection F1 (RoBERTa & GLTR) for distinguishing ChatGPT vs human answers, language-model perplexity (PPL) comparisons, linguistic feature statistics (POS, dependency), and domain-specific detector F1 scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Domain-level human-evaluation (English, Table 2): overall pair-expert detect rate 0.90, single-expert 0.81, single-amateur 0.48; helpfulness overall 0.57. Examples by domain (English): reddit_eli5 pair-expert 0.97 single-expert 0.94 single-amateur 0.57 helpfulness 0.59; open_qa pair-expert 0.98 single-expert 0.78 single-amateur 0.34 helpfulness 0.72; wiki_csai pair-expert 0.97 single-expert 0.61 single-amateur 0.39 helpfulness 0.71; medicine pair-expert 0.97 single-expert 0.97 single-amateur 0.50 helpfulness 0.23; finance pair-expert 0.79 single-expert 0.73 single-amateur 0.58 helpfulness 0.60. Detection-performance (RoBERTa) on domain-filtered full-text (selected, Table 7): English full-text detection F1 (human vs ChatGPT) high across domains (e.g., medicine ~99.6 F1), but sentence-level detection much lower and variable by domain (sentence F1s drop substantially, e.g., open_qa ChatGPT detection as low as 26.78 in one reported cell). Language-model PPL: ChatGPT-generated texts show lower perplexity (more concentrated low-PPL distribution) compared to human-written texts. (All values quoted are reported in paper tables/figures.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>1) Domain/subdomain (task complexity and professional requirements): medical and legal domains show different helpfulness and error patterns (e.g., lower helpfulness in medical). 2) Prompt/context/chat history: authors note answers may vary across threads and chat history can influence outputs. 3) RLHF-induced style (longer, organized, neutral answers) affecting detectability. 4) Knowledge cutoff and refusal behavior (model cannot answer post-September 2021 facts). 5) Presence of obvious indicating phrases (e.g., 'AI assistant', 'I'm sorry to hear that') which both help detectors and can be filtered away. 6) Text granularity (full-paragraph vs single-sentence): sentence-level detection is harder. 7) Training/evaluation data composition for detectors (full vs sentence vs mixed) and detector architecture (RoBERTa vs GLTR).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>1) Domain effect: human helpfulness proportions and Turing-test detection rates reported per-domain (Table 2) show differences (e.g., medical helpfulness low). 2) Prompt/context: Section 2.2 notes that ChatGPT answers can vary with chat history and threads were refreshed per question. 3) RLHF/style: volunteers' summarization (Section 3.2) and POS/dependency analyses (Section 4) show ChatGPT produces longer, more organized, neutral outputs. 4) Knowledge cutoff/failure: volunteers observed refusals for post-Sept-2021 queries (Section 3.2(d)). 5) Indicating words: experiments creating 'filtered' datasets by removing sentences with indicating words improved robustness and changed detector performance (Section 5.4.2, Tables 4-5). 6) Granularity: sentence vs full experiments (raw-sent vs raw-full) show detectors trained on sentences generalize better to full, but full-trained detectors perform poorly on sentences (Section 5.4.3, Table 5). 7) Detector architecture: RoBERTa-based detectors are more robust than GLTR; GLTR is more sensitive to indicating words and shows larger drops (Section 5.4.1, Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert/amateur Turing tests (paired and single-answer formats) and helpfulness tests (human raters judged which answer is more helpful); automated linguistic analyses (POS, dependency parsing, sentiment classification), perplexity computed with GPT-2 small (text- and sentence-level); and supervised detector evaluations (RoBERTa fine-tuned classifiers and GLTR Test-2 logistic regression) with multiple train/test splits (raw, filtered, full, sentence, mixed) and F1 reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported failure modes include: fabrication of facts in professional domains (e.g., inventing non-existent legal provisions or incorrect details in medical answers), poor helpfulness in medical domain despite long answers, inability to answer post-September-2021 knowledge, variability across threads, and style differences (overly neutral/less emotional) that make it distinct from humans. Detection is notably harder on single sentences and for certain sources (wiki_csai and some open QA splits showed lower detection robustness). Authors also note dataset and style limitations (no special prompting used) that limit generality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct human vs ChatGPT comparisons across multiple domains via the HC3 corpus (human answers vs ChatGPT answers). Detector comparisons: RoBERTa-based deep classifiers (single-text and QA-pair variants) outperform GLTR Test-2 logistic regression, particularly in robustness to filtering and sentence granularity. Training-corpus comparisons: sentence-based or mixed training improves sentence-level detection; filtered (indicating-words removed) training improves cross-granularity robustness. QA-style detector (question+answer input) generally outperforms single-text detector in robustness (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For using LLMs in domain-specific answer generation: (a) exercise caution in professional domains (medicine, law) because of fabrication risks and lower perceived helpfulness; (b) be explicit about knowledge-cutoff limitations; (c) for detectors: include sentence-level (fine-grained) examples and filtered data to improve robustness, prefer robust deep classifiers (e.g., RoBERTa) and consider QA-pair detectors where question context is available; (d) remove or account for obvious indicating phrases if evaluating detector generalization; (e) note that special prompting (not explored here) can change model style and may bypass detectors—this remains a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_page_or_section</strong></td>
                            <td>Main supporting material: Sections 2 (ChatGPT answers collection), 3 (Human evaluation & summarization), 4 (Linguistic analysis including PPL), 5 (ChatGPT content detection experiments and ablations), Tables 1-7 and Figures 2-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>GLTR: Statistical detection and visualization of generated text <em>(Rating: 1)</em></li>
                <li>A general language assistant as a laboratory for alignment <em>(Rating: 1)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5595",
    "paper_id": "paper-cb29cf52f0f7d2e4324c68690a55b22890f2212d",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "ChatGPT (GPT-3.5)",
            "name_full": "ChatGPT (fine-tuned GPT-3.5 with RLHF)",
            "brief_description": "ChatGPT (GPT-3.5 series, fine-tuned with Reinforcement Learning from Human Feedback) was used in this work as a text-based simulator of human experts by generating domain-specific answers (finance, medicine, law, psychology, computer-science, open QA) to the same questions answered by humans, enabling direct human-vs-model comparison, linguistic analyses, and downstream detection experiments.",
            "citation_title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5 series)",
            "model_description": "Described by the authors as fine-tuned from the GPT-3.5 series using Reinforcement Learning from Human Feedback (RLHF); pretrained on large-scale web-crawled text, books, and code. Used via OpenAI's ChatGPT preview interface, where outputs can vary with chat history and decoding randomness was mitigated by refreshing threads per question.",
            "model_size": null,
            "scientific_subdomain": "Multiple applied subdomains: medicine (medical QA/dialog), finance (investment/financial QA), law (legal QA), psychology (psychological QA/advice), computer science/concepts (wiki_csai), open-domain QA/ELI5",
            "simulation_task": "Generate expert-style, text-based answers to user questions in the listed subdomains (i.e., simulate responses that a domain expert or high-quality web answer would provide); the paper treats ChatGPT as a simulator of human expert answers for comparative evaluation.",
            "accuracy_metric": "Human evaluation metrics (Turing test detection accuracy: pair-expert, single-expert, single-amateur), Helpfulness proportion (fraction of cases where ChatGPT judged more helpful), automated detection F1 (RoBERTa & GLTR) for distinguishing ChatGPT vs human answers, language-model perplexity (PPL) comparisons, linguistic feature statistics (POS, dependency), and domain-specific detector F1 scores.",
            "reported_accuracy": "Domain-level human-evaluation (English, Table 2): overall pair-expert detect rate 0.90, single-expert 0.81, single-amateur 0.48; helpfulness overall 0.57. Examples by domain (English): reddit_eli5 pair-expert 0.97 single-expert 0.94 single-amateur 0.57 helpfulness 0.59; open_qa pair-expert 0.98 single-expert 0.78 single-amateur 0.34 helpfulness 0.72; wiki_csai pair-expert 0.97 single-expert 0.61 single-amateur 0.39 helpfulness 0.71; medicine pair-expert 0.97 single-expert 0.97 single-amateur 0.50 helpfulness 0.23; finance pair-expert 0.79 single-expert 0.73 single-amateur 0.58 helpfulness 0.60. Detection-performance (RoBERTa) on domain-filtered full-text (selected, Table 7): English full-text detection F1 (human vs ChatGPT) high across domains (e.g., medicine ~99.6 F1), but sentence-level detection much lower and variable by domain (sentence F1s drop substantially, e.g., open_qa ChatGPT detection as low as 26.78 in one reported cell). Language-model PPL: ChatGPT-generated texts show lower perplexity (more concentrated low-PPL distribution) compared to human-written texts. (All values quoted are reported in paper tables/figures.)",
            "factors_affecting_accuracy": "1) Domain/subdomain (task complexity and professional requirements): medical and legal domains show different helpfulness and error patterns (e.g., lower helpfulness in medical). 2) Prompt/context/chat history: authors note answers may vary across threads and chat history can influence outputs. 3) RLHF-induced style (longer, organized, neutral answers) affecting detectability. 4) Knowledge cutoff and refusal behavior (model cannot answer post-September 2021 facts). 5) Presence of obvious indicating phrases (e.g., 'AI assistant', 'I'm sorry to hear that') which both help detectors and can be filtered away. 6) Text granularity (full-paragraph vs single-sentence): sentence-level detection is harder. 7) Training/evaluation data composition for detectors (full vs sentence vs mixed) and detector architecture (RoBERTa vs GLTR).",
            "evidence_for_factors": "1) Domain effect: human helpfulness proportions and Turing-test detection rates reported per-domain (Table 2) show differences (e.g., medical helpfulness low). 2) Prompt/context: Section 2.2 notes that ChatGPT answers can vary with chat history and threads were refreshed per question. 3) RLHF/style: volunteers' summarization (Section 3.2) and POS/dependency analyses (Section 4) show ChatGPT produces longer, more organized, neutral outputs. 4) Knowledge cutoff/failure: volunteers observed refusals for post-Sept-2021 queries (Section 3.2(d)). 5) Indicating words: experiments creating 'filtered' datasets by removing sentences with indicating words improved robustness and changed detector performance (Section 5.4.2, Tables 4-5). 6) Granularity: sentence vs full experiments (raw-sent vs raw-full) show detectors trained on sentences generalize better to full, but full-trained detectors perform poorly on sentences (Section 5.4.3, Table 5). 7) Detector architecture: RoBERTa-based detectors are more robust than GLTR; GLTR is more sensitive to indicating words and shows larger drops (Section 5.4.1, Table 4).",
            "evaluation_method": "Human expert/amateur Turing tests (paired and single-answer formats) and helpfulness tests (human raters judged which answer is more helpful); automated linguistic analyses (POS, dependency parsing, sentiment classification), perplexity computed with GPT-2 small (text- and sentence-level); and supervised detector evaluations (RoBERTa fine-tuned classifiers and GLTR Test-2 logistic regression) with multiple train/test splits (raw, filtered, full, sentence, mixed) and F1 reporting.",
            "limitations_or_failure_cases": "Reported failure modes include: fabrication of facts in professional domains (e.g., inventing non-existent legal provisions or incorrect details in medical answers), poor helpfulness in medical domain despite long answers, inability to answer post-September-2021 knowledge, variability across threads, and style differences (overly neutral/less emotional) that make it distinct from humans. Detection is notably harder on single sentences and for certain sources (wiki_csai and some open QA splits showed lower detection robustness). Authors also note dataset and style limitations (no special prompting used) that limit generality.",
            "comparisons": "Direct human vs ChatGPT comparisons across multiple domains via the HC3 corpus (human answers vs ChatGPT answers). Detector comparisons: RoBERTa-based deep classifiers (single-text and QA-pair variants) outperform GLTR Test-2 logistic regression, particularly in robustness to filtering and sentence granularity. Training-corpus comparisons: sentence-based or mixed training improves sentence-level detection; filtered (indicating-words removed) training improves cross-granularity robustness. QA-style detector (question+answer input) generally outperforms single-text detector in robustness (Table 6).",
            "recommendations_or_best_practices": "For using LLMs in domain-specific answer generation: (a) exercise caution in professional domains (medicine, law) because of fabrication risks and lower perceived helpfulness; (b) be explicit about knowledge-cutoff limitations; (c) for detectors: include sentence-level (fine-grained) examples and filtered data to improve robustness, prefer robust deep classifiers (e.g., RoBERTa) and consider QA-pair detectors where question context is available; (d) remove or account for obvious indicating phrases if evaluating detector generalization; (e) note that special prompting (not explored here) can change model style and may bypass detectors—this remains a limitation.",
            "evidence_page_or_section": "Main supporting material: Sections 2 (ChatGPT answers collection), 3 (Human evaluation & summarization), 4 (Linguistic analysis including PPL), 5 (ChatGPT content detection experiments and ablations), Tables 1-7 and Figures 2-4.",
            "uuid": "e5595.0",
            "source_info": {
                "paper_title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "GLTR: Statistical detection and visualization of generated text",
            "rating": 1
        },
        {
            "paper_title": "A general language assistant as a laboratory for alignment",
            "rating": 1
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1
        }
    ],
    "cost": 0.01241925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection</h1>
<p>Biyang Guo ${ }^{11 <em>}$, Xin Zhang ${ }^{2 </em>}$, Ziyuan Wang ${ }^{1 <em>}$, Minqi Jiang ${ }^{1 </em>}$, Jinran Nie ${ }^{3 *}$<br>Yuxuan Ding ${ }^{4}$, Jianwei Yue ${ }^{5}$, Yupeng Wu ${ }^{6}$<br>${ }^{1}$ AI Lab, School of Information Management and Engineering<br>Shanghai University of Finance and Economics<br>${ }^{2}$ Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)<br>${ }^{3}$ School of Information Science, Beijing Language and Culture University<br>${ }^{4}$ School of Electronic Engineering, Xidian University<br>${ }^{5}$ School of Computing, Queen's University, ${ }^{6}$ Wind Information Co., Ltd</p>
<h4>Abstract</h4>
<p>The introduction of ChatGPT ${ }^{2}$ has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https : //github.com/Hello-SimpleAI/chatgpt-comparison-detection.</p>
<h2>1 Introduction</h2>
<p>Since its dazzling debut in November 2022, OpenAI's ChatGPT has gained huge attention and wide discussion in the natural language processing (NLP) community and many other fields. According to OpenAI, ChatGPT is fine-tuned from the GPT-3.5 series with Reinforcement Learning from Human Feedback (RLHF; [7, 32]), using nearly the same methods as InstructGPT [25], but with slight differences in the data collection setup. The vast amount of knowledge in GPT-3.5 and the meticulous fine-tuning based on human feedback enable ChatGPT to excel at many challenging NLP</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>tasks, such as translating natural language to code [5], completing the extremely masked text [15] or generating stories given user-defined elements and styles [40], let alone typical NLP tasks like text classification, entity extraction, translation, etc. Furthermore, the carefully collected human-written demonstrations also make ChatGPT able to admit its mistakes, challenge incorrect premises and reject even inappropriate requests, as claimed by OpenAI ${ }^{3}$.
The surprisingly strong capabilities of ChatGPT have raised many interests, as well as concerns:
On the one hand, people are curious about how close is ChatGPT to human experts. Different from previous LLMs like GPT-3 [4], which usually fails to properly respond to human queries, InstructGPT [25] and the stronger ChatGPT have improved greatly in interactions with humans. Therefore, ChatGPT has great potential to become a daily assistant for general or professional consulting purposes [20, 21]. From the linguistic or NLP perspectives, we are also interested in where are the remaining gaps between ChatGPT and humans and what are their implicit linguistic differences $[14,18]$.
On the other hand, people are worried about the potential risks brought by LLMs like ChatGPT. With the free preview demo of ChatGPT going virus, a large amount of ChatGPT-generated content crowded into all kinds of UGC (User-Generated Content) platforms, threatening the quality and reliability of the platforms. For example, Stack Overflow, the famous programming questionanswering website, has temporarily banned ChatGPT-generated content ${ }^{4}$, because it believes "the average rate of getting correct answers from ChatGPT is too low, the posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers". Many other applications and activities are facing similar issues, such as online exams [33] and medical analysis [20]. Our empirical evaluation of ChatGPT on legal, medical, and financial questions also reveals that potentially harmful or fake information can be generated.
Considering the opaqueness of ChatGPT and the potential social risks associated with model misuse, we make the following contributions to both the academy and society:</p>
<ol>
<li>To facilitate LLM-related research, especially the study on the comparison between humans and LLMs, we collect nearly 40 K questions and their corresponding answers from human experts and ChatGPT, covering a wide range of domains (open-domain, computer science, finance, medicine, law, and psychology), named as the Human ChatGPT Comparison Corpus (HC3) dataset. The HC3 dataset is a valuable resource to analyze the linguistic and stylist characteristics of both humans and ChatGPT, which helps to investigate the future improvement directions for LLMs;</li>
<li>We conduct comprehensive human evaluations as well as linguistic analysis on human/ChatGPT-generated answers, discovering many interesting patterns exhibited by humans and ChatGPT. These findings can help to distinguish whether certain content is generated by LLMs, and also provide insights about where language models should be heading in the future;</li>
<li>Based on the HC3 dataset and the analysis, we develop several ChatGPT detecting models, targeting different detection scenarios. These detectors show decent performance in our held-out test sets. We also conclude several key factors that are essential to the detector's effectiveness.</li>
<li>We open-source all the collected comparison corpus, evaluations, and detection models, to facilitate future academic research and online platform regulations on AI-generated content.</li>
</ol>
<h1>2 Human ChatGPT Comparison Corpus (HC3)</h1>
<p>ChatGPT is based on the GPT-3.5 series, which is pre-trained on the super-large corpus, consisting of web-crawled text, books, and codes, making it able to respond to all kinds of questions. Therefore, we are curious how will a human (especially an expert) and ChatGPT respond to the same question respectively. Inspired by [1], we also want to evaluate whether ChatGPT can keep honest (not fabricate information or mislead the user), harmless (shouldn't generate harmful or offensive content),</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">HC3-English</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"># Questions</td>
<td style="text-align: center;"># Human Answers</td>
<td style="text-align: center;"># ChatGPT Answers</td>
<td style="text-align: center;">Source</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">24322</td>
<td style="text-align: center;">58546</td>
<td style="text-align: center;">26903</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">reddit_eli5</td>
<td style="text-align: center;">17112</td>
<td style="text-align: center;">51336</td>
<td style="text-align: center;">16660</td>
<td style="text-align: center;">ELI5 dataset [10]</td>
</tr>
<tr>
<td style="text-align: center;">open_qa</td>
<td style="text-align: center;">1187</td>
<td style="text-align: center;">1187</td>
<td style="text-align: center;">3561</td>
<td style="text-align: center;">WikiQA dataset [39]</td>
</tr>
<tr>
<td style="text-align: center;">wiki_csai</td>
<td style="text-align: center;">842</td>
<td style="text-align: center;">842</td>
<td style="text-align: center;">842</td>
<td style="text-align: center;">Crawled Wikipedia (A.1)</td>
</tr>
<tr>
<td style="text-align: center;">medicine</td>
<td style="text-align: center;">1248</td>
<td style="text-align: center;">1248</td>
<td style="text-align: center;">1337</td>
<td style="text-align: center;">Medical Dialog dataset [6]</td>
</tr>
<tr>
<td style="text-align: center;">finance</td>
<td style="text-align: center;">3933</td>
<td style="text-align: center;">3933</td>
<td style="text-align: center;">4503</td>
<td style="text-align: center;">FiQA dataset [23]</td>
</tr>
<tr>
<td style="text-align: center;">HC3-Chinese</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"># Questions</td>
<td style="text-align: center;"># Human Answers</td>
<td style="text-align: center;"># ChatGPT Answers</td>
<td style="text-align: center;">Source</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">12853</td>
<td style="text-align: center;">22259</td>
<td style="text-align: center;">17522</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">open_qa</td>
<td style="text-align: center;">3293</td>
<td style="text-align: center;">7377</td>
<td style="text-align: center;">3991</td>
<td style="text-align: center;">WebTextQA \&amp; BaikeQA [38]</td>
</tr>
<tr>
<td style="text-align: center;">baike</td>
<td style="text-align: center;">4617</td>
<td style="text-align: center;">4617</td>
<td style="text-align: center;">4617</td>
<td style="text-align: center;">Crawled BaiduBaike (A.1)</td>
</tr>
<tr>
<td style="text-align: center;">nlpcc_dbqa</td>
<td style="text-align: center;">1709</td>
<td style="text-align: center;">1709</td>
<td style="text-align: center;">4253</td>
<td style="text-align: center;">NLPCC-DBQA dataset [8]</td>
</tr>
<tr>
<td style="text-align: center;">medicine</td>
<td style="text-align: center;">1074</td>
<td style="text-align: center;">1074</td>
<td style="text-align: center;">1074</td>
<td style="text-align: center;">Medical Dialog dataset [6]</td>
</tr>
<tr>
<td style="text-align: center;">finance</td>
<td style="text-align: center;">689</td>
<td style="text-align: center;">1572</td>
<td style="text-align: center;">1983</td>
<td style="text-align: center;">ChineseNlpCorpus (A.1)</td>
</tr>
<tr>
<td style="text-align: center;">psychology</td>
<td style="text-align: center;">1099</td>
<td style="text-align: center;">5220</td>
<td style="text-align: center;">1099</td>
<td style="text-align: center;">from Baidu AI Studio (A.1)</td>
</tr>
<tr>
<td style="text-align: center;">law</td>
<td style="text-align: center;">372</td>
<td style="text-align: center;">690</td>
<td style="text-align: center;">505</td>
<td style="text-align: center;">LegalQA dataset (A.1)</td>
</tr>
</tbody>
</table>
<p>Table 1: Meta-information of the HC3 dataset. The English (resp. Chinese) contains 5 (resp. 7) splits.
and how helpful (provide concrete and correct solutions to the user's question) it is compared to human experts.</p>
<p>Taking these into account, we decided to collect a comparison corpus that consists of both human and ChatGPT answers to the same questions. We believe such a comparison corpus can be a valuable and interesting source to study the nature of the language of both humans and language models.</p>
<h1>2.1 Human Answers Collection</h1>
<p>Inviting human experts to manually write questions and answers is tedious and unaffordable for us to collect a large amount of data, therefore we construct the comparison dataset mainly from two sources:</p>
<ul>
<li>Publicly available question-answering datasets, where answers are given by experts in specific domains or the high-voted answers by web users;</li>
<li>Wiki text. We construct question-answer pairs using the concepts and explanations from wiki sources like Wikipedia ${ }^{5}$ and BaiduBaike ${ }^{6}$.
The split-data source mapping is shown in Table 1, and please refer to Appendix A. 1 for further detailed information.</li>
</ul>
<h3>2.2 ChatGPT Answers Collection</h3>
<p>Based on the collected human question-answering datasets, we use ChatGPT to generate answers to these questions. Since the ChatGPT is currently only available through its preview website, we manually input the questions into the input box, and get the answers, with the aid of some automation testing tools. Answers by ChatGPT can be influenced by the chatting history, so we refresh the thread for each question.
To make the answer more aligned with human answers, we add additional instructions to ChatGPT for specific datasets. For example, the human answers from the reddit-eli5 dataset split are under the context of "Explain like I'm five", therefore we use this context to instruct ChatGPT by adding "Explain like I'm five" at the end of the original question. More detail can be found in the Appendix.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ChatGPT can generate different answers given the same question in different threads, which is perhaps due to the random sampling in the decoding process. However, we found the differences can be very small, thereby we only collect one answer for most questions.</p>
<h1>2.3 Human ChatGPT Comparison Corpus (HC3)</h1>
<p>For each question, there can be more than one human/ChatGPT answer, therefore we organize the comparison data using the following format:</p>
<div class="codehilite"><pre><span></span><code>{
    &quot;question&quot;: &quot;Q1&quot;,
    &quot;human_answers&quot;: [&quot;A1&quot;, &quot;A2&quot;],
    &quot;chatgpt_answers&quot;: [&quot;B1&quot;]
}
</code></pre></div>

<p>Overall, we collected 24, 322 questions, 58, 546 human answers and 26, 903 ChatGPT answers for the English version, and 12, 853 questions, 22, 259 human answers and 17, 522 ChatGPT answers for the Chinese version. The meta-information of each dataset split is illustrated in Table 1.</p>
<h2>3 Human Evaluation \&amp; Summarization</h2>
<p>In this section, we invite many volunteer testers and conduct extensive human evaluations from different aspects. After the human evaluation, we make our collected comparison corpus available to the volunteers and ask them to manually conclude some characteristics. We then summarize the feedback from the volunteers combined with our observations.</p>
<h3>3.1 Human Evaluation</h3>
<p>The human evaluation is divided into the Turing test and the Helpfulness Test. The Turing Test [34] is a test of a machine's ability to exhibit intelligent behavior that is indistinguishable from a human. We invite 17 volunteers, divided into two groups: 8 experts (who are frequent users of ChatGPT) and 9 amateurs (who have never heard of ChatGPT). This is because people who are familiar with ChatGPT may have memorized some patterns exhibited by ChatGPT, helping them to easily distinguish the role.</p>
<p>We designed four types of evaluations, using different query formats or testing groups. We introduce the specific evaluation design and results in the following parts:</p>
<h2>$\mathcal{A}$. Expert Turing Test, Paired Text (pair-expert)</h2>
<p>The pair-expert test is conducted in the expert group. Each tester is required to do a series of tests, each test containing one question and a pair of answers (one from humans and another from ChatGPT). The tester needs to determine which answer is generated by ChatGPT.</p>
<h2>$\mathcal{B}$. Expert Turing Test, Single Text (single-expert)</h2>
<p>The single-expert test is also conducted in the expert group. Each tester is required to do a series of tests, each test containing one question and a single answer randomly given by humans or ChatGPT. The tester needs to determine whether the answer is generated by ChatGPT.</p>
<h2>C. Amateur Turing Test, Single Text (single-amateur)</h2>
<p>The single-amateur test is conducted in the amateur group. Each tester is required to do a series of tests, each test containing one question and a single answer randomly given by humans or ChatGPT. The tester needs to determine whether the answer is generated by ChatGPT.</p>
<h2>$\mathcal{D}$. Helpfulness Test (helpfulness)</h2>
<p>We are also curious about how helpful are the answers from ChatGPT compared with humans' answers to one question. Note that helpfulness is a very subjective metric, which can be influenced by many factors, including emotion, tester personality, personal preference, etc. Therefore, simply providing more accurate information or a more detailed analysis may not always lead to a more helpful answer.</p>
<p>The helpfulness test is conducted in the expert group. Each tester is required to do a series of tests, each containing one question and a pair of answers (one from human and another from ChatGPT).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Human Evaluation (En)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Pair-expert</td>
<td style="text-align: center;">Single-expert</td>
<td style="text-align: center;">Single-amateur</td>
<td style="text-align: center;">Helpfulness</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: left;">reddit_eli5</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: left;">open_qa</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: left;">wiki_csai</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">medical</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: left;">finance</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Human Evaluation (Zh)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Pair-expert</td>
<td style="text-align: center;">Single-expert</td>
<td style="text-align: center;">Single-amateur</td>
<td style="text-align: center;">Helpfulness</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: left;">open_qa</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">baike</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">nlpcc_dbqa</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">medicine</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: left;">finance</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">psychology</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">law</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.56</td>
</tr>
</tbody>
</table>
<p>Table 2: Human evaluations of ChatGPT generated answers for both English and Chinese.</p>
<p>Each tester is asked to pretend that the question is proposed by him/herself, and needs to determine which answer is more helpful to him/her.</p>
<p>Settings. We sample around $30&lt;$ question, human_answer, chatgpt_answer&gt; triplets from each split (i.e., reddit_eli5, wikipedia, medical, etc.) as the samples for the human evaluation. We allocate 2-5 testers for each split and report their average results. For all Turing tests, we report the proportion that ChatGPT-generated answer is correctly detected by testers. For the helpfulness test, we report the proportion that ChatGPT-generated answer is considered to be more helpful.</p>
<p>Results. Several conclusions can be drawn from the results shown in Table 2. Comparing the results of pair-expert and single-expert, we can find that it is easier to distinguish ChatGPTgenerated content when providing a comparison pair than only providing a single answer. Comparing the results of single-expert and single-amateur, we can find that the accuracy of experts is much higher than that of amateurs. The helpfulness test gives the proportion of questions that volunteers think the ChatGPT answer is more helpful to them. Surprisingly, results show that ChatGPT's answers are generally considered to be more helpful than humans' in more than half of questions, especially for finance and psychology areas. By checking the specific answers in these domains, we find that ChatGPT can usually provide more concrete and specific suggestions. However, ChatGPT performs poorly in terms of helpfulness for the medical domain in both English and Chinese. The ChatGPT often gives lengthy answers to medical consulting in our collected dataset, while human experts may directly give straightforward answers or suggestions, which may partly explain why volunteers consider human answers to be more helpful in the medical domain.</p>
<h1>3.2 Human Summarization</h1>
<p>After the above evaluations, we open our collected HC3 dataset to the volunteers where they can freely browse the comparison answers from humans and ChatGPT. All dataset splits are allocated to different volunteers, and each volunteer is asked to browse at least 100 groups of comparison data. After that, we ask them to summarize the characteristics of both human answers and ChatGPT answers. Eventually, we received more than 200 feedbacks, and we summarize these findings as follows:</p>
<h1>Distinctive Patterns of ChatGPT</h1>
<p>(a) ChatGPT writes in an organized manner, with clear logic. Without loss of generality, ChatGPT loves to define the core concept in the question. Then it will give out detailed answers step by step and offers a summary at the end, following the deduction and summary structure;
(b) ChatGPT tends to offer a long and detailed answer. This is the direct product of the Reinforcement Learning with Human Feedback, i.e. RLHF, and also partly related to the pattern (a) unless you offer a prompt such as "Explain it to me in one sentence";
(c) ChatGPT shows less bias and harmful information. ChatGPT is neutral on sensitive topics, barely showing any attitude towards the realm of politics or discriminatory toxic conversations;
(d) ChatGPT refuses to answer the question out of its knowledge. For instance, ChatGPT cannot respond to queries that require information after September 2021. Sometimes ChatGPT also refuses to answer what it believes it doesn't know. It is also RLHF's ability to implicitly and automatically determine which information is within the model's knowledge and which is not.
(e) ChatGPT may fabricate facts. When answering a question that requires professional knowledge from a particular field, ChatGPT may fabricate facts in order to give an answer, though [25] mentions that InstructGPT model has already shown improvements in truthfulness over GPT-3. For example, in legal questions, ChatGPT may invent some non-existent legal provisions to answer the question. This phenomenon warns us to be extra careful when using ChatGPT for professional consultations. Additionally, when a user poses a question that has no existing answer, ChatGPT may also fabricate facts in order to provide a response.</p>
<p>Many of the conclusions mentioned above like (b),(c),(d) are also discussed in [12] by Fu et al.</p>
<h2>Major Differences between Human and ChatGPT</h2>
<p>(a) ChatGPT's responses are generally strictly focused on the given question, whereas humans' are divergent and easily shift to other topics. In terms of the richness of content, humans are more divergent in different aspects, while ChatGPT prefers focusing on the question itself. Humans can answer the hidden meaning under the question based on their own common sense and knowledge, but the ChatGPT relies on the literal words of the question at hand;
(b) ChatGPT provides objective answers, while humans prefer subjective expressions. Generally, ChatGPT generates safer, more balanced, neutral, and informative texts compared to humans. As a result, ChatGPT is excellent at interpreting terminology and concepts. On the other hand, human answers are more specific and include detailed citations from sources based on legal provisions, books, and papers, especially when providing suggestions for medical, legal, and technical problems, etc.;
(c) ChatGPT's answers are typically formal, meanwhile humans' are more colloquial. Humans tend to be more succinct with full of oral abbreviations and slang such as "LOL", "TL;DR", "GOAT" etc. Humans also love to apply humor, irony, metaphors, and examples, whereas ChatGPT never uses antiphrasis. Additionally, human communication often includes the "Internet meme" as a way to express themselves in a specific and vivid way;
(d) ChatGPT expresses less emotion in its responses, while human chooses many punctuation and grammar feature in context to convey their feelings. Human uses multiple exclamation mark('!'), question mark(‘?'), ellipsis(‘...') to express their strong emotion, and use various brackets('(', ')', '[', ']') to explain things. By contrast, ChatGPT likes to use conjunctions and adverbs to convey a logical flow of thought, such as "In general", "on the other hand", "Firstly,..., Secondly,..., Finally" and so on.</p>
<p>Overall, these summarised features indicate that ChatGPT has improved notably in questionanswering tasks for a wide range of domains. Compared with humans, we can imagine ChatGPT as a conservative team of experts. As a "team", it may lack individuality but can have a more comprehensive and neutral view towards questions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">English</th>
<th style="text-align: center;">avg. len.</th>
<th style="text-align: center;">vocab size</th>
<th style="text-align: center;">density</th>
<th style="text-align: center;">Chinese</th>
<th style="text-align: center;">avg. len.</th>
<th style="text-align: center;">vocab size</th>
<th style="text-align: center;">density</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">human</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">142.50</td>
<td style="text-align: center;">79157</td>
<td style="text-align: center;">2.33</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">102.27</td>
<td style="text-align: center;">75483</td>
<td style="text-align: center;">5.75</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">198.14</td>
<td style="text-align: center;">66622</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">115.3</td>
<td style="text-align: center;">45168</td>
<td style="text-align: center;">3.05</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">reddit_eli5</td>
<td style="text-align: center;">134.21</td>
<td style="text-align: center;">55098</td>
<td style="text-align: center;">2.46</td>
<td style="text-align: center;">nlpcc_dbqa</td>
<td style="text-align: center;">24.44</td>
<td style="text-align: center;">10621</td>
<td style="text-align: center;">25.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">194.84</td>
<td style="text-align: center;">44926</td>
<td style="text-align: center;">1.38</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">78.21</td>
<td style="text-align: center;">11971</td>
<td style="text-align: center;">8.96</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">open_qa</td>
<td style="text-align: center;">35.09</td>
<td style="text-align: center;">9606</td>
<td style="text-align: center;">23.06</td>
<td style="text-align: center;">open_qa</td>
<td style="text-align: center;">93.68</td>
<td style="text-align: center;">40328</td>
<td style="text-align: center;">13.13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">131.68</td>
<td style="text-align: center;">16251</td>
<td style="text-align: center;">10.40</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">150.66</td>
<td style="text-align: center;">26451</td>
<td style="text-align: center;">5.35</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">wiki_csai</td>
<td style="text-align: center;">229.34</td>
<td style="text-align: center;">15859</td>
<td style="text-align: center;">8.21</td>
<td style="text-align: center;">baike</td>
<td style="text-align: center;">112.25</td>
<td style="text-align: center;">28966</td>
<td style="text-align: center;">5.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">208.33</td>
<td style="text-align: center;">9741</td>
<td style="text-align: center;">5.55</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.19</td>
<td style="text-align: center;">14041</td>
<td style="text-align: center;">3.94</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">medicine</td>
<td style="text-align: center;">92.98</td>
<td style="text-align: center;">11847</td>
<td style="text-align: center;">10.42</td>
<td style="text-align: center;">medicine</td>
<td style="text-align: center;">92.34</td>
<td style="text-align: center;">9855</td>
<td style="text-align: center;">9.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">209.61</td>
<td style="text-align: center;">7694</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">165.41</td>
<td style="text-align: center;">7211</td>
<td style="text-align: center;">4.06</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;">202.07</td>
<td style="text-align: center;">25500</td>
<td style="text-align: center;">3.21</td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;">80.76</td>
<td style="text-align: center;">2759</td>
<td style="text-align: center;">5.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">226.01</td>
<td style="text-align: center;">21411</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">120.84</td>
<td style="text-align: center;">4043</td>
<td style="text-align: center;">4.94</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">psychology</td>
<td style="text-align: center;">254.82</td>
<td style="text-align: center;">16160</td>
<td style="text-align: center;">5.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">164.53</td>
<td style="text-align: center;">5897</td>
<td style="text-align: center;">3.26</td>
</tr>
<tr>
<td style="text-align: center;">human <br> ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">law</td>
<td style="text-align: center;">28.77</td>
<td style="text-align: center;">2093</td>
<td style="text-align: center;">19.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">143.76</td>
<td style="text-align: center;">3857</td>
<td style="text-align: center;">7.21</td>
</tr>
</tbody>
</table>
<p>Table 3: Average answer length, vocabulary size and density comparisons on our corpus.</p>
<h1>4 Linguistic Analysis</h1>
<p>In this section, we analyze the linguistic features of both humans' and ChatGPT's answers, and try to find some statistical evidence for the characteristics concluded in Section 3.</p>
<h3>4.1 Vocabulary Features</h3>
<p>In this part, we analyze the vocabulary features of our collected corpus. We are interested in how humans and ChatGPT differ in the choice of words when answering the same set of questions.
Since the number of human/ChatGPT answers is unbalanced, we randomly sample one answer from humans and one answer from ChatGPT during our statistical process. We calculated the following features: average length $(L)$, which is the average number of words in each question; vocab size $(V)$, the number of unique words used in all answers; we also propose another feature called density $(D)$, which is calculated by $D=100 \times V /(L \times N)$ where $N$ is the number of answers. Density measures how crowded different words are used in the text. For example, if we write some articles that add up to 1000 words, but only 100 different words are used, then the density is $100 \times 100 / 1000=10$. The higher the density is, the more different words are used in the same length of text.
In Table 3, we report the vocabulary features for both English and Chinese corpus. Looking at both features of average length and vocab size, we can see that: compared to ChatGPT, human answers are relatively shorter, but a larger vocabulary is used. This phenomenon is particularly obvious in the Chinese open_qa split and the medical splits in both languages, where the average length of ChatGPT is nearly twice longer than that of humans, but the vocab size is significantly smaller.
This phenomenon is also reflected by the density factor. The word density of humans is greater than ChatGPT's in every split, which further reveals that humans use a more diverse vocabulary in their expressions.</p>
<h3>4.2 Part-of-Speech \&amp; Dependency Analysis</h3>
<p>In this part, we compare the occurrences of different part-of-speech (POS) tags and the characteristics of the dependency relations.</p>
<h3>4.2.1 Part-of-Speech</h3>
<p>Figure 1 illustrates the comparisons between humans and ChatGPT in terms of POS usage. In HC3-English, ChatGPT uses more NOUN, VERB, DET, ADJ, AUX, CCONJ and PART words, while using less ADV and PUNCT words.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Part-of-Speech distribution comparison between ChatGPT and human answers. Results are sorted by POS proportion of human answers. The upper figure is for the HC3-English dataset and the lower is for the HC3-Chinese dataset.</p>
<p>A high proportion of nouns (NOUN) often indicates that the text is more argumentative, exhibiting informativeness and objectivity [24]. Accordingly, adposition (ADP) and adjective (ADJ) words also tend to appear more frequently [11]. The frequent co-occurrence of conjunctions (CCONJ) along with nouns, verbs, and adposition words indicates that the structure of the article and the relationships of cause-and-effect, progression, or contrast are clear. The above are also typical characteristics in academic papers or official documents [29]. We believe the RLHF training process has a great influence on ChatGPT's writing style, which partly explains the difference in the POS tags distribution.</p>
<h1>4.2.2 Dependency Parsing</h1>
<p>Dependency parsing is a technique that analyzes the grammatical structure of a sentence by identifying the dependencies between its words. We parse the answers in the corpus and compare the proportion of different dependency relations and their corresponding dependency distances. Figure 2 shows the comparison between humans and ChatGPT in HC3-English. Due to the limited space, the Chinese version is placed in the Appendix A.2.</p>
<p>The comparison of dependency relations exhibits similar characteristics to that of POS tags, where ChatGPT uses more determination, conjunction, and auxiliary relations. In terms of the dependency distance, ChatGPT has much longer distances for the punct and dep relations, which is perhaps due to the fact that CharGPT tends to use longer sentences. However, ChatGPT has obviously shorter conj relations. According to the analysis of POS tags, ChatGPT usually uses more conjunctions than humans to make the content more logical, this may explain why the conj relations of ChatGPT are relatively shorter than humans.</p>
<h3>4.3 Sentiment Analysis</h3>
<p>Humans are emotional beings, it is natural that our emotions are reflected in our words, to some extent. ChatGPT is learned on large-scale human-generated text, but it is further fine-tuned with human instructions. Therefore we are curious how "emotional" ChatGPT is compared with humans.
We use a multilingual sentiment classification model ${ }^{7}$ fine-tuned on Twitter corpus [2] to conduct sentiment analysis for both English and Chinese comparison data. Note that deep learning-based models can be greatly influenced by some indicating words (such as "but" and "sorry" can easily</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Top-30 dependency relations (upper) and corresponding dependency distances (lower) comparison between human and ChatGPT answers in HC3-English. Results are sorted by relations proportion of human answers.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Proportions of three kinds of sentiments (neutral, positive, and negative) in our corpus.
fool the classifier to predict the "negative" label), making the predictions biased [16]. Therefore, the sentiment given by the classifier is only a reference to the true sentiment behind the text.</p>
<p>Figure 3 shows the comparison of the sentiment distribution of humans and ChatGPT. Several findings can be drawn from the results: First, we find that the proportion of neutral emotions is the largest for both humans and ChatGPT, which is in line with our expectations. However, ChatGPT generally expresses more neutral sentiments than humans. Then, the proportion of negative emotions is significantly higher than that of positive emotions. Notably, humans express significantly more negative emotions than ChatGPT. The proportion of humans' positive emotions is also slightly higher than that of ChatGPT. Overall, ChatGPT is less emotional than humans, though it is not completely emotionless.</p>
<h1>4.4 Language Model Perplexity</h1>
<p>The perplexity (PPL) is commonly used as a metric for evaluating the performance of language models (LM). It is defined as the exponential of the negative average log-likelihood of the text under</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: PPL distributions on both English and Chinese data, as well as both text and sentence levels.
the LM. A lower PPL indicates that the language model is more confident in its predictions, and is therefore considered to be a better model. The training of LMs is carried out on large-scale text corpora, it can be considered that it has learned some common language patterns and text structures. Therefore, we can use PPL to measure how well a text conforms to common characteristics.</p>
<p>We use the open-source GPT-2 small ${ }^{8}$ (Wenzhong-GPT2-110M ${ }^{9}$ for Chinese) model to compute the PPL (both text-level and sentence-level ${ }^{10}$ PPLs) of the collected texts. The PPL distributions of text written by humans and text generated by ChatGPT are shown in Figure 4.</p>
<p>It is clearly observed that, regardless of whether it is at the text level or the sentence level, the content generated by ChatGPT has relatively lower PPLs compared to the text written by humans. ChatGPT captured common patterns and structures in the text it was trained on, and is very good at reproducing them. As a result, text generated by ChatGPT have relatively concentrated low PPLs.</p>
<p>Humans have the ability to express themselves in a wide variety of ways, depending on the context, audience, and purpose of the text they are writing. This can include using creative or imaginative elements, such as metaphors, similes, and unique word choices, which can make it more difficult for GPT2 to predict. Therefore, human-written texts have more high-PPL values, and show a long-tailed distribution, as demonstrated in Figure 4.</p>
<h1>5 ChatGPT Content Detection</h1>
<p>AI-generated content (AIGC) is becoming increasingly prevalent on the internet, and it can be difficult to distinguish it from human-generated content, as shown in our human evaluation (sec 3.1). Therefore, AIGC detectors are needed to help identify and flag content that has been created by a machine, to reduce the potential risks to society caused by improper or malicious use of AI models, and to improve the transparency and accountability of the information that is shared online.</p>
<p>In this section, we conduct several empirical experiments to investigate the ChatGPT content detection systems. Detecting AI-generated content is a widely studied topic [19, 27]. Based on these [30, 13, 27], we establish three different types of detection systems, including machine learning-based and deep learning-based methods, and evaluate them on different granularities and data sources. Detailed results and discussions are provided.</p>
<h3>5.1 Methods</h3>
<p>Detection of machine-generated text has been gaining popularity as text generation models have advanced in recent years[19, 27]. Here, we implement three representative methods from classic machine learning and deep learning, i.e, a logistic regression model trained on the GLTR Test-2[13] features, a deep classifier for single-text detection and a deep classifier for QA detection. The deep classifiers for both single-text and QA are based on RoBERTa [22], a strong pre-trained Transformer [35] model. In fact, algorithms for OOD detection or anomaly detection [17] can also be applied to develop ChatGPT content detectors, which we leave for future work.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The experiment design for the training and testing of detectors. Different dataset versions are generated through filtering or splitting.</p>
<p>GLTR. [13] studied three tests to compute features of an input text. Their major assumption is that to generate fluent and natural-looking text, most decoding strategies sample high probabilities tokens from the head of the distribution. We select the most powerful Test-2 feature, which is the number of tokens in the Top-10, Top-100, Top-1000, and 1000+ ranks from the LM predicted probability distributions. And then a logistic regression model is trained to finish the classification.</p>
<p>RoBERTa-sinlge. A deep classifier based on the pre-trained LM is always a good choice for this kind of text classification problem. It is also investigated in many studies and demo systems [30, 9, 27]. Here we fine-tune the RoBERTa [22] model.</p>
<p>RoBERTa-QA. While most content detectors are developed to classify whether a single piece of text is AI-generated, we claim that a detector that supports inputting both a question and an answer can be quite useful, especially for question-answering scenarios. Therefore, we decide to also build a QA version detector. The RoBERTa model supports a text pair input format, where a separating token is used to join a question and its corresponding answer.</p>
<h1>5.2 Implementation Details</h1>
<p>For the LM used by GLTR, we use gpt2-small [28] for English, and Wenzhong-GPT2-110M released by [36] for Chinese, it is the same with sec. 4.4. For RoBERTa-based deep classifiers, we use roberta-base ${ }^{11}$ and hfl/chinese-roberta-wwm-ext ${ }^{12}$ checkpoints for English and Chinese, respectively. All the above models are obtained from huggingface transformers [37].
We train the logistic regression model by sklearn [26] on the GLTR Test-2 features from trainset, and search hyper-params following the code of [27]. The RoBERTa-based detectors are trained by the facilities of transformers. Specifically, we use the AdamW optimizer, setting batch size to 32 and learning rate to $5 e-5$. We finetune models by 1 epoch for English, and 2 epochs for Chinese.</p>
<h3>5.3 Experiment Design</h3>
<p>The HC3 dataset consists of questions and their corresponding human/ChatGPT answers. We extracted all the <question, answer> pairs, and assigned label 0 to pairs with human answers and label 1 to pairs with ChatGPT answers.</p>
<p>Simply using the original answers from humans and ChatGPT to train a binary classifier is the most straightforward way. However, there might be some issues by doing so:</p>
<ul>
<li>First, based on the observations in Section 3, both human answers and ChatGPT answers may contain some obvious indicating words that may influence the effectiveness of models;</li>
<li>Second, users may want to detect whether a single sentence is generated by ChatGPT, instead of the full text. This can be quite difficult for a classifier that is only trained on full texts;</li>
<li>Third, taking the corresponding question of the answer into account may help the detector to make a more accurate judgment, compared with only considering the answer itself. This</li>
</ul>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>can be widely applied to many QA platforms (like Quora, Stack Overflow, and Zhihu) to find out which answer below a certain question is generated by AI.</p>
<p>Therefore, we design different groups of experiments to study these key questions:</p>
<ul>
<li>How will the indicating words influence the detector?</li>
<li>Is it more challenging for the ChatGPT detectors to detect sentence-level content? Is it harder to train a sentence-level classifier?</li>
<li>Can the corresponding question help detectors detect the origin of the answer more accurately?</li>
</ul>
<p>Figure 5 shows how we generate different types of training and testing sets. Specifically, we use the collected raw corpus to construct the first train-test sets (the "full text (raw)" in the figure), which we call the raw-full version. Then we filter away the indicated words in the text to obtain the filtered-full version. By splitting the full text into sentences, we obtain the raw-sent version and the filtered-sent version. We also combine the full text and the sentences into a mixed version, namely the raw-mix and filtered-mix version. Overall, we have six different versions of training and testing sets. Evaluating a model's performance on version B's testing set which is trained on version A's training set can be seen as an out-of-distribution (OOD) generalization evaluation, which is more challenging since it requires the model to be robust when facing sample style changes.</p>
<h1>5.4 Results</h1>
<p>Following the above experiment design, we conduct comprehensive empirical studies on all kinds of derived corpus. Table 4 shows the test F1 scores.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">English</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chinese</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test $\rightarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">raw <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">filtered <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">raw <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">filtered <br> sent</td>
<td style="text-align: center;">mix</td>
</tr>
<tr>
<td style="text-align: center;">Train $\downarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">raw</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">81.89</td>
<td style="text-align: center;">84.67</td>
<td style="text-align: center;">99.72</td>
<td style="text-align: center;">81.00</td>
<td style="text-align: center;">84.07</td>
<td style="text-align: center;">88.53</td>
<td style="text-align: center;">98.79</td>
<td style="text-align: center;">83.64</td>
<td style="text-align: center;">86.32</td>
<td style="text-align: center;">98.57</td>
<td style="text-align: center;">82.77</td>
<td style="text-align: center;">85.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">98.43</td>
<td style="text-align: center;">98.56</td>
<td style="text-align: center;">99.24</td>
<td style="text-align: center;">98.47</td>
<td style="text-align: center;">98.59</td>
<td style="text-align: center;">98.78</td>
<td style="text-align: center;">97.76</td>
<td style="text-align: center;">95.75</td>
<td style="text-align: center;">96.11</td>
<td style="text-align: center;">97.68</td>
<td style="text-align: center;">95.31</td>
<td style="text-align: center;">95.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">99.44</td>
<td style="text-align: center;">98.31</td>
<td style="text-align: center;">98.47</td>
<td style="text-align: center;">99.32</td>
<td style="text-align: center;">98.37</td>
<td style="text-align: center;">98.51</td>
<td style="text-align: center;">98.74</td>
<td style="text-align: center;">97.70</td>
<td style="text-align: center;">95.68</td>
<td style="text-align: center;">96.04</td>
<td style="text-align: center;">97.65</td>
<td style="text-align: center;">95.27</td>
<td style="text-align: center;">95.73</td>
</tr>
<tr>
<td style="text-align: center;">filtered</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">87.17</td>
<td style="text-align: center;">89.05</td>
<td style="text-align: center;">99.79</td>
<td style="text-align: center;">86.60</td>
<td style="text-align: center;">88.67</td>
<td style="text-align: center;">91.85</td>
<td style="text-align: center;">98.25</td>
<td style="text-align: center;">91.04</td>
<td style="text-align: center;">92.30</td>
<td style="text-align: center;">98.14</td>
<td style="text-align: center;">91.15</td>
<td style="text-align: center;">92.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">96.97</td>
<td style="text-align: center;">97.22</td>
<td style="text-align: center;">97.19</td>
<td style="text-align: center;">99.09</td>
<td style="text-align: center;">98.43</td>
<td style="text-align: center;">98.53</td>
<td style="text-align: center;">97.91</td>
<td style="text-align: center;">96.60</td>
<td style="text-align: center;">92.81</td>
<td style="text-align: center;">93.47</td>
<td style="text-align: center;">97.94</td>
<td style="text-align: center;">95.86</td>
<td style="text-align: center;">96.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">96.28</td>
<td style="text-align: center;">96.43</td>
<td style="text-align: center;">96.41</td>
<td style="text-align: center;">99.45</td>
<td style="text-align: center;">98.37</td>
<td style="text-align: center;">98.53</td>
<td style="text-align: center;">97.58</td>
<td style="text-align: center;">97.43</td>
<td style="text-align: center;">94.09</td>
<td style="text-align: center;">94.68</td>
<td style="text-align: center;">97.66</td>
<td style="text-align: center;">95.61</td>
<td style="text-align: center;">96.01</td>
</tr>
<tr>
<td style="text-align: center;">Train $\downarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLTR Test-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">raw</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">98.26</td>
<td style="text-align: center;">71.58</td>
<td style="text-align: center;">76.15</td>
<td style="text-align: center;">98.22</td>
<td style="text-align: center;">70.19</td>
<td style="text-align: center;">75.23</td>
<td style="text-align: center;">81.61</td>
<td style="text-align: center;">89.61</td>
<td style="text-align: center;">44.02</td>
<td style="text-align: center;">53.72</td>
<td style="text-align: center;">85.89</td>
<td style="text-align: center;">43.58</td>
<td style="text-align: center;">53.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">86.26</td>
<td style="text-align: center;">88.18</td>
<td style="text-align: center;">87.96</td>
<td style="text-align: center;">87.72</td>
<td style="text-align: center;">88.23</td>
<td style="text-align: center;">88.19</td>
<td style="text-align: center;">87.76</td>
<td style="text-align: center;">84.49</td>
<td style="text-align: center;">71.79</td>
<td style="text-align: center;">74.01</td>
<td style="text-align: center;">84.06</td>
<td style="text-align: center;">70.29</td>
<td style="text-align: center;">72.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">95.97</td>
<td style="text-align: center;">86.45</td>
<td style="text-align: center;">87.81</td>
<td style="text-align: center;">96.13</td>
<td style="text-align: center;">86.24</td>
<td style="text-align: center;">87.73</td>
<td style="text-align: center;">90.06</td>
<td style="text-align: center;">86.45</td>
<td style="text-align: center;">70.85</td>
<td style="text-align: center;">73.59</td>
<td style="text-align: center;">84.94</td>
<td style="text-align: center;">69.14</td>
<td style="text-align: center;">72.14</td>
</tr>
<tr>
<td style="text-align: center;">filtered</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">98.31</td>
<td style="text-align: center;">70.91</td>
<td style="text-align: center;">75.65</td>
<td style="text-align: center;">98.30</td>
<td style="text-align: center;">69.48</td>
<td style="text-align: center;">74.72</td>
<td style="text-align: center;">81.23</td>
<td style="text-align: center;">89.46</td>
<td style="text-align: center;">58.69</td>
<td style="text-align: center;">64.52</td>
<td style="text-align: center;">86.51</td>
<td style="text-align: center;">55.45</td>
<td style="text-align: center;">62.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">88.25</td>
<td style="text-align: center;">87.71</td>
<td style="text-align: center;">85.68</td>
<td style="text-align: center;">88.35</td>
<td style="text-align: center;">87.99</td>
<td style="text-align: center;">87.00</td>
<td style="text-align: center;">84.56</td>
<td style="text-align: center;">71.85</td>
<td style="text-align: center;">74.07</td>
<td style="text-align: center;">84.22</td>
<td style="text-align: center;">70.59</td>
<td style="text-align: center;">73.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">95.36</td>
<td style="text-align: center;">86.73</td>
<td style="text-align: center;">87.97</td>
<td style="text-align: center;">95.60</td>
<td style="text-align: center;">86.56</td>
<td style="text-align: center;">87.92</td>
<td style="text-align: center;">90.02</td>
<td style="text-align: center;">86.30</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">73.70</td>
<td style="text-align: center;">84.98</td>
<td style="text-align: center;">69.45</td>
<td style="text-align: center;">72.40</td>
</tr>
</tbody>
</table>
<p>Table 4: F1 scores (\%) of different models on each testset, average of each language are reported.</p>
<h3>5.4.1 Which detector(s) is more useful? ML-based or DL-based? and Why?</h3>
<p>According to Table 4, we can derive following conclusions:
Firstly, the robustness of RoBERTa-based-detector is better than GLTR. The F1-scores of RoBERTa decrease slightly ( $1.5-2 \%$ in English datasets and 2-3\% in Chinese datasets) when sentences are split by comparing the leading diagonal elements in raw $\rightarrow$ raw and filtered $\rightarrow$ filtered. In contrast, the GLTR reduces significantly by over $10 \%$ in English datasets, and above $15 \%$ in Chinese datasets. Above all, the RoBERTa-based-detector is more robust with anti-interference character. In contrast, the GLTR reduces significantly by over $10 \%$ in English datasets, above $15 \%$ in Chinese datasets. Above all, the RoBERTa-based-detector is more robust with anti-interference character.
Secondly, RoBERTa-based-detector is not affected by indicating words. The F1-scores of RoBERTa only slightly decreased by $0.03 \%$ in English full dataset, and $0.65 \%$ in Chinese full dataset, as seen in the minus of relevant leading diagonal elements in raw $\rightarrow$ raw versus filtered $\rightarrow$ filtered. On the contrary, evaluations based on GLTR decrease by up to $3.1 \%$ on Chinese datasets, though tiny rise on English datasets, indicating that GLTR is sensitive to indicating words, easily influenced by the patterns of ChatGPT.</p>
<p>Lastly, RoBERTa-based-detector is effective in handling Out-Of-Distribution scenarios. When compared to the original model, it demonstrates a significant decrease in performance on GLTR's OOD test datasets, with a drop of up to $28.8 \%$ on English datasets(filtered-full $\rightarrow$ filtered-full -filtered-full $\rightarrow$ filtered-sent) and $45.5 \%$ on Chinese datasets(raw-full $\rightarrow$ raw-full - raw-full $\rightarrow$ raw-sent). However, RoBERTa maintains consistent performance with F1-scores varying by no more than $19 \%$.</p>
<h1>5.4.2 How will the indicating words influence the detector?</h1>
<p>We first collected a bunch of indicating words for both humans and ChatGPT. For example, ChatGPT's indicating words (or phrases) include "AI assistant", "I'm sorry to hear that", and "There're a few steps...", etc. and humans' indicating words may include "Hmm", "Nope", "My view is", etc. In the filtered version, we remove all sentences in the answers that contain the indicating words for both humans and ChatGPT.</p>
<p>According to Table 4, removing the indicating words helps the models trained on full-text to perform better across different content granularities. For example, the RoBERTa-filter-full performs significantly better than RoBERTa-raw-full in terms of sentence-level and mix-level evaluations, improving more than $3 \%$ F1 scores on average. However, the filtering may slightly hurt the performances of the models trained on sentences. This may be because the indicating words play a bigger part in the sentence-level text compared with the full text. Removing the indicating words may make some sentences literally unable to be distinguished.</p>
<h3>5.4.3 Which granularity is more difficult to detect? Full-text or sentence?</h3>
<p>Through the extensive experimental results in Table 5, we conclude that detecting ChatGPT generated texts is more difficult in a single sentence than in a full text. This conclusion can be proved by the following two points: First, our results show that both English and Chinese sentence-based detectors (i.e., raw-sent and filtered-sent versions) achieve satisfactory results w.r.t. the testing task of detecting either ChatGPT generated paragraphs or sentences, whereas the opposite is not true-—raw-full and filtered-full are relatively inferior when detecting ChatGPT generated sentences. In other words, detectors trained on "hard samples" (i.e., sentence corpus) are much easier to solve simple task (i.e., detecting full corpus), while "simple samples" (i.e., full corpus) may be less useful for solving more difficult task (i.e., sentence corpus).</p>
<p>Second, we observe that although both full and sentence corpus are provided in the raw-mix and filtered-mix versions, it is still more difficult for them to detect single sentences generated by ChatGPT. This is even more obvious for the Chinese corpus, where the F1-score of raw-mix trained on the Chinese corpus is $94.09 \%$ for testing raw sentence answers, compared to that $97.43 \%$ for testing raw full answers. Similar results can be observed for the filtered corpus, where F1-score of filtered-mix is $95.61 \%$ for testing filtered sentence answers, compared to its F1-score of $97.66 \%$ for testing filtered full answers. One possible explanation is that the expression pattern of ChatGPT is more obvious (therefore more easily detected) when paragraphs of text are provided, whereas it is more difficult to detect generated single sentences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">English</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chinese</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test $\rightarrow$ <br> Train $\downarrow$</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">raw <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">filtered <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">raw <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">filtered <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">full-raw</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">81.89</td>
<td style="text-align: center;">84.67</td>
<td style="text-align: center;">99.72</td>
<td style="text-align: center;">81.00</td>
<td style="text-align: center;">84.07</td>
<td style="text-align: center;">88.53</td>
<td style="text-align: center;">98.79</td>
<td style="text-align: center;">83.64</td>
<td style="text-align: center;">86.32</td>
<td style="text-align: center;">98.57</td>
<td style="text-align: center;">82.77</td>
<td style="text-align: center;">85.85</td>
<td style="text-align: center;">89.32</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">sent-raw</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">98.43</td>
<td style="text-align: center;">98.56</td>
<td style="text-align: center;">99.24</td>
<td style="text-align: center;">98.47</td>
<td style="text-align: center;">98.59</td>
<td style="text-align: center;">98.78</td>
<td style="text-align: center;">97.76</td>
<td style="text-align: center;">95.75</td>
<td style="text-align: center;">96.11</td>
<td style="text-align: center;">97.68</td>
<td style="text-align: center;">95.31</td>
<td style="text-align: center;">95.77</td>
<td style="text-align: center;">96.40</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mix-raw</td>
<td style="text-align: center;">99.44</td>
<td style="text-align: center;">98.31</td>
<td style="text-align: center;">98.47</td>
<td style="text-align: center;">99.32</td>
<td style="text-align: center;">98.37</td>
<td style="text-align: center;">98.51</td>
<td style="text-align: center;">98.74</td>
<td style="text-align: center;">97.70</td>
<td style="text-align: center;">95.68</td>
<td style="text-align: center;">96.04</td>
<td style="text-align: center;">97.65</td>
<td style="text-align: center;">95.27</td>
<td style="text-align: center;">95.73</td>
<td style="text-align: center;">96.35</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">full-filtered</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">87.17</td>
<td style="text-align: center;">89.05</td>
<td style="text-align: center;">99.79</td>
<td style="text-align: center;">86.60</td>
<td style="text-align: center;">88.67</td>
<td style="text-align: center;">91.85</td>
<td style="text-align: center;">98.25</td>
<td style="text-align: center;">91.04</td>
<td style="text-align: center;">92.30</td>
<td style="text-align: center;">98.14</td>
<td style="text-align: center;">91.15</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">93.89</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">sent-filtered</td>
<td style="text-align: center;">96.97</td>
<td style="text-align: center;">97.22</td>
<td style="text-align: center;">97.19</td>
<td style="text-align: center;">99.09</td>
<td style="text-align: center;">98.43</td>
<td style="text-align: center;">98.53</td>
<td style="text-align: center;">97.91</td>
<td style="text-align: center;">96.60</td>
<td style="text-align: center;">92.81</td>
<td style="text-align: center;">93.47</td>
<td style="text-align: center;">97.94</td>
<td style="text-align: center;">95.86</td>
<td style="text-align: center;">96.26</td>
<td style="text-align: center;">95.49</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mix-filtered</td>
<td style="text-align: center;">96.28</td>
<td style="text-align: center;">96.43</td>
<td style="text-align: center;">96.41</td>
<td style="text-align: center;">99.45</td>
<td style="text-align: center;">98.37</td>
<td style="text-align: center;">98.53</td>
<td style="text-align: center;">97.58</td>
<td style="text-align: center;">97.43</td>
<td style="text-align: center;">94.09</td>
<td style="text-align: center;">94.68</td>
<td style="text-align: center;">97.66</td>
<td style="text-align: center;">95.61</td>
<td style="text-align: center;">96.01</td>
<td style="text-align: center;">95.91</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: F1 scores (\%) of RoBERTa models at full \&amp; sent \&amp; mix mode.</p>
<h1>5.4.4 Which corpus is more helpful for model training? Full-text, sentence, or mix of the two?</h1>
<p>We find that both English and Chinese RoBERTa-based detectors are more robust when finegrained corpus data is available in model training. The sentence-based detectors outperform full-based detectors w.r.t. F1-scores, while the latter can be significantly improved when the sentence corpus is injected in model training, as we observe that mix-based detectors also achieve satisfactory results. For English corpus, raw-full only achieves $81.89 \%$ F1-score for testing sentence answers, while raw-sent is significantly better with $98.43 \%$ F1-score, as shown in Table 5. Moreover, the relatively inferior detection performance can be improved by injecting sentence answers into the detector, where we find that raw-mix can also obtain significant improvement (with $98.31 \%$ F1-score) over the detectors trained on only full answers. Similar conclusions can be acquired for the filtered versions, where both filtered-sent and filtered-mix significantly outperform filtered-full version w.r.t. F1-score, which holds for both English and Chinese corpus.
We indicate that the above conclusions could also hold for other types of detectors like GLTR Test-2 feature-based detectors, as is shown in Table 4. For GLTR Test-2, the average performance of F1score of raw-full and filtered-full is $61.74 \%$ and $69.47 \%$, respectively, compared to that of raw-sent $76.26 \%$ and filtered-sent $76.41 \%$, where the performance of detectors trained on the mixed corpus is close to the sentence-based versions.
Taking into account the conclusions of the previous paragraph about the detection difficulty between full and sentence answers, we indicate that the fine-grained corpus is helpful for distinguishing ChatGPT generated texts, as it additionally provides guidance and hints in model training for detecting the subtle patterns of ChatGPT hidden in single sentences.</p>
<h3>5.4.5 Will a QA-style detector be more effective than a single-text detector?</h3>
<p>Table 6 demonstrates the results of both raw-full and filtered-full models across all test datasets.
On English datasets, the QA model's F1-scores are superior to that of the single model, except for two full test datasets, where it averages $97.48 \%$ F1-scores and surpasses single model by $5.63 \%$. There exist some differences in Chinese datasets, where the single model outperforms QA in raw-full train dataset. However, the QA model still yields the best evaluation at $94.22 \%$.
In conclusion, the QA model is generally more effective than the single model and is suitable for filtered scenarios. And the QA training makes models more robust to the sentence inputs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">English</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chinese</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test $\rightarrow$</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">raw <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">filtered <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">raw <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">filtered <br> sent</td>
<td style="text-align: center;">mix</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train $\rightarrow$ raw - full</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">81.89</td>
<td style="text-align: center;">84.67</td>
<td style="text-align: center;">99.72</td>
<td style="text-align: center;">81.00</td>
<td style="text-align: center;">84.07</td>
<td style="text-align: center;">88.53</td>
<td style="text-align: center;">98.79</td>
<td style="text-align: center;">83.64</td>
<td style="text-align: center;">86.32</td>
<td style="text-align: center;">98.57</td>
<td style="text-align: center;">82.77</td>
<td style="text-align: center;">85.85</td>
<td style="text-align: center;">89.32</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">99.84</td>
<td style="text-align: center;">92.68</td>
<td style="text-align: center;">93.70</td>
<td style="text-align: center;">99.75</td>
<td style="text-align: center;">92.34</td>
<td style="text-align: center;">93.46</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">98.99</td>
<td style="text-align: center;">80.56</td>
<td style="text-align: center;">83.85</td>
<td style="text-align: center;">98.73</td>
<td style="text-align: center;">80.24</td>
<td style="text-align: center;">83.89</td>
<td style="text-align: center;">87.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train $\rightarrow$ filtered - full</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">99.82</td>
<td style="text-align: center;">87.17</td>
<td style="text-align: center;">89.05</td>
<td style="text-align: center;">99.79</td>
<td style="text-align: center;">86.60</td>
<td style="text-align: center;">88.67</td>
<td style="text-align: center;">91.85</td>
<td style="text-align: center;">98.25</td>
<td style="text-align: center;">91.04</td>
<td style="text-align: center;">92.30</td>
<td style="text-align: center;">98.14</td>
<td style="text-align: center;">91.15</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">93.89</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">99.70</td>
<td style="text-align: center;">96.14</td>
<td style="text-align: center;">96.64</td>
<td style="text-align: center;">99.70</td>
<td style="text-align: center;">96.07</td>
<td style="text-align: center;">96.61</td>
<td style="text-align: center;">97.48</td>
<td style="text-align: center;">97.29</td>
<td style="text-align: center;">92.10</td>
<td style="text-align: center;">93.01</td>
<td style="text-align: center;">97.18</td>
<td style="text-align: center;">92.40</td>
<td style="text-align: center;">93.31</td>
<td style="text-align: center;">94.22</td>
</tr>
</tbody>
</table>
<p>Table 6: F1 scores (\%) of RoBERTa models trained with QA \&amp; Single settings.</p>
<h3>5.4.6 Which data sources are more difficult for the ChatGPT detectors? and What are the conditions that make it easier to detect ChatGPT?</h3>
<p>As shown in Table 7, the evaluation results based on filtered-full model are separated by various sources in our HC3 dataset.</p>
<p>On the English datasets, the F1-scores for human answers are slightly higher than those for ChatGPT without any exceptions, regardless of whether RoBERTa or GLTR is used on full-text test datasets. However, the F1-scores for ChatGPT are highly inconsistent on transferring test datasets particularly open-qa dataset with varying performance. In terms of data resource, reddit-eli5 and finance-en has higher values, while wiki-csai poses a challenge for detectors.</p>
<p>On the Chinese datasets, the F1-scores of humans and ChatGPT are comparable with no significant difference. This suggests that the difficulty in detecting ChatGPT depends on the data source. It is observed that open-qa and baike have better performance, whereas the nlpcc-dbqa has lower performance.</p>
<p>Above all, the evaluations on Chinese dataset show more stability on transferring test dataset compared to the English datasets. Furthermore, it's evident that the F1-scores of ChatGPT are lower than those of human answers, regardless of whether the dataset is English or Chinese. This indicates that ChatGPT's detector relies more heavily on In-Distribution models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">F1-hu</th>
<th style="text-align: center;">F1-ch</th>
<th style="text-align: center;">F1-hu</th>
<th style="text-align: center;">F1-ch</th>
<th style="text-align: center;">F1-hu</th>
<th style="text-align: center;">F1-ch</th>
<th style="text-align: center;">F1-hu</th>
<th style="text-align: center;">F1-ch</th>
<th style="text-align: center;">F1-hu</th>
<th style="text-align: center;">F1-ch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">English</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">medicine</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">open_qa</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">reddit_eli5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">wiki_csai</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">99.34</td>
<td style="text-align: center;">99.28</td>
<td style="text-align: center;">99.69</td>
<td style="text-align: center;">99.62</td>
<td style="text-align: center;">99.53</td>
<td style="text-align: center;">98.60</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">96.59</td>
<td style="text-align: center;">96.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">78.84</td>
<td style="text-align: center;">85.84</td>
<td style="text-align: center;">84.06</td>
<td style="text-align: center;">80.45</td>
<td style="text-align: center;">70.74</td>
<td style="text-align: center;">26.78</td>
<td style="text-align: center;">77.27</td>
<td style="text-align: center;">93.31</td>
<td style="text-align: center;">68.91</td>
<td style="text-align: center;">84.12</td>
</tr>
<tr>
<td style="text-align: center;">GLTR</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">97.50</td>
<td style="text-align: center;">97.37</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">97.96</td>
<td style="text-align: center;">92.68</td>
<td style="text-align: center;">82.20</td>
<td style="text-align: center;">98.22</td>
<td style="text-align: center;">99.40</td>
<td style="text-align: center;">95.76</td>
<td style="text-align: center;">95.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">46.60</td>
<td style="text-align: center;">75.26</td>
<td style="text-align: center;">45.41</td>
<td style="text-align: center;">61.72</td>
<td style="text-align: center;">42.01</td>
<td style="text-align: center;">17.81</td>
<td style="text-align: center;">38.12</td>
<td style="text-align: center;">87.05</td>
<td style="text-align: center;">39.24</td>
<td style="text-align: center;">76.94</td>
</tr>
<tr>
<td style="text-align: center;">Chinese</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">finance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">law</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">open_qa</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">nlpcc_dbqa</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">baike</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">98.87</td>
<td style="text-align: center;">97.99</td>
<td style="text-align: center;">97.78</td>
<td style="text-align: center;">98.50</td>
<td style="text-align: center;">98.75</td>
<td style="text-align: center;">99.33</td>
<td style="text-align: center;">97.42</td>
<td style="text-align: center;">95.42</td>
<td style="text-align: center;">94.61</td>
<td style="text-align: center;">93.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">80.46</td>
<td style="text-align: center;">93.77</td>
<td style="text-align: center;">86.23</td>
<td style="text-align: center;">91.17</td>
<td style="text-align: center;">93.77</td>
<td style="text-align: center;">90.10</td>
<td style="text-align: center;">63.29</td>
<td style="text-align: center;">86.08</td>
<td style="text-align: center;">88.88</td>
</tr>
<tr>
<td style="text-align: center;">GLTR</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">86.67</td>
<td style="text-align: center;">80.42</td>
<td style="text-align: center;">82.41</td>
<td style="text-align: center;">88.89</td>
<td style="text-align: center;">85.75</td>
<td style="text-align: center;">93.15</td>
<td style="text-align: center;">77.25</td>
<td style="text-align: center;">69.78</td>
<td style="text-align: center;">81.62</td>
<td style="text-align: center;">77.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sent</td>
<td style="text-align: center;">36.91</td>
<td style="text-align: center;">32.80</td>
<td style="text-align: center;">33.99</td>
<td style="text-align: center;">46.22</td>
<td style="text-align: center;">36.45</td>
<td style="text-align: center;">75.21</td>
<td style="text-align: center;">46.39</td>
<td style="text-align: center;">27.50</td>
<td style="text-align: center;">48.10</td>
<td style="text-align: center;">71.72</td>
</tr>
</tbody>
</table>
<p>Table 7: Human (F1-hu) and ChatGPT (F1-ch) detection F1 scores (\%) w.r.t. different data source, models are trained on filtered full text, tested on filtered full and sent. On HC3-Chinese, we omitted the results of medicine and psychology domains, which are similar to finance and open_qa, respectively.</p>
<h1>6 Conclusion</h1>
<p>In this work, we propose the HC3 (Human ChatGPT Comparison Corpus) dataset, which consists of nearly 40 K questions and their corresponding human/ChatGPT answers. Based on the HC3 dataset, we conduct extensive studies including human evaluations, linguistic analysis, and content detection experiments. The human evaluations and linguistics analysis provide us insights into the implicit differences between humans and ChatGPT, which motivate our thoughts on LLMs' future directions. The ChatGPT content detection experiments illustrate some important conclusions that can provide beneficial guides to the research and development of AIGC-detection tools. We make all our data, code, and models publicly available to facilitate related research and applications at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.</p>
<h2>7 Limitations</h2>
<p>Despite our comprehensive analysis of ChatGPT, there are still several limitations in the current paper, which will be considered for improvement in our future work:</p>
<ol>
<li>Despite our efforts in data collection, the amount and range of collected data are still not enough and the data from different sources are unbalanced, due to limited time and resources. To make more accurate linguistic analyses and content detection, more data with different styles, sources, and languages are needed;</li>
<li>
<p>Currently, all the collected ChatGPT answers are generated without special prompts. Therefore, the analysis and conclusions in this paper are built upon ChatGPT's most general style/state. For example, using special prompts such as "Pretending you are Shakespeare..." can generate content that bypasses our detectors or make the conclusions in this paper untenable;</p>
</li>
<li>
<p>ChatGPT (perhaps) is mainly trained on English corpus while less on Chinese. Therefore, the conclusions drawn from the HC3-Chinese dataset may not always be precise.</p>
</li>
</ol>
<h1>Acknowledgments</h1>
<p>We would like to thank the volunteers that participated in our human evaluations, many of them are our good friends and dear family members. We would like to thank Junhui Zhu (BLCU-ICALL) for the valuable discussions on linguistic analysis. Biyang Guo would like to thank Prof. Hailiang Huang and Prof. Songqiao Han (AI Lab, SUFE) for providing insightful feedback on the topics and directions for this project. Xin Zhang would like to thank Yu Zhao (NeXt, NUS and CIC, TJU) for sharing the OpenAI account. Finally, we thank all team members of this project for their unique contributions. We together make this possible.</p>
<h1>References</h1>
<p>[1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.
[2] Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond. In Proceedings of the Language Resources and Evaluation Conference, pages 258-266, Marseille, France, June 2022. European Language Resources Association.
[3] Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc., 2009.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[6] Shu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang, Sicheng Wang, Yue Yang, Jiaqi Zeng, Ruisi Zhang, Ruoyu Zhang, Meng Zhou, Penghui Zhu, and Pengtao Xie. Meddialog: a large-scale medical dialogue dataset. arXiv preprint arXiv:2004.03329, 2020.
[7] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. neural information processing systems, 2017.
[8] Nan Duan. Overview of the nlpcc-iccpol 2016 shared task: Open domain chinese question answering. In Natural Language Understanding and Intelligent Applications, pages 942-948, Cham, 2016. Springer International Publishing.
[9] Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: About detecting deepfake tweets. Plos one, 16(5):e0251415, 2021.
[10] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3558-3567. Association for Computational Linguistics, 2019.
[11] Zhihui Fang. The language demands of science reading in middle school. International journal of science education, 28(5):491-520, 2006.
[12] Yao Fu, Hao Peng, and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion, Dec 2022.
[13] Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111-116, Florence, Italy, July 2019. Association for Computational Linguistics.
[14] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational principles for language processing in humans and deep language models. Nature neuroscience, 25(3):369-380, 2022.
[15] Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang, Nan Duan, and Weizhu Chen. Genius: Sketch-based language model pre-training via extreme and selective masking for text generation and augmentation. arXiv preprint arXiv:2211.10330, 2022.
[16] Biyang Guo, Songqiao Han, and Hailiang Huang. Selective text augmentation with word roles for low-resource text classification. arXiv preprint arXiv:2209.01560, 2022.
[17] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly detection benchmark. Advances in Neural Information Processing Systems (NeurIPS), 2022.</p>
<p>[18] Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, and Edward Gibson. A fine-grained comparison of pragmatic language understanding in humans and language models. arXiv preprint arXiv:2212.06801, 2022.
[19] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. Automatic detection of machine generated text: A critical survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2296-2309, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.
[20] Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, et al. Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports. arXiv preprint arXiv:2212.14882, 2022.
[21] Michael R King. The future of ai in medicine: a perspective from a chatbot, 2022.
[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.
[23] Macedo Maia, Siegfried Handschuh, Andr'e Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www'18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of the The Web Conference 2018, WWW '18, page 1941-1942, Republic and Canton of Geneva, CHE, 2018. International World Wide Web Conferences Steering Committee.
[24] William Nagy and Dianna Townsend. Words as tools: Learning academic vocabulary as language acquisition. Reading research quarterly, 47(1):91-108, 2012.
[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[26] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.
[27] Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin Javed, and Bimal Viswanath. Deepfake text detection: Limitations and opportunities. In Proc. of IEEE S\&amp;P, 2023.
[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[29] Mary J Schleppegrell. The language of schooling: A functional linguistics perspective. Routledge, 2004.
[30] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203, 2019.
[31] SophonPlus. Chinesenlpcorpus. https://github.com/SophonPlus/ChineseNlpCorpus, 2019.
[32] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. neural information processing systems, 2020.
[33] Teo Susnjak. Chatgpt: The end of online exam integrity? arXiv preprint arXiv:2212.09292, 2022.
[34] Alan M Turing. Computing machinery and intelligence. In Parsing the turing test, pages 23-65. Springer, 2009.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>[36] Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. CoRR, abs/2209.02970, 2022.
[37] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics.
[38] Bright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019.
[39] Yi Yang, Scott Wen-tau Yih, and Chris Meek. Wikiqa: A challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. ACL - Association for Computational Linguistics, September 2015.
[40] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-and-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378-7385, 2019.</p>
<h1>A Appendix</h1>
<h2>A. 1 HC3 Dataset Splits Creation</h2>
<p>We create 5 and 7 splits for HC3 English and Chinese, respectively. Most of the data come from the publicly available Question-Answering (QA) datasets, where details are listed in the following. For these QA data, we directly input the questions to ChatGPT and collect at least one answer.</p>
<p>We also crawled some wiki concepts and explanations from Wikipedia and BaiduBaike, where explanations are treated as human expert answers and concepts are used to construct the questions, details ref to bellow paragraphs.</p>
<p>For HC3-English, we create five dataset splits:</p>
<ol>
<li>reddit_eli5. Sampled from the ELI5 dataset [10].</li>
<li>open_qa. Sampled from the WikiQA dataset [39].</li>
<li>wiki_csai. We collected the descriptions of hundreds of computer science-related concepts from Wikipedia ${ }^{13}$ as the human experts' answers to questions like "Please explain what is <concept>?"</li>
<li>medicine. Sampled from the Medical Dialog dataset [6].</li>
<li>finance. Sampled from the FiQA dataset [23], which is built by crawling StackExchange ${ }^{14}$ posts under the Investment topic.</li>
</ol>
<p>For HC3-Chinese, we create seven dataset splits:</p>
<ol>
<li>open_qa. Sampled from the WebTextQA and BaikeQA corpus in [38].</li>
<li>baike. We collected the descriptions of more than a thousand information science-related concepts from BaiduBaike ${ }^{15}$ as the human experts' answers to questions like "我有一个计算机相关的问题，请用中文回答，什么是<concept>"</li>
<li>nlpcc_dbqa. Sampled from the NLPCC-DBQA dataset [8].</li>
<li>medicine. Sampled from the Medical Dialog dataset [6].</li>
<li>
<p>finance. Sampled from the FinanceZhidao dataset [31].
<sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
</li>
<li>
<p>psychology Sampled from a public Chinese Psychological Question Answering Dataset ${ }^{16}$.</p>
</li>
<li>law. Sampled from the LegalQA dataset ${ }^{17}$.</li>
</ol>
<h1>A. 2 Additional Results</h1>
<p>Here we demonstrate the additional results of dependency relations for the Chinese corpus, as is shown in Figure 6. The conclusion is basically consistent with the main paper.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Top-30 dependency relations (upper) and corresponding dependency distances (lower) comparison between human and ChatGPT answers in the HC3-Chinese. Results are sorted by relations proportion of human answers.</p>
<p>Other detailed results, including vocabulary features, sentiment analyses, and dependency parsing results for each data source are all available at our project GitHub repository at https://github. com/Hello-SimpleAI/chatgpt-comparison-detection.</p>
<h2>A. 3 Human Evaluations Examples</h2>
<p>For evaluation examples of our human evaluations, please visit our project GitHub repository at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ https://aistudio.baidu.com/aistudio/datasetdetail/38489
${ }^{17}$ https://github.com/siatnlp/LegalQA&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
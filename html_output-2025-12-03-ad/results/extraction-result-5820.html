<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5820 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5820</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5820</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-9ba50f992ccd92f428503ea6246157260a26cd77</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ba50f992ccd92f428503ea6246157260a26cd77" target="_blank">Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively “good” prompts, and instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots.</p>
                <p><strong>Paper Abstract:</strong> Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively “good” prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models’ impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans’ use of task instructions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5820.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5820.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instructive_vs_Irrelevant_Templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of instructive versus irrelevant discrete prompt templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that LLM few-shot performance on NLI (RTE) is often indistinguishable when using instructive natural-language templates versus semantically irrelevant 'chitchat' templates; observed across models and scales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT, T5, T0 (3B/11B), T0++, GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235M; 770M/3B/11B; 11B; 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE / SuperGLUE RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary NLI: given a premise and hypothesis, predict entailment vs non-entailment (reporting validation accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompt templates wrapping {premise} and {hypothesis} with additional template text; categories: Instructive (natural task instruction) vs Irrelevant (premise + unrelated sentence + hypothesis). Evaluated in few-shot regimes k={4,8,16,32,64,128,256} and zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Instructive templates compared against Irrelevant templates (and other categories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: 'no practical difference' in learning speed/accuracy between instructive and irrelevant templates for most tested models; example ALBERT 32-shot median accuracy: instructive 70.22% vs irrelevant 72.92%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See example above; across models the two template categories produce statistically indistinguishable (or very small) differences at most shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example numeric: ALBERT at 32 shots: irrelevant - instructive = +2.70 percentage points (72.92% vs 70.22%); generally effect sizes are small and inconsistent across models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect (i.e., minimal practical effect; sometimes irrelevant slightly better).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that models do not use prompt wording as humans do; instead prompts may interact with model inductive biases and spurious/text-distributional features, enabling learning even from semantically irrelevant text. Instruction-tuned models can be robust to prompt semantics, and the model often relies more on target-word mappings and other surface features than on template meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5820.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5820.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Misleading_Templates_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of misleading-moderate and misleading-extreme templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of templates that instruct the model to perform related-but-wrong tasks (misleading-moderate) or unrelated tasks (misleading-extreme); results show mixed sensitivity depending on model and size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (3B, 11B), ALBERT (235M), T5 (3B, 11B), GPT-3 (davinci 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B; 11B; 235M; 3B/11B; 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary entailment classification using discrete prompt templates that are misleading (ask for paraphrase, sentiment, grammar, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompt templates labeled misleading-moderate (e.g., 'Can that be paraphrased as "{hypothesis}"?') or misleading-extreme (e.g., 'Is this a sports news? {hypothesis}'), evaluated in few-shot settings (k up to 256) and zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to instructive templates and to irrelevant/null templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed: no consistent global pattern — some models (T0 3B) show no practical difference between instructive and misleading-moderate but significantly worse performance with misleading-extreme from 8–128 shots; other models (ALBERT, T5 3B) sometimes perform better with misleading-extreme.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Per-model differences reported (see Table 2); e.g., T0 (3B) — misleading-extreme statistically worse than instructive for shots 8–128; ALBERT shows instructive > misleading-moderate significant across many shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (for some models misleading templates reduced performance, for others there was little difference or occasional improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that models exhibit idiosyncratic sensitivities: while they do not reliably interpret instructions like humans, they are somewhat sensitive to prompt-category-specific surface cues. Instruction tuning changes these sensitivities in complex ways, sometimes making models less sensitive to prompt semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>No consistent relation between misleading-moderate and misleading-extreme across models; models typically distinguish instructive from at least one misleading category but patterns are model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5820.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5820.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Null_Templates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance with null prompt formats (minimal or mask-only concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Null templates (concatenating premise and hypothesis with little/no additional text, or using [MASK] positions) generally yield much worse performance, though some null variants can learn nearly as fast after enough shots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT (235M), other encoder-only MLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235M (ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary NLI where input formatting uses minimal templates (e.g., '{premise} {hypothesis}' or '{premise} [mask] {hypothesis}').</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Null templates: concatenation with no instruction text; also MLM-specific nulls placing mask token between premise and hypothesis. Evaluated in few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to instructive, irrelevant, and misleading templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Null templates perform far worse in aggregate (e.g., ALBERT 32-shot median: null 63.18% vs instructive 70.22%). However, some null variants (e.g., '{premise} [mask] {hypothesis}') converge close to instructive templates after ~32 shots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Example effect size at 32 shots for ALBERT: instructive 70.22% vs null 63.18% (~ -7.04 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-7.04 percentage points (ALBERT, 32-shot example); aggregate nulls consistently lower than instructive/irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (null template formatting generally reduces performance).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that some punctuation choices and exact ordering matter; Schick & Schütze-style punctuated 'null' prompts perform much better than vanilla concatenation, indicating that surface formatting cues (punctuation, mask placement) strongly affect MLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some null templates (MLM mask in the middle) learn nearly as fast as instructive templates after ~32 shots, showing not all nulls are equally bad.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5820.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5820.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Target_Words_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of LM target words (label token choices) on few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Choice of target words (the single-token words mapped to labels) strongly affects few-shot learning speed, often more than template semantics; yes/no targets outperform semantically equivalent alternatives and arbitrary/reversed pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT (235M), T0 (3B, 11B), T5 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235M; 3B/11B; 770M/3B/11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Label prediction via rank classification of LM target words (single-token targets mapped to entailment/non-entailment).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrete prompts with fixed target word pairs; categories: yes-no ("yes"/"no"), yes-no-like ("true"/"false", "agree"/"disagree", etc.), arbitrary ("cat"/"dog"), reversed (label meanings flipped). Evaluated across shots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared target-word categories holding template constant; also cross-comparison of best combinations (e.g., irrelevant+yes-no vs instructive+arbitrary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Yes/no targets substantially speed learning vs yes-no-like and dramatically vs arbitrary/reversed. Example: difference between 'yes'/'no' vs 'no'/'yes' at 32 shots = 22.2 percentage points in median accuracy (reported in paper). Models with arbitrary targets often fail to reach 60% median accuracy even by 64 shots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>'Irrelevant template + yes/no targets' often outperforms 'Instructive template + arbitrary targets' by large margins (figures show dramatic wins for yes/no combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>32-shot difference yes/no vs reversed example: 22.2 percentage points (median) reported; arbitrary targets cause large negative effects (often >>10 points).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>strong effect; yes/no targets improve performance dramatically, arbitrary/reversed targets reduce it.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors conclude models are highly sensitive to the surface form of label tokens (tokenization and distributional priors), which can override prompt semantics; plausible causes include pretraining biases and token frequency/surface-form artifacts. Attempts to help models (e.g., appending 'True or false?') sometimes hurt, indicating complex interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Although yes/no-like words are semantically similar, they perform worse (e.g., 'agree'/'disagree' < 'yes'/'no'), showing semantic equivalence is insufficient; token-level differences matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5820.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5820.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Punctuation_Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of punctuation (quotation marks and question marks) in templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation showing that adding/removing quotation marks and question marks in templates changes model performance, especially for irrelevant templates and depending on model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT (235M), T0 (3B), T5 LM-Adapted (3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235M; 3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt templates with or without quotation marks and question marks around the hypothesis phrase; evaluated in few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Ablation comparing templates with qmarks ("{hypothesis}"?) vs without qmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>With qmarks vs without qmarks for both instructive and irrelevant template sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For irrelevant templates, removing qmarks substantially degrades performance for ALBERT and T0; for T5 there is no significant difference for irrelevant templates and removing qmarks from instructive templates actually improved T5 at some shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative differences documented in Figures 7–9: 'irrelevant sans qmarks' significantly worse than 'irrelevant' for ALBERT and T0; T5 differs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>substantial effect for some models (reduced when qmarks removed), model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest punctuation interacts with spurious distributional cues and model inductive biases; punctuation can help disambiguate or induce desired tokenization/formatting patterns, thereby affecting performance. The interaction between punctuation and template semantics is complex and model-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>T5 did not follow the same pattern (qmarks had no effect on irrelevant templates and removal improved some instructive templates), so punctuation effects are not uniform across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5820.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5820.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_Priming_vs_Format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constraints and behavior of GPT-3 (davinci 175B) under priming and prompt-format variations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 experiments used in-context priming (not fine-tuning) due to API limits; performance patterns broadly mirrored other models: template category often had little effect except for null templates, and zero-shot performance depended heavily on exact prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Priming (in-context learning) with k examples prepended to the evaluation example, using same discrete template variants as other experiments; evaluated at up to 16-shot due to token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Priming (in-context examples) with discrete prompt templates; rank classification of target words using model logprobs; zero-shot and up to 16-shot due to token limit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared prompt categories (instructive, irrelevant, misleading, null) under priming; also compared specific prompt from Brown et al. (2020) to other prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: GPT-3 performed marginally above random for most prompts except the specific prompt reported by Brown et al.; 16-shot: GPT-3 shows no practical difference between template categories except null templates, which are significantly worse. Exact numeric 16-shot values not reproducible due to token/context limits; Table 2 records significant difference only for instructive > null at 16 shots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Specific Brown et al. prompt reproduced Brown et al.'s zero-shot RTE result, while other instructive prompts did not; priming versus fine-tuning was constrained by API (fine-tuning impractical).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mostly no effect across template categories under priming (except null templates reduce performance); zero-shot is highly sensitive to exact prompt wording for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note GPT-3's behavior may be prompt-cherrypicked in prior reports; priming setup differs from gradient-updated few-shot evaluation (priming sees different k examples per inference), and tokenization/logprob artifacts make target-word surface form critical. API and token-limit constraints shape observed effects.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Although Brown et al.'s exact prompt gives good zero-shot results, most other prompts do not — showing that exact prompt wording can be an outlier and that large-scale priming performance is fragile.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5820.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5820.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructionTuning_Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of instruction-tuning (T0 / T0++) on prompt-semantic sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned models (T0 family) are overall more robust across prompt variations and often perform well zero-shot, but can also become less sensitive to prompt semantics, sometimes producing strong performance even with pathological prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (3B, 11B), T0++ (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B; 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Language Inference (RTE) and held-out NLI prompts/datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot and few-shot evaluation on RTE using many manually written prompts; T0 family was trained on many datasets with hundreds of prompts and held out NLI prompts/datasets for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction-tuned models evaluated with the same discrete prompt categories (instructive, misleading, irrelevant, null) in zero-shot and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared T0/T0++ to non-instruction-tuned variants (e.g., T5 LM-Adapted) and across prompt categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>T0 improves zero-shot and few-shot robustness; T0++ (trained on more datasets) shows statistically significant differentiation across prompt categories at zero-shot and achieves high zero-shot accuracy in some pathological prompts (example: T0++ zero-shot accuracy 78% on the extremely misleading template 'Is that grammatically correct? {hypothesis}', matching performance on proper instructive prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Instruction tuning improves overall performance and variance robustness compared to non-instruction-tuned equivalents; however, it also sometimes reduces sensitivity to whether a prompt is semantically correct.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed: instruction tuning generally improves performance (robustness), but can reduce sensitivity to prompt semantics (pathological prompts still work).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Training on many datasets and prompts makes models robust to superficial prompt variations; as a result, they may learn to map surface patterns to tasks without relying on natural-language instruction meaning, producing 'illusion of instruction following.' Authors caution this robustness can mask lack of true instruction understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Although T0++ shows more sensitivity to prompt semantics than smaller T0 variants in zero-shot, many pathological prompts still achieve comparable performance, indicating instruction tuning does not fully solve the issue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Prompt-Based Models Really Understand the Meaning of Their Prompts?', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>It's not just size that matters: Small language models are also few-shot learners <em>(Rating: 2)</em></li>
                <li>How many data points is a prompt worth? <em>(Rating: 2)</em></li>
                <li>Natural instructions: Benchmarking generalization to new tasks from natural language instructions <em>(Rating: 1)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5820",
    "paper_id": "paper-9ba50f992ccd92f428503ea6246157260a26cd77",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Instructive_vs_Irrelevant_Templates",
            "name_full": "Comparison of instructive versus irrelevant discrete prompt templates",
            "brief_description": "Empirical finding that LLM few-shot performance on NLI (RTE) is often indistinguishable when using instructive natural-language templates versus semantically irrelevant 'chitchat' templates; observed across models and scales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ALBERT, T5, T0 (3B/11B), T0++, GPT-3 (davinci)",
            "model_size": "235M; 770M/3B/11B; 11B; 175B",
            "task_name": "Natural Language Inference (RTE / SuperGLUE RTE)",
            "task_description": "Binary NLI: given a premise and hypothesis, predict entailment vs non-entailment (reporting validation accuracy).",
            "problem_format": "Discrete prompt templates wrapping {premise} and {hypothesis} with additional template text; categories: Instructive (natural task instruction) vs Irrelevant (premise + unrelated sentence + hypothesis). Evaluated in few-shot regimes k={4,8,16,32,64,128,256} and zero-shot.",
            "comparison_format": "Instructive templates compared against Irrelevant templates (and other categories).",
            "performance": "Qualitative: 'no practical difference' in learning speed/accuracy between instructive and irrelevant templates for most tested models; example ALBERT 32-shot median accuracy: instructive 70.22% vs irrelevant 72.92%.",
            "performance_comparison": "See example above; across models the two template categories produce statistically indistinguishable (or very small) differences at most shot counts.",
            "format_effect_size": "Example numeric: ALBERT at 32 shots: irrelevant - instructive = +2.70 percentage points (72.92% vs 70.22%); generally effect sizes are small and inconsistent across models.",
            "format_effect_direction": "no effect (i.e., minimal practical effect; sometimes irrelevant slightly better).",
            "explanation_or_hypothesis": "Authors hypothesize that models do not use prompt wording as humans do; instead prompts may interact with model inductive biases and spurious/text-distributional features, enabling learning even from semantically irrelevant text. Instruction-tuned models can be robust to prompt semantics, and the model often relies more on target-word mappings and other surface features than on template meaning.",
            "counterexample_or_null_result": null,
            "uuid": "e5820.0",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Misleading_Templates_Effect",
            "name_full": "Effect of misleading-moderate and misleading-extreme templates",
            "brief_description": "Evaluation of templates that instruct the model to perform related-but-wrong tasks (misleading-moderate) or unrelated tasks (misleading-extreme); results show mixed sensitivity depending on model and size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (3B, 11B), ALBERT (235M), T5 (3B, 11B), GPT-3 (davinci 175B)",
            "model_size": "3B; 11B; 235M; 3B/11B; 175B",
            "task_name": "Natural Language Inference (RTE)",
            "task_description": "Binary entailment classification using discrete prompt templates that are misleading (ask for paraphrase, sentiment, grammar, etc.).",
            "problem_format": "Discrete prompt templates labeled misleading-moderate (e.g., 'Can that be paraphrased as \"{hypothesis}\"?') or misleading-extreme (e.g., 'Is this a sports news? {hypothesis}'), evaluated in few-shot settings (k up to 256) and zero-shot.",
            "comparison_format": "Compared to instructive templates and to irrelevant/null templates.",
            "performance": "Mixed: no consistent global pattern — some models (T0 3B) show no practical difference between instructive and misleading-moderate but significantly worse performance with misleading-extreme from 8–128 shots; other models (ALBERT, T5 3B) sometimes perform better with misleading-extreme.",
            "performance_comparison": "Per-model differences reported (see Table 2); e.g., T0 (3B) — misleading-extreme statistically worse than instructive for shots 8–128; ALBERT shows instructive &gt; misleading-moderate significant across many shot counts.",
            "format_effect_size": null,
            "format_effect_direction": "mixed (for some models misleading templates reduced performance, for others there was little difference or occasional improvement).",
            "explanation_or_hypothesis": "Authors suggest that models exhibit idiosyncratic sensitivities: while they do not reliably interpret instructions like humans, they are somewhat sensitive to prompt-category-specific surface cues. Instruction tuning changes these sensitivities in complex ways, sometimes making models less sensitive to prompt semantics.",
            "counterexample_or_null_result": "No consistent relation between misleading-moderate and misleading-extreme across models; models typically distinguish instructive from at least one misleading category but patterns are model-dependent.",
            "uuid": "e5820.1",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Null_Templates",
            "name_full": "Performance with null prompt formats (minimal or mask-only concatenation)",
            "brief_description": "Null templates (concatenating premise and hypothesis with little/no additional text, or using [MASK] positions) generally yield much worse performance, though some null variants can learn nearly as fast after enough shots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ALBERT (235M), other encoder-only MLMs",
            "model_size": "235M (ALBERT)",
            "task_name": "Natural Language Inference (RTE)",
            "task_description": "Binary NLI where input formatting uses minimal templates (e.g., '{premise} {hypothesis}' or '{premise} [mask] {hypothesis}').",
            "problem_format": "Null templates: concatenation with no instruction text; also MLM-specific nulls placing mask token between premise and hypothesis. Evaluated in few-shot.",
            "comparison_format": "Compared to instructive, irrelevant, and misleading templates.",
            "performance": "Null templates perform far worse in aggregate (e.g., ALBERT 32-shot median: null 63.18% vs instructive 70.22%). However, some null variants (e.g., '{premise} [mask] {hypothesis}') converge close to instructive templates after ~32 shots.",
            "performance_comparison": "Example effect size at 32 shots for ALBERT: instructive 70.22% vs null 63.18% (~ -7.04 percentage points).",
            "format_effect_size": "-7.04 percentage points (ALBERT, 32-shot example); aggregate nulls consistently lower than instructive/irrelevant.",
            "format_effect_direction": "reduced (null template formatting generally reduces performance).",
            "explanation_or_hypothesis": "Authors note that some punctuation choices and exact ordering matter; Schick & Schütze-style punctuated 'null' prompts perform much better than vanilla concatenation, indicating that surface formatting cues (punctuation, mask placement) strongly affect MLM behavior.",
            "counterexample_or_null_result": "Some null templates (MLM mask in the middle) learn nearly as fast as instructive templates after ~32 shots, showing not all nulls are equally bad.",
            "uuid": "e5820.2",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Target_Words_Effect",
            "name_full": "Effect of LM target words (label token choices) on few-shot learning",
            "brief_description": "Choice of target words (the single-token words mapped to labels) strongly affects few-shot learning speed, often more than template semantics; yes/no targets outperform semantically equivalent alternatives and arbitrary/reversed pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ALBERT (235M), T0 (3B, 11B), T5 variants",
            "model_size": "235M; 3B/11B; 770M/3B/11B",
            "task_name": "Natural Language Inference (RTE)",
            "task_description": "Label prediction via rank classification of LM target words (single-token targets mapped to entailment/non-entailment).",
            "problem_format": "Discrete prompts with fixed target word pairs; categories: yes-no (\"yes\"/\"no\"), yes-no-like (\"true\"/\"false\", \"agree\"/\"disagree\", etc.), arbitrary (\"cat\"/\"dog\"), reversed (label meanings flipped). Evaluated across shots.",
            "comparison_format": "Compared target-word categories holding template constant; also cross-comparison of best combinations (e.g., irrelevant+yes-no vs instructive+arbitrary).",
            "performance": "Yes/no targets substantially speed learning vs yes-no-like and dramatically vs arbitrary/reversed. Example: difference between 'yes'/'no' vs 'no'/'yes' at 32 shots = 22.2 percentage points in median accuracy (reported in paper). Models with arbitrary targets often fail to reach 60% median accuracy even by 64 shots.",
            "performance_comparison": "'Irrelevant template + yes/no targets' often outperforms 'Instructive template + arbitrary targets' by large margins (figures show dramatic wins for yes/no combinations).",
            "format_effect_size": "32-shot difference yes/no vs reversed example: 22.2 percentage points (median) reported; arbitrary targets cause large negative effects (often &gt;&gt;10 points).",
            "format_effect_direction": "strong effect; yes/no targets improve performance dramatically, arbitrary/reversed targets reduce it.",
            "explanation_or_hypothesis": "Authors conclude models are highly sensitive to the surface form of label tokens (tokenization and distributional priors), which can override prompt semantics; plausible causes include pretraining biases and token frequency/surface-form artifacts. Attempts to help models (e.g., appending 'True or false?') sometimes hurt, indicating complex interactions.",
            "counterexample_or_null_result": "Although yes/no-like words are semantically similar, they perform worse (e.g., 'agree'/'disagree' &lt; 'yes'/'no'), showing semantic equivalence is insufficient; token-level differences matter.",
            "uuid": "e5820.3",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Punctuation_Ablation",
            "name_full": "Effect of punctuation (quotation marks and question marks) in templates",
            "brief_description": "Ablation showing that adding/removing quotation marks and question marks in templates changes model performance, especially for irrelevant templates and depending on model architecture.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ALBERT (235M), T0 (3B), T5 LM-Adapted (3B)",
            "model_size": "235M; 3B",
            "task_name": "Natural Language Inference (RTE)",
            "task_description": "Prompt templates with or without quotation marks and question marks around the hypothesis phrase; evaluated in few-shot.",
            "problem_format": "Ablation comparing templates with qmarks (\"{hypothesis}\"?) vs without qmarks.",
            "comparison_format": "With qmarks vs without qmarks for both instructive and irrelevant template sets.",
            "performance": "For irrelevant templates, removing qmarks substantially degrades performance for ALBERT and T0; for T5 there is no significant difference for irrelevant templates and removing qmarks from instructive templates actually improved T5 at some shot counts.",
            "performance_comparison": "Qualitative differences documented in Figures 7–9: 'irrelevant sans qmarks' significantly worse than 'irrelevant' for ALBERT and T0; T5 differs.",
            "format_effect_size": null,
            "format_effect_direction": "substantial effect for some models (reduced when qmarks removed), model-dependent.",
            "explanation_or_hypothesis": "Authors suggest punctuation interacts with spurious distributional cues and model inductive biases; punctuation can help disambiguate or induce desired tokenization/formatting patterns, thereby affecting performance. The interaction between punctuation and template semantics is complex and model-specific.",
            "counterexample_or_null_result": "T5 did not follow the same pattern (qmarks had no effect on irrelevant templates and removal improved some instructive templates), so punctuation effects are not uniform across architectures.",
            "uuid": "e5820.4",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "GPT-3_Priming_vs_Format",
            "name_full": "Constraints and behavior of GPT-3 (davinci 175B) under priming and prompt-format variations",
            "brief_description": "GPT-3 experiments used in-context priming (not fine-tuning) due to API limits; performance patterns broadly mirrored other models: template category often had little effect except for null templates, and zero-shot performance depended heavily on exact prompt wording.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci)",
            "model_size": "175B",
            "task_name": "Natural Language Inference (RTE)",
            "task_description": "Priming (in-context learning) with k examples prepended to the evaluation example, using same discrete template variants as other experiments; evaluated at up to 16-shot due to token limits.",
            "problem_format": "Priming (in-context examples) with discrete prompt templates; rank classification of target words using model logprobs; zero-shot and up to 16-shot due to token limit.",
            "comparison_format": "Compared prompt categories (instructive, irrelevant, misleading, null) under priming; also compared specific prompt from Brown et al. (2020) to other prompts.",
            "performance": "Zero-shot: GPT-3 performed marginally above random for most prompts except the specific prompt reported by Brown et al.; 16-shot: GPT-3 shows no practical difference between template categories except null templates, which are significantly worse. Exact numeric 16-shot values not reproducible due to token/context limits; Table 2 records significant difference only for instructive &gt; null at 16 shots.",
            "performance_comparison": "Specific Brown et al. prompt reproduced Brown et al.'s zero-shot RTE result, while other instructive prompts did not; priming versus fine-tuning was constrained by API (fine-tuning impractical).",
            "format_effect_size": null,
            "format_effect_direction": "mostly no effect across template categories under priming (except null templates reduce performance); zero-shot is highly sensitive to exact prompt wording for GPT-3.",
            "explanation_or_hypothesis": "Authors note GPT-3's behavior may be prompt-cherrypicked in prior reports; priming setup differs from gradient-updated few-shot evaluation (priming sees different k examples per inference), and tokenization/logprob artifacts make target-word surface form critical. API and token-limit constraints shape observed effects.",
            "counterexample_or_null_result": "Although Brown et al.'s exact prompt gives good zero-shot results, most other prompts do not — showing that exact prompt wording can be an outlier and that large-scale priming performance is fragile.",
            "uuid": "e5820.5",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "InstructionTuning_Robustness",
            "name_full": "Effect of instruction-tuning (T0 / T0++) on prompt-semantic sensitivity",
            "brief_description": "Instruction-tuned models (T0 family) are overall more robust across prompt variations and often perform well zero-shot, but can also become less sensitive to prompt semantics, sometimes producing strong performance even with pathological prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (3B, 11B), T0++ (11B)",
            "model_size": "3B; 11B",
            "task_name": "Natural Language Inference (RTE) and held-out NLI prompts/datasets",
            "task_description": "Zero-shot and few-shot evaluation on RTE using many manually written prompts; T0 family was trained on many datasets with hundreds of prompts and held out NLI prompts/datasets for fairness.",
            "problem_format": "Instruction-tuned models evaluated with the same discrete prompt categories (instructive, misleading, irrelevant, null) in zero-shot and few-shot.",
            "comparison_format": "Compared T0/T0++ to non-instruction-tuned variants (e.g., T5 LM-Adapted) and across prompt categories.",
            "performance": "T0 improves zero-shot and few-shot robustness; T0++ (trained on more datasets) shows statistically significant differentiation across prompt categories at zero-shot and achieves high zero-shot accuracy in some pathological prompts (example: T0++ zero-shot accuracy 78% on the extremely misleading template 'Is that grammatically correct? {hypothesis}', matching performance on proper instructive prompt).",
            "performance_comparison": "Instruction tuning improves overall performance and variance robustness compared to non-instruction-tuned equivalents; however, it also sometimes reduces sensitivity to whether a prompt is semantically correct.",
            "format_effect_size": null,
            "format_effect_direction": "mixed: instruction tuning generally improves performance (robustness), but can reduce sensitivity to prompt semantics (pathological prompts still work).",
            "explanation_or_hypothesis": "Training on many datasets and prompts makes models robust to superficial prompt variations; as a result, they may learn to map surface patterns to tasks without relying on natural-language instruction meaning, producing 'illusion of instruction following.' Authors caution this robustness can mask lack of true instruction understanding.",
            "counterexample_or_null_result": "Although T0++ shows more sensitivity to prompt semantics than smaller T0 variants in zero-shot, many pathological prompts still achieve comparable performance, indicating instruction tuning does not fully solve the issue.",
            "uuid": "e5820.6",
            "source_info": {
                "paper_title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "It's not just size that matters: Small language models are also few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "How many data points is a prompt worth?",
            "rating": 2
        },
        {
            "paper_title": "Natural instructions: Benchmarking generalization to new tasks from natural language instructions",
            "rating": 1
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 1
        }
    ],
    "cost": 0.0170555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</h1>
<p>Albert Webson ${ }^{1,2}$ and Ellie Pavlick ${ }^{1}$<br>{albert_webson, ellie_pavlick}@brown.edu<br>${ }^{1}$ Department of Computer Science, Brown University<br>${ }^{2}$ Department of Philosophy, Brown University</p>
<h4>Abstract</h4>
<p>Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively "good" prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.</p>
<h2>1 Introduction</h2>
<p>Suppose a human is given two sentences: "No weapons of mass destruction found in Iraq yet." and "Weapons of mass destruction found in Iraq." They are then asked to respond 0 or 1 and receive a reward if they are correct. In this setup, they would likely need a large number of trials and errors before figuring out what they are really being rewarded to do. This setup is akin to the pretrain-and-fine-tune setup which has dominated NLP in recent years, in which models are asked to classify a sentence representation (e.g., a CLS token) into some</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>arbitrary dimensions of a one-hot vector. In contrast, suppose a human is given a prompt such as: Given that "no weapons of mass destruction found in Iraq yet.", is it definitely correct that "weapons of mass destruction found in Iraq."? ${ }^{1}$ Then it would be no surprise that they are able to perform the task more accurately and without needing many examples to figure out what the task is.</p>
<p>Similarly, reformatting NLP tasks with prompts such as the underlined text above has dramatically improved zero-shot and few-shot performance over traditional fine-tuned models (Schick and Schütze, 2021b; Le Scao and Rush, 2021; Sanh et al., 2021; Wei et al., 2021). Such results naturally give rise to the hypothesis that the extra prompt text included within each input example serves as semantically meaningful task instructions which help models to learn faster, in the way task instructions help humans to learn faster. This hypothesis is implicitly assumed by many and explicitly argued by Mishra et al. (2021), Schick and Schütze (2021a), and Brown et al. (2020).</p>
<p>While last years saw a gold rush of papers (summarized in §2) that proposed automatic methods for optimizing prompts, Logan IV et al. (2021) compare a representative sample of these newly proposed methods and report that Schick and Schütze (2021b)'s manually written prompts still on average outperform the automatically searched prompts across a range of SuperGLUE tasks (Wang et al., 2019). Such findings suggest that expert-crafted prompts are among the best, if not the best, which reinforces the above hypothesis that models benefit from meaningful instructions.</p>
<p>In this paper, we test this hypothesis by evaluating various models on NLI in zero-shot and fewshot settings using more than 30 manually written templates and 13 sets of LM target words for a</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>total of over 390 prompts. We find that in most cases models learn identically as fast when given irrelevant or misleading templates as they do when given instructively good templates. Further, models ranging from 235 million to 175 billion parameters all exhibit this behavior, as do the instructiontuned models, which are trained on hundreds of manually written prompts. While we confirm Sanh et al. (2021)'s finding that instruction tuning substantially improves the performance and robustness of prompts, we also find that instruction-tuned models can be, in some sense, too robust and less sensitive to the semantics of the prompts, as compared to their non-instruction-tuned equivalents. Finally, models are much more sensitive to the choice of the LM target words as opposed to the meaning of the instruction templates. In sum, despite promptbased models' dramatic improvement in zero-shot and few-shot learning, we find limited evidence that models' improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.</p>
<h2>2 Related Work</h2>
<h3>2.1 Prompt-Based Models</h3>
<p>At the time of writing, the terms "prompt tuning" and "prompting" can refer to any one or combination of three approaches described below:</p>
<p>Discrete Prompts reformat each example with some template text. For example, in a sentiment analysis task, the template can be {sent} In summary, the restaurant is [prediction], where the predicted mask word is then converted to a class prediction by a predefined mapping, e.g., { "great" $\rightarrow$ positive, "terrible" $\rightarrow$ negative}. The prompts can be manually written (Schick and Schütze, 2021a; Bragg et al., 2021) or automatically generated (Gao et al., 2021b; Shin et al., 2020). This approach typically tunes all parameters of the model, but its few-shot performance can exceed that of very large models (e.g., GPT-3 175B) despite using a 3 orders of magnitude smaller LM (Schick and Schütze, 2021b; Tam et al., 2021).</p>
<p>Priming (a.k.a. in-context learning) prepends $k$ priming examples to the evaluation example, where each example is optionally wrapped in a template such as Question: {sent $\left.{ }<em 1="1">{1}\right}$ True or false? {label $\left.{ }</em>}\right}$... Question: {sent $\left.{ <em k="k">{k}\right}$ True or false? $\left{\right.$ label $\left.{ }</em>\right}$ Question: {eval_sent} True or false? [prediction]. Notably, although models see labeled examples, their parameters do not receive gradient updates based on those examples. Although this approach is intriguing, Brown et al. (2020) report that it only performs well on the largest GPT-3 model, the API of which is costly and difficult to use for academic research (see Appendix B for details).</p>
<p>Continuous Prompts prepend examples with special tokens, optionally initialized with word embeddings; but during learning, those tokens can be updated arbitrarily such that the final embeddings often do not correspond to any real word in the vocabulary (e.g., Lester et al., 2021; Li and Liang, 2021; Qin and Eisner, 2021). This approach often efficiently tunes a much smaller set of model parameters, but these methods have not yet reported success in few-shot settings. Moreover, foregoing prompts as expressed in natural language makes it much harder to study their semantics, and it is not clear if continuous prompts serve as task-specific instructions or simply more efficient model parameters (see He et al., 2021 for a detailed analysis).</p>
<h3>2.2 Analyses of Prompts</h3>
<p>In this paper, we focus on discrete prompts because we can manually write and control their wording and semantics. We measure the effect of prompt semantics by the model's $k$-shot performance where $k={0,4,8,16,32,64,128,256}$. This setup resembles that of Le Scao and Rush (2021), but their study focuses on comparing Schick and Schütze (2021b)'s existing small set of prompts against traditional fine-tuning over the training trajectories of entire training sets, whereas our study focuses on the few-shot learning trajectories among a much more diverse set of prompts designed to test specific hypotheses about the effect of prompt semantics on few-shot learning speed.</p>
<p>At a high-level, our findings contradict Mishra et al. (2021)'s claim that models benefit from elaborate instructions adapted from crowdsourcing annotation guides. But note that they define "instructions" more broadly as including priming examples, and they find that "GPT-3 benefits the most from positive examples, mildly from definition, and deteriorates with negative examples." (p. 18). In other words, if we ablate priming and narrow "instructions" to just the description of a task, we in fact have the same finding that instructions are only modestly beneficial over no instructions (cf. our</p>
<p>irrelevant templates). In a similar vein, concurrent work by Lampinen et al. (2022) finds that other components of a prompt such as explanations of priming examples are helpful, but models are indifferent to whether the instructions in fact describe their tasks.</p>
<p>Finally, a growing body of concurrent work also questions the degree to which models need meaningful instructions (Khashabi et al., 2021; Prasad et al., 2022). One particularly noteworthy finding is that Min et al. (2022) show that models learn just as well with incorrect labels as opposed to correct labels in priming, concluding that prompts are helping models to learn the distribution of the input text and space of possible labels (as opposed to specifying instructions of the task).</p>
<h2>3 Overall Setup</h2>
<p>We implement a manual discrete prompt modelwhich in essence is the same as that of Schick and Schütze (2021b), except their implementation includes several augmentations such as self-labeling and ensembling of multiple prompts for competitive results. In order to focus on measuring the effect of prompts themselves, our implementation does not include those augmentations. Following Sanh et al. (2021) and Wei et al. (2021), we evaluate by a rank classification of the target words.</p>
<p>Baseline Model In preliminary experiments, we fine-tuned and prompt-tuned BERT, DistilBERT, RoBERTa, ALBERT, and T5 (Devlin et al., 2019; Sanh et al., 2019; Liu et al., 2019; Lan et al., 2020; Raffel et al., 2020; all implemented via Wolf et al., 2020). Confirming prior work (Schick and Schütze, 2021b; Tam et al., 2021), we find that ALBERT consistently yields the best performance, so we use it as our baseline model.</p>
<p>To verify that our implementation is comparable with prior work, Figure 10 reports the RTE validation accuracy of our baseline model. At 32 shots, our implementation yields a median accuracy of $70.22 \%$ (mean $=69.29 \%$, std. dev. $=6.3 \%$ ), which is comparable to the $69.8 \%$ reported by Schick and Schütze (2021b). Further, Figure 10 confirms Le Scao and Rush (2021)'s finding that, while both fine-tuning and prompt-tuning converge to similar results when fully trained on the entire set ( $n=2490$ for RTE), prompt-tuning yields the largest improvement in the few-shot setting. Going forward, we focus on studying the few-shot learning trajectory between 4 and 256 examples.</p>
<p>Instruction-Tuned Model We additionally experiment with T0, a recently proposed instructiontuned model which is trained on over 60 datasets formatted with hundreds of manually written prompts (Sanh et al., 2021). We experiment with both sizes of T0 (3B and 11B), as well as their non-instruction-tuned version, T5 LM-Adapted (Lester et al., 2021), as a baseline.</p>
<p>Very Large Model Lastly, we experiment with the largest GPT-3 (175B) via priming (a.k.a. incontext learning). Although fine-tuning is technically available, it is extremely limited by OpenAI's various quotas. See Appendix B for details on how we circumvent challenges in reproducing Brown et al. (2020)'s results.</p>
<p>Data NLI is a task where a model is asked to classify whether one piece of text (the "premise") entails another (the "hypothesis"). We focus on NLI because all T0 variants holds out all NLI prompts and all NLI datasets in its training, which makes it a fair comparison to other models in this paper.</p>
<p>We use Recognizing Textual Entailment (RTE, Dagan et al., 2006, inter alios), a series of expertannotated NLI datasets. Specifically, we use the SuperGLUE collection of RTE (i.e., RTE1, 2, 3, and 5; all converted to binary classification) and report their validation accuracy for comparability with prior work on prompts.</p>
<p>We also experiment with Adversarial NLI (ANLI, Nie et al., 2020), Heuristic Analysis for NLI Systems (HANS, McCoy et al., 2019), and Winograd Schema Challenge (WSC, Levesque et al., 2012), reported in Appendices G.2, K, and L, respectively. We find no qualitative difference between their and the main RTE results except that ANLI requires much larger number of shots before obtaining any above-random accuracy, as it is designed to be a highly challenging set.</p>
<p>Random Seeds \&amp; Example Sampling All experiments are run over the same set of 4 random seeds. Within a given seed, all models see the same set of examples. For instance, under seed 1 , the 4 -shot models see examples $550-553$, the 8 -shot models see examples $550-557$, and so on. Across different seeds, a different starting example index is drawn. The exact training example indices are also recorded in our GitHub repository for reproducibility.</p>
<p>Statistical Tests We use both ANOVA and its nonparametric equivalent, the Kruskal-Wallis test. After finding a significant difference among multiple categories of templates, we report pairwise significance with the independent two-sample $t$-test and the Wilcoxon rank-sum test. We set $\alpha=0.05$ and apply the Bonferroni correction to account for multiple comparisons. For all results reported in this paper, both $t$-test and Wilcoxon agree.</p>
<h2>4 Effect of Templates</h2>
<p>Our research question is whether models understand prompts as meaningful task instructions analogous to how humans would. For intuition, suppose an experimenter provides a human annotator with an informative instruction of a reasonably easy task. If the annotator understands the instruction, we expect them to perform better than when the experimenter provides intentionally misleading instructions, makes irrelevant chitchat, or says nothing at all. Accordingly, we write various prompt templates that correspond to these different scenarios and evaluate models' performance with these templates in zero-shot and few-shot settings.</p>
<h3>4.1 Method</h3>
<p>We write 5 categories of templates (Table 1), with at least 5 templates for each category ( 10 for instructive):</p>
<ul>
<li>Instructive: how we would describe the NLI task to a human who has never seen this task before.</li>
<li>Misleading-Moderate: instruct the models to perform a task related or tangential to NLI such that, if the model were to perform the task as explicitly instructed, it would perform poorly on NLI in general. ${ }^{2}$</li>
<li>Misleading-Extreme: instruct the models to perform a task unrelated to NLI.</li>
<li>Irrelevant: concatenate the premise, a sentence unrelated to any NLP task, and the hypothesis.</li>
<li>Null: concatenate the premise and the hypothesis without any additional text.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Example templates for NLI.
See Table 1 for examples and Appendix F for the full list. We use "prompt" to mean a unique combination of a template and a predefined LM target word for each class label. For example, {"yes" $\rightarrow$ entailment, "no" $\rightarrow$ non-entailment} are the default targets for the template {premise} Should we assume that {hypothesis}? [prediction]. In this section, to control for the effect of target words, a template's performance is always reported with "yes"/"no" as its target words, which consistently perform best. In Section 5, we control for the templates and study the effect of different target words. We further control for punctuation, declarative vs. interrogative templates, and the order of concatenation (always {premise} some template text {hypothesis} [prediction]).</p>
<p>After preliminary experiments, to avoid cherry picking, all prompts reported in this paper were written prior to evaluation, i.e., we do not allow retroactively editing prompts for performance manipulations, except for an ablation study that explicitly studies the effect of punctuation (Appendix A).</p>
<h3>4.2 Result</h3>
<p>Irrelevant Templates We find that models trained with irrelevant templates learn just as fast as those trained with instructive templates, with no practical difference ${ }^{3}$ at any number of shots (Figure 1). This is true for all models and all datasets in our experiments, including the largest GPT-3 (Figure 2).</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: T0 (3B) on RTE. There is no practical difference between the performance of the models trained with instructive templates vs. those trained with irrelevant templates at any number of shots.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: 16-shot accuracy of four large models on RTE. For GPT-3, there is no practical difference between any template categories except null (not plotted because they are below 0.5). For T5, there is no practical difference between instructive and irrelevant. For T0, there is no practical difference between instructive and irrelevant nor between instructive and misleadingmoderate. For T0++, there is no practical difference between instructive and irrelevant nor between instructive and misleading-extreme.</p>
<p>Misleading Templates There is no consistent relation between the performance of models trained with templates that are moderately misleading (e.g. {premise} Can that be paraphrased as " ${$ hypothesis }" ?) vs. templates that are extremely misleading (e.g., {premise} Is this a sports news? {hypothesis}). T0 (both 3B and 11B) perform better given misleading-moderate (Figure 3), ALBERT and T5 3B perform better given misleading-extreme (Appendices E and G.4), whereas T5 11B and GPT-3 perform comparably on both sets (Figure 2; also see Table 2 for a summary of statistical significances.) Despite a lack of pattern between
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: T0 (3B) on RTE. There is no practical difference between models trained with instructive and misleading-moderate templates at any number of shots. But models trained with misleading-extreme templates are statistically significantly worse from 8 to 128 shots.
the two misleading categories, however, it is consistent that each model exhibits significantly better performance on instructive templates compared to at least one category of misleading templates.</p>
<p>Null Templates Models trained with null templates perform far worse than all other categories of templates (see Appendix G for all null results). Here, we focus on ALBERT (an encoderonly masked language model), which allows more permutation of concatenation orders by placing mask in the middle of sentences. We see that, although null templates are much worse in aggregate, some subset of them (e.g., {premise} [mask] {hypothesis}) are still able to learn nearly as fast as the average instructive template after 32 shots (Figure 13).</p>
<p>Zero-Shot So far, we have focused on few-shot results. At zero shots, all models (including GPT-3 175B) perform only marginally above random, except the instruction-tuned T0. Thus, for our analysis of zero shot performance, we focus on T0. Figure 4 shows that there is no practical difference between the performance of T0 3B given instructive templates and either category of misleading templates. T0 11B performs better, although it also shows no practical difference between misleading-moderate and instructive templates. Lastly, T0++ (trained on more datasets than other T0 variants), is the only model in this paper that shows statistically significantly different performance across all categories of prompts. However, there remains the caveat that it still performs arguably too well in absolute terms with pathological prompts, which we discuss in the next section.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Zero-shot accuracy of instruction-tuned models on RTE. Each prompt's performance is a single point (unlike the few-shot figures where each prompt is approximated by multiple points with multiple samplings of few-shot examples.) Arrows highlight some prompts with their excerpts. See Appendix I for the full results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">size</th>
<th style="text-align: center;">#shots</th>
<th style="text-align: center;">inst. $&gt;$ mis-moderate</th>
<th style="text-align: center;">inst. $&gt;$ mis-extreme</th>
<th style="text-align: center;">inst. $&gt;$ irrelevant</th>
<th style="text-align: center;">inst. $&gt;$ null</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T0++</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">235M</td>
<td style="text-align: center;">$4-256$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T5 LMA</td>
<td style="text-align: center;">770M</td>
<td style="text-align: center;">$4-256$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T5 LMA</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">$4-256$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">$4-256$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T5 LMA</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">T0++</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 2: Checkmarks indicate where two categories of templates lead to statistically significantly different performance, as measured by an independent two-sample $t$-test and a Wilcoxon rank-sum test; both tests always agree in this table. A lack of checkmark indicates where model performance fails to differentiate the two categories, i.e., models do not understand the differences between the prompt categories. We consider significant differences (checkmarks) between categories of prompts to be necessary-but not sufficient-for language understanding.</p>
<h3>4.3 Discussion</h3>
<p>Recall that a common assumption in the literature is that prompts require experts to clearly and correctly describe the task at hand (§1). In contrast, Table 2 summarizes that, with the exception of T0++ at zero shots, all models perform essentially as well with some pathological prompts as they do with proper prompts. Notably, despite being much larger than its competitors, GPT-3 shows the same patterns of behaviors, suggesting that mere scaling does not address this issue. Meanwhile, the evidence from instruction tuning is mixed. Although Sanh et al. (2021) are right that instruction tuning yields substantial improvement in performance as
well as robustness as measured by variance, T0 is somewhat too robust and less sensitive to the semantics of the prompts in terms of distinguishing proper instructions from pathological ones, compared to T5 of the same size in the few-shot setting (Figure 2).</p>
<p>In the zero-shot setting, we do see that that the largest model instruction-tuned with the most datasets (T0++) improves a model's sensitivity to prompt semantics. This is a positive result, but it comes with the caveat that there still exist numerous examples of pathological prompts that perform just as well as the proper ones do. To be charitable to randomness in neural models, we hold</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The best-performing instructive template for ALBERT on RTE, {prem} Are we justified in saying that "{hypo}"? with select LM targets from each category.
this study to a higher standard by comparing means and medians among categories with statistical tests. Nevertheless, for our research question, existence proofs alone are still alarming. For example, without any gradient update nor priming, it is striking that out-of-the-box T0++ scores a high accuracy of $78 \%$ with the extremely misleading {premise} Is that grammatically correct? {hypothesis}, the same accuracy as it achieves with a proper instruction {premise} Are we justified in saying "{hypothesis}"? If models were truly classifying whether the text is grammatical, it would have only scored $52.7 \%$ because RTE is written by experts and all examples are grammatical. Even templates that underperform the instructive ones seem to be too good. For example, it is difficult to imagine a human scoring $72 \%$ zero-shot with the prompt {premise} Inflections are annoying and thank god that Middle English got rid of most of them. {hypothesis} for a nuanced task like NLI.</p>
<h2>5 Effect of Target Words</h2>
<h3>5.1 Method</h3>
<p>In this experiment, we study the effect of different LM target words given a fixed template. We write 4 categories of targets, with at least 3 pairs of target words for each category (except the singleton yesno category):</p>
<ol>
<li>Yes-no: Model is expected to predict the word "yes" for entailment and "no" for nonentailment.
<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
</ol>
<p>Figure 6: T0 (3B) on RTE. Misleading templates + yesno targets (red) learn substantially faster than instructive templates + arbitrary targets (green), which is the opposite of what we expect from humans.
2. Yes-no-like: Semantically equivalent to yesno but using superficially different words, e.g., "true"/"false", "positive"/"negative".
3. Arbitrary: Model is expected to predict arbitrary words that have no semantic relation to the entailment task, e.g., "cat" for entailment, "dog" for non-entailment.
4. Reversed: Model is expected to predict the opposite of the (intuitive) yes-no and yes-nolike labels, e.g., "no" for entailment, "yes" for non-entailment.</p>
<p>See Appendix F. 3 for the full list. Within the arbitrary category, in addition to the common anglophone first names as Le Scao and Rush (2021) use, we also include word pairs with high semantic similarity, low similarity, and pairs which are highly frequent in the English language, but we find no consistent difference among these various subcategories of the arbitrary category.</p>
<h3>5.2 Result</h3>
<p>For both ALBERT and T0, we find that models trained with yes-no targets learn a good deal faster than those trained with yes-no-like targets and dramatically faster than those with arbitrary and reversed targets. For example, Figure 5 shows the top-performing instructive template trained with different target words. At 32 shots, the difference between the median accuracies of "yes"/"no" vs. "no"/"yes" is $22.2 \%$, far larger than the effect size of varying categories of templates in Section 4. Aggregating over all combination of templates and</p>
<p>targets, Figure 16 confirms that the choice of target words matter much more than the meaning of the templates.</p>
<h3>5.3 Discussion</h3>
<p>The fact that models consistently learn slower with arbitrary and reversed target words is a positive result: this type of performance differential is consistent with what we expect for models that are correctly sensitive to the semantics of the words. However, there are several important negative results in these experiments as well. First, the effect of the target words overrides the semantics of the overall prompt. Consider two kinds of templatetarget combinations:</p>
<ol>
<li>An irrelevant or misleading template + yes-no targets, e.g., {premise} Does the paragraph start with "the"? [yes/no] {hypothesis}</li>
<li>An instructive template + arbitrary targets, e.g., {premise} Based on the previous passage, is it true that "{hypothesis}"? [cat/dog]</li>
</ol>
<p>Figure 6 shows that combinations such as (1) often dramatically outperform (2). However, (2) simply requires figuring out a mapping: "Reply 'cat' if entailed and reply 'dog' if not entailed". For humans, this can be learned in a few shots, e.g., Ferrigno et al. (2017) showed that adults can reach $60 \%$ accuracy in 18 trials ${ }^{4}$ for an arbitrary map of ${$ more numerous $\rightarrow$ star shape, less numerous $\rightarrow$ diamond shape } without receiving any language instructions. In contrast, models under many arbitrary LM targets struggle to reach $60 \%$ median accuracy even by 64 shots with instructive templates (Figure 6 green; Figure 5 red, purple).</p>
<p>Further, even given intuitive yes-no-like targets such as "agree"/"disagree" and "good"/"bad", models learn much slower compared to when given "yes"/"no". As Figure 5 (green vs. dark green) and Figure 16 (first vs. second x-axis group) show, there exists a large performance gap between yes-no and yes-no-like targets which is not closed until 256 shots. Moreover, when we try to help the models by appending target hints such as "True or false?" to the templates, performance often drops instead, echoing Sanh et al. (2021) and Wei et al. (2021)'s</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>findings that including answer choices in input sequence make models perform worse for certain tasks.</p>
<h2>6 General Discussion</h2>
<h3>6.1 Summary and Interpretation</h3>
<p>Our main research question is whether models understand prompts as meaningful task instructions analogous to how humans would. Again, suppose an experimenter provides a human annotator with an informative instruction of a reasonably easy task. If the annotator understands the instruction, we expect them to perform better than when the experimenter provides misleading instructions, irrelevant instructions, or no instructions at all. Section 4 shows that the performance of most models is insensitive to the difference between instructive and irrelevant templates, moderately sensitive between instructive and misleading templates, and highly sensitive between instructive and null templates. Comparing to the effect of the templates, however, Section 5 shows that models are much more sensitive to the semantics of the target words: they learn far slower with arbitrary or reversed target words as desired. However, they are overly sensitive to semantically equivalent yes-no-like words (i.e., performing much worse with "agree"/"disagree" than with "yes"/"no"), and the choice of target words override the semantics of the templates (e.g., performing much better given a irrelevant template with "yes"/"no" targets than with an instructive template with arbitrary targets such as "cat"/"dog").</p>
<p>Our main argument throughout the paper shares the same logic as a recent line of studies (Sinha et al., 2021; O'Connor and Andreas, 2021; Pham et al., 2021; Gupta et al., 2021) which argue that the fact that LMs achieve good performance under ideal conditions is insufficient to establish language understanding because they also succeed under pathological conditions (e.g., sentences with shuffled word order) where humans fail catastrophically. ${ }^{5}$ In other words, the fact that models are so good at inferring the gold labels from pathologi-</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>cal inputs casts major doubts on whether models make inferences in any way that resembles how humans make inferences. For our results, the fact that models are so good at learning from pathological instructions likewise casts major doubts on whether models understand prompts as instructions in any way that resembles how humans understand instructions.</p>
<h3>6.2 Alternative Interpretations and Future Directions</h3>
<p>As with any extrinsic evaluation, accuracy cannot directly measure understanding. For example, a human could perfectly understand an instruction but still, e.g., have the same accuracy with instructive vs. irrelevant templates because the task itself is too hard (a lack of competence) or because they for some reason ignore the instructions (a lack of compliance). We discuss these two possibilities below.</p>
<p>Lack of Competence This is primarily a concern for non-instruction-tuned models at zero shots, where all models perform only slightly above random, and thus a lack of statistical significance among template categories is ambiguous as to whether models lack understanding of NLI instructions vs. if models lack the competence in NLI per se. This is why our study largely focuses on the fewshot setting, where a lack of competence is less of a concern, as models do competently achieve good accuracies that are only moderately below the state-of-the-art non-few-shot models.</p>
<p>Another counterargument is that maybe no models ever actually reason about if a premise entails a hypothesis. Maybe they just always exploit spurious or heuristic features and, if only they were competent in properly reasoning about entailment relations, then the meaning of NLI instructions would matter. This argument is possible, although, first, it hinges on to what extent NLI (or any other behavioral evaluation) can measure language understanding, which is a complex debate beyond the scope of this paper. Second, in preliminary experiments (Appendix K), our models actually zero-shot transfer reasonably well to HANS (McCoy et al., 2019), a dataset designed to diagnoses models use of NLI heuristics. Thus, it is unlikely that models are entirely incompetent in reasoning about entailment relations and solely rely on heuristics. Regardless, further differentiating competence in understanding task instructions vs. competence in tasks per se is an important direction for future work.</p>
<p>Lack of Compliance Another interpretation is that irrelevant prompts perform the same as the instructive ones because models simply ignore the prompts altogether. However, a lack of compliance alone cannot explain our results. If models truly ignore the prompts, we should not see any systematic differences between any categories of prompts. Instead, we do see consistent patterns that instructive and irrelevant templates make models learn significantly faster than misleading and null templates do (Table 2).</p>
<p>A more nuanced counterargument is that although models do not ignore their prompts entirely, perhaps it "takes less effort" for models to use the spurious or heuristic features for predictions as opposed to the more complex syntactic or semantic features (Lovering et al., 2021; Warstadt et al., 2020) required to properly comply with the instructions. However, spurious features alone likewise cannot explain the observed performance gaps. Recall that, within each random seed, all models see exactly the same training examples (with the same spurious features). Thus, to the extent that models perform differently with some prompts compared to others, it may be due to some complex interactions between the (spurious or semantic) features in prompts and the spurious features in data examples. One possible example of this interaction is that punctuation has a large effect for irrelevant templates, but instructive templates seem to be able to suppress such effect (Appendix A). Investigating the nature of this interaction is a promising direction for future work, and it suggests a way in which the semantics of the prompt might matter, e.g., by affecting the models' inductive biases, even if models do not interpret or use the instructions in the same way as humans would.</p>
<h2>7 Conclusion</h2>
<p>In this study, we train several models with over 30 manually written templates and 13 sets of LM targets for NLI. We find that models often learn equally fast with misleading and irrelevant templates as they do with instructive ones, and that the choice of the target words overrides the meaning of the overall prompts. Although models do not entirely ignore the meaning of the prompts, our results contradict a hypothesis commonly assumed in the literature that models use prompts as semantically meaningful task instructions in ways analogous to humans' use of instructions.</p>
<h2>Ethical Considerations</h2>
<p>The fact that even the largest LMs appear to follow yet do not actually follow users' instructions has important implications, especially considering the increasing commercial use of LMs. While traditional fine-tuned models also pose challenges in interpretability, with prompt-based models, an illusion of instruction following can be more pernicious than having no instructions at all. The intuitive interface that prompts provide might make them more accessible to lay users, and can mislead users to think that their instructions are being understood and followed. Our results suggest that cautions are needed even more than they were with traditional fine-tuned models.</p>
<h2>Acknowledgments</h2>
<p>We are grateful to Colin Raffel, Victor Sanh, Sasha Rush, Stephen Bach, Roman Feiman, Teven Le Scao, Ian Tenney, Dan Garrette, Jason Wei, Satoshi Sekine, Mike Tien-Chien Chiang, Xavier Fontaine, Pierre Colombo, Ryan Teehan, Debajyoti Datta, William Rudman, Ruochen Zhang, Daniel Cohen, George Zerveas, Eric Rosen, Kaiyu Zheng, Nihal Nayak, Roma Patel, Charles Lovering, Tian Yun, Jack Merullo, and Aaron Traylor for comments and discussions on early drafts of this paper. Special thanks to Victor, Colin, and Teven for technical clarifications and code review.</p>
<p>Furthermore, Albert is indebted to Colin and Sasha for their patience on the many iterations of the zero-shot Figure 4 as well as invaluable mentorship throughout the T0 project.</p>
<p>This work was supported in part by the IARPA BETTER program.</p>
<h2>References</h2>
<p>Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Beltagy. 2021. FLEX: Unifying evaluation for few-shot NLP. ArXiv preprint, abs/2107.07170.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing</p>
<p>Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Stephen Ferrigno, Julian Jara-Ettinger, Steven T Piantadosi, and Jessica F Cantlon. 2017. Universal and uniquely human factors in spontaneous number perception. Nature communications, 8(1):1-10.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021a. A framework for few-shot language model evaluation.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021b. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online. Association for Computational Linguistics.</p>
<p>Marvin J Greenberg. 1974. Euclidean and nonEuclidean Geometries: Development and history. W. H. Freeman and Company.</p>
<p>Ashim Gupta, Giorgi Kvernadze, and Vivek Srikumar. 2021. Bert \&amp; family eat word salad: Experiments with text understanding.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. CoRR, abs/2110.04366.</p>
<p>Daniel Khashabi, Shane Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sameer Singh, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, et al. 2021. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. arXiv preprint arXiv:2112.08348.</p>
<p>Artur Kulmizev and Joakim Nivre. 2021. Schr\" odinger's tree-on syntax and neural language models. arXiv preprint arXiv:2110.08887.</p>
<p>Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. 2022. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2627-2636, Online. Association for Computational Linguistics.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In EMNLP.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582-4597, Online. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692.</p>
<p>Robert L Logan IV, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. 2021. Cutting down on prompts and parameters: Simple few-shot learning with language models. ArXiv preprint, abs/2106.13353.</p>
<p>Charles Lovering, Rohan Jha, Tal Linzen, and Ellie Pavlick. 2021. Predicting inductive biases of pretrained models. In International Conference on Learning Representations.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 34283448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Metaicl: Learning to learn in context. CoRR, abs/2110.15943.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. ArXiv preprint, abs/2104.08773.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.</p>
<p>Joe O'Connor and Jacob Andreas. 2021. What context features can transformer language models use? arXiv preprint arXiv:2106.08367.</p>
<p>Isabel Papadimitriou, Richard Futrell, and Kyle Mahowald. 2022. When classifying grammatical role, bert doesn't care about word order... except when it matters. arXiv preprint arXiv:2203.06204.</p>
<p>Thang Pham, Trung Bui, Long Mai, and Anh Nguyen. 2021. Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1145-1160, Online. Association for Computational Linguistics.</p>
<p>Plato. c. 399 BC. Euthyphro. Penguin Books.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Vinit Ravishankar, Mostafa Abdou, Artur Kulmizev, and Anders Søgaard. 2022. Word order does matter (and shuffled language models know it). arXiv preprint arXiv:2203.10995.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv preprint, abs/1910.01108.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization.</p>
<p>Timo Schick and Hinrich Schütze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021b. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, Online. Association for Computational Linguistics.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and Adina Williams. 2021. UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7329-7346, Online. Association for Computational Linguistics.</p>
<p>Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training. ArXiv preprint, abs/2103.11955.</p>
<p>Shizuo Tsuji and Mary Sutherland. 1980. Japanese Cooking: A Simple Art. Kodansha International.</p>
<p>Prasetya Utama, Nafise Sadat Moosavi, Victor Sanh, and Iryna Gurevych. 2021. Avoiding inference heuristics in few-shot prompt-based finetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9063-9074, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, $B C$, Canada, pages 3261-3275.</p>
<p>Alex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, and Samuel R. Bowman. 2020. Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually). In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 217235, Online. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. ArXiv preprint, abs/2109.01652.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Contents
1 Introduction ..... 1
2 Related Work ..... 2
2.1 Prompt-Based Models ..... 2
2.2 Analyses of Prompts ..... 2
3 Overall Setup ..... 3
4 Effect of Templates ..... 4
4.1 Method ..... 4
4.2 Result ..... 4
4.3 Discussion ..... 6
5 Effect of Target Words ..... 7
5.1 Method ..... 7
5.2 Result ..... 7
5.3 Discussion ..... 8
6 General Discussion ..... 8
6.1 Summary and Interpretation ..... 8
6.2 Alternative Interpretations and Fu- ture Directions ..... 9
7 Conclusion ..... 9
A Effect of Punctuation ..... 14
B Details and Lessons from Experiment- ing with GPT-3's API ..... 15
B. 1 Choice of Model ..... 15
B. 2 Priming vs. Fine-Tuning ..... 15
B. 3 Other Tips for Working with GPT-3 ..... 16
C Hyperparameters ..... 16
D Compute Used ..... 16
E Additional Figures Discussed in the Main Text ..... 17
F All Prompts ..... 19
F. 1 Main Experiment Templates ..... 19
F. 2 Ablation Experiment Templates ..... 20
F. 3 All Target Words ..... 20
G Aggregated Results ..... 21
G. 1 ALBERT on RTE ..... 21
G. 2 ALBERT on ANLI R1 ..... 22
G. 3 T5 770M on RTE ..... 23
G. 4 T5 3B on RTE ..... 24
G. 5 T0 3B on RTE ..... 25
G. 6 T0 3B on ANLI R1 ..... 26
G. 7 T5 11B, T0 11B, and GPT-3 175B (Figure 2) ..... 27
H Results of Individual Templates ..... 28
H. 1 ALBERT ..... 28
H. 2 T0 (3B) ..... 32
H. 3 T5 LM-Adapted (3B) ..... 36
1 Zero-Shot Results (Figure 4) ..... 40
J Comparison of LM targets, Controlling for the Template ..... 41
K Preliminary Results on HANS ..... 44
L Preliminary Results on Winograd ..... 45</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: ALBERT on RTE. Note that (1) irrelevant templates slightly outperform the instructive templates, albeit without statistical significance. (2) Irrelevant templates are far worse without quotation and question marks. (3) But there is no significant difference between instructive templates with or without qmarks.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: T0 (3B) on RTE. Like ALBERT, irrelevant sans qmarks are significantly worse than irrelevant at each and every shot, but there is no significant difference between instructive with or without qmarks.</p>
<h3>A Effect of Punctuation</h3>
<p>For irrelevant templates, we find a large effect from the use of quotation and question marks in templates. It is natural to write such punctuation in instructive templates as they help humans to parse an NLI hypothesis as an embedded clause within an instruction sentence (e.g., Given {premise} Should we assume that "{hypothesis}" is true?). For control, we also use quotation and question marks ("qmarks" hereafter) in irrelevant templates where they would not have made sense naturally, e.g., {premise} Single-family zoning is bad for American cities.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: T5 LM-Adapted (3B). Unlike the other models, there is no statistical significance between irrelevant with or without qmarks. However, instructive sans qmarks statistically significantly outperform instructive at 32 and 64 shots.</p>
<p>"{hypothesis}"? As an ablation, when we remove these qmarks from irrelevant templates, the performance of ALBERT and T0 drops substantially (Figures 7 and 8). In contrast, for T5, qmarks make no difference for irrelevant templates; yet, removing qmarks from instructive templates—where qmarks are natural—boosted performance instead for T5 (Figure 9), but not for T0 nor ALBERT.</p>
<p>Additionally, as a coincidence, most misleading templates contain both quotation and question marks, while most misleading-far templates contain only question marks (Appendix F). But as noted in Section 4.2, there is no consistent pattern between those two misleading categories. In other words, punctuations alone cannot explain everything. As discussed in Section 6.2, the full explanation is likely a combined interaction between the spurious features and the semantics of the templates.</p>
<p>Lastly, note that Schick and Schütze (2021b) and many subsequent papers' prompts for NLI (e.g., "{hypothesis}" ? | [mask]. "{premise}") are basically null templates with some variation in punctuation between the hypothesis and the premise. We find that models learn poorly with the vanilla {hypothesis} [mask] {premise}, but they learn as fast as the instructive templates with Schick &amp; Schütze's punctuated version. That being said, note again that punctuation alone cannot explain the performance gap, since models trained with [mask] {hypothesis} {premise} (Fig-</p>
<p>ure 13, pink) perform second to best, yet swapping their premises and hypotheses (Figure 13, purple) makes it the worst performing among all null templates.</p>
<h2>B Details and Lessons from Experimenting with GPT-3's API</h2>
<h2>B. 1 Choice of Model</h2>
<p>We use the davinci model provided by OpenAI LP's API, which corresponds to ${ }^{6}$ the 175 billion parameter model reported in Brown et al. (2020). Concurrent to our work, OpenAI released a new product called the "Instruct Series", but we decided to not experiment with the Instruct Series because no academic paper or technical documentation of any kind is available with the Instruct Series at the time of writing aside from the following claim on their website: ${ }^{7}$</p>
<p>The Instruct models share our base GPT-3 models' ability to understand and generate natural language, but they're better at understanding and following your instructions. You simply tell the model what you want it to do, and it will do its best to fulfill your instructions. This is an important step forward in our goal of building safe models that are aligned with human interests.</p>
<p>Crucially, the Instruct Series is inappropriate for reproducible research because it is unknown what datasets and prompts these models are trained on, and whether any task categories are systematically held out as done by Sanh et al. (2021) and Wei et al. (2021). If it is trained on any prompt or dataset of NLI, it would not be zero-shot, making it an unfair comparison to other models in our experiments. Second, it is still in beta and its training, held-out, and prompt mixtures could change. At least two Instruct Series models were made available in sequence during our writing, and it is not clear if we experiment on an older version, whether it will still be available and reproducible in the future.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>B. 2 Priming vs. Fine-Tuning</h2>
<p>As mentioned in Section 3, we use priming (a.k.a. in-context learning) in lieu of fine-tuning because, at the time of writing, OpenAI's fine-tuning API is limited to 10 runs per month. To train 30 prompts at only two number of shots would take 6 months, assuming we get hyperparameters right at first try. Further, each training run is limited to a maximum of 5 epochs, which often entails an insufficient number steps for few-shot training. We were unable to fine-tune GPT to any reasonable accuracy with our allowed 10 tries in the first month. Finally, the fine-tuning API is limited to GPT variants up to 6.7 B , not the 175 B model we plan to experiment with.</p>
<p>With priming, we are able to reproduce Brown et al. (2020)'s zero-shot performance on RTE but only with their exact prompt reported in their Figure G.31, all other (even instructive) prompts perform at random at zero shots, suggesting that their reported prompt is highly cherry-picked. We are unable to reproduce their reported few-shot result because they report it at 32 shots, but their API only permits a context length up to 2049 tokens, which is insufficient for RTE. We find that 16 shots are the highest one can reach within the token limit. ${ }^{8}$</p>
<p>Like the gradient updated models, we document the exact examples we use for few-shot priming in our GitHub repository. Unlike the gradient updated models, which are trained on the same $k$ examples, priming models use different sets of $k$ priming examples for each inference example (Brown et al., 2020, p. 20). This means that GPT's performance reflects the fact that, overall, it has seen far more than $k$ examples, making it not directly comparable to the few shots of the gradient updated models. This is not ideal, but our GPT few-shot performance already underperforms what Brown et al. (2020) report, so we choose to not further restrict it to have the same fixed priming examples for all inference examples, which could run into a lack of competence issue (§6.2) that make its results unusable for our research question.</p>
<p>Lastly, unlike the gradient updated models, we do not run multiple seeds with our GPT experiments because, first, they are expensive. As the API bills by token, using $k$ shots of priming example effectively multiplies the total cost by $k$. Sec-</p>
<p><sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>ond, OpenAI imposes a monthly quota for each lab, so running multiple seeds will take several more months to complete.</p>
<h2>B. 3 Other Tips for Working with GPT-3</h2>
<p>Using the logprobs argument in their API, we obtain the top 99 predicted target word and their log probabilities. ${ }^{9}$ Following Sanh et al. (2021) and Wei et al. (2021), we evaluate by a rank classification of the target words, i.e., if the gold target word is "yes", we consider it as correct as long as the probability of "yes" is higher than that of "no", regardless of whether "yes" is the top-1 prediction generated by the model.</p>
<p>Alarmingly, we find that these top-99 predictions are semantically inconsistent ranked, e.g., for one data example and its top-99 word predictions, it is often the case that, e.g., $\mathrm{P}(\mathrm{yes})&gt;\mathrm{P}(\mathrm{no})$ but $\mathrm{P}(\mathrm{Yes})$ $&lt;\mathrm{P}(\mathrm{No})$. Thus, the choice of the target words' surface form makes a substantial difference in the overall performance. (Not to mention the problem of choosing between yes/no, true/false, correct/incorrect, etc. as studied in Section 5.) OpenAI recommends having no trailing space in the input and let the model predict the first token with a leading space as in "_Yes". We find that although stripping the leading space sometimes leads to higher performance for some prompts, overall not applying stripping or other token normalization performs the best.</p>
<p>Another point researchers should pay attention to is the use of what OpenAI calls a "separator" inserted between priming examples. In preliminary experiments, we initially use newline characters as appeared in Brown et al. (2020)'s Appendix G. We later discover that OpenAI recommends using # # # or $\backslash n # # # \backslash n$ as separators. We use the latter and find consistent performance improvement over just using newline characters, and we use it throughout in our main experiments.</p>
<h2>C Hyperparameters</h2>
<p>For encoder-only models, we follow Schick and Schütze (2021b) and Le Scao and Rush (2021)'s recommendations and use a learning rate of $1 e^{-5}$. For T5 and T0 models, we follow Raffel et al. (2020) and Sanh et al. (2021)'s recommendations</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and use a learning rate of $1 e^{-4}$. We run several preliminary experiments with learning rates $\left(3 e^{-4}, 1 e^{-4}, 5 e^{-5}, 1 e^{-5}\right)$ deviating from their recommendations and they perform worse, although our search is not exhaustive due to the high cost of running multiple prompts with multiple random seeds.</p>
<p>Note that T5 and T0 are trained with the Adafactor optimizer (Shazeer and Stern, 2018) in Mesh TensorFlow. Our implementation is in PyTorch, and we find that fine-tuning T5 with PyTorch's implementation of Adafactor yields substantially worse results than the usual choice of the AdamW optimizer. We corresponded with Raffel et al. (2020), who advised us that it might be due to the fact that PyTorch does not have the same learning rate scheduler implementation as TensorFlow's Adafactor does. They recommended us to simply use AdamW, which is what we did. This is somewhat unfortunate because Adafactor is much more memory efficient, which would have drastically reduced the compute resources required and thus enable more comprehensive experiments of the 11B models, which are currently limited to 0 shots and 16 shots only.</p>
<p>Although most models seem to obtain the highest validation accuracy at very early epochs, we train all models to 30 epochs ( 20 epochs for 11B models) to be safe and select the checkpoint with the highest validation accuracy.</p>
<p>All models use a batch size of 4 with 4 gradient accumulation steps for an effective batch size of 16.</p>
<p>Note that because we use a rank classification of single-token target words, decoding sampling methods (e.g., beam search, top- $k$, top- $p$ ) are unnecessary.</p>
<p>We follow Raffel et al. (2020) and add EOS tokens for input sequences, which yields higher fewshot performance compared to not adding EOS as done by Sanh et al. (2021). However, we omit EOS in the zero-shot setting, which exactly reproduces the results reported by Sanh et al. (2021). See T0's GitHub repository readme ${ }^{10}$ for more information.</p>
<h2>D Compute Used</h2>
<p>Each ALBERT 235M model is trained on a single Nvidia RTX3090. Their main experiments took approximately 192 GPU hours.</p>
<p><sup id="fnref5:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Each T5 LMA 770M model is trained on a single A6000. Their main experiments took approximately 48 GPU hours.</p>
<p>The 3B models are each trained by partitioning their layers over four RTX3090s. T5 and T0's main experiments took approximately 2,304 GPU hours in total.</p>
<p>The 11B models are each trained on eight V100s (each with 32GB of memory). T5, T0, and T0++'s main experiments took approximately 1,728 GPU hours in total. (Due to their large GPU memory requirement, we were only able to complete one number of shots.)</p>
<h2>E Additional Figures Discussed in the Main Text</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: How to read these figures: Each dot is the performance of one prompt under one random seed (which controls the sets of few-shot examples) of our baseline model (ALBERT) on RTE validation set. Boxes span from the first quartile to the third quartile, while lines inside boxes mark the medians. Later figures omit the points except outliers in order to improve legibility. See the interactive figures in our GitHub repository or Appendix H for the results of individual prompts.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: ALBERT on RTE. Models trained with irrelevant templates actually slightly outperform the instructive templates, albeit without statistical significance at any number of shots.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: ALBERT on RTE. There is no statistical significance between misleading-extreme and instructive at any number of shots. In contrast, models trained with misleading-moderate templates are significantly worse than the instructive ones from 16 to 64 shots.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: ALBERT on RTE. After 32 shots, models trained with 2 null templates learn just as fast as the instructive templates, but models trained with other null templates (e.g., purple) are much worse.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: The best-performing instructive template for T0 (3B) on RTE, (prem) Based on the previous passage, is it true that "(hypo)"? with select LM targets from each category.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Median accuracies of all template-target combinations at 32 shots. In general, the choice of target words (x-axis groups) matters much more than the choice of templates (colors).
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: T0 (3B)'s 32-shot accuracy with of all template-target combinations on RTE. In general, the choice of target words (x-axis groups) matters much more than the choice of templates (colors).</p>
<h1>F All Prompts</h1>
<h2>F. 1 Main Experiment Templates</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">category</th>
<th style="text-align: center;">template</th>
<th style="text-align: center;">adapted from</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">{premise} Using only the above description and what you know about the world, " ${$ hypothesis}" is definitely correct. Yes or no?</td>
<td style="text-align: center;">Williams et al. (2018, p. 3)</td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">{premise} \nquestion: {hypothesis}Yes or no? \nanswer:</td>
<td style="text-align: center;">Brown et al. (2020, p. 59)</td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">{premise} Are we justified in saying that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">Given {premise} Should we assume that "{hypothesis}" is true?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">{premise} Based on the previous passage, is it true that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">Given {premise} Is it guaranteed true that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">Suppose {premise} Can we infer that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">Given that {premise} Does it follow that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">{premise} Question: Does this imply that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">Given that {premise} Therefore, it must be true that "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">{premise} Do most of the above words appear in the following passage? {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">{premise} Are there lots of similar words in "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">{premise} Does that have the same meaning as "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">{premise} Can that be paraphrased as: "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">{premise} Can that be summarized as "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">{premise} Does the paragraph start with "the"? {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">{premise} Is this grammatically correct? {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">{premise} Is the sentiment positive? {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">{premise} Is this a sports news? {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">{premise} Is this French? {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">{premise} Single-family zoning is bad for American cities. "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">{premise} Inflections are annoying and thank god that <br> Middle English got rid of most of them. "{hypothesis}"?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">{premise} When Bolyai sent Gauss his discovery of non-Euclidean geometry, <br> Gauss replied that he arrived at the same results 30 years ago. "{hypothesis}"?</td>
<td style="text-align: center;">Greenberg (1974, p. 141)</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">{premise} If bonito flakes boil more than a few seconds, <br> the stock becomes too strong? "{hypothesis}"?</td>
<td style="text-align: center;">Tsuji and Sutherland (1980, p. 148)</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">{premise} Is the pious loved by the gods because it is pious? <br> Or is it pious because it is loved by the gods? "{hypothesis}"?</td>
<td style="text-align: center;">Plato (c. 399 BC, 10a)</td>
</tr>
<tr>
<td style="text-align: center;">null</td>
<td style="text-align: center;">{premise} {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">null</td>
<td style="text-align: center;">{hypothesis}{premise}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">null (MLM only)</td>
<td style="text-align: center;">{premise} {mask} {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">null (MLM only)</td>
<td style="text-align: center;">{hypothesis}{mask} {premise}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">null (MLM only)</td>
<td style="text-align: center;">{mask} {premise} {hypothesis}</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">null (MLM only)</td>
<td style="text-align: center;">{mask} {hypothesis}{premise}</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: All prompts used in the main text of the paper. All templates use "yes"/"no" as target words for the entailment and non-entailment classes, respectively. For ternary NLI datasets, we use "unclear" for the neutral class, which performs best after preliminary experiments with other ternary words: "maybe", "sometimes", "perhaps", "possibly", and "neither". Keen readers may notice that some of the instructive templates (e.g., should we assume) do not instruct a strict entailment task. We intentionally wrote a mixture of instructions that asks for strictly logical entailment and pragmatic inference, intending to measure if models can distinguish between the two on datasets such as HANS (McCoy et al., 2019) that magnify different predictions caused by pragmatic effects. Of course, this research question became moot as we found that models cannot even distinguish among much more pathological prompts.</p>
<h1>F. 2 Ablation Experiment Templates</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">category</th>
<th style="text-align: center;">template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">{premise} Using only the above description and what you know about the world, {hypothesis}is definitely correct. Yes or no</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">{premise} \nquestion: {hypothesis}Yes or no $\backslash$ nanswer:</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">{premise} Are we justified in saying that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">Given {premise} Should we assume that {hypothesis}is true</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">{premise} Based on the previous passage, is it true that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">Given {premise} Is it guaranteed true that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">Suppose {premise} Can we infer that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">Given that {premise} Does it follow that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">{premise} Question: Does this imply that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">instructive sans qmarks</td>
<td style="text-align: center;">Given that {premise} Therefore, it must be true that {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant sans qmarks</td>
<td style="text-align: center;">{premise} Single-family zoning is bad for American cities. {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant sans qmarks</td>
<td style="text-align: center;">{premise} Inflections are annoying and thank god that Middle English got rid of most of them. {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant sans qmarks</td>
<td style="text-align: center;">{premise} When Bolyai sent Gauss his discovery of non-Euclidean geometry, Gauss replied that he arrived at the same results 30 years ago. {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant sans qmarks</td>
<td style="text-align: center;">{premise} If bonito flakes boil more than a few seconds, the stock becomes too strong. {hypothesis}</td>
</tr>
<tr>
<td style="text-align: center;">irrelevant sans qmarks</td>
<td style="text-align: center;">{premise} Is the pious loved by the gods because it is pious. Or is it pious because it is loved by the gods. {hypothesis}</td>
</tr>
</tbody>
</table>
<p>Table 4: Used in the study of the effect of question and quotation marks in Appendix A.</p>
<h2>F. 3 All Target Words</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Target Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">yes-no</td>
<td style="text-align: left;">yes;no</td>
</tr>
<tr>
<td style="text-align: left;">yes-no-like</td>
<td style="text-align: left;">true;false</td>
</tr>
<tr>
<td style="text-align: left;">yes-no-like</td>
<td style="text-align: left;">positive;negative</td>
</tr>
<tr>
<td style="text-align: left;">yes-no-like</td>
<td style="text-align: left;">right;wrong</td>
</tr>
<tr>
<td style="text-align: left;">yes-no-like</td>
<td style="text-align: left;">correct;incorrect</td>
</tr>
<tr>
<td style="text-align: left;">yes-no-like</td>
<td style="text-align: left;">agree;disagree</td>
</tr>
<tr>
<td style="text-align: left;">yes-no-like</td>
<td style="text-align: left;">good;bad</td>
</tr>
<tr>
<td style="text-align: left;">reversed</td>
<td style="text-align: left;">no;yes</td>
</tr>
<tr>
<td style="text-align: left;">reversed</td>
<td style="text-align: left;">false;true</td>
</tr>
<tr>
<td style="text-align: left;">reversed</td>
<td style="text-align: left;">negative;positive</td>
</tr>
<tr>
<td style="text-align: left;">arbitrary</td>
<td style="text-align: left;">B;C</td>
</tr>
<tr>
<td style="text-align: left;">arbitrary</td>
<td style="text-align: left;">cat;dog</td>
</tr>
<tr>
<td style="text-align: left;">arbitrary</td>
<td style="text-align: left;">she;he</td>
</tr>
</tbody>
</table>
<p>Table 5: LM targets used in Section 5. Again, for ternary NLI datasets, we use "unclear" for the neutral class, which performs best after preliminary experiments with other ternary words: "maybe", "sometimes", "perhaps", "possibly", and "neither". Within the arbitrary category, in addition to the common anglophone first names as Le Scao and Rush (2021) use, we also tried word pairs with high semantic similarity ("cat"/"dog"), low similarity ("cake"/"piano", "write"/"sleep"), and pairs which are highly frequent in the English language ("she"/"he", "the"/"a") in preliminary experiments, but we find no consistent difference among these various subcategories of the arbitrary category.</p>
<h1>G Aggregated Results</h1>
<h2>G. 1 ALBERT on RTE</h2>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: center;">num. shots</th>
<th style="text-align: center;">template category</th>
<th style="text-align: center;">median</th>
<th style="text-align: center;">q3 - q1</th>
<th style="text-align: center;">mean</th>
<th style="text-align: center;">std. dev.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">0.5830</td>
<td style="text-align: center;">0.0885</td>
<td style="text-align: center;">0.5907</td>
<td style="text-align: center;">0.0517</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">0.6300</td>
<td style="text-align: center;">0.1291</td>
<td style="text-align: center;">0.6170</td>
<td style="text-align: center;">0.0645</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">0.5884</td>
<td style="text-align: center;">0.0469</td>
<td style="text-align: center;">0.5787</td>
<td style="text-align: center;">0.0342</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">0.5650</td>
<td style="text-align: center;">0.0722</td>
<td style="text-align: center;">0.5753</td>
<td style="text-align: center;">0.0418</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">0.5560</td>
<td style="text-align: center;">0.0433</td>
<td style="text-align: center;">0.5599</td>
<td style="text-align: center;">0.0324</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">0.6155</td>
<td style="text-align: center;">0.0920</td>
<td style="text-align: center;">0.6186</td>
<td style="text-align: center;">0.0524</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">0.6570</td>
<td style="text-align: center;">0.0307</td>
<td style="text-align: center;">0.6471</td>
<td style="text-align: center;">0.0374</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">0.6101</td>
<td style="text-align: center;">0.0677</td>
<td style="text-align: center;">0.5899</td>
<td style="text-align: center;">0.0595</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">0.6047</td>
<td style="text-align: center;">0.0767</td>
<td style="text-align: center;">0.5969</td>
<td style="text-align: center;">0.0490</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">0.5632</td>
<td style="text-align: center;">0.0397</td>
<td style="text-align: center;">0.5586</td>
<td style="text-align: center;">0.0326</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">0.6697</td>
<td style="text-align: center;">0.0605</td>
<td style="text-align: center;">0.6594</td>
<td style="text-align: center;">0.0558</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">0.6787</td>
<td style="text-align: center;">0.0488</td>
<td style="text-align: center;">0.6787</td>
<td style="text-align: center;">0.0294</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">0.6390</td>
<td style="text-align: center;">0.0506</td>
<td style="text-align: center;">0.6413</td>
<td style="text-align: center;">0.0384</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">0.6083</td>
<td style="text-align: center;">0.0443</td>
<td style="text-align: center;">0.6072</td>
<td style="text-align: center;">0.0427</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">0.5722</td>
<td style="text-align: center;">0.0379</td>
<td style="text-align: center;">0.5767</td>
<td style="text-align: center;">0.0327</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">0.7022</td>
<td style="text-align: center;">0.0813</td>
<td style="text-align: center;">0.6929</td>
<td style="text-align: center;">0.0638</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">0.7292</td>
<td style="text-align: center;">0.0235</td>
<td style="text-align: center;">0.7206</td>
<td style="text-align: center;">0.0236</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">0.7076</td>
<td style="text-align: center;">0.0334</td>
<td style="text-align: center;">0.7056</td>
<td style="text-align: center;">0.0340</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">0.6516</td>
<td style="text-align: center;">0.0992</td>
<td style="text-align: center;">0.6350</td>
<td style="text-align: center;">0.0666</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">0.6318</td>
<td style="text-align: center;">0.0731</td>
<td style="text-align: center;">0.6414</td>
<td style="text-align: center;">0.0392</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">0.7545</td>
<td style="text-align: center;">0.0542</td>
<td style="text-align: center;">0.7353</td>
<td style="text-align: center;">0.0548</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">0.7491</td>
<td style="text-align: center;">0.0198</td>
<td style="text-align: center;">0.7455</td>
<td style="text-align: center;">0.0218</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">0.7509</td>
<td style="text-align: center;">0.0416</td>
<td style="text-align: center;">0.7451</td>
<td style="text-align: center;">0.0299</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">0.7310</td>
<td style="text-align: center;">0.0993</td>
<td style="text-align: center;">0.6953</td>
<td style="text-align: center;">0.0688</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">0.7004</td>
<td style="text-align: center;">0.0848</td>
<td style="text-align: center;">0.6998</td>
<td style="text-align: center;">0.0516</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">instructive</td>
<td style="text-align: center;">0.7834</td>
<td style="text-align: center;">0.0451</td>
<td style="text-align: center;">0.7661</td>
<td style="text-align: center;">0.0551</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">irrelevant</td>
<td style="text-align: center;">0.7671</td>
<td style="text-align: center;">0.0343</td>
<td style="text-align: center;">0.7704</td>
<td style="text-align: center;">0.0200</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">misleading-extreme</td>
<td style="text-align: center;">0.7798</td>
<td style="text-align: center;">0.0334</td>
<td style="text-align: center;">0.7729</td>
<td style="text-align: center;">0.0255</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">misleading-moderate</td>
<td style="text-align: center;">0.7744</td>
<td style="text-align: center;">0.0550</td>
<td style="text-align: center;">0.7354</td>
<td style="text-align: center;">0.0842</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">0.7329</td>
<td style="text-align: center;">0.0695</td>
<td style="text-align: center;">0.7369</td>
<td style="text-align: center;">0.0389</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ Although sometimes the API returns less than the number of logprobs the user specifies, in which case we contacted OpenAI's customer support who provided us refund by store credit. At the time of publishing, OpenAI now restricts logprobs to a maximum of 5 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{10} \mathrm{https}: / /$ github.com/bigscience-workshop/t-zero/tree/ master/examples&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3832 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3832</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3832</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-384affb888eda2c437207115de0555b19c68664b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/384affb888eda2c437207115de0555b19c68664b" target="_blank">Probing Conceptual Understanding of Large Visual-Language Models</a></p>
                <p><strong>Paper Venue:</strong> 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</p>
                <p><strong>Paper TL;DR:</strong> This work proposes novel benchmarking datasets for probing three different aspects of content understanding, 1) relations, 2) composition, and 3) context, and reveals several interesting insights such as that cross-attention helps learning conceptual understanding, and that CNNs are better with texture and patterns, while Transformers are better at color and shape.</p>
                <p><strong>Paper Abstract:</strong> In recent years large visual-language (V+L) models have achieved great success in various downstream tasks. However, it is not well studied whether these models have a conceptual grasp of the visual content. In this work we focus on conceptual understanding of these large V+L models. To facilitate this study, we propose novel benchmarking datasets for probing three different aspects of content understanding, 1) relations, 2) composition, and 3) context. Our probes are grounded in cognitive science and help determine if a V+L model can, for example, determine if snow garnished with a man is implausible, or if it can identify beach furniture by knowing it is located on a beach. We experimented with many recent state-of-the-art V+L models and observe that these models mostly fail to demonstrate a conceptual understanding. This study reveals several interesting insights such as that cross-attention helps learning conceptual understanding, and that CNNs are better with texture and patterns, while Transformers are better at color and shape. We further utilize some of these insights and investigate a simple finetuning technique that rewards the three conceptual understanding measures with promising initial results. The proposed benchmarks will drive the community to delve deeper into conceptual understanding and foster advancements in the capabilities of large V+L models. The code and dataset is available at: https://tinyurl.com/vlm-robustness</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3832",
    "paper_id": "paper-384affb888eda2c437207115de0555b19c68664b",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0076625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Probing Conceptual Understanding of Large Visual-Language Models</h1>
<p>Madeline Schiappa ${ }^{1}$<br>Jared Claypoole ${ }^{2}$<br>Raiyaan Abdullah ${ }^{1 *}$<br>Michael Cogswell ${ }^{2}$<br>Yogesh Rawat ${ }^{1}$<br>Shehreen Azad ${ }^{1}$<br>Ajay Divakaran ${ }^{2}$</p>
<p>${ }^{1}$ Center for Research in Computer Vision, University of Central Florida<br>${ }^{2}$ SRI International</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Conceptual understanding of an existing V+L model. Here, CLIP failure to understand relational, compositional and contextual reasoning is shown. This benchmark presents three datasets to evaluate V+L models on relational, compositional, and contextual understanding. They utilize image-text matching tasks with predicate, object/subject, compositions, or background swaps.</p>
<h4>Abstract</h4>
<p>In recent years large visual-language ( $V+L$ ) models have achieved great success in various downstream tasks. However, it is not well studied whether these models have a conceptual grasp of the visual content. In this work we focus on conceptual understanding of these large $V+L$ models. To facilitate this study, we propose novel benchmarking datasets for probing three different aspects of content understanding, 1) relations, 2) composition, and 3) context. Our probes are grounded in cognitive science and help determine if a $V+L$ model can, for example, determine if snow garnished with a man is implausible, or if it can identify beach furniture by knowing it is located on a beach. We experimented with many recent state-of-the-art $V+L$ models and observe that these models mostly fail to demonstrate a conceptual understanding. This study reveals several interesting insights such as that cross-attention helps learning conceptual understanding, and that CNNs are better with texture and patterns, while Transformers are better at color and shape. We further utilize some of these insights and investigate a simple finetuning technique that rewards the three conceptual understanding measures with promising initial results. The proposed benchmarks will drive the community to delve deeper into conceptual understanding and foster advancements in the capabilities of large $V+L$ models. The code and dataset is available at: https://tinyurl.com/vlm-robustness</p>
<h2>1. Introduction</h2>
<p>Humans navigate the world by learning an "understanding" of how it works. Understanding may be defined as the underlying organization of all concepts, including objects, situations, events, and more [8, 27]. They are organized in our brains as conceptual maps, which encode structured, relational information [13]. Conceptual maps highlight major objects and actions in a system and the causal relations between them. While deep learning models have impressive performance in a variety of tasks, it is still unclear if their impressive performance is due to learnt conceptual maps.
Large visual-language ( $\mathrm{V}+\mathrm{L}$ ) models are recently and greatly successful deep learning models that learn representations of image and text in a shared space. These representations are useful for downstream tasks like image classification, visual-question answering, image retrieval and more [1, 31, 32, 43, 47, 53]. However, for use in real-world applications, it is also vital that models "understand" rather than memorize to perform on more general tasks [29]. While large-language models have been shown to have a moderate amount of "theory of mind," as measured by conceptual consistency [35], V+L models have not been investigated</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>in a similar way using real-world examples. This is partly because images are more challenging, as shown by preliminary studies [5, 10, 42]. With this in mind, we focus on probing models on their conceptual maps.
We develop a benchmark by combining insights from wellknown tests such as the Peabody Picture test, semantic analysis underpinning knowledge bases such as ConceptNet [39], and comprehension in elementary school education [35] to identify three key areas for probing: relations, composition, and context (Figure 1). Our benchmark could be seen as a computational instantiation of visual comprehension testing along three important fundamental skills. These skills form a compact set of necessary, but not sufficient, prerequisites for key tasks such as concept transfer, analysis, evaluation, and generation. They thus provide us a basis for probing comprehension of large V+L models.
We propose three benchmark datasets, Probe-R, Probe-C, and Probe-B. Probe-R looks at model understanding of possible object relations by comparing an image to a correct prompt and an incorrect prompt where the predicate is swapped with an unlikely relation. Probe-C looks at model understanding of possible compositional relations by comparing two images and two prompts where either the composition is swapped with an antonym or the object is swapped. Finally, Probe-B looks at model understanding of objects and their relationships to their surroundings by removing background and observing the change in performance.
We experimented with several state-of-the-art V+L models and provide several interesting insights regarding these models. For compositional understanding, we observe that (1) models struggle with compositionality, and (2) CNN based backbones may be better at recognizing texture and patterns while ViT backbones are better with color and shape. For relational understanding, we observe that (1) both modality specific attention and co-attention in parallel improve relational understanding, and (2) Predicate swapping that violates expectations surfaces the lack of an underlying conceptual model. For contextual understanding we observe that (1) models tend to not use context in order to recognize most objects, again indicating a lack of an underlying conceptual model. We further utilize these findings and develop a simple finetuning approach based on selective negatives paradigm and observe improvement on our understanding-related probes.
In summary, we make the following contributions:</p>
<ul>
<li>We study the capability of existing large V+L models for complex visual perception focusing on relational, compositional, and contextual understanding.</li>
<li>We propose three benchmark datasets: Probe-R, ProbeC, and Probe-B focusing on subject-object relations, composition-object relations, and background-object relations.</li>
</ul>
<p>Table 1. Comparison of ours with various recent works probing relational, attribute, and context understanding of models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Relational</th>
<th style="text-align: center;">Compositional</th>
<th style="text-align: center;">Contextual</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VL-CheckList [52]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">ARO [50]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">SVLC [12]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">ControlledImCaps [19]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">CREPE [26]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">SugarCREPE [15]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<ul>
<li>We perform extensive evaluation of existing models and provide new insights about their capabilities.</li>
<li>We present a simple approach, based on prompting that rewards compositionality and preservation of relations between objects, which yields a more robust performance on complex visual perception tasks.</li>
</ul>
<h2>2. Related Works</h2>
<p>Several works have probed models to understand what models are learning $[10,12,15,18,19,26,28,42,49,50,52]$. Table 1 shows a comparison of our proposed benchmark against several other existing works that probe the different understanding property of V+L models. From the table, it is evident that none of the existing works probe the contextual understanding of V+L models, which our proposed dataset does. Moreover, our proposed benchmark has more images and is evaluated on more models than most of the existing methods, which will be discussed later. Even though SVLC [12] is a large-scale benchmark in terms of both size of dataset and number of evaluation models than our proposed benchmark, this is not sufficient for contextual understanding of the V+L models. An extension of Winoground [10] showed that models perform worse than humans because it requires both compositional understanding and commonsense reasoning. Without disentangling the individual skills required to perform well, it limits insights to why/how they are failing and where to improve. In this work, we generate a benchmark that isolates components of understanding into compositional, relational, and context, allowing for more detailed insights.</p>
<h2>3. Benchmark and Evaluation Metrics</h2>
<p>We evaluate three discrete concepts: object-relations, compositionality, and background context. We have generated three datasets: Probe-R, Probe-C, and Probe-B. A summary of each dataset is shown in Table 2 and an overview in Figure 2.</p>
<p>Prompting is typically done in downstream image classification by forming sentences with each class name in the prompt, such as "a photo of a dog."</p>
<p>The one with the highest similarity to the visual features</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. <strong>Overview of proposed benchmarks</strong>. <em>Probe-R</em> swaps the real subject or relation with an unlikely one and swaps a set of subject-only images to a subject-only prompt and the ground-truth relation prompt. <em>Probe-C</em> asks the model to match two images and two prompts, swapping object or composition. <em>Probe-B</em> compares object recognition performance before and after swapping out context from background and other surrounding objects.</p>
<p>is the predicted class [31, 38]. These benchmarks heavily rely on "prompting" the model by changing text input as well as image input in Probe-B.</p>
<h3>3.1. Dataset</h3>
<h4>Probe-R: Relational Understanding</h4>
<p>To generate a dataset that can be used to probe for relational understanding, we collected samples from the Visual Genome [21] dataset. These samples are used to probe whether models have learned consistent concepts of objects and their potential relationships to each other.</p>
<p>For each group, we have four prompts, <em>P</em> ∈ {<em>R</em><sub>1</sub>, <em>R</em><sub>2</sub>, <em>R</em><sub>3</sub>, <em>O</em><sub>1</sub>}, one anchor image <em>X</em><sub><em>R</em><sub>1</sub></sub> and 10 images <em>X</em><sub><em>O</em><sub>1</sub></sub> with the subject present and no other objects found in the anchor image. For each <em>X</em><sub><em>R</em><sub>1</sub></sub>, the ground truth relation <em>R</em><sub>1</sub> = 〈<em>s</em><sub>1</sub>, <em>r</em><sub>1</sub>, <em>o</em><sub>1</sub>〉 is compared to a swap of subject <em>R</em><sub>3</sub> = 〈<em>s</em><sub>1</sub>, <em>r</em><sub>1</sub>, <em>o</em><sub>1</sub>〉 or predicate <em>R</em><sub>2</sub> = 〈<em>s</em><sub>1</sub>, <em>r</em><sub>1</sub>, <em>o</em><sub>1</sub>〉. We sample <em>s</em><sub>1</sub> uniformly from subjects that do not occur in the dataset with <em>r</em><sub>1</sub> and <em>o</em><sub>1</sub> and similarly <em>r</em><sub>1</sub> is sampled uniformly from relations that do not occur with <em>s</em><sub>1</sub> and <em>o</em><sub>1</sub>. This swapping of unlikely subjects and predicates allows us to test whether V+L models have learned consistent conceptual models of what object relations are possible in a system by comparing existing ones to unlikely ones. The final comparison is subject-only images <em>X</em><sub><em>O</em><sub>1</sub></sub> to <em>P</em><sub><em>R</em><sub>1</sub></sub>, and a prompt with only the subject <em>P</em><sub><em>O</em><sub>1</sub></sub>.</p>
<p><strong>Probe-C: Compositional Understanding</strong> To generate a dataset that can be used to probe for compositional understanding, we collected samples from the MS COCO Captions dataset [24]. These samples are used to probe whether models have learned an understanding of object attributes and their relationships to each other. For each group, we have two images <em>x</em><sub>1</sub> and <em>x</em><sub>2</sub> and two prompts <em>p</em><sub>1</sub> and <em>p</em><sub>2</sub>. This dataset has two splits, one where the compositions are swapped in the prompts and the other where objects are swapped. When swapping compositions, antonyms were manually mapped to each attribute to ensure that the attribute is not present in the image. For example, if there is a "small dog" in an image, the comparison could be "a large dog." When swapping objects, the images must have the same composition but different objects.</p>
<p><strong>Probe-B: Contextual Understanding</strong> To generate a dataset that probes for model understanding on objects and their relationship to contextual cues found in an image's background, we collected samples from MS COCO [24] consisting of 80 objects. These samples are used to probe model reliance on background cues and reliance on co-occurrence between objects.</p>
<p>For each group, there is an unmodified image <em>x</em><sub>0</sub>, an image with a random patch on the background <em>x</em><sub>0</sub>, a modified image where the background is removed <em>x</em><sub>1</sub> and 80 or fewer prompts. We have two splits in this data, the first removing the background but keeping all objects Probe-B<sub>MR</sub> and the other removing both background and all other objects Probe-B<sub>R</sub>. Probe-B<sub>R</sub> aims to probe models on whether they use conceptual maps on object co-occurrence to improve recognition. Probe-B<sub>MR</sub> aims to probe models on whether they have conceptual maps related to what group of objects are likely to be in what scenery or possible physical relations to each other. Poor performance on these tasks would indicate model use of such conceptual mappings, while good performance means they are focusing on object recognition only.</p>
<p>We experiment with four fillers: black, gray, gaussian noise, or a random scene. Random scenery was collected from the Indoor Scenes Dataset [30] and the Kaggle Landscape dataset [34]. These images were manually filtered to ensure none of the 80 MS COCO classes were present. For single objects, images were only kept if the size of the object was between a threshold where the object was not too large and not too small relative to the image size.</p>
<p>Table 2. A summary of proposed benchmark datasets. Tasks include Image-Text matching (ITM), multi-label object recognition (MLR), and object recognition (R). Groups refer to the group of images/text for each comparison being made. Under attributes, we list the dataset properties, where fillers are the types of replacements we use when removing background pixels.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Task</th>
<th>Description</th>
<th>Source</th>
<th>Images</th>
<th>Group Description</th>
<th>Groups</th>
<th>Attributes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probe-R</td>
<td>ITM</td>
<td>Predicate/ Object Swapping</td>
<td>Visual Genome</td>
<td>99,960</td>
<td>1 image, 10 pos. images, 4 prompts</td>
<td>99,960</td>
<td>2,456 Relations, 6,006 Objects</td>
</tr>
<tr>
<td>Probe-C</td>
<td>ITM</td>
<td>Composition Swapping</td>
<td>MS COCO</td>
<td>40,681</td>
<td>2 images, 2 prompts</td>
<td>79,925</td>
<td>114 Compositions, 2,462 Objects</td>
</tr>
<tr>
<td>Probe-B</td>
<td>MLR</td>
<td>Background Removal</td>
<td>MS COCO</td>
<td>31,745</td>
<td>3 images, 80 prompts</td>
<td>31,745</td>
<td>4 fillers, 80 objects</td>
</tr>
<tr>
<td></td>
<td>R</td>
<td>Background+Object Removal</td>
<td></td>
<td>1,484</td>
<td>3 images, $&lt;80$ prompts</td>
<td>9,375</td>
<td>4 fillers, 76 objects</td>
</tr>
</tbody>
</table>
<h3>3.2 Evaluation Metrics</h3>
<p>We use different evaluation metrics for each of the three datasets, but with a focus on change in model confidence. This allows us to relate to the psychological paradigm "violation-of-expectation" (VoE) [2, 29]. If V+Ls are learning conceptual models, then a violation of those models should be easily recognized and confidence should remain high when choosing between the correct prompt and the prompt that is violating expectation. For Probe-R, by probing models with data that is intended to violate expectation, we expect the confidence to remain high. For Probe-C, by probing models with paired opposites, we also expect the confidence to be high. For Probe-B, by removing visual information or replacing it with a violation of the original information, we expect the model to become confused and therefore the confidence to be low.</p>
<p>Probe-R: We evaluate Probe-R using the mean confidence $\mu(c)$ and mean accuracy (acc) over all groups using equation 1, 2, and 3. We compare one image $x$ to two prompts $p_{1}$ and $p_{2}$.</p>
<p>$$ \begin{aligned} \left(c_{1}, c_{2}\right) &amp; =\sigma\left(f\left(x, p_{1}\right), f\left(x, p_{2}\right)\right) \ \mu(c) &amp; =\frac{1}{N} \sum_{i=1}^{N} c_{1}^{i} \ \operatorname{acc} &amp; =\left{\begin{array}{ll} 1 &amp; \text { if } c_{1}&gt;c_{2} \ 0 &amp; \text { otherwise } \end{array}\right. \end{aligned} $$</p>
<p>Logit scores from model $f$ are converted to softmax $\sigma$ predictions to measure the confidence $c_{i}$ of prompt $p_{i}$. Here $N$ denotes total number of images.</p>
<p>Probe-C: To measure image and text matching between two images, $x_{1}$ and $x_{2}$, and two prompts, $p_{1}$ and $p_{2}$, using logit output from a model $f$, we adopt metrics from [42] measuring a text score $(t)$, an image score $(i)$, and a group score $(g) . t$ measures the accuracy of the model selecting the correct prompt for a given image by equation 4, 5, and 6.</p>
<p>$$ \begin{aligned} &amp; t\left(p_{1}, x_{1}, p_{2}, x_{2}\right)=\left{\begin{array}{ll} 1 &amp; \text { if } f\left(p_{1}, x_{1}\right)&gt;f\left(p_{2}, x_{1}\right) \ &amp; \text { and } f\left(p_{2}, x_{2}\right)&gt;f\left(p_{1}, x_{2}\right) \ 0 &amp; \text { otherwise } \end{array}\right. \ &amp; i\left(p_{1}, x_{1}, p_{2}, x_{2}\right)=\left{\begin{array}{ll} 1 &amp; \text { if } f\left(p_{1}, x_{1}\right)&gt;f\left(p_{1}, x_{2}\right) \ &amp; \text { and } f\left(p_{2}, x_{2}\right)&gt;f\left(p_{2}, x_{1}\right) \ 0 &amp; \text { otherwise } \end{array}\right. \ &amp; g\left(p_{1}, x_{1}, p_{2}, x_{2}\right)=\left{\begin{array}{ll} 1 &amp; \text { if } t\left(p_{1}, x_{1}, p_{2}, x_{2}\right) \ &amp; \text { and } i\left(p_{1}, x_{1}, p_{2}, x_{2}\right) \ 0 &amp; \text { otherwise } \end{array}\right. \end{aligned} $$</p>
<p>Probe-B: We evaluate model reliance on either the cooccurrence of objects or background cues. Both tasks compare to both an original image $x_{0}$ and the original image with an added patch of the respective filler $\tilde{x_{0}}$ to take into account general robustness. $\tilde{x_{1}}$ will have either the background removed and replaced with a filler or have the background and all other objects replaced. The fillers are one of: "black," "gray," "noise," or a random "scene" that does not have objects. The metrics we use for comparisons are the mean average precision (mAP) for multi-object recognition precision, relative robustness $\gamma^{r}$ measuring the relative drop/increase in performance (equation 7, and mean change in mAP $\mu(\triangle(c))$ (equation 8) for the objects. $\gamma^{r}$ and $m A P$ evaluates how much the models rely on background context to accurately describe the scenario. We collect the similarity between the image $x_{n}$ and for each object $o$ placed in a prompt $p_{o} \in \mathbf{p}$. This results in a set of similarity scores for each object prompt which is used to calculate the score of model's change in confidence $\triangle c$.</p>
<p>$$ \begin{aligned} \gamma^{r} &amp; =1-\frac{h(x, \mathbf{p})-h(\tilde{x}, \mathbf{p})}{h(x, \mathbf{p})} \ \triangle \mathrm{c}(x, \tilde{x}) &amp; =\frac{1}{o} \sum_{i=1}^{o} f\left(x, p_{o}\right)-f\left(\tilde{x}, p_{o}\right) \end{aligned} $$</p>
<h2>4. Benchmark Results</h2>
<p>Here we go through the models we are evaluating in this benchmark and then report the results of those models on the proposed datasets Probe-R, Probe-C, and Probe-B.</p>
<p>Models We perform our experiments on ten recently developed and publicly available models: CLIP [31], FLAVA [38], ViLT [20], BridgeTower [47], BLIP [22], BLIP2 [23], OTTER [45], ALIGN [17], MetaCLIP [46], and SigLIP [51].</p>
<p>CLIP [31] is a dual-stream, modality specific model that has a visual and text encoder of equal length and limited modality interaction. It uses a contrastive loss between textimage pairs as its only multimodal signal.</p>
<p>FLAVA [38] is also a dual-stream encoder with an additional multimodal encoder that takes the ViT based [11] single-stream encoders, merges them, and co-attends. It performs unimodal training for single-stream encoders followed by multimodal training on a global contrastive loss, a masked multimodal modeling task (MMM), and an imagetext matching (ITM) loss.</p>
<p>ViLT [20] is a single-stream transformer that uses coattention between modalities. It concatenates word embeddings and linear projections of image patches as input to a pre-trained ViT [11]. It trains using an ITM loss, a masked language modeling (MLM) loss, and a word-patch alignment loss.</p>
<p>Bridgetower [47] uses a dual-stream encoder with a multimodal encoder that incorporates the single-stream encoders at multiple layers using cross-attention based "bridge layers." It uses a pre-trained ViT from CLIP as visual encoder, RoBERTa [25] as text encoder, and is trained with MLM and ITM losses.</p>
<p>BLIP [22] utilizes a mixture of encoder-decoder, and can operate in three functionalities: unimodal encoder, imagegrounded text encoder, and image-grounded text decoder.</p>
<p>BLIP2 [23] uses a querying transformer that's at first trained in vision-language representation learning stage then vision-to language generative learning stage. It is a trainable module bridging the gap between the frozen image encoder and LLM.</p>
<p>OTTER [45] improves upon CLIP by using online entropic optimal transport to efficiently learn image-text pairs.</p>
<p>ALIGN [17] is a dual-encoder which uses EfficientNet as image encoder and BERT as text encoder trained on a noisy dataset over one-billion image-text pairs.</p>
<p>MetaCLIP [46] follows CLIP by constructing metadata and carefully curating image-text pairs to imitate their dataset and training procedure.</p>
<p>SigLIP [51] improves upon CLIP by introducing a pairwise Sigmoid loss instead of standard contrastive learning.</p>
<h3>4.1. Relational Evaluation</h3>
<p>Models become confused when predicate is swapped, but more confident when object is swapped: The overall results for the relation evaluation benchmark are shown in Figure 3 (left) where it shows each model's accuracy and mean confidence $\mu(e)$ for matching the prompt to the an- chor image $X_{R_{1}}$. When comparing an image to a correct prompt and an incorrect prompt where the relation/predicate is swapped with one not likely nor present in the image, the model's $\mu(c)$ for the correct prompt compared to incorrect is very low. This may indicated that selected models become "confused" when the relation is switched, even if it is a highly unlikely relation to even exist between the two objects. When swapping objects, the object that is swapped $\widetilde{s}$ is one that is highly unlikely, making this task simple if the model has a consistent understanding of what relationships are possible. Model confidence is higher when the object is swapped versus when the predicate is swapped. This may indicate that models are less confused when the task is specific to object recognition, focusing more on objects rather than the relationships between them. This may additionally indicate they are not understanding prompts as a "whole" but rather parts to a whole. To visualize the differences between models, we plot some of their feature space in Figure 3 (right). We see very different structures for BridgeTower and ViLT which heavily rely on cross-attention and imagetext matching (ITM) when compared to FLAVA and CLIP.
Summary: BridgeTower and ViLT's performance indicates that co-attention is a method that can improve relational understanding. (1) This would indicate that both modality specific attention and co-attention simultaneously improves relational understanding. (2) When the predicate is swapped to something that violates expectation, the drop in confidence, regardless of accuracy, indicates that their performance may not be due to an underlying conceptual map. (3) When the subject is swapped, all models show better performance compared to predicate swapping, indicating they are focusing on objects less-so than their relations to each other.</p>
<h3>4.2. Compositional Evaluation</h3>
<p>Modality-specific attention and co-attention simultaneously greatly improves attribute-object relation understanding: Overall results for evaluating model understanding of composition-object relationships are shown in Figure 4 (left) with additional results in the Supplementary. We show the image, group, and object scores for when the object (Obj.) is switched and for when the composition (Comp.) is switched. When presented with two images and two captions where the composition is the same but the objects are different, all models other than BridgeTower and BLIP2 perform on average double the performance versus when the composition is switched. This discrepancy indicates typically models are relying more on object recognition when compositions are involved. BridgeTower and BLIP2's high performance indicates further support that a combination of modality-specific attention and crossattention in parallel improves the learning of underlying concepts.
Models stronger with more physical attributes like "ma-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Model's performance on relational understanding on Probe-R. (left) Radar plot showing accuracy and mean confidence $\mu(c)$ of different models. Here, the anchor image $X_{R_{1}}$ contains the relation $R_{1}=\langle s, r, o\rangle$, image $X_{O_{1}}$ contains $O_{1}=\langle s\rangle$. Prompts contain either the relation $P_{R_{1}}, P_{R_{2}}=\langle s, \bar{r}, o\rangle, P_{R_{3}}=\langle\bar{s}, r, o\rangle$, or $P_{O_{1}}=\langle s\rangle$. (right) TSNE plot of the feature space for image features for some models where the prompt with the predicate swapped is denoted by $P_{R_{2}}$ and the ground truth prompt denoted by $P_{R_{1}}$.
terials" compared to visual-related like "color": To better understand model failures when keeping the objects the same but swapping an attribute with its antonym, we categorized each attribute into 11 categories with results shown in Figure 4 (middle). Attribute details are presented in the supplementary. All models struggle with "visibility" related compositions. The best performance was within the "material" and "pattern" category.
Transformers and CNNs differ on which attributes they understand best: To compare backbone models, we average the image scores over CLIP backbone architectures in Figure 4 (right). Some noticeable patterns are that the CNN backbone models are better with "material," "pattern," and "texture" related compositions while ViT's are better at "color" and "shape." This finding aligns with the findings of [14] where they found ImageNet trained CNNs are biased towards texture.
Summary: (1) Models struggle with compositionality but are better with those most associated with objects such as "materials." (2) CNN based backbones may be better at recognizing texture and patterns while ViT backbones with color and shape. Surprisingly, (3) these models are typically better at matching captions given the image rather than text.</p>
<h3>4.3. Background Context Evaluation</h3>
<p>Models ignore what the background is replaced with, indicating little use of it: Overall results for evaluating model context understanding of background-object relationships are shown in Figure 5 and 6.</p>
<p>Figure 5 (left) shows the results averaged over filler type when only the background is removed. The most noticeable
change is when comparing the ground truth image to $\hat{x_{0}}$ and $\hat{x_{1}}$ as expected. Overall, models are slightly less robust to when the background is replaced with either Gaussian noise or scenery. However, if models had underlying understanding of what objects belong in what context, models should be less robust to scenery. This indicates they may not have conceptual maps about objects and their relationship to context.</p>
<p>More co-attention may result in greater trade-off between robustness and performance: Figure 5 (right) shows the overall results averaged over model type when only the background is removed. Similar to when looking at fillers, models are typically robust to background removal, indicating little use of context. ALIGN tends to improve when the background is removed. However, ViLT, ALIGN and MetaCLIP tend to be less robust when a patch is added to the image, noticeable even more so when the robustness between $\hat{x_{0}}$ and $\hat{x_{1}}$ is so high. This appears to be a trade-off between robustness and performance.</p>
<p>Some objects benefit from the presence of others, but most are better off without: Figure 6 shows the overall results for when the background and all other objects but one are removed, averaged over either filler (left) or model (middle). When averaging over filler, models appear to be more robust when detecting one object as opposed to multiple objects in an image. When averaging scores over models, $\gamma^{r}$ tends to be over 1 when comparing to the background removed image $\hat{x_{1}}$, indicating models improve when objects are in isolation. This case is especially prominent for ALIGN. This indicates that models may be distracted from background information rather than using it for object recognition. In order to better understand what</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Model's performance on compositional understanding on Probe-C. (left) The overall results for Probe-C showing the image, text, and group scores for when the object is swapped (<em>Obj.</em>) or when the composition is swapped (<em>Comp.</em>). (middle) Mean group score averaged across attribute categories. (right) CLIP scores averaged over different backbones.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Model's performance on contextual understanding on Probe-B for only background removal. (left) Mean results for replacing background with filler and (right) for each model averaged over fillers. Comparisons between the original <em>x</em><sub>0</sub>, original+random patch <em>x</em><sub>0</sub> and modified <em>x</em><sub>1</sub>. The metrics are <em>mAP</em> and <em>γ</em><sup>*</sup>.</p>
<p>objects models are using background with more than others, we categorize objects into sub-categories as shown in Figure 8. The object-types that models struggle with most appear to be large objects used in a common setting, such as "ovens" for appliances and "sink" for fixtures. This may indicate that there is some context used but for certain objects more than others.</p>
<p><strong>Summary</strong> (1) Models tend to not use context in order to recognize multiple objects but (2) for some individual objects, models do use context. (3) These models are typically robust to a change in background where models like ViLT, ALIGN, and BridgeTower are more susceptible to a particular patch being changed. (4) When objects are placed in random scenery that violates expectation, models still perform similarly to when the original background is there. This may indicate that overall, models are not learning conceptual maps relating objects to their context.</p>
<h1>5. Finetuning for better conceptual understanding</h1>
<p>Dual-stream encoders like CLIP and FLAVA allow unimodal feature representations that can be extracted and used for a variety of downstream tasks. Improving models that do not require paired input would provide greater value and stronger representations. To explore this idea, we finetune (FT) CLIP ViT-B/32 on a new dataset inspired by this benchmark called RelComp. The new dataset RelComp for attribute-object and object-object relations is based on MS COCO [24] and VisualGenome [21] and has no overlap between the benchmark datasets.</p>
<p>We propose using selective negative and positive pairing based on attribute and predicate swaps and finetune using image-text matching (ITM) loss and a contrastive loss (C) [31, 38] for finetuning (see Figure 7). We linearly interpo</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Model's performance on contextual understanding on Probe-B on background and all but one object removal.(Left): Results for when the background and all other objects are replaced with a filler $\tilde{x_{1}}$, compared to the original $x_{0}$, and (right) original+random patch $\tilde{x_{0}}$.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Exploratory finetuning training scheme for CLIP. Image-text matching (ITM) is used as a triplet loss whose pairings vary depending on if it is a compositional or a relational task. A contrastive loss is used to maintain general representations.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Performance on contextual understanding for certain object categories. Mean change in confidence (µ(△c)) from the ground truth $x_{0}$ to the modified $\tilde{x}_{1}$, where the background and other objects are removed.</p>
<p>late the original CLIP weights with our FT weights using an alpha= 0.2 to prevent "catastrophic forgetting" [16, 44]. We call this "CLIP Patched" and finetune visual-encoder only (V), text-encoder only (T), or both (VT). More details about losses, implementation, and dataset are in the Supple-</p>
<p>Table 3. Performance on finetuned and patched CLIP on proposed RelComp dataset. ImageNet accuracy is shown to measure the drift from the original CLIP space. RelComp and Probe-C/R respectively report image score and mean accuracy for the correct image-to-prompt matching.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>ImageNet</th>
<th>RelComp</th>
<th>Probe-C</th>
<th>Probe-R</th>
</tr>
</thead>
<tbody>
<tr>
<td>ViLT</td>
<td>–</td>
<td>76.00</td>
<td>90.78</td>
<td>69.00</td>
</tr>
<tr>
<td>BridgeTower</td>
<td>–</td>
<td>85.00</td>
<td>90.06</td>
<td>82.20</td>
</tr>
<tr>
<td>FLAVA</td>
<td>56.83</td>
<td>47.12</td>
<td>83.85</td>
<td>68.29</td>
</tr>
<tr>
<td>CLIP ViT B32</td>
<td>63.60</td>
<td>51.93</td>
<td>88.15</td>
<td>53.52</td>
</tr>
<tr>
<td>CLIP Patched (T)</td>
<td>57.85</td>
<td>67.85</td>
<td>89.49</td>
<td>71.14</td>
</tr>
<tr>
<td>CLIP Patched (V)</td>
<td>61.45</td>
<td>54.66</td>
<td>89.81</td>
<td>61.40</td>
</tr>
<tr>
<td>CLIP Patched (VT)</td>
<td>54.61</td>
<td>64.27</td>
<td>90.30</td>
<td>71.20</td>
</tr>
</tbody>
</table>
<p>mentary.</p>
<p>Overall results for our exploratory experiment are shown in Table 3. We observe drift as measured by ImageNet accuracy, even when patching. When finetuning using the visual-encoder only, the drift is less pronounced, but so is the improvement on RelComp. The largest increase is seen with FT text encoder only. This may indicate that for non-cross-attention models, text is more important for conceptual mapping. Our findings indicate that by using selective</p>
<p>negative sampling we can enforce compositional and relational learning without extensive co-attention and computational complexity.</p>
<h2>6. Conclusions</h2>
<p>In this benchmark we evaluated large visual-language $(\mathrm{V}+\mathrm{L})$ models on relational, compositional, and contextual understanding with three new datasets: Probe-C, Probe-R, and Probe-B. For compositional understanding, we observe (1) models struggle with compositionality. (2) CNN backbones may be better at recognizing texture and patterns while ViT backbones are with color and shape. For relational understanding, we observe (1) both modality specific attention and co-attention in parallel improves relational understanding. (2) An expectation violating predicate swap surfaces the lack of a conceptual map through drop in confidence. For contextual understanding we observe (1) models mostly tend to not use context in order to recognize multiple objects. (2) When objects are placed in random scenery that violates expectation, model performance is unchanged, indicating a lack of conceptual map of context. When trying to improve CLIP, the dual-encoder with no cross-attention, by finetuning on our proposed selective negatives training paradigm on the proposed RelComp dataset, (1) we find that there is a small drop in classification performance, but (2) an improvement on Probe-R, Probe-C, and RelComp is observed, indicating an improvement in relational and compositional learning. We hope these insights will help drive future work on building V+L models that better "understand."</p>
<h2>References</h2>
<p>[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.1
[2] Renee Baillargeon. Object permanence in 31/2-and 41/2-month-old infants. Developmental psychology, 23(5):655, 1987. 4
[3] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. " O'Reilly Media, Inc.", 2009. 1
[4] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. " O'Reilly Media, Inc.", 2009. 11
[5] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188, 2023. 2
[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558-3568, 2021. 7
[7] Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image similarity through ranking. Journal of Machine Learning Research, 11(3), 2010. 7
[8] for Social Security Administration Disability Determinations; Board on the Health of Select Populations; Institute of Medicine Committee on Psychological Testing, Including Validity Testing. Psychological testing in the service of disability determination, 2015. 1
[9] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. 7
[10] Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. Why is winoground hard? investigating failures in visuolinguistic compositionality. arXiv preprint arXiv:2211.00768, 2022. 2
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 5
[12] Sivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar Panda, Roei Herzig, Eli Schwartz, Donghyun Kim, Raja Giryes, Rogerio Feris, Shimon Ullman, and Leonid Karlinsky. Teaching structured vision\&amp;language concepts to vision\&amp;language models, 2023. 2
[13] Steven M. Frankland and Joshua D. Greene. Concepts and compositionality: In search of the brain's language of thought. Annual Review of Psychology, 71(1):273-303, 2020. 1
[14] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018. 6
[15] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality, 2023. 2
[16] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. NeurIPs, 2022. 8, 2, 7, 9,10
[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision, 2021. 5
[18] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2
[19] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Text encoders bottleneck compositionality in contrastive visionlanguage models, 2023. 2</p>
<p>[20] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583-5594. PMLR, 2021. 5, 7, 10
[21] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalanditis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016. 3, 7, 1, 9,11
[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. 5
[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 5
[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014. 3, 7, 1, 9, 11
[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 5
[26] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally?, 2023. 2
[27] Richard E Mayer. Models for understanding. Review of educational research, 59(1):43-64, 1989. 1
[28] Genevieve Patterson and James Hays. Coco attributes: Attributes for people, animals, and objects. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 85-100. Springer, 2016. 2
[29] Luis S Piloto, Ari Weinstein, Peter Battaglia, and Matthew Botvinick. Intuitive physics learning in a deep-learning model inspired by developmental psychology. Nature human behaviour, 6(9):1257-1267, 2022. 1, 4
[30] Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In 2009 IEEE conference on computer vision and pattern recognition, pages 413-420. IEEE, 2009. 3, 1, 11
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 1, 3, 5, 7, 10
[32] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with contextaware prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18082-18091, 2022. 1
[33] Arnaud ROUGETET. Kaggle landscape dataset. https : / / www . kaggle . com / datasets / arnaud5@/landscape-pictures/. 11
[34] Arnaud Rougetet. Landscape pictures. https://www. kaggle.com/datasets/arnaud5@/landscapepictures, 2021. Accessed: February 16, 2023. 3, 1
[35] Pritish Sahu, Michael Cogswell, Yunye Gong, and Ajay Divakaran. Unpacking large language models with conceptual consistency. arXiv preprint arXiv:2209.15093, 2022. 1, 2
[36] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 7
[37] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, Melbourne, Australia, 2018. Association for Computational Linguistics. 7
[38] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15638-15650, 2022. 3, 5, 7
[39] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI conference on artificial intelligence, 2017. 2
[40] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2443-2449, 2021. 7
[41] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Соттип. АСМ, 59(2):64-73, 2016. 7
[42] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visiolinguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238-5248, 2022. 2, 4
[43] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clipdriven referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11686-11695, 2022. 1
[44] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer</p>
<p>Vision and Pattern Recognition, pages 7959-7971, 2022. 8, 10
[45] Bichen Wu, Ruizhe Cheng, Peizhao Zhang, Tianren Gao, Peter Vajda, and Joseph E. Gonzalez. Data efficient languagesupervised zero-shot recognition with optimal transport distillation, 2023. 5
[46] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data, 2023. 5
[47] Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, and Nan Duan. Bridge-tower: Building bridges between encoders in vision-language representation learning. arXiv preprint arXiv:2206.08657, 2022. 1, 5, 7
[48] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78, 2014. 11
[49] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bag-of-words models, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. 2
[50] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it?, 2023. 2
[51] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. 5
[52] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. Vlchecklist: Evaluating pre-trained vision-language models with objects, attributes and relations, 2023. 2
[53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816-16825, 2022. 1</p>
<h1>Probing Conceptual Understanding of Large Visual-Language Models</h1>
<h2>Supplementary Material</h2>
<p>The supplementary will provide additional details about our proposed datasets, finetuning CLIP and the models evaluated on in this benchmark. Additional details and results for Probe-R, Probe-C and Probe-B are in Section 7. We provide more details about finetuning CLIP and additional results in Section 8. In Section 9 we provide additional details about the models we evaluated in this benchmark.</p>
<h2>7. Datasets Details</h2>
<p>In this section we will provide additional results for the different dataset benchmarks.</p>
<h3>7.1. Probe-R: Relational Understanding</h3>
<p>This dataset was created using Visual Genome (VG) [21]. To collect unlikely " $&lt;$ subject, predicate, object $&gt;$ " triplets, we first cleaned the relationship aliases. This was done by mapping repeated aliases that meant the same thing into one, for example "are standing next to" would become "standing next to". This was done to reduce the space to map all objects to aliases they have been associated with as well as to confirm they have not been associated with one similar. We then collect all the objects each cleaned alias was associated with using regex and NLTK part-of-speech (POS) tagging [3]. Using these object collections, we iterated through 100, 000 VG annotations of $R_{1}=\left\langle s_{1}, r_{1}, o_{1}\right\rangle$ to (1) replace the existing alias with an alias that the current subject and object are not associated with as swap $\left(R_{2}=\left\langle s_{1}, \bar{r}<em 1="1">{1}, o</em>}\right\rangle\right)$ and (2) replace the existing subject with an object that is not associated with the current alias $\left(R_{3}=\left\langle\bar{s<em 1="1">{1}, r</em>$.}, o_{1}\right\rangle\right)$. To better collect images with specific objects in them, we iterated through VG and generated a mapping of each image ID to all objects present in the image according to the relationships annotations. We extract positive images $X_{O_{1}}$ that do not have the relation but have the subject and no other objects present in the anchor image $X_{R_{1}</p>
<p>The results for all models for the Probe-R benchmark are shown in Table 4. We include CLIP models we finetuned on RelComp, training either the text encoder (T), visual encoder (V) or both encoders (VT). Training only the text encoder seems to have the highest improvement, but as mentioned in the paper, the largest occurrence of "catastrophic forgetting" when evaluated on ImageNet. A TSNE plot of model features that includes CLIP Patched (VT) is shown in Figure 9. In black we have the image features, in red we have the predicate swapped text features $\left(P_{R_{2}}\right)$, and in green we have the ground truth relation text features $\left(P_{R_{1}}\right)$. This finetuned and patched version appears to have
tighter clusters compared to the original CLIP model.</p>
<h3>7.2. Probe-C: Compositional Understanding</h3>
<p>This dataset was generated using MSCOCO [24]. To guarantee that the images had no similarity or overlap, we focused on using antonyms of select attributes. We started by using NLTK POS [3] to find adjective-noun pairs. We then manually cleaned and extracted the adjectives to guarantee the attribute is a visual one such as "red" or "young" as opposed to a subjective one such as "hungry" or "thirsty". While these are useful attributes, we are primarily interested in visual perception as opposed to subjective inference. We then iterated through all images and mapped each attribute to their corresponding image IDs, and we did the same with objects. Using this collection, we were able to create groups of pairs based on either swapping the attribute to one of its antonyms or swapping the object with one that has the same attribute.</p>
<p>The overall results for Probe-C for all models is in Table 5. The mappings we used to categorize different attributes is shown in Table 6, these were manually generated. A visual break down of different model performances for each attribute is shown in Figure 10. From there, you can see the changes in score based on whether it is matching the caption given the image versus given text. We also see that most models struggle with "visibility" and often "texture".</p>
<h3>7.3. Probe-B: Context Understanding</h3>
<p>In set 1, for each image we remove the background using segmentation masks from original annotations. We replace the background with 1 of four fillers: black, gray, Gaussian noise, or a random scene. Random scenery was collected from the Indoor Scenes Dataset [30] and the Kaggle Landscape dataset [34]. These images were manually filtered to ensure none of the 80 MSCOCO classes were present. The total collection is 31,745 images with 4 fillings each for a total of 126,980 images. We filtered images based on a threshold for how much background can be removed to ensure that some context was actually removed. In set 2, for each image we remove all other objects and the background using segmentation masks. In this case, $x_{0}$ is the image with all objects with just the background removed while $\tilde{x_{1}}$ is the image with just one object remaining and all other objects and the background removed. This allows us to isolate whether it is the other objects compared to background removal. Like in set 1, we replace them with the different possible fillers. Images are chosen if they do not have overlapping bounding boxes and if their object area is over a threshold to allow for better visibility. Prompts for</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. TSNE plots of image features (black) and text features from the Probe-R. Text features are prompts generated from either the ground truth relation R1 (green) or the relation with the predicate swapped to an unrealistic one R2 (red). Both ViLT and BridgeTower rely on cross-attention heavily, showing the impact on the feature space. While the features for the other models are more visibly in the same space, ViLT and BridgeTower generally show higher performance. CLIP patched is finetuning both visual and text encoders using RelComp and patching with an alpha of 0.2 [16]. CLIP ViT is ViT/L-14@336px while CLIP CNN is RN50x4.</p>
<p>Table 4. Overall results for relation evaluation. The anchor image XR1 contains the relation R1 = 〈s, r, o〉, image XO1 contains O1 = 〈s〉. Prompts contain either the relation PR1, PR2 = 〈s, P, o〉, PR3 = 〈s, r, o〉, PO1 = 〈s〉, or PO3 = 〈s〉. The mean confidence µ(c) is for the correct prompt to image. Models with CLIP Patched are those we finetuned on our training dataset RelComp. We finetuned either the text encoder (T), the visual encoder (V) or both (VT). Models show higher performance for when objects are switched but lower performance when the relation is switched, showing the models are confused.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>XR1</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>XO1</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>PR1 vs. PR3</td>
<td></td>
<td>PR1 vs. PR2</td>
<td></td>
<td>PR1 vs. PO1</td>
<td></td>
<td>PR1 vs. PR3</td>
</tr>
<tr>
<td></td>
<td>µ(c)</td>
<td>Acc</td>
<td>µ(c)</td>
<td>Acc</td>
<td>µ(c)</td>
<td>Acc</td>
<td>µ(c)</td>
</tr>
<tr>
<td>CLIP RN50</td>
<td>69.77</td>
<td>72.14</td>
<td>51.33</td>
<td>51.13</td>
<td>61.19</td>
<td>61.69</td>
<td>78.44</td>
</tr>
<tr>
<td>CLIP ViT L/14</td>
<td>71.59</td>
<td>73.68</td>
<td>52.44</td>
<td>52.59</td>
<td>59.09</td>
<td>58.67</td>
<td>84.17</td>
</tr>
<tr>
<td>CLIP ViT-B/16</td>
<td>71.08</td>
<td>73.40</td>
<td>52.84</td>
<td>53.37</td>
<td>61.69</td>
<td>62.07</td>
<td>79.62</td>
</tr>
<tr>
<td>CLIP ViT/B-32</td>
<td>69.00</td>
<td>71.21</td>
<td>53.02</td>
<td>53.53</td>
<td>58.83</td>
<td>58.56</td>
<td>82.21</td>
</tr>
<tr>
<td>CLIP ViT</td>
<td>72.09</td>
<td>74.27</td>
<td>53.52</td>
<td>53.97</td>
<td>59.53</td>
<td>59.14</td>
<td>83.97</td>
</tr>
<tr>
<td>CLIP RN101</td>
<td>70.62</td>
<td>73.28</td>
<td>54.01</td>
<td>55.11</td>
<td>60.66</td>
<td>60.83</td>
<td>79.08</td>
</tr>
<tr>
<td>CLIP RN50x64</td>
<td>72.79</td>
<td>74.79</td>
<td>56.66</td>
<td>58.03</td>
<td>64.10</td>
<td>64.88</td>
<td>78.10</td>
</tr>
<tr>
<td>CLIP CNN</td>
<td>72.71</td>
<td>75.59</td>
<td>56.35</td>
<td>58.14</td>
<td>62.31</td>
<td>62.81</td>
<td>78.29</td>
</tr>
<tr>
<td>CLIP RN50x16</td>
<td>73.91</td>
<td>76.57</td>
<td>58.08</td>
<td>60.52</td>
<td>59.80</td>
<td>59.79</td>
<td>83.03</td>
</tr>
<tr>
<td>CLIP Patched (V)</td>
<td>78.58</td>
<td>81.41</td>
<td>59.36</td>
<td>62.27</td>
<td>66.56</td>
<td>68.07</td>
<td>81.32</td>
</tr>
<tr>
<td>FLAVA</td>
<td>76.79</td>
<td>79.09</td>
<td>64.65</td>
<td>68.29</td>
<td>64.40</td>
<td>65.56</td>
<td>84.19</td>
</tr>
<tr>
<td>ViLT</td>
<td>76.41</td>
<td>78.45</td>
<td>64.77</td>
<td>69.00</td>
<td>54.84</td>
<td>54.78</td>
<td>94.23</td>
</tr>
<tr>
<td>CLIP Patched (VT)</td>
<td>80.56</td>
<td>84.46</td>
<td>64.53</td>
<td>71.12</td>
<td>67.63</td>
<td>70.07</td>
<td>81.74</td>
</tr>
<tr>
<td>CLIP Patched (T)</td>
<td>82.37</td>
<td>86.25</td>
<td>66.28</td>
<td>72.55</td>
<td>67.93</td>
<td>70.51</td>
<td>79.76</td>
</tr>
<tr>
<td>BridgeTower</td>
<td>83.03</td>
<td>89.01</td>
<td>72.93</td>
<td>82.04</td>
<td>71.73</td>
<td>78.90</td>
<td>76.58</td>
</tr>
<tr>
<td>BLIP</td>
<td>62.38</td>
<td>69.2</td>
<td>56.8</td>
<td>65.02</td>
<td>48.31</td>
<td>46.68</td>
<td>76.65</td>
</tr>
<tr>
<td>BLIP2</td>
<td>70.82</td>
<td>81.57</td>
<td>59.31</td>
<td>68.39</td>
<td>47.26</td>
<td>41.9</td>
<td>78.06</td>
</tr>
<tr>
<td>OTTER</td>
<td>49.87</td>
<td>42.18</td>
<td>50.02</td>
<td>52.33</td>
<td>49.6</td>
<td>24.48</td>
<td>50.49</td>
</tr>
<tr>
<td>ALIGN</td>
<td>75.68</td>
<td>79.81</td>
<td>56.88</td>
<td>60.34</td>
<td>65.35</td>
<td>66.24</td>
<td>73.42</td>
</tr>
<tr>
<td>MetaCLIP</td>
<td>72.66</td>
<td>74.53</td>
<td>52.72</td>
<td>53.42</td>
<td>54.93</td>
<td>54.16</td>
<td>88.68</td>
</tr>
<tr>
<td>SigLIP</td>
<td>73.88</td>
<td>75.78</td>
<td>54.14</td>
<td>55.31</td>
<td>63.86</td>
<td>63.92</td>
<td>82.77</td>
</tr>
</tbody>
</table>
<p>Set 2 only include objects not present in the original image and the target object.</p>
<p>To better compare CLIP backbones, Figure 11 shows a comparison between the change in confidence from a patched image x́0 to the image where all other objects and background x́1 is removed aggregated over CLIP backbones. Table 8 shows what objects are assigned to which category and how many samples are present in the annotations. The main differences are in objects they struggle with by how much and in which order.</p>
<p>Table 5. Overall results for the compositional evaluation on select models with highest scores in bold and second highest underlined. Mean confidence for the correct prompt-to-image is $\mu(c)$. CLIP ViT is ViT/L-14@336px while CLIP CNN is RN50x4.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Composition Switch</th>
<th></th>
<th></th>
<th></th>
<th>Object Switch</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mu(c) \uparrow$</td>
<td>Image $\uparrow$</td>
<td>Text $\uparrow$</td>
<td>Group $\uparrow$</td>
<td>$\mu(c) \uparrow$</td>
<td>Image $\uparrow$</td>
<td>Text $\uparrow$</td>
<td>Group $\uparrow$</td>
</tr>
<tr>
<td>CLIP ViT</td>
<td>69.69</td>
<td>33.06</td>
<td>52.82</td>
<td>26.64</td>
<td>88.15</td>
<td>61.96</td>
<td>81.89</td>
<td>58.05</td>
</tr>
<tr>
<td>CLIP RN50</td>
<td>69.47</td>
<td>33.41</td>
<td>54.60</td>
<td>26.92</td>
<td>87.00</td>
<td>61.40</td>
<td>80.17</td>
<td>56.81</td>
</tr>
<tr>
<td>CLIP ViT-B/16</td>
<td>69.23</td>
<td>34.29</td>
<td>52.23</td>
<td>26.94</td>
<td>88.02</td>
<td>63.53</td>
<td>81.44</td>
<td>59.12</td>
</tr>
<tr>
<td>CLIP ViT L/14</td>
<td>69.41</td>
<td>33.73</td>
<td>52.36</td>
<td>27.01</td>
<td>87.89</td>
<td>61.93</td>
<td>81.31</td>
<td>57.86</td>
</tr>
<tr>
<td>CLIP RN101</td>
<td>69.24</td>
<td>34.95</td>
<td>51.82</td>
<td>27.42</td>
<td>86.99</td>
<td>61.75</td>
<td>80.58</td>
<td>57.46</td>
</tr>
<tr>
<td>CLIP RN50x64</td>
<td>70.44</td>
<td>35.21</td>
<td>52.89</td>
<td>27.95</td>
<td>87.75</td>
<td>63.09</td>
<td>80.55</td>
<td>58.27</td>
</tr>
<tr>
<td>CLIP ViT/B-32</td>
<td>69.79</td>
<td>34.71</td>
<td>53.85</td>
<td>27.96</td>
<td>87.75</td>
<td>62.01</td>
<td>80.92</td>
<td>57.65</td>
</tr>
<tr>
<td>CLIP RN50x16</td>
<td>69.77</td>
<td>35.51</td>
<td>53.24</td>
<td>28.07</td>
<td>87.91</td>
<td>63.12</td>
<td>82.23</td>
<td>59.38</td>
</tr>
<tr>
<td>CLIP CNN</td>
<td>69.75</td>
<td>36.07</td>
<td>54.56</td>
<td>28.79</td>
<td>87.24</td>
<td>61.29</td>
<td>81.24</td>
<td>57.06</td>
</tr>
<tr>
<td>FLAVA</td>
<td>67.45</td>
<td>60.93</td>
<td>39.65</td>
<td>33.09</td>
<td>83.85</td>
<td>82.66</td>
<td>70.08</td>
<td>65.37</td>
</tr>
<tr>
<td>CLIP Patched (T)</td>
<td>71.94</td>
<td>40.96</td>
<td>58.79</td>
<td>33.83</td>
<td>89.58</td>
<td>68.81</td>
<td>84.36</td>
<td>65.19</td>
</tr>
<tr>
<td>CLIP Patched (V)</td>
<td>73.65</td>
<td>42.30</td>
<td>59.10</td>
<td>34.48</td>
<td>89.79</td>
<td>66.17</td>
<td>84.00</td>
<td>62.45</td>
</tr>
<tr>
<td>CLIP Patched (VT)</td>
<td>73.65</td>
<td>44.53</td>
<td>61.92</td>
<td>37.18</td>
<td>90.30</td>
<td>70.01</td>
<td>85.41</td>
<td>66.83</td>
</tr>
<tr>
<td>ViLT</td>
<td>79.02</td>
<td>53.74</td>
<td>66.84</td>
<td>46.65</td>
<td>90.78</td>
<td>73.82</td>
<td>85.88</td>
<td>70.26</td>
</tr>
<tr>
<td>BridgeTower</td>
<td>81.88</td>
<td>65.95</td>
<td>75.02</td>
<td>59.28</td>
<td>90.05</td>
<td>77.44</td>
<td>87.63</td>
<td>74.54</td>
</tr>
<tr>
<td>BLIP</td>
<td>73.1</td>
<td>65.64</td>
<td>70.91</td>
<td>56.74</td>
<td>81.59</td>
<td>74.24</td>
<td>81.37</td>
<td>47.26</td>
</tr>
<tr>
<td>BLIP2</td>
<td>70.98</td>
<td>62.55</td>
<td>72.03</td>
<td>54.69</td>
<td>81.8</td>
<td>74.01</td>
<td>81.31</td>
<td>67.5</td>
</tr>
<tr>
<td>OTTER</td>
<td>50.05</td>
<td>12.71</td>
<td>22.24</td>
<td>7.14</td>
<td>50.21</td>
<td>31.17</td>
<td>24.5</td>
<td>14.62</td>
</tr>
<tr>
<td>ALIGN</td>
<td>71.85</td>
<td>61.48</td>
<td>39.16</td>
<td>33.13</td>
<td>87.9</td>
<td>83.6</td>
<td>68.79</td>
<td>65.08</td>
</tr>
<tr>
<td>MetaCLIP</td>
<td>71.56</td>
<td>36.55</td>
<td>56.01</td>
<td>29.63</td>
<td>87.31</td>
<td>66.02</td>
<td>79.41</td>
<td>60.53</td>
</tr>
<tr>
<td>SigLIP</td>
<td>74.5</td>
<td>40.59</td>
<td>60.82</td>
<td>33.59</td>
<td>90.1</td>
<td>70.55</td>
<td>83.65</td>
<td>66.64</td>
</tr>
</tbody>
</table>
<p>Table 6. The attributes that belong to each category for the compositional analysis on specific attributes in Probe-C.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Groups</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">age</td>
<td style="text-align: left;">[young, old, new]</td>
<td style="text-align: left;">2,051</td>
</tr>
<tr>
<td style="text-align: left;">color</td>
<td style="text-align: left;">[greyscale, coloured, septa, reddish, bronze, greenish, green, turquoise, blue, tan, red, white, silver, purple, gold, pink, navy, brown, teal, gray,</td>
<td style="text-align: left;">39,971</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">black, yellow, grey, golden, camo, pinkish, beige, orange, blonde]</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">expression</td>
<td style="text-align: left;">[happy, unhappy, smiling, laughing, smiley, sad]</td>
<td style="text-align: left;">2,088</td>
</tr>
<tr>
<td style="text-align: left;">gender</td>
<td style="text-align: left;">[male, female]</td>
<td style="text-align: left;">2,346</td>
</tr>
<tr>
<td style="text-align: left;">material</td>
<td style="text-align: left;">[tin, aluminum, cloth, gravel, unpaved, wooden, stainless, marble, metallic, metal, grassy, porcelain, wooded, pebbled]</td>
<td style="text-align: left;">3,875</td>
</tr>
<tr>
<td style="text-align: left;">pattern</td>
<td style="text-align: left;">[checkered, patterned, striped, spotted, plaid, stripped, checkerboard]</td>
<td style="text-align: left;">3,08</td>
</tr>
<tr>
<td style="text-align: left;">shape</td>
<td style="text-align: left;">[triangular, flat, circular, triangle, oval, round, dotted, rectangular, square]</td>
<td style="text-align: left;">1,164</td>
</tr>
<tr>
<td style="text-align: left;">size</td>
<td style="text-align: left;">[bulky, long, thin, large, big, tall, short, small, huge, tiny, giant, little, chubby, pudgy]</td>
<td style="text-align: left;">16,575</td>
</tr>
<tr>
<td style="text-align: left;">texture</td>
<td style="text-align: left;">[smooth, fluffy, fuzzy, dry, wet, rusty, bald, hairy, stony]</td>
<td style="text-align: left;">1,090</td>
</tr>
<tr>
<td style="text-align: left;">visibility</td>
<td style="text-align: left;">[shiny, unclear, sun, nightime, blurry, shadowy, lit, shady, light, darkened, hazy, dark, barren, cloudy, clear, sunlit, bright, foggy, rainy,</td>
<td style="text-align: left;">10,454</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">sparkling]</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Performance on compositional understandingMean image, text and group scores for a subset of models. Models are typically better matching a caption given an image rather than the reverse.</p>
<p>Table 7. Mean image, text and group scores for each category of attributes for each model.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>age</th>
<th></th>
<th></th>
<th>color</th>
<th></th>
<th></th>
<th>expression</th>
<th></th>
<th></th>
<th>gender</th>
<th></th>
<th></th>
<th>material</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
</tr>
<tr>
<td>CLIP RN50</td>
<td>47.74</td>
<td>68.84</td>
<td>39.20</td>
<td>18.18</td>
<td>13.64</td>
<td>0.00</td>
<td>21.07</td>
<td>35.45</td>
<td>13.14</td>
<td>34.71</td>
<td>60.75</td>
<td>30.18</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP RN50x64</td>
<td>53.77</td>
<td>82.41</td>
<td>49.25</td>
<td>4.55</td>
<td>13.64</td>
<td>0.00</td>
<td>20.08</td>
<td>36.28</td>
<td>13.97</td>
<td>39.64</td>
<td>70.61</td>
<td>36.29</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>CLIP RN101</td>
<td>48.74</td>
<td>70.35</td>
<td>39.20</td>
<td>18.18</td>
<td>63.64</td>
<td>18.18</td>
<td>37.60</td>
<td>43.97</td>
<td>24.30</td>
<td>32.35</td>
<td>55.23</td>
<td>27.02</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT</td>
<td>53.27</td>
<td>71.36</td>
<td>46.73</td>
<td>18.18</td>
<td>77.27</td>
<td>18.18</td>
<td>20.91</td>
<td>38.26</td>
<td>15.62</td>
<td>45.17</td>
<td>69.03</td>
<td>40.43</td>
<td>100.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT-B/16</td>
<td>45.73</td>
<td>76.88</td>
<td>40.70</td>
<td>27.27</td>
<td>77.27</td>
<td>22.73</td>
<td>28.84</td>
<td>66.94</td>
<td>24.05</td>
<td>41.03</td>
<td>62.92</td>
<td>35.90</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT L/14</td>
<td>53.77</td>
<td>75.88</td>
<td>47.74</td>
<td>31.82</td>
<td>72.73</td>
<td>27.27</td>
<td>18.43</td>
<td>16.28</td>
<td>11.82</td>
<td>41.62</td>
<td>69.43</td>
<td>37.67</td>
<td>100.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP CNN</td>
<td>57.29</td>
<td>76.38</td>
<td>49.75</td>
<td>31.82</td>
<td>68.18</td>
<td>31.82</td>
<td>25.21</td>
<td>47.93</td>
<td>19.01</td>
<td>37.67</td>
<td>56.80</td>
<td>30.77</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP Patched (T)</td>
<td>41.71</td>
<td>79.40</td>
<td>37.69</td>
<td>31.82</td>
<td>68.18</td>
<td>31.82</td>
<td>37.27</td>
<td>61.98</td>
<td>31.74</td>
<td>46.94</td>
<td>54.83</td>
<td>36.09</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>CLIP ViT/B-32</td>
<td>40.20</td>
<td>74.37</td>
<td>35.68</td>
<td>18.18</td>
<td>13.64</td>
<td>4.55</td>
<td>23.22</td>
<td>58.02</td>
<td>20.08</td>
<td>39.05</td>
<td>60.75</td>
<td>33.93</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>CLIP Patched (V)</td>
<td>44.72</td>
<td>76.38</td>
<td>39.70</td>
<td>40.91</td>
<td>90.91</td>
<td>40.91</td>
<td>31.65</td>
<td>60.74</td>
<td>27.19</td>
<td>53.06</td>
<td>60.16</td>
<td>43.20</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>CLIP Patched (VT)</td>
<td>48.74</td>
<td>72.86</td>
<td>42.21</td>
<td>50.00</td>
<td>81.82</td>
<td>40.91</td>
<td>34.55</td>
<td>61.57</td>
<td>29.67</td>
<td>52.47</td>
<td>57.00</td>
<td>42.60</td>
<td>100.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>FLAVA</td>
<td>73.87</td>
<td>47.24</td>
<td>43.22</td>
<td>86.36</td>
<td>81.82</td>
<td>68.18</td>
<td>39.67</td>
<td>24.88</td>
<td>10.33</td>
<td>65.09</td>
<td>42.60</td>
<td>37.67</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>CLIP RN50x16</td>
<td>52.76</td>
<td>83.92</td>
<td>49.75</td>
<td>9.09</td>
<td>13.64</td>
<td>9.09</td>
<td>21.98</td>
<td>42.98</td>
<td>14.71</td>
<td>40.83</td>
<td>62.72</td>
<td>35.11</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>BridgeTower</td>
<td>68.34</td>
<td>80.40</td>
<td>65.33</td>
<td>95.45</td>
<td>90.91</td>
<td>90.91</td>
<td>32.23</td>
<td>52.07</td>
<td>24.79</td>
<td>57.79</td>
<td>64.89</td>
<td>49.90</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>ViLT</td>
<td>38.19</td>
<td>53.27</td>
<td>28.64</td>
<td>90.91</td>
<td>95.45</td>
<td>86.36</td>
<td>4.79</td>
<td>18.43</td>
<td>1.57</td>
<td>57.59</td>
<td>67.46</td>
<td>51.28</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
</tr>
<tr>
<td>Model</td>
<td>pattern</td>
<td></td>
<td></td>
<td>shape</td>
<td></td>
<td></td>
<td>size</td>
<td></td>
<td></td>
<td>texture</td>
<td></td>
<td></td>
<td>visibility</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
<td>Image</td>
<td>Text</td>
<td>Group</td>
</tr>
<tr>
<td>CLIP RN50</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>3.69</td>
<td>23.88</td>
<td>2.56</td>
<td>21.21</td>
<td>47.95</td>
<td>17.11</td>
<td>0.00</td>
<td>20.69</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP RN50x64</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>9.46</td>
<td>22.92</td>
<td>6.25</td>
<td>18.83</td>
<td>33.40</td>
<td>12.37</td>
<td>3.45</td>
<td>20.69</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP RN101</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>9.13</td>
<td>27.88</td>
<td>5.45</td>
<td>20.96</td>
<td>37.63</td>
<td>12.72</td>
<td>3.45</td>
<td>31.03</td>
<td>3.45</td>
<td>50.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>10.26</td>
<td>38.94</td>
<td>6.41</td>
<td>24.13</td>
<td>42.98</td>
<td>18.85</td>
<td>6.90</td>
<td>6.90</td>
<td>0.00</td>
<td>50.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT-B/16</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
<td>8.01</td>
<td>21.63</td>
<td>3.21</td>
<td>15.69</td>
<td>28.33</td>
<td>7.73</td>
<td>6.90</td>
<td>17.24</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT L/14</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
<td>13.46</td>
<td>37.02</td>
<td>8.49</td>
<td>24.84</td>
<td>45.11</td>
<td>19.34</td>
<td>3.45</td>
<td>6.90</td>
<td>0.00</td>
<td>50.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP CNN</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>12.82</td>
<td>39.58</td>
<td>8.81</td>
<td>17.51</td>
<td>33.43</td>
<td>10.62</td>
<td>0.00</td>
<td>6.90</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP Patched (T)</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>6.25</td>
<td>22.92</td>
<td>3.04</td>
<td>20.40</td>
<td>43.08</td>
<td>16.09</td>
<td>3.45</td>
<td>10.34</td>
<td>0.00</td>
<td>50.00</td>
<td>50.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP ViT/B-32</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>7.85</td>
<td>26.60</td>
<td>4.97</td>
<td>13.63</td>
<td>34.47</td>
<td>7.05</td>
<td>0.00</td>
<td>3.45</td>
<td>0.00</td>
<td>50.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP Patched (V)</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
<td>10.10</td>
<td>28.04</td>
<td>6.41</td>
<td>16.29</td>
<td>44.60</td>
<td>10.42</td>
<td>0.00</td>
<td>6.90</td>
<td>0.00</td>
<td>50.00</td>
<td>50.00</td>
<td>50.00</td>
</tr>
<tr>
<td>CLIP Patched (VT)</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>7.37</td>
<td>23.24</td>
<td>5.45</td>
<td>25.14</td>
<td>45.67</td>
<td>17.03</td>
<td>3.45</td>
<td>27.59</td>
<td>3.45</td>
<td>0.00</td>
<td>50.00</td>
<td>0.00</td>
</tr>
<tr>
<td>FLAVA</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>34.62</td>
<td>3.37</td>
<td>2.72</td>
<td>48.66</td>
<td>16.09</td>
<td>12.60</td>
<td>31.03</td>
<td>3.45</td>
<td>3.45</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>CLIP RN50x16</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>7.85</td>
<td>19.87</td>
<td>4.49</td>
<td>22.93</td>
<td>54.13</td>
<td>18.88</td>
<td>3.45</td>
<td>20.69</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>BridgeTower</td>
<td>100.00</td>
<td>100.00</td>
<td>100.00</td>
<td>7.53</td>
<td>32.53</td>
<td>6.41</td>
<td>25.54</td>
<td>49.06</td>
<td>20.40</td>
<td>0.00</td>
<td>17.24</td>
<td>0.00</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
</tr>
<tr>
<td>ViLT</td>
<td>0.00</td>
<td>100.00</td>
<td>0.00</td>
<td>5.13</td>
<td>20.99</td>
<td>2.56</td>
<td>17.44</td>
<td>45.36</td>
<td>14.75</td>
<td>6.90</td>
<td>27.59</td>
<td>3.45</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>Table 8. The objects that belong to each category for the object-context analysis on specific objects in Probe-B.</p>
<table>
<thead>
<tr>
<th>Object</th>
<th>Category</th>
<th>Groups</th>
</tr>
</thead>
<tbody>
<tr>
<td>accessories</td>
<td>[backpack, umbrella, handbag, tie, suitcase]</td>
<td>174</td>
</tr>
<tr>
<td>animals</td>
<td>[bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe]</td>
<td>627</td>
</tr>
<tr>
<td>appliances</td>
<td>[microwave, oven, toaster, refrigerator]</td>
<td>591</td>
</tr>
<tr>
<td>decor</td>
<td>[clock, vase]</td>
<td>138</td>
</tr>
<tr>
<td>electronics</td>
<td>[tv, laptop, mouse, remote, keyboard, cell phone]</td>
<td>1095</td>
</tr>
<tr>
<td>fixtures</td>
<td>[toilet, sink]</td>
<td>387</td>
</tr>
<tr>
<td>foods</td>
<td>[sandwich, hot dog, pizza, donut, cake]</td>
<td>258</td>
</tr>
<tr>
<td>fruits</td>
<td>[banana, orange]</td>
<td>120</td>
</tr>
<tr>
<td>furniture</td>
<td>[chair, couch, bed, dining table]</td>
<td>546</td>
</tr>
<tr>
<td>kitchenware</td>
<td>[bottle, wine glass, cup, fork, knife, spoon, bowl]</td>
<td>399</td>
</tr>
<tr>
<td>people</td>
<td>[person]</td>
<td>720</td>
</tr>
<tr>
<td>plants</td>
<td>[potted plant]</td>
<td>108</td>
</tr>
<tr>
<td>recreation</td>
<td>[frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket]</td>
<td>117</td>
</tr>
<tr>
<td>roadway</td>
<td>[traffic light, fire hydrant, stop sign, parking meter]</td>
<td>144</td>
</tr>
<tr>
<td>street furniture</td>
<td>[bench]</td>
<td>42</td>
</tr>
<tr>
<td>tools</td>
<td>[scissors, hair drier, toothbrush]</td>
<td>15</td>
</tr>
<tr>
<td>toys</td>
<td>[book, teddy bear]</td>
<td>144</td>
</tr>
<tr>
<td>vegetables</td>
<td>[broccoli, carrot]</td>
<td>111</td>
</tr>
<tr>
<td>vehicles</td>
<td>[bicycle, car, motorcycle, airplane, bus, train, truck, boat]</td>
<td>603</td>
</tr>
</tbody>
</table>
<p>Overall results for Probe-B are in Table 9 and 10. In both cases, replacing with scene and noise produces worse results compared to black and gray fillers. For aggregating across filler, we only include CLIP ViT-L/14@336px,</p>
<p>CLIP RN50x4, FLAVA, ViLT, BridgeTower, BLIP, BLIP2, OTTER, ALIGN, MetaCLIP and SigLIP. When comparing individual model results in Table 10, performance tends to increase when only the other object remains, meaning that</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Comparing the change in confidence from a patched image $\tilde{x_{0}}$ to the image where all other objects and background $\tilde{x_{1}}$ is removed aggregated over CLIP backbones.</p>
<p>Table 9. Mean results for Probe- $B_{M R}$ when the background of an image is replaced with each filler (top) and for each model averaged over fillers (bottom). Comparisons are between the original image $x_{0}$, original image with a random patch $\tilde{x_{0}}$ and the modified image $\tilde{x_{1}}$ where the background is removed. The metrics are mean average precision (mAP), relative robustness $\left(\gamma_{r}\right)$ measuring the relative drop/increase in performance, and mean change in softmax confidence $\mu(\nabla(c))$ for the objects.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Filler</th>
<th style="text-align: center;">Average Precision (mAP)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relative Robustness $\left(\gamma_{r}\right)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mean Change Confidence $\left(\mu(\nabla(c))\right.$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$x_{0}$</td>
<td style="text-align: center;">$\tilde{x_{0}}$</td>
<td style="text-align: center;">$\tilde{x_{1}}$</td>
<td style="text-align: center;">$\left(x_{0}, \tilde{x_{1}}\right)$</td>
<td style="text-align: center;">$\left(x_{0}, \tilde{x_{0}}\right)$</td>
<td style="text-align: center;">$\left(\tilde{x_{0}}, \tilde{x_{1}}\right)$</td>
<td style="text-align: center;">$\left(c_{0}-\tilde{c_{0}}\right)$</td>
<td style="text-align: center;">$\left(\tilde{c_{0}}-\tilde{c_{1}}\right)$</td>
<td style="text-align: center;">$\left(c_{0}-\tilde{c_{1}}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">black</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">69.62</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">$-0.91$</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: center;">noise</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">67.75</td>
<td style="text-align: center;">68.14</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;">$-0.91$</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: center;">gray</td>
<td style="text-align: center;">69.84</td>
<td style="text-align: center;">69.68</td>
<td style="text-align: center;">71.02</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">$-0.86$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">scene</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">67.85</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">$-1.31$</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">$x_{0}$</td>
<td style="text-align: center;">$\tilde{x_{0}}$</td>
<td style="text-align: center;">$\tilde{x_{1}}$</td>
<td style="text-align: center;">$\left(x_{0}, \tilde{x_{1}}\right)$</td>
<td style="text-align: center;">$\left(x_{0}, \tilde{x_{0}}\right)$</td>
<td style="text-align: center;">$\left(\tilde{x_{0}}, \tilde{x_{1}}\right)$</td>
<td style="text-align: center;">$\left(c_{0}-\tilde{c_{0}}\right)$</td>
<td style="text-align: center;">$\left(\tilde{c_{0}}-\tilde{c_{1}}\right)$</td>
<td style="text-align: center;">$\left(c_{0}-\tilde{c_{1}}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">CLIP RN50</td>
<td style="text-align: center;">65.05</td>
<td style="text-align: center;">65.47</td>
<td style="text-align: center;">60.30</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">$-0.05$</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">4.82</td>
</tr>
<tr>
<td style="text-align: center;">CLIP ViT/B-32</td>
<td style="text-align: center;">68.77</td>
<td style="text-align: center;">67.49</td>
<td style="text-align: center;">61.10</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">5.12</td>
<td style="text-align: center;">5.52</td>
</tr>
<tr>
<td style="text-align: center;">CLIP CNN</td>
<td style="text-align: center;">63.23</td>
<td style="text-align: center;">63.56</td>
<td style="text-align: center;">63.46</td>
<td style="text-align: center;">1.12</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1.10</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">3.11</td>
</tr>
<tr>
<td style="text-align: center;">CLIP RN101</td>
<td style="text-align: center;">64.56</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">63.80</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">3.00</td>
</tr>
<tr>
<td style="text-align: center;">CLIP ViT-B/16</td>
<td style="text-align: center;">69.97</td>
<td style="text-align: center;">68.54</td>
<td style="text-align: center;">65.15</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">2.92</td>
<td style="text-align: center;">3.28</td>
</tr>
<tr>
<td style="text-align: center;">FLAVA</td>
<td style="text-align: center;">72.05</td>
<td style="text-align: center;">74.47</td>
<td style="text-align: center;">66.75</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">CLIP ViT L/14</td>
<td style="text-align: center;">70.98</td>
<td style="text-align: center;">69.38</td>
<td style="text-align: center;">68.99</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">2.12</td>
</tr>
<tr>
<td style="text-align: center;">CLIP ViT</td>
<td style="text-align: center;">71.05</td>
<td style="text-align: center;">70.94</td>
<td style="text-align: center;">71.50</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">1.22</td>
</tr>
<tr>
<td style="text-align: center;">ViLT</td>
<td style="text-align: center;">83.49</td>
<td style="text-align: center;">71.38</td>
<td style="text-align: center;">83.26</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">6.68</td>
<td style="text-align: center;">$-6.61$</td>
<td style="text-align: center;">0.08</td>
</tr>
<tr>
<td style="text-align: center;">BridgeTower</td>
<td style="text-align: center;">81.85</td>
<td style="text-align: center;">81.88</td>
<td style="text-align: center;">83.40</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">$-0.79$</td>
<td style="text-align: center;">$-0.42$</td>
<td style="text-align: center;">$-1.22$</td>
</tr>
<tr>
<td style="text-align: center;">BLIP</td>
<td style="text-align: center;">68.67</td>
<td style="text-align: center;">73.43</td>
<td style="text-align: center;">71.76</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">$-0.78$</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">$-0.62$</td>
</tr>
<tr>
<td style="text-align: center;">BLIP2</td>
<td style="text-align: center;">77.37</td>
<td style="text-align: center;">76.19</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">$-1.96$</td>
<td style="text-align: center;">1.51</td>
</tr>
<tr>
<td style="text-align: center;">OTTER</td>
<td style="text-align: center;">30.75</td>
<td style="text-align: center;">29.23</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">ALIGN</td>
<td style="text-align: center;">50.53</td>
<td style="text-align: center;">50.52</td>
<td style="text-align: center;">48.89</td>
<td style="text-align: center;">1.48</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: center;">MetaCLIP</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">61.79</td>
<td style="text-align: center;">66.76</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">5.22</td>
<td style="text-align: center;">$-4.86$</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">SigLIP</td>
<td style="text-align: center;">69.96</td>
<td style="text-align: center;">70.46</td>
<td style="text-align: center;">69.77</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">2.19</td>
<td style="text-align: center;">$-2.07$</td>
<td style="text-align: center;">0.13</td>
</tr>
</tbody>
</table>
<p>Table 10. Results for when the background and all other objects are replaced with a filler $\tilde{x_{1}}$, compared to the original image $x_{0}$, and an image with a random patch of the same filler type $\tilde{x_{0}}$. Metrics used are the accuracy of detecting the object compared to other objects that are not present in the image and the relative robustness $\gamma_{r}$, which is the relative change in confidence.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Relative Robustness $\gamma^{r}$</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Filler</td>
<td style="text-align: left;">$x_{0}$</td>
<td style="text-align: left;">$\tilde{x_{0}}$</td>
<td style="text-align: left;">$\tilde{x_{1}}$</td>
<td style="text-align: left;">$\left(x_{0}, \tilde{x_{1}}\right)$</td>
<td style="text-align: left;">$\left(x_{0}, \tilde{x_{0}}\right)$</td>
<td style="text-align: left;">$\left(\tilde{x_{0}}, \tilde{x_{1}}\right)$</td>
</tr>
<tr>
<td style="text-align: left;">noise</td>
<td style="text-align: left;">54.48</td>
<td style="text-align: left;">54.95</td>
<td style="text-align: left;">60.09</td>
<td style="text-align: left;">1.1</td>
<td style="text-align: left;">$\underline{1.01}$</td>
<td style="text-align: left;">1.09</td>
</tr>
<tr>
<td style="text-align: left;">scene</td>
<td style="text-align: left;">49.08</td>
<td style="text-align: left;">52</td>
<td style="text-align: left;">56.1</td>
<td style="text-align: left;">$\mathbf{1 . 1 4}$</td>
<td style="text-align: left;">$\mathbf{1 . 0 6}$</td>
<td style="text-align: left;">1.08</td>
</tr>
<tr>
<td style="text-align: left;">black</td>
<td style="text-align: left;">$\underline{56.62}$</td>
<td style="text-align: left;">$\mathbf{5 7 . 1 3}$</td>
<td style="text-align: left;">$\underline{63.79}$</td>
<td style="text-align: left;">$\underline{1.13}$</td>
<td style="text-align: left;">$\underline{1.01}$</td>
<td style="text-align: left;">$\underline{1.12}$</td>
</tr>
<tr>
<td style="text-align: left;">gray</td>
<td style="text-align: left;">$\mathbf{5 7 . 0 8}$</td>
<td style="text-align: left;">$\underline{56.83}$</td>
<td style="text-align: left;">$\mathbf{6 3 . 9 8}$</td>
<td style="text-align: left;">1.12</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$\mathbf{1 . 1 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">$x_{0}$</td>
<td style="text-align: left;">$\tilde{x_{0}}$</td>
<td style="text-align: left;">$\tilde{x_{1}}$</td>
<td style="text-align: left;">$\left(x_{0}, \tilde{x_{1}}\right)$</td>
<td style="text-align: left;">$\left(x_{0}, \tilde{x_{0}}\right)$</td>
<td style="text-align: left;">$\left(\tilde{x_{0}}, \tilde{x_{1}}\right)$</td>
</tr>
<tr>
<td style="text-align: left;">BridgeTower</td>
<td style="text-align: left;">$\mathbf{7 7 . 3 6}$</td>
<td style="text-align: left;">$\mathbf{7 6 . 1 4}$</td>
<td style="text-align: left;">$\mathbf{7 7 . 7 0}$</td>
<td style="text-align: left;">1.01</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">1.02</td>
</tr>
<tr>
<td style="text-align: left;">FLAVA</td>
<td style="text-align: left;">56.33</td>
<td style="text-align: left;">59.16</td>
<td style="text-align: left;">58.32</td>
<td style="text-align: left;">1.03</td>
<td style="text-align: left;">1.06</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">CLIP ViT/B-32</td>
<td style="text-align: left;">46.19</td>
<td style="text-align: left;">50.50</td>
<td style="text-align: left;">54.20</td>
<td style="text-align: left;">1.17</td>
<td style="text-align: left;">1.10</td>
<td style="text-align: left;">1.07</td>
</tr>
<tr>
<td style="text-align: left;">CLIP ViT-B/16</td>
<td style="text-align: left;">51.12</td>
<td style="text-align: left;">51.31</td>
<td style="text-align: left;">60.30</td>
<td style="text-align: left;">1.18</td>
<td style="text-align: left;">1.01</td>
<td style="text-align: left;">1.17</td>
</tr>
<tr>
<td style="text-align: left;">CLIP ViT L/14</td>
<td style="text-align: left;">56.27</td>
<td style="text-align: left;">54.18</td>
<td style="text-align: left;">67.16</td>
<td style="text-align: left;">1.19</td>
<td style="text-align: left;">0.96</td>
<td style="text-align: left;">1.24</td>
</tr>
<tr>
<td style="text-align: left;">CLIP RN50</td>
<td style="text-align: left;">45.79</td>
<td style="text-align: left;">48.64</td>
<td style="text-align: left;">55.68</td>
<td style="text-align: left;">1.21</td>
<td style="text-align: left;">1.07</td>
<td style="text-align: left;">1.14</td>
</tr>
<tr>
<td style="text-align: left;">CLIP ViT</td>
<td style="text-align: left;">59.31</td>
<td style="text-align: left;">56.08</td>
<td style="text-align: left;">71.76</td>
<td style="text-align: left;">1.21</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">$\underline{1.28}$</td>
</tr>
<tr>
<td style="text-align: left;">CLIP RN101</td>
<td style="text-align: left;">46.48</td>
<td style="text-align: left;">47.92</td>
<td style="text-align: left;">57.32</td>
<td style="text-align: left;">1.23</td>
<td style="text-align: left;">1.03</td>
<td style="text-align: left;">1.20</td>
</tr>
<tr>
<td style="text-align: left;">CLIP RN50x16</td>
<td style="text-align: left;">53.88</td>
<td style="text-align: left;">50.92</td>
<td style="text-align: left;">66.18</td>
<td style="text-align: left;">1.23</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">$\mathbf{1 . 3 0}$</td>
</tr>
<tr>
<td style="text-align: left;">CLIP RN50x64</td>
<td style="text-align: left;">56.86</td>
<td style="text-align: left;">53.70</td>
<td style="text-align: left;">70.04</td>
<td style="text-align: left;">1.23</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">$\mathbf{1 . 3 0}$</td>
</tr>
<tr>
<td style="text-align: left;">CLIP CNN</td>
<td style="text-align: left;">50.08</td>
<td style="text-align: left;">49.66</td>
<td style="text-align: left;">62.18</td>
<td style="text-align: left;">$\underline{1.24}$</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">1.25</td>
</tr>
<tr>
<td style="text-align: left;">ViLT</td>
<td style="text-align: left;">54.73</td>
<td style="text-align: left;">55.82</td>
<td style="text-align: left;">$\underline{72.09}$</td>
<td style="text-align: left;">$\mathbf{1 . 3 3}$</td>
<td style="text-align: left;">1.02</td>
<td style="text-align: left;">$\mathbf{1 . 3 0}$</td>
</tr>
<tr>
<td style="text-align: left;">BLIP</td>
<td style="text-align: left;">$\underline{68.41}$</td>
<td style="text-align: left;">$\underline{73.95}$</td>
<td style="text-align: left;">64.62</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">1.084</td>
<td style="text-align: left;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">BLIP2</td>
<td style="text-align: left;">68.02</td>
<td style="text-align: left;">65.13</td>
<td style="text-align: left;">65.67</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">0.964</td>
<td style="text-align: left;">1.01</td>
</tr>
<tr>
<td style="text-align: left;">OTTER</td>
<td style="text-align: left;">12.16</td>
<td style="text-align: left;">10.45</td>
<td style="text-align: left;">11.89</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.864</td>
<td style="text-align: left;">1.14</td>
</tr>
<tr>
<td style="text-align: left;">ALIGN</td>
<td style="text-align: left;">63.12</td>
<td style="text-align: left;">61.66</td>
<td style="text-align: left;">69.17</td>
<td style="text-align: left;">1.1</td>
<td style="text-align: left;">0.984</td>
<td style="text-align: left;">1.12</td>
</tr>
<tr>
<td style="text-align: left;">MetaCLIP</td>
<td style="text-align: left;">48.01</td>
<td style="text-align: left;">55.03</td>
<td style="text-align: left;">51.37</td>
<td style="text-align: left;">1.07</td>
<td style="text-align: left;">$\underline{1.154}$</td>
<td style="text-align: left;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">SigLIP</td>
<td style="text-align: left;">63.51</td>
<td style="text-align: left;">73.82</td>
<td style="text-align: left;">62.22</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">$\mathbf{1 . 1 6 4}$</td>
<td style="text-align: left;">0.84</td>
</tr>
</tbody>
</table>
<p>other objects may actually distract models. BridgeTower is the highest performer and has the lowest robustness from $x_{0}$ to $x_{1}$ meaning that it may be using some level of object relationship understandings to help recognize objects. However, this difference is minor and therefore inconclusive. Other models' robustness though is higher indicating they perform better when objects are in isolation, indicating they are not using object relationship understanding to help object detection of particular objects. In Table 9, when only background is removed, we see little change. However, in ViLT, which is one transformer that takes both text and visual tokens, adding a patch reduced performance noticeably worse when compared to other models. This may indicate a weakness in a single-stream, transformer based approach.</p>
<h2>8. Exploring Improving Dual-Stream Only Conceptual Models</h2>
<p>Based on our evaluation of these models, we see that crossattention between modalities improves the learning of conceptual models about objects and actions in a system and the relationships between them. However, a limitation of this approach is its use for downstream tasks. Both ViLT and BridgeTower require image-text pairs of input, making other tasks like image classification computationally expensive and difficult. Meanwhile, dual-stream encoders like CLIP and FLAVA allow uni-modal feature representations that can be extracted and used for a variety of downstream tasks. Improving models that do not require paired input would provide greater value and stronger representations. To explore this idea, we fine-tune CLIP on a new dataset inspired by this benchmark called RelComp.</p>
<p>Table 11. The results for varying the alpha values for patching [16] finetuned CLIP models on either text encoder (T), visual encoder (V), or both (VT). There is a clear trade-off with downstream ImageNet classification and finetuning on a smaller, compositional and relational focused dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">RelComp</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">ImageNet</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Stream</td>
<td style="text-align: left;">alpha</td>
<td style="text-align: left;">Group Score</td>
<td style="text-align: left;">Image Score</td>
<td style="text-align: left;">Text Score</td>
<td style="text-align: left;">Top1</td>
<td style="text-align: left;">Top5</td>
</tr>
<tr>
<td style="text-align: left;">v</td>
<td style="text-align: left;">0.2</td>
<td style="text-align: left;">31.52</td>
<td style="text-align: left;">54.67</td>
<td style="text-align: left;">53.17</td>
<td style="text-align: left;">$\mathbf{6 1 . 4 5}$</td>
<td style="text-align: left;">$\mathbf{8 7 . 7 3}$</td>
</tr>
<tr>
<td style="text-align: left;">v</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">32.58</td>
<td style="text-align: left;">55.61</td>
<td style="text-align: left;">53.97</td>
<td style="text-align: left;">$\underline{58.25}$</td>
<td style="text-align: left;">$\underline{85.69}$</td>
</tr>
<tr>
<td style="text-align: left;">v</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">33.29</td>
<td style="text-align: left;">56.40</td>
<td style="text-align: left;">54.89</td>
<td style="text-align: left;">54.19</td>
<td style="text-align: left;">82.72</td>
</tr>
<tr>
<td style="text-align: left;">v</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">34.15</td>
<td style="text-align: left;">57.31</td>
<td style="text-align: left;">55.60</td>
<td style="text-align: left;">49.36</td>
<td style="text-align: left;">78.87</td>
</tr>
<tr>
<td style="text-align: left;">v</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">34.43</td>
<td style="text-align: left;">57.62</td>
<td style="text-align: left;">55.94</td>
<td style="text-align: left;">44.04</td>
<td style="text-align: left;">73.96</td>
</tr>
<tr>
<td style="text-align: left;">vt</td>
<td style="text-align: left;">0.2</td>
<td style="text-align: left;">42.19</td>
<td style="text-align: left;">64.30</td>
<td style="text-align: left;">62.45</td>
<td style="text-align: left;">54.62</td>
<td style="text-align: left;">83.05</td>
</tr>
<tr>
<td style="text-align: left;">t</td>
<td style="text-align: left;">0.2</td>
<td style="text-align: left;">47.18</td>
<td style="text-align: left;">67.86</td>
<td style="text-align: left;">66.62</td>
<td style="text-align: left;">57.42</td>
<td style="text-align: left;">85.14</td>
</tr>
<tr>
<td style="text-align: left;">vt</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">49.58</td>
<td style="text-align: left;">69.83</td>
<td style="text-align: left;">68.00</td>
<td style="text-align: left;">44.92</td>
<td style="text-align: left;">73.93</td>
</tr>
<tr>
<td style="text-align: left;">vt</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">50.11</td>
<td style="text-align: left;">70.14</td>
<td style="text-align: left;">68.78</td>
<td style="text-align: left;">34.95</td>
<td style="text-align: left;">63.07</td>
</tr>
<tr>
<td style="text-align: left;">vt</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">51.57</td>
<td style="text-align: left;">71.12</td>
<td style="text-align: left;">70.35</td>
<td style="text-align: left;">27.00</td>
<td style="text-align: left;">52.54</td>
</tr>
<tr>
<td style="text-align: left;">vt</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">53.38</td>
<td style="text-align: left;">72.62</td>
<td style="text-align: left;">71.77</td>
<td style="text-align: left;">20.98</td>
<td style="text-align: left;">43.88</td>
</tr>
<tr>
<td style="text-align: left;">t</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">56.31</td>
<td style="text-align: left;">74.56</td>
<td style="text-align: left;">73.55</td>
<td style="text-align: left;">50.69</td>
<td style="text-align: left;">79.35</td>
</tr>
<tr>
<td style="text-align: left;">t</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">62.51</td>
<td style="text-align: left;">78.57</td>
<td style="text-align: left;">78.29</td>
<td style="text-align: left;">44.39</td>
<td style="text-align: left;">72.39</td>
</tr>
<tr>
<td style="text-align: left;">t</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">$\underline{70.36}$</td>
<td style="text-align: left;">$\underline{83.55}$</td>
<td style="text-align: left;">$\underline{83.21}$</td>
<td style="text-align: left;">38.66</td>
<td style="text-align: left;">66.23</td>
</tr>
<tr>
<td style="text-align: left;">t</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">$\mathbf{7 4 . 6 3}$</td>
<td style="text-align: left;">$\mathbf{8 6 . 2 6}$</td>
<td style="text-align: left;">$\mathbf{8 5 . 6 2}$</td>
<td style="text-align: left;">33.54</td>
<td style="text-align: left;">60.21</td>
</tr>
</tbody>
</table>
<p>Table 12. The pre-training datasets include MSCOCO [24], SBU Captions, Localized Narratives (LN), Visual Genome (VG) [21], Wikipedia Image Text (WIT) [40], Conceptual Captions (CC) [37], Conceptual Captions 12M (CC12) [6], Red Caps (RC) [9], YFCC100M [41], and LAION-400M [36].</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: left;">Datasets</th>
<th style="text-align: center;">Images</th>
<th style="text-align: center;">Captions</th>
<th style="text-align: center;">Arch.</th>
<th style="text-align: center;">Attn</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CLIP RN50 [31]</td>
<td style="text-align: center;">102M</td>
<td style="text-align: left;">LAION-400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">dual-stream</td>
<td style="text-align: center;">modality-specific</td>
</tr>
<tr>
<td style="text-align: left;">CLIP RN101 [31]</td>
<td style="text-align: center;">121M</td>
<td style="text-align: left;">LAION-400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">dual-stream</td>
<td style="text-align: center;">modality-specific</td>
</tr>
<tr>
<td style="text-align: left;">CLIP ViT B16/32 [31]</td>
<td style="text-align: center;">150M</td>
<td style="text-align: left;">LAION-400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">dual-stream</td>
<td style="text-align: center;">modality-specific</td>
</tr>
<tr>
<td style="text-align: left;">CLIP ViT L14 [31]</td>
<td style="text-align: center;">428M</td>
<td style="text-align: left;">LAION-400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">400M</td>
<td style="text-align: center;">dual-stream</td>
<td style="text-align: center;">modality-specific</td>
</tr>
<tr>
<td style="text-align: left;">FLAVA [38]</td>
<td style="text-align: center;">358M</td>
<td style="text-align: left;">MSCOCO, SBU, LN, CC, CC12, VG, WIT, RC, YFCC100M</td>
<td style="text-align: center;">70M</td>
<td style="text-align: center;">70M</td>
<td style="text-align: center;">dual-stream</td>
<td style="text-align: center;">modality-specific, <br> merged</td>
</tr>
<tr>
<td style="text-align: left;">ViLT [20]</td>
<td style="text-align: center;">112M</td>
<td style="text-align: left;">MSCOCO,VG,SBU,CC</td>
<td style="text-align: center;">4.20M</td>
<td style="text-align: center;">9.58M</td>
<td style="text-align: center;">single-stream</td>
<td style="text-align: center;">modality-specific, <br> merged</td>
</tr>
<tr>
<td style="text-align: left;">Bridgetower [47]</td>
<td style="text-align: center;">865M</td>
<td style="text-align: left;">MSCOCO,VG,SBU,CC</td>
<td style="text-align: center;">4.20M</td>
<td style="text-align: center;">9.58M</td>
<td style="text-align: center;">dual-stream</td>
<td style="text-align: center;">modality-specific, <br> co-attn, merged</td>
</tr>
</tbody>
</table>
<h3>8.1. Method</h3>
<p>In order to improve CLIP for compositional and relational understanding, we propose using selective negative and positive pairing based on compositional and predicate swaps. We propose using two losses, an image-text matching (ITM) loss and a contrastive loss (C) similar to CLIP [31] and FLAVA [38]. The ITM loss is a triplet loss with two instances [7], maximizing the distance between an anchor and a negative sample while minimizing the distance between an anchor and a positive sample. We use this in order to focus model learning on compositions and relations.</p>
<p>The first is where the anchor is the image $x$, the posi- tive is the caption $p$, and the negative $\bar{p}$ is the same caption but with either the predicate or the composition swapped. The second uses a real-world caption $y$ as an anchor and the corresponding image $x$ as a positive. The final ITM loss is the average of the two. For the contrastive loss, we maximize the cosine similarities between image and text pairs and minimize those for the image and negative text pairs. We use two versions, the first uses the real-world captions $y$ and their corresponding images, and the second uses the positive text prompts $p$ and their images. The final contrastive loss is the average of the two. A summary of this approach is shown in Figure 7.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. Examples from Probe-R comparing CLIP ViT-B/32 to the same model finetuned on RelComp for both the visual and text encoder, then patched [16]. The values are the softmax confidence for the correct prompt $P_{B_{1}}$ shown as 1) vs the incorrect prompt 2), where the predicate is swapped, or $P_{\mathrm{B}_{2}}$.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. Examples from Probe-C comparing CLIP ViT-B/32 to the same model finetuned on RelComp for both the visual and text encoder, then patched [16]. For each group, the first image and its corresponding prompt are on top, and the second image and prompt are on the bottom. The values are the softmax confidence for the corresponding prompt when compared to the alternative prompt.</p>
<h3>8.2. Dataset: RelComp</h3>
<p>We used our existing knowledge of the benchmark to generate a new training and testing dataset. For compositions, we use images and captions from the MSCOCO dataset [24]. For anchor text we use the real-world caption, for positive we replace all compositions with synonyms, and for negatives we replace all compositions with antonyms. No captions seen in this dataset are also seen in Probe-C. For relations, we use images, region descriptions and relationships from the VisualGenome dataset [21]. For each image, we find the region description that has the most overlap with prompts generated in the same way as Probe-R and use this as our anchor caption. For negative, we use the same template but use prompt with the predicate swapped to an unlikely one, as in Probe-R. To prevent exact prompts from the benchmark being included, we filtered for images that are not present in Probe-R. This results in 149,166 groups with 78,155 of those swapping compositions and 71,011 swapping predicates for training. The test set has 15,836 groups and of those, 8,734 are swap compositions and 7,102 swap predicates.</p>
<h3>8.3. Implementation</h3>
<p>We finetune the CLIP ViT-B/32 model using our proposed ITM and contrastive loss on the proposed dataset RelComp. We use stochastic gradient descent with a cosine learning rate scheduler with a minimum learning rate of .001, momentum 0.9, weight decay of .0001. We train for 40 epochs using an 11GB GPU and a batch size of 128. We use these smaller configurations to show the benefits with just light</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. We design probes that measure relational understanding in V+L models, in this case we compare ViLT [20] that uses cross-attention (top) and CLIP [31] which does not (bottom). With cross-attention, the model can change its focus based on the prompt and performs better when compositions and relations are swapped for unrealistic/non-present ones. Meanwhile, CLIP does not adapt and focuses more highly on objects, like "man" and "skateboard".</p>
<p>tuning. One of the many challenges of fine-tuning a large model, is that the distribution shift may lead to a loss of the original feature space. In order to prevent this "catastrophic forgetting" of the original feature space, we linearly interpolate the original CLIP weights with our finetuned weights using an alpha= 0.2, leaning more towards the original weights, in order to reduce this shift [16, 44]. This is referred to as patching and therefore we call the finetuned and patched version "CLIP Patched". We finetune three configurations based on which encoders we finetune: visual only (V), text only (T) or both (VT).</p>
<h3>8.4. Results</h3>
<p>Overall results for our experiment are shown in Table 13. When finetuning on the new dataset, there is an issue of drift from the original CLIP performance as measured by ImageNet accuracy, even when patching. When finetuning using the visual-encoder only, the drift is less pronounced, but so is the improvement on RelComp. The largest increase in RelComp is seen when just training the text encoder. (1) This may indicate that for non-cross-attention models text is more important for conceptual mapping. Overall, (2) our findings indicate that it is possible by using selective negative sampling to enforce compositional and relational learning without extensive co-attention and computational complexity. Limitations of this experiment is our training data.</p>
<p>Table 13. Overall results for finetuning and patching the CLIP ViT-B/32 on the proposed RelComp dataset. ImageNet accuracy is shown to measure the drift from the original CLIP space. RelComp is the image score for the correct image-to-prompt matching. Probe-C/R are the mean accuracy for the correct image-prompt match. Top scores are in bold while second are underlined.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>ImageNet</th>
<th>RelComp</th>
<th>Probe-C</th>
<th>Probe-R</th>
</tr>
</thead>
<tbody>
<tr>
<td>ViLT</td>
<td>–</td>
<td>76.00</td>
<td>90.78</td>
<td>69.00</td>
</tr>
<tr>
<td>BridgeTower</td>
<td>–</td>
<td>85.00</td>
<td>90.06</td>
<td>82.20</td>
</tr>
<tr>
<td>FLAVA</td>
<td>56.83</td>
<td>47.12</td>
<td>83.85</td>
<td>68.29</td>
</tr>
<tr>
<td>CLIP ViT B32</td>
<td>63.60</td>
<td>51.93</td>
<td>88.15</td>
<td>53.52</td>
</tr>
<tr>
<td>CLIP Patched (T)</td>
<td>57.85</td>
<td>67.85</td>
<td>89.49</td>
<td>71.14</td>
</tr>
<tr>
<td>CLIP Patched (V)</td>
<td>61.45</td>
<td>54.66</td>
<td>89.81</td>
<td>61.40</td>
</tr>
<tr>
<td>CLIP Patched (VT)</td>
<td>54.61</td>
<td>64.27</td>
<td>90.30</td>
<td>71.20</td>
</tr>
</tbody>
</table>
<p>is very small in comparison to recent works, further work should investigate this relationship with a larger dataset with more variation. Table 11 shows the results based on different alphas for RelComp and ImageNet. There is a definite trade-off between original performance and performance on the new task. We also see that training only the text encoder yields the greatest improvement in these tasks but also the largest "forgetting". Some examples of where CLIP patched improved over CLIP in Probe-R is shown in Figure 12. The first column are the original images, the second the attention maps of visual and text features for CLIP ViT-B/32 and the third are the attention maps for CLIP Patched (VT). The values are the softmax confidence for the correct prompt P_{R1} shown as 1) versus the incorrect prompt P_{R2} where the predicate is switched 2). Similar examples for Probe-C are shown in Figure 13. For each group, the first image and its corresponding prompt are on top, and the second image and prompt are on the bottom. The values are the softmax confidence for the corresponding prompt when compared to the alternative prompt.</p>
<h3>9. Model Details</h3>
<p>A summary of the model details can be found in Table 12. The highest performing model is BridgeTower but it also had the largest number of parameters and the slowest. Additionally, BridgeTower utilizes a pre-trained CLIP visual encoder, improving upon CLIPs performance. All models require image-text pairs, making a greater number of comparisons difficult, especially for downstream tasks like image classification on ImageNet where there are 1000 classes. However, because FLAVA merges dual-stream encoder output prior to cross-encoding, it is easier to extract feature embeddings prior to the cross-encoding for a greater number of comparisons. This however does not utilize its full potential for performance. Figure 14 shows examples of how this image-text pair input is a strength for performance in these kinds of tasks. The bottom shows ViLT and how its visual attention changes based on its input while the</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding Author: raiyaanabdullah@gmail.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
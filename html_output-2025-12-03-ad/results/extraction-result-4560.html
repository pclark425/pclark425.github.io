<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4560 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4560</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4560</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-36b203d5fee14d34853021cb4bdcfa0d8e988986</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/36b203d5fee14d34853021cb4bdcfa0d8e988986" target="_blank">Confidence Improves Self-Consistency in LLMs</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The notion of within-question confidence evaluation is introduced, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question.</p>
                <p><strong>Paper Abstract:</strong> Self-consistency decoding enhances LLMs'performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4560.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4560.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CISC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confidence-Informed Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation framework that augments self-consistency by extracting a confidence score for each sampled chain-of-thought and performing a confidence-weighted majority vote after softmax normalization with a tunable temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Confidence-Informed Self-Consistency (CISC)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate m reasoning paths (r_i,a_i), extract a scalar confidence c_i for each path, normalize confidences with softmax(c_i/T), and select the final answer by maximizing the sum of normalized confidences per answer: argmax_a sum_i I[a_i=a] * softmax(c_i/T). Temperature T controls frequency vs confidence tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability to correctly pick the true answer using fewer reasoning samples (efficiency), final answer accuracy (empirical accuracy), sensitivity to confidence signal (within-question discrimination), and computational cost (inference token generation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various instruction-tuned open models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various; experiments used Gemma2-2B/9B/27B, Qwen 3B/14B/72B, Mistral 8B/22B/123B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning tasks (mathematical and commonsense reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Chain-of-thought explanations / multi-step reasoning outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CISC with the best confidence method (P(True)) reduced required sample size by >40% on average (41% cost reduction for budget=5, 46% for budget=10) compared to standard self-consistency; corresponding relative accuracy improvements were modest but positive (1.6% and 1.1%). CISC outperformed self-consistency across nearly all examined models and datasets and across three confidence extraction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: uses model-derived confidence scores, softmax normalization, and quantitative metrics (% Cost Reduction, % Accuracy Improvement). Hybrid validation includes a human qualitative study correlating model confidence with human-identified low-quality indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparisons to self-consistency across nine models/four datasets, bootstrap sampling (n=500 draws per question from 30 sampled paths) to compute stability/CI, and correlation with human qualitative judgments; temperature tuned on 10% held-out set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires reliable confidence extraction; performance sensitive to temperature scaling; access to token probabilities / P(True) may be unavailable in some frameworks; confidence calibration across questions does not guarantee within-question discrimination needed by CISC.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard (selected tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4560.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency majority-vote decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline decoding approach that samples multiple chain-of-thought traces and selects the most frequent final answer (majority vote) as the model output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Self-Consistency (majority voting over sampled chains)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sample many reasoning paths from the model; aggregate final answers by simple frequency (argmax_a count(a_i=a)). No per-path confidence is used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Final answer accuracy as a function of the number of sampled reasoning paths (efficacy vs compute), robustness to sampling variability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning tasks (mathematical and commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Chain-of-thought explanations / multi-step reasoning outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the baseline; observed to require many (up to 30+) samples to match CISC performance; CISC often achieved comparable or better accuracy with far fewer samples (e.g., CISC with 8 samples surpassed 30-sample self-consistency in one example).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (majority vote over sampled traces).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison vs CISC across models/datasets using bootstrapped sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Computationally expensive due to need for many long chain samples; ignores per-sample self-assessed confidence, which can reduce sampling efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4560.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WQD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Within-Question Discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric introduced in this paper that measures how well confidence scores discriminate between correct and incorrect responses to the same question by counting within-question correct>incorrect confidence orderings over sampled pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Within-Question Discrimination (WQD)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each question q with sampled responses R_q, partition into correct R_q^+ and incorrect R_q^−. WQD is the fraction of pairs (r in R_q^+, r' in R_q^−) for which c(r)>c(r'). Formally: WQD(c) = (1/N) sum_q sum_{r in R_q^+} sum_{r' in R_q^-} [c(r)>c(r')], where N = total pairs. In experiments m=30 samples per question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Within-question discrimination (ability of confidence to rank correct traces above incorrect traces), predictive power for use in weighted aggregation like CISC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / chain-of-thought quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of explanations/reasoning chains</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>WQD accurately predicted which confidence extraction methods yielded the best CISC performance: P(True) had the highest WQD (62.3%) and yielded the largest CISC cost reductions; verbal methods had better calibration but lower WQD and lower CISC gains. WQD values exceeded chance (>50%) across models/datasets (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computed over sampled model responses; compared against CISC performance (automated) and human qualitative signals (correlative evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by correlation with downstream CISC cost-reduction and accuracy improvements (WQD predicted relative CISC performance across confidence methods), and by inspecting monotonic relationship between confidence gap and WQD (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires multiple sampled responses per question (m large enough, e.g., 30); does not capture absolute calibration across questions; sensitive to sampling diversity and definition of correctness for complex open-ended outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4560.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Between-question calibration metrics (ECE, Brier Score, temperature-scaled variants ECE-t/Brier-t)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard metrics that evaluate how well confidence values align with actual correctness probabilities across questions (between-question calibration), often after temperature scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expected Calibration Error (ECE) and Brier Score (temperature-scaled: ECE-t, Brier-t)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ECE bins predicted confidences across examples and measures absolute difference between average confidence and empirical accuracy per bin; Brier score is mean squared error between predicted probability and true label. Temperature scaling fits a single T to minimize NLL before computing scaled metrics (ECE-t, Brier-t).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Between-question calibration (are higher confidence values associated with higher empirical correctness across the dataset), overall probabilistic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / confidence estimation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of probabilistic confidence of answers/explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Verbal confidence methods obtained the best ECE-t and Brier-t scores (e.g., Verbal Binary ECE-t=0.005, Brier-t=0.187) but they performed worse in CISC compared to P(True). The authors show between-question calibration metrics were poor predictors of within-question discrimination and CISC utility.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric evaluation (statistical).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Metrics computed after temperature scaling (Ovadia et al., 2019); compared to WQD and CISC downstream performance to evaluate predictive validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Good between-question calibration does not imply good within-question discrimination necessary for selecting among responses to the same question; may be misleading for tasks where within-question ranking matters.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4560.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence extraction methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Response Probability, Verbal Confidence (Binary, 0-100), P(True)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three families of methods to extract per-path confidence: (1) model log-probability (length-normalized), (2) prompting the model to verbalize confidence (binary or 0–100), (3) asking a binary confidence token and using the model's assigned token probability (P(True)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Response Probability / Verbal Confidence / P(True) confidence extraction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Response Probability: compute length-normalized product of next-token probabilities for full (r,a). Verbal Binary/0-100: prompt model to output a confidence score after generating (r,a). P(True): prompt for 0/1 confidence and use the model's probability assigned to token '1' as a scalar confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>How well per-path confidences correlate with actual correctness (within-question discrimination), calibration across questions, computational overhead to extract confidences, and compatibility with caching/prefix reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / chain-of-thought scoring</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Per-path confidence estimation for explanations/reasoning outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>P(True) had the highest WQD (62.3%) and yielded the largest CISC cost reductions (41% at budget=5, 46% at budget=10) and best overall CISC accuracy improvements; verbal methods were better calibrated across questions but had worse WQD and reduced CISC gains. Response Probability performed intermediately.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (model-internal probabilities and prompted scores). Also correlated with human qualitative judgments in separate analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared via ECE-t/Brier-t and WQD; downstream validated by CISC performance and bootstrap confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Verbal prompts can be well-calibrated but may not discriminate within-question; response-probability can be affected by sequence length and tokenization; P(True) requires access to model token probabilities and careful prompt formatting to avoid confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4560.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence Normalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Softmax normalization with temperature scaling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Within-question normalization of per-path confidence scores using softmax with a tunable temperature to produce relative weights prior to aggregation; temperature tuned on held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Softmax confidence normalization (temperature-scaled)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given raw confidences c_i for m paths, compute |tilde{c}_i = exp(c_i / T) / sum_j exp(c_j / T). Temperature T is tuned per model and confidence method on a 10% held-out set to control peakiness (T->inf collapses to uniform weights; T->0 selects max).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Enhances discriminative power of confidence weights for within-question ranking and stabilizes aggregation; measured by downstream CISC performance (cost reduction, accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Aggregation weighting of explanation confidences</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Normalization with tuned temperature consistently improved CISC versus unnormalized confidences across methods (e.g., P(True) cost reduction improved from 32% to 46% at budget=10). Using softmax without temperature tuning can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (numerical tuning and downstream evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Temperature chosen by grid search on held-out 10% aggregated set; performance validated by bootstrap on test samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Optimal temperature varies across confidence methods and models; improper scaling can negate benefits; requires held-out tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Aggregated across GSM8K, MATH, MMLU-Pro, Big-Bench-Hard (tuning used 10% held-out aggregated across datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4560.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregate experimental metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>% Cost Reduction and % Accuracy Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task-level metrics used to quantify CISC efficiency vs self-consistency: percent compute saved to reach equivalent accuracy, and percent accuracy gain when using same sample budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>% Cost Reduction; % Accuracy Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>% Cost Reduction = 100*(1 - CISC_budget / #Comparable_SC_responses) where #Comparable_SC_responses is number of SC samples needed to reach CISC accuracy (capped at 31 if >30). % Accuracy Improvement = 100*(CISC_Acc / SC_Acc - 1) when both use same samples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Computational efficiency (samples needed) and relative accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Empirical performance evaluation of aggregation methods</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Macro-averaged across datasets/models: P(True) produced ~41% (budget=5) and ~46% (budget=10) cost reduction; accuracy improvements small but significant (e.g., 1.6% at budget=5). Table 1 and detailed breakdowns provide dataset-level numbers (e.g., up to 53% reduction on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated numeric metrics derived from bootstrapped experiment results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Bootstrap sampling (draw 30 baseline samples, then draw n=500 sets of b paths per question and average), micro- and macro- averaging across datasets/models with bootstrap CIs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Metric depends on the cap for self-consistency comparisons (max 31), sample budget choices, and may not capture latency or throughput tradeoffs beyond sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4560.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrap evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap sampling procedure for decoding evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A resampling procedure used to estimate decoding strategy performance and variability by repeatedly sampling subsets of sampled reasoning paths and computing aggregated accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bootstrap sampling (30 initial traces, n=500 draws of b paths)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each question, sample 30 reasoning paths from the model; then draw n=500 random subsets of size b (1..30), apply decoding strategy s (SC or CISC) to each subset, compute per-set accuracy, and average across bootstrap samples to estimate performance and confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Estimates of mean accuracy for each decoding budget b and method, and enables computation of bootstrap CIs for statistical significance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / method evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation methodology for reasoning outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to produce the reported accuracy curves and to compute micro-averaged confidence intervals (Table 7). Demonstrated statistical significance of CISC improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated resampling methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Bootstrap sampling across combined dataset of ~150k rows with 10k bootstrap resamples to compute 95% CIs for aggregated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on having an initial large pool of samples (30); computational burden to create many initial traces; may depend on sampling diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4560.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human qualitative eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of low-quality reasoning indicators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small-scale human study where annotators flagged logical indicators of low-quality reasoning in sampled chains to test alignment with model-assigned confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human qualitative identification of low-quality reasoning indicators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Two unreferenced NLP PhD-level evaluators reviewed 90 model responses (triplets) and labeled presence/description of low-quality indicators (e.g., 'no choice', 'incomplete calculations', 'multiple candidates'). Evaluators were blind to model confidences and ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlation between human-identified low-quality indicators and model self-assessed low confidence; qualitative categories of reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-3B, Gemma2-9B, Mistral-123B (representative models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 9B, 123B (selected for qualitative study)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning / chain-of-thought quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human judgment of reasoning quality for model-generated explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>67% of samples the model marked as relative-low-confidence were also judged by humans to contain low-quality indicators; only 33% of model high-confidence samples had human-identified low-quality indicators. Categories 'No choice' and 'Incomplete calculations' had strong association with low model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based expert annotation (two annotators) correlated with model self-assessed confidences (hybrid validation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Interpretable categories and agreement statistics reported; human annotators blind to model confidences and labels; sampling ensured balanced sets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small sample (90 responses), annotators lacked specialization for MMLU-Pro tasks (dataset difficulty), results limited to MMLU domain and three models, potential annotator subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MMLU-Pro (qualitative subset)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4560.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4560.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablations (Max/Tie)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simplified ablation baselines: Max-confidence selection and Tie-breaker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation experiments comparing CISC to simpler heuristics: selecting the single response with maximum confidence (Max) and using confidence only to break ties in self-consistency (Tie).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Max and Tie ablation baselines</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Max: pick the answer of the single path with highest confidence. Tie: use CISC only when self-consistency results in a tie; otherwise use majority vote. Both evaluated using P(True) confidences and same bootstrap setup.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Final answer accuracy and cost reduction relative to full CISC and self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2, Qwen2.5, Mistral (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B–123B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Heuristic selection strategies for explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Max selection degraded performance (negative acc improvements); Tie baseline provided some gains but was outperformed by full CISC. Table 9 shows CISC (41%/46% cost reduction) >> Tie (~27%-28%), Max degenerated performance.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated experimental ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same bootstrap evaluation and aggregation as main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Max approach too brittle; tie-only approach wastes potential discrimination signal when majority is present.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GSM8K, MATH, MMLU-Pro, Big-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence Improves Self-Consistency in LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Teaching models to express their uncertainty in words <em>(Rating: 2)</em></li>
                <li>On calibration of modern neural networks <em>(Rating: 1)</em></li>
                <li>Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift <em>(Rating: 1)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4560",
    "paper_id": "paper-36b203d5fee14d34853021cb4bdcfa0d8e988986",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "CISC",
            "name_full": "Confidence-Informed Self-Consistency",
            "brief_description": "A decoding/aggregation framework that augments self-consistency by extracting a confidence score for each sampled chain-of-thought and performing a confidence-weighted majority vote after softmax normalization with a tunable temperature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Confidence-Informed Self-Consistency (CISC)",
            "evaluation_method_description": "Generate m reasoning paths (r_i,a_i), extract a scalar confidence c_i for each path, normalize confidences with softmax(c_i/T), and select the final answer by maximizing the sum of normalized confidences per answer: argmax_a sum_i I[a_i=a] * softmax(c_i/T). Temperature T controls frequency vs confidence tradeoff.",
            "evaluation_criteria": "Ability to correctly pick the true answer using fewer reasoning samples (efficiency), final answer accuracy (empirical accuracy), sensitivity to confidence signal (within-question discrimination), and computational cost (inference token generation).",
            "model_name": "Gemma2, Qwen2.5, Mistral (various instruction-tuned open models evaluated)",
            "model_size": "2B–123B (various; experiments used Gemma2-2B/9B/27B, Qwen 3B/14B/72B, Mistral 8B/22B/123B)",
            "scientific_domain": "General reasoning tasks (mathematical and commonsense reasoning benchmarks)",
            "theory_type": "Chain-of-thought explanations / multi-step reasoning outputs",
            "human_comparison": false,
            "evaluation_results": "CISC with the best confidence method (P(True)) reduced required sample size by &gt;40% on average (41% cost reduction for budget=5, 46% for budget=10) compared to standard self-consistency; corresponding relative accuracy improvements were modest but positive (1.6% and 1.1%). CISC outperformed self-consistency across nearly all examined models and datasets and across three confidence extraction methods.",
            "automated_vs_human_evaluation": "Automated: uses model-derived confidence scores, softmax normalization, and quantitative metrics (% Cost Reduction, % Accuracy Improvement). Hybrid validation includes a human qualitative study correlating model confidence with human-identified low-quality indicators.",
            "validation_method": "Empirical comparisons to self-consistency across nine models/four datasets, bootstrap sampling (n=500 draws per question from 30 sampled paths) to compute stability/CI, and correlation with human qualitative judgments; temperature tuned on 10% held-out set.",
            "limitations_challenges": "Requires reliable confidence extraction; performance sensitive to temperature scaling; access to token probabilities / P(True) may be unavailable in some frameworks; confidence calibration across questions does not guarantee within-question discrimination needed by CISC.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard (selected tasks)",
            "uuid": "e4560.0",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency majority-vote decoding",
            "brief_description": "Baseline decoding approach that samples multiple chain-of-thought traces and selects the most frequent final answer (majority vote) as the model output.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Self-Consistency (majority voting over sampled chains)",
            "evaluation_method_description": "Sample many reasoning paths from the model; aggregate final answers by simple frequency (argmax_a count(a_i=a)). No per-path confidence is used.",
            "evaluation_criteria": "Final answer accuracy as a function of the number of sampled reasoning paths (efficacy vs compute), robustness to sampling variability.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning tasks (mathematical and commonsense)",
            "theory_type": "Chain-of-thought explanations / multi-step reasoning outputs",
            "human_comparison": false,
            "evaluation_results": "Used as the baseline; observed to require many (up to 30+) samples to match CISC performance; CISC often achieved comparable or better accuracy with far fewer samples (e.g., CISC with 8 samples surpassed 30-sample self-consistency in one example).",
            "automated_vs_human_evaluation": "Automated (majority vote over sampled traces).",
            "validation_method": "Empirical comparison vs CISC across models/datasets using bootstrapped sampling.",
            "limitations_challenges": "Computationally expensive due to need for many long chain samples; ignores per-sample self-assessed confidence, which can reduce sampling efficiency.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.1",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "WQD",
            "name_full": "Within-Question Discrimination",
            "brief_description": "A metric introduced in this paper that measures how well confidence scores discriminate between correct and incorrect responses to the same question by counting within-question correct&gt;incorrect confidence orderings over sampled pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Within-Question Discrimination (WQD)",
            "evaluation_method_description": "For each question q with sampled responses R_q, partition into correct R_q^+ and incorrect R_q^−. WQD is the fraction of pairs (r in R_q^+, r' in R_q^−) for which c(r)&gt;c(r'). Formally: WQD(c) = (1/N) sum_q sum_{r in R_q^+} sum_{r' in R_q^-} [c(r)&gt;c(r')], where N = total pairs. In experiments m=30 samples per question.",
            "evaluation_criteria": "Within-question discrimination (ability of confidence to rank correct traces above incorrect traces), predictive power for use in weighted aggregation like CISC.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning / chain-of-thought quality assessment",
            "theory_type": "Evaluation of explanations/reasoning chains",
            "human_comparison": false,
            "evaluation_results": "WQD accurately predicted which confidence extraction methods yielded the best CISC performance: P(True) had the highest WQD (62.3%) and yielded the largest CISC cost reductions; verbal methods had better calibration but lower WQD and lower CISC gains. WQD values exceeded chance (&gt;50%) across models/datasets (Table 10).",
            "automated_vs_human_evaluation": "Automated metric computed over sampled model responses; compared against CISC performance (automated) and human qualitative signals (correlative evidence).",
            "validation_method": "Validated by correlation with downstream CISC cost-reduction and accuracy improvements (WQD predicted relative CISC performance across confidence methods), and by inspecting monotonic relationship between confidence gap and WQD (Figure 4).",
            "limitations_challenges": "Requires multiple sampled responses per question (m large enough, e.g., 30); does not capture absolute calibration across questions; sensitive to sampling diversity and definition of correctness for complex open-ended outputs.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.2",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Calibration metrics",
            "name_full": "Between-question calibration metrics (ECE, Brier Score, temperature-scaled variants ECE-t/Brier-t)",
            "brief_description": "Standard metrics that evaluate how well confidence values align with actual correctness probabilities across questions (between-question calibration), often after temperature scaling.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Expected Calibration Error (ECE) and Brier Score (temperature-scaled: ECE-t, Brier-t)",
            "evaluation_method_description": "ECE bins predicted confidences across examples and measures absolute difference between average confidence and empirical accuracy per bin; Brier score is mean squared error between predicted probability and true label. Temperature scaling fits a single T to minimize NLL before computing scaled metrics (ECE-t, Brier-t).",
            "evaluation_criteria": "Between-question calibration (are higher confidence values associated with higher empirical correctness across the dataset), overall probabilistic accuracy.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning / confidence estimation",
            "theory_type": "Evaluation of probabilistic confidence of answers/explanations",
            "human_comparison": false,
            "evaluation_results": "Verbal confidence methods obtained the best ECE-t and Brier-t scores (e.g., Verbal Binary ECE-t=0.005, Brier-t=0.187) but they performed worse in CISC compared to P(True). The authors show between-question calibration metrics were poor predictors of within-question discrimination and CISC utility.",
            "automated_vs_human_evaluation": "Automated metric evaluation (statistical).",
            "validation_method": "Metrics computed after temperature scaling (Ovadia et al., 2019); compared to WQD and CISC downstream performance to evaluate predictive validity.",
            "limitations_challenges": "Good between-question calibration does not imply good within-question discrimination necessary for selecting among responses to the same question; may be misleading for tasks where within-question ranking matters.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.3",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Confidence extraction methods",
            "name_full": "Response Probability, Verbal Confidence (Binary, 0-100), P(True)",
            "brief_description": "Three families of methods to extract per-path confidence: (1) model log-probability (length-normalized), (2) prompting the model to verbalize confidence (binary or 0–100), (3) asking a binary confidence token and using the model's assigned token probability (P(True)).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Response Probability / Verbal Confidence / P(True) confidence extraction",
            "evaluation_method_description": "Response Probability: compute length-normalized product of next-token probabilities for full (r,a). Verbal Binary/0-100: prompt model to output a confidence score after generating (r,a). P(True): prompt for 0/1 confidence and use the model's probability assigned to token '1' as a scalar confidence.",
            "evaluation_criteria": "How well per-path confidences correlate with actual correctness (within-question discrimination), calibration across questions, computational overhead to extract confidences, and compatibility with caching/prefix reuse.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning / chain-of-thought scoring",
            "theory_type": "Per-path confidence estimation for explanations/reasoning outputs",
            "human_comparison": false,
            "evaluation_results": "P(True) had the highest WQD (62.3%) and yielded the largest CISC cost reductions (41% at budget=5, 46% at budget=10) and best overall CISC accuracy improvements; verbal methods were better calibrated across questions but had worse WQD and reduced CISC gains. Response Probability performed intermediately.",
            "automated_vs_human_evaluation": "Automated (model-internal probabilities and prompted scores). Also correlated with human qualitative judgments in separate analysis.",
            "validation_method": "Compared via ECE-t/Brier-t and WQD; downstream validated by CISC performance and bootstrap confidence intervals.",
            "limitations_challenges": "Verbal prompts can be well-calibrated but may not discriminate within-question; response-probability can be affected by sequence length and tokenization; P(True) requires access to model token probabilities and careful prompt formatting to avoid confusion.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.4",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Confidence Normalization",
            "name_full": "Softmax normalization with temperature scaling",
            "brief_description": "Within-question normalization of per-path confidence scores using softmax with a tunable temperature to produce relative weights prior to aggregation; temperature tuned on held-out data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Softmax confidence normalization (temperature-scaled)",
            "evaluation_method_description": "Given raw confidences c_i for m paths, compute |tilde{c}_i = exp(c_i / T) / sum_j exp(c_j / T). Temperature T is tuned per model and confidence method on a 10% held-out set to control peakiness (T-&gt;inf collapses to uniform weights; T-&gt;0 selects max).",
            "evaluation_criteria": "Enhances discriminative power of confidence weights for within-question ranking and stabilizes aggregation; measured by downstream CISC performance (cost reduction, accuracy).",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning",
            "theory_type": "Aggregation weighting of explanation confidences",
            "human_comparison": false,
            "evaluation_results": "Normalization with tuned temperature consistently improved CISC versus unnormalized confidences across methods (e.g., P(True) cost reduction improved from 32% to 46% at budget=10). Using softmax without temperature tuning can hurt performance.",
            "automated_vs_human_evaluation": "Automated (numerical tuning and downstream evaluation).",
            "validation_method": "Temperature chosen by grid search on held-out 10% aggregated set; performance validated by bootstrap on test samples.",
            "limitations_challenges": "Optimal temperature varies across confidence methods and models; improper scaling can negate benefits; requires held-out tuning data.",
            "benchmark_dataset": "Aggregated across GSM8K, MATH, MMLU-Pro, Big-Bench-Hard (tuning used 10% held-out aggregated across datasets)",
            "uuid": "e4560.5",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Aggregate experimental metrics",
            "name_full": "% Cost Reduction and % Accuracy Improvement",
            "brief_description": "Task-level metrics used to quantify CISC efficiency vs self-consistency: percent compute saved to reach equivalent accuracy, and percent accuracy gain when using same sample budget.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "% Cost Reduction; % Accuracy Improvement",
            "evaluation_method_description": "% Cost Reduction = 100*(1 - CISC_budget / #Comparable_SC_responses) where #Comparable_SC_responses is number of SC samples needed to reach CISC accuracy (capped at 31 if &gt;30). % Accuracy Improvement = 100*(CISC_Acc / SC_Acc - 1) when both use same samples.",
            "evaluation_criteria": "Computational efficiency (samples needed) and relative accuracy improvements.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning",
            "theory_type": "Empirical performance evaluation of aggregation methods",
            "human_comparison": false,
            "evaluation_results": "Macro-averaged across datasets/models: P(True) produced ~41% (budget=5) and ~46% (budget=10) cost reduction; accuracy improvements small but significant (e.g., 1.6% at budget=5). Table 1 and detailed breakdowns provide dataset-level numbers (e.g., up to 53% reduction on GSM8K).",
            "automated_vs_human_evaluation": "Automated numeric metrics derived from bootstrapped experiment results.",
            "validation_method": "Bootstrap sampling (draw 30 baseline samples, then draw n=500 sets of b paths per question and average), micro- and macro- averaging across datasets/models with bootstrap CIs.",
            "limitations_challenges": "Metric depends on the cap for self-consistency comparisons (max 31), sample budget choices, and may not capture latency or throughput tradeoffs beyond sample counts.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.6",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Bootstrap evaluation",
            "name_full": "Bootstrap sampling procedure for decoding evaluation",
            "brief_description": "A resampling procedure used to estimate decoding strategy performance and variability by repeatedly sampling subsets of sampled reasoning paths and computing aggregated accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Bootstrap sampling (30 initial traces, n=500 draws of b paths)",
            "evaluation_method_description": "For each question, sample 30 reasoning paths from the model; then draw n=500 random subsets of size b (1..30), apply decoding strategy s (SC or CISC) to each subset, compute per-set accuracy, and average across bootstrap samples to estimate performance and confidence intervals.",
            "evaluation_criteria": "Estimates of mean accuracy for each decoding budget b and method, and enables computation of bootstrap CIs for statistical significance.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning / method evaluation",
            "theory_type": "Evaluation methodology for reasoning outputs",
            "human_comparison": false,
            "evaluation_results": "Used to produce the reported accuracy curves and to compute micro-averaged confidence intervals (Table 7). Demonstrated statistical significance of CISC improvements.",
            "automated_vs_human_evaluation": "Automated resampling methodology.",
            "validation_method": "Bootstrap sampling across combined dataset of ~150k rows with 10k bootstrap resamples to compute 95% CIs for aggregated metrics.",
            "limitations_challenges": "Relies on having an initial large pool of samples (30); computational burden to create many initial traces; may depend on sampling diversity.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.7",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Human qualitative eval",
            "name_full": "Human evaluation of low-quality reasoning indicators",
            "brief_description": "A small-scale human study where annotators flagged logical indicators of low-quality reasoning in sampled chains to test alignment with model-assigned confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human qualitative identification of low-quality reasoning indicators",
            "evaluation_method_description": "Two unreferenced NLP PhD-level evaluators reviewed 90 model responses (triplets) and labeled presence/description of low-quality indicators (e.g., 'no choice', 'incomplete calculations', 'multiple candidates'). Evaluators were blind to model confidences and ground truth.",
            "evaluation_criteria": "Correlation between human-identified low-quality indicators and model self-assessed low confidence; qualitative categories of reasoning errors.",
            "model_name": "Qwen2.5-3B, Gemma2-9B, Mistral-123B (representative models)",
            "model_size": "3B, 9B, 123B (selected for qualitative study)",
            "scientific_domain": "General reasoning / chain-of-thought quality assessment",
            "theory_type": "Human judgment of reasoning quality for model-generated explanations",
            "human_comparison": true,
            "evaluation_results": "67% of samples the model marked as relative-low-confidence were also judged by humans to contain low-quality indicators; only 33% of model high-confidence samples had human-identified low-quality indicators. Categories 'No choice' and 'Incomplete calculations' had strong association with low model confidence.",
            "automated_vs_human_evaluation": "Human-based expert annotation (two annotators) correlated with model self-assessed confidences (hybrid validation).",
            "validation_method": "Interpretable categories and agreement statistics reported; human annotators blind to model confidences and labels; sampling ensured balanced sets.",
            "limitations_challenges": "Small sample (90 responses), annotators lacked specialization for MMLU-Pro tasks (dataset difficulty), results limited to MMLU domain and three models, potential annotator subjectivity.",
            "benchmark_dataset": "MMLU-Pro (qualitative subset)",
            "uuid": "e4560.8",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Ablations (Max/Tie)",
            "name_full": "Simplified ablation baselines: Max-confidence selection and Tie-breaker",
            "brief_description": "Ablation experiments comparing CISC to simpler heuristics: selecting the single response with maximum confidence (Max) and using confidence only to break ties in self-consistency (Tie).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Max and Tie ablation baselines",
            "evaluation_method_description": "Max: pick the answer of the single path with highest confidence. Tie: use CISC only when self-consistency results in a tie; otherwise use majority vote. Both evaluated using P(True) confidences and same bootstrap setup.",
            "evaluation_criteria": "Final answer accuracy and cost reduction relative to full CISC and self-consistency.",
            "model_name": "Gemma2, Qwen2.5, Mistral (various)",
            "model_size": "2B–123B (various)",
            "scientific_domain": "General reasoning",
            "theory_type": "Heuristic selection strategies for explanations",
            "human_comparison": false,
            "evaluation_results": "Max selection degraded performance (negative acc improvements); Tie baseline provided some gains but was outperformed by full CISC. Table 9 shows CISC (41%/46% cost reduction) &gt;&gt; Tie (~27%-28%), Max degenerated performance.",
            "automated_vs_human_evaluation": "Automated experimental ablation.",
            "validation_method": "Same bootstrap evaluation and aggregation as main experiments.",
            "limitations_challenges": "Max approach too brittle; tie-only approach wastes potential discrimination signal when majority is present.",
            "benchmark_dataset": "GSM8K, MATH, MMLU-Pro, Big-Bench-Hard",
            "uuid": "e4560.9",
            "source_info": {
                "paper_title": "Confidence Improves Self-Consistency in LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Teaching models to express their uncertainty in words",
            "rating": 2
        },
        {
            "paper_title": "On calibration of modern neural networks",
            "rating": 1
        },
        {
            "paper_title": "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift",
            "rating": 1
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.01933325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Confidence Improves Self-Consistency in LLMs</h1>
<p>Amir Taubenfeld ${ }^{<em> 12}$, Tom Sheffer ${ }^{</em> 12}$, Eran Ofek ${ }^{1}$, Amir Feder ${ }^{13}$, Ariel Goldstein ${ }^{2}$, Zorik Gekhman ${ }^{1}$, Gal Yona ${ }^{1}$<br>${ }^{1}$ Google Research, ${ }^{2}$ The Hebrew University of Jerusalem, ${ }^{3}$ Columbia University<br>{amirt,tomsheffer}@google.com</p>
<h4>Abstract</h4>
<p>Self-consistency decoding enhances LLMs' performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce ConfidenceInformed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms selfconsistency in nearly all configurations, reducing the required number of reasoning paths by over $40 \%$ on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.</p>
<h2>1 Introduction</h2>
<p>Modern large language models (LLMs) demonstrate strong reasoning capabilities (Bubeck et al., 2023; Guo et al., 2025), driven in part by their capacity to generate a sequence of intermediate reasoning steps that lead them toward a final answer (Wei et al., 2022; Jaech et al., 2024). Selfconsistency (Wang et al., 2022) is a popular decoding strategy that further improves LLMs' reasoning performance by sampling a diverse set of reasoning paths and selecting the most frequent answer</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Accuracy as a function of the number of sampled responses for self-consistency vs CISC, using Gemma2-9B on the MATH dataset. CISC achieves higher overall accuracy while significantly reducing computational costs. With just 8 samples, it surpasses the performance of 30 -sample self-consistency.
as the final output. Despite its effectiveness, this approach is also computationally expensive, as it requires generating a large number of (long) reasoning paths to increase the chances that the correct answer emerges as the most frequent one.</p>
<p>Motivated by recent evidence that LLMs possess the ability to judge the correctness of their own outputs (Kadavath et al., 2022; Zhang et al., 2024), we hypothesize that self-consistency could be made significantly more efficient if the model could review each generated reasoning path before selecting a final answer. We therefore introduce Confidence-Informed Self-Consistency (CISC), a lightweight extension of self-consistency. As illustrated in Figure 2, CISC uses the model to generate a self-assessment score for each path and employs these scores in a weighted majority vote.</p>
<p>We conducted a comprehensive comparison of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A simplified example comparing self-consistency vs CISC. (1) Given an input question, (2) both methods first sample multiple reasoning paths. (4, top) Self-consistency then simply selects the most frequent answer. Conversely, (3) CISC adds a self-assessment step, where a confidence score is assigned to each path (see §4.1 for more advanced methods). Then, (4, bottom) it selects the final answer via a weighted majority vote.</p>
<p>CISC and self-consistency, spanning nine LLMs of various sizes, four datasets covering a wide range of mathematical and commonsense reasoning tasks, and three popular methods for deriving self-assessment confidence scores from the model. Our results demonstrate that CISC outperforms self-consistency in virtually all the examined configurations. Using the best-performing confidence estimation method, CISC achieves comparable performance to self-consistency while reducing the required number of reasoning paths by over 40% on average (See Figure 1 for an example).</p>
<p>Surprisingly, the most calibrated confidence method is actually the least useful for CISC. We offer a potential explanation: existing confidence evaluation metrics measure the usefulness of confidence scores for comparing answers across different questions, while CISC requires distinguishing correct and incorrect answers for the same question. To address this, we propose the Within-Question Discrimination (WQD) metric that specifically measures this ability, and demonstrate that it can predict the relative performance of CISC with different confidence methods.</p>
<p>Finally, we conduct a qualitative-analysis and find a significant agreement between model confidence scores and human assessments of the reasoning-paths' quality. Specifically, responses identified by the model as low-confidence were also significantly more likely to be flagged by human evaluators as exhibiting signs of low-quality reasoning patterns.</p>
<p>To summarize, we contribute practical methods and foundational insights:</p>
<ul>
<li>We propose CISC, a decoding strategy that can be used as a drop-in replacement to self-consistency, achieving comparable accuracy at a significantly lower computational cost.</li>
<li>We introduce the concept of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question.</li>
<li>We present empirical evidence supporting the idea that LLMs are capable of self-assessing their responses, contributing to the ongoing debate regarding this capability (Gero et al., 2023; Huang et al., 2023; Li et al., 2024a; Stechly et al., 2024)</li>
<li>We open-source our implementation of CISC to facilitate further research<sup>1</sup>.</li>
</ul>
<h2>2 Notations</h2>
<p>We consider an auto-regressive language model <em>M</em> with parameters θ. We use <em>pθ</em>(·|<em>x</em>) to denote <em>M</em>'s distribution over the next token given the provided context <em>x</em>. Given a question <em>q</em> (e.g., "<em>Jane had 4 apples and ate half of her apples. How many apples she has now?")</em>, we denote the model's response as (<strong>r</strong>, <strong>a</strong>), where <strong>a</strong> is the answer (e.g., "2") and <strong>r</strong> is a <em>reasoning path</em> (or chain-of-thought), a sequence of logical steps supposedly leading up to this answer (e.g., "<em>If Jane ate half her apples, this means she ate 2 apples. 4 minus 2 is 2.</em>").</p>
<p><sup>1</sup>Our code is available at https://github.com/google-research/google-research/tree/master/cisc.</p>
<p>3 Confidence-Informed Self-Consistency</p>
<p>In this section we present Confidence-Informed Self-Consistency (CISC). When designing CISC, we hypothesized that it is possible to reduce selfconsistency's computational costs by generating a confidence score for each reasoning path, and performing a weighted majority vote.</p>
<p>As an intuitive example, consider a hypothetical setting where there exist only two possible answers, one correct and one incorrect. For a model that responds with the correct answer $60 \%$ of the time, standard majority voting will require 40 samples to reach $90 \%$ accuracy ${ }^{2}$. However, a weighted majority vote that weights correct answers twice as much as incorrect ones, will achieve $90 \%$ accuracy with less than 10 samples.</p>
<p>With this motivation in mind, we build on recent findings suggesting that LLMs are capable of judging the correctness of their own outputs (Kadavath et al., 2022; Tian et al., 2023b; Zhang et al., 2024), and incorporate the model's self-assessment of its reasoning paths into the final answer selection:
Definition 3.1 (Confidence-Informed Self-Consistency). Given a question $q$ and responses $\left{\left(\boldsymbol{r}<em 1="1">{1}, \boldsymbol{a}</em>}\right), \ldots,\left(\boldsymbol{r<em m="m">{m}, \boldsymbol{a}</em>\right)\right}$, CISC involves:</p>
<ul>
<li>Confidence Extraction: A self-assessed confidence score $c_{i} \in \mathbb{R}$ is derived for each $\left(\boldsymbol{r}<em i="i">{i}, \boldsymbol{a}</em>\right)$.</li>
<li>Confidence Normalization: The confidence scores are normalized using Softmax: $\tilde{c}<em i="i">{i}=$ $\frac{\exp (\frac{c</em>$, where $T$ is a tunable temperature hyper-parameter (see discussion below).}}{T})}{\sum_{j=1}^{m} \exp \left(\frac{c_{j}}{T}\right)</li>
<li>Aggregation: The final answer is selected using a confidence-weighted majority vote: $\tilde{a}<em a="a">{C I S C}=\arg \max </em>} \sum_{i=1}^{m} \boldsymbol{I}\left[\boldsymbol{a<em i="i">{i}=a\right] \cdot \tilde{c}</em>$.</li>
</ul>
<p>The temperature parameter $T$ controls the relative importance of the answer frequency versus the confidence scores. Namely, as $T \rightarrow \infty$, the distribution of normalized confidence scores approaches the uniform distribution, and CISC collapses to vanilla self-consistency. Conversely, as $T \rightarrow 0$, the softmax normalization approaches the hard maximum function, prioritizing the single response with the highest confidence and disregarding the overall frequency of answers. This may lead CISC to select a different answer than self-consistency (see Figure 2).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>4 Experimental Setup</h2>
<p>We compare CISC and self-consistency across a range of confidence extraction methods (§4.1), reasoning tasks (§4.2) and models (§4.3).</p>
<h3>4.1 Confidence Extraction Methods</h3>
<p>We use the following methods:</p>
<ul>
<li>Response Probability (Wang et al., 2022): The confidence in a response $(\mathbf{r}, \mathbf{a})$ is taken to be the model's (length-normalized) probability of generating $(\mathbf{r}, \mathbf{a})=\left(x_{1}, \ldots, x_{n}\right)$ given the question:</li>
</ul>
<p>$$
p_{\theta}(\mathbf{r}, \mathbf{a})=\left[\Pi_{i=1}^{n} p_{\theta}\left(x_{i} \mid x_{1} \ldots x_{i-1}, q\right)\right]^{\frac{1}{n}}
$$</p>
<ul>
<li>Verbal Confidence (Lin et al., 2022): After sampling $(\mathbf{r}, \mathbf{a})$ from the model, we prompt it to rate its confidence in its previously generated output. We implement two variants: (1) Verbal Binary instructs the model to output either 0 or 1 , and (2) Verbal 0-100 instructs the model to output a score on a scale of 0-100.</li>
<li>P(True) Kadavath et al. (2022): We prompt the model to rate its confidence in $(\mathbf{r}, \mathbf{a})$ in binary format (either 0 or 1 ), and compute the probability that the model assigns to the token 1.</li>
</ul>
<p>Efficient and Consistent Confidence Prompting. Our implementation of the prompt-based methods employs a two-step prompting procedure (as depicted in Figure 2). Given a question prompt $q$, we first use the model to generate the reasoning chain and answer $(r, a)$. We then concatenate a confidence extraction prompt $e$ (e.g., "Now I will rate my confidence..."), and continue the generation on $(q, r, a, e)$. This serves two important purposes. First, it ensures that when comparing selfconsistency and CISC, the reasoning chains are identical. Second, the fact that the prefix $(q, r, a)$ remains unchanged after concatenating the confidence extraction prompt $e$ means it does not require reprocessing by the LLM. Consequently, the additional cost of the confidence extraction step consists only of encoding $\operatorname{len}(e) \approx 20$ tokens and generating a single token. Since a single $(q, r, a)$ typically contains hundreds of tokens, the confidence extraction step adds only a negligible computational overhead to self-consistency. Further overhead reduction can be achieved through prompt optimization or by using the single-step procedure described in Appendix B. The precise prompts used and additional technical details are also provided in Appendix B.</p>
<p>4.2 Datasets</p>
<p>We used four large reasoning benchmarks: ${ }^{3}$</p>
<ul>
<li>GSM8K (Cobbe et al., 2021a): A dataset of grade-school level math word problems. We evaluate on the entire validation set (1320 questions).</li>
<li>MATH (Hendrycks et al., 2021): A more challenging dataset of math word problems. We used the entire test set (5K questions).</li>
<li>MMLU-Pro (Wang et al., 2024d): A more challenging version of the Multitask Language Understanding (MMLU) benchmark, testing language models' general knowledge and reasoning abilities with a wide range of topics such as science and history. We randomly sampled 5 K questions.</li>
<li>Big-Bench-Hard (Suzgun et al., 2022): A challenging selection of tasks from the big-bench benchmark (bench authors, 2023), comprises a variety of reasoning tasks that pose challenges to LLMs, such as counting objects. We selected 20 out of 23 tasks (5,761 examples), eliminating three sub-tasks that required designated answer extraction methods.</li>
</ul>
<h3>4.3 Models</h3>
<p>We use nine instruction-tuned open-weights LLMs from 3 different families:</p>
<ul>
<li>GEMMA2 (Team et al., 2024): A Google AI model family, including 2B, 9B, and 27B parameter models.</li>
<li>QWEN2.5 (Yang et al., 2024): A model family from Alibaba AI, with 7 models ranging from 0.5 B to 72 B parameters. We selected three models: 3B, 14B, and 72B.</li>
<li>Mistral (Mistral-AI, 2024): We used three of the latest models available - Ministral-8B-Instruct-2410, Mistral-Small-Instruct-2409, mistralai/Mistral-Large-Instruct-2411 - with 8B, 22B, 123B parameters respectively.</li>
</ul>
<h3>4.4 Metrics</h3>
<p>We compare CISC against self-consistency using the following metrics:</p>
<ul>
<li>\% Cost Reduction: The percentage of computational cost saved by using CISC. We fix the compute budget for CISC ( 5 or 10 model responses)</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and measure the number of responses ${ }^{4}$ required for self-consistency to achieve equivalent accuracy:</p>
<p>$$
100 \times\left(1-\frac{\text { CISC budget }}{\text { # Comparable SC responses }}\right)
$$</p>
<ul>
<li>\% Accuracy Improvement: The relative accuracy gain of CISC over self-consistency when both methods utilize the same number of responses per question:</li>
</ul>
<p>$$
100 \times\left(\frac{\text { CISC Acc }}{\text { SC Acc }}-1\right)
$$</p>
<h3>4.5 Temperature Scaling</h3>
<p>As discussed in §3, CISC re-scales the confidence values using a softmax transformation, parameterized by a temperature $T&gt;0$. We tune the temperature separately for each model and confidence extraction method using a $10 \%$ held-out set, aggregated across all four datasets (§4.2). The fact that CISC only employees a single dataset-agnostic hyper-parameter, makes the tuning process lightweight and robust. More details and the optimal temperature values for each configuration are in appendix D.</p>
<h3>4.6 Bootstrap</h3>
<p>To compute the performance of a decoding strategy $s$ (either self-consistency or a variant of CISC) with a sample budget of $b \in[1, \ldots, 30]$, we perform bootstrap sampling. We first sample 30 different reasoning paths from the model. Next, we draw $n=500$ sets of $b$ paths for each question, apply $s$ to each set, and compute the accuracy per set. We then average the results across all bootstrap samples to obtain the final score.</p>
<h2>5 Main Results</h2>
<p>This section demonstrates CISC's (§3.1) substantial performance advantage over self-consistency. We compare CISC, using fixed compute budgets of 5 and 10 responses per question, based on the metrics defined in $\S 4.4$.</p>
<p>CISC outperforms self-consistency across virtually all models and datasets. Table 1 presents the Cost Reduction and Accuracy Improvement (see §4.4) achieved by CISC with each confidence</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Cost Reduction</th>
<th></th>
<th>Acc Improvement</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Confidence Method</td>
<td>Budget 5</td>
<td>Budget 10</td>
<td>Budget 5</td>
<td>Budget 10</td>
</tr>
<tr>
<td>Verbal Binary</td>
<td>$18 \%(6.1)$</td>
<td>$10 \%(11.1)$</td>
<td>$0.4 \%$</td>
<td>$0.2 \%$</td>
</tr>
<tr>
<td>Verbal 1-100</td>
<td>$22 \%(6.4)$</td>
<td>$30 \%(14.4)$</td>
<td>$0.8 \%$</td>
<td>$0.4 \%$</td>
</tr>
<tr>
<td>Response Probability</td>
<td>$22 \%(6.5)$</td>
<td>$31 \%(14.6)$</td>
<td>$1.1 \%$</td>
<td>$0.8 \%$</td>
</tr>
<tr>
<td>P(True)</td>
<td>$\mathbf{4 1 \% ( 8 . 4 )}$</td>
<td>$\mathbf{4 6 \% ( 1 8 . 6 )}$</td>
<td>$\mathbf{1 . 6 \%}$</td>
<td>$\mathbf{1 . 1 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 1: CISC performance (macro-averaged over all datasets and models) per confidence method. CISC performs better than standard self-consistency in terms of both efficiency gains and accuracy improvements across all confidence methods. Specifically, the P-True method achieves the best results. For instance, self-consistency must use 18.6 sampled responses on average to match the accuracy obtained by CISC using only 10 samples, representing a $46 \%$ reduction in computational costs.
method. The results are macro-averaged across all models and datasets. CISC outperforms selfconsistency with every confidence method.</p>
<p>The $P($ True $)$ method yields the best results, achieving an average Cost Reduction of $41 \%$ and $46 \%$ with budgets of 5 and 10 responses, respectively. Figure 3 presents a detailed breakdown of CISC's performance using $P($ True $)$ across all models and datasets. Notably, CISC is effective across nearly all configurations, with some exceeding $67 \%$ cost reduction.</p>
<p>We provide additional results in Appendix C. In particular, Table 6 shows a per-dataset breakdown of Table 1, and Table 7 shows the Accuracy Improvement metric micro-averaged across configurations, which enables the computation of confidence intervals. These demonstrate that the observed improvements of CISC (for each confidence method examined) are strongly statistically significant.</p>
<h2>Confidence Normalization improves CISC's per-</h2>
<p>formance. We drill down into the importance of the within-question confidence normalization step in CISC. In Table 2, we compare CISC's performance with and without confidence normalization. We see that for every confidence method examined, CISC with normalization (softmax with a tunable temperature value) outperforms its unnormalized counterpart. However, as shown in Supplementary Table 8, normalization is effective only when using appropriate temperature hyper-parameters. Because different confidence extraction methods produce scores on different scales, their optimal temperatures vary considerably (values are provided in Supplementary Figure 8). For instance, the P(True) method yields confidence values with high similarity, thus requiring lower temperatures
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results breakdown for CISC using the P(True) method and a budget of 10 responses per question. Each cell is annotated with the Cost Reduction (Percentage; §4.4) of CISC compared to self-consistency. As can be seen, CISC improves performance across almost all model families and datasets. In many cases, even 30 samples are not enough for self-consistency to reach CISC performance, leading to Cost Reduction of over $67 \%$.
to distinguish between them.</p>
<h2>6 Within-Question Confidence Evaluation</h2>
<p>Recent work demonstrated that verbal confidence methods significantly outperform $\mathrm{P}($ True $)$ in terms of calibration (Tyen et al., 2023), which is the defacto approach to evaluate the quality of confidence measures. Yet, perhaps surprisingly, CISC is more effective with $\mathrm{P}($ True $)$ than with verbal confidence methods (Table 1). In this section we settle these differences, and explain why well-calibrated confidence measures can still be less useful for CISC.</p>
<p>We argue that existing evaluation metrics, whether calibration based (Kadavath et al., 2022; Tian et al., 2023b) or discrimination based (Kuhn et al., 2023; Nguyen et al., 2024) examine the confi-</p>
<table>
<thead>
<tr>
<th>Confidence Method</th>
<th>Cost Reduction @ 10</th>
</tr>
</thead>
<tbody>
<tr>
<td>P(True) (w/o normalization)</td>
<td>32% (14.8)</td>
</tr>
<tr>
<td>P(True) (w/ normalization)</td>
<td>46% (18.6)</td>
</tr>
<tr>
<td>SP (w/o normalization)</td>
<td>24% (13.1)</td>
</tr>
<tr>
<td>SP (w/ normalization)</td>
<td>31% (14.6)</td>
</tr>
<tr>
<td>Verbal (w/o normalization)</td>
<td>20% (12.5)</td>
</tr>
<tr>
<td>Verbal (w/ normalization)</td>
<td>30% (14.4)</td>
</tr>
</tbody>
</table>
<p>Table 2: CISC performance with and without confidence normalization (bottom and top rows, respectively). We see that while CISC demonstrates substantial cost reductions even without normalization, employing normalization (Softmax and temperature scaling) significantly enhances performance, across all three confidence methods.</p>
<p>| Confidence | ECE-t $\downarrow$ | Brier-t $\downarrow$ | WQD $\uparrow$ | CISC Cost |
| Method |  |  |  | Reduction $\uparrow$ |
| --- | --- | --- | --- | --- |
| Verbal Binary | 0.005 | 0.187 | 52.2% | 10% |
| Verbal 0-100 | 0.046 | 0.173 | 56.1% | 30% |
| Response Prob. | 0.090 | 0.192 | 59.0% | 31% |
| P(True) | 0.030 | 0.182 | 62.3% | 46% |</p>
<p>Table 3: Comparison of different confidence extraction methods in terms of between-question and within-question confidence evaluation metrics. We see that between-question metrics (ECE-t, Brier-t) are poor indicators of effective confidence extraction for CISC, while our novel WQD metric (6.1) effectively predicts which confidence extraction method yields the best CISC performance.
dence behavior between the input questions. However, for CISC to work well, we want the confidence scores to be able to distinguish correct and incorrect responses to the same question.</p>
<p>To gain an intuition for the difference between within-question and between-question confidence evaluation, consider the following simple example. Imagine a model $M$ and a dataset with two types of questions: questions that $M$ finds "easy" (e.g., answers correctly $95 \%$ of the time) and questions that $M$ finds "hard" (e.g., answers correctly 5\% of the time). Consider a confidence measure that assigns every answer to an "easy" question a confidence of 0.95 and every answer to a hard question a confidence of 0.05 . This confidence signal is useless for CISC, as it does not make any distinctions between answers to the same question. On the other hand, it scores well under existing metrics (e.g., it is perfectly calibrated).</p>
<p>The above thought experiment shows that the fact that well-calibrated confidence scores can be derived from a model does not necessarily imply
the model possesses a capacity to self-assess its own responses. To isolate this specific ability, we design a metric that measures whether the confidence scores can distinguish correct and incorrect responses to the same question:
Definition 6.1 (Within-Question Discrimination). Given a dataset of questions, for each question $q$, denote the sampled responses by $R_{q}=$ $\left{\left(\boldsymbol{r}<em i="i">{i}, \boldsymbol{a}</em>\right)\right}<em q="q">{i=1}^{m}$, and let $R</em>$ as:}^{+}, R_{q}^{-} \subseteq R_{q}$ be the subsets of correct and incorrect responses respectively. We evaluate the Within-Question Discrimination (WQD) of a confidence method $c:(r, a) \mapsto \mathbb{R</p>
<p>$$
\begin{aligned}
&amp; \operatorname{WQD}(c) \equiv \
&amp; \frac{1}{N} \cdot \sum_{q} \sum_{(r, a) \in R_{q}^{+}} \sum_{\left(r^{\prime}, a^{\prime}\right) \in R_{q}^{-}}\left[c(r, a)&gt;c\left(r^{\prime}, a^{\prime}\right)\right]
\end{aligned}
$$</p>
<p>where $N=\sum_{q}\left|R_{q}^{+}\right| \cdot\left|R_{q}^{-}\right|$.
That is, we compute the fraction of cases where the higher confidence response is indeed the correct response, out of pairs of responses to the same question (exactly one of which is correct). In our work, we use $m=30$ (as described in $\S 4.6$ ).</p>
<p>To emphasize the importance of within-question evaluation, we test if WQD is more predictive of CISC's success than standard between-question confidence metrics. We compare each confidence method from $\S 4.1$ in terms of: (i) standard metrics, such as ECE (Guo et al., 2017) and Brier Score (Brier, 1950), (ii) WQD, (iii) CISC performance at a budget of 10 samples. We follow previous work (Tyen et al., 2023) and report the standard metrics after applying temperature scaling (Ovadia et al., 2019), a technique that fits a single temperature parameter $T$ to the model's confidences to minimize the negative log-likelihood on the data. We use ECE-t and Brier-t to denote the scaled scores.</p>
<p>The results of this comparison, averaged across all datasets (§4.2) and models (§4.3), are summarized in Table 3. Indeed, we see that the verbal confidence methods obtain the best ECE-t and Brier-t scores while also achieving the worst performance in CISC. On the other hand, the WQD metric is able to perfectly predict the relative scores of each confidence method in CISC. This emphasizes the limitations of relying solely on traditional confidence evaluation methods for evaluating the models ability to self-assess its reasoning.</p>
<p>The WQD metric prioritizes interpretability, focusing on the discrimination ability of the confidence scores irrespective of the relative magnitude</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Within-Question Discrimination score (indicated by color) increases smoothly as a function of the confidence gap (percentiles, x-axis). Here we use the P(True) method, Gemma2-9B and the MATH dataset.
of the confidence values $c(r, a)$ and $c\left(r^{\prime}, a^{\prime}\right)$. However, examining the relationship between WQD and the confidence gap $\left|c(r, a)-c\left(r^{\prime}, a^{\prime}\right)\right|$ offers additional insights. Figure 4 illustrates a near monotonic relationship: the within-question discrimination ability (indicated by color) smoothly increases with the confidence gap (x-axis). These findings suggest a fine-grained self-assessment mechanism, where even small differences in confidence scores reflect significant variations in the probability of a response being correct</p>
<p>Taken together, our findings provide a compelling evidence that LLMs indeed posses an intrinsic ability to reassess their own responses.</p>
<h2>7 Qualitative Analysis</h2>
<p>In $\S 5$ we showed that CISC has clear performance advantages over standard self-consistency, and argued that this suggests LLMs are capable of selfassessing their confidence in responses to the same question. To facilitate a better understanding of this phenomenon, we asked human evaluators to identify indicators of low-quality model responses (i.e., logical patterns that reduced the evaluators' confidence in the correctness of the LLM response). Our analysis revealed a strong correlation between the prevalence of these indicators and lower confidence scores assigned by the LLM.</p>
<p>Sampling Process. We performed the analysis on MMLU-Pro (§4.2), using three representative models, one from each model family.</p>
<p>To reduce the evaluation burden we limited it to three LLM responses per question. We selected these triplets based on two criteria: (1) CISC and SC produced different results, where one method yielded a correct answer and the other did not, and (2) the final answers of the three responses were not all distinct, which would otherwise degenerate self-consistency's majority voting.</p>
<p>Out of the remaining triplets, we randomly chose 45 for which SC was correct and 45 where SC was wrong. Then, for each triplet, we randomly took
either the response with highest relative-confidence or the response with lowest relative-confidence. This ensured an equal number of low relativeconfidence responses that were correct and incorrect, mitigating potential bias of answer correctness on our analysis. The process resulted in 90 responses for human evaluation.</p>
<p>Human Evaluation. Two human evaluators (NLP Phd students), unaware of both the model's confidence scores and the ground truth labels, reviewed 90 samples. The evaluators' task was to identify logical patterns in the LLM reasoningchain which reduce their confidence that the LLM has reached a correct answer; we call these patterns low-quality-indicators. Also, the evaluators were asked to briefly describe each identified pattern.</p>
<p>Results. Our evaluation demonstrated a significant correlation in confidence assessments: $67 \%$ of the samples assessed as relative-low confidence by the model were also judged to contain low-quality indicators by human evaluators, while only $33 \%$ of the samples assessed as relative-high confidence by the model contained the human identified low-quality-indicators. This strong correlation suggests that LLMs are adept at assessing their own reasoning processes and identifying patterns that humans consider indicative of low quality.</p>
<p>In addition, we categorized these low-quality indicators. Three primary categories emerged: (1) the LLM's final answer was not among the provided options; (2) the LLM deliberated between multiple options; and (3) the LLM omitted necessary calculations. Of these, only categories (1) and (3) showed a strong correlation with the LLM's low-confidence scores. Further details regarding these categories and their correlation statistics are available in the Appendix E.</p>
<h2>8 Related Work</h2>
<p>Confidence signals for LLMs. There is a long line of work on deriving confidence measures from LLMs. Popular approaches use the agreement across multiple samples (Kuhn et al., 2023; Manakul et al., 2023; Tian et al., 2023a; Lyu et al., 2024; Gekhman et al., 2024), the model's internal representations (Azaria and Mitchell, 2023; Burns et al., 2022; Orgad et al., 2025) or directly prompting the model to verbalize its confidence (Tian et al., 2023b; Kadavath et al., 2022). All papers in this line of work focused on fact-seeking tasks, so confi-</p>
<p>dence is typically derived based on the final answer alone. To the best of our knowledge, our work is the first to apply these approaches to scoring the entire reasoning path.</p>
<p>Reasoning verification. While learned verifiers have been demonstrated to significantly improve performance on math word problems (Cobbe et al., 2021b; Lightman et al., 2023; Li et al., 2022), the ability of LLMs to perform self-verification and self-correction is still heavily contested, with some works providing positive evidence for such capabilities (Weng et al., 2022; Gero et al., 2023; Madaan et al., 2024; Liu et al., 2024; Li et al., 2024a) and others arguing that the gains can mostly be attributed to clever prompt design, unfair baselines, data contamination and using overly simple tasks (Tyen et al., 2023; Valmeekam et al., 2023; Hong et al., 2023; Huang et al., 2023; Stechly et al., 2024; Zhang et al., 2024). This work contributes to this ongoing discussion by presenting multiple lines of evidence supporting LLM self-verification. In particular, we demonstrate clear benefits from a simple confidence-based self-verification approach.</p>
<p>Improving self-consistency's efficiency. Numerous attempts (Chen et al., 2024) have been made to reduce SC computational overhead while maintaining quality. However, none have matched the widespread adoption of self-consistency. This can be largely attributed to several limitations: (1) a trade-off where throughput is reduced while latency increases, for example by sampling chains sequentially (instead of in parallel) until reaching a certain condition (Aggarwal et al., 2023; Li et al., 2024b; Wang et al., 2024b) or running expensive LLM calls instead of the cheap majority voting (Yoran et al., 2023), (2) the need for manual feature crafting and tuning tailored to each dataset (Wan et al., 2024), (3) promising results on specialized setups (Wang et al., 2024a) which did not generalize to standard benchmarks (Table 9), and (4) as highlighted by Huang et al. (2023), many of the more sophisticated methods that appear promising actually don't outperform self-consistency when evaluated with a thorough analysis of inference costs. Our approach is different in that CISC adds minimal complexity to self-consistency, and still allows parallel sampling which enables to improve throughput without compromising latency, a crucial requirement for many applications.</p>
<p>Self-consistency with confidence. Related approaches to CISC's confidence-weighted majority vote were previously explored in both the original self-consistency paper Wang et al. (2022), that considered a weighted majority using Sequence Probability (§4.4), and in Miao et al. (2023), that concluded that verbally "asking the LLM to check its own reasoning is largely ineffective" for improving self-consistency. In both cases, these failures are attributed to the confidence scores being too similar to one another. Our work shows that despite this, the scores contain a useful signal (reflected in the WQD scores; Table 3) that can be utilized by a normalization step prior to aggregation to significantly improve the efficiency of self-consistency. Furthermore, the P(True) method, which achieves the highest WQD scores, has not been previously used for attempting to improve self-consistency.</p>
<h2>9 Discussion</h2>
<p>In this work we introduced CISC, a lightweight extension of self-consistency. Across diverse models, datasets, and confidence extraction methods, CISC consistently outperformed self-consistency, reducing computation costs by over $40 \%$ on average.</p>
<p>The performance gains achieved by using modelderived confidence scores provide a practical evidence that LLMs can effectively judge the quality of their own outputs, contributing to the ongoing debate on this topic (Huang et al., 2023; Li et al., 2024a). This is further strengthened by our qualitative evaluation, revealing significant agreement between model confidence and human assessments of response quality.</p>
<p>Complementing our investigation of LLM selfassessment, we address the crucial aspect of evaluating confidence methods. Traditional calibration metrics, which assess confidence across different questions, fail to capture a model's ability to distinguish between high and low quality responses to the same question. To overcome this, we introduce the Within-Question Discrimination (WQD) metric and demonstrate its effectiveness.</p>
<p>We encourage future research to explore the integration of model self-confidence into more sophisticated reasoning frameworks like Tree of Thoughts (Yao et al., 2024) or Graph of Thoughts (Besta et al., 2024), believing that harnessing this inherent capability can further boost performance. Another promising avenue is training models to produce more accurate intrinsic or verbal confidence</p>
<p>(Lin et al., 2022; Chaudhry et al., 2024), which would directly improve CISC and related methods. For instance, recent evidence suggests that a better signal can be derived from the model's internal states, even outperforming P(True) (Gekhman et al., 2025). Conversely, CISC and WQD can be used to assess the impact of advancements in confidence generation.</p>
<h2>10 Limitations</h2>
<p>Confidence Prompting. Our confidence extraction prompting approach minimizes the computational overhead ( $\S 4.1$ ) by using short confidence prompts (less than $5 \%$ of the input and reasoning chain length) that, unlike other works, are appended after the reasoning chain. This allows us to continue to use the auto-regressive cache that was used when the models generated the answer. While this approach is readily implementable within frameworks like HuggingFace (Hugging-Face, 2024a), it may not be universally supported. An alternative one-step prompting approach, which does not rely on prefix caching, is discussed in Appendix B. We opted for the two-step approach in this study to ensure a clear and robust evaluation of CISC, fully mitigating the impact of confidence integration on the generated reasoning paths.</p>
<p>Access to the model's probabilities. The preferred CISC approach calculates P(True) (as described in $\S 4.1$ ) by examining the model's assigned probability to the verbal confidence token. This method is available in both popular openweights frameworks (e.g., Hugging-Face (2024a)) and closed-weights frameworks (e.g., OpenAI (2025)). However, this feature may not be universally available across all frameworks.</p>
<p>Human Evaluation. The qualitative human evaluation presented in Section 7 provides further support for our claims regarding LLMs' ability to self-assess the correctness of their responses. This evaluation was conducted on the MMLU dataset, which offers a diverse set of single-choice questions. Extending this analysis to other datasets could offer additional insights.</p>
<p>Additional ablations. We examined the performance of CISC across several key aspects, focusing on the impact of the choice of confidence extraction method and the impact of the confidence normalization step. Additional ablations could include examining the effect of zero-shot vs few-shot prompting, different choices of normalization techniques, and using trainable confidence methods (Lin et al., 2022; Chaudhry et al., 2024) to improve the performance of CISC.</p>
<h2>11 Ethics Statement</h2>
<p>This work improves LLM reasoning efficiency by introducing a new decoding strategy (CISC). While CISC itself introduces no new ethical issues, LLMs can perpetuate biases and have societal impacts. Responsible LLM development and deployment, including bias mitigation, are crucial.</p>
<h2>References</h2>
<p>Pranjal Aggarwal, Aman Madaan, Yiming Yang, et al. 2023. Let's sample step by step: Adaptiveconsistency for efficient reasoning and coding with llms. arXiv preprint arXiv:2305.11860.</p>
<p>Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734.</p>
<p>BIG bench authors. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682-17690.</p>
<p>Glenn W Brier. 1950. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1-3.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827.</p>
<p>Arslan Chaudhry, Sridhar Thiagarajan, and Dilan Gorur. 2024. Finetuning language models to emit linguistic expressions of uncertainty. arXiv preprint arXiv:2409.12180.</p>
<p>Wenqing Chen, Weicheng Wang, Zhixuan Chu, Kui Ren, Zibin Zheng, and Zhichao Lu. 2024. Self-paraconsistency: Improving reasoning tasks at low cost for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 14162-14167, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021b. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Zorik Gekhman, Eyal Ben David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, and Roi Reichart. 2025. Inside-out: Hidden factual knowledge in llms. arXiv preprint arXiv:2503.15299.</p>
<p>Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning LLMs on new knowledge encourage hallucinations? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 7765-7784, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and Hoifung Poon. 2023. Self-verification improves fewshot clinical information extraction. arXiv preprint arXiv:2306.00024.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 13211330. PMLR.</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS.</p>
<p>Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang. 2023. A closer look at the self-verification abilities of large language models in logical reasoning. arXiv preprint arXiv:2311.07954.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</p>
<p>Hugging-Face. 2024a. Hugging face. https:// huggingface.co/.</p>
<p>Hugging-Face. 2024b. Hugging face leaderboard. https://huggingface.co/ spaces/open=llm-leaderboard/open_llm_ leaderboard#/.</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664.</p>
<p>Loka Li, Zhenhao Chen, Guangyi Chen, Yixuan Zhang, Yusheng Su, Eric Xing, and Kun Zhang. 2024a. Confidence matters: Revisiting intrinsic self-correction capabilities of large language models. arXiv preprint arXiv:2402.12563.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. Making large language models better reasoners with stepaware verifier. arXiv preprint arXiv:2206.02336.</p>
<p>Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. 2024b. Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning. arXiv preprint arXiv:2401.10480.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334.</p>
<p>Dancheng Liu, Amir Nassereldine, Ziming Yang, Chenhui Xu, Yuting Hu, Jiajie Li, Utkarsh Kumar, Changjae Lee, and Jinjun Xiong. 2024. Large language models have intrinsic self-correction ability. arXiv preprint arXiv:2406.15673.</p>
<p>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, and Chris Callison-Burch. 2024. Calibrating large language models with sample consistency. arXiv preprint arXiv:2402.13904.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.</p>
<p>Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436.</p>
<p>Mistral-AI. 2024. Mistral models. https://mistral. ai/news/ministraux/.</p>
<p>Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, and Gholamreza Haffari. 2024. Direct evaluation of chain-of-thought in multi-hop reasoning with knowledge graphs. arXiv preprint arXiv:2402.11199.</p>
<p>OpenAI. 2025. Openai api. https://platform. openai.com/docs/api-reference/chat/create.</p>
<p>Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. 2025. Llms know more than they show: On the intrinsic representation of LLM hallucinations. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net.</p>
<p>Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32.</p>
<p>Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. 2024. On the self-verification limitations of large language models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118.</p>
<p>Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. 2023a. Finetuning language models for factuality. arXiv preprint arXiv:2311.08401.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. 2023b. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975.</p>
<p>Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Cărbune. 2023. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516.</p>
<p>Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118.</p>
<p>Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. 2024. Dynamic self-consistency: Leveraging reasoning paths for efficient llm sampling. arXiv preprint arXiv:2408.17017.</p>
<p>Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. 2024a. Soft self-consistency improves language model agents. arXiv preprint arXiv:2402.13212.</p>
<p>Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. 2024b. Make every penny count: Difficultyadaptive self-consistency for cost-efficient reasoning. arXiv preprint arXiv:2408.13457.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024c. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024d. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2022. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561.</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007.</p>
<p>Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. 2024. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140.</p>
<p>A Quantitative example from §3</p>
<p>Consider a simplified binary setting in which there are two possible answers: correct and incorrect. Given a number of samples $n$ and a probability $p=0.6$ of generating the correct answer, the number of samples with the correct answer follows the Binomial distribution $X \sim \operatorname{Binomial}(n, p)$. For such distribution, the majority vote is accurate whenever $X&gt;\frac{n}{2}$ and it has $50 \%$ chance to be accurate when $X=\frac{n}{2}$ (i.e., a random choice).</p>
<p>Now, to illustrate how the self-assessment score of LLMs can be helpful, consider that we have an oracle that assigns twice the weight for answers that are correct. In this case, a weighted majority vote would be accurate whenever $X&gt;\frac{n}{3}$ and it has $50 \%$ chance to be accurate when $X=\frac{n}{3}$.</p>
<p>In Figure 5 we plot the relationship between, (x-axis) the number of samples, and (y-axis) the accuracy of the weighted majority vote over these samples. The graph features two lines: (blue) each sample gets an equal weight, and (orange) correct answers are assigned twice the weight of incorrect ones.</p>
<p>While this intuition about cost-saving also applies to the general case of an arbitrary set of answers, this setting is trickier to analyze in closedform because the specific distribution of incorrect answers impacts the majority vote. E.g., an answer that appears only $20 \%$ of the time can still be correct under majority vote if all the other $80 \%$ incorrect answers are different from one another. This could be obtained by placing additional distributional assumptions on the sampled answers. The analysis of the binary case can be thought of as a worst-case analysis of the general case, since in the worst case, all the incorrect answers are identical and the majority will be accurate if and only if more than half the sampled answers are correct.</p>
<h2>B Prompting Techniques</h2>
<p>As described in Section 4.1, for our prompt based confidence extraction techniques (Verbal Confidence, P(True)), we used a two-step approach: First, we prompted the model to answer benchmark questions using the prompts shown in Table 4. Then, we extracted confidence by concatenating the prompts shown in Table 5 and running the model again. This two-step process allowed using the same answers when comparing self-consistency and CISC.</p>
<p>While a simpler single-step implementation (out-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The relationship between the number of samples (x-axis) and the accuracy of majority vote over these samples (y-axis), for two different hypothetical cases sampled from a Binomial distribution: (blue) Each sample receives an equal weight in majority voting, and (orange) Correct answers are assigned double the weight of incorrect ones. Adding this additional weighting information translated into $4 X$ reduction in the number of samples required for the majority vote to reach $90 \%$ accuracy.
putting both answer and confidence in a single response) is possible, we did not explore it in this study. For research purposes, we prioritized a clean setup that ensured requesting confidence scores did not influence the generated answers and chain-ofthoughts.</p>
<p>As shown in Table 5, all the confidence extraction prompts that we used are extremely lightweight. We deliberately avoided methods that significantly increase the number of generated tokens like generating $k$ guesses with associated probabilities (Tian et al., 2023b).</p>
<p>For the P(True) method, we modified the prompts from Kadavath et al. (2022) in two ways: (1) We changed the format to allow concatenation after the model provided its answer, ensuring that prefix caching could be re-used between the two steps. (2) We changed the prompt format to $0 / 1$ instead of True/False, as some benchmarks are using True/False as ground truth labels, and we observed that it might confuse the model when extracting confidence scores.</p>
<p>C Additional Results</p>
<p>For each confidence method, Table 1 shows the macro-average results across all models and datasets. A more detailed version of this table, with a per dataset breakdown, is given at Table 6.</p>
<p>In addition, Table 7 shows micro-averaged aggregated results with confidence intervals, demonstrating the strong statistical significance of our findings. These bootstrap confidence intervals were calculated as follows: (1) For each confidence method, results from all datasets and models were combined into a single dataset of approximately $n \approx 150,000$ rows. (2) 10,000 bootstrap sets were generated by repeatedly sampling $n$ elements with replacement. (3) The procedure described in 4.6 was applied to each set, yielding 10,000 estimates of the mean accuracy difference. (4) We used these estimates to calculate the $95 \%$ interval.</p>
<p>Table 8 is an extended version of table 2. One important insight that can be derived from the extended table, is that using softmax normalization without temperature scaling is strongly discouraged for CISC.</p>
<p>We also add Figures 6, 7 featuring additional graphs similar to Figure 1, but with all the confidence methods.</p>
<p>Finally, in Table 9, we include ablations comparing CISC's weighted majority mechanism to more simple methods like selecting the max confidence (Wang et al., 2024a) or using the confidence values as a tie-breaker for self-consistency.</p>
<h2>D Temperature Scaling Results</h2>
<p>As discussed in $\S 4.5$, a single optimal temperature, $T^{\star}$, was determined for each model and confidence extraction method by using a $10 \%$ held-out set, aggregated across all datasets. Fitting is done using grid search on 80 evenly spaced values ranging from $10^{-4}$ to $10^{4}$. This was a light-weight process, only taking a few minutes on a standard desktop since no LLM re-runs were necessary. The temperatures for each configuration are presented in Figure 8. As can be seen, each of the confidence extraction method work with a different temperature magnitude because it produce confidence values on a different scale.</p>
<h2>E Qualitative Appendix</h2>
<p>The qualitative analysis presented in $\S 7$ involved sampling the reasoning paths using three models: Qwen2.5 3B, Gemma2 9B and Mistral Large
(123B). To broaden our evaluated sample pool, we employed a bootstrap process, sampling three distinct traces per question multiple times. Then, we first filtered these samples so that each of them arrived from a different question, and continued with the sampling process described in $\S 7$.</p>
<p>Human evaluators were asked to identify logical patterns in the LLMs' reasoning paths that reduced the evaluators' confidence in the correctness of the LLMs' answers. Importantly, the MMLU dataset requires significant domain knowledge and unspecialized humans achieved only $34.5 \%$ accuracy (Hendrycks et al., 2020), compared to a random baseline of $25 \%$. The MMLU-pro dataset is based on the MMLU dataset, but is considered much harder. This means that our evaluators, which lacked specialized knowledge, could not easily how to solve each question. Instead, we instructed them to focus on identifying low-quality reasoning errors in the responses of the LLMs. This approach aligns with findings from a prior analysis on GPT4o (Wang et al., 2024c), which attributed $39 \%$ of its errors to reasoning flaws that do not rely on specialized domain knowledge.</p>
<p>Following this review, we aggregated the indicators of low quality into high-level categories. Three main categories encompassed $49 \%$ of the samples. The remaining samples either lacked low-quality indicators (50\%) or had indicators that did not fit into a sizable category (1\%). The different categories and their prevalence are presented in Table 11 .</p>
<p>Two of these three categories show a strong association with relative-low confidence scores from the model: (1) The model arrived at solutions not present among the available options, and (2) The model only conducted partial calculations necessary. Interestingly, the pattern where the model explores several plausible solutions without identifying a definitive "correct" one was not specifically associated with either high or low confidence in the model's reasoning paths, underscoring that not all human-identified patterns significantly influence the model's assessment.</p>
<p>Overall, the alignment of human-identified lowquality indicators with low-confidence scores provides another evidence of the ability of LLMs to self-assess and prioritize high confidence solutions. An ability that is leveraged by CISC.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison between different confidence extraction methods using Gemma2-9B model and four datasets (§4.2). CISC with P(True) outperforms Self-Consistency and is the best of all the CISC variants.</p>
<h2>F Compute</h2>
<p>For each model (§4.3), we generated approximately 500,000 responses - 17,000 questions (§4.2), with 30 samples (§4.6). As a reference, inference with Gemma2-2-Billion (1K token context length) required an order of 100 Nvidia H100 GPU hours.</p>
<p>We consider 30 samples to be a substantial sample size. In more practical scenarios, we anticipate practitioners would likely use a smaller number of samples. As illustrated in Figures 1 and 6, the improvement curves show a logarithmic shape. On average, across all models and datasets, we found that just 13 responses were sufficient to achieve $90 \%$ of the maximum effect observed with 30 re-
sponses.</p>
<h1>General Instructions</h1>
<p>Before giving your answer, provide a step-by-step explanation of your thought process. Then on a new line, give your proposed answer adhering to this precise format: 'Proposed answer: (X).', where X is your proposed answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MMLU-Pro</td>
<td style="text-align: center;">You will be given a single-choice question. Answer the question by selecting the letter of the best fitting option.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[General Instructions]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The answer MUST ALWAYS be the letter of one of the available options; it CANNOT be "None of the Above".</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">You will be given a question and your goal is to answer it correctly.\nYour proposed answer should be a TeX expression, such as '\$5\$', '\$3.14\$', or '\$\\sqrt{8}\$}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[General Instructions]</td>
</tr>
<tr>
<td style="text-align: center;">BBH <br> (no options)</td>
<td style="text-align: center;">You will be given a question and your goal is to answer it correctly.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[General Instructions]</td>
</tr>
<tr>
<td style="text-align: center;">BBH <br> (with options)</td>
<td style="text-align: center;">You will be given a question and your goal is to answer it correctly.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[General Instructions]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Select the letter of the best fitting option. The answer CANNOT be "None of the Above".</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">You will be given a question and your goal is to answer it correctly.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[General Instructions]</td>
</tr>
</tbody>
</table>
<p>Table 4: The prompts used to generate model responses for benchmark questions. For all datasets, we used the General Instructions (shown at the top) asking the model to solve each question step-by-step and provide its final answer in a specified format. In addition, for each dataset we briefly explained the expected questions format. All prompts were zero-shot; few-shot experiments are reserved for future work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Confidence Method</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Verbal 0-100</td>
<td style="text-align: left;">Now I will rate my confidence in the proposed answer on a scale of 0-100. <br> Proposed confidence: (</td>
</tr>
<tr>
<td style="text-align: center;">Verbal Binary</td>
<td style="text-align: left;">Now I will rate my confidence in the proposed answer as either 0 or 1. <br> Proposed confidence: (</td>
</tr>
</tbody>
</table>
<p>Table 5: The prompts used to extract the model confidence in its response. As explained in section B, these prompts are concatenated as a second step, after the model already answers the question. For the P(True) method, we used the Verbal Binary prompt and looked at the probably the model assigns to the token 1. Importantly, in all the models evaluated in this work, "(0" and "(1" are tokenized as two separate tokens.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison between different confidence extraction methods using Mistral 123B model and four datasets (§4.2). CISC with P(True) outperforms Self-Consistency in all 4 graphs and is the best of all the CISC variants in 3 graphs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Comparable SC Samples</th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Acc Improvement (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: left;">Confidence Method</td>
<td style="text-align: left;">Budget 5</td>
<td style="text-align: left;">Budget 10</td>
<td style="text-align: center;">Budget 5</td>
<td style="text-align: center;">Budget 10</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Verbal Binary</td>
<td style="text-align: left;">$18 \%(6.1)$</td>
<td style="text-align: left;">$12 \%(11.3)$</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Verbal 1-100</td>
<td style="text-align: left;">$25 \%(6.7)$</td>
<td style="text-align: left;">$32 \%(14.6)$</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Response Probability</td>
<td style="text-align: left;">$17 \%(6.0)$</td>
<td style="text-align: left;">$23 \%(13.0)$</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">P(True)</td>
<td style="text-align: left;">$\mathbf{3 7 \% ( 7 . 9 )}$</td>
<td style="text-align: left;">$\mathbf{4 7 \% ( 1 8 . 8 )}$</td>
<td style="text-align: center;">$\mathbf{1 . 4}$</td>
<td style="text-align: center;">$\mathbf{1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Verbal Binary</td>
<td style="text-align: left;">$18 \%(6.1)$</td>
<td style="text-align: left;">$11 \%(11.2)$</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Verbal 1-100</td>
<td style="text-align: left;">$17 \%(6.0)$</td>
<td style="text-align: left;">$12 \%(11.3)$</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Response Probability</td>
<td style="text-align: left;">$19 \%(6.2)$</td>
<td style="text-align: left;">$17 \%(12.0)$</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">P(True)</td>
<td style="text-align: left;">$\mathbf{3 2 \% ( 7 . 3 )}$</td>
<td style="text-align: left;">$\mathbf{3 4 \% ( 1 5 . 2 )}$</td>
<td style="text-align: center;">$\mathbf{3 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Verbal Binary</td>
<td style="text-align: left;">$18 \%(6.1)$</td>
<td style="text-align: left;">$7 \%(10.8)$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: left;">Verbal 1-100</td>
<td style="text-align: left;">$22 \%(6.4)$</td>
<td style="text-align: left;">$32 \%(14.6)$</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Response Probability</td>
<td style="text-align: left;">$21 \%(6.3)$</td>
<td style="text-align: left;">$33 \%(14.9)$</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">P(True)</td>
<td style="text-align: left;">$\mathbf{4 3 \% ( 8 . 8 )}$</td>
<td style="text-align: left;">$\mathbf{5 3 \% ( 2 1 . 2 )}$</td>
<td style="text-align: center;">$\mathbf{0 . 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">BBH</td>
<td style="text-align: left;">Verbal Binary</td>
<td style="text-align: left;">$17 \%(6.0)$</td>
<td style="text-align: left;">$10 \%(11.1)$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Verbal 1-100</td>
<td style="text-align: left;">$22 \%(6.4)$</td>
<td style="text-align: left;">$41 \%(17.0)$</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Response Probability</td>
<td style="text-align: left;">$32 \%(7.3)$</td>
<td style="text-align: left;">$45 \%(18.3)$</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">P(True)</td>
<td style="text-align: left;">$\mathbf{4 8 \% ( 9 . 7 )}$</td>
<td style="text-align: left;">$\mathbf{4 7 \% ( 1 9 . 0 )}$</td>
<td style="text-align: center;">$\mathbf{1 . 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Aggregated results across all models for each dataset and confidence extraction method. All methods demonstrate better performance than standard self-consistency, with the P -True method achieving the best results and leading to an computational cost reduction of up to $53 \%$</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Acc Improvement</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Confidence Method</td>
<td style="text-align: center;">Budget 5</td>
<td style="text-align: center;">Budget 10</td>
</tr>
<tr>
<td style="text-align: left;">Verbal Binary</td>
<td style="text-align: center;">$0.35(0.34-0.37)$</td>
<td style="text-align: center;">$0.20(0.18-0.21)$</td>
</tr>
<tr>
<td style="text-align: left;">Verbal 1-100</td>
<td style="text-align: center;">$0.68(0.64-0.72)$</td>
<td style="text-align: center;">$0.46(0.40-0.51)$</td>
</tr>
<tr>
<td style="text-align: left;">Response Probability</td>
<td style="text-align: center;">$0.88(0.84-0.92)$</td>
<td style="text-align: center;">$0.69(0.63-0.74)$</td>
</tr>
<tr>
<td style="text-align: left;">P(True)</td>
<td style="text-align: center;">$\mathbf{1 . 3 8 ( 1 . 3 2 - 1 . 4 3 )}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 3 ( 0 . 9 6 - 1 . 1 0 )}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Micro-averaged Aggregated Results. This table presents the micro-averaged aggregated results with confidence intervals for each confidence method. Each confidence method demonstrates statistically significant improvements over self-consistency, and $\mathbf{P}$ (True) method exhibits significant superiority over other methods. This detailed view supplements the macro-average results shown in Table 1 and provides statistical verification of the efficiency gains and accuracy improvements attributed to CISC methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">\% Cost Reduction</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">\% Acc Improvement</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">30</td>
</tr>
<tr>
<td style="text-align: left;">Confidence Method</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">P(True) - No Normalization</td>
<td style="text-align: left;">$29 \%(7.0)$</td>
<td style="text-align: left;">$32 \%(14.8)$</td>
<td style="text-align: left;">1.4</td>
<td style="text-align: left;">0.8</td>
<td style="text-align: left;">0.4</td>
</tr>
<tr>
<td style="text-align: left;">P(True) - Softmax T=1</td>
<td style="text-align: left;">$27 \%(6.8)$</td>
<td style="text-align: left;">$30 \%(14.2)$</td>
<td style="text-align: left;">1.3</td>
<td style="text-align: left;">0.8</td>
<td style="text-align: left;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">P(True) - Softmax T=Tuned</td>
<td style="text-align: left;">$\mathbf{4 1 \% ( 8 . 4 )}$</td>
<td style="text-align: left;">$\mathbf{4 6 \% ( 1 8 . 6 )}$</td>
<td style="text-align: left;">$\mathbf{1 . 6}$</td>
<td style="text-align: left;">$\mathbf{1 . 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Sequence Probability - No Normalization</td>
<td style="text-align: left;">$21 \%(6.3)$</td>
<td style="text-align: left;">$24 \%(13.1)$</td>
<td style="text-align: left;">1.1</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">Sequence Probability - Softmax T=1</td>
<td style="text-align: left;">$20 \%(6.3)$</td>
<td style="text-align: left;">$23 \%(13.0)$</td>
<td style="text-align: left;">1.1</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">Sequence Probability - Softmax T=Tuned</td>
<td style="text-align: left;">$\mathbf{2 2 \% ( 6 . 5 )}$</td>
<td style="text-align: left;">$\mathbf{3 1 \% ( 1 4 . 6 )}$</td>
<td style="text-align: left;">$\mathbf{1 . 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Verbal 0 - 100 - No Normalization</td>
<td style="text-align: left;">$20 \%(6.3)$</td>
<td style="text-align: left;">$20 \%(12.5)$</td>
<td style="text-align: left;">0.7</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Verbal 0 - 100 - Softmax T=1</td>
<td style="text-align: left;">$12 \%(5.7)$</td>
<td style="text-align: left;">$-1 \%(9.9)$</td>
<td style="text-align: left;">-0.3</td>
<td style="text-align: left;">-1.4</td>
<td style="text-align: left;">-2.6</td>
</tr>
<tr>
<td style="text-align: left;">Verbal 0 - 100 - Softmax T=Tuned</td>
<td style="text-align: left;">$\mathbf{2 2 \% ( 6 . 4 )}$</td>
<td style="text-align: left;">$\mathbf{3 0 \% ( 1 4 . 4 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Normalization Ablation. This table extends Table 2, showing that temperature-scaled softmax is optimal for all methods, and that softmax should be avoided without temperature scaling.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Comparable SC Samples</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Acc Improvement (\%)</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Confidence Method</td>
<td style="text-align: left;">Budget 5</td>
<td style="text-align: left;">Budget 10</td>
<td style="text-align: left;">Budget 5</td>
<td style="text-align: left;">Budget 10</td>
</tr>
<tr>
<td style="text-align: left;">Max</td>
<td style="text-align: left;">$-11 \%(4.5)$</td>
<td style="text-align: left;">$-84 \%(5.4)$</td>
<td style="text-align: left;">-1.9</td>
<td style="text-align: left;">-4.5</td>
</tr>
<tr>
<td style="text-align: left;">Tie</td>
<td style="text-align: left;">$27 \%(6.8)$</td>
<td style="text-align: left;">$28 \%(13.9)$</td>
<td style="text-align: left;">1.3</td>
<td style="text-align: left;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">CISC</td>
<td style="text-align: left;">$\mathbf{4 1 \% ( 8 . 4 )}$</td>
<td style="text-align: left;">$\mathbf{4 6 \% ( 1 8 . 6 )}$</td>
<td style="text-align: left;">$\mathbf{1 . 6}$</td>
<td style="text-align: left;">$\mathbf{1 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Simplified ablation. Here we compare CISC with two simplified ablations: (Max) Which selects the answer with highest confidence score, and (Tie) Only uses CISC if self-consistency resulted in a tie. All methods are calculated using the P(True) confidence. Results are aggregated across all models and datasets. CISC significantly outperforms both ablations, and the Max method even degenerates performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset <br> Model</th>
<th style="text-align: left;">BBH</th>
<th style="text-align: left;">GSM8K</th>
<th style="text-align: left;">MATH</th>
<th style="text-align: left;">MMLU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Gemma 27b</td>
<td style="text-align: left;">57.1</td>
<td style="text-align: left;">66.1</td>
<td style="text-align: left;">62.9</td>
<td style="text-align: left;">59.9</td>
</tr>
<tr>
<td style="text-align: left;">Gemma 2b</td>
<td style="text-align: left;">55.8</td>
<td style="text-align: left;">66.2</td>
<td style="text-align: left;">64.3</td>
<td style="text-align: left;">53.6</td>
</tr>
<tr>
<td style="text-align: left;">Gemma 9b</td>
<td style="text-align: left;">55.3</td>
<td style="text-align: left;">68.3</td>
<td style="text-align: left;">71.8</td>
<td style="text-align: left;">58.9</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 123</td>
<td style="text-align: left;">56.2</td>
<td style="text-align: left;">66.1</td>
<td style="text-align: left;">61.2</td>
<td style="text-align: left;">63.4</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 22</td>
<td style="text-align: left;">64.1</td>
<td style="text-align: left;">81.4</td>
<td style="text-align: left;">74.9</td>
<td style="text-align: left;">67.7</td>
</tr>
<tr>
<td style="text-align: left;">Mistral 8</td>
<td style="text-align: left;">59.4</td>
<td style="text-align: left;">71.8</td>
<td style="text-align: left;">62.9</td>
<td style="text-align: left;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">Qwen 14b</td>
<td style="text-align: left;">58.9</td>
<td style="text-align: left;">65.5</td>
<td style="text-align: left;">59.0</td>
<td style="text-align: left;">60.2</td>
</tr>
<tr>
<td style="text-align: left;">Qwen 3b</td>
<td style="text-align: left;">56.3</td>
<td style="text-align: left;">61.9</td>
<td style="text-align: left;">57.5</td>
<td style="text-align: left;">56.0</td>
</tr>
<tr>
<td style="text-align: left;">Qwen 72b</td>
<td style="text-align: left;">53.5</td>
<td style="text-align: left;">62.4</td>
<td style="text-align: left;">63.6</td>
<td style="text-align: left;">58.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Within-Question-Discrimination Breakdown. This table presents a breakdown of the aggregated Within-Question-Discrimination (WQD) results presented in Table 3, using the P(True) method. In all cases, WQD scores exceed the $50 \%$ chance level.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: center;">Low</th>
<th style="text-align: center;">High</th>
<th style="text-align: center;">Snippet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No choice</td>
<td style="text-align: center;">The model arrives at a solution which is not present in the list of available options. This can include case where a mathematical answer significantly diverging from all options, answers that are only partially correct, or the elimination of all options as part of the reasoning process.</td>
<td style="text-align: center;">$38 \%$</td>
<td style="text-align: center;">$13 \%$</td>
<td style="text-align: center;">"... After reviewing the options, it's clear that none of them perfectly fit the requirements. However, the closest correct option is (A), which only has a minor error in calculating the remaining inches. Proposed answer: (A)"</td>
</tr>
<tr>
<td style="text-align: center;">Incomplete <br> Calculations</td>
<td style="text-align: center;">The model begins to solve the problem but does not complete the full calculation, often due to the lack of necessary data. For example, when attempting to compute acceleration, the absence of mass data prevents an exact and full calculation.</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">"..."<em>Calculate Heat Flow:</em><em> q" = h * (T-surface - T-air) </em><em>Note:</em>* Without the actual values for air density, viscosity, and thermal conductivity at $68^{\circ} \mathrm{F}$, we cannot perform the precise calculations. Proposed answer: (C)."</td>
</tr>
<tr>
<td style="text-align: center;">Multiple candidates</td>
<td style="text-align: center;">The model explores several plausible solutions without identifying a definitive "correct" one. This occurs when the model solves a problem generally, relying on estimations rather than concrete data, resulting in a range of potential answers.</td>
<td style="text-align: center;">$11 \%$</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">"... 2.<strong>Identify Buddhist Thinkers:</strong> The options list several prominent Buddhist figures from various traditions... 4. <strong>Most Prominent:</strong> The Dalai Lama and Thich Nhat Hanh stand out for their consistent emphasis on self-sacrifice in their teachings and actions. Proposed answer: (I)"</td>
</tr>
</tbody>
</table>
<p>Table 11: Human evaluators identified low-quality reasoning indicators in LLM responses (see §7). These indicators were then clustered into three categories, each described above with a definition and an example snippet from an LLM response. The (Low, High) columns show the percentage of LLM responses with low/high self-assessed confidence that exhibited each pattern. The "No Choice" and "Incomplete Calculation" categories are strongly associated with low confidence.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">General <br> Instructions</th>
<th style="text-align: center;">Evaluate the LLMs’ reasoning paths, looking for logical inconsistencies or errors that lower your confidence in their conclusions. Because the questions are very difficult, even for experts, your task is to identify general reasoning flaws, not to assess the correctness of the final answers themselves. Examples: <br> - Incorrect Assumption: The model assumes something without justification <br> - Missing Step: The model skips a crucial step in the reasoning process <br> - Contradiction: The model states both A and not-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">[Pre-filled - The original question given to the LLM]</td>
</tr>
<tr>
<td style="text-align: center;">LLM <br> Output</td>
<td style="text-align: center;">[Pre-filled - The LLM output for the given question]</td>
</tr>
</tbody>
</table>
<p>Table 12: The input given to human evaluators as part of our qualitative analysis (§7).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Other than the popular GSM8K, the other datasets were chosen as the three largest datasets in the Hugging Face Leaderboard (Hugging-Face, 2024b) (as of December 1st, 2024).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ If self-consistency failed to reach CISC's accuracy using up to 30 responses, we use a maximal value of 31 for this metric.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
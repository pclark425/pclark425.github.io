<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2222 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2222</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2222</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-279410902</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.14521v1.pdf" target="_blank">Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction</a></p>
                <p><strong>Paper Abstract:</strong> Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2222.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2222.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>standard-metrics-pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard metric-driven ML modeling pipeline (accuracy / F1 / AUC / Youden optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional ML workflow that optimizes and selects models using common threshold-dependent and threshold-independent metrics (accuracy, F1-score, AUC (PRC), and Youden index) as surrogate objectives instead of application-specific business metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Standard metric-based ML pipeline (AUC / accuracy / F1 / Youden optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models (kNN, RFC, BRFC, XGBoost, AutoML and a dummy classifier) were trained with hyperparameters chosen by Bayesian optimization using standard metrics (AUC, accuracy, F1, Youden) as optimization targets; thresholds were typically set using the Youden index from cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>industrial computer vision / quality inspection (AOI for SMT PCB solder joints)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Accuracy, F1-score, AUC (PRC), Youden score</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Standard ML evaluation metrics computed on labeled AOI measurement data: accuracy (overall correct fraction), F1 (harmonic mean of precision and recall for class 1), area under precision-recall curve (AUC of PRC), and Youden index (max sensitivity+specificity-1 over thresholds). Used as surrogate objectives for model selection and hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate (statistical evaluation metrics derived from labeled training/validation data)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Shopfloor business ground-truth: slip rate and true volume reduction measured against operator labels in production / held-out labeled data</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground-truth labels are operator-determined defect / false-call labels from manual inspection (the dataset label). Business metrics derived from these labels are slip rate s = FN/(TP+FN) (defective boards missed by ML) and volume reduction v = TN/(TN+FP) (reduction of non-defective boards sent to manual inspect). Targets used: S_target ≤ 1% slip and V_target ≥ 40% volume reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Multiple concrete examples from Table V show large disagreements: e.g., AutoML1 achieved Accuracy = 98.7% ±0.9% while slip rate = 11.3% ±8.9% (target 1.0%); gap = +10.3 percentage points in slip relative to target despite high accuracy. XGBoost1: Accuracy = 99.3% ±0.5% while slip = 7.8% ±4.3% (gap +6.8 pp). Dummy classifier (DC1): Accuracy = 99.2% but V@S = 0.0 and cV = -0.99, cAUC = -1.0, showing extreme misleadingness. These illustrate proxy→ground-truth gaps of multiple percentage points to >10 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Examples on test set (mean ± std, 10 runs): Accuracy up to 99.3% (XGBoost1 0.993 ±0.005), high PRC (e.g., XGBoost1 PRC 0.909 ±0.015), Youden up to ~0.959.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>On same test set the business-grounded metrics often show failure: example AutoML1 volume reduction = 98.8% ±1.0% but slip = 11.3% ±8.9% (violates S_target); BRFC2 and RFC2 achieved slip ≈0.8% and 1.0% and volume reduction ≈79.7% and 89.7% respectively on the test set (average) — these were the only models meeting targets on test on average.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Reportable from business metric: false-positive rate ≈ 1 - volume_reduction. Examples: AutoML1 FP rate ≈ 1.2% (1 - 0.988), XGBoost1 FP ≈ 0.007 (1 - 0.993). Dummy DC1 FP rate = 0.0? (table shows volume reduction 1.0) but other requirement-aware metrics negative.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Equivalent to slip rate s. Examples: AutoML1 slip = 11.3% ±8.9%; XGBoost1 slip = 7.8% ±4.3%; BRFC2 slip = 0.8% ±0.6% (0.008 ±0.006).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Models optimized under standard metrics are typically in-distribution relative to the training/validation split used during hyperparameter search, but the paper shows they do not generalize to temporally shifted (later) data — effectively becoming out-of-distribution in production.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>The paper reports that when the model is evaluated on chronological future slices (temporal drift), slip rates and volume reduction degrade immediately: models that looked good on random-split test data (in-distribution) show substantially worse ground-truth business metrics in later evaluation slices (out-of-distribution). No single correlation coefficient is given, but multiple runs and temporal slices show consistent decay; e.g., models that met targets on the test split fail in all evaluation slices with slip exceeding S_target already in the first slice.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Authors propose replacing/augmenting standard metrics with requirement-aware metrics (cAUC, V@S, cV) for hyperparameter optimization / threshold selection; strict chronological splitting for validation; multiple random-seed runs; and improving threshold-setting methodology (avoid post-hoc threshold tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Using requirement-aware optimization changed model selection: e.g., models optimized by AUC appeared superior under standard metrics but worse under cV; optimizing cAUC selected RFC2/BRFC2 which reached business targets on test (RFC2 cV = 0.611 ±0.416, slip ≈1.0% ±1.2%, VR ≈89.7% ±3.9%), indicating improved alignment on test. However, temporal evaluation showed these still degraded in production — so effectiveness was partial: improved test-time alignment but did not fully eliminate temporal proxy-to-ground-truth gap.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Paper discusses relative cost qualitatively: ground-truth labels require manual inspection and thus human labor; generating ground-truth at scale is expensive, which motivates surrogate metrics for offline optimization. The authors note the paradox that the business objective reduces label generation (by reducing false calls), making long-term ground-truth monitoring costly; no dollar/time estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td>Short-term/held-out test performance (proxy) overestimates long-term production performance: models meeting test-set business targets failed quickly in chronological evaluation slices, with slip rates exceeding S_target in the first slice. Quantitatively: although BRFC2 and RFC2 met targets on test (avg), their slip on temporal slices climbed above S_target for all slices (exact slice numbers in Table IV not reproduced in the main text), indicating worse long-term ground-truth outcomes than proxy predicted.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging/maturing: the domain has a body of ML work but the paper argues that applied ML methodology is immature in industrial AOI, with prevalent methodological weaknesses (lack of reproducibility, inappropriate metrics, no temporal validation) that cause proxy metrics to misrepresent real business outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Uncertainty is reported as mean ± standard deviation over 10 random-seed runs for all metrics, but the paper does not perform formal calibration of predictive uncertainties against the proxy-to-ground-truth gap (no reliability diagrams or calibration metrics reported). Authors note high standard deviations for cV and threshold instability.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>The paper demonstrates that proxies can disagree: high accuracy or AUC can coexist with unacceptable slip; no formal correlation statistics between proxies and business metrics are reported, but counterexamples (e.g., DC1 high accuracy, cV strongly negative) show failure modes are not monotonic and can be misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Workflow: computational model training (hyperparameter search via Bayesian optimization using proxy metric) → cross-validation with Youden threshold selection → held-out test evaluation (proxy and business metrics) → chronological evaluation slices (temporal validation). Errors propagate from optimistic proxy-driven selection to deployment: models chosen by standard proxies often fail in later slices due to drift and threshold instability.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Key limitations identified: extreme class imbalance (99:1), temporal distribution drift in the dataset, unstable threshold adaptation (post-hoc thresholds not usable in production), lack of reproducibility in prior works, high variance across random seeds (lack of stability), and difficulty obtaining ongoing ground-truth once deployed (because the system's success reduces labels).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Factors affecting proxy-to-ground-truth gap include severe class imbalance, changing inspection/measurement definitions over time (process improvement causes distributional shifts), measurement noise in AOI-extracted features, and economic asymmetry (slips are much worse than extra manual inspections), making standard symmetric metrics (accuracy, F1) inappropriate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2222.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2222.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>requirement-aware-pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Requirement-aware metric-driven ML pipeline (cAUC / V@S / cV optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modeling and selection approach that uses custom, business-aligned surrogate metrics (constrained volume reduction cV, constrained AUC cAUC, and volume reduction at target slip V@S) to directly reflect industrial objectives during optimization and threshold selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Requirement-aware metric-based modeling (cV, cAUC, V@S)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Custom metrics were defined to directly encode business requirements: V@S = maximum achievable volume reduction under an allowed slip rate (uses ground-truth labels to compute the best threshold post-hoc), cV = constrained volume reduction computed using only a priori threshold settings (penalizes exceeding slip target), and cAUC = area measure quantifying classifier curve intersection with target region. Models were optimized using cAUC for hyperparameter search in the second experimental run.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>industrial computer vision / quality inspection (AOI for SMT PCB solder joints)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>cV (constrained volume reduction), cAUC (constrained AUC), V@S (volume reduction at target slip)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>cV: if slip at chosen threshold ≤ S_target then cV = v else cV = S_target - s (negative values indicate target slip exceeded); cAUC: normalized area under the slip vs volume curve that lies within the target zone (range [-1,1]), encoding how well any threshold can satisfy both slip and volume targets; V@S: maximal volume reduction achievable while keeping slip ≤ S_target (uses ground-truth labels to evaluate thresholds). These metrics either constrain or evaluate directly against business targets rather than generic statistical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate / application-specific objective (derived from labeled data and business targets); V@S is a ground-truth-dependent post-hoc surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Same business-ground-truth: slip rate and true volume reduction measured from operator labels (used both to compute cV/cAUC and to evaluate final performance on chronological slices).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground truth is operator labels of defective vs false-call outcomes; business metrics computed from confusion matrix entries on held-out chronological data slices provide final validation (slip rate and achieved volume reduction in each slice).</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Using requirement-aware metrics reduced some proxy/ground-truth mismatch on the test split: e.g., RFC2 (optimized with cAUC) on test: cV = 0.611 ±0.416, V@S = 0.901 ±0.042, slip = 0.01 ±0.012 (≈1.0%) and VR = 0.897 ±0.039 — meeting targets on average. However, temporal evaluation exposed residual gaps: the same RFC2/BRFC2 models that met targets on test degraded on evaluation slices with slip exceeding S_target from the first slice onward, showing remaining proxy→ground-truth gap under temporal drift.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>On the test split when optimized by requirement-aware metrics, some models achieved substantial V@S and positive cV: e.g., RFC2 V@S = 0.901 ±0.042, cV = 0.611 ±0.416; BRFC2 V@S = 0.796 ±0.064, cV = 0.423 ±0.396.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>On chronological evaluation slices, models' slip rates deteriorated and exceeded S_target for all slices; although exact slice-by-slice numbers not reproduced in main text, authors state that 'for all evaluation slices the model performances decays immediately' and 'slip rates exceed already for the first evaluation slice strongly the set target'.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Can be derived: e.g., RFC2 VR = 0.897 → FP rate ≈ 0.103 (10.3%). BRFC2 VR = 0.797 → FP rate ≈ 0.203 (20.3%). These are the proportion of non-defective boards still sent to manual inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Slip rate s (false-negative rate for defective class) examples: RFC2 slip ≈ 1.0% ±1.2%, BRFC2 slip ≈ 0.8% ±0.6% on test (averages) but higher in future slices.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Models selected by requirement-aware metrics were still largely in-distribution for the test slice but faced out-of-distribution temporal shifts in evaluation slices; hence their successes were mostly incremental and not robustly generalizing to future data.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Paper documents that gap increases under temporal distribution shifts: models that meet targets in in-distribution test perform worse (higher slips, lower cV) on later chronological slices; no per-slice numeric correlation is given in the main text but qualitative trend is explicit and consistent across models.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Directly optimizing cAUC and selecting thresholds by the requirement-aware criterion (V@S or slip-constrained threshold), chronological splitting during evaluation to detect drift, multiple random-seed runs to quantify variability, and recommending monitoring & re-labeling schemes (random or smart sampling) for long-term validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Partially effective: requirement-aware optimization selected models that on average met slip/volume targets on held-out test data (RFC2/BRFC2). Quantitatively: RFC2 test slip ≈1.0% and VR ≈89.7% (averages) versus many standard-metric-selected models having much higher slips. However, temporal evaluation revealed the remaining effectiveness was brittle — models still failed in production slices.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Authors explain qualitatively that ground-truth collection is expensive (manual inspection labor) and that the business objective reduces label generation, complicating long-term validation; no quantitative cost figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td>Requirement-aware models still suffered immediate degradation in chronological validation: although they met targets on the test split, slip rates exceeded S_target for the first and subsequent evaluation slices; short-term proxy performance did not reliably predict long-term outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: the requirement-aware approach improves alignment with business goals but is not yet sufficient to guarantee robust production performance under temporal drift without additional monitoring and retraining processes.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Metrics reported with mean ± std over 10 runs provide a measure of variability. The authors report high standard deviation for cV and threshold instability, but no formal calibration of uncertainty against production performance is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>The three requirement-aware proxies are related but expose different aspects: V@S (post-hoc, optimistic) is typically higher than cV (a priori constrained); cAUC captures area-over-threshold behavior. The authors explicitly highlight that V@S is optimistic relative to cV because it uses ground-truth knowledge post-hoc, so failure modes (optimism vs deployable expectation) are correlated but systematically biased.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Stages: train (hyperparameter CV using cAUC) → set threshold based on slip-target (V@S or constrained method) → test on held-out data → chronological evaluation slices. The cascade reveals that even requirement-aware optimization in early stages does not prevent error amplification under distribution drift in later stages.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>cV depends on stable, a priori threshold-setting methods which were found to be unstable across cross-validation folds and random seeds; V@S can be overoptimistic because it uses ground-truth labels for threshold selection (post-hoc knowledge); temporal drift can invalidate thresholds and models; requirement-aware optimization reduces but does not eliminate mismatch to production; obtaining continuous ground-truth is costly.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Same as above: extreme class imbalance, temporal instability of production process and inspection definitions, AOI measurement changes over time, and asymmetric business costs (slips >>> extra manual inspection) all make construction and deployment of surrogate objectives challenging.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Machine Learning Based Approach to Detect False Calls in SMT Manufacturing <em>(Rating: 2)</em></li>
                <li>Fine-Tuning Strategy for Re-Classification of False Call in Automated Optical Inspection Post Reflow <em>(Rating: 2)</em></li>
                <li>Supervised Learning Approach for Surface-Mount Device Production: 4th International Conference <em>(Rating: 2)</em></li>
                <li>Scalable Learning of Non-Decomposable Objectives <em>(Rating: 2)</em></li>
                <li>SMT Solder Joint Inspection via a Novel Cascaded Convolutional Neural Network <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2222",
    "paper_id": "paper-279410902",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "standard-metrics-pipeline",
            "name_full": "Standard metric-driven ML modeling pipeline (accuracy / F1 / AUC / Youden optimization)",
            "brief_description": "A conventional ML workflow that optimizes and selects models using common threshold-dependent and threshold-independent metrics (accuracy, F1-score, AUC (PRC), and Youden index) as surrogate objectives instead of application-specific business metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Standard metric-based ML pipeline (AUC / accuracy / F1 / Youden optimization)",
            "system_description": "Models (kNN, RFC, BRFC, XGBoost, AutoML and a dummy classifier) were trained with hyperparameters chosen by Bayesian optimization using standard metrics (AUC, accuracy, F1, Youden) as optimization targets; thresholds were typically set using the Youden index from cross-validation.",
            "domain": "industrial computer vision / quality inspection (AOI for SMT PCB solder joints)",
            "proxy_metric_name": "Accuracy, F1-score, AUC (PRC), Youden score",
            "proxy_metric_description": "Standard ML evaluation metrics computed on labeled AOI measurement data: accuracy (overall correct fraction), F1 (harmonic mean of precision and recall for class 1), area under precision-recall curve (AUC of PRC), and Youden index (max sensitivity+specificity-1 over thresholds). Used as surrogate objectives for model selection and hyperparameter tuning.",
            "proxy_metric_type": "empirical surrogate (statistical evaluation metrics derived from labeled training/validation data)",
            "ground_truth_metric": "Shopfloor business ground-truth: slip rate and true volume reduction measured against operator labels in production / held-out labeled data",
            "ground_truth_description": "Ground-truth labels are operator-determined defect / false-call labels from manual inspection (the dataset label). Business metrics derived from these labels are slip rate s = FN/(TP+FN) (defective boards missed by ML) and volume reduction v = TN/(TN+FP) (reduction of non-defective boards sent to manual inspect). Targets used: S_target ≤ 1% slip and V_target ≥ 40% volume reduction.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Multiple concrete examples from Table V show large disagreements: e.g., AutoML1 achieved Accuracy = 98.7% ±0.9% while slip rate = 11.3% ±8.9% (target 1.0%); gap = +10.3 percentage points in slip relative to target despite high accuracy. XGBoost1: Accuracy = 99.3% ±0.5% while slip = 7.8% ±4.3% (gap +6.8 pp). Dummy classifier (DC1): Accuracy = 99.2% but V@S = 0.0 and cV = -0.99, cAUC = -1.0, showing extreme misleadingness. These illustrate proxy→ground-truth gaps of multiple percentage points to &gt;10 percentage points.",
            "proxy_performance": "Examples on test set (mean ± std, 10 runs): Accuracy up to 99.3% (XGBoost1 0.993 ±0.005), high PRC (e.g., XGBoost1 PRC 0.909 ±0.015), Youden up to ~0.959.",
            "ground_truth_performance": "On same test set the business-grounded metrics often show failure: example AutoML1 volume reduction = 98.8% ±1.0% but slip = 11.3% ±8.9% (violates S_target); BRFC2 and RFC2 achieved slip ≈0.8% and 1.0% and volume reduction ≈79.7% and 89.7% respectively on the test set (average) — these were the only models meeting targets on test on average.",
            "false_positive_rate": "Reportable from business metric: false-positive rate ≈ 1 - volume_reduction. Examples: AutoML1 FP rate ≈ 1.2% (1 - 0.988), XGBoost1 FP ≈ 0.007 (1 - 0.993). Dummy DC1 FP rate = 0.0? (table shows volume reduction 1.0) but other requirement-aware metrics negative.",
            "false_negative_rate": "Equivalent to slip rate s. Examples: AutoML1 slip = 11.3% ±8.9%; XGBoost1 slip = 7.8% ±4.3%; BRFC2 slip = 0.8% ±0.6% (0.008 ±0.006).",
            "novelty_characterization": "Models optimized under standard metrics are typically in-distribution relative to the training/validation split used during hyperparameter search, but the paper shows they do not generalize to temporally shifted (later) data — effectively becoming out-of-distribution in production.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "The paper reports that when the model is evaluated on chronological future slices (temporal drift), slip rates and volume reduction degrade immediately: models that looked good on random-split test data (in-distribution) show substantially worse ground-truth business metrics in later evaluation slices (out-of-distribution). No single correlation coefficient is given, but multiple runs and temporal slices show consistent decay; e.g., models that met targets on the test split fail in all evaluation slices with slip exceeding S_target already in the first slice.",
            "gap_reduction_method": "Authors propose replacing/augmenting standard metrics with requirement-aware metrics (cAUC, V@S, cV) for hyperparameter optimization / threshold selection; strict chronological splitting for validation; multiple random-seed runs; and improving threshold-setting methodology (avoid post-hoc threshold tuning).",
            "gap_reduction_effectiveness": "Using requirement-aware optimization changed model selection: e.g., models optimized by AUC appeared superior under standard metrics but worse under cV; optimizing cAUC selected RFC2/BRFC2 which reached business targets on test (RFC2 cV = 0.611 ±0.416, slip ≈1.0% ±1.2%, VR ≈89.7% ±3.9%), indicating improved alignment on test. However, temporal evaluation showed these still degraded in production — so effectiveness was partial: improved test-time alignment but did not fully eliminate temporal proxy-to-ground-truth gap.",
            "validation_cost_comparison": "Paper discusses relative cost qualitatively: ground-truth labels require manual inspection and thus human labor; generating ground-truth at scale is expensive, which motivates surrogate metrics for offline optimization. The authors note the paradox that the business objective reduces label generation (by reducing false calls), making long-term ground-truth monitoring costly; no dollar/time estimates provided.",
            "temporal_validation": "Short-term/held-out test performance (proxy) overestimates long-term production performance: models meeting test-set business targets failed quickly in chronological evaluation slices, with slip rates exceeding S_target in the first slice. Quantitatively: although BRFC2 and RFC2 met targets on test (avg), their slip on temporal slices climbed above S_target for all slices (exact slice numbers in Table IV not reproduced in the main text), indicating worse long-term ground-truth outcomes than proxy predicted.",
            "domain_maturity": "Emerging/maturing: the domain has a body of ML work but the paper argues that applied ML methodology is immature in industrial AOI, with prevalent methodological weaknesses (lack of reproducibility, inappropriate metrics, no temporal validation) that cause proxy metrics to misrepresent real business outcomes.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Uncertainty is reported as mean ± standard deviation over 10 random-seed runs for all metrics, but the paper does not perform formal calibration of predictive uncertainties against the proxy-to-ground-truth gap (no reliability diagrams or calibration metrics reported). Authors note high standard deviations for cV and threshold instability.",
            "multiple_proxies": true,
            "proxy_correlation": "The paper demonstrates that proxies can disagree: high accuracy or AUC can coexist with unacceptable slip; no formal correlation statistics between proxies and business metrics are reported, but counterexamples (e.g., DC1 high accuracy, cV strongly negative) show failure modes are not monotonic and can be misleading.",
            "validation_cascade": "Workflow: computational model training (hyperparameter search via Bayesian optimization using proxy metric) → cross-validation with Youden threshold selection → held-out test evaluation (proxy and business metrics) → chronological evaluation slices (temporal validation). Errors propagate from optimistic proxy-driven selection to deployment: models chosen by standard proxies often fail in later slices due to drift and threshold instability.",
            "publication_bias_discussion": true,
            "limitations_challenges": "Key limitations identified: extreme class imbalance (99:1), temporal distribution drift in the dataset, unstable threshold adaptation (post-hoc thresholds not usable in production), lack of reproducibility in prior works, high variance across random seeds (lack of stability), and difficulty obtaining ongoing ground-truth once deployed (because the system's success reduces labels).",
            "domain_specific_factors": "Factors affecting proxy-to-ground-truth gap include severe class imbalance, changing inspection/measurement definitions over time (process improvement causes distributional shifts), measurement noise in AOI-extracted features, and economic asymmetry (slips are much worse than extra manual inspections), making standard symmetric metrics (accuracy, F1) inappropriate.",
            "uuid": "e2222.0"
        },
        {
            "name_short": "requirement-aware-pipeline",
            "name_full": "Requirement-aware metric-driven ML pipeline (cAUC / V@S / cV optimization)",
            "brief_description": "A modeling and selection approach that uses custom, business-aligned surrogate metrics (constrained volume reduction cV, constrained AUC cAUC, and volume reduction at target slip V@S) to directly reflect industrial objectives during optimization and threshold selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Requirement-aware metric-based modeling (cV, cAUC, V@S)",
            "system_description": "Custom metrics were defined to directly encode business requirements: V@S = maximum achievable volume reduction under an allowed slip rate (uses ground-truth labels to compute the best threshold post-hoc), cV = constrained volume reduction computed using only a priori threshold settings (penalizes exceeding slip target), and cAUC = area measure quantifying classifier curve intersection with target region. Models were optimized using cAUC for hyperparameter search in the second experimental run.",
            "domain": "industrial computer vision / quality inspection (AOI for SMT PCB solder joints)",
            "proxy_metric_name": "cV (constrained volume reduction), cAUC (constrained AUC), V@S (volume reduction at target slip)",
            "proxy_metric_description": "cV: if slip at chosen threshold ≤ S_target then cV = v else cV = S_target - s (negative values indicate target slip exceeded); cAUC: normalized area under the slip vs volume curve that lies within the target zone (range [-1,1]), encoding how well any threshold can satisfy both slip and volume targets; V@S: maximal volume reduction achievable while keeping slip ≤ S_target (uses ground-truth labels to evaluate thresholds). These metrics either constrain or evaluate directly against business targets rather than generic statistical performance.",
            "proxy_metric_type": "empirical surrogate / application-specific objective (derived from labeled data and business targets); V@S is a ground-truth-dependent post-hoc surrogate.",
            "ground_truth_metric": "Same business-ground-truth: slip rate and true volume reduction measured from operator labels (used both to compute cV/cAUC and to evaluate final performance on chronological slices).",
            "ground_truth_description": "Ground truth is operator labels of defective vs false-call outcomes; business metrics computed from confusion matrix entries on held-out chronological data slices provide final validation (slip rate and achieved volume reduction in each slice).",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Using requirement-aware metrics reduced some proxy/ground-truth mismatch on the test split: e.g., RFC2 (optimized with cAUC) on test: cV = 0.611 ±0.416, V@S = 0.901 ±0.042, slip = 0.01 ±0.012 (≈1.0%) and VR = 0.897 ±0.039 — meeting targets on average. However, temporal evaluation exposed residual gaps: the same RFC2/BRFC2 models that met targets on test degraded on evaluation slices with slip exceeding S_target from the first slice onward, showing remaining proxy→ground-truth gap under temporal drift.",
            "proxy_performance": "On the test split when optimized by requirement-aware metrics, some models achieved substantial V@S and positive cV: e.g., RFC2 V@S = 0.901 ±0.042, cV = 0.611 ±0.416; BRFC2 V@S = 0.796 ±0.064, cV = 0.423 ±0.396.",
            "ground_truth_performance": "On chronological evaluation slices, models' slip rates deteriorated and exceeded S_target for all slices; although exact slice-by-slice numbers not reproduced in main text, authors state that 'for all evaluation slices the model performances decays immediately' and 'slip rates exceed already for the first evaluation slice strongly the set target'.",
            "false_positive_rate": "Can be derived: e.g., RFC2 VR = 0.897 → FP rate ≈ 0.103 (10.3%). BRFC2 VR = 0.797 → FP rate ≈ 0.203 (20.3%). These are the proportion of non-defective boards still sent to manual inspection.",
            "false_negative_rate": "Slip rate s (false-negative rate for defective class) examples: RFC2 slip ≈ 1.0% ±1.2%, BRFC2 slip ≈ 0.8% ±0.6% on test (averages) but higher in future slices.",
            "novelty_characterization": "Models selected by requirement-aware metrics were still largely in-distribution for the test slice but faced out-of-distribution temporal shifts in evaluation slices; hence their successes were mostly incremental and not robustly generalizing to future data.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Paper documents that gap increases under temporal distribution shifts: models that meet targets in in-distribution test perform worse (higher slips, lower cV) on later chronological slices; no per-slice numeric correlation is given in the main text but qualitative trend is explicit and consistent across models.",
            "gap_reduction_method": "Directly optimizing cAUC and selecting thresholds by the requirement-aware criterion (V@S or slip-constrained threshold), chronological splitting during evaluation to detect drift, multiple random-seed runs to quantify variability, and recommending monitoring & re-labeling schemes (random or smart sampling) for long-term validation.",
            "gap_reduction_effectiveness": "Partially effective: requirement-aware optimization selected models that on average met slip/volume targets on held-out test data (RFC2/BRFC2). Quantitatively: RFC2 test slip ≈1.0% and VR ≈89.7% (averages) versus many standard-metric-selected models having much higher slips. However, temporal evaluation revealed the remaining effectiveness was brittle — models still failed in production slices.",
            "validation_cost_comparison": "Authors explain qualitatively that ground-truth collection is expensive (manual inspection labor) and that the business objective reduces label generation, complicating long-term validation; no quantitative cost figures provided.",
            "temporal_validation": "Requirement-aware models still suffered immediate degradation in chronological validation: although they met targets on the test split, slip rates exceeded S_target for the first and subsequent evaluation slices; short-term proxy performance did not reliably predict long-term outcomes.",
            "domain_maturity": "Emerging: the requirement-aware approach improves alignment with business goals but is not yet sufficient to guarantee robust production performance under temporal drift without additional monitoring and retraining processes.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Metrics reported with mean ± std over 10 runs provide a measure of variability. The authors report high standard deviation for cV and threshold instability, but no formal calibration of uncertainty against production performance is provided.",
            "multiple_proxies": true,
            "proxy_correlation": "The three requirement-aware proxies are related but expose different aspects: V@S (post-hoc, optimistic) is typically higher than cV (a priori constrained); cAUC captures area-over-threshold behavior. The authors explicitly highlight that V@S is optimistic relative to cV because it uses ground-truth knowledge post-hoc, so failure modes (optimism vs deployable expectation) are correlated but systematically biased.",
            "validation_cascade": "Stages: train (hyperparameter CV using cAUC) → set threshold based on slip-target (V@S or constrained method) → test on held-out data → chronological evaluation slices. The cascade reveals that even requirement-aware optimization in early stages does not prevent error amplification under distribution drift in later stages.",
            "publication_bias_discussion": true,
            "limitations_challenges": "cV depends on stable, a priori threshold-setting methods which were found to be unstable across cross-validation folds and random seeds; V@S can be overoptimistic because it uses ground-truth labels for threshold selection (post-hoc knowledge); temporal drift can invalidate thresholds and models; requirement-aware optimization reduces but does not eliminate mismatch to production; obtaining continuous ground-truth is costly.",
            "domain_specific_factors": "Same as above: extreme class imbalance, temporal instability of production process and inspection definitions, AOI measurement changes over time, and asymmetric business costs (slips &gt;&gt;&gt; extra manual inspection) all make construction and deployment of surrogate objectives challenging.",
            "uuid": "e2222.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Machine Learning Based Approach to Detect False Calls in SMT Manufacturing",
            "rating": 2
        },
        {
            "paper_title": "Fine-Tuning Strategy for Re-Classification of False Call in Automated Optical Inspection Post Reflow",
            "rating": 2
        },
        {
            "paper_title": "Supervised Learning Approach for Surface-Mount Device Production: 4th International Conference",
            "rating": 2
        },
        {
            "paper_title": "Scalable Learning of Non-Decomposable Objectives",
            "rating": 2
        },
        {
            "paper_title": "SMT Solder Joint Inspection via a Novel Cascaded Convolutional Neural Network",
            "rating": 1
        }
    ],
    "cost": 0.017673499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction
17 Jun 2025</p>
<p>Korbinian Pfab 
Department of Computer Science
Helsinki University / Siemens AG
0009-0009-8352-4216ErlangenFoundational Technology, Germany</p>
<p>Marcel Rothering 
Foundational Technology Siemens AG
0000-0002-6028-9868ErlangenGermany</p>
<p>Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction
17 Jun 2025C317F0685AB6CBB97ECE6F987AAFD6C3arXiv:2506.14521v1[cs.LG]Research methodolgyFalse call reductionElectronic productionIndustrial AI applications
Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications?This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices.We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences.We show that the best-practice methodology would fail for this use case.We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets.Our work encourages researchers to critically assess their methodologies for more successful applied AI research.</p>
<p>I. INTRODUCTION</p>
<p>The rise of automation in manufacturing has brought significant advancements to production processes.However, are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications?Despite extensive research, the success of industrial AI applications has not kept pace with other industrial automation technologies due to methodological weaknesses.</p>
<p>In this work, we address these methodological flaws using a case study on false call reduction in automated optical inspection (AOI) of printed circuit boards (PCBs).AOI systems, which use computer vision to inspect soldering quality, often produce a high number of false calls-incorrect classifications of non-defective PCBs as defective.These false calls consume valuable human resources in manual inspection stages.</p>
<p>Our study identifies seven prevalent weaknesses in related research on this topic and demonstrates their negative impacts experimentally.We highlight the necessity of using requirement-aware performance metrics over standard metrics, verifying assumptions about data distribution over time, and defining clear success criteria for experiments.By addressing these issues, we aim to challenge existing methodologies for evaluating and improving industrial AI applications, ultimately enhancing their practical value and effectiveness.The key contributions of our paper are:</p>
<p>• An analysis of common weaknesses in industrial AI research based on related work on AI applications to surface-mounted technology (SMT) production • A demonstration of measures to overcome the listed weaknesses such as the definition of requirement-aware metrics • Delivering the first scientific results on the performance of machine learning (ML) algorithms applied to false call reduction with published dataset and source code</p>
<p>II. BACKGROUND</p>
<p>In electronic production, there exist two main technologies for soldering PCBs: through-hole technology and SMT.For this work, we focus on SMT.To ensure product quality and detect defects early and cost-efficiently, AOI is commonly used directly after the SMT soldering process.Images recorded by the AOI are evaluated with computer vision algorithms to determine physical measurements such as displacement or rotation [17,19,26].Inspection types, defined by the test engineer, specify the measurements and their acceptable values, which can change over time due to continuous improvement initiatives.</p>
<p>Based on the measurement defined by the inspection type, the AOI classifies each soldering spot as defective or nondefective.Non-defective PCBs move to the next process step, while defective ones go to a manual inspection station (MIS) for manual inspection.However, many PCBs classified as defective by the AOI are later deemed non-defective (false calls) by the operator.</p>
<p>Reducing false calls with ML is a recent research topic.Different approaches introduce an ML-based decision gate between the AOI and MIS to reduce the number of false calls to release operator capacity. Figure 1 compares the common SMT scenario with the enhanced false call reduction scenario.</p>
<p>If a truly defective board is wrongly classified as a false call by the ML model and forwarded to further processes, it is considered a slip.Reducing the number of non-defective PCBs at the MIS through the ML model is termed volume reduction.Incorrectly identifying a non-defective board as defective (false positives) decreases volume reduction.Volume reduction and slip rate are critical business metrics, with slips being significantly worse due to their negative impact on other</p>
<p>III. RELATED WORK</p>
<p>Two major approaches can be identified in related scientific work: using raw image data recorded by the AOI or using measurement data extracted by the AOI, soldering process, or soldering paste inspection.The classification object can vary from the quality of a whole PCB, a single component, or a single soldering pin.</p>
<p>Lin and Su [15] suggest a two-stage approach using image data of components.Features like the count of white pixels are calculated to classify a board as normal or one of three defective classes.They used 7768 non-defective and 90 defective samples, resulting in a highly imbalanced dataset.They used the false call rate and slip rate for evaluation but concluded that more efforts are needed.Their dataset is unpublished.</p>
<p>Jamal et al. [12] use transfer learning with pre-trained convolutional neural networks, such as Xception [7], on an image dataset of 4036 component images.They applied data augmentation methods and used accuracy as the evaluation metric.They achieved 91% accuracy but admitted this might not be promising.Their dataset is unpublished.</p>
<p>Jaidan et al. [11] used data from soldering paste inspection, labeling good, false call, and real defect.They downsampled the dataset to address extreme imbalance and tested tree-based models in a 10-fold cross-validation, achieving around 98% accuracy and 97% recall.An additional dataset of 2000 samples achieved 98.3% accuracy, but classification level and class distribution details are unclear.Their dataset is unpublished.</p>
<p>Thielen et al. [27] collected data on a component level from AOI measurements, creating datasets of 1144 and 4264 samples.They evaluated neural networks, k-nearest neighbors, and random forest classifiers, with the latter performing best.They optimized thresholds to reduce slips to zero, but it remains uncertain if this would hold for future datasets.Their dataset is unpublished.</p>
<p>The discussed peer-reviewed related work contains seven common weaknesses:</p>
<p>W1: Lack of verification and reproducibility</p>
<p>The results cannot be verified or reproduced at all since neither the dataset nor the source code of the experiment is given.None of the discussed related work publishes its dataset or source code (cf.[11,12,15,27]).W2: Utilization of common models over advanced AutoML tools Instead of using state-of-the-art AutoML tools, a large topic are common models without naming certain requirements for that like explainability or inference time.None of the discussed related work considers AutoML approaches (cf.[11,12,15,27]).W3: Overemphasis on standard metrics and neglect of business impact A strong focus is set on standard metrics, in specific accuracy, instead of metrics that consider the domainspecific use case requirements directly quantifying the business impact or standard metrics that incorporate potential trade-offs in a weighted manner.In [11,12] accuracy is used as main metric.Jaidan et al. [11] use accuracy, precision, F1-score, recall and Huber-loss.</p>
<p>Only the recall is for this use case a truly meaningful metric.Nonetheless, for their final evaluation dataset only the accuracy value is given.In [12] only accuracy is considered.</p>
<p>W4: Lack of success criteria</p>
<p>The definition of requirements to classify the experiment as successful or not is neglected.Thus, the judgment of the results is often vague.The related works [11,12,15] do not have a success criteria.The authors of [27] define the goal of not introducing any slips, however, they do not define a goal for volume reduction.</p>
<p>W5: Inadequate handling of available information</p>
<p>The separation of information that is available at the time of the implementation and information that is not available is not strictly done.In [27] the results for an adapted decision threshold are discussed.No methodology for determining such a threshold a priori is discussed and the work strongly implies that the decision threshold are determined a posteriori on the evaluated dataset which is not feasible in production.Also, performances based on decision threshold set on a specific dataset a posteriori cannot be seen as representative for additionally datasets.</p>
<p>W6: Neglecting temporal dynamics in the dataset</p>
<p>Temporal attributes of the dataset are not considered in the sense that there is no investigation done regarding distribution drifts in the dataset.While all of [11,12,15,27] use different datasets for training and validation, it is not clear if the split was done randomly or sequentially.Only Jaidan et al. [11] mention for their final evaluation a new dataset, which implies a temporal split.However, none of the discussed related works gives a evaluation or analysis regarding similarity of their splits or temporal dynamics like data drifts in their dataset.W7: Limited experiment variability due to single experiment runs Experiments are just executed once for a certain random seed instead of evaluating multiple runs using different random seeds.None of the discussed related works shows the results of multiple runs (cf.[11,12,15,27]).As the number of related work for this use case is limited, in Table I we present an analysis of extended related work to strengthen the point that the listed weaknesses are common in industrial AI research.This analysis is based on the publications discussed in [18], which is a literature review of ML application related to SMT.Besides W5, all weaknesses are regularly present.However, to truly analyzing whether W5 is present, one would require the source code used and the corresponding dataset, which is not the case due to W1.
W1 W2 W3 W4 W5 W6 W7 [2] ✓ ✓ ✓ (✓) ✗ ✓ ✓ [4] ✓ ✓ ✓ ✓ ✗ ✓ ✓ [3] ✓ (✓) ✓ ✓ ✗ ✗ ✓ [25] ✓ ✓ (✗) ✓ ✗ ✓ (✓) [13] ✓ - ✗ ✗ ✗ ✓ (✓) [14] ✓ - ✗ ✗ ✗ ✓ (✓) [21] ✓ ✓ ✓ ✓ ✗ ✓ ✓ [20] ✓ ✓ ✓ ✓ ✗ ✓ ✓</p>
<p>IV. METHODOLOGY</p>
<p>In our research, we give insights into the effects of common weaknesses in the research about industrial AI applications on the example of false call reduction.Thereby, we utilize the dataset described and published in [23] and share our source code in [22].This dataset is chronologically ordered, tabular and consists of 77 columns including a timestamp and a label column featuring the labels false calls and defect.Furthermore, it partially consists of categorical features that we one-hotencode and we drop the timestamp column for the modelling.The label classes have an extreme imbalance ratio of 99%.</p>
<p>Additionally, a time dependency of the dataset can be seen in Figure 2. It shows the output of a two-dimensional principal component analysis applied to the dataset and the color of the points are based on their classes and their indexes in the dataset.Multiple clusters can be identified which clearly have different colors indicating that a certain cluster did just appear in a certain period of time.More information about the dataset can be found in the data repository [23] and its corresponding publication.</p>
<p>This dataset follows the approach of using the extracted measurements of the AOI machines on the soldering pin level.</p>
<p>For our experiment, we initially split our dataset chronologically into two halves.The first half is used for the subsequent modeling.Therefrom, we make a stratified random split in the ratio of 80% for a hyper-parameter dataset and 20% for a test dataset.The second half is used after the modeling to evaluate the performance of the model over time by splitting it chronologically into five slices, i.e. 10% of the total dataset.By this, we can evaluate how the model would perform over five evaluation intervals if it would be deployed to production and the original test dataset has the same size as the evaluation sliced.This logic of splitting our dataset can be seen in Figure 3, thereby the percent values are related to the total dataset.With this dataset, we then start a modeling phase.Thereby, we train common models with optimized hyper-parameters determined by Bayesian optimization, whereby in each run of the Bayesian optimization a stratified 5-fold cross-validation on the hyper-parameter dataset is executed.During crossvalidation, we evaluate the optimal decision threshold t optimal for each fold.Once we have found proper hyper-parameters, we train once more the model on the whole hyper-parameter dataset and set the decision threshold of the model to the mean value of the optimal decision thresholds from the crossvalidation.Then we evaluate the performance of this dataset on the test dataset.By this, we follow the common ML modeling scenario with the best practice methods of hyperparameter optimization and train, validation, and test data splits (cf. Figure 3).In this modeling, we consider the models k-nearest-neighbor (kNN) [8], random forest classifier (RFC) [1], eXtreme Gradient Boosting (XGBoost) [6], and balanced random forest classifier (BRFC) [5].This procedure is also described in Algorithm 1 in Appendix C. Additionally, we train a dummy classifier (DC) always predicting the most frequent class and an automated machine learning (AutoML) model based on [10] hyper-parameter optimization.The decision threshold of the AutoML model is adapted as well.</p>
<p>We execute this modeling phase twice.First, we use standard metrics as the target of the optimization and for evaluation.Second, we use requirement-aware metrics.For more details about the metric definitions see Section V. Finally, we then evaluate the created models on the evaluation slices to gain insights on how the model would have performed if those models would have been deployed productively.</p>
<p>As we do not have any benchmark values, we define our research target to find a model that enables a target volume reduction V target ≥ 40% while having a target slip rate S target ≤ 1% as one may assume that slips are much more crucial to the productions than volume reduction.Those values represent realistic requirements from the industrial shopfloor for the use case of false call reduction for AOI.Eventually, the AOI machine would not be configured that conservatively if it would be another.</p>
<p>V. EMPLOYED METRICS</p>
<p>We use a set of standard metrics and custom requirementaware metrics for our experiments.We will start in this section to define again the standard metrics first and then come to the custom metrics.We define positives to be defective boards, and negatives as false calls.For the use case itself, the false negatives, i.e. a defective PCB wrongly classified as false call slipping the manual inspection, must be considered much more crucial than false positives, i.e. a non-defective PCB boards that are manually inspected, since the latter case just reflects the state of the art situation without ML application.</p>
<p>For classification problems, metrics can be grouped by their relationship to a potential decision threshold of the models.Metrics may depend on a set threshold, are independent of a threshold, or give the result on the best possible threshold.</p>
<p>Common standard metrics that are threshold dependent are accuracy, recall, precision, and F1-score.While accuracy is an often used and widespread metric, it has a strong weakness against imbalanced datasets.For those cases, F1-score is considered often since it is more resilient against those cases and weights recall and precision evenly.</p>
<p>In comparison to that, different curves for evaluating classifiers can be used for taking different decision thresholds into account, for instance, the receiver operating curve (ROC) or precision recall curve (PRC).As discussed in [24], for imbalanced datasets the PRC is more informative than the ROC.Consequently, we use for this application the area under the precision recall curve (AUC) for this application.</p>
<p>A metric that expresses the trade-off between the recall of two groups is the Youden index [28].For our evaluation, we use the best Youden index of a classifier that can be achieved for any decision threshold t and call it Youden score.</p>
<p>In our modeling approach based on standard metrics, we use the metrics accuracy, F1-score, AUC, and Youden score as evaluation metrics for model selection.Furthermore, the AUC is used as an optimization target and the threshold is set based on the threshold found while calculating the Youden score.By this, we have a mix of commonly used metrics, that either assume a set decision threshold, are completely independent of a decision threshold, or that evaluate the case of the best possible decision threshold.</p>
<p>Even though the named metrics have shown their capabilities for different theoretical research applications, for research on actual applications of ML they remain unfit for evaluation purposes.Unlike theoretical research, applications of ML always have to justify their cost by achieving certain business criteria to be able to reach a return on investment.However, typically no standard metric reflects those business criteria.Therefore, it is necessary to evaluate ML models based on requirement-aware metrics that do directly reflect those business metrics.</p>
<p>The main business metrics for the application of false call reduction are the achieved test volume reduction v (cf.Equation 1) and the slip rate s (cf.Equation 2).Note that in Equation 1 and 2 recall 0 and recall 1 refer to the recall metrics for class 0 and class 1, respectively.While v is promising savings for the applications, s has the danger to produce additional cost.Thus, to identify if the application of false call reduction has positive business impact it is necessary to identify if a model is able to stay below a target slip rate S target and over a target volume reduction
V target . v = T N T N + F P = recall 0(1)s = F N T P + F N = 1 − recall 1(2)
Building onto this, we define three additional metrics that directly reflect the possible business impact that our model might have.As the first metric, we define constrained volume reduction (cV) as in Equation 3 with a minimal value of S target − 1 and a maximal value of one.All negative values of cV indicate that the maximum slip rate S target is exceeded with the set threshold.
cV = S target − s if s ≥ S target v otherwise(3)
An area-under-curve-based metric is our second requirement-aware metric constrained area under curve (cAUC).In this metric, we express the area under the slip volume reduction curve that lies within the target zone defined by S target and V target .The definition of this metric foresees three different cases.For the case where for any t the classifier can fulfill our targets, it has the value of the ratio of the area in the target below the classifier's curve and the target area.In the case where there exists an intersection between the area under the curve and the target zone but in the case where no t the targets are met, the value is zero.The classifier's performance curve is defined by the performance for different decision thresholds and is a discrete function.Thus, the steps of this discrete function can be in such a way that an overlap with the target area is created, even though for all potential thresholds, there is no threshold resulting in a performance within the target area.For the case that there is no intersection of both areas, the metric has the negative area of the gap between the curve and the target zone.Those different cases can also be seen in Figure 4 and are formalized in Equation 4. Thus, cAUC can give values between minus one and one.By this metric, it can be evaluated how well a classifier fulfills for any arbitrary t the business criteria.Fig. 4. Visualization of the cAUC metric for case that the classifier has at least one t creating a point in the target zone (left), the case that the classifier has no threshold in the target area but the area under curve intersects with the target area (middle), and the case that the classifier does not intersect with the target area at all (right)
if ∃ t ∈ [0, 1] : s(t) ≤ S target ∧ v(t) ≥ V target : cAU C = 1 0 (1 − s)dv − V target 0 (1 − s)dv − 1−S target 0 (v − V target )d(1 − s) (V target * S target ) else if: 1 V target (1 − s)dv &gt; 1 V target min(1 − s, 1 − S target )dv cAU C = 0 otherwise: cAU C = V target 0 min(1 − s, 1 − S target )dv − (V target * (1 − S target )) (V target * (1 − S target ))(4)
As a third requirement-aware metric, we define volume reduction at target slip (V@S) as the maximal volume reduction that fulfills the criteria of falling below the target slip rate for any t (cf.Equation 5).By this metric, it is possible to evaluate what volume reduction could be achieved by fulfilling the slip criteria when the perfect decision threshold is set.The metric V@S has a minimal value of zero and a maximal value of one.</p>
<p>V @S = max
t (v(t)) ∀t ∈ [0, 1] ∧ s(t) ≤ S target(5)
By those definitions, we again have a metric that is decision threshold specific, a metric that is decision threshold independent and a metric that evaluates the case of the best possible decision threshold.We use all three custom metrics for evaluation metrics for model selection.Furthermore, we use cAUC as optimization target and determine the threshold based on the threshold found while calculating V@S.</p>
<p>After discussion the standard and custom metrics, we want to give a theoretical comparison of their values for the edge case that one of the targets is exactly not fulfilled and compare the scenario in which the standard metrics are maximal and therefore most misleading.Figure 5 shows a comparison for the metrics accuracy, F1-score, and cV for different slip rates and rates of volume reduction.While accuracy overly depends on the volume reduction and therefore has a complete vertical region with values around 1, F1-score is more defines and only gives a smaller vertical area with values around 1.</p>
<p>Nevertheless, cV shows only high values in a vertical line on the top right corner, which is indeed aligns with our metric targets.That accuracy is prone to imbalanced data is well known -yet, even F1-Score can reach a value of 0.995 that still does not satisfy the set targets while all of our metric clearly indicates this edge case with values close to 0. Similarly the Youden Score can reach a value up to 0.99 as well as PRC.</p>
<p>Indeed, the defined metrics are customized towards the researched use case.However, the approach of using a set of threshold-dependent, optimal-threshold, and thresholdindependent metrics as well as the approach of having requirement-aware metrics can be seen as universal.For our needs, cAUC is defined for the slip rate and volume reduction, yet it could also be used for example as a constrained version of the PRC or ROC curve.</p>
<p>VI. EXPERIMENTAL RESULTS</p>
<p>In this section, we will show and elaborate on the two modeling procedures, whereby one is using standard metrics and the other requirement-aware metrics.For each modeling procedure, we evaluate the created models to rank their performance and select the best ones.After this, we give a comparison with all metrics of the created models to compare the true performance.As the final step, we show the performance of the models on the evaluation slices to investigate their performance if they would have been deployed to the production environment.All results are given as average ± standard deviation of ten runs with different random seeds.</p>
<p>In this first modeling approach, we use standard metrics as discussed in Section V and adapt the threshold based on the Youden index.The results can be seen in Table II.</p>
<p>The results show that the model XGBoost1 seems to be the best model independently of the metrics.Furthermore, based on the accuracy metric the model DC1 seems to outperform all other models except XGBoost1.Nevertheless, the other metrics show its poor performance which indicates the in general poor informative value of this metric in regards to imbalanced datasets.In general, accuracy and F1-score indicate poor performance of the model BRFC1 while the Youden score indicates a performance close to the other models.Additionally, the fact that the F1-score of BRFC1 is higher than that of the model kNN1 while their Youden score has the opposite ranking catches one's eye.This can be explained by the fact that the Youden score evaluates the recalls regarding both classes while the F1-score is considering the precision and the recall in regards to class one combined with the extreme imbalance of the dataset.The model AutoML1 is in general performing better than most of the models.Lastly, even though we can analyze and compare the performance of the trained models, it is not possible to conclude whether we have reached our business goals or not as they are not reflected in the available metrics.In this first modeling approach, we use our requirementaware metrics as discussed in Section V and adapt the threshold based on the target slip rate.The results can be seen in Table III.</p>
<p>Based on our requirement-aware metrics V@S and cAUC the model XGBoost2 seems to perform best.However, the metric cV reveals that this model performs much worse than the other models.This pattern indicates, that in general, XGBoost2 seems to be a superior model however the method for adapting the threshold works poorly.A similar pattern can be seen for the model AutoML2.Note, that the difference between AutoML1 and AutoML2 is the target metric used for threshold adaption.Thus, one either must improve the method for adapting the decision threshold first or should instead take the model BRFC2 or the model RFC2 as those two models are the only ones that on average reach a constrained volume larger than 40%, i.e. fulfill our set targets for slip rate and volume reduction.However, for both models, their cV has a high standard deviation.This might indicate that for the different randoms seeds, not all runs lead to a sufficient model but instead, some models do not fulfill the business requirements and some exceed them significantly.</p>
<p>If we now compare the results of both modeling approaches including all the metrics, we can see in Table V and Figure 6 how the used metrics have impacted our model selection.</p>
<p>The results show, that the standard metrics are in many cases contrary to the requirement-aware metrics.For instance, kNN2 and DC1 seem to have similar poor behavior based on accuracy, F1-score, and PRC, which is connected to the extreme imbalance of the used dataset.Also in general, according Fig. 6.Performances of all models on the test dataset comparing all metrics to the standard metrics, all models, which hyper-parameters were optimized according to the standard metric AUC, seem superior to the models, which hyper-parameters have been optimized according to the requirement-aware metric cAUC of the same algorithm.Meanwhile, the requirement-aware metrics show that either the models of the second modeling execution are comparable or better, in specific considering cV.Looking at the business metrics, one can see that they are well represented by the requirement-aware metrics but poorly by the standard metrics.</p>
<p>The two models BRFC2 and RFC2 have reached the set targets for slip rate and volume reduction on average.Thus, they now could be deployed to a productive environment.For evaluating the performance over time, we can now use the evaluation slice.Table IV shows the slip rate and volume reduction for all slices of models BRFC2 and RFC2. Figure 7 offers a visualization of the chronological performance trends.</p>
<p>Even though both models have been promising in the modeling phase, for all evaluation slices the model performances decays immediately.Especially the slip rates exceed already for the first evaluation slice strongly the set target for the slip rate S target .The volume reduction decays as well yet the values are still mostly larger than the target V target .Thus, a productive deployment would be a failure and might result in high effort on the shopfloor and cause financial harm.The common evaluation method of a randomly split k-fold cross validation would have failed.In this section, we now discuss the effect the different weaknesses defined in Section IV could have had.W1 is addressing the reproducibility and transparency of scientific results when not sharing the dataset and the used source code.As our dataset and our source code are openly available [22,23] it is possible for the scientific community to verify the results but also easily build upon the existing results without the need to have an in detail description of our implementation.The application of anonymization techniques, as for example discussed in [16], can enable the publication of potentially sensitive data.</p>
<p>Looking at our code base, a large part is the implementation of a Bayesian optimization for different models with different hyper-parameters.Meanwhile, the code for training AutoML models is less and simpler.Because of that, W2 is addressing the lack of usage of AutoML models in related work and instead a strong focus on common models without naming reasons.As by now there exist mature ML frameworks making it trivially simple to train models on arbitrary datasets, research should move on and either develop application-specific models, tackle problems that prevent automatizing this aspect of modeling, or focus on problems beyond the modeling.In our results, the AutoML models do not perform best however the performance is absolutely comparable in terms of thresholdindependent metrics.For the threshold-dependent metric cV, the models performed poorer.This is a consequence of the used method for setting a threshold as it does not work stable enough and has a high standard deviation for the calculated thresholds within the cross-validation.A first benchmark for more selective modeling approaches should be the result of AutoML models along with a DC.</p>
<p>Regarding W3, our results show clearly the weaknesses of standard metrics, in particular accuracy.As shown in Table II a DC can achieve a better accuracy than properly trained models.Some may argue, that it is common sense that the more imbalanced a dataset is, the more insignificant the metric accuracy is.Yet, it is frequently used in the related works discussed in Section III.Nevertheless, also other standard metrics are insufficient as they do not reflect the business targets that are met and thus do not allow to classify a modeling procedure to be successful or not.Having proper evaluation metrics is of specific importance when a machine learning operation concept for the application including automatic retraining and model re-deployment is wanted.Also, Table V shows clearly that the usage of requirement-aware metrics in the hyperparameter optimization and the threshold adaption for the model has a crucial impact on successful modeling as standard metrics and application-specific metrics can indicate model performance contrary.Therefore, ML research should always first identify the direct business metrics then use evaluation metrics that are directly linked to those business metrics.In the optimal case, those evaluation metrics should be requirementaware.</p>
<p>W4 demands clear targets and requirements in research of ML applications that can classify whether an experiment was successful or not.By this, it was possible to select the potential deployable models directly and definitive judge the success of the experiment.Speaking for our results, we have found two models that on average fulfill our set targets on the test data.However, the standard deviation is comparably high and indicates that not all models reach the targets but just the average model.For the evaluation slices, those models fail.Thus, by having defined targets, we can derive now potential next steps to stabilize the modeling by improving the method for setting the threshold and handling the distribution drifts within the dataset.This demonstrates how crucial it is to define clear research targets, as otherwise we could now argue that our research was successful.Especially considering that this is common praxis in other scientific domains, such as statistics, all future applied ML research should have clear success criteria defined beforehand.Furthermore, success criteria are the foundations of requirement-aware metrics.</p>
<p>In W5 we mention the importance of separating knowledge that is known a priori and knowledge that is only known a posteri.An example can be seen in Table III and is a matter of setting the threshold.The metric V@S uses knowledge that is only available with the ground truth label while cV is using only knowledge that is available beforehand by just using the beforehand set threshold.All models have a higher V@S than a cV even though both metrics actually express both in their positive ranges the volume reduction.In fact, all models of Table V, except kNN1 and DC1, would reach the success criteria according to the metric V@S, even though the metric cV shows a complete different picture.In a real-world implementation, only a priori information can be used and thus, the expected values are likely to be closer to cV than to V@S.</p>
<p>The common best practice in ML modeling uses a training, validation, and test dataset and assumes that the performance on the test dataset is valid for future data.However, this can be wrong.The used dataset is in its nature tabular and not a time series.So, a random split for training, validating, and testing data is intuitive.However, this should only be done after checking for temporal attributes like distribution drifts within the dataset.There are various distribution drifts happening in this dataset but with the common best practice, researchers risk the potential pitfall of oversee distribution drifts.For instance, Figure 2 would clearly show that there is a time dependency of the dataset and a random split will not lead to realistic performance values.However, this aspect is either not done for academic industrial AI research or not discussed (cf.[11,12,15,27]).Therefore, W6 pleads for an extension of those common best practices also checking for temporal attributes in a dataset before modeling or at least take the chronological last data points as test data set.</p>
<p>For an implementation of an ML application, results need to be stable.For this, it is essential to not only evaluate only one run with only one random seed but have multiple runs with different random seeds as pointed out in W7.For instance, for the models BRFC2 and RFC2 exist runs that have an outstanding low slip rate.However, we can see with multiple runs, that on average the model performances just so reach the targets, but the standard deviation is relatively high.Thus, there is still potential to find more stable methods, for example, by refining the method for setting the threshold.</p>
<p>VIII. CONCLUSION AND OUTLOOK</p>
<p>In this work, we discuss the weaknesses of the research methodolgy for the use case of research using false call reduction for AOI.We derive seven common weaknesses from peer-reviewed related work and show their consequences experimentally.</p>
<p>In our methodology we explain how we split our dataset allowing a best practice modeling phase but also an analysis of the model performance over time if one would deploy the models.As preparation for our experiment, we define clear success criteria, and define a set of custom requirement-aware metrics that are directly expressing the business impact in contrast to regular metrics such as accuracy, F1-score, or AUC.</p>
<p>Our experiment consists of two different modeling executions using regular metrics and requirement-aware metrics, respectively.For each modeling phase, we select the best model depending on the different metrics to then compare all metrics side by side to demonstrate how misleading inappropriate metrics can be considering the business objectives.Furthermore, we discuss the challenge of setting a proper decision threshold and show the importance of strictly separating between datasets on which the threshold was set and on which the model was evaluated.</p>
<p>Additionally, we evaluate the performance of the created models over a temporal split evaluation dataset showing that even as sufficient evaluated models fail completely in production.Consequently, we must conclude that in regards of the set success criteria our modelling is successful, but the model would fail from the first moment in production.</p>
<p>In regards of improving the experimental results, a large contribution to this was done using requirement-aware metrics.While in this work the model type of neural networks has been neglected, the authors [9] suggest a way to embed performance metrics like precision at recall directly into the loss function used for training the neural network.Further work may dig deeper into extending this approach to a loss function based on the volume reduction on a certain slip rate.</p>
<p>In the analysis of the model performance over time, one observed a strong performance decay.Thus, it is necessary to create a monitoring system allowing to successfully deploy the model long term.However, for this application, this seems to be in specific a challenge.For monitoring the performance, ground truth labels are required.However, the business case of the application is to no longer generate this ground truth.A naive approach could be using random sampling, but also smarter sampling methods can be found with further research.</p>
<p>Concluding, the results of our work are useful for two audiences: for researchers focusing on the use case of false call reduction for AOI as this is the first work including its source code and its data and for researcher striving for an optimal research methodology for applied AI research, as we have shown that the discussed weaknesses are not only present in the related work for false call reduction for AOI use, but also in general for use cases in electronic production, and probably beyond.</p>
<p>Fig. 1 .
1
Fig. 1.Comparison of the common SMT scenario and the enhanced false call reduction scenario</p>
<p>Fig. 2 .
2
Fig. 2. Principal component analysis of the inspection type 2 of the dataset.The left color scale is for false calls and the right color scale is for true errors.</p>
<p>Fig. 3 .
3
Fig. 3. Data splitting process for our experiment</p>
<p>Fig. 5 .
5
Fig. 5. Visualization of the metrics Accuracy, F1-Score, and cV for the given dataset's class imbalance.</p>
<p>Fig. 7 .
7
Fig. 7. Performances of deployable models for the test dataset and the evaluation slices</p>
<p>TABLE V PERFORMANCES
V
OF ALL MODELS ON THE TEST DATASET SHOWING ALL METRICS
ModelStandard metricsRequirement-aware metricsBusiness metricsreferenceAccuracy F1-scorePRCYouden scoreV@ScVcAUCSlip rateVolume ReductionAutoML10.987 ±0.0090.581 ±0.1760.866 ±0.0160.944 ±0.0060.906 ±0.021−0.103 ±0.0890.497 ±0.1360.113 ±0.0890.988 ±0.01AutoML20.949 ±0.0490.413 ±0.2830.866 ±0.0160.944 ±0.0060.906 ±0.0210.103 ±0.4020.497 ±0.1360.083 ±0.0950.949 ±0.049DC10.992 ±0.00.0 ±0.00.504 ±0.00.0 ±0.00.0 ±0.0−0.99 ±0.0−1 ±0.01.0 ±0.01.0 ±0.0BRFC10.964 ±0.0050.299 ±0.0290.713 ±0.020.919 ±0.0130.804 ±0.072−0.042 ±0.0150.307 ±0.1850.052 ±0.0150.964 ±0.005BRFC20.799 ±0.030.074 ±0.010.704 ±0.0190.918 ±0.0120.796 ±0.0640.423 ±0.3960.305 ±0.190.008 ±0.0060.797 ±0.03kNN10.981 ±0.0110.473 ±0.1460.739 ±0.0230.879 ±0.0360.0 ±0.0−0.14 ±0.139−0.044 ±0.0260.15 ±0.1390.982 ±0.012kNN20.558 ±0.3080.057 ±0.0490.521 ±0.0670.853 ±0.0310.475 ±0.3440.227 ±0.3390.079 ±0.0910.014 ±0.0180.555 ±0.311RFC10.983 ±0.0050.485 ±0.0940.842 ±0.0160.948 ±0.010.765 ±0.328−0.047 ±0.0570.339 ±0.2390.057 ±0.0570.983 ±0.005RFC20.898 ±0.0380.015 ±0.0570.799 ±0.0490.942 ±0.0120.901 ±0.0420.611 ±0.4160.5 ±0.1690.01 ±0.0120.897 ±0.039XGBoost10.993 ±0.0050.699 ±0.1240.909 ±0.0150.959 ±0.0130.923 ±0.051−0.068 ±0.0430.574 ±0.1520.078 ±0.0430.993 ±0.005XGBoost20.599 ±0.4250.185 ±0.2060.872 ±0.0290.953 ±0.0130.923 ±0.0510.16 ±0.3240.626 ±0.120.015 ±0.0210.596 ±0.429
APPENDIX A DECLARATION ON THE USE OF AIFor creating this work, generative AI was used to improve the grammar, conciseness, and clarity of the text.APPENDIX B PUBLICATION ACKNOWLEDGEMENT
Random Forests. Leo Breiman, Machine learning. 452001. 2001</p>
<p>SMT Solder Joint Inspection via a Novel Cascaded Convolutional Neural Network. Nian Cai, Guandong Cen, Jixiu Wu, Feiyang Li, Han Wang, Xindu Chen, 10.1109/TCPMT.2018.2789453IEEE Transactions on Components, Packaging and Manufacturing Technology. 82018. 2018</p>
<p>An online machine learning framework for early detection of product failures in an Industry 4.0 context. José Carvajal Soto, F Tavakolizadeh, Dávid Gyulai, 10.1080/0951192X.2019.1571238International Journal of Computer Integrated Manufacturing. 32022019. 2019</p>
<p>An Implementation of Health Prediction in SMT Solder Joint via Machine Learning. Yi-Ming Chang, Chia-Chen Wei, Jeffrey Chen, Pack Hsieh, 10.1109/BIGCOMP.2019.86794282019 IEEE International Conference on Big Data and Smart Computing (BigComp). 1-4. 2019</p>
<p>Using Random Forest to Learn Imbalanced Data. Chao Chen, Andy Liaw, Leo Breiman, 666Department of Statistics. UC Berkley2004Technical Report</p>
<p>XGBoost: A scalable tree boosting system. Tianqi Chen, Carlos Guestrin, 10.1145/2939672.2939785Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 13-17. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 13-172016. August-2016 (8 2016</p>
<p>Xception: Deep Learning with Depthwise Separable Convolutions. Francois Chollet, 10.1109/CVPR.2017.1952017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Honolulu, HI, USAIEEE2017</p>
<p>Approximate formulas for the information transmitted bv a discrete communication channel. T M Cover, P E Hart, IEEE TRANSACTIONS ON INFORMA-TION THEORY. 2411952. 1952</p>
<p>Scalable Learning of Non-Decomposable Objectives. Elad Eban, Mariano Schain, Alan Mackey, Ariel Gordon, Ryan Rifkin, Gal Elidan, PMLR, 832-840Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research. Aarti Singh, Jerry Zhu, the 20th International Conference on Artificial Intelligence and Statistics ( Machine Learning Research201754</p>
<p>Efficient and Robust Automated Machine Learning. Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, Frank Hutter, Advances in Neural Information Processing Systems. N Cortes, D Lawrence, M Lee, R Sugiyama, Garnett, Curran Associates, Inc201528</p>
<p>Eva Jaidan, Philippe Besse, Jean-Michel Loubes, Nathalie Roa, Christophe Merle, Rémi Dettai, 10.1007/978-3-030-13709-0_21Supervised Learning Approach for Surface-Mount Device Production: 4th International Conference. LOD; Volterra, Italy; Volterra, ItalySpringer International Publishing2019. 2018. September 13-16. 2018Revised Selected Papers</p>
<p>Fine-Tuning Strategy for Re-Classification of False Call in Automated Optical Inspection Post Reflow. I H Jamal, H A Mh, Che Nasir, M I F Ani Adi Izhar, K A Maruzuki, Ishak, 10.1109/eSmarTA56775.2022.99353662022 2nd International Conference on Emerging Smart Technologies and Applications (eSmarTA). 2022 2nd International Conference on Emerging Smart Technologies and Applications, eSmarTA 2022. 20222</p>
<p>Stencil Printing Process Optimization to Control Solder Paste Volume Transfer Efficiency. Nourma Khader, Sang Won Yoon, 10.1109/TCPMT.2018.2830391IEEE Transactions on Components, Packaging and Manufacturing Technology. 82018. 2018</p>
<p>Stencil Printing Optimization using a Hybrid of Support Vector Regression and Mixed-integer Linear Programming. Nourma Khader, Sang Won Yoon, Debiao Li, 10.1016/j.promfg.2017.07.31827th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017. Modena, Italy2017. 2017. June 201711</p>
<p>A visual inspection system for surface mounted devices on printed circuit board. Shih Chieh, Lin , Chia Hsin, Su , 10.1109/ICCIS.2006.2522372006 IEEE Conference on Cybernetics and Intelligent Systems. 2006 IEEE Conference on Cybernetics and Intelligent Systems, 1-4. 2006</p>
<p>Anonymization Techniques for Privacy Preserving Data Publishing: A Comprehensive Survey. Abdul Majeed, Sungchang Lee, 10.1109/ACCESS.2020.3045700IEEE Access. 92021. 2021</p>
<p>Design and development of automatic visual inspection system for PCB manufacturing. N S S Mar, P K D V Yarlagadda, C Fookes, 10.1016/j.rcim.2011.03.007Robotics and Computer-Integrated Manufacturing. 272011. 10 2011</p>
<p>Short Review on Machine Learning Optimization Methods in Surface Mounted Electronics Assembly Technologies. Péter Martinek, Made Putrama, Olivér Krammer, Attila Géczy, 10.1109/ISSE57496.2023.101685242023 46th International Spring Seminar on Electronics Technology (ISSE). 2023</p>
<p>Magdalena Michalska, 10.35784/iapgos.2379OVERVIEW OF AOI USE IN SURFACE-MOUNT TECHNOLOGY CON-TROL. Informatyka, Automatyka, Pomiary w Gospodarce i Ochronie Środowiska 10. 202012</p>
<p>Data-Driven Prediction Model of Components Shift during Reflow Process in Surface Mount Technology. Irandokht Parviziomran, Shun Cao, Haeyong Yang, Seungbae Park, Daehan Won, 10.1016/j.promfg.2020.01.01429th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019). Limerick, Ireland2019. 2019. June 24-28, 201938Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing</p>
<p>Optimization of Passive Chip Components Placement with Self-Alignment Effect for Advanced Surface Mounting Technology. Irandokht Parviziomran, Shun Cao, Haeyong Yang, Seungbae Park, Daehan Won, 10.1016/j.promfg.2020.01.31325th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing. Chicago, Illinois (USA)2019. 2019. August 9-14, 201939</p>
<p>Towards Improved Research Methodologies for Industrial AI: A Critical Examination using Automated Optical Inspection False Call Reduction as a Case Study -Source code. Korbinian Pfab, 2023</p>
<p>Publication data: Data of automated optical inspection of surface-mounted technology electronic production. Korbinian Pfab, Eichler Roman, Mallandur Adarsh, Rothering Marcel, 10.17632/99jzmh9658.12023</p>
<p>The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. Takaya Saito, Marc Rehmsmeier, 10.1371/journal.pone.0118432PLOS ONE. 103e01184322015. 2015</p>
<p>Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing. Jacqueline Schmitt, Jochen Bönig, Thorbjörn Borggräfe, Gunter Beitinger, Jochen Deuse, 10.1016/j.aei.2020.101101Advanced Engineering Informatics. 451011012020. 2020</p>
<p>Automatic Optical Inspection for PCB Manufacturing: a Survey. Eid M Taha, Khalid Emary, Moustafa, International Journal of Scientific &amp; Engineering Research. 52014. 2014</p>
<p>A Machine Learning Based Approach to Detect False Calls in SMT Manufacturing. Nils Thielen, Dominik Werner, Konstantin Schmidt, Reinhardt Seidel, Andreas Reinhardt, Jorg Franke, 10.1109/ISSE49702.2020.91210442020 43rd International Spring Seminar on Electronics Technology (ISSE). 2020 43rd International Spring Seminar on Electronics Technology (ISSE). 2020</p>
<p>Index for rating diagnostic tests. W J Youden, 10.1002/1097-0142(1950)3:1&lt;32::AID-CNCR2820030106&gt;3.0.CO;2-3AID-CNCR2820030106⟩3.0.CO;2-3Cancer. 31950. 1950</p>            </div>
        </div>

    </div>
</body>
</html>
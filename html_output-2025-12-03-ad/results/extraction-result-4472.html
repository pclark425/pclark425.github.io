<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4472 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4472</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4472</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-268248855</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.02574v1.pdf" target="_blank">ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</a></p>
                <p><strong>Paper Abstract:</strong> The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4472.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4472.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent that mimics a human literature-review workflow: it first extracts structured key elements from each paper, then iteratively composes comparative related-work summaries using a reflective incremental generation and LLM-based voting to select top candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-module agent: (1) Key Element Extractor — prompts an LLM with seven guiding questions concatenated with paper content to extract and store structured key elements for each paper; (2) Reflective Incremental Generator — an iterative breadth-first-search-style generator where a Comparative Summarizer produces multiple candidate continuations per step (using prior summary candidates plus the new paper's key elements), and a Reflective Evaluator (an LLM) votes multiple times to score/rank candidates, retaining the top-n for the next iteration until all reference papers are processed; final output chosen by highest voting score. The system uses LLMs as both generator and in-process evaluator and avoids separate model training by relying on prompting, candidate generation, and voting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Experiments used GPT-3.5 (gpt-3.5-turbo-1106) as the generation/decoder in the ChatCite pipeline and GPT-4 (gpt-4-turbo-preview) as the evaluator for automatic scoring; paper describes the design as LLM-agnostic but these were used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompt-based extraction: seven pre-defined guiding questions (Q_e) concatenated with paper content form the extraction prompt P_e; LLM decodes responses into stored key elements per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative comparative incremental generation: Comparative Summarizer composes multiple candidate summaries per iteration by integrating prior candidates and current paper key elements; Reflective Evaluator (LLM) votes nv times to score candidates; top-n_c candidates retained, continuing until all papers processed. Breadth-first-search-style expansion over papers to produce organized comparative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Per example, processes the provided reference-paper set R = {r1...rn} for each target; evaluated on the NuDtRwG-Citation test set of 50 target papers (each with its associated reference set).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (NLP / computational linguistics papers in the dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summaries / related-work sections (structured multi-paper synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Automatic: ROUGE-1/2/L (F1), G-Score (LLM-based aggregate metric using GPT-4); Human: expert human ratings along six dimensions (Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported ROUGE and LLM-evaluator numbers: ROUGE-1 25.30, ROUGE-2 6.36, ROUGE-L 23.13; G-Score (aggregate LLM-eval, 1-5 scale) reported ~4.06. Paper states ChatCite outperformed direct LLM baselines and the LitLLM baseline on multi-dimensional quality metrics and human preferences, though ROUGE was slightly below GPT-4 zero-shot in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct LLM generation (GPT-3.5 and GPT-4 zero-shot and few-shot) and LitLLM (Agarwal et al., 2024) reproduced with GPT-4 as decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ChatCite achieved higher G-Score and better alignment with human preferences than the direct LLM baselines and LitLLM; ROUGE scores were sometimes slightly lower than GPT-4 zero-shot, but ChatCite's multi-dimensional human-aligned quality measures favored it over baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Guiding LLM generation with a human-like workflow (structured key-element extraction + iterative comparative summarization + reflective voting) improves comparative analysis, organizational structure, citation accuracy, and stability of generated literature summaries compared to simple CoT or direct generation; reflective voting reduces variance and selects higher-quality candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Experiments limited to computer-science papers (dataset of 50 targets); high-cost LLMs (GPT-4) not used for full generation due to cost — GPT-3.5 used as decoder; remaining randomness and instability in outputs; evaluation relies on LLM-based metrics which may themselves carry bias; ChatCite focuses on summarization stage and does not handle literature retrieval/filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Algorithm maintains a fixed number (n_c) of top candidates per step and expands over the number of reference papers (tree depth = number of papers); paper does not present large-scale (>50-target) scaling experiments — increasing papers increases iterations and LLM calls and may raise compute cost without further empirical scaling claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4472.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Key Element Extractor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key Element Extractor (ChatCite module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured prompt-based extraction module that asks seven guiding questions per paper and uses an LLM to extract and store essential elements (e.g., contributions, methods, datasets) to prevent omission during summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Key Element Extractor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Creates a prompt P_e = [Q_e, C] where Q_e is a set of seven simple guiding questions and C is the target paper content; feeds P_e to an LLM acting as an extraction decoder to produce structured key elements which are stored in memory for later use by the Comparative Summarizer.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>In the ChatCite experiments GPT-3.5 (gpt-3.5-turbo-1106) was used for generation/extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting with an explicit schema (seven targeted questions) to elicit specific key elements from each paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesis module itself; supplies structured inputs (key elements) for downstream iterative synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied per-paper for each reference and the proposed work; used across the reference sets in the dataset (50 target instances overall).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (as used in the experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured key-element records per paper (to be consumed by the synthesis module).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirectly evaluated via ablation: downstream ROUGE, G-Score, and human evaluations comparing ChatCite with and without the extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ablation showed ChatCite without the Key Element Extractor performed worse across ROUGE and LLM-based metrics; the extractor improved content consistency in human studies (exact numeric deltas not fully enumerated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>ChatCite without the Key Element Extractor (i.e., direct summarization prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inclusion of the extractor improved ROUGE and LLM-evaluation metrics and human-judged content consistency relative to the no-extractor ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly extracting and retaining structured elements reduces information omission and yields more consistent content in generated literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Performance depends on the quality of the guiding questions and LLM extraction accuracy; context window limits may still restrict extraction for very long papers; no automated verification of extraction fidelity beyond downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Operates per paper; scaling involves repeated LLM extraction calls linear in the number of reference papers — no empirical large-scale scaling reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4472.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflective Incremental Generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Incremental Generator (ChatCite module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative multi-candidate generation mechanism that composes comparative summaries incrementally, using an LLM-based reflective evaluator to vote and prune candidates at each step to improve stability and comparative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflective Incremental Generator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each turn i the Comparative Summarizer G constructs n_s summary candidates by combining the proposed-work key elements, the i-th reference paper's key elements, and each candidate s in S_{i-1}. The Reflective Evaluator E (an LLM) votes n_v times over the expanded candidate set S'_i, producing scores E_i; the system sorts and retains the top n_c candidates for the next iteration. The process repeats until all reference papers have been incorporated, effectively maintaining a beam of promising multi-paper summaries and using LLM-based voting to stabilize selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Generation experiments used GPT-3.5 (gpt-3.5-turbo-1106) as the generator; the design uses LLMs in-process for reflective evaluation (the paper also uses GPT-4 as an external evaluator in evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Relies on pre-extracted key elements from the Key Element Extractor; no additional extraction technique beyond using those structured elements.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative candidate generation + LLM voting/pruning (breadth-first-search-like beam expansion) to synthesize multi-document comparative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Iterates over the reference set; experiments conducted on dataset with 50 target papers; per-target number of references varies (n).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (NLP papers in dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summaries / related-work drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, G-Score (LLM evaluator), human ratings; ablation study comparing with/without the reflective mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Inclusion of the reflective mechanism produced more stable results with slightly higher overall scores; boxplot analysis showed reduced variance across evaluation dimensions. Exact numeric improvements not exhaustively enumerated but reported as meaningful for stability and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>ChatCite without Reflective Mechanism (i.e., no LLM voting/pruning), and standard few-shot CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reflective mechanism improved stability and selected higher-quality candidate summaries versus the no-reflection ablation; overall ChatCite (with reflection) outperformed baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based reflective voting (multiple evaluations per candidate) helps filter unstable outputs and produces more consistent, higher-quality summaries; treating generation as a candidate search with LLM-based scoring is effective for multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Higher computational cost due to multiple candidate generations and multiple LLM evaluation calls per iteration; choice of hyperparameters (n_v, n_s, n_c) impacts cost and outcomes; still exhibits some randomness and requires further work to improve stability/scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Design maintains a fixed beam size (n_c) while depth equals number of reference papers; without careful pruning or parameter tuning, candidate expansion could become expensive as number of papers grows; no empirical large-scale scaling results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4472.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit that applies Retrieval-Augmented Generation (RAG) principles, specialized prompting, and instructive techniques to generate literature reviews while addressing hallucination and recency issues; used as a comparative baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Toolkit that combines retrieval (to fetch relevant papers or passages) with instructive prompting of LLM decoders (RAG-style) and tailored prompts to reduce hallucination and incorporate recent research; in this paper the authors reproduced LitLLM with GPT-4 used as the decoder to form a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reproduced baseline used GPT-4 as decoder in the experiments reported in this paper; original LitLLM is designed to be used with large LLM decoders (unspecified here).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-based (RAG) — embedding or index-based retrieval to gather grounding context for generation; specialized prompting for retrieval and filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented generation: combine retrieved contexts with LLM generation; uses simpler Chain-of-Thought guidance for certain steps in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the reference sets for each target in the same dataset (50 target tasks); exact per-target counts vary and are determined by the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review (general), evaluated on computer science papers in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature summaries / related-work sections.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, G-Score (LLM-based evaluator), human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to produce ROUGE scores comparable to GPT-4 zero-shot in some metrics (paper table indicates LitLLM ROUGE values), but exhibited lower G-Score and human preference than ChatCite; exact numeric comparisons in paper show ChatCite superior on multi-dimensional quality despite some ROUGE differences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against direct LLM baselines (GPT-3.5/GPT-4 zero/few-shot) and ChatCite.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LitLLM performed similarly to GPT-4 zero-shot on ROUGE, but was significantly worse than ChatCite on G-Score and human-preference metrics according to the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG plus specialized prompting reduces hallucination and helps incorporate recency, but simple CoT guidance and lack of a structured multi-step synthesis pipeline limit LitLLM's ability to produce in-depth comparative and organizational analyses compared to ChatCite.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependence on retrieval quality; simple CoT guidance used for summarization can produce disorganized or non-comparative outputs; still susceptible to hallucination if retrieval is incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly analyzed in this paper; LitLLM's retrieval-based design suggests scaling depends on retrieval/indexing infrastructure and retrieval quality as number of papers grows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4472.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Eval: NLG evaluation using GPT-4 with better human alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based evaluation framework that uses GPT-4 and chain-of-thought / structured prompts to score NLG outputs across dimensions in a way that aligns better with human judgments than traditional metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>G-eval: Nlg evaluation using gpt-4 with better human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses GPT-4 with chain-of-thought and a form-filling evaluation paradigm to assess NLG outputs across multiple criteria, producing scores that correlate well with human judgments; inspired the G-Score metric devised in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (as used in the referenced G-Eval work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based scoring via structured prompts and CoT-style reasoning to justify scoring decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregates per-dimension judgments into overall evaluation scores; can include voting or multi-run persistence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not an information-extraction system over papers; used as an evaluator across NLG outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General NLG evaluation (applied across benchmarks in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evaluation scores and human-aligned rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared to ROUGE and other automatic evaluators; reported correlation with human judgments on SummEval and Topical-Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to outperform prior automatic evaluators on some benchmarks and align more closely with human evaluations (per the cited G-Eval paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional automatic metrics (ROUGE, BLEU, etc.) and prior automatic evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperforms ROUGE-like metrics in alignment with humans on cited benchmarks, but the method can be sensitive to prompt/instruction and may favor LLM-generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based evaluators with CoT improve human alignment for NLG evaluation, providing more nuanced multi-dimensional scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential bias towards LLM-generated outputs; sensitivity to instruction and prompt phrasing; not necessarily immune to evaluator hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Evaluation cost scales with number of outputs and complexity of multi-run CoT evaluations; no large-scale extractor-to-synthesis scaling claims in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4472.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Score (LLM-based automatic evaluation metric, this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A six-dimensional LLM-based automatic evaluation metric introduced in this paper that scores generated literature summaries on Consistency, Coherence, Comparative, Integrity, Fluency, and Cite Accuracy using an LLM (GPT-4) and aggregates into a total score aligned with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>G-Score</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation pipeline using an LLM to simultaneously score multiple model-generated summaries in one conversation across six literature-review-specific dimensions (each 1-5). The system also uses LLM-based voting to choose best summaries and produces model-preference statistics; it was inspired by G-Eval and adapted to literature-summary evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (gpt-4-turbo-preview) used as the automatic evaluator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based scoring via guided prompts (no information extraction from source papers — it scores generated summaries relative to gold summaries and criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregates multi-dimensional scores into an overall G-Score and derives model preference metrics by voting.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to evaluate generated summaries for the full test set (50 target papers) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature summary generation (computer-science papers in the dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-dimension and aggregated quality scores; model preference rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Six-dimension 1-5 scores (Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy); aggregated G-Score; compared to ROUGE and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>G-Score evaluations were reported to align with human evaluations and indicated ChatCite's superiority vs baselines; paper states G-Score is more consistent with human preferences than ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>ROUGE metrics and human expert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>G-Score provided rankings and judgments more consistent with human evaluators than ROUGE; used to demonstrate ChatCite's advantage in multiple quality dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A domain-specific multi-dimensional LLM-based evaluator (G-Score) better captures literature-summary quality and aligns with human preference compared to lexicon overlap metrics such as ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on LLMs as evaluators (may inherit evaluator biases); the automatic evaluation accuracy can be improved further per the paper; potential sensitivity to prompt design and the choice of LLM used for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Evaluation cost scales with number of summaries and number of candidate outputs jointly evaluated; applied on 50 examples in the paper but no broad scaling analysis presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4472.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve performance on complex reasoning tasks; prior literature-review automation work used simple CoT guidance but found it insufficient for comparative, organized literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides LLMs with prompts that encourage step-by-step reasoning or self-decomposed plans (CoT) so that the model exposes intermediate rationales that can improve final outputs; used in prior works for retrieval, filtering and some summarization steps.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied generally to large LLMs in cited works (e.g., GPT-family models in Wei et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based internal decomposition (not directly an extractor of paper facts, but helps LLMs structure reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Used to guide LLMs to produce stepwise outputs; in practice prior usages produced outputs lacking deep comparative analysis per this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Technique-level; not quantified by paper for a specific number of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM reasoning tasks including literature review summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Intermediate reasoning steps and final generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typically evaluated by downstream task performance (ROUGE, human judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT improves complex reasoning ability of LLMs, but simple CoT guidance used in prior literature-review pipelines led to summaries with insufficient comparative analysis and poor organizational structure according to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human workflow guided approach (ChatCite) and other direct generation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ChatCite's human-workflow approach outperformed simple CoT-guided summarization in producing comprehensive comparative literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT helps reasoning but is often insufficient alone for structured multi-document comparative summarization; explicit workflow-guided decomposition and iterative candidate selection improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Stochastic and unpredictable outputs; does not inherently impose document-level organizational structure; susceptible to information omission due to context window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4472.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4472.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM generation with retrieved external documents or passages to ground outputs and reduce hallucination; used by LitLLM to improve factuality and recency in literature-review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Retrieve relevant passages/documents (typically via embedding/index retrieval) and provide them as context to an LLM to condition generation, thereby grounding outputs in retrieved evidence and reducing hallucination; in the paper RAG principles are cited as part of LitLLM's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>When reproduced in this paper, LitLLM used GPT-4 as the decoder; RAG can be paired with various LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval and contextual augmentation of LLM prompts (retrieval of relevant paper passages).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Combine retrieved contexts with LLM generation to produce grounded summaries; does not by itself perform multi-step comparative synthesis without additional pipeline design.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies by retrieval configuration; applied over reference sets in experiments (per-target counts vary).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review; evaluated on computer science papers in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded summaries / related-work text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, G-Score, human ratings in experiments when used within LitLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RAG-based LitLLM reduced hallucinations and recency issues relative to naive generation, but still performed worse than ChatCite on comparative quality and human preference in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>CoT-only or direct LLM generation; ChatCite's workflow-guided, reflective incremental approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Better factual grounding than CoT-only generation, but did not match ChatCite's strengths in organized comparative synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG is helpful to ground LLM outputs, but needs stronger summarization/synthesis strategies to produce high-quality comparative literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on retrieval quality and index coverage; retrieving many documents increases context management complexity; still requires careful prompt engineering to integrate retrieved evidence effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Litllm: A toolkit for scientific literature review. <em>(Rating: 2)</em></li>
                <li>G-eval: Nlg evaluation using gpt-4 with better human alignment. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Towards automated related work summarization <em>(Rating: 1)</em></li>
                <li>Automatic related work section in scientific article: Research trends and future directions <em>(Rating: 1)</em></li>
                <li>Automatic generation of citation texts in scholarly papers: A pilot study <em>(Rating: 1)</em></li>
                <li>BACO: A background knowledge-and content-based framework for citing sentence generation <em>(Rating: 1)</em></li>
                <li>Capturing relations between scientific papers: An abstractive model for related work section generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4472",
    "paper_id": "paper-268248855",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "brief_description": "An LLM-based agent that mimics a human literature-review workflow: it first extracts structured key elements from each paper, then iteratively composes comparative related-work summaries using a reflective incremental generation and LLM-based voting to select top candidates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ChatCite",
            "system_description": "Two-module agent: (1) Key Element Extractor — prompts an LLM with seven guiding questions concatenated with paper content to extract and store structured key elements for each paper; (2) Reflective Incremental Generator — an iterative breadth-first-search-style generator where a Comparative Summarizer produces multiple candidate continuations per step (using prior summary candidates plus the new paper's key elements), and a Reflective Evaluator (an LLM) votes multiple times to score/rank candidates, retaining the top-n for the next iteration until all reference papers are processed; final output chosen by highest voting score. The system uses LLMs as both generator and in-process evaluator and avoids separate model training by relying on prompting, candidate generation, and voting.",
            "llm_model_used": "Experiments used GPT-3.5 (gpt-3.5-turbo-1106) as the generation/decoder in the ChatCite pipeline and GPT-4 (gpt-4-turbo-preview) as the evaluator for automatic scoring; paper describes the design as LLM-agnostic but these were used in experiments.",
            "extraction_technique": "Structured prompt-based extraction: seven pre-defined guiding questions (Q_e) concatenated with paper content form the extraction prompt P_e; LLM decodes responses into stored key elements per paper.",
            "synthesis_technique": "Iterative comparative incremental generation: Comparative Summarizer composes multiple candidate summaries per iteration by integrating prior candidates and current paper key elements; Reflective Evaluator (LLM) votes nv times to score candidates; top-n_c candidates retained, continuing until all papers processed. Breadth-first-search-style expansion over papers to produce organized comparative summaries.",
            "number_of_papers": "Per example, processes the provided reference-paper set R = {r1...rn} for each target; evaluated on the NuDtRwG-Citation test set of 50 target papers (each with its associated reference set).",
            "domain_or_topic": "Computer science (NLP / computational linguistics papers in the dataset).",
            "output_type": "Comparative literature summaries / related-work sections (structured multi-paper synthesis).",
            "evaluation_metrics": "Automatic: ROUGE-1/2/L (F1), G-Score (LLM-based aggregate metric using GPT-4); Human: expert human ratings along six dimensions (Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy).",
            "performance_results": "Reported ROUGE and LLM-evaluator numbers: ROUGE-1 25.30, ROUGE-2 6.36, ROUGE-L 23.13; G-Score (aggregate LLM-eval, 1-5 scale) reported ~4.06. Paper states ChatCite outperformed direct LLM baselines and the LitLLM baseline on multi-dimensional quality metrics and human preferences, though ROUGE was slightly below GPT-4 zero-shot in some cases.",
            "comparison_baseline": "Direct LLM generation (GPT-3.5 and GPT-4 zero-shot and few-shot) and LitLLM (Agarwal et al., 2024) reproduced with GPT-4 as decoder.",
            "performance_vs_baseline": "ChatCite achieved higher G-Score and better alignment with human preferences than the direct LLM baselines and LitLLM; ROUGE scores were sometimes slightly lower than GPT-4 zero-shot, but ChatCite's multi-dimensional human-aligned quality measures favored it over baselines.",
            "key_findings": "Guiding LLM generation with a human-like workflow (structured key-element extraction + iterative comparative summarization + reflective voting) improves comparative analysis, organizational structure, citation accuracy, and stability of generated literature summaries compared to simple CoT or direct generation; reflective voting reduces variance and selects higher-quality candidates.",
            "limitations_challenges": "Experiments limited to computer-science papers (dataset of 50 targets); high-cost LLMs (GPT-4) not used for full generation due to cost — GPT-3.5 used as decoder; remaining randomness and instability in outputs; evaluation relies on LLM-based metrics which may themselves carry bias; ChatCite focuses on summarization stage and does not handle literature retrieval/filtering.",
            "scaling_behavior": "Algorithm maintains a fixed number (n_c) of top candidates per step and expands over the number of reference papers (tree depth = number of papers); paper does not present large-scale (&gt;50-target) scaling experiments — increasing papers increases iterations and LLM calls and may raise compute cost without further empirical scaling claims.",
            "uuid": "e4472.0",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Key Element Extractor",
            "name_full": "Key Element Extractor (ChatCite module)",
            "brief_description": "A structured prompt-based extraction module that asks seven guiding questions per paper and uses an LLM to extract and store essential elements (e.g., contributions, methods, datasets) to prevent omission during summarization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Key Element Extractor",
            "system_description": "Creates a prompt P_e = [Q_e, C] where Q_e is a set of seven simple guiding questions and C is the target paper content; feeds P_e to an LLM acting as an extraction decoder to produce structured key elements which are stored in memory for later use by the Comparative Summarizer.",
            "llm_model_used": "In the ChatCite experiments GPT-3.5 (gpt-3.5-turbo-1106) was used for generation/extraction.",
            "extraction_technique": "Structured prompting with an explicit schema (seven targeted questions) to elicit specific key elements from each paper.",
            "synthesis_technique": "Not a synthesis module itself; supplies structured inputs (key elements) for downstream iterative synthesis.",
            "number_of_papers": "Applied per-paper for each reference and the proposed work; used across the reference sets in the dataset (50 target instances overall).",
            "domain_or_topic": "Computer science literature (as used in the experiments).",
            "output_type": "Structured key-element records per paper (to be consumed by the synthesis module).",
            "evaluation_metrics": "Indirectly evaluated via ablation: downstream ROUGE, G-Score, and human evaluations comparing ChatCite with and without the extractor.",
            "performance_results": "Ablation showed ChatCite without the Key Element Extractor performed worse across ROUGE and LLM-based metrics; the extractor improved content consistency in human studies (exact numeric deltas not fully enumerated in paper).",
            "comparison_baseline": "ChatCite without the Key Element Extractor (i.e., direct summarization prompt).",
            "performance_vs_baseline": "Inclusion of the extractor improved ROUGE and LLM-evaluation metrics and human-judged content consistency relative to the no-extractor ablation.",
            "key_findings": "Explicitly extracting and retaining structured elements reduces information omission and yields more consistent content in generated literature summaries.",
            "limitations_challenges": "Performance depends on the quality of the guiding questions and LLM extraction accuracy; context window limits may still restrict extraction for very long papers; no automated verification of extraction fidelity beyond downstream metrics.",
            "scaling_behavior": "Operates per paper; scaling involves repeated LLM extraction calls linear in the number of reference papers — no empirical large-scale scaling reported.",
            "uuid": "e4472.1",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Reflective Incremental Generator",
            "name_full": "Reflective Incremental Generator (ChatCite module)",
            "brief_description": "An iterative multi-candidate generation mechanism that composes comparative summaries incrementally, using an LLM-based reflective evaluator to vote and prune candidates at each step to improve stability and comparative analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reflective Incremental Generator",
            "system_description": "At each turn i the Comparative Summarizer G constructs n_s summary candidates by combining the proposed-work key elements, the i-th reference paper's key elements, and each candidate s in S_{i-1}. The Reflective Evaluator E (an LLM) votes n_v times over the expanded candidate set S'_i, producing scores E_i; the system sorts and retains the top n_c candidates for the next iteration. The process repeats until all reference papers have been incorporated, effectively maintaining a beam of promising multi-paper summaries and using LLM-based voting to stabilize selection.",
            "llm_model_used": "Generation experiments used GPT-3.5 (gpt-3.5-turbo-1106) as the generator; the design uses LLMs in-process for reflective evaluation (the paper also uses GPT-4 as an external evaluator in evaluations).",
            "extraction_technique": "Relies on pre-extracted key elements from the Key Element Extractor; no additional extraction technique beyond using those structured elements.",
            "synthesis_technique": "Iterative candidate generation + LLM voting/pruning (breadth-first-search-like beam expansion) to synthesize multi-document comparative summaries.",
            "number_of_papers": "Iterates over the reference set; experiments conducted on dataset with 50 target papers; per-target number of references varies (n).",
            "domain_or_topic": "Computer science literature (NLP papers in dataset).",
            "output_type": "Comparative literature summaries / related-work drafts.",
            "evaluation_metrics": "ROUGE, G-Score (LLM evaluator), human ratings; ablation study comparing with/without the reflective mechanism.",
            "performance_results": "Inclusion of the reflective mechanism produced more stable results with slightly higher overall scores; boxplot analysis showed reduced variance across evaluation dimensions. Exact numeric improvements not exhaustively enumerated but reported as meaningful for stability and quality.",
            "comparison_baseline": "ChatCite without Reflective Mechanism (i.e., no LLM voting/pruning), and standard few-shot CoT baselines.",
            "performance_vs_baseline": "Reflective mechanism improved stability and selected higher-quality candidate summaries versus the no-reflection ablation; overall ChatCite (with reflection) outperformed baselines.",
            "key_findings": "LLM-based reflective voting (multiple evaluations per candidate) helps filter unstable outputs and produces more consistent, higher-quality summaries; treating generation as a candidate search with LLM-based scoring is effective for multi-document synthesis.",
            "limitations_challenges": "Higher computational cost due to multiple candidate generations and multiple LLM evaluation calls per iteration; choice of hyperparameters (n_v, n_s, n_c) impacts cost and outcomes; still exhibits some randomness and requires further work to improve stability/scalability.",
            "scaling_behavior": "Design maintains a fixed beam size (n_c) while depth equals number of reference papers; without careful pruning or parameter tuning, candidate expansion could become expensive as number of papers grows; no empirical large-scale scaling results reported.",
            "uuid": "e4472.2",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM: A toolkit for scientific literature review",
            "brief_description": "A toolkit that applies Retrieval-Augmented Generation (RAG) principles, specialized prompting, and instructive techniques to generate literature reviews while addressing hallucination and recency issues; used as a comparative baseline in the paper.",
            "citation_title": "Litllm: A toolkit for scientific literature review.",
            "mention_or_use": "use",
            "system_name": "LitLLM",
            "system_description": "Toolkit that combines retrieval (to fetch relevant papers or passages) with instructive prompting of LLM decoders (RAG-style) and tailored prompts to reduce hallucination and incorporate recent research; in this paper the authors reproduced LitLLM with GPT-4 used as the decoder to form a strong baseline.",
            "llm_model_used": "Reproduced baseline used GPT-4 as decoder in the experiments reported in this paper; original LitLLM is designed to be used with large LLM decoders (unspecified here).",
            "extraction_technique": "Retrieval-based (RAG) — embedding or index-based retrieval to gather grounding context for generation; specialized prompting for retrieval and filtering.",
            "synthesis_technique": "Retrieval-augmented generation: combine retrieved contexts with LLM generation; uses simpler Chain-of-Thought guidance for certain steps in the original work.",
            "number_of_papers": "Applied to the reference sets for each target in the same dataset (50 target tasks); exact per-target counts vary and are determined by the dataset.",
            "domain_or_topic": "Scientific literature review (general), evaluated on computer science papers in the experiments.",
            "output_type": "Literature summaries / related-work sections.",
            "evaluation_metrics": "ROUGE, G-Score (LLM-based evaluator), human evaluation.",
            "performance_results": "Reported to produce ROUGE scores comparable to GPT-4 zero-shot in some metrics (paper table indicates LitLLM ROUGE values), but exhibited lower G-Score and human preference than ChatCite; exact numeric comparisons in paper show ChatCite superior on multi-dimensional quality despite some ROUGE differences.",
            "comparison_baseline": "Compared against direct LLM baselines (GPT-3.5/GPT-4 zero/few-shot) and ChatCite.",
            "performance_vs_baseline": "LitLLM performed similarly to GPT-4 zero-shot on ROUGE, but was significantly worse than ChatCite on G-Score and human-preference metrics according to the reported experiments.",
            "key_findings": "RAG plus specialized prompting reduces hallucination and helps incorporate recency, but simple CoT guidance and lack of a structured multi-step synthesis pipeline limit LitLLM's ability to produce in-depth comparative and organizational analyses compared to ChatCite.",
            "limitations_challenges": "Dependence on retrieval quality; simple CoT guidance used for summarization can produce disorganized or non-comparative outputs; still susceptible to hallucination if retrieval is incomplete.",
            "scaling_behavior": "Not explicitly analyzed in this paper; LitLLM's retrieval-based design suggests scaling depends on retrieval/indexing infrastructure and retrieval quality as number of papers grows.",
            "uuid": "e4472.3",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "G-Eval",
            "name_full": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "brief_description": "An LLM-based evaluation framework that uses GPT-4 and chain-of-thought / structured prompts to score NLG outputs across dimensions in a way that aligns better with human judgments than traditional metrics.",
            "citation_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment.",
            "mention_or_use": "mention",
            "system_name": "G-Eval",
            "system_description": "Uses GPT-4 with chain-of-thought and a form-filling evaluation paradigm to assess NLG outputs across multiple criteria, producing scores that correlate well with human judgments; inspired the G-Score metric devised in this paper.",
            "llm_model_used": "GPT-4 (as used in the referenced G-Eval work).",
            "extraction_technique": "LLM-based scoring via structured prompts and CoT-style reasoning to justify scoring decisions.",
            "synthesis_technique": "Aggregates per-dimension judgments into overall evaluation scores; can include voting or multi-run persistence.",
            "number_of_papers": "Not an information-extraction system over papers; used as an evaluator across NLG outputs.",
            "domain_or_topic": "General NLG evaluation (applied across benchmarks in prior work).",
            "output_type": "Evaluation scores and human-aligned rankings.",
            "evaluation_metrics": "Compared to ROUGE and other automatic evaluators; reported correlation with human judgments on SummEval and Topical-Chat.",
            "performance_results": "Reported to outperform prior automatic evaluators on some benchmarks and align more closely with human evaluations (per the cited G-Eval paper).",
            "comparison_baseline": "Traditional automatic metrics (ROUGE, BLEU, etc.) and prior automatic evaluators.",
            "performance_vs_baseline": "Outperforms ROUGE-like metrics in alignment with humans on cited benchmarks, but the method can be sensitive to prompt/instruction and may favor LLM-generated text.",
            "key_findings": "LLM-based evaluators with CoT improve human alignment for NLG evaluation, providing more nuanced multi-dimensional scoring.",
            "limitations_challenges": "Potential bias towards LLM-generated outputs; sensitivity to instruction and prompt phrasing; not necessarily immune to evaluator hallucination.",
            "scaling_behavior": "Evaluation cost scales with number of outputs and complexity of multi-run CoT evaluations; no large-scale extractor-to-synthesis scaling claims in this paper.",
            "uuid": "e4472.4",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "G-Score",
            "name_full": "G-Score (LLM-based automatic evaluation metric, this paper)",
            "brief_description": "A six-dimensional LLM-based automatic evaluation metric introduced in this paper that scores generated literature summaries on Consistency, Coherence, Comparative, Integrity, Fluency, and Cite Accuracy using an LLM (GPT-4) and aggregates into a total score aligned with human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "G-Score",
            "system_description": "Evaluation pipeline using an LLM to simultaneously score multiple model-generated summaries in one conversation across six literature-review-specific dimensions (each 1-5). The system also uses LLM-based voting to choose best summaries and produces model-preference statistics; it was inspired by G-Eval and adapted to literature-summary evaluation.",
            "llm_model_used": "GPT-4 (gpt-4-turbo-preview) used as the automatic evaluator in experiments.",
            "extraction_technique": "LLM-based scoring via guided prompts (no information extraction from source papers — it scores generated summaries relative to gold summaries and criteria).",
            "synthesis_technique": "Aggregates multi-dimensional scores into an overall G-Score and derives model preference metrics by voting.",
            "number_of_papers": "Applied to evaluate generated summaries for the full test set (50 target papers) in experiments.",
            "domain_or_topic": "Literature summary generation (computer-science papers in the dataset).",
            "output_type": "Per-dimension and aggregated quality scores; model preference rankings.",
            "evaluation_metrics": "Six-dimension 1-5 scores (Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy); aggregated G-Score; compared to ROUGE and human evaluation.",
            "performance_results": "G-Score evaluations were reported to align with human evaluations and indicated ChatCite's superiority vs baselines; paper states G-Score is more consistent with human preferences than ROUGE.",
            "comparison_baseline": "ROUGE metrics and human expert ratings.",
            "performance_vs_baseline": "G-Score provided rankings and judgments more consistent with human evaluators than ROUGE; used to demonstrate ChatCite's advantage in multiple quality dimensions.",
            "key_findings": "A domain-specific multi-dimensional LLM-based evaluator (G-Score) better captures literature-summary quality and aligns with human preference compared to lexicon overlap metrics such as ROUGE.",
            "limitations_challenges": "Relies on LLMs as evaluators (may inherit evaluator biases); the automatic evaluation accuracy can be improved further per the paper; potential sensitivity to prompt design and the choice of LLM used for scoring.",
            "scaling_behavior": "Evaluation cost scales with number of summaries and number of candidate outputs jointly evaluated; applied on 50 examples in the paper but no broad scaling analysis presented.",
            "uuid": "e4472.5",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve performance on complex reasoning tasks; prior literature-review automation work used simple CoT guidance but found it insufficient for comparative, organized literature summaries.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Thought prompting",
            "system_description": "Provides LLMs with prompts that encourage step-by-step reasoning or self-decomposed plans (CoT) so that the model exposes intermediate rationales that can improve final outputs; used in prior works for retrieval, filtering and some summarization steps.",
            "llm_model_used": "Applied generally to large LLMs in cited works (e.g., GPT-family models in Wei et al., 2023).",
            "extraction_technique": "Prompt-based internal decomposition (not directly an extractor of paper facts, but helps LLMs structure reasoning).",
            "synthesis_technique": "Used to guide LLMs to produce stepwise outputs; in practice prior usages produced outputs lacking deep comparative analysis per this paper.",
            "number_of_papers": "Technique-level; not quantified by paper for a specific number of papers.",
            "domain_or_topic": "General LLM reasoning tasks including literature review summarization.",
            "output_type": "Intermediate reasoning steps and final generated text.",
            "evaluation_metrics": "Typically evaluated by downstream task performance (ROUGE, human judgments).",
            "performance_results": "CoT improves complex reasoning ability of LLMs, but simple CoT guidance used in prior literature-review pipelines led to summaries with insufficient comparative analysis and poor organizational structure according to this paper.",
            "comparison_baseline": "Human workflow guided approach (ChatCite) and other direct generation approaches.",
            "performance_vs_baseline": "ChatCite's human-workflow approach outperformed simple CoT-guided summarization in producing comprehensive comparative literature summaries.",
            "key_findings": "CoT helps reasoning but is often insufficient alone for structured multi-document comparative summarization; explicit workflow-guided decomposition and iterative candidate selection improves results.",
            "limitations_challenges": "Stochastic and unpredictable outputs; does not inherently impose document-level organizational structure; susceptible to information omission due to context window limits.",
            "uuid": "e4472.6",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A method that augments LLM generation with retrieved external documents or passages to ground outputs and reduce hallucination; used by LitLLM to improve factuality and recency in literature-review generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation",
            "system_description": "Retrieve relevant passages/documents (typically via embedding/index retrieval) and provide them as context to an LLM to condition generation, thereby grounding outputs in retrieved evidence and reducing hallucination; in the paper RAG principles are cited as part of LitLLM's approach.",
            "llm_model_used": "When reproduced in this paper, LitLLM used GPT-4 as the decoder; RAG can be paired with various LLMs.",
            "extraction_technique": "Embedding-based retrieval and contextual augmentation of LLM prompts (retrieval of relevant paper passages).",
            "synthesis_technique": "Combine retrieved contexts with LLM generation to produce grounded summaries; does not by itself perform multi-step comparative synthesis without additional pipeline design.",
            "number_of_papers": "Varies by retrieval configuration; applied over reference sets in experiments (per-target counts vary).",
            "domain_or_topic": "Scientific literature review; evaluated on computer science papers in this work.",
            "output_type": "Grounded summaries / related-work text.",
            "evaluation_metrics": "ROUGE, G-Score, human ratings in experiments when used within LitLLM.",
            "performance_results": "RAG-based LitLLM reduced hallucinations and recency issues relative to naive generation, but still performed worse than ChatCite on comparative quality and human preference in these experiments.",
            "comparison_baseline": "CoT-only or direct LLM generation; ChatCite's workflow-guided, reflective incremental approach.",
            "performance_vs_baseline": "Better factual grounding than CoT-only generation, but did not match ChatCite's strengths in organized comparative synthesis.",
            "key_findings": "RAG is helpful to ground LLM outputs, but needs stronger summarization/synthesis strategies to produce high-quality comparative literature reviews.",
            "limitations_challenges": "Depends on retrieval quality and index coverage; retrieving many documents increases context management complexity; still requires careful prompt engineering to integrate retrieved evidence effectively.",
            "uuid": "e4472.7",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Litllm: A toolkit for scientific literature review.",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment.",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Towards automated related work summarization",
            "rating": 1,
            "sanitized_title": "towards_automated_related_work_summarization"
        },
        {
            "paper_title": "Automatic related work section in scientific article: Research trends and future directions",
            "rating": 1,
            "sanitized_title": "automatic_related_work_section_in_scientific_article_research_trends_and_future_directions"
        },
        {
            "paper_title": "Automatic generation of citation texts in scholarly papers: A pilot study",
            "rating": 1,
            "sanitized_title": "automatic_generation_of_citation_texts_in_scholarly_papers_a_pilot_study"
        },
        {
            "paper_title": "BACO: A background knowledge-and content-based framework for citing sentence generation",
            "rating": 1,
            "sanitized_title": "baco_a_background_knowledgeand_contentbased_framework_for_citing_sentence_generation"
        },
        {
            "paper_title": "Capturing relations between scientific papers: An abstractive model for related work section generation",
            "rating": 1,
            "sanitized_title": "capturing_relations_between_scientific_papers_an_abstractive_model_for_related_work_section_generation"
        }
    ],
    "cost": 0.02807075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary
5 Mar 2024</p>
<p>Yutong Li 
Tsinghua University
BeijingChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Aiwei Liu 
Tsinghua University
BeijingChina</p>
<p>Kai Yu 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Lijie Wen 
Tsinghua University
BeijingChina</p>
<p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary
5 Mar 2024F78380900CFD74D904ED56562D982163arXiv:2403.02574v1[cs.IR]
The literature review is an indispensable step in the research process.It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works.However, literature summary is challenging and time consuming.The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization.However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary.In this work, we firstly focus on the independent literature summarization step and introduce ChatCite 1 , an LLM agent with human workflow guidance for comparative literature summary.This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism.In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria.The ChatCite agent outperformed other models in various dimensions in the experiments.The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.</p>
<p>Introduction</p>
<p>As the rapid advancement of academic research, scholars must delve into existing literature to understand past studies, recognize future research trends, and find innovative approaches in their fields.Crafting a literature review entails searching for relevant literature and conducting detailed comparative summarization.It typically involves two main steps: literature collection followed by literature summary generation based on the collected sources.However, organizing a high-quality literature review 1 Our code will be released after the review process.</p>
<p>Reference papers set</p>
<p>Proposed work description</p>
<p>Generatedliterature summary</p>
<p>Figure 1: Literature Summary Task Description necessitates scholars to engage in thorough analysis, organization, comparison, and integration of an extensive of related works, which is often a challenging and time-consuming task.</p>
<p>Therefore, Hoang and Kan (2010) have proposed the automatic generation of literature summary.However, machine-generated literature summaries often encounter challenges like information omission, lack of linguistic fluency, and insufficient comparative analysis.In traditional models, summaries generated through extraction and abstraction approach may miss key information due to the limitations of the model, leading to the lack of crucial points or findings of the generated summaries.Some automated systems may lack the ability for in-depth comparative analysis, potentially resulting in literature summaries that lack a comprehensive understanding of the relevant research in the field.</p>
<p>In recent years, with the rapid development of large language models (LLMs) (Radford et al., 2019;Brown et al., 2020), their powerful capabilities in natural language generation tasks have been demonstrated across various tasks, that provides possibilities for handling longer texts and generating comprehensive summaries.Researchers have started exploring how to leverage LLMs to generate automatic literature summaries.Wei et al. (2023) propose a Chain-of-Thought (CoT) prompting method to enhance the ability of large language models to perform complex reasoning.CoT allows LLMs to devise their own plan, resulting in generated text that aligns more closely with human preferences.Recent study by (Huang and Tan, 2023) and Agarwal et al. (2024) on literature review has focused more on how to retrieve relevant papers more accurately and neglected research on literature summarization.They use only simple CoT guidance to generate literature summaries, resulting in a lack of comparative and organizational analysis.Large language models, despite their fluent language generation, struggle to consistently produce comparative literature summaries due to their unpredictable an stochastic nature.The length limitations of these models require a two-step summarization approach, increasing the risk of information omission during abstract generation.</p>
<p>In this work, we focus on the independent literature summarization task, aiming to generate a comprehensive comparative literature summary through a certain collection of literature and a description of the proposed work, as illustrated in Figure 1.To address these challenges mentioned above, our work proposes ChatCite, a LLM-based agent guided by human workflow.Different from simple CoT prompting approach, the agent is designed with the human workflow guidance, rather than formulating the generation process in a blackbox manner, ensuring a more stable generation of higher-quality generic summaries.</p>
<p>Furthermore, quality assessment for generative tasks has always been a challenge.Prior studies on literature summarization have primarily relied on text summarization metrics, such as ROUGE (Lin (2004a)).However, traditional text summary evaluation metrics, like ROUGE, are not sufficient to assess the quality of literature summaries.More comprehensive evaluation criteria covering multiple dimensions are required to ensure that the generated literature summaries truly meet the requirements.Therefore, we combine human studies on literature reviews (Justitia and Wang, 2022) to formulate the evaluation criteria for literature summaries from multiple dimensions2 , and propose an LLM-based automatic evaluation metric, G-Score.Experimental results demonstrate its consistency with human evaluations.</p>
<p>In this paper, we summarize our main contributions of our framework as follows:</p>
<p>• we focus on the independent literature summarization step of literature review, and in-troduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary.</p>
<p>• Based on research on literature summaries, we have developed a multidimensional quality assessment criterion for literature summaries.Additionally, we propose an LLM-based automatic evaluation metric, G-Score, demonstrating results consistent with human preferences.</p>
<p>• The experimental results indicate that</p>
<p>ChatCite outperforms other LLM-based literature summarization methods in all quality dimensions.The literature summaries produced by ChatCite can be directly utilized for drafting literature reviews.</p>
<p>• We demonstrate that LLMs with human workflow guidance, have the ability to effectively perform comprehensive comparative summarization of multiple documents.Therefore, we infer that Large Language Models (LLMs) have the potential to handle more complex inferential summarization tasks.</p>
<p>2 Related Work3</p>
<p>In recent years, there is abundant research on generated literature summaries with the initial proposal made by Hoang and Kan (2010), to automate related work summarization created by a topicrelated work summary based on an extractive approach.To generate citation sentence, Xing et al.</p>
<p>(2020) adopted a multi-source pointer-generator network with cross-attention mechanism, while AbuRa'ed et al. (2020)  Large Language Models (LLMs), such as GPT (Radford et al. (2019), Brown et al. (2020)), have demonstrated their powerful capabilities in natural language generation tasks.The study by Huang and Tan (2023) on the use of AI tools like Chat-GPT in writing scientific review articles reveals the potential benefits and drawbacks of artificial intelligence in academic writing.Building on these insights, Agarwal et al. (2024) introduces the LitLLM toolkit, which overcomes challenges such as generating hallucinated content and overlooking recent research by adopting Retrieval Augmented Generation (RAG) principles, specialized prompting, and instructive techniques.However, these studies only applied a simple Chain of Thought (CoT) to the search and filtering process in literature reviews, resulting in poor readability.By comparison, ChatCite focuses on the independent task of text summarization, aiming to generate higher-quality summaries.</p>
<p>Furthermore, this paper introduced a multidimensional G-Score evaluation metric inspired by the previous attempt to use Large Language Models (LLMs) through chain-of-thought methods to evaluate the quality of natural language generation (NLG) systems (Liu et al. (2023), Goyal et al. (2023)) which is more consistent with human evaluation compared to traditional ROUGE metrics (Lin (2004b)).</p>
<p>ChatCite</p>
<p>The literature review task can be decomposed into two sub tasks: relevant papers retrieval and literature summaries generation.This work focuses on the independent task of literature summary generation.Our task is to generate the literature summary based on the proposed work description D and a certain reference papers set R = {r 1 , r 2 , ..., r n }.Given D and R, our agent generates a literature summary Y = f (D,R).</p>
<p>Diverging from other types of summaries, such as news summaries, the literature summary generated directly by large language models using simple Chain-of-Thought (CoT) guidance in existing work mainly faces the following issues: Key Elements missing: Because of the window limitations of LLMs, generating the complete literature review directly is challenging.Typically, a two-step approach is used involving summarization and literature review generation.However, this process can lead to the loss of key elements during summarization.Even if the entire literature summary can be directly generated, using the entire text may result in mistakes in understanding key elements and the loss of such elements.Lack of Comparative Analysis: Comparative analysis is crucial in literature summary, requiring an analysis on the limitations and advantages of existing research methods, and focusing on differences and similarities in methods, experimental design, dataset usage, and more.Directly using CoT-generated results often lacks comparative analysis.</p>
<p>Lack of Organizational Structure: The literature summary generated solely by CoT tends to be discrete for each paper, lacking classification for similar works and an organized structure for the literature review.</p>
<p>To address these challenges, we have proposed an LLM agent for comparative literature summary with human workflow guidance, ChatCite, consisting two modules: the Key Element Extractor and the Reflective Incremental Generator, as illustrated in Figure 2. In this process, we utilize large language models as both generation and evaluation components, eliminating the need for additional model training and improving the quality of generated text to some extent.</p>
<p>The generation process guided by human workflow is as follows:</p>
<p>1.The proposed work description and reference papers in the reference papers set are initially processed using the Key Element Extractor separately.</p>
<ol>
<li>Iteratively generate literature summaries using reference papers set.In each iteration, use the comparative summarizer to generate a comparative analysis summary.Then, use the reflective evaluator to vote on the generated candidate results, ranking the vote score and retaining the top n c results.Iterate continuously until all reference papers are processed.</li>
</ol>
<p>The final output is selected based on the highest voting score among the generated related work summaries.</p>
<p>In this section, we first elaborate on the specifics of the Key Element Extractor ( §3.1) and the Reflective Iterative Generator module ( §3.2) in detail.</p>
<p>Comparative summarizer</p>
<p>Reflective Evaluator</p>
<p>Given the target paper summary and references, Analyze each choice in detail, then conclude in the last line " The best choice is {s}" , wher e s the integer id of the choice.Choices 1,...;Choice2,...;Choice3,...;...;The best choice is 3.  summary generated with the Reflective Incremental Generator.This process is iteratively repeated until a complete related work summary is generated, and the optimal one is selected as the final result.</p>
<p>Reflective Mechanism</p>
<p>Rank &amp; Select</p>
<p>Key Element Extractor</p>
<p>In order to retain sufficient key element for literature summary, we create seven simple guiding questions based on analysis (Justitia and Wang, 2022) on literature review.We concatenate theses questions and the content required extraction as prompt to instruct LLMs extract the key elements.For each element, a simple question (shown in Figure 2) is set to guide the model in extraction, and these questions are Q e = [q 1 , q 2 , ..., q 7 ] .These questions Q e and paper content C are concatenated to form the key element extraction prompt
P e = [Q e , C]
. Using LLM as extraction decoder to extract key elements and storing them in memory.</p>
<p>Reflective incremental Generator</p>
<p>To overcome the challenges of lacking comparative analysis and organizational structure in literature reviews generated by LLMs, we designed the reflective incremental generator.The generator uses the Comparative Summarizer to continue writing comparative summaries, combining the results from the previous turn and the key elements of the proposed work and reference papers.It then utilizes the reflective evaluator to filter the generated re-sults.This process is interatively applied to each reference paper in the reference papers set until all reference papers are processed.The best result is ultimately retained as the model's generated output.</p>
<p>Comparative Summarizer</p>
<p>For turn i, based on the proposed work key element pro, the key element of the i-th reference paper ref i and comparative summarization guidance sequentially generated summary for each summary s ∈ S i−1 , and generating n s samples each time.
S i = {G(D g , pro, ref i , s, n s ), ∀s ∈ S i−1 }
Here, to enhance the comparability and organization of the generated summaries, comparative summarization guidance are provided: "Considering the relationship between the reference paper and the target paper, as well as existing references in the previously completed related work, while retaining the content of all referenced papers mentioned in the previously completed related work."</p>
<p>Reflective Mechanism</p>
<p>Due to significant uncertainty in text generation tasks, we employ reflective generation to enhance the quality and stability of generated paragraphs.</p>
<p>Here, we use LLMs as Reflective Evaluator to vote n v times on the generated results in each turn and then perform a statistical analysis on the voting results to obtain voting scores E i = E(D e , S ′ i ).Then we sort the scores, and retain the top n c candidates S i = {S t , t ∈ Sort(E i )(1, n c )} .These selected candidates will be used for the next round of incremental generation.This approach helps identify the most promising results, ensures the quality of the generated text, and enhances generation stability.</p>
<p>Reflective Incremental Generator Algorithm</p>
<p>In implementing reflective incremental generation, we drew inspiration from the breadth-first search algorithm for trees (Algorithm 1).
), s ∈ S i−1 } E i ← E(D e , S ′ i ) S i ← {S t , t ∈ Sort(E i )(1, n c )} end for return S argmax i En(i)
notes: G() corresponds to the Comparative Summarizer function described in §3.2.1, and E() corresponds to the Reflective Envaluation function described in §3.2.2.At each step, a collection containing n c most promising generated results is maintained, where the depth of the tree equals the number of documents in the relevant literature collection, S ′ t contains n c * n s results, while S i−1 and S i each contain n c results.</p>
<p>G-Score: LLM-based automatic Evaluation Metrics</p>
<p>The evaluation of generative tasks has always been challenging.Previous research on literature summarization predominantly depended on text summarization metrics, like ROUGE (Lin (2004a)).</p>
<p>However, conventional text summary evaluation metrics such as ROUGE fall short in gauging the quality of literature summaries.It is crucial to adopt more comprehensive evaluation criteria across various dimensions to guarantee that the generated literature summaries align with the necessary standards.Here, inspired by G-Eval (Liu et al., 2023), we attempted to assess it using LLMs.We established six-dimensional metrics for automatic evaluation based on research on literature summaries (Justitia and Wang, 2022).Evaluation Steps.We used Large Language Models (LLMs) to score the six dimensions of generic quality and voted for the best summary from a series of model-generated summaries.Specially, to ensure fairness and consistency in evaluation, we simultaneously scored and voted for the generated results of multiple models in a single conversation.</p>
<p>Evaluation Criterion: Consistency (1-5): Content consistency between the generated summary and the gold summary.The generated summary must not contain content that conflicts with the gold summary.Coherence(1-5): The quality of language coherence in generated summaries, which should not just be a heap of related information.</p>
<p>Comparative (1-5): Assess the extent to whether the generated summary conducts a comparative analysis on references and proposed work.Whether it provides an integrated summary of similar related works.Integrity (1-5): Assess if the summary covers essential elements: research context, reference paper summaries, past research evaluation, contributions, and innovations.Fluency (1-5): Assess the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.Cite Accuracy(1-5): Assess whether the summary correctly cites reference paper in the format '[Reference i]' when mention the reference paper.</p>
<p>Experiment</p>
<p>We validate the capabilities of our proposed ChatCite agent by verifying the following questions: 1) Is the literature summary generated by ChatCite better than that generated directly by LLMs with CoT and other LLM-based literature review approach?2) Do all the modules in the ChatCite contribute to its effectiveness? 3) What specific impact do the modules in the ChatCite framework have on the quality of generated summary?</p>
<p>In this section, we conducted a series of experiments to address these questions.Firstly, we introduced our experimental setup ( §5.1).We compared the performance of existing large language models (LLMs) in directly generating related work under zero-shot and few-shot settings, as well as the bestperforming LLM-based literature review approach ( §5.2).Additionally, we performed ablation analysis on each module in our agent to verify their respective capabilities ( §5.3).Finally, we conducted a human study for a detailed quality assessment of the generated related work summaries ( §5.4).</p>
<p>Experimental Setup</p>
<p>Dataset.We conducted experiments to validate on a paper dataset NudtRwG-Citation dataset (Wang et al., 2020) designed for related work summarization task.This test set includes 50 academic research papers in the field of Computer Science, each data containing the following components: 1) A target paper requiring related work generation without the related work section.2) A ground truth related work section.3) Reference papers of the target paper (annotated with authors and years).</p>
<p>Each paper is well-received in conferences of computational linguistics and natural language processing, with an average citation number reaching 63.59, which indicates these target papers are widely recognized by the academic community.Models.For the LLMs baseline, we employed the GPT-3.5 model (Ouyang et al. (2022)) with a 16k context window (version gpt-3.5-turbo-1106)and the GPT-4.0model (Achiam et al. (2023)) with a 128K context window (gpt-4-turbo-preview).We evaluated their performance under zero-shot and few-shot settings.For the previously bestperforming LLM-based literature review approach, we use the recently proposed approach LitLLM (Agarwal et al., 2024) as the baseline.We reproduce their ability to generate literature summaries according to the CoT prompt mentioned in their paper.To showcase its best performance, we use GPT-4.0 as the decoder for the LitLLM baseline.For our model, due to the high cost of GPT-4.0,we conducted experiment based on GPT-3.5 (version gpt-3.5-turbo-1106)as the decoder for the experiment.For evaluation, we use GPT-4.0 (gpt-4-turbopreview) as decoder.</p>
<p>Implementation.In zero-shot setting, for GPT-3.5 model, due to the limitation of the context window, a two-step approach is used for generation: 1) summarizing and then generating with the prompt [p s ] ="Summarize the current article, preserving as much information as possible.Content:{content}" for summarization.For generating the related work section, we use the prompt [p g ] = "Generate the related work section based on the given target paper summary and its references summary.Read the Target Paper Content: {Target}.References content: {References}".For GPT-4.0 and LitLLM with GPT-4.0,[p g ] is directly used for summarization.</p>
<p>In the few-shot setting, we add the instruction "Follow the writing style of the example but without including any content from the example.{Exam-ples}" to the zero-shot prompt.Evaluation metrics.We utilize both automatic metrics and human evaluations to assess the generic result.We employed traditional automatic metrics for summarization evaluation -the vocabulary overlap measures ROUGE-1/2/L (F1) (Lin (2004b)), our proposed LLM-based evaluation metrics G-Eval, and human evaluation under the same evaluation criterion.</p>
<p>Main Results</p>
<p>We compared the performance of different baseline models on the paper test set (see Table 1).In traditional summary evaluation metrics, such as ROUGE, GPT-4.0 achieved the best results under zero-shot settings.Although ROUGE scores of ChatCite may be slightly lower than GPT-4.0 with zero-shot, its performance in quality metrics generated by LLMs and the preference of LLMs is far superior to results obtained directly from other LLM baselines.</p>
<p>Surprisingly, GPT-4.0 performed poorly in fewshot settings.It is found that influenced by examples in the few-shot, resulting in irrelevant and erroneous summaries after case study.Notably, LitLLM with GPT-4.0 produced outcomes similar to GPT-4.0 in zero-shot but significantly lower than ChatCite.</p>
<p>Therefore, we conclude that "ChatCite performs best among LLM-based literature summarization methods, and the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method."The results are automatically evaluated using ROUGE-1/2/L (F1) and the GPT-4.0evaluator.G-Score represents the total score assessed by the GPT-4.0evaluator, while G-Prf.indicates the model preferences among the five models.
Model ROUGE Metrics G-Score G-Prf. ROUGE-1 ROUGE-2 ROUGE-L (1-5) (%) GPT-3.5 w/</p>
<p>Ablation Analysis</p>
<p>Our Comparative Incremental Generator, with the results of GPT-3.5 w/few-shot used as the baseline for GPT-3.5.</p>
<p>of ChatCite with and without the Reflective Mechanism.However, the overall results of ChatCite are slightly higher, with minimal distribution outliers, suggesting a more stable generation of results.This affirms that the Reflective Mechanism effectively improves the quality and stability of the text generated in ChatCite.</p>
<p>Overall, through ablation experiments on three components, we have demonstrated that "each part of ChatCite framework contributes to the improvement of the quality and stability of the generated results in literature summaries".</p>
<p>Human Study</p>
<p>To conduct a fine-grained analysis on the quality of summary generated by ChatCite and to understand the specific impact of individual modules on summarization, we conducted a human study.Several researchers in the field of computer science, with experience in academic writing, were enlisted to evaluate 10 selected samples using the same set of criteria and choose the better summary.Figure 4 demonstrates the results of G-score metric align with human preferences.Specifically, the method incorporating Key Element Extractor exhibits higher content consistency.Summaries generated with the Comparative Incremental gener- Additionally, Figure 5 shows the extinct human preference of the ChatCite model over the others.</p>
<p>Conclusion</p>
<p>LLMs are powerful tools in generating literature summaries, however, it poses the challenges of information omission, lack of comparative summaries and organizational deficiencies.In ChatCite, the Key Element Extractor contributes to improving content consistency, and the Comparative Incremental Generator effectively enhances the organizational structure, comparative analysis, and citation accuracy of the generated summary.Additionally, the literature summaries generated by ChatCite can be directly used for drafting literature reviews.Our study also demonstrated that the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method.In the future, we hope that our work will further inspire research on complex inferential writing, enabling the full potential of LLMs in open-ended writing tasks.</p>
<p>Limitations</p>
<p>In this work, we focused mainly on the summarization of specific topics based on the selected literatures instead of the collection and the filtering of the literatures themselves.The datasets primarily consist of research articles in the area of computer science and lack research articles from other fields of study to validate our model.Our experimentation used Chat GPT 3.5 as the tool for validating the quality of the generated content and the functionalities of the various components of the agent.We did not explore any additional spec that can influence the result of the GPT3.5 model nor the possibility of using other models as the validation tool.The evaluation of the generated content poses a great challenge.We evaluated the generated results from multiple dimensions using G-Score as the performance metric, but there is still room for improvements over the accuracy of the automatic evaluation process.The generated results exhibit randomness and instability.While our proposed approach demonstrates the effectiveness of the agent, the results have shown further research potential on improving the stability and quality of the output.</p>
<p>Ethics Statement</p>
<p>The dataset we used consists of research articles sourced only from publicly available papers, eliminating concerns about data origin.We employ large language models as generators used and only used for summarizing people's ideas and literature and never on the innovative writing processes of the academic papers.However, if generated literature summaries are to be incorporated into academic paper writing, a review and editing of the generated results should be conducted.This ensures that academic writing content is free from harmful information and plagiarism issues.</p>
<p>We will make our code publicly available to ensure experiment reproducibility.</p>
<p>A Appendix</p>
<p>A.1 An Example of generated results of all the models mentioned Table 3: An Example of literature summary results generated for Paper: BEL: Bagging for Entity Linking Gold literature Summary Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality.Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus.Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang,2007;Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014;Gao et al., 2014;Sundermeyer et al., 2014;Liu et al., 2014).On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation.A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a "bridge" to generate source-target translation for language pair with few training corpus.Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese.Considering the problem of translating one source language to many target languages, pivot based SMT approaches does work well given a large-scale source language to pivot language bilingual corpus and large-scale pivot language to target languages corpus.However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem.Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation.Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training.Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence.A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. ( 2014), which utilizes a bidirectional recurrent neural network to compress the source sentence information and fits the conditional probability of words in target languages with a recurrent manner.Moreover, soft alignment parameters are considered in this model.As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework.In the natural language processing field, a1724 notable work related with multi-task learning was proposed by Collobert et al. ( 2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used.Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach.Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing.Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks.Researchers have also explored similar approaches (Sennrich et al., 2013;Cui et al., 2013) in statistical machine translation which are often refered as domain adaption.Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks.To the best of our knowledge, this is thefirst trial of end to end machine translation under multi-task learning framework.</p>
<p>ChatCite with GPT-3.5 Related Work</p>
<p>The field of machine translation has witnessed remarkable progress in addressing data scarcity and enhancing translation quality.Early approaches such as the pivot language approach proposed by Wu and Wang (2007)  ] presents a joint model for word segmentation, POS tagging, and dependency parsing for Chinese, demonstrating significant improvements in accuracies of the three tasks over baseline models.The study's positive results in Chinese segmentation accuracies and the effectiveness of the proposed joint model are highly relevant to the advancements in multi-task learning frameworks for machine translation, providing further insights into joint model approaches and their potential for improving translation quality.The 11th reference paper by Rie Kubota Ando and Tong Zhang (2005) focuses on learning predictive structures from multiple tasks and unlabeled data.It attempts to improve the performance of supervised learning algorithms by including unlabeled data and investigates a closely related problem that leads to a novel approach to semi-supervised learning.The paper introduces a general framework for structural learning and proposes algorithms for structural learning, demonstrating the effectiveness of the proposed algorithms in semi-supervised learning.In a similar vein, the proposed multi-task learning framework in the target paper aims to improve the translation quality and address data scarcity by leveraging multiple tasks and large-scale parallel training corpora.By integrating insights from Rie Kubota Ando and Tong Zhang (2005) [Reference 11] with existing literature, the target paper aligns with the broader trend of leveraging multiple tasks and unlabeled data to enhance machine learning models.The contributions of both the target paper and the referenced works collectively enrich the understanding of addressing data scarcity, enhancing translation quality, and advancing the capabilities of machine translation models, laying the foundation for the multi-task learning framework proposed in the target paper.These contributions provide valuable insights into the advancements in machine translation, emphasizing contributions to the research field and predicting potential future directions.</p>
<p>ChatCite without Key Elements Extractor Related Work</p>
<p>The field of machine translation has witnessed significant advancements in overcoming challenges related to limited training resources and enhancing translation quality across various language pairs.Notably, approaches such as the pivot language method, multi-parallel corpora, and the triangulation method have been explored to address scenarios with limited training data [Radford et al. (2019)[Reference 5]].Additionally, the development of neural network joint models (NNJM) [Sutskever, Vinyals, and Le (2014)[Reference 8]] and the integration of bidirectional recurrent neural networks have shown promise in improving translation quality, particularly for language pairs with limited resources.In alignment with these developments, the target paper focuses on the translation of sentences from a source language to multiple target languages using a multi-task learning framework inspired by neural machine translation.This approach not only demonstrates substantial progress in machine translation, especially for languages with limited training resources but also introduces a neural network joint model (NNJM) for statistical machine translation, aligning with the innovative approaches presented in the referenced studies and offering promise for enhancing translation quality for language pairs with limited resources.Moreover, Ïncremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese [Reference 10] proposes a joint model for word segmentation, POS tagging, and dependency parsing in Chinese, showing improvements particularly for out-of-vocabulary words.This study contributes to a comprehensive understanding of the methods used to address challenges in machine translation and natural language processing.Expanding on these foundations, the 11-th reference paper introduces a framework for learning predictive structures from multiple tasks and unlabeled data.This framework presents a general approach for formulating and analyzing the structural learning problem in the context of learning with unlabeled data, demonstrating its relevance to the challenges in machine translation.The proposed algorithm for joint empirical risk minimization is particularly notable, as it addresses the problem of learning predictive structures on hypothesis spaces from multiple learning tasks, aligning with the multi-task learning framework proposed in the target paper.Furthermore, the experiments presented in the 11-th reference show significant performance improvements over supervised methods in various tasks, indicating the effectiveness of the proposed framework in semi-supervised learning settings.In summary, the integrated literature on machine translation and natural language processing contributes valuable insights for the development of machine translation systems, offering innovative approaches to enhance translation quality, particularly for language pairs with limited resources.The introduction of the framework for learning predictive structures from multiple tasks and unlabeled data adds a new dimension to the existing research landscape, emphasizing the potential for semi-supervised learning to address challenges in machine translation and natural language processing.These advancements collectively pave the way for potential future research directions in the field of machine translation.</p>
<p>ChatCite without Reflective Incremental Generator</p>
<p>Related Work</p>
<p>The related work section provides an overview of the literature relevant to the target paper's research topic, which aims to address the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages.The related work encompasses various studies in the field of machine translation, natural language processing, and neural network models.</p>
<p>[1] Hua Wu and Haifeng Wang (2007) proposed a pivot language approach for phrase-based statistical machine translation, which addresses the translation problem for language pairs with scarce resources by using a pivot language and making use of large bilingual corpora without language-dependent resources or deep linguistic processing.The study demonstrated the effectiveness of the pivot language method for translation on language pairs with a small bilingual corpus.</p>
<p>[2] Trevor Cohn and Mirella Lapata ( 2007) introduced the method of triangulation for translation modeling, which translates from a source to a target language via an intermediate third language, to exploit multi-parallel corpora for training and improve the coverage and quality of phrase-based statistical machine translation.The research focused on addressing the issue of poor performance of current phrase-based SMT systems when using small training sets.</p>
<p>[3] Jacob Devlin et al. ( 2014) formulated a neural network joint model (NNJM) for machine translation, along with techniques to overcome the high cost of using NNLM-style models in MT decoding.The study demonstrated significant improvements in machine translation performance using the proposed NNJM and its variations.</p>
<p>[4] Jianfeng Gao et al. ( 2014) introduced the Continuous-space Phrase Translation Model (CPTM) to address the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations.The study demonstrated substantial improvement over the baseline system with a statistically significant margin.</p>
<p>[5] Martin Sundermeyer et al. ( 2014) explored the effectiveness of recurrent neural networks in translation modeling, specifically focusing on word-based and phrase-based approaches, as well as the inclusion of bidirectional architectures.The research demonstrated improvements over strong baselines in translation modeling.</p>
<p>[6] Zhenghua Li et al. ( 2011) proposed a joint optimization approach for Chinese POS tagging and dependency parsing, showcasing significant improvements in parsing accuracy.The study addressed the issue of error propagation in parsing accuracy due to using automatic POS tags instead of gold ones.</p>
<p>[7] Nal Kalchbrenner and Phil Blunsom (2013) introduced Recurrent Continuous Translation Models (RCTMs), demonstrating their translation capabilities and low perplexities with respect to reference translations.The study highlighted the sensitivity of RCTMs to word order, syntax, and meaning despite lacking alignments.</p>
<p>[8] Ilya Sutskever et al. ( 2014) presented a general end-to-end approach to sequence learning using Deep Neural Networks (DNNs) for mapping sequences to sequences, showcasing the outperformance of the proposed LSTM-based approach over a mature SMT system.</p>
<p>[9] Ronan Collobert et al. (2011) proposed a unified neural network architecture and learning algorithm for various natural language processing tasks, demonstrating the effectiveness of leveraging large unlabeled datasets and multitask supervised training to improve performance.</p>
<p>[10] Jun Hatori et al. ( 2012) introduced an incremental joint approach to word segmentation, POS tagging, and dependency parsing for Chinese, showcasing substantial improvements in the accuracies of the three tasks over baseline models.</p>
<p>[11] Rie Kubota Ando and Tong Zhang (2005) proposed a framework for learning predictive structures from multiple tasks and unlabeled data, demonstrating significant performance improvements over supervised methods in various tasks through structural learning.These studies collectively contribute to the field of machine translation, natural language processing, and neural network models, providing insights and advancements that are relevant to the target paper's research on learning a machine translation model for translating sentences from one source language to multiple target languages.The innovations and findings from these studies serve as valuable background and insights for the target research, highlighting the significance of the related work to the target study and potential future research directions in the field.</p>
<p>GPT-3.5 with zero shot Related Work Section: The proposed multi-task learning framework for translating sentences from one source language to multiple target languages using a machine translation model builds upon and is related to several key works in the field of machine translation and natural language processing. 1. Pivot language approach for phrase-based statistical machine translation The article Pivot language approach for phrase-based statistical machine translationproposes a method for phrase-based statistical machine translation using a pivot language to address translation problems for language pairs with scarce resources.This method is related to the challenge of translating between languages with limited parallel data, which is also addressed in the target paper.The pivot language approach demonstrates the effectiveness of utilizing large bilingual corpora from a pivot language to improve translation quality, especially for language pairs with limited training data.2. Machine translation by triangulation: Making effective use of multi-parallel corpora The article discusses the method of triangulation, which involves translating from a source to a target language via an intermediate third language, aiming to improve machine translation systems by exploiting multi-parallel corpora.This work is relevant to the target paper as it addresses the issue of improving translation quality using a wider range of parallel corpora, particularly with small training sets and for translating between low-density language pairs.3. Fast and robust neural network joint models for statistical machine translation The article presents a novel neural network joint model (NNJM) for statistical machine translation, which is related to the use of neural network models in machine translation.The NNJM augments a neural network language model with a source context window and is purely lexicalized, demonstrating significant improvements in BLEU scores.This work is relevant to the target paper's exploration of neural machine translation and the effectiveness of neural network-based models in improving translation quality.</p>
<p>Learning continuous phrase representations for translation modeling</p>
<p>The article introduces a novel phrase translation model, the Continuous-space Phrase Translation Model (CPTM), which addresses the sparsity problem in estimating phrase translation probabilities.This work is related to the target paper's exploration of continuous representations and their use in improving translation accuracy.The CPTM significantly improves the accuracy of a state-of-the-art phrase-based SMT system, demonstrating its effectiveness in improving translation quality.</p>
<p>Translation modeling with bidirectional recurrent neural networks</p>
<p>The article presents two different translation models using recurrent neural networks, demonstrating their capability to improve machine translation performance.This work is related to the target paper in showcasing the effectiveness of recurrent neural networks in improving translation quality, particularly with the use of bidirectional recurrent models.6. Joint optimization for Chinese POS tagging and dependency parsing The article discusses the importance of joint optimization for Chinese POS tagging and dependency parsing, proposing a unique model to optimize both tasks.This work is related to the target paper's exploration of joint learning frameworks and their impact on improving parsing accuracy, demonstrating the potential of joint optimization for improving translation quality.</p>
<p>Recurrent continuous translation models</p>
<p>The article introduces a class of probabilistic continuous translation models called Recurrent Continuous Translation Models (RCTMs), which are purely based on continuous representations for words, phrases, and sentences.This work is related to the target paper's exploration of continuous representations and their potential in estimating translation quality, highlighting the effectiveness of RCTMs in capturing syntactic and semantic information.8. Sequence to sequence learning with neural networks The article discusses the use of Deep Neural Networks (DNNs) for sequence to sequence learning, particularly in the context of machine translation.This work is related to the target paper's exploration of neural network-based models and their effectiveness in improving translation performance, demonstrating the potential of sequence to sequence learning with neural networks for various sequence to sequence problems.9. Incremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese The article proposes a joint model for word segmentation, POS tagging, and dependency parsing in Chinese, demonstrating significant improvements in accuracies for all three tasks over baseline models.This work is related to the target paper's exploration of joint learning frameworks and their impact on improving translation quality, particularly in the context of joint modeling for multiple language translation tasks.10.A framework for learning predictive structures from multiple tasks and unlabeled data The article presents a framework for learning predictive structures from multiple tasks and unlabeled data, addressing the concept of semi-supervised learning and the problem of learning predictive structures on hypothesis spaces from multiple learning tasks.This work is related to the target paper's exploration of multi-task learning frameworks and their effectiveness in improving translation quality, particularly in the context of learning from multiple tasks and unlabeled data.The related work presented here demonstrates the diverse approaches and techniques employed in the field of machine translation and natural language processing, providing valuable insights and methods that contribute to the advancement of multi-task learning frameworks and neural machine translation models for improving translation quality.</p>
<p>GPT-3.5 with few shot Related Work Section: The related work section will include a review of existing literature and studies that are relevant to the topic of multi-task learning for machine translation and related methods for improving translation quality.The section will discuss the following papers: 1. Pivot language approach for phrase-based statistical machine translation -This study proposes a new method for phrase-based statistical machine translation using a pivot language to address translation problems for language pairs with scarce resources.The method demonstrates the use of large bilingual corpora from a pivot language to improve translation quality for language pairs with limited training data.2. Machine translation by triangulation: Making effective use of multi-parallel corpora -The article discusses a method for improving machine translation systems by exploiting multi-parallel corpora.The proposed method, called triangulation, involves translating from a source to a target language via an intermediate third language, allowing the use of a wider range of parallel corpora for training.3. Fast and robust neural network joint models for statistical machine translation -This article discusses the development of a novel neural network joint model (NNJM) for statistical machine translation (SMT).The NNJM augments a neural network language model (NNLM) with a source context window and demonstrates significant improvements in translation quality.4. Learning continuous phrase representations for translation modeling -The study presents a novel phrase translation model for statistical machine translation (SMT) that addresses the sparsity problem in estimating phrase translation probabilities.The model utilizes continuous phrase representations to improve the accuracy of a state-of-the-art phrase-based SMT system. 5. Translation modeling with bidirectional recurrent neural networks -The article introduces two different translation models using recurrent neural networks and demonstrates their effectiveness in improving machine translation performance on various tasks.6. Recurrent continuous translation models -This study introduces a class of probabilistic continuous translation models called Recurrent Continuous Translation Models (RCTMs) and demonstrates their potential in capturing syntactic and semantic information and estimating translation quality.7. Sequence to sequence learning with neural networks -The article discusses the use of Deep Neural Networks (DNNs) for sequence to sequence learning and demonstrates the effectiveness of this approach on an English to French translation task.8. Ïncremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese -The article proposes a joint model for word segmentation, POS tagging, and dependency parsing in Chinese and demonstrates significant improvements in accuracies for all three tasks over baseline models.9. Ä framework for learning predictive structures from multiple tasks and unlabeled data -The study presents a framework for learning predictive structures from multiple tasks and unlabeled data, demonstrating significant performance improvements over supervised methods in various text categorization, named entity chunking, part-of-speech tagging, and hand-written digit image classification tasks.</p>
<p>GPT-4.0 with zero shot</p>
<p>Based on the given information, here is a related work section for the target paper: Related Work The problem of learning from multiple tasks and utilizing unlabeled data has been an area of significant research interest in machine learning and natural language processing (NLP).The concept of semi-supervised learning, which involves the use of both labeled and unlabeled data, has been explored in various studies with the aim of improving the performance of supervised learning algorithms.Vapnik introduced the notion of transductive inference, which is a form of semi-supervised learning, although its effectiveness has been debated (Vapnik, 1998;Zhang and Oles, 2000).Co-training, another semi-supervised approach, was proposed by Blum and Mitchell (1998) and is related to the bootstrap method used in NLP applications (Yarowsky, 1995).However, the effectiveness of co-training can be limited by the assumptions made by the method and the potential introduction of noise into the labels (Pierce and Cardie, 2001).The idea of learning predictive structures from multiple tasks, also known as structural learning or multi-task learning, has been discussed in the literature.Ando and Zhang (2005) presented a framework for structural learning where multiple tasks are used to discover a shared low-dimensional predictive structure.This approach is based on the principle that a good hypothesis space should have a small approximation error and a small estimation error.The authors proposed an algorithm for structural learning that finds a common low-dimensional feature space shared by multiple problems.Their method was shown to be effective in semi-supervised learning settings, improving the performance of various NLP tasks.In the context of NLP, the joint approach to tasks such as word segmentation, POS tagging, and dependency parsing has been explored.Hatori et al. (2012) proposed an incremental joint model for Chinese POS tagging and dependency parsing, demonstrating improvements over baseline models.The work by Kubota Ando and Zhang (2005) extends this idea to a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, capturing global interactions among these tasks.Their model employs an efficient character-based decoding method that combines features from state-of-the-art segmentation, POS tagging, and dependency parsing models.The use of external resources, such as dictionaries, has also been considered in joint models to enhance performance.Wang et al. (2011) used semi-supervised methods with large auto-analyzed data to improve Chinese word segmentation and POS tagging.The incorporation of syntactic information has been shown to improve the segmentation of out-of-vocabulary words, which is a common challenge in languages like Chinese where transliterated words often include characters used as common or function words.Overall, the related work demonstrates the potential of joint and semi-supervised approaches in leveraging multiple tasks and unlabeled data to improve the performance of machine learning models in NLP tasks.The target paper builds upon these concepts by proposing a unified framework that avoids task-specific engineering and learns internal representations from vast amounts of unlabeled data, achieving good performance across various NLP tasks.</p>
<p>GPT-4.0 with few shot</p>
<p>The examples provided discuss the use of Large Language Models (LLMs) as proxies for human behaviors and the systematic study of LLMs' personalities based on established personality trait theories.Here's a summary of the key points from the examples: 1. LLMs Mimicking Human Behaviors: -Recent studies have shown that LLMs can mimic human behaviors, including reasoning and cognitive abilities, as well as participate in social science experiments.</p>
<p>-These studies are mostly empirical and based on case-by-case observations.2. Systematic Study of LLMs' Personalities: -The authors propose a systematic and quantitative approach to study LLMs' behaviors by evaluating and inducing personality traits in LLMs.</p>
<p>-This approach does not require supervised fine-tuning or human evaluation of generated utterances.</p>
<p>Personality Trait Theories:</p>
<p>-The Big Five and Sixteen Personality Factors (16PF) are two prominent theories that provide consistent descriptions of individual differences.</p>
<p>-Psychometric tests based on these theories, such as NEO-PI-R, are effective in assessing personality traits and have shown that human differences can be categorized into continuous factor dimensions. 4. Correlation Between Personality and Behavior: -Empirical studies have confirmed a strong correlation between personality traits and real-world human behaviors.</p>
<p>-Specifically, there is a significant link between the Big Five traits and language use, suggesting that personality influences how people communicate.The examples highlight the potential of LLMs to simulate complex human-like behaviors and the importance of understanding the personality traits of these models for various applications.</p>
<p>LitLLM with GPT-4.0The Unified Neural Network Model for NLP Tasks Natural Language Processing (NLP) tasks such as part-of-speech (POS) tagging, chunking, named entity recognition (NER), and semantic role labeling (SRL) are essential for understanding and generating human language.Traditional approaches to these tasks often involve task-specific feature engineering, which can be labor-intensive and may not generalize well to new tasks or datasets.To address these issues, researchers have proposed unified neural network models that can learn to perform multiple NLP tasks simultaneously without extensive feature engineering.These models leverage large amounts of both labeled and unlabeled data to learn useful representations for words, phrases, and sentences that capture syntactic and semantic information.One such model is the Long Short-Term Memory (LSTM) network, which has been shown to be effective for sequence-to-sequence learning tasks like machine translation.LSTMs can process input sequences of variable lengths and learn long-range dependencies, making them suitable for NLP tasks.They can be trained end-to-end with backpropagation and can learn from both supervised and unsupervised data.Another approach is the use of semi-supervised learning methods, which combine labeled and unlabeled data to improve model performance.For example, structural learning algorithms can learn predictive structures from multiple tasks and use them to enhance the performance of individual tasks.This can be particularly useful when there is a large amount of unlabeled data available.The success of these unified neural network models demonstrates the potential of using deep learning techniques for NLP tasks.By avoiding task-specific engineering and leveraging large datasets, these models can achieve state-of-the-art performance on a variety of benchmarks, making them valuable tools for researchers and practitioners in the field of NLP."</p>
<p>A.2 Related work draft for this paper generated by ChatCite with GPT-4.0Table 4: We show the related work summary using this paper and our collection of related work papers generated by the ChatCite model based on .It can be observed that the citations in the paper are consistent with those in the collection, the content is accurate at the same time.We utilized this summary as a draft and, completed our related work section after some minor modifications.</p>
<p>Generated literature summary for this paper In the evolving landscape of automated scholarly writing, particularly in the domain of literature review summarization, the proposed ChatCite framework within our target paper represents a significant advancement, aiming to harness the capabilities of Large Language Models (LLMs) to generate high-quality related work summaries.The foundational work in this field began with hierarchical keyword structures for extractive summarization [Reference 1], and has progressed through various iterations including multi-source pointer-generator networks [Reference 2], and sophisticated models like Recent studies have concentrated on the unsupervised and few-shot learning capabilities of LLMs, notably GPT-3, establishing a solid groundwork for AI application in scholarly communication .The exploration of AI tools such as ChatGPT by Huang, Jingshan, and Ming Tan (2023) [Reference 10] in crafting scientific review articles, sheds light on both the potential benefits and drawbacks of AI in academic writing, such as enhancement in speed and quality versus risks of inaccuracies and bias.Building on these insights, the LitLLM toolkit suggested by Shubham Agarwal, Issam H. Laradji et al. (2024) [Reference 11] brings novel approaches to overcome challenges like the generation of hallucinated content and the overlooking of recent studies by employing Retrieval Augmented Generation (RAG) principles, specialized prompting, and instructive techniques.This showcases a significant alignment with our target paper's objectives, particularly in mitigating hallucinations and enhancing summarization accuracy through the ChatCite framework.The latest contribution in this domain is presented by Yang Liu, Dan Iter, et al. (2023) [Reference 12] through the G-EVAL framework, which introduces a methodology for evaluating the quality of texts generated by natural language generation (NLG) systems using LLMs, leveraging chain-of-thoughts (CoT) and a form-filling paradigm.This approach, which significantly outperforms previous methods in evaluating the creativity and diversity required in text generation tasks, aligns closely with the goals of the ChatCite framework, especially in its pursuit of generating high-quality literature summaries.The G-EVAL framework not only surpasses all previous state-of-the-art evaluators on the SummEval and Topical-Chat benchmarks but also underscores the importance of instruction sensitivity and the benefits of a CoT approach, which could potentially inform future enhancements in the ChatCite framework [Reference 12].By evaluating the quality of NLG outputs, [Reference 12] addresses the limitations identified in prior works, including the potential bias of LLM-based evaluators towards LLM-generated texts, and calls for further research to understand the behavior of LLM-based evaluators fully.This contribution is particularly relevant to our target paper, which aims to enhance the quality and stability of literature review summarizations generated by LLMs.Thus, integrating insights from G-EVAL [Reference 12] can provide a more nuanced understanding of evaluating the effectiveness of automated scholarly writing tools, highlighting a potential area for future development within the scope of the ChatCite framework.In summary, the development of automated scholarly writing, particularly in literature review summarization, has progressed through key innovations in LLM capabilities, methodological enhancements, and now, through improved evaluation frameworks.This journey from extractive summarization to sophisticated LLM-based frameworks underscores a collaborative advancement in the field, with each contribution building upon the last to address emerging challenges and achieve greater accuracy, efficiency, and relevance in automated scholarly writing.The ChatCite framework, supported by insights from references [References 1-12], continues this trajectory by proposing innovative solutions to the challenges in literature review summarization, setting a platform for further exploration and innovation in the field.</p>
<p>Figure 2 :
2
Figure2: The ChatCite consists of two modules, the Key Element Extractor and the Reflective Incremental Generator.The agent mimicking human workflow generates literature summary utilizing the Key Element Extractor to process the proposed work description and reference paper in Reference Papers Set.It then iteratively generates literature summaries using each paper in the Reference Papers Set, proposed work key elements and previous summary generated with the Reflective Incremental Generator.This process is iteratively repeated until a complete related work summary is generated, and the optimal one is selected as the final result.</p>
<p>Figure 4 :
4
Figure 4: Human Evaluation vs. G-Score on six dimensions of the generic summary quality.The scoring results of the G-Score model is aligned with the distribution of human evaluations.</p>
<p>[Reference 1], and the triangulation method introduced by Cohn and Lapata (2007) [Reference 2], focused on improving phrase-based statistical machine translation, particularly for language pairs with scarce resources.Additionally, Devlin et al. (2014) [Reference 3] contributed by introducing a neural network joint model (NNJM) for machine translation, offering valuable insights into the formulation of a novel neural network joint model and techniques for enhancing machine translation performance.Moreover, Gao et al. (2014) [Reference 4] enriched the understanding of addressing data scarcity and improving translation quality by introducing a Continuous-space Phrase Translation Model (CPTM) to project source and target phrases into continuous-valued vector representations, complementing the approaches of previous researchers.Furthermore, Sundermeyer et al. (2014) [Reference 5] expanded the scope of machine translation by investigating the effectiveness of recurrent neural networks in translation modeling, contributing to the broader landscape of machine translation research.In the context of the target paper, which proposed a multi-task learning framework for machine translation, the works by Zhenghua Li et al. (2011) [Reference 6] and Nal Kalchbrenner and Phil Blunsom (2013) [Reference 7] proved highly relevant.Zhenghua Li et al. (2011) proposed joint optimization for Chinese POS tagging and dependency parsing, offering valuable insights into joint optimization techniques.Similarly, Nal Kalchbrenner and Phil Blunsom (2013) [Reference 7] introduced recurrent continuous translation models (RCTMs), providing a new perspective in the domain of machine translation and contributing to the advancement of purely continuous sentence-level translation models.The work by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le (2014) [Reference 8] on sequence to sequence learning with neural networks is also highly relevant to the target paper's objectives.The paper addresses the challenge of mapping sequences to sequences using Deep Neural Networks (DNNs) and proposes a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, aligning with the multi-task learning framework proposed in the target paper.Additionally, the 9th reference paper by Ronan Collobert et al. (2011) [Reference 9] addresses the effectiveness of leveraging large unlabeled datasets and multitask supervised training to improve performance, aligning with the multi-task learning framework proposed in the target paper.The recently explored Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese by Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun (2012) [Reference 10</p>
<p>the ARWG system [Reference 3], BACO [Reference 4], and the Relation-aware Related work Generator (RRG) [Reference 5].The introduction of contrastive learning to improve summarization quality [Reference 6], and automatic citation sentence generation methods [Reference 7], have further refined the capabilities of LLMs in this space.</p>
<p>Turn One Turn Two Turn Three Trun Three Reflective Incremental Generator
푟� 1푟� 2푟� 2Proposedwork discripition푟� 3푟� 3푟� 3푟� 3푟� 4푟� 4푟� 4푟� 4푟� 4푟� 4Choice 1Choice 2Choice 3Choice 4Choice 5Choice 6.........ReferencePapersSetChoice 3Choice 4Choice 1Choice 6Choice 2Choice 5</p>
<p>Table 1 :
1
Main Results:
zero shot26.016.1124.023.41022.21GPT-3.5 w/few shot25.846.0123.553.596810.80GPT-4 w/zero shot30.028.0327.973.507626.40GPT-4 w/few shot15.521.7814.201.66210.21LitLLM w/GPT-427.086.0724.943.544824.51ChatCite25.306.3623.134.064235.86</p>
<p>Table 2 :
2
proposed framework can be decomposed into two components: the Key Element Extractor and the Reflective Incremental Generator.The Reflective Incremental Generator comprises two key points: the Comparative Incremental Generation and the Reflective Mechanism.Therefore, we will analyze the three part separately.Key Element Extractor.To validate the effectiveness of the Key Element Extractor, we chose ChatCite without the Key Element Extractor as a comparison.The ChatCite without Key Element Extractor used the baseline summary prompt [p s ] to directly summarize the article and then use Reflective Incremental Generator generate the literature summary.In Table2, comparing the results of ChatCite without Key Element Extractor and ChatCite, we can observe that ChatCite performs better in all dimensions of ROUGE metrics and the metrics generated by the LLM based evaluator.Therefore, it indicates that the Topic Extractor module plays an effective role in literature summarization.Comparative Incremental Mechanism.To validate the effectiveness of the Comparative Incremental Mechanism, we choose ChatCite without Comparative Incremental Mechanism as comparison, following the few-shot baseline prompt [p s ] and few-shot examples as prompts to directly generate literature summaries from the text after standard summarization.Considering controlling variables for the incremental mechanism, we also incorporated CoT writing instructions into the method to ensure that the experimental results are not influenced by the writing instructions.In Table 2, when comparing ChatCite with and without the Comparative Incremental Mechanism, the results indicate that ChatCite achieves higher ROUGE metrics and LLM-based evaluation metrics compared to ChatCite without the Comparative Incremental Mechanism.This suggests that the Comparative Incremental Mechanism significantly contributes to the effectiveness of literature summarization in the ChatCite framework.Ablation Results: This table presents the ablation results on the model's Key Element Extractor and
ChatCite -w/o Reflective54321Scores5Consistency Coherence ComparativeIntegrity ChatCiteFluency Cite_Accuracy Overall4321Consistency Coherence ComparativeIntegrityFluency Cite_Accuracy OverallEvaluation DimensionsFigure 3: Ablation Study on the Reflective Mechanism.The upper and lower whiskers represent the overallrange of the data, while the box displays the distributionof the middle 50% of the dataset, with a line inside thebox representing the median of the data. Data pointsoutside the boxplot are considered outliers, indicatingdata points that significantly deviate from the box andwhiskers. It can be observed that ChatCite performsmore stable across all dimensions.
Reflective Mechanism.In conclusion, we analyzed the reflective mechanism's impact.G-Scores for various dimensions were assessed based on multiple results from ChatCite, both with and without the Reflective Mechanism.The boxplot results in Figure3show similarities between the outcome</p>
<p>Six evaluation dimensions are: Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy.
Our related work utilizes summaries generated by ChatCite with GPT-4 as a draft, with minimal refinement. The information is comprehensive with minimal errors. The generated results organize the literature and include comparative analysis. The generated results are presented in the appendix (
Table 4).
Please extr act the following key elements fr om the content:
Automatic related work section generation: experiments in scientific document abstracting. Ahmed Abura'ed, Horacio Saggion, Alexander Shvets, Àlex Bravo, 10.1007/s11192-020-03630-22020125</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 2024</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, 10.18653/v1/2021.acl-long.473Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Target-aware abstractive related work generation with contrastive learning. Xiuying Chen, Hind Alamro, Li Mingzhe, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>BACO: A background knowledge-and content-based framework for citing sentence generation. Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner, 10.18653/v1/2021.acl-long.116Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>News summarization and evaluation in the era of. Tanya Goyal, Junyi , Jessy Li, Greg Durrett, 20233</p>
<p>Towards automated related work summarization20. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010: Posters. Beijing, China2010. 2010Organizing Committee</p>
<p>The role of chatgpt in scientific communication: writing better scientific review articles. Jingshan Huang, Ming Tan, American Journal of Cancer Research. 13411482023</p>
<p>Automatic related work section in scientific article: Research trends and future directions. Army Justitia, Hei-Chia Wang, 2022 International Seminar on Intelligent Technology and Its Applications (ISITIA). IEEE2022</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Annual Meeting of the Association for Computational Linguistics. 2004a</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004b</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation. Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang, Ting Wang, IEEE Access. 82020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Automatic generation of citation texts in scholarly papers: A pilot study. Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Annual Meeting of the Association for Computational Linguistics. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
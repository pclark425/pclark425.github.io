<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9660 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9660</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9660</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-267301349</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.15042v4.pdf" target="_blank">P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively un-explored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose P ROXY QA, an innovative framework dedicated to assessing long-text generation. P ROXY QA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, P ROX - Y QA assesses the generated content’s quality through the evaluator’s accuracy in addressing the proxy-questions . We examine multiple LLMs, emphasizing P ROXY QA’s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9660.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9660.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROXYQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROXYQA (Proxy Question Answering) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark and evaluation framework that measures informativeness and knowledge coverage of long-form LLM outputs by having an external evaluator answer many human-curated proxy-questions derived from each meta-question using only the generated text as context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Long-form knowledge-rich text generation across domains (AI research, history, sports, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluator (GPT-4 / GPT-4-Turbo) answers a set of hidden proxy-questions based solely on the generated long-form text; quality is measured by evaluator accuracy on those proxy-questions rather than matching to a gold reference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on proxy-questions (boolean True/False/Not mentioned labels), inter-annotator agreement (Randolph's free-marginal multi-rater κ for proxy creation), self-agreement (consistency of evaluator votes), and human-agreement measures (M2G, G2M, G2G defined in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PROXYQA dataset: 100 human-curated meta-questions across multiple domains, each paired with on average 15.5 proxy-questions (answers hidden from tested LLMs); easy/hard splits and a continuously updated leaderboard for time-sensitive items.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Dataset statistics and validation: 100 meta-questions, avg 15.5 proxy-questions/meta-question; inter-annotator κ = 0.936 for proxy annotation. Aggregate model accuracies (average over easy+hard) reported: New Bing (Creative) ≈ 39.37, GPT-4-Turbo ≈ 33.94, GPT-4 ≈ 27.19, GPT-3.5-Turbo ≈ 23.94, instruction-finetuned LLaMA variants ≈ 19–21, LLaMA2-13B (base) much lower. Hard questions cause substantial performance drops (e.g., GPT-4 drop ≈ 6.55 points); New Bing (search-augmented) shows smaller degradation (≈ 39.95 → 38.75).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Focuses on informativeness and coverage, not on factuality/hallucination detection or fine-grained verifiability; proxy-questions capture essential points but cannot exhaustively cover every missing fact; requires periodic updates for time-sensitive items; annotation resource constraints limited meta-question count.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>PROXYQA correlates strongly with human majority preferences and yields substantially higher self-agreement (88.00%) than raw human inter-evaluator agreement (~52.19%); it also reduces the bias observed with LLM-as-judge approaches (GPT-evaluators overconfident on GPT outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Hide proxy-answers from evaluated models to avoid contamination; curate multiple proxy-questions per meta-question with iterative multi-expert annotation; measure evaluator validation accuracy; report easy/hard splits and update time-sensitive proxy-questions periodically; average results from multiple evaluator models (GPT-4 and GPT-4-Turbo used here) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9660.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9660.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proxy-questions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-curated proxy-questions and boolean answers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Short, direct boolean questions created by domain experts that probe the essential points a high-quality long-form answer should contain; used as the ground items the evaluator must answer from the generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of knowledge coverage in long-form outputs across multiple domains</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Experts identify key points for each meta-question and write multiple straightforward proxy-questions with annotated boolean answers; an evaluator must derive the answers using only the generated content.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary correctness of evaluator answers to each proxy-question (True/False/Not mentioned). Aggregate accuracy across proxy-questions per meta-question and across dataset is the primary metric.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PROXYQA: proxy-questions are internal to the dataset (avg 15.5 proxy-questions per meta-question); easy/hard split depending on whether Wikipedia suffices to answer.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Iterative multi-expert annotation produces high inter-annotator reliability (Randolph's κ = 0.936). Proxy-questions enable non-expert evaluators (LLM evaluators) to assess content quality by correctness on these items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Proxy-questions are purposely simple (avoid multi-hop/complex reasoning) and therefore measure coverage/informativeness rather than deep multi-step reasoning or fine-grained factual verification; cannot capture all possible missing facts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Allows non-expert evaluators to judge knowledge coverage by providing concrete checks rather than relying on open-ended human subjective scoring; reduces variability compared to free-form human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Create proxy-questions iteratively with alternating expert reviews to ensure multi-perspective coverage; keep proxy-questions and answers private to avoid data contamination; balance easy/hard items and mark time-sensitive questions for periodic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9660.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9660.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 / GPT-4-Turbo evaluator (used as answerer of proxy-questions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 and GPT-4-Turbo are used as automatic evaluators that read generated long-form content and answer proxy-questions, with the evaluation score computed as their accuracy against gold proxy answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 / GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Proprietary OpenAI LLMs; GPT-4 corresponds to GPT-4-0613, GPT-4-Turbo corresponds to GPT-4-1106-Preview; training cutoffs differ (GPT-4-turbo up-to-date to Apr 2023 per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automatic evaluation of long-form text generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompted to answer one proxy-question at a time using only provided generated text; answers are constrained to 'True', 'False', or 'Not mentioned'; final score is average accuracy across proxy-questions and across both GPT evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary/string labels per proxy-question; average accuracy used as the content quality metric. Additional meta-measures: evaluator self-agreement across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied on the PROXYQA dataset proxy-questions; evaluator prompts and decoding hyperparameters described in Appendix A.5.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Evaluator validation: GPT-4 achieved 91% accuracy on sampled (100) proxy-questions when validated against human experts' judgments of whether the evaluator used only document content or resorted to internal knowledge; averaging GPT-4 and GPT-4-Turbo used for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM evaluators can resort to internal knowledge rather than using the provided context (mitigated via prompt instructions and human validation); LLM evaluators have inconsistent behavior in other settings (cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>When used inside PROXYQA, the GPT-evaluators' decisions highly correlate with human majority preferences; but LLM-as-judge approaches without proxy-questions can be overconfident on models from the same family (GPT-as-judges bias).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Constrain evaluators to use only the given text via prompt; validate evaluator behavior with human experts; average multiple LLM evaluators to reduce single-model bias; limit per-proxy decoding budget to ensure consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9660.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9660.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agreement metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-agreement and human-agreement (M2G, G2M, G2G)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative agreement statistics used to measure consistency of an evaluation method (self-agreement) and its alignment with human judgments (Majority-to-Group M2G, Group-to-Majority G2M, Group-to-Group G2G).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology / metrics for LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Self-agreement: measure consistency across repeated votes by same evaluator setup; Human-agreement: compute M2G (how often evaluator majority agrees with human majority), G2M (how often evaluator votes agree with human majority), and G2G (pairwise agreement between evaluator and human votes) as defined in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement rates (%) computed over evaluated reports: R_self (self-agreement), R_M2G, R_G2M, R_G2G as algebraic formulas (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Computed on sampled reports (20 reports) from PROXYQA; comparisons include GPT-Separate, GPT-Batch, PROXYQA and human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PROXYQA self-agreement: 88.00% (high consistency). Human inter-evaluator agreement: ~52.19% (substantially lower). PROXYQA M2G ≈ 66.19% and G2M ≈ 63.33% (indicating strong alignment with human majority), while GPT-as-judges show poorer alignment and overconfidence on GPT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Majority human preference is itself variable (low baseline), so alignment to majority is only one measure; agreement metrics don't measure absolute correctness of content (they measure evaluator consistency and human alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These metrics quantify how well an automatic evaluation method reproduces human consensus and its internal consistency; PROXYQA scores substantially higher than raw human inter-annotator agreement, implying lower evaluator variability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report both self-agreement and human-agreement metrics when introducing new automatic evaluation methods; use repeated sampling and multiple evaluators to estimate stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9660.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9660.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation results (model rankings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported PROXYQA evaluation results and gaps across models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical outcomes from running PROXYQA: proprietary web-augmented models (New Bing, GPT-4-Turbo) substantially outperform open-source LLMs on knowledge-coverage accuracy; retrieval augmentation reduces drop on time-sensitive/hard items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Multiple (New Bing, GPT-4-Turbo, GPT-4, GPT-3.5-Turbo, LLaMA variants, Vicuna)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Mix of proprietary (OpenAI GPT family, New Bing/Gemini Pro/Bard) and open-source LLaMA-based models; OpenAI models differ in token-context capacity and training data cutoffs (GPT-4-turbo up-to-date to Apr 2023 per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Long-form knowledge generation evaluation spanning multiple domains (AI, history, sports, infrastructure, agriculture, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Zero-shot prompting to produce long reports; use generated text as context for GPT evaluators to answer proxy-questions; report accuracy on easy/hard splits and averages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Proxy-question accuracy averaged per model; also average generation length reported; additionally, qualitative error analysis and win-rate pairwise comparisons against human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PROXYQA (100 meta-questions; easy/hard splits; time-sensitive subset updated periodically); leaderboard reports easy/hard accuracies and avg performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key numeric summaries: New Bing (Creative) average accuracy ≈ 39.37; GPT-4-Turbo ≈ 33.94; GPT-4 ≈ 27.19; GPT-3.5-Turbo ≈ 23.94; instruction-finetuned LLaMA variants in ~19–21 range; base LLaMA family much lower. Retrieval-augmented systems (New Bing) show smaller performance drops on hard/time-sensitive questions. GPT-4 evaluator validated at 91% accuracy on whether it used document-only evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Even best models degrade on hard questions requiring non-Wikipedia or up-to-date knowledge; RAG helps but is not guaranteed (ReAct RAG underperformed in this setup); long generation length does not guarantee informativeness (some models produce long but uninformative text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>PROXYQA choices correlate strongly with human majority and pairwise win rates, whereas naive LLM-as-judge setups (GPT-S/GPT-B) show bias and overconfidence favoring GPT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include easy/hard splits, measure effect of retrieval augmentation, validate evaluators with human oversight, report both accuracy and agreement measures, and consider length vs informativeness tradeoffs when assessing long-form generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>ELI5: long form question answering <em>(Rating: 1)</em></li>
                <li>LongBench: A bilingual, multitask benchmark for long context understanding <em>(Rating: 1)</em></li>
                <li>BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9660",
    "paper_id": "paper-267301349",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "PROXYQA",
            "name_full": "PROXYQA (Proxy Question Answering) framework",
            "brief_description": "A benchmark and evaluation framework that measures informativeness and knowledge coverage of long-form LLM outputs by having an external evaluator answer many human-curated proxy-questions derived from each meta-question using only the generated text as context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Long-form knowledge-rich text generation across domains (AI research, history, sports, etc.)",
            "evaluation_method": "Evaluator (GPT-4 / GPT-4-Turbo) answers a set of hidden proxy-questions based solely on the generated long-form text; quality is measured by evaluator accuracy on those proxy-questions rather than matching to a gold reference.",
            "evaluation_criteria": "Accuracy on proxy-questions (boolean True/False/Not mentioned labels), inter-annotator agreement (Randolph's free-marginal multi-rater κ for proxy creation), self-agreement (consistency of evaluator votes), and human-agreement measures (M2G, G2M, G2G defined in paper).",
            "benchmark_or_dataset": "PROXYQA dataset: 100 human-curated meta-questions across multiple domains, each paired with on average 15.5 proxy-questions (answers hidden from tested LLMs); easy/hard splits and a continuously updated leaderboard for time-sensitive items.",
            "results_summary": "Dataset statistics and validation: 100 meta-questions, avg 15.5 proxy-questions/meta-question; inter-annotator κ = 0.936 for proxy annotation. Aggregate model accuracies (average over easy+hard) reported: New Bing (Creative) ≈ 39.37, GPT-4-Turbo ≈ 33.94, GPT-4 ≈ 27.19, GPT-3.5-Turbo ≈ 23.94, instruction-finetuned LLaMA variants ≈ 19–21, LLaMA2-13B (base) much lower. Hard questions cause substantial performance drops (e.g., GPT-4 drop ≈ 6.55 points); New Bing (search-augmented) shows smaller degradation (≈ 39.95 → 38.75).",
            "limitations_or_challenges": "Focuses on informativeness and coverage, not on factuality/hallucination detection or fine-grained verifiability; proxy-questions capture essential points but cannot exhaustively cover every missing fact; requires periodic updates for time-sensitive items; annotation resource constraints limited meta-question count.",
            "comparison_to_human_or_traditional": "PROXYQA correlates strongly with human majority preferences and yields substantially higher self-agreement (88.00%) than raw human inter-evaluator agreement (~52.19%); it also reduces the bias observed with LLM-as-judge approaches (GPT-evaluators overconfident on GPT outputs).",
            "recommendations_or_best_practices": "Hide proxy-answers from evaluated models to avoid contamination; curate multiple proxy-questions per meta-question with iterative multi-expert annotation; measure evaluator validation accuracy; report easy/hard splits and update time-sensitive proxy-questions periodically; average results from multiple evaluator models (GPT-4 and GPT-4-Turbo used here) to improve robustness.",
            "uuid": "e9660.0",
            "source_info": {
                "paper_title": "P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Proxy-questions",
            "name_full": "Human-curated proxy-questions and boolean answers",
            "brief_description": "Short, direct boolean questions created by domain experts that probe the essential points a high-quality long-form answer should contain; used as the ground items the evaluator must answer from the generated text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Evaluation of knowledge coverage in long-form outputs across multiple domains",
            "evaluation_method": "Experts identify key points for each meta-question and write multiple straightforward proxy-questions with annotated boolean answers; an evaluator must derive the answers using only the generated content.",
            "evaluation_criteria": "Binary correctness of evaluator answers to each proxy-question (True/False/Not mentioned). Aggregate accuracy across proxy-questions per meta-question and across dataset is the primary metric.",
            "benchmark_or_dataset": "PROXYQA: proxy-questions are internal to the dataset (avg 15.5 proxy-questions per meta-question); easy/hard split depending on whether Wikipedia suffices to answer.",
            "results_summary": "Iterative multi-expert annotation produces high inter-annotator reliability (Randolph's κ = 0.936). Proxy-questions enable non-expert evaluators (LLM evaluators) to assess content quality by correctness on these items.",
            "limitations_or_challenges": "Proxy-questions are purposely simple (avoid multi-hop/complex reasoning) and therefore measure coverage/informativeness rather than deep multi-step reasoning or fine-grained factual verification; cannot capture all possible missing facts.",
            "comparison_to_human_or_traditional": "Allows non-expert evaluators to judge knowledge coverage by providing concrete checks rather than relying on open-ended human subjective scoring; reduces variability compared to free-form human judgments.",
            "recommendations_or_best_practices": "Create proxy-questions iteratively with alternating expert reviews to ensure multi-perspective coverage; keep proxy-questions and answers private to avoid data contamination; balance easy/hard items and mark time-sensitive questions for periodic updates.",
            "uuid": "e9660.1",
            "source_info": {
                "paper_title": "P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-evaluator",
            "name_full": "GPT-4 / GPT-4-Turbo evaluator (used as answerer of proxy-questions)",
            "brief_description": "GPT-4 and GPT-4-Turbo are used as automatic evaluators that read generated long-form content and answer proxy-questions, with the evaluation score computed as their accuracy against gold proxy answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4 / GPT-4-Turbo",
            "llm_description": "Proprietary OpenAI LLMs; GPT-4 corresponds to GPT-4-0613, GPT-4-Turbo corresponds to GPT-4-1106-Preview; training cutoffs differ (GPT-4-turbo up-to-date to Apr 2023 per paper).",
            "scientific_domain": "Automatic evaluation of long-form text generation",
            "evaluation_method": "Prompted to answer one proxy-question at a time using only provided generated text; answers are constrained to 'True', 'False', or 'Not mentioned'; final score is average accuracy across proxy-questions and across both GPT evaluators.",
            "evaluation_criteria": "Binary/string labels per proxy-question; average accuracy used as the content quality metric. Additional meta-measures: evaluator self-agreement across repeated runs.",
            "benchmark_or_dataset": "Applied on the PROXYQA dataset proxy-questions; evaluator prompts and decoding hyperparameters described in Appendix A.5.",
            "results_summary": "Evaluator validation: GPT-4 achieved 91% accuracy on sampled (100) proxy-questions when validated against human experts' judgments of whether the evaluator used only document content or resorted to internal knowledge; averaging GPT-4 and GPT-4-Turbo used for robustness.",
            "limitations_or_challenges": "LLM evaluators can resort to internal knowledge rather than using the provided context (mitigated via prompt instructions and human validation); LLM evaluators have inconsistent behavior in other settings (cited prior work).",
            "comparison_to_human_or_traditional": "When used inside PROXYQA, the GPT-evaluators' decisions highly correlate with human majority preferences; but LLM-as-judge approaches without proxy-questions can be overconfident on models from the same family (GPT-as-judges bias).",
            "recommendations_or_best_practices": "Constrain evaluators to use only the given text via prompt; validate evaluator behavior with human experts; average multiple LLM evaluators to reduce single-model bias; limit per-proxy decoding budget to ensure consistency.",
            "uuid": "e9660.2",
            "source_info": {
                "paper_title": "P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Agreement metrics",
            "name_full": "Self-agreement and human-agreement (M2G, G2M, G2G)",
            "brief_description": "Quantitative agreement statistics used to measure consistency of an evaluation method (self-agreement) and its alignment with human judgments (Majority-to-Group M2G, Group-to-Majority G2M, Group-to-Group G2G).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Evaluation methodology / metrics for LLM evaluation",
            "evaluation_method": "Self-agreement: measure consistency across repeated votes by same evaluator setup; Human-agreement: compute M2G (how often evaluator majority agrees with human majority), G2M (how often evaluator votes agree with human majority), and G2G (pairwise agreement between evaluator and human votes) as defined in paper.",
            "evaluation_criteria": "Agreement rates (%) computed over evaluated reports: R_self (self-agreement), R_M2G, R_G2M, R_G2G as algebraic formulas (see paper).",
            "benchmark_or_dataset": "Computed on sampled reports (20 reports) from PROXYQA; comparisons include GPT-Separate, GPT-Batch, PROXYQA and human evaluators.",
            "results_summary": "PROXYQA self-agreement: 88.00% (high consistency). Human inter-evaluator agreement: ~52.19% (substantially lower). PROXYQA M2G ≈ 66.19% and G2M ≈ 63.33% (indicating strong alignment with human majority), while GPT-as-judges show poorer alignment and overconfidence on GPT outputs.",
            "limitations_or_challenges": "Majority human preference is itself variable (low baseline), so alignment to majority is only one measure; agreement metrics don't measure absolute correctness of content (they measure evaluator consistency and human alignment).",
            "comparison_to_human_or_traditional": "These metrics quantify how well an automatic evaluation method reproduces human consensus and its internal consistency; PROXYQA scores substantially higher than raw human inter-annotator agreement, implying lower evaluator variability.",
            "recommendations_or_best_practices": "Report both self-agreement and human-agreement metrics when introducing new automatic evaluation methods; use repeated sampling and multiple evaluators to estimate stability.",
            "uuid": "e9660.3",
            "source_info": {
                "paper_title": "P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Evaluation results (model rankings)",
            "name_full": "Reported PROXYQA evaluation results and gaps across models",
            "brief_description": "Empirical outcomes from running PROXYQA: proprietary web-augmented models (New Bing, GPT-4-Turbo) substantially outperform open-source LLMs on knowledge-coverage accuracy; retrieval augmentation reduces drop on time-sensitive/hard items.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Multiple (New Bing, GPT-4-Turbo, GPT-4, GPT-3.5-Turbo, LLaMA variants, Vicuna)",
            "llm_description": "Mix of proprietary (OpenAI GPT family, New Bing/Gemini Pro/Bard) and open-source LLaMA-based models; OpenAI models differ in token-context capacity and training data cutoffs (GPT-4-turbo up-to-date to Apr 2023 per paper).",
            "scientific_domain": "Long-form knowledge generation evaluation spanning multiple domains (AI, history, sports, infrastructure, agriculture, etc.)",
            "evaluation_method": "Zero-shot prompting to produce long reports; use generated text as context for GPT evaluators to answer proxy-questions; report accuracy on easy/hard splits and averages.",
            "evaluation_criteria": "Proxy-question accuracy averaged per model; also average generation length reported; additionally, qualitative error analysis and win-rate pairwise comparisons against human judgments.",
            "benchmark_or_dataset": "PROXYQA (100 meta-questions; easy/hard splits; time-sensitive subset updated periodically); leaderboard reports easy/hard accuracies and avg performance.",
            "results_summary": "Key numeric summaries: New Bing (Creative) average accuracy ≈ 39.37; GPT-4-Turbo ≈ 33.94; GPT-4 ≈ 27.19; GPT-3.5-Turbo ≈ 23.94; instruction-finetuned LLaMA variants in ~19–21 range; base LLaMA family much lower. Retrieval-augmented systems (New Bing) show smaller performance drops on hard/time-sensitive questions. GPT-4 evaluator validated at 91% accuracy on whether it used document-only evidence.",
            "limitations_or_challenges": "Even best models degrade on hard questions requiring non-Wikipedia or up-to-date knowledge; RAG helps but is not guaranteed (ReAct RAG underperformed in this setup); long generation length does not guarantee informativeness (some models produce long but uninformative text).",
            "comparison_to_human_or_traditional": "PROXYQA choices correlate strongly with human majority and pairwise win rates, whereas naive LLM-as-judge setups (GPT-S/GPT-B) show bias and overconfidence favoring GPT outputs.",
            "recommendations_or_best_practices": "Include easy/hard splits, measure effect of retrieval augmentation, validate evaluators with human oversight, report both accuracy and agreement measures, and consider length vs informativeness tradeoffs when assessing long-form generation.",
            "uuid": "e9660.4",
            "source_info": {
                "paper_title": "P ROXY QA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "ELI5: long form question answering",
            "rating": 1,
            "sanitized_title": "eli5_long_form_question_answering"
        },
        {
            "paper_title": "LongBench: A bilingual, multitask benchmark for long context understanding",
            "rating": 1,
            "sanitized_title": "longbench_a_bilingual_multitask_benchmark_for_long_context_understanding"
        },
        {
            "paper_title": "BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models",
            "rating": 1,
            "sanitized_title": "bamboo_a_comprehensive_benchmark_for_evaluating_long_text_modeling_capacities_of_large_language_models"
        }
    ],
    "cost": 0.015134249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models
4 Jun 2024</p>
<p>Haochen Tan haochetan2-c@my.cityu.edu.hk 
City University</p>
<p>Zhijiang Guo guozhijiang@huawei.com 
Zhan Shi 
Lu Xu 
Zhili Liu 
Hong Kong University of Science and Technology ◁ Harbin Institute of Technology ♥ City University of Hong Kong Shenzhen Research Institute</p>
<p>Yunlong Feng 
◁ Xiaoguang Li 
Yasheng Wang 
Qun Liu 
Linqi Song linqi.song@cityu.edu.hk 
City University</p>
<p>Hong Kong 
Huawei Noah's Ark 
Huawei Hisilicon 
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models
4 Jun 20240F0D7A336990E232DE1A3F5F7EE9A94BarXiv:2401.15042v4[cs.CL]
Large Language Models (LLMs) have succeeded remarkably in understanding longform contents.However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks.The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria.In this paper, we propose PROXYQA, an innovative framework dedicated to assessing longtext generation.PROXYQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers.LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, PROX-YQA assesses the generated content's quality through the evaluator's accuracy in addressing the proxy-questions.We examine multiple LLMs, emphasizing PROXYQA's demanding nature as a high-quality assessment tool.Human evaluation demonstrates that the proxyquestion method is notably self-consistent and aligns closely with human evaluative standards.The dataset and leaderboard is available at https://proxy-qa.com.</p>
<p>Introduction</p>
<p>Recent Large Language Models (LLMs) have made significant advancements (Brown et al., 2020;Touvron et al., 2023a,b;OpenAI, 2022aOpenAI, , 2023b)).GPU technology innovations and memory-efficient attention mechanisms (Dao et al., 2022;Dao, 2023)  spanning tens of thousands of tokens (Anthropic, 2023;OpenAI, 2023c), paving the way for sophisticated applications such as analyzing complex scientific essays and generating detailed reports.As long-context LLMs evolve, several benchmarks have emerged to evaluate their ability to handle extensive contexts (Shaham et al., 2023;Bai et al., 2023;An et al., 2023;Zhang et al., 2023).However, these assessments primarily focus on LLMs' comprehension of lengthy passages, using automated metrics to measure performance.This leaves a significant gap in understanding LLMs' proficiency in generating long-form texts, an essential aspect that requires further investigation.</p>
<p>One primary roadblock to understanding LLMs' capability to generate long-form texts is the lack of competent evaluation methods.Current methods, often involving a combination of automated metrics and crowdsourced annotations, leave much to be desired (Xu et al., 2023).For instance, automated metrics that use word-level string (Lin, 2004) or meaning representation matching (Yuan et al., 2021) rely on gold references, which unfortunately do not exist for many generation tasks, such as reports or essay writing.Furthermore, these automated metrics are inadequate for reliably assessing long-form content due to the considerable and unstructured space of potential outputs (Celikyilmaz et al., 2020;Krishna et al., 2021).Human evaluation has its own set of issues too.Crowdsourced workers may lack the necessary expertise for evaluating knowledge-rich content, and domain experts' subjective preferences could result in inconsistent evaluations (Xu et al., 2023).While recent studies explored using LLMs for evaluation (Chiang and Lee, 2023;Liu et al., 2023), LLMs have been found to lack the most current information required for precise verification.Moreover, their assessments have been observed to be inconsistent (Shen et al., 2023).There is a clear need for more robust and precise evaluation methods.</p>
<p>To address this issue, we introduce PROXYQA, a benchmark comprising human-curated metaquestions covering a wide range of subjects, from computer science to history.These meta-questions require domain expertise and up-to-date knowledge, prompting LLMs to generate detailed and comprehensive responses.To assess knowledge coverage and informativeness, we pair each metaquestion with a series of proxy-questions and answers that capture its essential points.As illustrated in Figure 1, PROXYQA uses an evaluator to answer proxy-questions based on the long-form content produced by LLMs, rather than comparing the output to a reference.If the generated content is sufficiently detailed and accurate, it should equip the evaluator with enough information to thoroughly answer all associated proxy-questions.</p>
<p>PROXYQA offers several benefits.By employing proxy questions and an evaluator, it eliminates the need for direct comparison against a single gold reference, enabling a more accessible and subjective evaluation.Using this approach allows evaluators without specific domain knowledge to assess content.Additionally, unlike previous datasets compiled from online sources (Nguyen et al., 2016;Fan et al., 2019a) that potentially leading to data contamination (Sainz et al., 2023), all the proxy-questions and answers are invisible to public, thereby preventing data leakage.We apply PROXYQA to extensively test different LLMs (Touvron et al., 2023a,b;Taori et al., 2023;Chiang and Lee, 2023;OpenAI, 2022aOpenAI, , 2023b)), including the LLMs that enhanced iterative reasoning (Yao et al., 2023a) and retrieval augmentation (Bing, 2023;Gemini, 2023).A systematic human evaluation demonstrates that PROXYQA offers a highly consistent evaluation scope, surpassing inter-human agreement rates while maintaining strong correlations with the majority preferences of humans.In-depth analysis shows that ProxyQA effectively eliminates the bias that LLM-as-a-judge (Zheng et al., 2023) prefer content generated by GPT models, and the evaluators' accuracy on the proxyquestions accurately reflects the quality of the generated contents.</p>
<p>2 Related Work</p>
<p>Long-Form Text Generation</p>
<p>Significant strides have been made in long-form text generation, particularly in story generation (Fan et al., 2019b;Xu et al., 2020), paragraph completion (Kang and Hovy, 2020), long-term conversation (Xu et al., 2022) and article generation (Hua and Wang, 2020;Hu et al., 2022).A closely related field is long-form question answering (Fan et al., 2019a;Dasigi et al., 2021;Stelmakh et al., 2022;Lee et al., 2023), which involves generating detailed responses to complex informationseeking questions.ELI5 (Fan et al., 2019a) was a pioneer dataset for generating explanatory paragraphs in response to open-ended questions, utilizing answers from Reddit.QASPER (Dasigi et al., 2021) and QASA (Lee et al., 2023) extend general factoid questions to the domain of scientific literature.Evaluating answers on these datasets relies on comparing the generated texts with the provided single reference.However, open-ended questions can be answered in myriad different ways.ASQA (Stelmakh et al., 2022) also introduces a set of disambiguated questions from AmbigQA (Min et al., 2023) for evaluating ambiguous questions.They assume that long-form answers to ambiguous questions should resolve ambiguity.In contrast, ProxyQA aims to gauge the informativeness and comprehensiveness of long-form answers, without being confined solely to ambiguous questions.</p>
<p>History</p>
<p>Introduce the countries that involved in 1st and 2nd Industrial Revolutions and list the major inventions.</p>
<p>Compare the two world wars in terms of the scale, duration, weapon, and involved countries.</p>
<p>Sports</p>
<p>Is it possible for human to finish the marathon within 2 hours and 1 minute?Who is holding the record?</p>
<p>Introduce European Championship and World Cup in detail.</p>
<p>What is the Grand Slam? please tell the details about it.</p>
<p>Others</p>
<p>Introduce the hottest chili pepper, how it is bred?Introduce the top 3 most priced paintings and their artists.</p>
<p>Text Generation Evaluation</p>
<p>Automated metrics such as surface form matching (Lin, 2004;Banerjee and Lavie, 2005) and semantic representation comparison (Zhang et al., 2020;Yuan et al., 2021), face challenges with longform content due to their inability to handle the diversity of the potential outputs (Celikyilmaz et al., 2020;Krishna et al., 2021).They often do not align with human judgment (Xu et al., 2023).Attempts to use LLMs for evaluation (Liu et al., 2023;Chiang and Lee, 2023;Liu et al., 2024) are hindered by their limited access to current information and inconsistency in performance (Shen et al., 2023).Evaluators also have difficulties, particularly if they lack expertise, which can impair their judgment on key dimensions like informativeness and factuality (Gillick and Liu, 2010;Iskender et al., 2020).Strategies to enhance human evaluations include A/B testing, as seen with HURDLES (Krishna et al., 2021) and WebGPT (Nakano et al., 2021), with the latter demonstrating that providing evidence helps annotators make more informed decisions.The list of proxy questions in PROXYQA can be viewed as evidence to assist the evaluator in making decisions.While some research focused on coherence (Goyal et al., 2022;Jiang et al., 2022;Deng et al., 2022) and factuality (Goyal and Durrett, 2020;Laban et al., 2022;Min et al., 2023) in related tasks like summarization, our work emphasizes informativeness and coverage.</p>
<p>A Long-form Generation Benchmark</p>
<p>An alternative framework for evaluating long-form text generation is created.100 meta-questions are annotated to prompt LLMs to generate detailed and informative responses, which cover subjects in artificial intelligence (AI) research, historical event investigations, sports and entertainment event analysis.The topic distribution is shown in Figure 2.Each meta-question accompanies various proxyquestions with annotated answers, which are invisible to the LLMs to be tested.To proxy the objective evaluation of the generated contents to subjective metrics, we adopt an evaluator that takes the generated contents as contextual background and answers the proxy-questions.We assume that only if the contextual background is informative and comprehensive, can the proxy-questions be well-addressed by the evaluator.Therefore, the quality of the generated contents is reflected in the evaluator's accuracy on the proxy-questions.</p>
<p>Creating the Meta-questions</p>
<p>Meta-questions were manually raised by five experienced researchers, who were instructed to initiate meta-questions in areas with which they were most familiar or had a keen interest.Besides the mostconcerned topics such as AI research, sports and gaming, less popular domains such as infrastructure and agriculture are also included.We focus on questions that are aligned with real-life scenarios and should be well-addressable in reports or articles.For instance, a pertinent question within the Computer Science domain could be: "Could you elaborate on the development of Model Parallelism and Pipeline Parallelism, detailing key milestones and contributions?"This meta-question aligns with interests in parallel computing techniques.In contrast, questions of the sort "Did Aristotle use a laptop?" from StrategyQA (Geva et al., 2021) are omitted due to their lack of occurrence in realistic settings.</p>
<p>Difficulty of Meta-question Meta-questions can be classified into two levels of difficulty: easy and hard.The annotation process was bifurcated; half of the experts annotated questions using only Wikipedia, denoting easy questions, while the rest utilized various resources to labeling difficult questions.This method was adhered to in subsequent annotation rounds, with experts verifying knowledge sources to accurately categorize question difficulty.Easy questions can be sufficiently answered using only information from Wikipedia, while hard questions demand the integration of Wikipedia content with insights derived from a wider range of open-domain knowledge sources to formulate a comprehensive response.Generally, most LLMs can effectively address easy questions given their extensive training on Wikipedia corpora.Conversely, hard questions pose a challenge to the models' ability to acquire information beyond the commonly used pre-training data, necessitating access to specialized private corpora, web searches, or document retrieval for a comprehensive response.</p>
<p>For more details, please refer to Appendix A.8.</p>
<p>Annotation of Proxy-Question</p>
<p>The evaluation of the generated reports is proxied to the evaluator's accuracy on proxy-questions.</p>
<p>Experts are tasked with identifying the pivotal content that a satisfactory answer to a meta-question must contain.Then they craft a series of proxyquestions that probe these identified key points.For instance, regarding the example of model parallelism and pipeline parallelism mentioned in Section 3.1, a thorough answer should incorporate in-depth information about Gpipe (Huang et al., 2019), Megatron (Shoeybi et al., 2019), and other pertinent subjects.Therefore, annotators develop proxy-questions that specifically focus on Gpipe, Megatron and other related topics.As our destination is not to stress-test the evaluator but to quantify the quality of the generated contents, we present straightforward and concise proxy-questions, deliberately avoiding multi-hop and complex reasoning queries.Each annotated response is provided in a boolean format, ensuring that evaluators can effortlessly answer these proxy-questions, given a sufficiently high-quality generated context.Conversely, if an evaluator struggles to address these simple proxy-questions based on the provided context, it indicates a significant deficiency in the generated context, with crucial information being absent.</p>
<p>Iterative Annotation</p>
<p>As the foundation of the evaluation in PROXYQA, proxy-questions are curated through a multi-round annotation process, where experts iteratively exchange the meta-questions while supplementing and verifying proxy-questions annotated by the others.Each meta-question is thus repeatedly given to different experts to label different proxy-questions, until a consensus is reached that all experts agree that the points covered by the proxy-questions are sufficiently comprehensive.Such an alternate labeling process ensures a multi-perspective rubric, leading to an experts-consolidated benchmark.After the iterative annotation, each meta-question is coupled with 15.5 proxy-questions on average, we then measure the inter-annotator agreement of PROX-YQA.We randomly extracted a subset of 50 proxyquestions and tasked the annotators to re-annotate them.Following the Kazemi et al. (2021), the Randolph's free-marginal multi-rater κ (Randolph, 2010), an alternative to Fleiss' κ are measured.PROXYQA achieves κ = 0.936 thanks for the introduction of the iterative annotation process.</p>
<p>Quality Assurance</p>
<p>The meta-question and its corresponding proxyquestions were annotated and quality-checked iteratively.During the annotation, we excluded three meta-questions that were offensive, politically sensitive, ethically concerning, or not safe for work (NSFW).Meta-questions that have been previously posted and well-addressed on relevant forums are replaced.This ensures that LLMs cannot generate answers by directly copying content from these platforms, therefore preserving the integrity of our dataset.Additionally, it is worth noting that realworld knowledge evolves over time.Therefore, to ensure the quality of PRXOYQA, our experts are required to review and update proxy-questions periodically, as detailed in the Appendix A.9.</p>
<p>Evaluators</p>
<p>To ensure the generated contents such as markdown tables and math formulas can be well encoded, GPT-4 and its variant, GPT-4-Turbo (GPT-4-1106-Preview), are utilized as evaluators.Instead of applying retrieval augmented generation, evaluators are required to read the generated contents and answer the proxy-questions.We prompt the evaluator to formulate answers to the proxy-questions strictly from the information presented within the contextual background.Evaluator's accuracy can be high only if the generated contextual background is informative and comprehensive enough.The objective assessment of these reports is subsequently anchored to the precision of the GPT evaluator.Moreover, we take the mean value of evaluation results from GPT-4 and GPT-4-Turbo to reinforce the reliability and robustness of the assessment.</p>
<p>Experiments</p>
<p>Setup</p>
<p>Assessment and Models To measure LLMs' performance on PROXYQA, we compute the accuracy of the proxy-questions across easy and hard splits.Open-sourced LLMs such as LLaMA and its instruction-finetuned variants are tested.We also evaluated closed-sourced LLMs (e.g.GPT) and web-augmented LLMs, including Bard (Gemini Pro) (Gemini, 2023) and New Bing (Bing, 2023).Details of all the tested models are in the Appendix A.3.Each LLM is prompted with "Write a well-structured and extensive report to answer the question: [META-QUESTION]" under the setting of zero-shot evaluation.The max decoding length of open-sourced and closed-sourced LLMs is set to their reported maximum length.All the other hyperparameters of decoding strategies are the same as their reported settings in the original paper or API documentation.</p>
<p>Setting of the Evaluator We feed GPT-4 and GPT-4-Turbo with the prompt shown in Appendix A.5.Both evaluators used the same decoding strategy with top_p=1, max_tokens=10, and frequency_penalty=0.Each evaluator only processes one sub-questions at a time.We calculate the evaluator's average accuracy to represent the generated content's overall quality.</p>
<p>Research Questions To carry out an in-depth investigation of long-form content generation and the effectiveness of our proposed PROXYQA, the following research questions are naturally raised: RQ1.How do open-sourced LLMs compare to proprietary models' ability to generate extensive reports or articles?(Section 4.2) RQ2.How well do the modern LLMs grasp the knowledge with different difficulty levels?(Section 4.3) RQ3.How do LLMs perform in generating long content from different domains?(Section 4.4) RQ4.How does the iterative and alternate annotation of proxy-questions affect the performance of the LLMs?(Section 4.5) RQ5.Does PROXYQA give higher ratings for generated content with longer length?(Section 4.6)</p>
<p>Main Results</p>
<p>The ability of open-sourced LLMs to generate comprehensive and extensive content is far behind the proprietary models.As shown in Table 1, the base</p>
<p>Results on Different Levels of Difficulty</p>
<p>Well-pretrained LLMs surpass others on all fronts but struggle with hard questions, while welldesigned retrieval-augmented generation (RAG) significantly make up the shortcoming.Table 1 illustrates a pronounced decline in performance among most large language models (LLMs) on the more challenging subset of questions, which cannot be well-solved solely with information from Wikipedia.Notably, even the powerful GPT-4 exhibits a marked decrease in efficacy(6.55 ↓).In contrast, equipped with the GPT-4 with a search engine and well-designed searching strategy, New Bing Creative Mode performs more robustly than other LLMs, exhibiting a comparatively minor performance loss( 39.95 →38.75).However, RAG is not a one-size-fits-all solution, as the GPT-4 model equipped with the ReAct falls short of generating comprehensive content.This is attributed to the fact that ReAct repurposes the GPT into a role more akin to planner and executor, constraining its capacity as a parametric knowledge base.</p>
<p>Domain</p>
<p>Proprietary LLMs overwhelmingly outperform other competitors in all domains.The advantage is further extended by integrating the search engine.</p>
<p>Impact of Alternate Annotation</p>
<p>Despite the impressive performance, proprietary LLMs are still unable to cater to the preferences of every individual.Figure 4 compares the LLMs' performance on expert-consolidated and singleexpert-focused subsets.As the iterative verification and supplementation of proxy-questions proceed, the performance of all models decreases, suggesting that LLMs cannot cater to every individual's preferences.Remarkably, New Bing outperforms all other baselines by a considerable margin, no matter the sub-split where only a single expert is involved or on the complete expert-consolidated set.However, despite the impressive performance, significant degradation could be observed.This suggests that the multi-perspective evaluation criteria in PROXYQA pose critical challenges to LLMs.</p>
<p>Generation Length</p>
<p>Improving the readability and informativeness of generated content within limited token budgets remains an area for systematic exploration.The average generation lengths are presented in Table 2.</p>
<p>It is essential to emphasize that the degree of informativeness and comprehensiveness is not proportional to the length of the generated content.Specifically, LLaMA2-13B generates lengthy content, yet it exhibits the lowest quality in generating contextual background on PROXYQA.In contrast, GPT-4-Turbo produces concise content while conveying extensive and comprehensive information.Moreover, when GPT-4 is incorporated with the search engine, the New Bing Creative Mode yields highly informative and in-depth content, significantly surpassing all other baseline models with an acceptable increase in generation length.</p>
<p>Analysis</p>
<p>Win Rate</p>
<p>We study the pairwise win rate among various LLMs evaluated by PROXYQA and compare the results with human evaluation to validate the effectiveness of PROXYQA.</p>
<p>Setup Five well-educated postgraduate students are engaged; all have not participated in annotating the meta and proxy-questions of PROXYQA; they are required to score and rank the randomly sampled reports generated by different LLMs.The scoring guideline is shown in Appendix A.4.We sampled ten meta-questions from PROXYQA and employed four LLMs to generate comprehensive reports.As a further comparison, we also follow the settings in MT-Bench (Zheng et al., 2023) that adopt LLM-as-judges, which directly rate the generated report based on the scoring guideline.</p>
<p>Similarly, we utilize GPT-Seperate (GPT-S), which  evaluates a single report at a time, and GPT-Batch (GPT-B), which evaluates and compares multiple reports simultaneously, to score and rank each report.Given that five human evaluators are involved in the comparison, we ensure fairness and robustness by requiring GPT-S, GPT-B, and our proposed PROXYQA to evaluate each report five times.We then calculate the average win rate based on the pairwise comparison.</p>
<p>Result GPT-as-judges over-confident on the contents generated by GPT models, while ProxyQA's choice is highly correlated with humans.As shown in Figure 5, majority of the evaluation results highly correlated with the human's choices.Specifically, evaluators generally recognize the quality of the reports generated by GPT-4-Turbo and New Bing, i.e., their win rates are much higher than those of Vicuna and LLama2-Chat.It is worth noting, however, that GPT-S and GPT-B exhibit overconfidence in the quality of the reports generated by GPT-4-Turbo compared to New Bing.In contrast, both human evaluators and ProxyQA exhibit a preference for New Bing over GPT-4-Turbo.This outcome attests to the effectiveness of ProxyQA and demonstrates the correlation between ProxyQA and human evaluation.</p>
<p>Agreement Evaluation</p>
<p>To thoroughly examine the consistency of PROX-YQA and the correlation against human evaluation criteria, we investigate the agreement rate of human assessment and PROXYQA.Two categories of agreement rates are explored.Similar to MT-Bench (Zheng et al., 2023), to effectively gauge the consistency of our proposed PROXYQA, we assess the self-agreement rate, which calculates the inter-evaluator agreement rate.Furthermore, we establish that the evaluation method proposed in the PROXYQA exhibits a strong correlation with human judgment by determining the agreement rate between human evaluations, referred to as the human agreement rate.We employ GPT-Seperate and GPT-Batch, as discussed in section 5.1, for comparison purposes.</p>
<p>Setup Following the settings in section 5.1, 20 reports generated by GPT-4-Turbo and New Bing are evaluated and compared by experts that have not participated in the annotation of PROXYQA.We analyze the agreement between different evaluation methods and human evaluations.</p>
<p>Self-agreement Given a pair of meta-question and its corresponding generated report, each evaluation method is required to score and vote the preferred reports n times.Let V = {v 1 , . . ., v n } be the set of voting results, then the self-agreement rate is calculated as:
R self ≜ 1 C 2 n n ∑ i=1 n ∑ j=i+1 1{v i = v j }(1)
Where 1{⋅} denote the indicator function.The self-agreement rate, denoted as R self , quantifies the consistency of an evaluation method.</p>
<p>Human-agreement The calculation is divided into Majority-to-Group (M2G), Group-to-Majority (G2M) and Group-to-Group (G2G).The R M 2G quantifies the proportion of the majority vote of a specific evaluation method in concordance with the overall votes of human evaluation.Conversely, R G2M calculates the proportion of overall votes of an evaluation method that concur with the majority vote of humans, indicating how well the evaluation criteria are aligned with the majority opinion of humans.R G2G provides a view of the overall agreement between an evaluation method and the human.Let V e = {e 1 , . . ., e n }, V h = {h 1 , . . ., h n } represent the set of the voting results of an evaluation method and human, respectively.The majority vote of a set is represented as M(⋅).Then, the  3: Agreement between each evaluation method and human evaluation.Self-agreement is also reported.</p>
<p>agreement is calculated as:
R M 2G ≜ 1 n n ∑ i=1 1{h i = M(V e )} (2) R G2M ≜ 1 n n ∑ i=1 1{e i = M(V h )} (3) R G2G ≜ 1 n 2 n ∑ i=1 n ∑ j=1 1{e i = h j }(4)
Results Human preference varies from individual to individual, while ProxyQA shows firm consistency and is highly correlated to the majority of humans.Table 3 illustrates that PROXYQA offers a highly consistent metric for evaluating long content generation.Furthermore, PROXYQA strongly correlates with the majority opinion of human evaluators, thereby emphasizing its efficacy in validating long-form content.Notably, the subjective preferences of human experts vary, as the agreement rate reaches only 52.19%, leading to relatively inconsistent evaluations, which is consistent with the findings of Xu et al. (2023).In contrast, PROXYQA achieves an 88.00% agreement rate, indicating its potential as a highly consistent performance indicator.Moreover, when evaluating the consensus between different evaluation methods and the majority opinion of human experts, both GPT-S and GPT-B are contrary to human preference.GPT-asjudges is overconfident in the reports generated by GPT-4-Turbo, which becomes even more extreme when evaluating with GPT-B.However, the criteria of PROXYQA significantly align with the majority opinion of human experts, surpassing GPT-asjudges in all human-agreement rates with a substantial margin and attaining 66.19% and 63.33% M2G and G2M agreement rate respectively.These findings provide robust evidence that the proposed PROXYQA can effectively and reliably assess the capabilities of LLMs in generating long-form content.</p>
<p>Validation of the GPT Evaluator</p>
<p>To ensure the evaluator's accuracy and confirm that answers to proxy-questions are derived strictly from the information within the generated content, we involved five human experts to conduct a human evaluation.If the GPT-evaluator fails to extract correct information from the generated content to answer a proxy-question, or answers a proxyquestion without using the knowledge presented in the content (answer using the internal knowledge), the experts will consider it a negative sample.</p>
<p>We validated the accuracy of our proxy-evaluator.</p>
<p>Given the reports generated by GPT-4-Turbo, New Bing, Vicuna-13B, and LLama2-Chat, 100 proxyquestions and the boolean answer generated by the GPT-4 evaluator are sampled.The GPT-4 evaluator reaches a 91% accuracy rate, demonstrating its capability as a reliable evaluator.</p>
<p>Validation of Prompts</p>
<p>Furthermore, we evaluated the model's performance using different prompts such as "Please write a comprehensive long report on the following question".We compared the results to the performance without any prompts.Our analysis indicated that, in most instances, the presence or absence of different prompts had minimal impact on performance.We attribute this to the fact that the majority of meta-questions in ProxyQA already necessitate the LLM to "explain/introduce/compare in detail" or "generate a detailed report about ."</p>
<p>Conclusion</p>
<p>In this work, we introduce PROXYQA, a framework designed to evaluate LLMs' ability to generate long-form text.Unlike traditional methods that rely on a direct comparison with a reference text, by employing an evaluator to use the information provided in the LLM-generated text to answer proxy-questions, the framework assesses the LLMs' ability without fixed references or crowdsource workers.By mitigating concerns over data contamination and ensuring the relevance and freshness of evaluation content, PROXYQA enhances our understanding of LLMs and drives innovation towards developing long-form generation methods with LLMs.</p>
<p>Limitation</p>
<p>In this study, PROXYQA mainly focuses on evaluating the informative and knowledge coverage of the generated texts.However, multiple key dimensions such as factuality (Min et al., 2023), verifiability (Liu et al., 2023), and coherency (Deng et al., 2022) should be considered for long-form content generation.For instance, proxy questions cannot measure hallucination in long-form content, a critical issue for a long-form generation.On the other hand, each meta-question is annotated by five experts, but they cannot cover potentially all the proxy-questions.More advanced methods that consider these issues will be developed in future work.In addition, due to limited annotation resources, we prioritized adding as many proxyquestions as possible to maintain a high benchmark quality, rather than increasing the number of metaquestions.More meta-questions should be included in the future version.</p>
<p>Ethical Considerations</p>
<p>To avoid potential ethical issues, we carefully checked all questions in multiple aspects, as discussed in Section 3.4.We try to guarantee that all samples do not involve any offensive, genderbiased, or political content, and any other ethical issues.The source code will be released with instructions to support correct use.The baseline model we tested are all open-sourced LLMs and public APIs, these previous works have already considered ethical issues when creating the models.</p>
<p>A Appendix</p>
<p>A.1 Advancements in Long-Context LLMs</p>
<p>As LLMs gain global traction, their ability to adeptly process extensive sequences-such as protracted conversation histories or intricate scientific documents-becomes increasingly significant.Recent years have witnessed the emergence of LLMs capable of managing long context windows, a feat made possible by the advent of more powerful GPUs with expanded memory capacities, along with innovations in memory-efficient attention mechanisms (Dao et al., 2022;Dao, 2023).Historically, the context window size for models like GPT-2 encompassed 1024 tokens (Radford et al., 2019), which was then extended to 2048 in GPT-3 (Brown et al., 2020).Modern iterations, such as GPT-4-turbo, boast an impressive 128K token capacity (OpenAI, 2023c), while Claude 2.1 extends this even further to 200K tokens (Anthropic, 2023).Nevertheless, scaling the context window during the pretraining phase remains a daunting task as the computational demands surge quadratically with the length of the attention span, and a majority of texts within standard corpora, like Common Crawl, tend to be comparatively brief.</p>
<p>A novel approach gaining momentum among researchers is the augmentation of the LLMs' context window through the process of continued training or fine-tuning.For instance, Tworkowski et al. (2023) successfully refined the 3B and 7B OpenL-LaMA checkpoints, employing contrastive training techniques to adeptly handle contexts stretching up to 8K tokens.Similarly, Mohtashami and Jaggi (2023) achieved an expansion of the context length from 4K to 32K for LLaMA 7B by incorporating "landmark tokens" that effectively encapsulate blocks of the existing context.These tokens allow for focused fine-tuning of attention mechanisms, which in turn facilitates the selection of pertinent contextual blocks.</p>
<p>Furthermore, Chen et al. ( 2023) introduced a method known as positional interpolation, to be used with LLMs that incorporate Rotary Position Embeddings (RoPE) as their choice of positional encoding (Su et al., 2021).This technique yielded promising outcomes when applied to LLaMA models ranging from 7B to 65B in size, requiring minimal fine-tuning efforts-a mere 1000 optimization steps.A different paradigm, ALiBi (Press et al., 2022), circumvents the necessity of fine-tuning altogether for expanding the context window.By eschewing positional embeddings and instead applying a linear bias to the attention scores-which is proportionate to the distance between tokens-it elegantly adjusts to handle longer contexts.</p>
<p>Lastly, the strategy proposed by Ratner et al. (2022) partitions extensive contexts into several sub-windows, employing the same positional embeddings across them.This innovative reuse of embeddings enables the models to cope with extended contexts without the need for additional finetuning.This collective body of work represents the ongoing evolution of strategies to enhance the capabilities of LLMs in accommodating long context sequences, a critical requirement for their effective deployment in complex, real-world applications.</p>
<p>A.2 Evaluation for Long-Context LLMs</p>
<p>The advent of long-context LLMs has ushered in an era where evaluating performance over extensive text sequences is crucial.Benchmarks like Zero-SCROLLS (Shaham et al., 2023) have emerged to challenge these models' understanding of expansive texts in a zero-shot setting.ZeroSCROLLS extends the foundation laid by the SCROLLS benchmark (Shaham et al., 2022)-originally designed to handle longer texts through fine-tuning-by introducing four new tasks: query-based summarization, multi-hop question answering, sentiment aggregation, and ordering book chapter summaries.It distinguishes itself by focusing on zero-shot performance, using simple natural language prompts and eschewing training data, relying on non-public, high-quality references.</p>
<p>Another contribution to this domain is Long-Bench (Bai et al., 2023), a suite of 21 datasets across 6 categories of tasks such as single-and multi-document question answering, summarization, few-shot learning, specific synthetic tasks, and code completion.What sets LongBench apart is its uniform format for all datasets, promoting a unified and automated evaluation process with metrics like F1 and ROUGE.Bamboo (Dong et al., 2023) also provides a valuable framework for analyzing comprehension over lengthy texts, offering a selection of 10 datasets from 5 diverse activities that range from question answering to hallucination detection, text sorting, language modeling, and code completion.Bamboo specifically tackles potential data contamination by exclusively using sources released no earlier than 2023, maintaining the relevance and contemporaneity of its material.</p>
<p>L-Eval (An et al., 2023) introduces a bifurcated approach to evaluate LLMs, featuring both closedended and open-ended tasks.Closed-ended tasks focus on the model's reasoning and comprehension skills in a protracted context.In contrast, its open-ended tasks provide a variety of summarization challenges that require models to synthesize information from lengthier documents.In-finiteBench (Zhang et al., 2023) is tailored to assess LLMs that process, understand, and infer information from contexts that span over 100,000 tokens.It should be noted that these datasets prioritize the assessment of long-context understanding, and as a result, a significant portion of tokens are used as inputs for the LLMs rather than outputs.</p>
<p>A.3 Baseline Models</p>
<p>All the baselines are prompted with "Write a wellstructured and extensive report to answer the question: [META QUESTION]".We then employ the generated results as the contextual background and force the GPT-4 and GPT-4-turbo to answer the proxy-questions accordingly.Multiple competitive baselines are tested under PROXYQA.</p>
<p>Base LLaMA is a set of open-sourced LLMs pretrained on diverse sources spanning multiple domains (Touvron et al., 2023a,b).The pretraining corpora include the June 2022 Wikipedia dumps, which should enable the LLaMA family to effectively address most 'easy' meta-questions in PROXYQA.Our experiment evaluates LLaMA-7B, LLaMA2-7B, and LLaMA2-13B.</p>
<p>Instruction-Finetuned LLaMA includes Vicuna (Chiang et al., 2023), Alpaca (Taori et al., 2023), LLaMA2-Chat (Touvron et al., 2023b).Vicuna is a chat assistant trained by fine-tuning LLaMA on around 70k user-shared conversations collected from ShareGPT (Eccleston, 2022).Similarly, Alpaca is trained with 52k self-instructed demonstrations adapted text-davinci-003 (Ope-nAI, 2022b).As an extension of base LLaMA2, LLaMA2-Chat is optimized specifically for dialogue usage of over 1 million instructions.</p>
<p>OpenAI APIs includes GPT-3.5-turbo(OpenAI, 2023a), GPT-4 and GPT-4-turbo (OpenAI, 2023b).The default decoding configuration is utilized to generate responses, while the maximum decoding length is set as its maximum limitation.Both GPT-3.5-turbo and GPT-4-turbo are of version 1106, while GPT-4 corresponds to GPT-4-0613.The training data for GPT-4-turbo is up-to-date as of April 2023, while the remaining models are trained with data up to September 2021.</p>
<p>Web-Augmented LLMs utilize external search APIs are evaluated.Specifically, GPT-4 and GPT-4turbo are integrated with the Google Search API under the configuration of ReAct (Yao et al., 2023b).These models are tasked with processing metaquestions, reasoning through search traces, and extracting relevant content from search results from the internet across multiple turns.The implementation is adopted from LangChain (Chase, 2022).In addition to ReAct, the performance of New Bing (creative mode) (Bing, 2023) and Bard with Gemini Pro (Gemini, 2023) are also assessed.</p>
<p>A.4 Scoring Guideline for Human Evaluation and LLM-as-Judges</p>
<p>Scoring Guideline</p>
<p>Please rate the knowledge coverage of the reports provided, using a scale of 0-5.Assess how well the report covers the necessary information related to the question.Knowledge Coverage Scale: 0 -Nonsense: The report offers no useful information and is completely irrelevant to the question. 1 -Poor: The report provides very little useful information and barely addresses the question.</p>
<p>2 -Fair: The report offers some useful information but lacks depth and detail, leaving the question partially unanswered.</p>
<p>3 -Average: The report presents a decent amount of information, addressing the question adequately but not exceptionally.4 -Good: The report provides comprehensive information, covering the question well with appropriate depth and detail.5 -Excellent: The report thoroughly covers all aspects of the question, offering a high level of detail and leaving no gaps in knowledge.</p>
<p>A.5 Prompts for tested LLMs and Evaluator</p>
<p>All the tested baselines are prompted with "Write a well-structured and extensive report to answer the question: [META QUESTION]", while the evaluators are prompted with:</p>
<p>Prompt for Evaluator</p>
<p>Read the provided document and determine whether the statement below is "True" or "False".Use only the information in the text to make your decision.Do not rely on prior knowledge or information outside of the given text.If the text does not provide enough information to make a decision, respond with "Not mentioned".Format your answer as "True", "False", or "Not mentioned".Golden Answer: For each proxy-question, label the golden answer, which refers to the most accurate and relevant information.</p>
<p>Steps for Annotation: a) Choose a topic you are familiar with or interested in.b) Formulate a metaquestion following the criteria mentioned above c) Determine the essential proxyquestions that cover the critical contents of the topic, as per the guidelines above.d) For each proxy-question, label the golden answer.e) Review the meta-question, proxyquestions, and golden answer to ensure they adhere to the guidelines and criteria provided.f) Collaborate with your peers to review and refine the meta-questions and proxy-questions.Verify and supplement the proxy-questions in an alternate way.</p>
<p>A.7 Case Study and Error Analysis</p>
<p>Table 4-9 show the case study of the reports generated by LLaMA2-13B-Chat, New Bing and GPT-4-turbo.Although LLaMA2-13B-Chat generates lengthy contents, very little important information are contained.Therefore GPT-4 evaluator is unable to fetch useful information from the generated contents, result in low accuracy on the proxy-questions.</p>
<p>In contrast, both New Bing and GPT-4-turbo generate more concise results conveyed with essential contents.Specifically, critical hallucination can be observed in the output of the LLaMA2-13B-Chat, while none of the contents can be used to answer the proxy-questions.However, New Bing covers as much points as it can, providing a comprehensive overview of the given topic, while results of GPT-4-turbo is well-structured and offers detailed and helpful introduction to some of the critical contents.</p>
<p>A.8 Distribution of the ProxyQA 51% of meta-questions are categorized as easy, signifying that they can be addressed utilizing information solely from Wikipedia.In contrast, 49% of meta-questions are deemed difficult.Moreover, 53% of these questions are time-sensitive, as their answers may dynamically alter over time.A representative example of a time-sensitive question is: "Could you kindly offer a comprehensive overview of the historical development of pre-training in NLP, encompassing notable contributions?"</p>
<p>A.9 Maintaince of the Leaderboard</p>
<p>Concerning meta-questions of varying difficulty levels, the average accuracy of evaluators will be reported for both easy and hard subsets of PROXYQA.</p>
<p>In addition, for time-sensitive questions, proxyquestions will be incrementally updated, and the leaderboard will be dynamically adjusted to evaluate the LLMs' possession of current knowledge.</p>
<p>The leaderboard is updated on a weekly basis, while the proxy-question of time-sensitive metaquestion is incrementally updated on a bi-monthly basis.Each periodic update of proxy-questions will also renew the leaderboard.</p>
<p>Question: Data parallelism, model parallelism, and pipeline parallelism play a vital role in the training of large-scale language models.What are the representative works and frameworks among these technologies?Please introduce these technologies and frameworks in detail.</p>
<p>LLaMA2-13B-Chat-Page 1 Introduction:</p>
<p>The training of large-scale language models has become increasingly popular in recent years due to their wide range of applications in natural language processing (NLP) tasks.However, training such models can be computationally expensive and time-consuming, especially when dealing with large datasets.To address this challenge, researchers have proposed various parallelization techniques to speed up the training process.In this report, we will focus on three key parallelization techniques: data parallelism, model parallelism, and pipeline parallelism.We will introduce representative works and frameworks that utilize these techniques and discuss their advantages and limitations.Data Parallelism: Data parallelism is a technique that involves dividing the input data into smaller chunks and processing them in parallel across multiple devices or machines.This technique is particularly useful for training large-scale language models, as it allows for faster training times and more efficient use of computational resources.One of the most popular frameworks for data parallelism is TensorFlow, which is an open-source machine learning library developed by Google.TensorFlow provides a built-in data parallelism feature called TensorFlow Estimator, which allows users to parallelize their training processes across multiple machines.Another popular framework for data parallelism is PyTorch, which provides a module called PyTorch Distributed that allows users to parallelize their training processes across multiple devices.Representative Works: One of the most influential works in data parallelism for large-scale language models is the paper "Distributed Training of Deep Neural Networks" by Google researchers (2016).This paper introduced the concept of using data parallelism to train deep neural networks on large datasets, and demonstrated the effectiveness of this approach for training large-scale language models.Another influential work is the paper "Deep Learning with CUDA: General Purpose Training of Neural Networks on GPU Clusters" by NVIDIA researchers (2017).This paper introduced the concept of using GPU clusters for training large-scale language models, and demonstrated the effectiveness of this approach for improving training times and reducing computational costs.Model Parallelism: Model parallelism is a technique that involves dividing the model into smaller parts and processing them in parallel across multiple devices or machines.This technique is particularly useful for training large-scale language models, as it allows for faster training times and more efficient use of computational resources.One of the most popular frameworks for model parallelism is the open-source machine learning library TensorFlow, which provides a built-in feature called TensorFlow Model Parallelism.This feature allows users to parallelize their training processes across multiple machines, and has been used in a number of influential works for training large-scale language models.Another popular framework for model parallelism is the PyTorch framework, which provides a module called PyTorch Model Parallel that allows users to parallelize their training processes across multiple devices.LLaMA2-13B-Chat-Page 2 Representative Works: One of the most influential works in model parallelism for large-scale language models is the paper "Model Parallelism for Training Large-Scale Neural Networks" by Google researchers (2017).This paper introduced the concept of using model parallelism to train large-scale language models, and demonstrated the effectiveness of this approach for improving training times and reducing computational costs.Pipeline Parallelism: Another influential work is the paper "Scaling Up Neural Machine Translation with Model Parallelism" by Google researchers (2018).This paper introduced the concept of using model parallelism to train large-scale neural machine translation models, and demonstrated the effectiveness of this approach for improving translation quality and reducing computational costs.Pipeline parallelism is a technique that involves dividing the training process into smaller stages and processing them in parallel across multiple devices or machines.This technique is particularly useful for training large-scale language models, as it allows for faster training times and more efficient use of computational resources.One of the most popular frameworks for pipeline parallelism is the open-source machine learning library TensorFlow, which provides a built-in feature called TensorFlow Pipeline Parallelism.This feature allows users to parallelize their training processes across multiple machines, and has been used in a number of influential works for training large-scale language models.Another popular framework for pipeline parallelism is the PyTorch framework, which provides a module called PyTorch Pipeline that allows users to parallelize their training processes across multiple devices.Representative Works: One of the most influential works in pipeline parallelism for large-scale language models is the paper "Pipeline Parallelism for Training Large-Scale Neural Networks" by Google researchers (2018).This paper introduced the concept of using pipeline parallelism to train large-scale language models, and demonstrated the effectiveness of this approach for improving training times and reducing computational costs.Another influential work is the paper "Scaling Up Neural Machine Translation with Pipeline Parallelism" by Google researchers (2019).This paper introduced the concept of using pipeline parallelism to train large-scale neural machine translation models, and demonstrated the effectiveness of this approach for improving translation quality and reducing computational costs.</p>
<p>Conclusion:</p>
<p>In conclusion, data parallelism, model parallelism, and pipeline parallelism are three key parallelization techniques that have played a vital role in the training of large-scale language models.These techniques have been used in a number of influential works, and have demonstrated the effectiveness of parallelizing the training process to improve training times and reduce computational costs.As the field of NLP continues to evolve, it is likely that these parallelization techniques will continue to play an important role in the training of large-scale language models.Table 5: Case study of LLaMA2-13B-Chat New Bing -Page 1 Hello, this is Bing.I can help you with your question.Data parallelism, model parallelism, and pipeline parallelism are three types of parallelism methods that can be used to train large-scale language models efficiently on multiple GPUs or other devices.</p>
<p>Here is a brief introduction of these technologies and some representative works and frameworks that use them:</p>
<p>• Data parallelism (DP): This is the simplest and most common form of parallelism, where the same model is replicated on multiple devices, and each device processes a different slice of the input data.The gradients are then aggregated and synchronized across the devices to update the model parameters.This method is easy to implement and can scale well with the number of devices, but it has some limitations, such as requiring large memory to store the full model and communication overhead for gradient synchronization.Some examples of frameworks that support data parallelism are PyTorch's DistributedDataParallel and TensorFlow's MirroredStrategy.</p>
<p>• Model parallelism (MP): This is a more complex form of parallelism, where the model is split into smaller parts and distributed across multiple devices, such that each device only stores and computes a subset of the model parameters.This method can overcome the memory limitation of data parallelism and enable training very large models that cannot fit on a single device.However, it also introduces more challenges, such as how to partition the model, how to minimize the communication cost, and how to balance the workload among the devices.There are different types of model parallelism, such as tensor parallelism and pipeline parallelism, which will be explained below.Some examples of frameworks that support model parallelism are NVIDIA's Megatron-LM and Microsoft's DeepSpeed.</p>
<p>• Tensor parallelism (TP): This is a type of model parallelism where each tensor (such as a weight matrix or an activation vector) is split into smaller chunks along one or more dimensions, and each chunk is assigned to a different device.For example, a weight matrix of size [m, n] can be split into [m/k, n/k] chunks, where k is the number of devices, and each device only stores and computes one chunk.This method can reduce the memory footprint and communication cost by a factor of k, but it also requires modifying the model architecture and the computation logic to handle the tensor sharding.Some examples of works that use tensor parallelism are gShard and Mesh-TensorFlow.</p>
<p>• Pipeline parallelism (PP): This is another type of model parallelism where the model is split into smaller stages (such as layers or groups of layers) and distributed across multiple devices, such that each device only stores and computes one stage of the model.For example, a model with L layers can be split into S stages, where each stage contains L/S layers, and each device only processes one stage.The input data is then fed into the first device, and the output of each device is passed to the next device in a pipelined fashion.This method can also reduce the memory footprint and communication cost by a factor of S, but it also requires careful scheduling and synchronization of the pipeline stages to maximize the throughput and avoid idle periods.Some examples of works that use pipeline parallelism are PipeDream, GPipe, and PipeEdge.New Bing -Page 2 These parallelism methods can be combined in different ways to achieve higher efficiency and scalability.For example, one can use data parallelism and tensor parallelism together to train a large model on a large number of devices, as done in Megatron-LM.Alternatively, one can use data parallelism and pipeline parallelism together to train a deep model with high throughput, as done in GPipe.Furthermore, one can use tensor parallelism and pipeline parallelism together to train a very large and deep model with low memory and communication cost, as done in DeepSpeed.I hope this helps you understand the basics of parallelism methods for large-scale language model training.If you want to learn more, you can check out the links I provided or search for more resources on Bing.Have a nice day!</p>
<p>Figure 1 :
1
Figure1: Prior efforts assess generated content by matching it with references through human evaluation or automated metrics.PROXYQA evaluates the knowledge coverage and informativeness by checking if generated contents contain sufficient information to answer a set of proxy questions.</p>
<p>Film</p>
<p>a First-person shooter (FPS) game, could you generate a detailed report about it?Technology … introduce the H100 GPU and compare it with TPU-V5e in detail.…compare the VR headset from some of the biggest tech companies.Music Who are the best-selling music artists of all time?And what are their genres?Economics What impact do mega sports events (like the Olympics or World Cup) have on host cities economically?Which countries hold the record in the Best International Feature Film category at the Oscars as of 2022?</p>
<p>versions of the LLaMAs series demonstrate a limited capacity to produce long-form content.However, notable enhancements are achieved through instruction-based supervised fine-tuning (SFT), as the Vicuna-13B and LLaMA2-13B-Chat transcend most other open-sourced models, evidencing their superior capability in delivering acceptable content.However, compared to the proprietary models, the open-source LLMs far lag behind GPT models.Even the GPT-3.5-Turbooutperforms the entire suite of open-source LLMs by a significant margin, and GPT-4-Turbo maintains a substantial lead.This underscores the considerable gap that open-source LLMs must bridge to match the performance of their proprietary counterparts.</p>
<p>Figure 3 Figure 3 :
33
Figure 3: Performance of LLMs on different domains.</p>
<p>Figure 4 :
4
Figure 4: Performance difference on the expertsconsolidated and single-expert-focused set.</p>
<p>Figure 5 :
5
Figure 5: Comparison of win rate of various evaluation methods.GPT-evaluators are highly overconfident in the results produced by GPT-4-Turbo, while PROXYQA significantly correlated with human preference.</p>
<p>Document: [generated_report] Statement: [proxy_question] A.6 Annotation Guideline for Formulating Meta-Questions and Proxy-questions The meta-questions should be based on the topic the experts are most familiar with or keen on.Meta-question: The meta-question should be: a) Answerable through thorough research.b) Aligned with real-life scenarios.c) Avoid of offensive or ethical concerns.d) Without an absolute or unique answer.e) Addressable in long-form reports or articles.f) Ensure the questions are open-ended, promoting in-depth research and discussion.Proxy-questions: For each meta-question, determine the essential proxy-questions that cover the critical contents of the topic.proxy-questions should be: a) Directly related to the meta-question.b) Comprehensive enough to cover different angles of the meta-question.</p>
<p>Table 1 :
1
Evaluation of various LLMs' knowledge coverage and informativeness on PROXYQA, accuracy on the easy and hard splits are reported.
GPT-4GPT-4-TurboAverageEasy Hard avg.Easy Hard avg.Easy Hard avg.Base LLaMALLaMA-7B5.250.683.055.891.233.645.570.963.34LLaMA2-7B4.740.552.725.380.553.045.060.552.88LLaMA2-13B6.151.653.976.911.374.246.531.514.11Instruction-Finetuned LLaMAAlpaca-7B12.42 5.629.1414.60 9.3312.0513.51 7.4810.60Vicuna-13B19.85 17.15 18.5422.66 21.26 21.9921.25 19.20 20.26LLaMA2-7B-Chat21.25 16.74 19.0720.23 18.11 19.2020.74 17.42 19.14LLaMA2-13B-Chat21.13 16.87 19.0722.02 17.42 19.8021.57 17.15 19.44GPT APIsGPT-3.5-Turbo25.61 21.40 23.5726.12 22.36 24.3025.87 22.97 23.94GPT-430.35 23.05 26.8230.35 23.55 27.5530.35 23.80 27.19GPT-4-Turbo35.21 31.69 33.5034.83 33.88 34.3735.02 32.78 33.94Web-Augmented LLMsReAct (GPT-4)20.74 13.72 17.3520.49 13.17 16.9520.61 13.44 17.15ReAct (GPT-4-Turbo)23.94 18.11 21.1323.56 18.79 21.2623.75 18.45 21.19Bard (Gemini Pro)26.63 22.22 24.5025.48 25.51 25.5025.06 23.87 25.00New Bing (Creative Mode)39.56 37.72 38.6740.33 39.78 40.0639.95 38.75 39.37</p>
<p>Table 2 :
2
Average word count of the generated reports.
Avg. Len. Acc.LLaMA2-13B1906.874.11LLaMA2-13B-Chat869.4219.44Vicuna-13B727.8420.26GPT-3.5-Turbo823.3223.94GPT-4744.0027.19GPT-4-Turbo1029.4733.94ReAct (GPT-4-Turbo)355.8021.19Bard (Gemini Pro)922.8325.00New Bing (Creative)1167.6539.37</p>
<p>Table
SelfG2G G2M M2GGPT-S48.65 45.49 47.62 30.00GPT-B51.17 43.86 36.66 36.66ProxyQA 88.00 66.00 63.33 66.19Human52.19---</p>
<p>Table 4 :
4
Case study of LLaMA2-13B-Chat</p>
<p>Table 6 :
6
Case study of New Bing</p>
<p>AcknowledgementsWe would like to express our profound gratitude to the anonymous reviewers for their invaluable and insightful feedback.This research has been partially funded by the Research Grants Council of the Hong Kong SAR under Grant GRF 11217823 and the Collaborative Research Fund C1042-23GF.Additional support was provided by the National Natural Science Foundation of China under Grant 62371411, the InnoHK initiative, the Government of the HKSAR, and the Laboratory for AI-Powered Financial Technologies.A.10 IntroductionThe training of large-scale language models, such as GPT-3, BERT, and T5, requires significant computational resources due to the vast number of parameters and the size of the datasets involved.To address this challenge, parallelism techniques have been developed to distribute the computational workload across multiple processors or machines.The three main types of parallelism used in training large-scale language models are data parallelism, model parallelism, and pipeline parallelism.This report provides an in-depth look at these technologies and their representative works and frameworks.A.11 Data ParallelismA.11.1 Definition and OverviewData parallelism involves splitting the training data across multiple processors or devices, with each processor holding a complete copy of the model.Each processor computes the gradients based on its subset of the data, and the gradients are then aggregated to update the model parameters.A.12 Model Parallelism A.12.1 Definition and OverviewA.11.2 Representative Works and FrameworksModel parallelism involves splitting the model itself across multiple processors or devices.Each processor is responsible for a portion of the model's layers or parameters.This technique is particularly useful for models that are too large to fit into the memory of a single device.A.12.2 Representative Works and FrameworksA.14 ConclusionData parallelism, model parallelism, and pipeline parallelism are essential techniques for training large-scale language models.Each technique addresses different challenges associated with the size of the model and the dataset.Frameworks like Horovod, PyTorch DDP, TensorFlow's tf.distribute.Strategy, GPipe, Megatron-LM, Mesh-TensorFlow, DeepSpeed, and FairScale provide the necessary tools for implementing these parallelism strategies effectively.As the demand for larger and more complex models continues to grow, these parallelism techniques and frameworks will play a crucial role in enabling the development of next-generation AI systems.Researchers and practitioners must carefully choose the appropriate parallelism strategy based on the specific requirements of their models and the available computational resources.With ongoing advancements in distributed computing and parallel processing, we can expect to see further improvements in the efficiency and scalability of large-scale language model training.
L-eval: Instituting standardized evaluation for long context language models. Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu, 10.48550/ARXIV.2307.11088CoRR, abs/2307.110882023</p>
<p>Claude 2.1 with 200K Context Window. Anthropic, 2023</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li, 10.48550/ARXIV.2308.14508CoRR, abs/2308.145082023</p>
<p>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005. the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005Ann Arbor, Michigan, USAAssociation for Computational Linguistics2005. June 29, 2005</p>
<p>Ai-powered bing with chatgpt's gpt-4. Bing , 2023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Evaluation of text generation: A survey. Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, 2020. 2006.14799</p>
<p>. Harrison Chase, 2022Langchain</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, 10.48550/ARXIV.2306.15595CoRR, abs/2306.155952023</p>
<p>Can large language models be an alternative to human evaluations?. David Cheng, -Han Chiang, Hung-Yi Lee, 10.18653/V1/2023.ACL-LONG.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. Tri Dao, 10.48550/ARXIV.2307.08691CoRR, abs/2307.086912023</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, Christopher Ré, NeurIPS. 2022</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, 10.18653/V1/2021.NAACL-MAIN.365Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational Linguistics2021. June 6-11, 2021</p>
<p>Model criticism for long-form text generation. Yuntian Deng, Volodymyr Kuleshov, Alexander M Rush, 10.18653/V1/2022.EMNLP-MAIN.815Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, 10.48550/ARXIV.2309.13345CoRR, abs/2309.133452023</p>
<p>. Dom Eccleston, 2022Sharegpt</p>
<p>ELI5: long form question answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, 10.18653/V1/P19-1346Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy2019a. July 28-August 2, 20191Association for Computational Linguistics</p>
<p>Strategies for structuring story generation. Angela Fan, Mike Lewis, Yann N Dauphin, 10.18653/V1/P19-1254Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy; PapersAssociation for Computational Linguistics2019b. July 28-August 2, 20191</p>
<p>Gemini: A family of highly capable multimodal models. Gemini, 2023</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>Non-expert evaluation of summarization systems is risky. Dan Gillick, Yang Liu, Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk. the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical TurkLos AngelesAssociation for Computational Linguistics2010</p>
<p>Evaluating factuality in generation with dependency-level entailment. Tanya Goyal, Greg Durrett, 10.18653/v1/2020.findings-emnlp.3222020</p>
<p>Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics</p>
<p>SNaC: Coherence error detection for narrative summarization. Tanya Goyal, Junyi , Jessy Li, Greg Durrett, 10.18653/v1/2022.emnlp-main.29Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu DhabiAssociation for Computational Linguistics2022United Arab Emirates</p>
<p>PLANET: dynamic content planning in autoregressive transformers for long-form text generation. Zhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Hua Wu, Lifu Huang, 10.18653/V1/2022.ACL-LONG.163Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>PAIR: planning and iterative refinement in pre-trained transformers for long text generation. Xinyu Hua, Lu Wang, 10.18653/V1/2020.EMNLP-MAIN.57Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hyoukjoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, Zhifeng Chen, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>Best practices for crowd-based evaluation of German summarization: Comparing crowd, expert and automatic evaluation. Neslihan Iskender, Tim Polzehl, Sebastian Mller, 10.18653/v1/2020.eval4nlp-1.16Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems. the First Workshop on Evaluation and Comparison of NLP SystemsOnline. Association for Computational Linguistics2020</p>
<p>BlonDe: An automatic evaluation metric for document-level machine translation. Yuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, Ming Zhou, 10.18653/v1/2022.naacl-main.111Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Plan ahead: Self-supervised text planning for paragraph completion task. Dongyeop Kang, Eduard H Hovy, 10.18653/V1/2020.EMNLP-MAIN.529Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020. November 16-20, 20202020Association for Computational Linguistics</p>
<p>Claim matching beyond English to scale global fact-checking. Ashkan Kazemi, Kiran Garimella, Devin Gaffney, Scott Hale, 10.18653/v1/2021.acl-long.347Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021</p>
<p>Hurdles to progress in long-form question answering. Kalpesh Krishna, Aurko Roy, Mohit Iyyer, 10.18653/v1/2021.naacl-main.393Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>SummaC: Re-visiting NLIbased models for inconsistency detection in summarization. Philippe Laban, Tobias Schnabel, Paul N Bennett, Marti A Hearst, 10.1162/tacl_a_00453Transactions of the Association for Computational Linguistics. 102022</p>
<p>QASA: advanced question answering on scientific articles. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, Moontae Lee, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USA; Barcelona, SpainAssociation for Computational Linguistics2023. July 2023. 2004202Text Summarization Branches Out</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, Nigel Collier, arXiv:2403.169502024arXiv preprint</p>
<p>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Landmark attention: Random-access infinite context length for transformers. Amirkeivan Mohtashami, Martin Jaggi, 10.48550/ARXIV.2305.16300CoRR, abs/2305.163002023</p>
<p>Webgpt: Browserassisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, S Arun Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, ArXiv, abs/2112.09332Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2021</p>
<p>MS MARCO: A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). CEUR Workshop Proceedings. CEUR-WS.org. the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)Barcelona, Spain2016. December 9, 20161773</p>
<p>. 10.48550/arXiv.2303.08774CoRR, abs/2303.087742023a. GPT-3.5 Turbo. OpenAI. 2023bOpenAIGPT-4 Technical Report</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2023c. 2022. April 25-29, 2022OpenAIGPT-4 Turbo with 128K Context. Ofir Press, Noah A. Smith, and Mike Lewis. OpenReview.net</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Free-marginal multirater kappa (multirater kfree): An alternative to fleiss fixedmarginal multirater kappa. Justus Randolph, 20104</p>
<p>Kevin Leyton-Brown, and Yoav Shoham. 2022. Parallel context windows for large language models. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud D Karpas, Amnon Shashua, Annual Meeting of the Association for Computational Linguistics. </p>
<p>Oier Lopez de Lacalle, and Eneko Agirre. 2023. NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark. Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational LinguisticsDecember 6-10, 2023</p>
<p>Zeroscrolls: A zero-shot benchmark for long text understanding. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, Omer Levy, 10.48550/ARXIV.2305.14196CoRR, abs/2305.141962023</p>
<p>SCROLLS: standardized comparison over long language sequences. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, Omer Levy, 10.18653/V1/2022.EMNLP-MAIN.823Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, Lidong Bing, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Megatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, CoRR, abs/1909.080532019</p>
<p>ASQA: factoid questions meet long-form answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, 10.18653/V1/2022.EMNLP-MAIN.566Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu, ArXiv, abs/2104.098642021</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, 10.48550/arXiv.2302.13971CoRR, abs/2302.13971</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/arXiv.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>Focused transformer: Contrastive training for context scaling. Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Milos, 10.48550/ARXIV.2307.03170CoRR, abs/2307.031702023</p>
<p>A critical evaluation of evaluations for long-form question answering. Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi, 10.18653/V1/2023.ACL-LONG.181Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>yond goldfish memory: Long-term open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, 10.18653/V1/2022.ACL-LONG.356Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>MEGATRON-CNTRL: controllable story generation with external knowledge using large-scale language models. Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, Bryan Catanzaro, 10.18653/V1/2020.EMNLP-MAIN.226Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023a. May 1-5, 2023OpenReview.net</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. 2021. December 6-14, 20212021</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>. Xinrong Zhang, Yingfa Chen, Shengding Hu, Qihao Wu, Junhao Chen, Zihang Xu, Zhenning Dai, Xu Han, Shuo Wang, Zhiyuan Liu, Maosong Sun, 2023Infinitebench: 128k long-context benchmark for language models</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Model Parallelism -Hugging Face. </p>
<p>Efficient Large-Scale Language Model Training on GPU Clusters -arXiv.org. </p>
<p>PipeEdge: Pipeline Parallelism for Large-Scale Model Inference on. </p>
<p>Efficient Large-Scale Language Model Training on GPU Clusters -arXiv.org. </p>
<p>Optimizing Distributed Training on Frontier for Large Language Models. </p>
<p>arXiv:2102.07988v2[cs.LG]28Case study of New Bing GPT. Sep 20217Abstract</p>            </div>
        </div>

    </div>
</body>
</html>
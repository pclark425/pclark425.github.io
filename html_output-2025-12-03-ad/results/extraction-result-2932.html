<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2932 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2932</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2932</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-282384689</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.21306v1.pdf" target="_blank">PARL: Prompt-based Agents for Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated high performance on tasks expressed in natural language, particularly in zero- or few-shot settings. These are typically framed as supervised (e.g., classification) or unsupervised (e.g., clustering) problems. However, limited work evaluates LLMs as agents in reinforcement learning (RL) tasks (e.g., playing games), where learning occurs through interaction with an environment and a reward system. While prior work focused on representing tasks that rely on a language representation, we study structured, non-linguistic reasoning - such as interpreting positions in a grid world. We therefore introduce PARL (Prompt-based Agent for Reinforcement Learning), a method that uses LLMs as RL agents through prompting, without any fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling the model to learn through trial-and-error interaction. We evaluate PARL on three standard RL tasks that do not entirely rely on natural language. We show that it can match or outperform traditional RL agents in simple environments by leveraging pretrained knowledge. However, we identify performance limitations in tasks that require complex mathematical operations or decoding states and actions.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2932.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2932.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-based Agent for Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based RL agent that keeps an LLM frozen and encodes the task description plus a concatenated interaction history (states, actions, rewards) in the prompt so the LLM can learn via in-context trial-and-error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PARL constructs a prompt P_PARL = T ⊕ h where T describes the goal, action space, state representation and rewards, and h is a concatenated episodic history of (s_t, a_t, r_t). At each step the LLM (frozen) is asked to generate an action; the resulting state and reward are appended to the prompt for subsequent steps/episodes. Two state-decoding variants are supported: (1) raw environment state passed to LLM, and (2) an external script that decodes states into natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Blackjack; Frozen Lake; Taxi (Gymnasium toy-text environments)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Blackjack: card game with states = (player sum, dealer visible card, usable Ace), actions Hit/Stick, rewards +1/0/-1. Frozen Lake: grid navigation with slippery transitions, goal reward +1, holes end episode. Taxi: grid taxi task with encoded state (taxi position, passenger location, destination), actions include movement, Pickup, Drop-off, with step penalties and large penalties for illegal Pickup/Drop-off.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic history (prompt-based, in-context memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>A concatenated textual history h = {(s_t, a_t, r_t), ...} appended to the static task description T in the prompt. Episodes accumulate: Full-History experiment concatenates prior episodes so episode n's prompt includes all previous (s,a,r) entries; alternative variants include No-History (only current-episode steps) and History-with-Random-Rewards (rewards replaced by random values in history). There is no separate indexing structure, embedding store, or retrieval index—memory is simply the prompt text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>No explicit retrieval module: memory is retrieved by being present in the LLM input context (recency / explicit concatenation). Retrieval is therefore recency/order-based via what is included in the prompt; no semantic search or attention outside the model's internal attention over the input tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Grows with number of appended episodes; experiments used up to 100 training episodes where Full History included the previous 99 episodes (practically limited by LLM token window). No fixed capacity other than the model's input token limit.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Per-step and per-episode logs of state, action, and reward tuples (s_t, a_t, r_t); in script-decoding variant the stored state entries are natural-language descriptions produced by an external decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Blackjack (Full History): average reward 0.2 over 100 evaluation episodes; Frozen Lake (Full History): average reward 0.66; Taxi (Full History): average reward −696.6 (high negative cumulative reward due to penalties).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Blackjack (No History): average reward −0.09; Frozen Lake (No History): average reward 0.0; Taxi (No History): average reward −568.95. (Also a 'History with Random Rewards' variant: Blackjack −0.12, Frozen Lake 0.0, Taxi −807.5.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Blackjack: Full History vs No History = +0.29 absolute average reward (0.2 vs −0.09); vs Random-Rewards = +0.32. Frozen Lake: Full History vs No History = +0.66 absolute. Taxi: mixed — No History outperformed Full History (No History −568.95 vs Full −696.6), but Full History was better than Random-Rewards by +110.9 (−696.6 vs −807.5).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Including accurate episodic history in the prompt substantially improves PARL performance on tasks where the LLM can interpret states (e.g., Blackjack, Frozen Lake) — Full History outperforms No History and random-reward history. Script-based (natural-language) decoding of states into the prompt further accelerates learning. However, long or noisy histories can harm learning in complex/high-dimensional tasks (Taxi). The method is sample-efficient (PARL trained with 100 episodes vs SOTA baselines trained with ~100,000 episodes) when memory and state decoding are effective.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Memory is unstructured concatenation limited by the LLM token window (scalability issues). Long histories can introduce noise and degrade performance in complex environments. The approach places decoding burden on the LLM (self-decoding fails for complex encodings) unless an external script is used. No explicit retrieval or summarization mechanisms are used, so older relevant episodes may be truncated or buried.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARL: Prompt-based Agents for Reinforcement Learning', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2932.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2932.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published LLM-agent method that enables agents to self-critique and update behavior over time through memory-based adaptation (no fine-tuning), improving performance on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described as an agent that produces self-critiques about its decisions and uses those critiques to update future behavior via a memory-based adaptation mechanism. The paper cites Reflexion as enabling self-improving behavior without additional fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFWorld; HotPotQA (domains reported as effective in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>ALFWorld: instruction-following environment (simulated interactive environment). HotPotQA: multi-hop question answering across documents (mentioned as a domain where Reflexion was effective).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based / memory-based adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Paper states Reflexion retains self-critiques and past decision traces for adaptation (exact storage structure not described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in the referenced work to improve performance on ALFWorld and HotPotQA, but no numeric metrics are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Memory-based self-critique and adaptation can improve agent performance without fine-tuning, according to the referenced work; cited here as related work that demonstrates memory-based improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARL: Prompt-based Agents for Reinforcement Learning', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2932.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2932.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that interleaves chain-of-thought style reasoning traces with actions, improving decision-making and interpretability of LLM-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interleaves reasoning (textual chain-of-thought) with actions/outputs so the LLM both thinks and acts in the same prompt stream; improves task performance and reduces hallucinations in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Paper cites ReAct as a reasoning+action approach that improves agent behavior, but ReAct is not described here as using an explicit external memory mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARL: Prompt-based Agents for Reinforcement Learning', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2932.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2932.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-MCTS (LLM-informed Monte Carlo Tree Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses an LLM as a commonsense-informed world model and heuristic policy within Monte Carlo Tree Search to improve planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as commonsense knowledge for large-scale task planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LLM to provide commonsense knowledge and heuristic guidance inside an MCTS planner (LLM supplies world-model rollouts or heuristics to guide tree search). Cited as outperforming standard MCTS and pure LLM policies in complex planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Applied to complex planning tasks in the referenced work; no specific text-game benchmark is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as a hybrid that leverages LLM knowledge within a planning procedure; not described here as a memory-based agent in the same sense as PARL or Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARL: Prompt-based Agents for Reinforcement Learning', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2932.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2932.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdaPlanner: Adaptive planning from feedback with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that incrementally refines high-level plans (encoded as prompts) based on feedback from the environment, using that feedback to adjust future plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaplanner: Adaptive planning from feedback with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AdaPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Represents plans as prompt sequences of high-level steps and refines them iteratively by comparing predicted outcomes against actual outcomes; feedback informs plan updates.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Used for planning tasks; the paper references it as related work on incremental plan refinement with feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited for iterative refinement from feedback; not described here with details on memory architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PARL: Prompt-based Agents for Reinforcement Learning', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Large language models as commonsense knowledge for large-scale task planning <em>(Rating: 2)</em></li>
                <li>Adaplanner: Adaptive planning from feedback with language models <em>(Rating: 2)</em></li>
                <li>clembench: Using game play to evaluate chat-optimized language models as conversational agents <em>(Rating: 1)</em></li>
                <li>Benchmarking large language model (llm) performance for game playing via tic-tac-toe <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2932",
    "paper_id": "paper-282384689",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "PARL",
            "name_full": "Prompt-based Agent for Reinforcement Learning",
            "brief_description": "A prompting-based RL agent that keeps an LLM frozen and encodes the task description plus a concatenated interaction history (states, actions, rewards) in the prompt so the LLM can learn via in-context trial-and-error.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PARL",
            "agent_description": "PARL constructs a prompt P_PARL = T ⊕ h where T describes the goal, action space, state representation and rewards, and h is a concatenated episodic history of (s_t, a_t, r_t). At each step the LLM (frozen) is asked to generate an action; the resulting state and reward are appended to the prompt for subsequent steps/episodes. Two state-decoding variants are supported: (1) raw environment state passed to LLM, and (2) an external script that decodes states into natural-language descriptions.",
            "base_llm_model": "GPT-4o",
            "base_llm_size": null,
            "text_game_name": "Blackjack; Frozen Lake; Taxi (Gymnasium toy-text environments)",
            "text_game_description": "Blackjack: card game with states = (player sum, dealer visible card, usable Ace), actions Hit/Stick, rewards +1/0/-1. Frozen Lake: grid navigation with slippery transitions, goal reward +1, holes end episode. Taxi: grid taxi task with encoded state (taxi position, passenger location, destination), actions include movement, Pickup, Drop-off, with step penalties and large penalties for illegal Pickup/Drop-off.",
            "uses_memory": true,
            "memory_type": "episodic history (prompt-based, in-context memory)",
            "memory_architecture": "A concatenated textual history h = {(s_t, a_t, r_t), ...} appended to the static task description T in the prompt. Episodes accumulate: Full-History experiment concatenates prior episodes so episode n's prompt includes all previous (s,a,r) entries; alternative variants include No-History (only current-episode steps) and History-with-Random-Rewards (rewards replaced by random values in history). There is no separate indexing structure, embedding store, or retrieval index—memory is simply the prompt text.",
            "memory_retrieval_mechanism": "No explicit retrieval module: memory is retrieved by being present in the LLM input context (recency / explicit concatenation). Retrieval is therefore recency/order-based via what is included in the prompt; no semantic search or attention outside the model's internal attention over the input tokens.",
            "memory_capacity": "Grows with number of appended episodes; experiments used up to 100 training episodes where Full History included the previous 99 episodes (practically limited by LLM token window). No fixed capacity other than the model's input token limit.",
            "what_is_stored_in_memory": "Per-step and per-episode logs of state, action, and reward tuples (s_t, a_t, r_t); in script-decoding variant the stored state entries are natural-language descriptions produced by an external decoder.",
            "performance_with_memory": "Blackjack (Full History): average reward 0.2 over 100 evaluation episodes; Frozen Lake (Full History): average reward 0.66; Taxi (Full History): average reward −696.6 (high negative cumulative reward due to penalties).",
            "performance_without_memory": "Blackjack (No History): average reward −0.09; Frozen Lake (No History): average reward 0.0; Taxi (No History): average reward −568.95. (Also a 'History with Random Rewards' variant: Blackjack −0.12, Frozen Lake 0.0, Taxi −807.5.)",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Blackjack: Full History vs No History = +0.29 absolute average reward (0.2 vs −0.09); vs Random-Rewards = +0.32. Frozen Lake: Full History vs No History = +0.66 absolute. Taxi: mixed — No History outperformed Full History (No History −568.95 vs Full −696.6), but Full History was better than Random-Rewards by +110.9 (−696.6 vs −807.5).",
            "key_findings_about_memory": "Including accurate episodic history in the prompt substantially improves PARL performance on tasks where the LLM can interpret states (e.g., Blackjack, Frozen Lake) — Full History outperforms No History and random-reward history. Script-based (natural-language) decoding of states into the prompt further accelerates learning. However, long or noisy histories can harm learning in complex/high-dimensional tasks (Taxi). The method is sample-efficient (PARL trained with 100 episodes vs SOTA baselines trained with ~100,000 episodes) when memory and state decoding are effective.",
            "memory_limitations": "Memory is unstructured concatenation limited by the LLM token window (scalability issues). Long histories can introduce noise and degrade performance in complex environments. The approach places decoding burden on the LLM (self-decoding fails for complex encodings) unless an external script is used. No explicit retrieval or summarization mechanisms are used, so older relevant episodes may be truncated or buried.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2932.0",
            "source_info": {
                "paper_title": "PARL: Prompt-based Agents for Reinforcement Learning",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A previously published LLM-agent method that enables agents to self-critique and update behavior over time through memory-based adaptation (no fine-tuning), improving performance on some benchmarks.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Described as an agent that produces self-critiques about its decisions and uses those critiques to update future behavior via a memory-based adaptation mechanism. The paper cites Reflexion as enabling self-improving behavior without additional fine-tuning.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "ALFWorld; HotPotQA (domains reported as effective in the referenced work)",
            "text_game_description": "ALFWorld: instruction-following environment (simulated interactive environment). HotPotQA: multi-hop question answering across documents (mentioned as a domain where Reflexion was effective).",
            "uses_memory": true,
            "memory_type": "reflection-based / memory-based adaptation",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": "Paper states Reflexion retains self-critiques and past decision traces for adaptation (exact storage structure not described in this paper).",
            "performance_with_memory": "Reported in the referenced work to improve performance on ALFWorld and HotPotQA, but no numeric metrics are provided in this paper.",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Memory-based self-critique and adaptation can improve agent performance without fine-tuning, according to the referenced work; cited here as related work that demonstrates memory-based improvement.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2932.1",
            "source_info": {
                "paper_title": "PARL: Prompt-based Agents for Reinforcement Learning",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct",
            "brief_description": "An approach that interleaves chain-of-thought style reasoning traces with actions, improving decision-making and interpretability of LLM-based agents.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "Interleaves reasoning (textual chain-of-thought) with actions/outputs so the LLM both thinks and acts in the same prompt stream; improves task performance and reduces hallucinations in interactive tasks.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Paper cites ReAct as a reasoning+action approach that improves agent behavior, but ReAct is not described here as using an explicit external memory mechanism.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2932.2",
            "source_info": {
                "paper_title": "PARL: Prompt-based Agents for Reinforcement Learning",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "LLM-MCTS",
            "name_full": "LLM-MCTS (LLM-informed Monte Carlo Tree Search)",
            "brief_description": "An approach that uses an LLM as a commonsense-informed world model and heuristic policy within Monte Carlo Tree Search to improve planning.",
            "citation_title": "Large language models as commonsense knowledge for large-scale task planning",
            "mention_or_use": "mention",
            "agent_name": "LLM-MCTS",
            "agent_description": "Uses an LLM to provide commonsense knowledge and heuristic guidance inside an MCTS planner (LLM supplies world-model rollouts or heuristics to guide tree search). Cited as outperforming standard MCTS and pure LLM policies in complex planning tasks.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": "Applied to complex planning tasks in the referenced work; no specific text-game benchmark is provided in this paper.",
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as a hybrid that leverages LLM knowledge within a planning procedure; not described here as a memory-based agent in the same sense as PARL or Reflexion.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2932.3",
            "source_info": {
                "paper_title": "PARL: Prompt-based Agents for Reinforcement Learning",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "AdaPlanner",
            "name_full": "AdaPlanner: Adaptive planning from feedback with language models",
            "brief_description": "A method that incrementally refines high-level plans (encoded as prompts) based on feedback from the environment, using that feedback to adjust future plans.",
            "citation_title": "Adaplanner: Adaptive planning from feedback with language models",
            "mention_or_use": "mention",
            "agent_name": "AdaPlanner",
            "agent_description": "Represents plans as prompt sequences of high-level steps and refines them iteratively by comparing predicted outcomes against actual outcomes; feedback informs plan updates.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": "Used for planning tasks; the paper references it as related work on incremental plan refinement with feedback.",
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited for iterative refinement from feedback; not described here with details on memory architecture.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2932.4",
            "source_info": {
                "paper_title": "PARL: Prompt-based Agents for Reinforcement Learning",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Large language models as commonsense knowledge for large-scale task planning",
            "rating": 2,
            "sanitized_title": "large_language_models_as_commonsense_knowledge_for_largescale_task_planning"
        },
        {
            "paper_title": "Adaplanner: Adaptive planning from feedback with language models",
            "rating": 2,
            "sanitized_title": "adaplanner_adaptive_planning_from_feedback_with_language_models"
        },
        {
            "paper_title": "clembench: Using game play to evaluate chat-optimized language models as conversational agents",
            "rating": 1,
            "sanitized_title": "clembench_using_game_play_to_evaluate_chatoptimized_language_models_as_conversational_agents"
        },
        {
            "paper_title": "Benchmarking large language model (llm) performance for game playing via tic-tac-toe",
            "rating": 1,
            "sanitized_title": "benchmarking_large_language_model_llm_performance_for_game_playing_via_tictactoe"
        }
    ],
    "cost": 0.01578125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PARL: Prompt-based Agents for Reinforcement Learning
24 Oct 2025</p>
<p>Yarik Menchaca Resendiz 
Leibniz-Institut für Psychologie (ZPID)
TrierGermany</p>
<p>Fundamentals of Natural Language Processing
University of Bamberg
Germany</p>
<p>Roman Klinger roman.klinger@uni-bamberg.de 
Fundamentals of Natural Language Processing
University of Bamberg
Germany</p>
<p>PARL: Prompt-based Agents for Reinforcement Learning
24 Oct 20257F20FEC3841F4CB59C7D348B5C0FE03BarXiv:2510.21306v1[cs.CL]Reinforcement learningLLM agentsfew-shot learning
Large language models (LLMs) have demonstrated high performance on tasks expressed in natural language, particularly in zero-or few-shot settings.These are typically framed as supervised (e.g., classification) or unsupervised (e.g., clustering) problems.However, limited work evaluates LLMs as agents in reinforcement learning (RL) tasks (e.g., playing games), where learning occurs through interaction with an environment and a reward system.While prior work focused on representing tasks that rely on a language representation, we study structured, non-linguistic reasoning -such as interpreting positions in a grid world.We therefore introduce PARL (Prompt-based Agent for Reinforcement Learning), a method that uses LLMs as RL agents through prompting, without any fine-tuning.PARL encodes actions, states, and rewards in the prompt, enabling the model to learn through trial-and-error interaction.We evaluate PARL on three standard RL tasks that do not entirely rely on natural language.We show that it can match or outperform traditional RL agents in simple environments by leveraging pretrained knowledge.However, we identify performance limitations in tasks that require complex mathematical operations or decoding states and actions.</p>
<p>Introduction</p>
<p>Large language models (LLMs; e.g., OpenAI, 2024;Jiang et al., 2024;Devlin et al., 2019) have demonstrated strong performance across a wide range of tasks.While originally developed for text-based applications, they are being applied to other modalities such as vision (Dosovitskiy et al., 2021;Radford et al., 2021) and audio (Borsos et al., 2022;Rubenstein et al., 2023).In natural language processing (NLP), tasks are often framed as supervised learning problems, relying on labeled data, such as classification (Sun et al., 2023b;Schick and Schütze, 2021) and translation (Zhang et al., 2023a;Moslem et al., 2023;Brants et al., 2007), or as unsupervised learning problems, such as text clustering (Zhang et al., 2023b;Viswanathan et al., 2024).Although LLMs have addressed many such tasks using (zero-and few-shot) prompting, they are predominantly based on supervised or unsupervised learning frameworks.</p>
<p>Many real-world problems are, however, more appropriately modeled as reinforcement learning (RL; Kreutzer et al., 2021).In RL, agents learn by interacting with an environment, and take actions and receive feedback in the form of rewards.Unlike supervised learning, which relies on labeled data, RL involves trial-and-error learning with delayed rewards and requires balancing exploration and exploitation.Common applications include learning to play games (e.g., Go (Silver et al., 2016), Atari games (Schrittwieser et al., 2020), Chess (David et al., 2016), BlackJack (Zha et al., 2019)), robotics (Kober et al., 2013)</p>
<p>LLM</p>
<p>Figure 1: Prompt-based Agent Optimization for Reinforcement Learning (PARL) uses a large language model to make decisions through in-context learning.The optimization begins with a prompt that contains only the task description.As the agent interacts with the environment, the prompt is updated by concatenating (⊕) the history of interactions of the states, actions, and rewards generated at each step t. et al., 2017;Kaufmann et al., 2023).</p>
<p>While recent work has explored LLM-based agents for decision-making via techniques like chain-of-thought prompting or self-reflection (Yao et al., 2023;Zhao et al., 2023;Sun et al., 2023a;Shinn et al., 2023), these methods treat LLMs as text-based agents that rely on natural language for both input and output.Such designs are effective for language-centered tasks (e.g., planning), but their applicability to structured, non-linguistic environments remains underexplored.</p>
<p>In this paper, we investigate and evaluate whether LLMs can act as traditional RL agents in tasks where input and output are not naturally ex-pressed in natural language -such as navigating a grid world.We introduce PARL (Prompt-based Agent for Reinforcement Learning), a method that keeps LLM weights frozen and uses prompting to enable in-context learning through iterative interaction with an environment.At each step, PARL encodes states, actions, and rewards into a cumulative text prompt, allowing the model to improve decision-making via in-context learning (Figure 1).</p>
<p>We evaluate PARL against three state-of-theart RL policies across three established RL tasks (Blackjack, Frozen Lake, and Taxi) to address: (RQ1) Can a PARL agent learn from interactions with the environment similarly to other RL policies?: (RQ2) Can a PARL agent benefit from pre-trained knowledge from the LLMs?; (RQ3) Does a PARL agent explore and exploit similarly to a standard RL agent?Our findings show that PARL can match or outperform established policies in knowledgeintensive tasks (e.g., blackjack), where the agent benefits from the LLM's pre-trained information.However, in more complex tasks, PARL faces challenges due to LLMs' limitations in symbolic computation and state decoding.</p>
<p>Related Work</p>
<p>In Section 2.1, we review state-of-the-art reinforcement learning methods, whereas in Section 2.2, we discuss the use of LLMs as agents in RL.</p>
<p>Reinforcement Learning</p>
<p>Reinforcement learning aims at optimizing agents' actions in sequential decision-making tasks through trial-and-error learning guided by a reward system.The problem is typically formalized as a Markov decision process, defined by (S, A, P, R, γ), where S represents the state space, A the action space, P (s ′ |s, a) the state transition probability, R(s, a) the reward function, and γ ∈ [0, 1] the discount factor.The goal is to find an optimal policy π(a|s) that maximizes the expected cumulative return:
G = ∞ t=0 γ t R(s t , a t )(1)
Q-learning and Deep Q-Networks (DQN; Mnih et al., 2013Mnih et al., , 2015) ) set foundational work by integrating neural networks to approximate the action-value function Q(s, a), which represents the expected reward for taking action a in state s.In DQN, this function is parameterized as Q(s, a; θ), where θ denotes the weights of the neural network.The model learns these parameters by minimizing a sequence of loss functions L i (θ i ) that changes at each iteration i:
L DQN (θ) = E s,a∼p(•) (y i − Q(s, a; θ)) 2 , (2)where yi = E s ′ ∼E [r + γ max a ′ Q(s ′ , a ′ ; θi−1) | s, a]
is the target for iteration i and p(s, a) is a probability distribution over sequences s and actions a.</p>
<p>Proximal Policy Optimization (PPO; Schulman et al., 2017) balances exploration and exploitation while maintaining a stable policy updates by taking small policy updates -big updates may guide the policy in a suboptimal direction.PPO uses a clipped objective function to ensure small policy updates, defined as:
L PPO (θ) = E t min r t (θ) Ât , clip(r t (θ), 1 ± ϵ) Ât , (3)
where r t (θ) is the probability ratio, Ât the advantage estimate, and ϵ the clipping parameter.</p>
<p>Advantage Actor-Critic (A2C; Mnih et al., 2016) combines value-and policy-based methods in an actor-critic framework, where the actor π θ (a | s) selects actions and the critic V (s) estimates the value function to reduce variance, improving learning stability.The A2C loss function is defined as:
L A2C (θ) = E t log π θ (a t |s t ) Ât ,(4)
where Ât = r t + γV (S t+1 ) − V (s) is the advantage function computed from the critic's value estimated.</p>
<p>Prompting in Reinforcement Learning</p>
<p>LLMs, such as GPT-4 (OpenAI, 2024), LLaMa 3 (Team, 2024), andMixtral (Jiang et al., 2024), have demonstrated strong performance on zeroand few-shot tasks using prompts (Semnani et al., 2023;Lin et al., 2022).These prompting strategies are widely used in traditional NLP tasks.For example, text classification prompts combine instructions with labels (e.g.,"Tag the text as positive or negative. . ."; Hu et al., 2022;Gu et al., 2022), while summarization include keywords like "TL; DR" or "summarize" (Radford et al., 2019;Narayan et al., 2021).Translation prompts specify source and target languages (e.g., "Translate English to German"; Raffel et al., 2020).While these prompting strategies have primarily been applied to traditional (un)supervised tasks, recent work has explored how LLMs can be integrated into RL as support decision-making algorithms.For instance, Zhao et al. (2023) propose LLM-MCTS, an algorithm that uses LLMs as a commonsenseinformed world model and a heuristic policy within the Monte Carlo Tree Search (MCTS) framework.Their approach outperforms standard MCTS and purely LLM-based policies in complex planning tasks.Similarly, AdaPlanner (Sun et al., 2023a) proposes a method that incrementally refines plans -represented as prompts containing a sequence of high-level steps (e.g., clean lettuce on the dining table) -based on feedback from the environment.This feedback is evaluated by comparing predicted outcomes against actual outcomes, allowing the model to adjust its plan when misalignment occurs.</p>
<p>Other works aim to improve the LLM-based agents through reasoning (e.g., chain-of-thought) and learning from experience (Zhang et al., 2024).ReAct (Yao et al., 2023) interleaves reasoning with actions (interactions with tools or environments), which improves task performance and interpretability while reducing hallucinations.Reflexion (Shinn et al., 2023) extends this by enabling agents to self-critique (e.g., critique their own decisions) and update their behavior over time through memorybased adaptation, improving performance without additional fine-tuning.This self-improving behavior is effective in domains like ALFWorld (an environment for instruction-following tasks) and HotPotQA (question answering across documents).</p>
<p>To evaluate LLMs' capabilities, recent benchmarks use structured, game-like environments.Clembench (Chalamalasetti et al., 2023) exposes models to constrained decision-making tasks to test instruction-following and consistency.Results indicate that chat-optimized LLMs struggle with simple interactive scenarios, revealing limitations in current agent designs.Similarly, Topsakal and Harper (2024) use games like Tic-Tac-Toe for evaluation, benefiting from well-defined rules and outcomes.</p>
<p>Despite this progress, most prior work uses LLMs as text-based agents, where both input and output are in natural language.While this is effective for tasks like planning or instruction following, it has not been explored or evaluated in settings with structured, non-linguistic inputs -such as grid positions or card values in games like blackjack.</p>
<p>Methods</p>
<p>In the following section, we introduce PARL (in Section 3.1).PARL adopts the standard RL framework, learning through interaction with the environment to achieve task-specific goals (in Section 3.2) 1 .</p>
<p>PARL: Prompt Base Agent for Reinforcement Learning</p>
<p>PARL represents a reinforcement learning policy optimization method that uses LLMs as decisionmaking agents via prompting techniques.It combines the task description T with the concatenated interaction history (h) with the environment.The 1 Code and resources available at https://Blinded.</p>
<p>for/review/ PARL policy is defined by the prompt:
P PARL = T n t=0 h t ,(5)
where indicates concatenation and T -the task description -is defined as:
T = (G, A, S, R),
where G denotes the task goal.For example, in the context of Blackjack: "Blackjack is a card game where the goal is to get as close to 21 as possible without exceeding it.".A represents the set of actions available {a 1 , . . ., a k } to the agent (e.g., "0: Stick (Stop), 1: Hit (Draw)").S specifies the state representation, describing the information observed by the agent (e.g., "The observation is a tuple: (player's value, dealer's value)").Finally, R is the set of rewards {r 1 , . . ., r k } (e.g., "+1: Win, -1: Lose, 0: Draw.").h t represents the history of interactions with the environment, defined as:
h t = (s t , a t , r t ),
where s t denotes the state at time t, a t represents the action taken at time t, and r t is the reward received as a result of the action.This iterative log captures the sequence of interactions, recording the state, action, and reward for each time step t = 0, . . ., n.For example: "Action (taken): Stick; (new) State: [10,6,6], [7]; Reward: 0". Figure 2 presents a simplified depiction of the agent, while full examples are provided in the Appendix2 .State representations -provided by the environmentare often encoded in specific formats (e.g., data structures or numerical values such as hash values calculated out of structured representations).Therefore, decoding state representations can either be handled directly by the LLM -requiring it to interpret the encoded state -or delegated to an external function (e.g., a Python script)3 .Such script may include world knowledge on how to represent a particular state in a meaningful way in language.</p>
<p>PARL Training</p>
<p>Training a PARL agent requires creating the history h between the agent and the environment to enable in-context learning for the LLM.Since LLMs do not retain the memory of previous outputs during inference, the prompt must explicitly accumulate information from prior steps and episodes.</p>
<p>Figure 2 illustrates a simplified depiction of the prompt used for the blackjack RL task.In the first Rewards: +1 (win), -1 (lose), 0 (draw).</p>
<p>Task Description</p>
<p>Figure 2: Example of a simplified prompt interaction for the Blackjack task, over three steps with the environment, equivalent to two episodes.</p>
<p>iteration (Step 1 Prompt), the input prompt contains only the task description T (goal, action space, observation space, set of rewards), and the initial state.The LLM is prompted to generate an action from the specified action space (as described in Section 3.1).This action (e.g., "Hit") is then fed to the environment, to produce a new state and rewards (e.g., " [10,6,5] and [7]", representing the three user's and one dealer's cards).These updates are concatenated with the P PARL prompt to be used in the next step (e.g., Step 2 Prompt).This process iterates over n steps, each time appending the log of interactions to the prompt (Eq.5).With each new episode, additional logs are concatenated to the prompt, gradually increasing the context -with more examples -to enhance decision-making in future iterations through in-context learning.</p>
<p>PARL Inference</p>
<p>The inference process of a PARL agent uses the trained policy, represented as a prompt, which consists of two main components: (1) the task description (T , Section 3.1), which includes the task goal, action space, state representation, and reward space, and (2) the history (h = {(s 1 , a 1 , r 1 ), . . ., (s n , a n , r n )}), which records the sequence of states, actions, and rewards with the environment.During inference, only the history from the current episode is concatenated at each step and removed once the episode ends.The PARL agent uses the LLMs to generate an action from the defined action space.</p>
<p>Experiments</p>
<p>To address our research questions (Section 1), we evaluate the PARL agent under three configurations (Section 4.1): full context, context with random rewards, and no context.Each configuration is tested in two state-decoding variants (Section 4.3).GPT-4o4 is used as the underlying language model for all experiments.</p>
<p>The experiments are conducted on three RL tasks: Blackjack, Frozen Lake, and Taxi (Section 4.2).These environments are non-linguistic, offering a clear contrast to typical LLM applications, where natural language is central.</p>
<p>PARL Agent Configurations</p>
<p>Full History.This setup defines our standard approach for training PARL agents.We train a PARL agent for 100 episodes for each task.The history h is gradually concatenated, meaning that episode n includes context from all previous n − 1 episodes.For example, Episode 1 has no context, while Episode 100 has the history of all previous 99 episodes (h = {(s 1 , a 1 , r 1 ), . . ., (s 99 , a 99 , r 99 )}).</p>
<p>History with Random Rewards.To assess whether the LLM learns from reward signals, we adopt a similar setup to the Full History.The prompt includes the task description (T ) and interaction history (h), but with true rewards (r t ) replaced by random ones (r rand t ), sampled uniformly from the reward set (R).If performance remains comparable to setups with true rewards, it suggests the LLM is not learning from rewards but instead leveraging pre-trained knowledge.</p>
<p>No History.In this configuration, the PARL agent has no access to prior episode history.The prompt includes only the task description (T ) and the interaction history (h) from the current episode.Specifically, h = {(s t , a t , r t )} contains only the states, actions, and rewards from the current episode (t).Rewards (r t ) are included in the prompt only if provided by the environment at that step (e.g., per-step vs. end-of-episode rewards).</p>
<p>Reinforcement Learning Tasks</p>
<p>Blackjack.The task is to play a card game aiming to get closer to 21 than the dealer without exceeding it.The state is represented as a tuple: the player's current sum, the value of the dealer's visible card (1-10, where 1 represents an Ace), and whether the player holds a usable Ace (0 or 1).Actions are "Hit" (draw a card) or "Stick" (end turn).Rewards are +1 for winning, 0 for a tie, and −1 for losing.The episode ends if the player hits and the sum of the hand exceeds 21, or if the player sticks.The objective is to learn an optimal strategy for deciding when to "Hit" or "Stick".5</p>
<p>Frozen Lake.The task consists of crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake.The state represents the player's position as a single integer, given by row t • ncols + col t , where row t and col t are the current row and column, and ncols is the total number of columns.Actions are of moving Up, Down, Left, or Right.Rewards are +1 for reaching the goal and 0 otherwise.The episode ends if the player moves into a hole, reaches the goal, or exceeds a maximum number of movements.The objective is to learn an optimal path to the goal while minimizing the risk of falling into holes.6</p>
<p>Taxi.The task involves driving a passenger in a grid world, picking them up, and dropping them at one of the four locations on the map, one at a time.The state is encoded as a single integer that tracks the taxi's position, the passenger's location, and the destination: ((taxi row • 5 + taxi col ) • 5 + passenger location ) • 4 + passenger destination .Actions consist of Down, Up, Left, Right, Pickup or Drop-off.Rewards are +20 for successfully dropping off a passenger at the correct destination, −1 for each step taken, and −10 for attempting illegal Pickup or Drop-off actions.The episode ends when the taxi successfully drops off the passenger at the correct destination or when the maximum number of steps is reached.The objective is to learn efficient pickup and drop-off strategies to minimize steps and maximize rewards.7</p>
<p>State-decoding Variants</p>
<p>Two configurations are used for the state (s) in the history h: (1) the environment state is passed directly to the prompt without preprocessing, requiring the LLM to interpret the raw symbolic or numeric form.(2) An external decoding function (a Python script unique to each environment) converts the state into a natural-language description.In Blackjack, the raw state is used as produced by the environment, e.g., "State: [10,6,4], [7]", while the decoded version expresses it in natural language, e.g., "The player's hand totals 20, and the dealer shows a 7.".In Frozen Lake, the raw state is a grid index, e.g., "State: 6", and the decoded version describes the position, e.g., "The player is currently located at row 1, column 2 in a 4x4 grid.".In Taxi, the raw state is a tuple encoding position and passenger data, e.g., "State: 6", and the decoded version reformulates it as "The taxi is at row 0, column 0. The passenger is at location Green, and the destination is Yellow.".</p>
<p>Results</p>
<p>Before discussing the three configurations of the PARL agent used to address our research questions, we first examine the two state-decoding variants by comparing their training curves: LLM selfdecoding (Figure 3) and script-based decoding (Figure 4).In Blackjack, the script-based decoding reaches an average reward of 0.6 after only 23 episodes, while the self-decoding only reaches 0.3 after 50 episodes.A similar pattern can be seen in Frozen Lake, where the script variant achieves 1 after 32 episodes, whereas the self-decoding only reaches 0.55 at the end of training.The Taxi environment is more challenging due to its more complex encoding.Here, the LLM is unable to reliably decode positions on its own, leading to random actions.In contrast, the script provides accurate state representations, allowing the LLM to learn low-penalty actions.</p>
<p>Overall, the script-based decoding shows faster learning and higher performance, with steeper learning curves.Based on these results, the following section focuses only on the script-based decoding method.</p>
<p>RQ1: Can a PARL Agent Learn from Environment Interactions?</p>
<p>To evaluate whether the PARL agent learns to take better actions based on previous episodes, we investigate three subquestions: RQ1.Carlin and Robinson, 2009).</p>
<p>A similar behavior is observed in the Frozen Lake task, where the model initially explores to find the correct path.This training improvement shows the agent's ability to learn better action policies based on the history h.The Taxi task presents a greater challenge with its larger action space.The LLM is unable to learn to solve the task.</p>
<p>Table 1 reports the average rewards obtained by the three PARL agent configurations during inference.It presents the average rewards over 100 evaluation episodes for the three configurations of the PARL agent (top) and three state-of-the-art agents (bottom).When comparing the Full History configuration to the History with Random Rewards and No History setups, we observe that history improves the PARL agent's performance.For instance, in Blackjack, the PARL agent achieves an average reward of 0.2 with Full History, outperforming the History with Random Rewards (−0.12) and No History (−0.09) setups.Similarly, in the Frozen Lake task, the Full History setup achieves an average reward of 0.66, compared to the 0.0 from the other configurations.On the Taxi task, the PARL agent with Full History presents a similar behavior to the training.The agent fails to solve the task but tries to minimize negative rewards (−696.6 compared to −807.5 for History with Random Rewards and −568.95 for No History).The agent frequently selects sub-optimal actions, such as illegal pick-up or drop-off, which incur a significant penalty (−10).</p>
<p>RQ1.2: What happens when random or incorrect rewards are provided?</p>
<p>In the configuration where random rewards are included as context, the model fails to learn effectively across all tasks.Figure 6 illustrates that average rewards remain low regardless of training.In Blackjack, the agent's average reward decreases to −0.5.In Frozen Lake, the agent fails to learn a policy, achieving an average reward of 0.0 (not succeeding in the task even once).In the Taxi task, the agent selects random actions, accumulating high penalties, likely due to the large state and action space.This confirms that irrelevant or misleading context introduces noise, preventing the model from identifying optimal actions.</p>
<p>RQ1.3: How does the PARL policy compare to state-of-the-art policies?</p>
<p>We compare our policy to three state-of-the-art methods (PPO, DQN, and A2C).We start by comparing the training setups.Figure 4 presents the average reward during training for the PARL agent and Figure 5 for the SOTA agents.The two figures show similar patterns for Blackjack and Frozen Lake, where average rewards improve with more training iterations.</p>
<p>During training on the more challenging Taxi task, the SOTA agents learn to minimize negative rewards by avoiding high-penalty actions, such as illegal "pick-up" or "drop-off" actions, but they still fail to successfully complete the task.PARL agent initially shows a similar behavior, it then accumulates higher negative rewards per episode, suggesting that adding more examples may introduce noise during training.We can observe a similar result when comparing the episode length from both the PARL (Figure 7) and the SOTA policies (Figure 8).Notably, a PARL agent learns faster, requiring only a small fraction of the training examples needed by the SOTA agents -100 vs. 100, 000.</p>
<p>At inference time, Table 1 shows the average rewards.In Blackjack, the PARL agent (Full His-tory) outperforms the SOTA agents by 16 pp -0.2 vs. 0.04 for PPO, −0.04 for DQN, and −0.02 for A2C.In Frozen Lake, the performance is similar to the other agents -0.66 vs. 0.67 for PPO, 0.61 for DQN, and 0.62 for A2C.However, in the Taxi task, the PARL agent underperforms compared to all SOTA agents.In addition, none of the agents successfully completed the task, as indicated by the average episode length of 100 -the maximum number of actions allowed in the environment, resulting in the truncation/ending of the episode.The SOTA agents learn to minimize negative rewards by only selecting movement actions (−1) and avoiding high-penalty actions.In contrast, the PARL shows a random behavior, choosing among all possible actions, including illegal pick-up and drop-off actions (−10).</p>
<p>RQ2: Can a PARL policy benefit</p>
<p>from pre-trained knowledge from the LLMs?</p>
<p>To assess the impact of LLM pre-trained knowledge on PARL, we focus on the No History configuration in Table 1, where the agent has only access to the current episode's steps, actions, and rewards.In Blackjack, the agent performs slightly worse than the SOTA models, achieving an average reward of −0.7 vs. 0.04 (PPO), −0.04 (DQN), and −0.02 (A2C).This suggests that the LLM's prior knowledge of card games aids its decision-making process.However, in Frozen Lake, the absence of context prevents the agent from discovering the correct path, resulting in failure to accomplish the task.Interestingly, in the Taxi task, the No History configuration achieves the highest average reward among all PARL configurations.This suggests that long context introduces noise, complicating the decisionmaking process more challenging in complex environments.The Taxi tasks environment allows up to 100 steps per episode, compared to an average of only 1.5 steps per episode in Blackjack (Table 1).This extended episode length may provide a PARL agent with more information to learn the task and to prioritize actions with lower penalties -similar to the behavior of SOTA agents.In conclusion, a PARL agent benefits from pre-trained knowledge from the LLMs, but this only applies to tasks that are presumably frequent in the training data (e.g., popular card games and video games).</p>
<p>RQ3: Does a PARL agent explore</p>
<p>and exploit similarly to a standard RL agent?lengths across three tasks.We focus on Frozen Lake, which requires exploration, unlike Blackjack (where episode length depends on initial cards) and Taxi (where agents reach the maximum actions per episode without solving the task, as discussed in Section 5.1).In Figure 7b, we observe an initial increase in episode length from 4 to 5, followed by a drop and later stabilization around an average of 7 steps.A closer analysis reveals that the agent initially follows a repeated action sequence (path), which fails to reach the goal (noted by the drop between episodes 20 and 30).It later discovers a successful path (set of actions) and obtains positive rewards.Occasionally, drops in episode length are due to the task's "slippery" nature, where unintended moves may alter the next state (e.g., moving down might result in ending to the left).This analysis shows that the PARL agent is capable of exploration, but it is limited to exploitation.Once it successfully completes the task, it tends to follow the same set of actions repeatedly, without search-ing for a more optimal solution.</p>
<p>Conclusion and Future Work</p>
<p>We introduced PARL, a prompting-based agent that leverages large language models for reinforcement learning in non-linguistic environments.Unlike prior work that focuses on natural language-centered tasks, PARL shows that LLMs can be adapted to basic mathematical decision-making tasks by encoding the environment as textual prompts.Our experiments demonstrate that LLMs can learn from reward-based interactions in simple environments and outperform standard RL baselines in tasks involving common knowledge (e.g., Blackjack).Notably, PARL achieves competitive performance with significantly fewer iterations (100 vs. 100,000), highlighting its sample efficiency.While the PARL exhibits some capacity for exploration, it has limited exploitation behavior.Moreover, it struggles with more complex tasks that require precise computa-tion or abstract state interpretation -such as decoding grid positions -underscoring current limitations of LLMs in handling raw numeric values.This work leads to important future research.One promising direction is the use of retrievalbased methods to select relevant past episodes, as long histories (h) may introduce noise to the LLM.In addition, extending PARL to a multimodal framework incorporating vision or structured graphs may improve its performance in visual tasks (e.g., Atari and card games).</p>
<p>Ethical Considerations</p>
<p>The proposed methodology, Prompt-based Agent for Reinforcement Learning (PARL), introduces an alternative optimization policy by using a large language model through prompting techniques.However, the reliance on LLMs raises several ethical concerns that must be addressed.</p>
<p>Firstly, LLMs are trained on vast and diverse datasets, which can include societal biases or generate inappropriate content.When such biases are integrated into reinforcement learning tasks, they may influence the learning process, potentially leading to harmful or unfair outcomes.This issue is particularly critical when applying LLM-based agents in sensitive or real-world domains, such as healthcare, education, or autonomous systems, where biased or erroneous decisions can have significant consequences.</p>
<p>Secondly, the transparency of LLM decisionmaking poses a challenge.These models function as black boxes, making it difficult to interpret their reasoning processes and understand how decisions are reached.This lack of interpretability complicates efforts to identify and mitigate biases or errors.</p>
<p>To address these challenges, future research should prioritize the development of techniques to reduce biases in LLMs and improve the interpretability of their decision-making processes.It is important to note that such risks are not inherent to PARL methodology but stem from the base LLMs used.</p>
<p>Limitations</p>
<p>The effectiveness of the proposed PARL method is significantly influenced by the capabilities of the underlying language models (e.g., GPT, LLaMa-7B-Chat, and Mistral-7B).While the approach shows potential in using pre-trained LLMs for reinforcement learning tasks, several limitations must be acknowledged.</p>
<p>Firstly, the method struggles with complex tasks that require advanced mathematical reasoning or precise mappings of states and actions.These limitations present a challenge for the model to accomplish the tasks that require the encoding of information.</p>
<p>Secondly, the dependence on text-based prompts introduces scalability challenges.Longer training leads to larger interaction histories, which can result in prompts exceeding the input token limits of LLMs.This may lead to the truncation of important information or limit the number of training episodes.As a result, the agent may struggle to learn in longer tasks or more complicated environments.</p>
<p>Third, environments with large state-action spaces may introduce potential noise.This noise can complicate decision-making and reduce PARL's performance in dynamic or highly variable environments.</p>
<p>Fourth, there is an imbalance in model scale.PARL uses large language models (e.g., GPT-4o), while the RL baselines use much smaller networks.This raises fairness concerns.However, the PARL is trained on only a fraction of the episodes compared to the SOTA baselines, which helps balance the comparison to some extent.Still, we acknowledge that model size can affect performance and encourage future work to explore more size-aligned setups.</p>
<p>Despite these limitations, PARL is the first method to demonstrate an optimization policy using an LLM through prompting techniques.The method has proven effective in generating text-based reinforcement learning strategies, particularly for simpler or knowledge-intensive tasks.However, users should be aware of these limitations when evaluating the method's capabilities and potential applications.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Average reward during training with the PARL agent using LLM self-decoding of states.A smoothing window of five episodes is applied.</p>
<p>Figure 5 :
5
Figure 5: Comparison of average training rewards for SOTA agents (smoothing window = 200).</p>
<p>Figure 6 :
6
Figure 6: Comparison episode real reward during PARL agent training when informing the agent with random or incorrect rewards.</p>
<p>Figure 7 :
7
Figure 7: Average episode length during training from PARL agent, with a smoothing window size of five.</p>
<p>Figure 8 :
8
Figure 8: Average episode length during training for SOTA agents (smoothing window = 200).</p>
<p>), and improving large models with human feedback (Christiano
AgentPromptAction (t)Task descriptionAction (t-1)State (t)Reward (t)Reward (t+1) } . . . Action (t) State (t+1)Action (t) State (t+1) Reward (t+1)Environment</p>
<p>Table 1 :
1
Average (Avg.)reward and episode length across 100 episodes for the Blackjack, Frozen Lake, and Taxi tasks, comparing three PARL agent configurations (top) with state-of-the-art (SOTA; bottom) agents.Standard deviation is in parentheses.
Figure 7 examines whether a PARL agent can ex-plore its environment by reporting average episode
LREC does not allow including Appendices during review. We will add this appendix in the camera-ready version of the paper.
Notably, the LLM can generate these scripts itself.
The total cost of the experiments was 145 USD. They have been performed in October 2024.
Gymnasium Blackjack description: https: //gymnasium.farama.org/environments/toy_text/ blackjack/.
Gymnasium Frozen Lake description: https: //gymnasium.farama.org/environments/toy_text/ frozen_lake/.
Gymnasium Taxi task description: https:// gymnasium.farama.org/environments/toy_text/taxi/.
AcknowledgementsBlinded for review.
Audiolm: A language modeling approach to audio generation. Bibliographical References, Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matthew Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312022</p>
<p>Large language models in machine translation. Thorsten Brants, C Ashok, Peng Popat, Franz J Xu, Jeffrey Och, Dean, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)PragueCzech Republic. Association for Computational Linguistics2007</p>
<p>Fear and loathing in las vegas: Evidence from blackjack tables. Bruce I Carlin, David T Robinson, 10.1017/S1930297500001212Judgment and Decision Making. 452009</p>
<p>clembench: Using game play to evaluate chat-optimized language models as conversational agents. Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen, 10.18653/v1/2023.emnlp-main.689Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730</p>
<p>Deepchess: End-to-end deep neural network for automatic learning in chess. Nathan S Omid E David, Lior Netanyahu, Wolf, Artificial Neural Networks and Machine Learning-ICANN 2016: 25th International Conference on Artificial Neural Networks. Barcelona, SpainSpringer2016. September 6-9, 2016Proceedings, Part II 25</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota20191Association for Computational Linguistics</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>PPT: Pre-trained prompt tuning for fewshot learning. Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang, 10.18653/v1/2022.acl-long.576Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun, 10.18653/v1/2022.acl-long.158Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2023. A Survey of Reinforcement Learning from Human Feedback. </p>
<p>Reinforcement learning in robotics: A survey. Jens Kober, Andrew Bagnell, Jan Peters, 10.1177/0278364913495721The International Journal of Robotics Research. 32112013</p>
<p>Offline reinforcement learning from human feedback in real-world sequence-tosequence tasks. Julia Kreutzer, Stefan Riezler, Carolin Lawrence, 10.18653/v1/2021.spnlp-1.4Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021). the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)Online. Association for Computational Linguistics2021</p>
<p>Few-shot learning with multilingual generative language models. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O' Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li, 10.18653/v1/2022.emnlp-main.616Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of The 33rd International Conference on Machine Learning. The 33rd International Conference on Machine LearningNew York, New York, USAPMLR201648</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A Riedmiller, ArXiv, abs/1312.56022013</p>
<p>Human-level control through deep reinforcement learning. nature. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, 10.1038/nature142362015518</p>
<p>Adaptive machine translation with large language models. Yasmin Moslem, Rejwanul Haque, John D Kelleher, Andy Way, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>Planning with learned entity prompts for abstractive summarization. Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões, Vitaly Nikolaev, Ryan Mc-Donald, 10.1162/tacl_a_00438Transactions of the Association for Computational Linguistics. 92021</p>
<p>30.03.2024Gpt-4o model. 2024OpenAI</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Exploiting cloze-questions for few-shot text classification and natural language inference. Chulayuth Paul K Rubenstein, Asawaroengchai, Dung Duc, Ankur Nguyen, Zalán Bapna, Félix Borsos, De Chaumont, Peter Quitry, Dalia El Chen, Wei Badawy, Eugene Han, Kharitonov, 10.18653/v1/2021.eacl-main.20arXiv:2306.12925Audiopalm: A large language model that can speak and listen. Online. Association for Computational Linguistics2023. 2021arXiv preprintProceedings of the 16th Conference of the European Chapter</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, 10.1038/s41586-020-03051-4Nature. 58878392020</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia. Sina Semnani, Violet Yao, Heidi Zhang, Monica Lam, 10.18653/v1/2023.findings-emnlp.157Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Adaplanner: Adaptive planning from feedback with language models. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, Advances in Neural Information Processing Systems. Curran Associates, Inc2023a36</p>
<p>Text classification via large language models. Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, Guoyin Wang, 10.18653/v1/2023.findings-emnlp.603Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023bAssociation for Computational Linguistics</p>
<p>The llama 3 herd of models. Oguzhan Topsakal and Jackson B. Harper. 2024. Benchmarking large language model (llm) performance for game playing via tic-tac-toe. 10.3390/electronics13081532Electronics. 1382024</p>
<p>Large language models enable few-shot clustering. Vijay Viswanathan, Kiril Gashteovski, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, Graham Neubig, 10.1162/tacl_a_00648Transactions of the Association for Computational Linguistics. 122024</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Daochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo, Xia Hu, arXiv:1910.04376Rlcard: A toolkit for reinforcement learning in card games. 2019arXiv preprint</p>
<p>Agentpro: Learning to evolve via policy-level reflection and optimization. Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu, 10.18653/v1/2024.acl-long.292Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Machine translation with large language models: Prompting, few-shot learning, and fine-tuning with QLoRA. Xuan Zhang, Navid Rajabi, Kevin Duh, Philipp Koehn, 10.18653/v1/2023.wmt-1.43Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023a</p>
<p>ClusterLLM: Large language models as a guide for text clustering. Yuwei Zhang, Zihan Wang, Jingbo Shang, 10.18653/v1/2023.emnlp-main.858Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023bAssociation for Computational Linguistics</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, Advances in Neural Information Processing Systems. 202336</p>            </div>
        </div>

    </div>
</body>
</html>
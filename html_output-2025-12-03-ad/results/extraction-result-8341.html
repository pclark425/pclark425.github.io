<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8341 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8341</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8341</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278910926</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.20728v4.pdf" target="_blank">Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Spatial reasoning is a core component of human cognition, enabling individuals to perceive, comprehend, and interact with the physical world. It relies on a nuanced understanding of spatial structures and inter-object relationships, serving as the foundation for complex reasoning and decision-making. To investigate whether current vision-language models (VLMs) exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark consisting of 1,100 carefully curated real-world images with high spatial complexity. Based on this dataset, we design five tasks to rigorously evaluate VLMs'spatial perception, structural understanding, and reasoning capabilities, while deliberately minimizing reliance on domain-specific knowledge to better isolate and assess the general spatial reasoning capability. We conduct a comprehensive evaluation across 24 state-of-the-art VLMs. The results show that even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy and performs particularly poorly on the Order Generation task, with only 30.00% accuracy, far below the performance exceeding 90% achieved by human participants. This persistent gap underscores the need for continued progress, positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for advancing spatial reasoning research in VLMs. Our project page is at https://zesen01.github.io/jigsaw-puzzles.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8341.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8341.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary, reasoning-enhanced vision-language model evaluated as the top-performing VLM on the Jigsaw-Puzzles benchmark, showing self-correction behavior under choice constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A proprietary multimodal (vision-language) model variant described in the paper as a reasoning-enhanced VLM; noted for self-correction behavior when constrained to candidate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (Missing Piece Selection, Piece Localization, Connection Verification, Anomaly Detection, Order Restoration; Order Generation on Jigsaw-Puzzles-Lite)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like puzzle tasks requiring grid-based spatial layout reconstruction, adjacency reasoning, local transformation detection (rotation/mirroring), and multi-step order restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot evaluation with multi-image input; four-option multiple-choice for the five main tasks (exact-match accuracy); open-ended sequence generation for Order Generation on the Jigsaw-Puzzles-Lite subset; hardware scaled to model size.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning-enhanced variant; exhibits self-correction by re-evaluating visual input when initial prediction conflicts with provided answer choices; evaluated without external tools; benefits from reduced answer space under choice constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy 77.14% on full Jigsaw-Puzzles benchmark (highest among tested models); Order Generation (open-ended) on Jigsaw-Puzzles-Lite: 30.00% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative examples and analysis show self-correction and higher multi-step task performance relative to base variants; correlational analyses in paper show Missing Piece Selection performance predicts reasoning task performance, and Gemini-2.5-Pro succeeds more often on multi-step tasks under option constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms other tested VLMs (proprietary and open-source) in overall accuracy; still lags far behind humans (human overall 96.36%); reasoning-enhanced variants (e.g., Gemini-2.5-Flash-Thinking) show similar improvements over base versions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on open-ended Order Generation (30.00% vs human 94.09%); despite best-in-class performance, still >20 percentage points below human overall; relies on choice constraints for effective self-correction and struggles to autonomously construct multi-step spatial chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8341.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5-Flash-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.5-Flash-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced variant of the Gemini family evaluated on the Jigsaw-Puzzles benchmark and showing substantial gains over its base Flash variant, especially on multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.5-Flash-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary reasoning-enhanced vision-language model variant (Thinking suffix indicates reasoning augmentation) evaluated zero-shot on puzzle tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like puzzle tasks requiring spatial perception, adjacency reasoning, and order restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot, multiple-choice evaluation (four options) for main tasks; same Jigsaw-Puzzles QA templates; models supplied multi-image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning-enhanced training/decoding (paper marks as reasoning-enhanced); reportedly leverages enhanced internal reasoning heuristics compared to base Flash model; benefits observed particularly on Order Restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported at 72.42% (Table summary); substantial improvements over base Gemini-2.5-Flash (65.86%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Higher success on multi-step Order Restoration compared to base model; paper attributes improved multi-step performance to reasoning augmentation and self-correction dynamics under answer constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Shows consistent improvements vs Gemini-2.5-Flash; outperforms many open-source models but below Gemini-2.5-Pro; still well below human performance (96.36%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Like other VLMs, performance drops drastically in open-ended Order Generation; improvements appear contingent on choice-constrained settings rather than fully autonomous multi-step generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8341.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal LLM evaluated in the paper as one of the VLMs on the Jigsaw-Puzzles benchmark; shows moderate performance across the tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal model from OpenAI (GPT-4o family) evaluated zero-shot with multi-image inputs on the Jigsaw-Puzzles tasks; paper reports both GPT-4o and GPT-4o-mini variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (Missing Piece Selection, Piece Localization, Connection Verification, Anomaly Detection, Order Restoration)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multi-image input; multiple-choice for main tasks with exact-match accuracy metric; hardware scaled to model size.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard zero-shot multimodal inference as evaluated; no specific chain-of-thought or special puzzle strategy described in the paper for GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall accuracy ~60.71% (GPT-4o) and GPT-4o-mini ~58.97% across Jigsaw-Puzzles (from table summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs above random on single-step tasks (Piece Localization, Connection Verification, Anomaly Detection) but exhibits substantial gap on multi-step Order Restoration compared to humans, indicating limited multi-step spatial chain construction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Below reasoning-enhanced Gemini variants and some Claude models; substantially below human accuracy (96.36%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with open-ended multi-step Order Generation (paper shows broad failure across models on generation task); performance improves under choice constraints but still limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8341.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Claude family proprietary multimodal model evaluated on the Jigsaw-Puzzles benchmark, showing solid performance on perceptual and many reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.7-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Claude-3.7 Sonnet multimodal model evaluated zero-shot on the Jigsaw-Puzzles tasks; one of the higher-performing proprietary baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like spatial reasoning and perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice evaluation with multi-image input; exact-match accuracy reported per task.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard multimodal inference; no special puzzle-specific strategy detailed; benefits from high perceptual accuracy on missing-piece tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~64.88% (table summaries); strong performance on Missing Piece Selection and competitive single-step task results.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs well on Missing Piece Selection and some single-step spatial tasks, consistent with capability in local spatial understanding; still lags on multi-step Order Restoration and fails to match human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms several open-source models; below top Gemini-2.5-Pro; far below human benchmark (96.36%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Noted to underperform humans in multi-step reasoning and to show larger gaps on Order Generation open-ended task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8341.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grok-2-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grok-2-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary vision-capable model (Grok family) evaluated on Jigsaw-Puzzles; shows middling performance relative to other proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grok-2-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Grok family multimodal model with vision capabilities evaluated zero-shot on the Jigsaw-Puzzles benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice evaluation with multi-image input; exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No special puzzle-specific strategies described; evaluated in same zero-shot setting as other proprietary VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported approximately 42.70% (table summary), with varying per-task strengths (higher on some perceptual tasks, weaker on multi-step).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Exhibits partial competence on perceptual tasks but limited multi-step reasoning ability; performance often near or below stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms top proprietary models (Claude, Gemini variants) and some high-quality open-source models; below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails more often on multi-step Order Restoration and Order Generation; shows limited robustness to distractors in Hard settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8341.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-VL-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL-72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large vision-language model (Qwen family) evaluated across Jigsaw-Puzzles tasks with competitive performance among open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-VL-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model from the Qwen family with a 72B variant, evaluated zero-shot on Jigsaw-Puzzles; listed among the stronger open-source performers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based jigsaw-like tasks requiring spatial layout and adjacency reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice evaluation; exact-match accuracy per task; multi-image input supported.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard multimodal zero-shot inference; no explicit reasoning augmentation described in the paper for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~63.08% (table summaries); competitive on perceptual tasks and some single-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows scaling benefits and reasonable performance on single-step tasks but reduced capability on multi-step Order Restoration and near-chance on Order Generation compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Among top open-source models (compared to many smaller open-source models); below proprietary reasoning-enhanced models such as Gemini-2.5-Pro; well below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with open-ended Order Generation and complex multi-step tasks; performance depends on model size (scales positively) and lacks explicit reasoning-enhancements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8341.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL3-78B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL3-78B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source vision-language model (InternVL3 family) with a 78B variant that attains strong perceptual and spatial reasoning performance among open-source models on Jigsaw-Puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL3-78B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source VLM from the InternVL3 family; 78B-parameter variant evaluated zero-shot on the Jigsaw-Puzzles benchmark; highlighted as a strong open-source performer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>78B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like puzzle tasks requiring spatial perception, adjacency reasoning, and order restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice tasks with multi-image input; exact-match accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard inference; benefits from larger parameter count and training recipes; no explicit chain-of-thought or external reasoning module described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~68.79% (table summaries), making it one of the stronger open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs well on Missing Piece Selection and single-step reasoning tasks; scaling with size correlates with higher spatial reasoning performance as noted in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms many smaller open-source models and approaches some proprietary models on certain tasks; still below human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades on open-ended Order Generation; lacks explicit self-correction behavior emphasized for some proprietary reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8341.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kimi-VL-A3B-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kimi-VL-A3B-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced open-source VLM (Kimi family) evaluated on Jigsaw-Puzzles, showing modest improvements over its Instruct variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Kimi-VL-A3B-Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Kimi-VL 3B variant with 'Thinking' reasoning enhancement evaluated zero-shot on Jigsaw-Puzzles; compared with Instruct variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-inspired spatial tasks (perception and multi-step reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice evaluation; exact-match accuracy; compared directly with Kimi-VL-A3B-Instruct baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Reasoning-enhanced variant (Thinking) intended to improve multi-step reasoning; specific mechanisms not deeply detailed beyond being a 'thinking' variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy improves from 42.04% (Instruct) to 44.43% (Thinking) according to table summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Small improvement on multi-step tasks suggests some benefit from reasoning enhancement, but absolute performance remains low and near random on difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Slightly outperforms the Instruct variant; both perform substantially worse than large open-source and proprietary models; far below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance remains weak across tasks; limited spatial understanding and instruction following indicated by near-random accuracy in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8341.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QvQ-72B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QvQ-72B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source QvQ preview variant (72B) tested on the Jigsaw-Puzzles benchmark; shows mixed results with better spatial reasoning than some similarly sized models in specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QvQ-72B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Preview 72B variant from the QvQ / Qwen family evaluated on the Jigsaw-Puzzles tasks zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like spatial puzzle tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice tasks using multi-image input and exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard VLM multimodal inference; paper notes it can outperform some models on spatial reasoning tasks though overall ranking varied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy ~49.14% (table summary); performs better on some spatial reasoning tasks compared to certain other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows competence on specific single-step reasoning tasks but inconsistent multi-step performance; paper notes it 'achieves better results on spatial reasoning tasks' in some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms top open-source (InternVL3-78B, Qwen2.5-VL-72B) and proprietary reasoning-enhanced models but sometimes surpasses other models of similar size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Inconsistent performance across tasks; struggles on multi-step Order Restoration and Order Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8341.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aya-Vision-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aya-Vision-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLM (Aya family) evaluated on Jigsaw-Puzzles; shows very poor performance near random on many tasks according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aya-Vision-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Aya-Vision model at 32B parameter scale evaluated zero-shot on the Jigsaw-Puzzles benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D spatial jigsaw-like puzzle tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice with multi-image inputs; exact-match accuracy measured.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No specialized strategies reported; standard inference in the benchmark setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~33.28% (table summaries), near random for several tasks even at 32B scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Paper notes Aya-Vision series performs poorly across settings, indicating severe deficits in spatial understanding and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs much worse than top open-source and proprietary models; near random compared to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Accuracy remains near random even at 32B scale; fails basic spatial understanding tasks in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8341.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Small-3.1-24B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-Small-3.1-24B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small open-source multimodal instruct variant evaluated on Jigsaw-Puzzles that performs poorly and near random on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-Small-3.1-24B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mistral small 3.1 instruct-capable VLM at 24B evaluated zero-shot on Jigsaw-Puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>24B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like spatial puzzle tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice evaluation; exact-match accuracy per task.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard zero-shot multimodal inference; no special puzzle-solving strategies described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~36.87% (table summaries), near random baseline in several settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs poorly on spatial understanding tasks and thus shows little evidence of robust spatial reasoning; fails to scale well compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms larger open-source models and proprietary VLMs; substantially below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows severe deficits in spatial understanding and instruction following; near-random accuracy indicates inability to solve many puzzle tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8341.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8341.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-4-multimodal-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-4-multimodal-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal instruct-capable model evaluated on Jigsaw-Puzzles with modest performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-multimodal-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Phi-4 multimodal instruct model evaluated zero-shot on the benchmark's tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Jigsaw-Puzzles (all five tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D jigsaw-like spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice evaluation with exact-match accuracy as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard multimodal inference; no explicit puzzle-specific strategy described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy reported ~45.87% (table summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows partial competence on perceptual and some single-step reasoning tasks but is weak on multi-step Order Restoration and Order Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Better than some small/poorly performing open-source models but below top open-source and proprietary reasoning-enhanced models and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited multi-step reasoning capability; performance drops drastically on open-ended generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind the gap: Benchmarking spatial reasoning in vision-language models <em>(Rating: 2)</em></li>
                <li>LEGO-Puzzles: How good are mLLMs at multi-step spatial reasoning? <em>(Rating: 2)</em></li>
                <li>VGRP: Visual grid reasoning puzzle benchmark for large vision-language models <em>(Rating: 2)</em></li>
                <li>Capture: Evaluating spatial reasoning in vision language models via occluded object counting <em>(Rating: 2)</em></li>
                <li>Gemini: a family of highly capable multimodal models <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
                <li>The claude 3 model family: Opus, sonnet, haiku <em>(Rating: 1)</em></li>
                <li>Qwen2.5-vl technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8341",
    "paper_id": "paper-278910926",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Gemini-2.5-Pro",
            "name_full": "Gemini-2.5-Pro",
            "brief_description": "Proprietary, reasoning-enhanced vision-language model evaluated as the top-performing VLM on the Jigsaw-Puzzles benchmark, showing self-correction behavior under choice constraints.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.5-Pro",
            "model_description": "A proprietary multimodal (vision-language) model variant described in the paper as a reasoning-enhanced VLM; noted for self-correction behavior when constrained to candidate answers.",
            "model_size": null,
            "puzzle_name": "Jigsaw-Puzzles (Missing Piece Selection, Piece Localization, Connection Verification, Anomaly Detection, Order Restoration; Order Generation on Jigsaw-Puzzles-Lite)",
            "puzzle_type": "2D jigsaw-like puzzle tasks requiring grid-based spatial layout reconstruction, adjacency reasoning, local transformation detection (rotation/mirroring), and multi-step order restoration.",
            "task_setup": "Zero-shot evaluation with multi-image input; four-option multiple-choice for the five main tasks (exact-match accuracy); open-ended sequence generation for Order Generation on the Jigsaw-Puzzles-Lite subset; hardware scaled to model size.",
            "mechanisms_or_strategies": "Reasoning-enhanced variant; exhibits self-correction by re-evaluating visual input when initial prediction conflicts with provided answer choices; evaluated without external tools; benefits from reduced answer space under choice constraints.",
            "performance_metrics": "Overall accuracy 77.14% on full Jigsaw-Puzzles benchmark (highest among tested models); Order Generation (open-ended) on Jigsaw-Puzzles-Lite: 30.00% accuracy.",
            "evidence_of_spatial_reasoning": "Qualitative examples and analysis show self-correction and higher multi-step task performance relative to base variants; correlational analyses in paper show Missing Piece Selection performance predicts reasoning task performance, and Gemini-2.5-Pro succeeds more often on multi-step tasks under option constraints.",
            "comparisons": "Outperforms other tested VLMs (proprietary and open-source) in overall accuracy; still lags far behind humans (human overall 96.36%); reasoning-enhanced variants (e.g., Gemini-2.5-Flash-Thinking) show similar improvements over base versions.",
            "limitations_or_failure_cases": "Fails on open-ended Order Generation (30.00% vs human 94.09%); despite best-in-class performance, still &gt;20 percentage points below human overall; relies on choice constraints for effective self-correction and struggles to autonomously construct multi-step spatial chains.",
            "uuid": "e8341.0",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemini-2.5-Flash-Thinking",
            "name_full": "Gemini-2.5-Flash-Thinking",
            "brief_description": "A reasoning-enhanced variant of the Gemini family evaluated on the Jigsaw-Puzzles benchmark and showing substantial gains over its base Flash variant, especially on multi-step tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.5-Flash-Thinking",
            "model_description": "Proprietary reasoning-enhanced vision-language model variant (Thinking suffix indicates reasoning augmentation) evaluated zero-shot on puzzle tasks.",
            "model_size": null,
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like puzzle tasks requiring spatial perception, adjacency reasoning, and order restoration.",
            "task_setup": "Zero-shot, multiple-choice evaluation (four options) for main tasks; same Jigsaw-Puzzles QA templates; models supplied multi-image inputs.",
            "mechanisms_or_strategies": "Reasoning-enhanced training/decoding (paper marks as reasoning-enhanced); reportedly leverages enhanced internal reasoning heuristics compared to base Flash model; benefits observed particularly on Order Restoration.",
            "performance_metrics": "Overall accuracy reported at 72.42% (Table summary); substantial improvements over base Gemini-2.5-Flash (65.86%).",
            "evidence_of_spatial_reasoning": "Higher success on multi-step Order Restoration compared to base model; paper attributes improved multi-step performance to reasoning augmentation and self-correction dynamics under answer constraints.",
            "comparisons": "Shows consistent improvements vs Gemini-2.5-Flash; outperforms many open-source models but below Gemini-2.5-Pro; still well below human performance (96.36%).",
            "limitations_or_failure_cases": "Like other VLMs, performance drops drastically in open-ended Order Generation; improvements appear contingent on choice-constrained settings rather than fully autonomous multi-step generation.",
            "uuid": "e8341.1",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A proprietary multimodal LLM evaluated in the paper as one of the VLMs on the Jigsaw-Puzzles benchmark; shows moderate performance across the tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary multimodal model from OpenAI (GPT-4o family) evaluated zero-shot with multi-image inputs on the Jigsaw-Puzzles tasks; paper reports both GPT-4o and GPT-4o-mini variants.",
            "model_size": null,
            "puzzle_name": "Jigsaw-Puzzles (Missing Piece Selection, Piece Localization, Connection Verification, Anomaly Detection, Order Restoration)",
            "puzzle_type": "2D jigsaw-like spatial reasoning tasks.",
            "task_setup": "Zero-shot multi-image input; multiple-choice for main tasks with exact-match accuracy metric; hardware scaled to model size.",
            "mechanisms_or_strategies": "Standard zero-shot multimodal inference as evaluated; no specific chain-of-thought or special puzzle strategy described in the paper for GPT-4o.",
            "performance_metrics": "Reported overall accuracy ~60.71% (GPT-4o) and GPT-4o-mini ~58.97% across Jigsaw-Puzzles (from table summaries).",
            "evidence_of_spatial_reasoning": "Performs above random on single-step tasks (Piece Localization, Connection Verification, Anomaly Detection) but exhibits substantial gap on multi-step Order Restoration compared to humans, indicating limited multi-step spatial chain construction.",
            "comparisons": "Below reasoning-enhanced Gemini variants and some Claude models; substantially below human accuracy (96.36%).",
            "limitations_or_failure_cases": "Struggles with open-ended multi-step Order Generation (paper shows broad failure across models on generation task); performance improves under choice constraints but still limited.",
            "uuid": "e8341.2",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-3.7-Sonnet",
            "name_full": "Claude-3.7-Sonnet",
            "brief_description": "A Claude family proprietary multimodal model evaluated on the Jigsaw-Puzzles benchmark, showing solid performance on perceptual and many reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.7-Sonnet",
            "model_description": "Proprietary Claude-3.7 Sonnet multimodal model evaluated zero-shot on the Jigsaw-Puzzles tasks; one of the higher-performing proprietary baselines.",
            "model_size": null,
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like spatial reasoning and perception tasks.",
            "task_setup": "Zero-shot multiple-choice evaluation with multi-image input; exact-match accuracy reported per task.",
            "mechanisms_or_strategies": "Standard multimodal inference; no special puzzle-specific strategy detailed; benefits from high perceptual accuracy on missing-piece tasks.",
            "performance_metrics": "Overall accuracy reported ~64.88% (table summaries); strong performance on Missing Piece Selection and competitive single-step task results.",
            "evidence_of_spatial_reasoning": "Performs well on Missing Piece Selection and some single-step spatial tasks, consistent with capability in local spatial understanding; still lags on multi-step Order Restoration and fails to match human-level performance.",
            "comparisons": "Outperforms several open-source models; below top Gemini-2.5-Pro; far below human benchmark (96.36%).",
            "limitations_or_failure_cases": "Noted to underperform humans in multi-step reasoning and to show larger gaps on Order Generation open-ended task.",
            "uuid": "e8341.3",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Grok-2-Vision",
            "name_full": "Grok-2-Vision",
            "brief_description": "A proprietary vision-capable model (Grok family) evaluated on Jigsaw-Puzzles; shows middling performance relative to other proprietary models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grok-2-Vision",
            "model_description": "Grok family multimodal model with vision capabilities evaluated zero-shot on the Jigsaw-Puzzles benchmark.",
            "model_size": null,
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like spatial reasoning tasks.",
            "task_setup": "Zero-shot multiple-choice evaluation with multi-image input; exact-match accuracy.",
            "mechanisms_or_strategies": "No special puzzle-specific strategies described; evaluated in same zero-shot setting as other proprietary VLMs.",
            "performance_metrics": "Overall accuracy reported approximately 42.70% (table summary), with varying per-task strengths (higher on some perceptual tasks, weaker on multi-step).",
            "evidence_of_spatial_reasoning": "Exhibits partial competence on perceptual tasks but limited multi-step reasoning ability; performance often near or below stronger models.",
            "comparisons": "Underperforms top proprietary models (Claude, Gemini variants) and some high-quality open-source models; below human performance.",
            "limitations_or_failure_cases": "Fails more often on multi-step Order Restoration and Order Generation; shows limited robustness to distractors in Hard settings.",
            "uuid": "e8341.4",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen2.5-VL-72B",
            "name_full": "Qwen2.5-VL-72B",
            "brief_description": "An open-source large vision-language model (Qwen family) evaluated across Jigsaw-Puzzles tasks with competitive performance among open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-VL-72B",
            "model_description": "Open-source multimodal model from the Qwen family with a 72B variant, evaluated zero-shot on Jigsaw-Puzzles; listed among the stronger open-source performers.",
            "model_size": "72B",
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D grid-based jigsaw-like tasks requiring spatial layout and adjacency reasoning.",
            "task_setup": "Zero-shot multiple-choice evaluation; exact-match accuracy per task; multi-image input supported.",
            "mechanisms_or_strategies": "Standard multimodal zero-shot inference; no explicit reasoning augmentation described in the paper for this model.",
            "performance_metrics": "Overall accuracy reported ~63.08% (table summaries); competitive on perceptual tasks and some single-step reasoning tasks.",
            "evidence_of_spatial_reasoning": "Shows scaling benefits and reasonable performance on single-step tasks but reduced capability on multi-step Order Restoration and near-chance on Order Generation compared to humans.",
            "comparisons": "Among top open-source models (compared to many smaller open-source models); below proprietary reasoning-enhanced models such as Gemini-2.5-Pro; well below human performance.",
            "limitations_or_failure_cases": "Struggles with open-ended Order Generation and complex multi-step tasks; performance depends on model size (scales positively) and lacks explicit reasoning-enhancements.",
            "uuid": "e8341.5",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "InternVL3-78B",
            "name_full": "InternVL3-78B",
            "brief_description": "Open-source vision-language model (InternVL3 family) with a 78B variant that attains strong perceptual and spatial reasoning performance among open-source models on Jigsaw-Puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternVL3-78B",
            "model_description": "Open-source VLM from the InternVL3 family; 78B-parameter variant evaluated zero-shot on the Jigsaw-Puzzles benchmark; highlighted as a strong open-source performer.",
            "model_size": "78B",
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like puzzle tasks requiring spatial perception, adjacency reasoning, and order restoration.",
            "task_setup": "Zero-shot multiple-choice tasks with multi-image input; exact-match accuracy metric.",
            "mechanisms_or_strategies": "Standard inference; benefits from larger parameter count and training recipes; no explicit chain-of-thought or external reasoning module described.",
            "performance_metrics": "Overall accuracy reported ~68.79% (table summaries), making it one of the stronger open-source models.",
            "evidence_of_spatial_reasoning": "Performs well on Missing Piece Selection and single-step reasoning tasks; scaling with size correlates with higher spatial reasoning performance as noted in the paper.",
            "comparisons": "Outperforms many smaller open-source models and approaches some proprietary models on certain tasks; still below human participants.",
            "limitations_or_failure_cases": "Performance degrades on open-ended Order Generation; lacks explicit self-correction behavior emphasized for some proprietary reasoning-enhanced models.",
            "uuid": "e8341.6",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Kimi-VL-A3B-Thinking",
            "name_full": "Kimi-VL-A3B-Thinking",
            "brief_description": "A reasoning-enhanced open-source VLM (Kimi family) evaluated on Jigsaw-Puzzles, showing modest improvements over its Instruct variant.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Kimi-VL-A3B-Thinking",
            "model_description": "Open-source Kimi-VL 3B variant with 'Thinking' reasoning enhancement evaluated zero-shot on Jigsaw-Puzzles; compared with Instruct variant.",
            "model_size": "3B",
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-inspired spatial tasks (perception and multi-step reasoning).",
            "task_setup": "Zero-shot multiple-choice evaluation; exact-match accuracy; compared directly with Kimi-VL-A3B-Instruct baseline.",
            "mechanisms_or_strategies": "Reasoning-enhanced variant (Thinking) intended to improve multi-step reasoning; specific mechanisms not deeply detailed beyond being a 'thinking' variant.",
            "performance_metrics": "Overall accuracy improves from 42.04% (Instruct) to 44.43% (Thinking) according to table summaries.",
            "evidence_of_spatial_reasoning": "Small improvement on multi-step tasks suggests some benefit from reasoning enhancement, but absolute performance remains low and near random on difficult tasks.",
            "comparisons": "Slightly outperforms the Instruct variant; both perform substantially worse than large open-source and proprietary models; far below human performance.",
            "limitations_or_failure_cases": "Performance remains weak across tasks; limited spatial understanding and instruction following indicated by near-random accuracy in many settings.",
            "uuid": "e8341.7",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "QvQ-72B-Preview",
            "name_full": "QvQ-72B-Preview",
            "brief_description": "Open-source QvQ preview variant (72B) tested on the Jigsaw-Puzzles benchmark; shows mixed results with better spatial reasoning than some similarly sized models in specific tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QvQ-72B-Preview",
            "model_description": "Preview 72B variant from the QvQ / Qwen family evaluated on the Jigsaw-Puzzles tasks zero-shot.",
            "model_size": "72B",
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like spatial puzzle tasks.",
            "task_setup": "Zero-shot multiple-choice tasks using multi-image input and exact-match accuracy.",
            "mechanisms_or_strategies": "Standard VLM multimodal inference; paper notes it can outperform some models on spatial reasoning tasks though overall ranking varied.",
            "performance_metrics": "Overall accuracy ~49.14% (table summary); performs better on some spatial reasoning tasks compared to certain other open-source models.",
            "evidence_of_spatial_reasoning": "Shows competence on specific single-step reasoning tasks but inconsistent multi-step performance; paper notes it 'achieves better results on spatial reasoning tasks' in some comparisons.",
            "comparisons": "Underperforms top open-source (InternVL3-78B, Qwen2.5-VL-72B) and proprietary reasoning-enhanced models but sometimes surpasses other models of similar size.",
            "limitations_or_failure_cases": "Inconsistent performance across tasks; struggles on multi-step Order Restoration and Order Generation.",
            "uuid": "e8341.8",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Aya-Vision-32B",
            "name_full": "Aya-Vision-32B",
            "brief_description": "An open-source VLM (Aya family) evaluated on Jigsaw-Puzzles; shows very poor performance near random on many tasks according to the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aya-Vision-32B",
            "model_description": "Open-source Aya-Vision model at 32B parameter scale evaluated zero-shot on the Jigsaw-Puzzles benchmark.",
            "model_size": "32B",
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D spatial jigsaw-like puzzle tasks.",
            "task_setup": "Zero-shot multiple-choice with multi-image inputs; exact-match accuracy measured.",
            "mechanisms_or_strategies": "No specialized strategies reported; standard inference in the benchmark setting.",
            "performance_metrics": "Overall accuracy reported ~33.28% (table summaries), near random for several tasks even at 32B scale.",
            "evidence_of_spatial_reasoning": "Paper notes Aya-Vision series performs poorly across settings, indicating severe deficits in spatial understanding and instruction following.",
            "comparisons": "Performs much worse than top open-source and proprietary models; near random compared to human performance.",
            "limitations_or_failure_cases": "Accuracy remains near random even at 32B scale; fails basic spatial understanding tasks in the benchmark.",
            "uuid": "e8341.9",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Mistral-Small-3.1-24B-Instruct",
            "name_full": "Mistral-Small-3.1-24B-Instruct",
            "brief_description": "A small open-source multimodal instruct variant evaluated on Jigsaw-Puzzles that performs poorly and near random on many tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-Small-3.1-24B-Instruct",
            "model_description": "Open-source Mistral small 3.1 instruct-capable VLM at 24B evaluated zero-shot on Jigsaw-Puzzles.",
            "model_size": "24B",
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like spatial puzzle tasks.",
            "task_setup": "Zero-shot multiple-choice evaluation; exact-match accuracy per task.",
            "mechanisms_or_strategies": "Standard zero-shot multimodal inference; no special puzzle-solving strategies described.",
            "performance_metrics": "Overall accuracy reported ~36.87% (table summaries), near random baseline in several settings.",
            "evidence_of_spatial_reasoning": "Performs poorly on spatial understanding tasks and thus shows little evidence of robust spatial reasoning; fails to scale well compared to larger models.",
            "comparisons": "Underperforms larger open-source models and proprietary VLMs; substantially below human performance.",
            "limitations_or_failure_cases": "Shows severe deficits in spatial understanding and instruction following; near-random accuracy indicates inability to solve many puzzle tasks.",
            "uuid": "e8341.10",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Phi-4-multimodal-instruct",
            "name_full": "Phi-4-multimodal-instruct",
            "brief_description": "An open-source multimodal instruct-capable model evaluated on Jigsaw-Puzzles with modest performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-multimodal-instruct",
            "model_description": "Open-source Phi-4 multimodal instruct model evaluated zero-shot on the benchmark's tasks.",
            "model_size": null,
            "puzzle_name": "Jigsaw-Puzzles (all five tasks)",
            "puzzle_type": "2D jigsaw-like spatial reasoning tasks.",
            "task_setup": "Zero-shot multiple-choice evaluation with exact-match accuracy as metric.",
            "mechanisms_or_strategies": "Standard multimodal inference; no explicit puzzle-specific strategy described.",
            "performance_metrics": "Overall accuracy reported ~45.87% (table summaries).",
            "evidence_of_spatial_reasoning": "Shows partial competence on perceptual and some single-step reasoning tasks but is weak on multi-step Order Restoration and Order Generation.",
            "comparisons": "Better than some small/poorly performing open-source models but below top open-source and proprietary reasoning-enhanced models and humans.",
            "limitations_or_failure_cases": "Limited multi-step reasoning capability; performance drops drastically on open-ended generation tasks.",
            "uuid": "e8341.11",
            "source_info": {
                "paper_title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind the gap: Benchmarking spatial reasoning in vision-language models",
            "rating": 2,
            "sanitized_title": "mind_the_gap_benchmarking_spatial_reasoning_in_visionlanguage_models"
        },
        {
            "paper_title": "LEGO-Puzzles: How good are mLLMs at multi-step spatial reasoning?",
            "rating": 2,
            "sanitized_title": "legopuzzles_how_good_are_mllms_at_multistep_spatial_reasoning"
        },
        {
            "paper_title": "VGRP: Visual grid reasoning puzzle benchmark for large vision-language models",
            "rating": 2,
            "sanitized_title": "vgrp_visual_grid_reasoning_puzzle_benchmark_for_large_visionlanguage_models"
        },
        {
            "paper_title": "Capture: Evaluating spatial reasoning in vision language models via occluded object counting",
            "rating": 2,
            "sanitized_title": "capture_evaluating_spatial_reasoning_in_vision_language_models_via_occluded_object_counting"
        },
        {
            "paper_title": "Gemini: a family of highly capable multimodal models",
            "rating": 1,
            "sanitized_title": "gemini_a_family_of_highly_capable_multimodal_models"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "The claude 3 model family: Opus, sonnet, haiku",
            "rating": 1,
            "sanitized_title": "the_claude_3_model_family_opus_sonnet_haiku"
        },
        {
            "paper_title": "Qwen2.5-vl technical report",
            "rating": 1,
            "sanitized_title": "qwen25vl_technical_report"
        }
    ],
    "cost": 0.01826825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models
26 Aug 2025</p>
<p>Zesen Lyu 
Hangzhou Institute for Advanced Study
UCAS</p>
<p>Zhejiang Lab</p>
<p>Dandan Zhang 
Zhejiang Lab</p>
<p>Wei Ye 
Hangzhou Institute for Advanced Study
UCAS</p>
<p>Fangdi Li 
Zhejiang Lab</p>
<p>Zhejiang University</p>
<p>Zhihang Jiang jiangzhihang23@mails.ucas.ac.cn 
Hangzhou Institute for Advanced Study
UCAS</p>
<p>Zhejiang Lab</p>
<p>Yao Yang yangyao@zhejianglab.com 
Zhejiang Lab</p>
<p>Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models
26 Aug 202522527B19227F5EBA4851314188528DA8arXiv:2505.20728v4[cs.AI]
Spatial reasoning is a core component of human cognition, enabling individuals to perceive, comprehend, and interact with the physical world.It relies on a nuanced understanding of spatial structures and inter-object relationships, serving as the foundation for complex reasoning and decision-making.To investigate whether current vision-language models (VLMs) exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark consisting of 1,100 carefully curated real-world images with high spatial complexity.Based on this dataset, we design five tasks to rigorously evaluate VLMs' spatial perception, structural understanding, and reasoning capabilities, while deliberately minimizing reliance on domainspecific knowledge to better isolate and assess the general spatial reasoning capability.We conduct a comprehensive evaluation across 24 state-of-the-art VLMs.The results show that even the strongest model, Gemini-2.5-Pro,achieves only 77.14% overall accuracy and performs particularly poorly on the Order Generation task, with only 30.00% accuracy, far below the 90%+ performance achieved by human participants.This persistent gap underscores the need for continued progress, positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for advancing spatial reasoning research in VLMs.Our project page is at https: //zesen01.github.io/jigsaw-puzzles.</p>
<p>Introduction</p>
<p>The road to artificial general intelligence (AGI) demands more than language or vision alone: it requires models to possess a human-like spatial reasoning capability by constructing structured representations of the physical world (Lake et al., 2017).Spatial reasoning refers not just to the perception of visual input, but to the capability to comprehend spatial arrangements, model structural relations,  and infer the geometry and layout of a scene.These capabilities are fundamental to human cognition and develop naturally through everyday perception and interaction (Ishikawa and Newcombe, 2021).In contrast, current VLMs, while highly capable in tasks such as image captioning (Young et al., 2014;Lin et al., 2014;Sharma et al., 2018), visual question answering (Krishna et al., 2017;Singh et al., 2019;Marino et al., 2019), and image-text retrieval (Schuhmann et al., 2021;Thapliyal et al., 2022;Bitton-Guetta et al., 2023), consistently struggle with tasks requiring spatial reasoning (Stogiannidis et al., 2025).We show an example in Figure 1 and report the performance of some tested VLMs on Jigsaw-Puzzles in Figure 2.This gap underscores a critical limitation: while current VLMs have made substantial progress in basic visual understanding, they continue to struggle with structured spatial reasoning, which is essential for grounded understanding in real-world scenarios.Bridging this gap is essential for progressing towards generalizable human-like spatial cognition and ultimately AGI.</p>
<p>However, existing benchmarks have yet to provide a comprehensive evaluation of spatial reasoning capability in VLMs under complex, realworld visual scenarios.Some works (Fu et al., 2024;Li et al., 2024;Liu et al., 2024;Yue et al., 2024) focus primarily on foundational visual understanding by systematically evaluating perception, comprehension, and basic visual reasoning, revealing notable limitations in these areas.Although a few recent efforts (Pothiraj et al., 2025;Stogiannidis et al., 2025;Ren et al., 2025;Tang et al., 2025) have attempted to evaluate the spatial reasoning capability of VLMs, they often rely on overly synthetic settings, task-specific constraints, or domain-dependent priors (Song et al., 2025) (See Appendix A for examples), limiting the capability to capture generalizable spatial reasoning under natural visual conditions.A truly effective evaluation of human-like spatial reasoning capability should model the task as a multi-stage cognitive process-beginning with perception, advancing through structural understanding, and culminating in high-level reasoning.Such reasoning must be grounded in the visual richness and ambiguity of real images, requiring the integration of spatial structural modeling and goal-directed reasoning (Chen et al., 2024).Yet, this critical dimension of spatial cognition remains largely overlooked in existing benchmarks, underscoring the need for new benchmarks that move beyond narrow task formulations and embrace the full complexity of spatial reasoning.</p>
<p>To overcome the limitations of existing benchmarks in evaluating spatial reasoning capability of VLMs under real-world conditions, we introduce Jigsaw-Puzzles, a novel benchmark inspired by the cognitive mechanisms underlying human puzzlesolving.Puzzle-solving naturally reflects the multistage cognitive process (Fissler et al., 2018), making it a compelling testbed for spatial reasoning in</p>
<p>Benchmark</p>
<p>Understanding Reasoning High Visual Complexity Great Scalability Automated Construction Capture (Pothiraj et al., 2025)      Mind the Gap (Stogiannidis et al., 2025)      VGRP (Ren et al., 2025)      LEGO-Puzzles (Tang et al., 2025) VLMs.Unlike earlier studies that applied puzzlesolving primarily to visual representation learning (Noroozi and Favaro, 2016;Wang et al., 2022;Shen et al., 2024), our benchmark employs it to evaluate the spatial reasoning ability of VLMs under realistic visual conditions.In total, Jigsaw-Puzzles comprises 1,100 carefully curated real-world images and features five different tasks.First, we begin with the Missing Piece Selection task to evaluate VLMs' basic spatial understanding capability, which serves as the essential foundation for spatial reasoning.Building on this foundation, we introduce four reasoningcentric tasks: Piece Localization, Connection Verification, Anomaly Detection, and Order Restoration.These tasks are designed to assess various facets of spatial reasoning, including adjacency modeling, local structural consistency, spatial localization, geometric transformation understanding, and multi-step spatial reasoning.
     Jigsaw-Puzzles (Ours)     
Compared to existing benchmarks for evaluating spatial reasoning capability in VLMs, Jigsaw-Puzzles offers three key advantages, as summarized in Table 1: (1) Higher visual complexity.Jigsaw-Puzzles uses real-world images with diverse and rich visual elements, and significantly outperforms benchmarks based on synthetic images (Ren et al., 2025;Stogiannidis et al., 2025;Tang et al., 2025) and simple visual scenes (Pothiraj et al., 2025).This enables more realistic and challenging spatial reasoning evaluation.(2) Greater scalability.Any natural image that satisfies the construction rules can be directly used to generate puzzle tasks, without the need to manually synthesize target images.</p>
<p>(3) Fully automated construction pipeline.All Jigsaw-Puzzles tasks are generated automatically without manual annotation, with each question paired with a unique deterministic answer.This feature enables low-cost dataset construction and facilitates continuous expansion and refinement.</p>
<p>In summary, we introduce Jigsaw-Puzzles, a novel benchmark for systematically evaluating the human-like spatial reasoning capability of VLMs in realistic visual settings.Our main contributions are as follows: A new benchmark for spatial reasoning.We introduce Jigsaw-Puzzles, a puzzle-inspired benchmark constructed through a fully automated pipeline that improves existing benchmarks in visual complexity and scalability, while enabling structured evaluation of spatial reasoning in VLMs.</p>
<p>Comprehensive evaluation and analysis.We evaluate 24 state-of-the-art VLMs on Jigsaw-Puzzles and conduct detailed analysis.Our findings expose consistent limitations in current VLMs and provide insights to guide future improvements in spatial reasoning capability.Open-sourced dataset and construction tools.We release the full dataset along with the automated generation scripts to support the evaluation and continued advancement of spatial reasoning in VLMs under real-world visual scenarios, as well as to facilitate future benchmark expansion.</p>
<p>Related Work</p>
<p>General VLMs Evaluation Benchmarks.With the rapid progress of VLMs, systematically evaluating their diverse capabilities has become a key challenge.Although many benchmarks have been introduced, most focus primarily on visual understanding.MME (Fu et al., 2024) evaluates instruction following, perception, and basic reasoning across 14 subtasks, revealing persistent issues like object hallucination and limited spatial understanding.SEED-Bench (Li et al., 2024) includes 19,000 multiple-choice questions across 12 dimensions and shows continued struggles with text recognition and temporal reasoning.MMBench (Liu et al., 2024) offers fine-grained bilingual evaluations, enhancing the robustness of multilingual assessment.MMMU (Yue et al., 2024) provides 11,500 questions across 183 subfields and 30 image types to test expert-level reasoning, yet even advanced models like Gemini display notable knowledge gaps.While these benchmarks have advanced the evaluation of perceptual and semantic understanding, none systematically assess spatial reasoning-the core aspect of human cognition.This highlights the pressing need for more challenging and diagnostic benchmarks specifically targeting spatial reasoning capability in VLMs.Spatial Reasoning Evaluation Benchmarks in VLMs.Several recent benchmarks have aimed to evaluate the spatial reasoning capability of VLMs.Capture (Pothiraj et al., 2025) assesses occluded object counting, revealing that VLMs struggle to form coherent spatial representations under occlu-sion.Mind the Gap (Stogiannidis et al., 2025) evaluates spatial relations, navigation, and mental rotation, showing that VLMs often perform near chance level, indicating limited spatial cognition.VGRP (Ren et al., 2025) introduces 20 visual grid puzzles across varying difficulty levels to assess visual perception, rule-following, and logical reasoning.LEGO-Puzzles (Tang et al., 2025) provides 1,100 visual QA pairs over 11 subtasks to measure basic and multi-step spatial reasoning.Results consistently show that current VLMs struggle with perceptual complexity, rotation reasoning, and sequential reasoning.Despite these efforts, most benchmarks rely on simplified scenarios, failing to reflect the complexity of real-world spatial environments, thereby limiting their generalizability.More diagnostic benchmarks grounded in natural visual settings are needed to advance human-level spatial reasoning in VLMs.</p>
<p>Jigsaw-Puzzles</p>
<p>In this section, we introduce Jigsaw-Puzzles, a scalable and comprehensive benchmark designed to evaluate the spatial reasoning capability of VLMs in realistic visual environments.Specifically, Section 3.1 outlines the motivation and definition of each task, while Section 3.2 describes the dataset construction process, including image selection and the automated generation of question-answer pairs.</p>
<p>Task Definition</p>
<p>To systematically evaluate the spatial reasoning capability of VLMs, we design tasks around the core cognitive stages underlying human spatial reasoning-seeing, understanding, and reasoning.Inspired by the human process of solving jigsaw puzzles, our benchmark simulates how individuals integrate fragmented visual information into a coherent whole: beginning with the perception of local visual cues, followed by the comprehension of spatial relationships, and culminating in multi-step spatial reasoning to reconstruct the original scene.This sequence naturally reflects the progression from low-level perception to high-level spatial reasoning.Accordingly, the tasks span spatial understanding, single-step, and multi-step spatial reasoning, collectively providing a comprehensive evaluation across different levels of spatial reasoning.Figure 3 shows examples of each task in Jigsaw-Puzzles.</p>
<p>Task 1: Missing Piece Selection.The task evaluates VLMs' spatial understanding capability.Given  an image with a missing region, VLMs need to select the correct patch from four candidates.We define two difficulty levels: Easy, where distractors are randomly chosen, and Hard, where distractors are selected using CLIP (Radford et al., 2021) similarity to closely resemble the ground-truth patch, increasing the task's difficulty.Task 2: Piece Localization.This task evaluates spatial localization as a representative single-step spatial reasoning capability.Given a partially masked image and one masked patch, VLMs must identify the patch's original position.Difficulty is controlled by grid size and number of masked regions: Easy (22 with two masks), Hard (33 with four masks), increasing spatial complexity.Task 3: Connection Verification.This task evaluates adjacency reasoning, which also falls under single-step spatial reasoning.The full image is divided into 22 grids, and two patches are randomly selected.VLMs are asked to determine their spatial relationship in the original image (e.g., above-below, left-right, or non-adjacent).Task 4: Anomaly Detection.This task targets local spatial transformation detection, a process that inherently involves single-step spatial reasoning.One region in 22 grids is randomly rotated, mirrored, or left unchanged.The model must detect the change, locate the region, and identify the transformation.Task 5: Order Restoration.This task integrates the capabilities assessed in the previous tasks and serves as a complex multi-step spatial reasoning challenge.A complete image is split into four shuffled patches.VLMs must identify the correct order to reconstruct the original spatial layout.</p>
<p>Overall, the five puzzle-inspired tasks in Jigsaw-Puzzles cover a broad spectrum of spatial reasoning challenges-from basic spatial understanding to single-step and multi-step spatial reasoning-enabling a comprehensive evaluation of spatial reasoning in VLMs.</p>
<p>Dataset Curation</p>
<p>As illustrated in Figure 4, our dataset curation pipeline consists of two main stages: data collection and QA generation.</p>
<p>Data Collection.We integrate data collection and quality control into a unified process.Starting from the CC3M (Sharma et al., 2018) dataset, we apply task-specific filtering criteria-including minimum resolution and aspect ratio constraints-to construct an initial image pool of approximately 10,000 candidate images.Two human experts iteratively review the image pool while incrementally refining a shared set of filtering rules.Based on these evolving rules, they collaboratively filter the initial dataset to obtain the final set of high-quality, structurally diverse images.See Appendix B for the rules pool.To enhance generalizability, we emphasize semantic and structural diversity throughout the dataset.</p>
<p>QA Generation.To support scalable and consistent QA pairs generation, each task type is associated with a specific construction template.QA pairs are automatically generated using these templates.Figure 3 Instruct (Mistral, 2025).For proprietary models, we evaluate Claude-[3.5/3.7]-Sonnet(Anthropic, 2024), Gemini-[2.0/2.5]-Flash,Gemini-2.5-Flash-Thinking,Gemini-2.5-Pro(Anil et al., 2023), GPT-4o, GPT-4o-mini (Achiam et al., 2023), and Grok-2-Vision (Grok, 2024).Notably, QvQ-72B-Preview, Kimi-VL-A3B-Thinking, Gemini-2.5-Flash-Thinking,and Gemini-2.5-Proare categorized as reasoning-enhanced models.All models, supporting multi-image input, are evaluated in a zero-shot setting with hardware scaled to their parameter size, see details in Appendix C. Evaluation Metric.Since each QA pair in Jigsaw-Puzzles has a single correct answer, we use exact match accuracy (%) as the primary metric to evaluate VLMs' performance on each task.Baselines.We provide two baselines for comparison: (1) Random, which assumes equal probability across all options and calculates expected accuracy accordingly.(2) p-value-based critical value, which reports the minimum accuracy required to outperform random guessing at a significance level of p=0.05.Human Performance.To evaluate human performance, we construct a subset called Jigsaw-Puzzles-Lite by sampling 220 images from the full dataset.Three human participants complete all tasks on this subset under the same conditions as VLMs-without access to any external tools or the internet.Their performance serves as an empirical upper bound for spatial reasoning capability.</p>
<p>Main Results</p>
<p>Table 2, 3 report the performance of 24 VLMs on Jigsaw-Puzzles.Building on these results, we conduct a comprehensive and systematic analysis.We summarize several key findings as below.VLMs.As shown in Table 3, human participants consistently outperform VLMs, achieving an overall accuracy of 96.36%.By comparison, current VLMs perform considerably worse, with even the strongest models-Gemini-2.5-Pro-laggingmore than 20 percentage points behind human accuracy across all tasks.The persistent gap between humans and VLMs highlights the demanding nature of Jigsaw-Puzzles and affirms its utility as a robust benchmark for spatial reasoning evaluation.Although open source models generally perform poorly by comparison, certain models, such as the InternVL3 series and Qwen2.5-VL-72B,achieve perceptual understanding on par with proprietary VLMs.Notably, both the Aya-Vision series and the Mistral-Small-3.1-24B-Instructmodels perform poorly across all settings, even at the 32B scale, accuracy remains near random, revealing severe deficits in spatial understanding and instruction following.In single-step spatial reasoning tasks-Piece Localization, Connection Verification, and Anomaly Detection-most VLMs surpass the p-value-based critical value, indicating emerging competence in basic spatial reasoning.However, strong performance remains concentrated in only a few models, particularly reasoningenhanced proprietary models and the latest opensource InternVL3 series.This disparity becomes even more evident in the multi-step spatial reasoning task-Order Restoration, indicating that most VLMs struggle with complex spatial reasoning.</p>
<p>Spatial Reasoning Remains a Challenge for</p>
<p>In conclusion, Jigsaw-Puzzles effectively distinguishes VLMs across a spectrum of spatial reasoning capability-from basic understanding to complex multi-step reasoning.As shown by the results in Table 2, substantial room for improvement remains, particularly in multi-step spatial reasoning.Foundational Spatial Understanding Shapes Reasoning Performance.We analyze task similarity on Jigsaw-Puzzles by computing Pearson correlation coefficients between each task and all others, as proposed by Zhang et al. (2025), as shown in Figure 5.The results show that performance on the Missing Piece Selection task-a proxy for spatial understanding, is strongly correlated with performance on spatial reasoning tasks.In contrast, VLMs with weaker spatial understanding often struggle with reasoning tasks, with some per-  forming worse than random on reasoning-intensive tasks.This pattern reflects the human cognitive progression from perception to understanding to reasoning, underscoring the foundational role of spatial understanding in enabling higher-level spatial reasoning in VLMs.Spatial Reasoning Scales with VLM size.We analyze the relationship between VLM size and overall performance on Jigsaw-Puzzles.As shown in Figure 6, our results reveal that VLM accuracy consistently increases with model size, both across all models and within specific fami- lies (e.g., InternVL3, Qwen2.5-VL).This positive correlation suggests that spatial reasoning capability-like other cognitive competencies (Wang et al., 2024b)-benefits from larger model capacity, which scales with parameter count.Reasoning-Enhanced Models Show Superior Spatial Reasoning Performance.To assess the spatial reasoning capability of reasoning-enhanced VLMs, we evaluate Gemini-2.5-Flash-Thinking,Gemini-2.5-Pro,Kimi-VL-A3B-Thinking and QvQ-72B-Preview.Except for Gemini-2.5-Pro,each model has a corresponding base version for comparison.As shown in Table 2, these enhanced VLMs consistently achieve higher overall accuracy.For example, Kimi-VL-A3B-Thinking improves from 42.04% to 44.43%, and Gemini-2.5-Flash-Thinkingrises from 65.86% to 72.42%.Although QvQ-72B-Preview overall underperforms Qwen2-VL-72B, it achieves better results on spatial reasoning tasks.Notably, Gemini-2.5-Proachieves the highest overall accuracy (77.14%) among all VLMs tested.Furthermore, the largest improvements occur in the multi-step spatial reasoning task, Order Restoration, where reasoning-enhanced VLMs outperform their base counterparts more sub- stantially than in single-step tasks.To explain this, we analyze cases where only Gemini-2.5-Proanswers correctly, with Figure 7 presenting one such example.Gemini-2.5-Prodemonstrates a form of self-correction: when the model's initial prediction is not among the provided options, it will reevaluate the visual input and revise its judgment.This behavior, facilitated by the reduced answer space under choice constraints, may contribute to the superior performance of reasoning-enhanced models in the Order Restoration task.</p>
<p>Further Exploring Multi-Step Spatial Reasoning in VLMs.To further evaluate VLMs' multistep spatial reasoning beyond the constraints of predefined choices, we introduce the Order Generation task based on Jigsaw-Puzzles-Lite.In this setting, VLMs must directly generate the correct sequence of puzzle pieces without relying on answer options, thereby more authentically simulating open-ended spatial reasoning.As shown in Figure 8, current VLMs consistently struggle with this task-Gemini-2.5-Pro,the best-performing model, achieves only 30.00% accuracy, in stark contrast to 94.09% by human participants.This finding reveals that, despite exhibiting strong self-correction behavior under option constraints, existing VLMs face considerable challenges in autonomously constructing coherent spatial reasoning chains.This highlights a significant gap between current VLMs and human-level spatial reasoning in open-ended scenarios.</p>
<p>We propose Jigsaw-Puzzles, a novel benchmark for systematically evaluating the spatial reasoning capability of VLMs in real-world visual scenes.Through extensive experiments on 24 representative VLMs, we identify persistent gaps between current VLMs and human-level spatial reasoning-especially in multi-step spatial reasoning tasks.Jigsaw-Puzzles provides a scalable and cognitively grounded benchmark to advance future research on spatial reasoning in VLMs.</p>
<p>Limitations</p>
<p>While Jigsaw-Puzzles provides a structured benchmark tailored for 2D spatial reasoning in static images, it currently does not address 3D perception, temporal sequences, or embodied contexts-each of which represents an important and orthogonal axis of spatial cognition.We view this as a natural next step and encourage future work to extend the benchmark in these directions. Filtering out blurry, low-resolution, or visually ambiguous images.</p>
<p> Excluding images lacking semantic clarity or spatial structure.</p>
<p> Discarding images with structural ambiguity (e.g., multiple valid puzzle arrangements).</p>
<p> Eliminating misaligned images or those with overly small visual elements after cropping, which hinder spatial reasoning.</p>
<p>Task-Specific Template.The following are detailed templates for each task.Note that <image_x> denotes a placeholder for the corresponding image input.We evaluate open-source VLMs using hardware configurations scaled to model size.Models with fewer than 20B parameters run on a single NVIDIA A100 80GB GPU.Those between 20B and 40B use two NVIDIA A100 80GB GPUs, while models exceeding 40B are evaluated on four NVIDIA A100 80GB GPUs to meet their greater memory and computational demands.</p>
<p>Figure 1 :
1
Figure 1: Jigsaw-Puzzles example.While human participants effortlessly reconstruct the original spatial layout, all tested VLMs fail to recover the correct order.</p>
<p>Figure 2 :
2
Figure 2: Evaluation of VLMs on Jigsaw-Puzzles.The plot reports the accuracy of 8 representative VLMs on 5 tasks.</p>
<p>Figure 3 :
3
Figure 3: Task examples of Jigsaw-Puzzles.Note: the questions above are slightly simplified for clarity and brevity, and the blue option indicates the correct answer.</p>
<p>Figure 4 :
4
Figure 4: Dataset curation pipeline.Step 1 filters candidate images through expert-defined rules to build a spatial reasoning dataset.Step 2 uses automated templates to generate task-specific QA pairs from the curated images.</p>
<p>illustrates simplified examples of the templates, full versions are provided in Appendix B. 4 Evaluation on Jigsaw-Puzzles 4.1 Experimental Setting Benchmark Models.We evaluate 24 VLMs on Jigsaw-Puzzles, covering a diverse range of model scales and training paradigms.For open-source models, we evaluate Qwen2-VL-72B (Wang et al., 2024a), QvQ-72B-Preview (Qwen, 2024), Qwen2.5-VL-<a href="Bai et al., 2025">7B/32B/72B</a>, InternVL3-[8B/14B/38B/78B] (Zhu et al., 2025), Kimi-VL-A3B-[Instruct/Thinking] (Du et al., 2025), Phi-4-multimodal-instruct (Abouelenin et al., 2025), Aya-Vision-[8B/32B] (Dash et al., 2025), and Mistral-Small-3.1-24B-</p>
<p>Figure 5 :
5
Figure 5: Task Similarity Heatmap.The heatmap illustrates the pairwise correlation between tasks in our benchmark, measured using Pearson correlation coefficients.Task names are abbreviated using the initials of each word (e.g., Missing Piece Selection  MPS).The suffixes _e and _h indicate the Easy and Hard settings, respectively.</p>
<p>Figure 6 :
6
Figure 6: Relationship between VLM size and performance on Jigsaw-Puzzles.Each point represents a VLM, with its accuracy plotted against log-scaled parameter size.A clear positive correlation is observed both across and within model families, indicating that larger models tend to exhibit stronger performance.</p>
<p>Figure 7 :
7
Figure 7: An example of self-correction.Red shows the initial incorrect answer generated by Gemini-2.5-Pro;Blue indicates the ground-truth answer; Green illustrates the model's self-correction process.</p>
<p>Figure 8 :
8
Figure 8: Evaluation of Order Restoration and Order Generation tasks on Jigsaw-Puzzles-Lite.Without option constraints, VLM accuracy drops significantly-peaking at just 30.00% and falling far short of human performance.</p>
<p>Figure 9 and
9
Figure 9 and Figure 10 illustrate two representative question types commonly used to evaluate spatial reasoning capability of VLMs.The tested images are not based on real-world scenes, which limits the capability to evaluate spatial reasoning in VLMs under realistic conditions.</p>
<p>Figure 9 :
9
Figure 9: An example of Mind the Gap (Stogiannidis et al., 2025).</p>
<p>Figure 10 :
10
Figure 10: An example of LEGO-Puzzles (Tang et al., 2025).</p>
<p>Figure11shows examples of images that were rejected and accepted based on the filtering rules, the following are the rules defined by experts during the image selection process: Removing images containing explicit or violent content.</p>
<p>Figure 11 :
11
Figure 11: Top: examples of images rejected by expertdefined filtering rules.Bottom: examples of highquality images that pass the rules.</p>
<p>Figure 12 :
12
Figure 12: Template of Missing Piece Selection, notably, the templates for the Easy and Hard settings are identical.</p>
<p>Figure 13 :
13
Figure 13: Template of Piece Localization (Easy).</p>
<p>Figure 14 :
14
Figure 14: Template of Piece Localization (Hard).</p>
<p>Figure 15 :
15
Figure 15: Template of Connection Verification.</p>
<p>Figure 16 :
16
Figure 16: Template of Anomaly Detection.</p>
<p>Figure 17 :
17
Figure 17: Template of Order Restoration.Note: <option list> serves as a placeholder for the answer choices.The text in parentheses is an example and should be removed in actual use.One option is the correct answer, while the remaining three are randomly drawn from the other 23 candidates.</p>
<p>Figure 18 :
18
Figure 18: Template of Order Generation.</p>
<p>Table 1 :
1
Comparison of spatial reasoning benchmarks.</p>
<p>Table 2 :
2
Full Evaluation Results of 24 VLMs on Jigsaw-Puzzles.VLMs are grouped into proprietary and opensource categories.Dark Green and Light Green indicate the top-1 and top-2 performance within each group, respectively.Results of reasoning-enhanced are marked in bold.We also highlight the top three models based on their overall performance, using Dark Blue , Medium Blue , and Light Blue , respectively.
ModelsMissing Piece Selection Easy HardPiece Localization Easy HardConnection VerificationAnomaly DetectionOrder RestorationOverallBaselineRandom Guessing25.0025.0050.0025.0033.3328.1325.0030.21 Random (p &lt; 0.05)27.3027.3052.5027.3035.7030.5027.3032.56Proprietary ModelsGrok2-Vision64.5552.4553.0041.0034.9127.7325.2742.70GPT-4o-mini96.4583.6459.4537.8244.3657.9133.1858.97GPT-4o95.0089.1861.5553.4541.0953.1831.5560.71Claude-3.5-Sonnet99.7394.5562.4541.0945.6467.4535.0063.70Claude-3.7-Sonnet99.5595.0960.2744.5547.9167.0039.8264.88Gemini-2.0-Flash92.0985.6463.5554.0044.9168.7334.2763.31Gemini-2.5-Flash98.8292.4564.5554.5548.8267.3634.4565.86Gemini-2.5-Flash-Thinking99.5594.7376.6451.2757.9162.0064.8272.42Gemini-2.5-Pro99.9197.1878.8261.0959.3670.0073.6477.14Open-source ModelsKimi-VL-A3B-Instruct67.9152.5551.4529.8237.9121.8232.8242.04Kimi-VL-A3B-Thinking84.6458.0956.3632.6428.0030.9120.3644.43Phi-4-multimodal-instruct63.4551.6460.9137.4536.6443.3627.6445.87Qwen2.5-VL-7B87.1863.2754.2736.1838.5545.0028.0950.36Aya-Vision-8B26.8227.2749.6426.5535.0012.9124.7328.99InternVL3-8B98.0985.4553.9135.4544.9146.8234.3657.00InternVL3-14B99.7388.7359.6440.1851.7349.0940.9161.43Mistral-Small-3.1-24B-Instruct26.9128.5554.2731.2738.2752.3626.4536.87Qwen2.5-VL-32B97.2776.8262.0940.3650.0961.5533.6460.26Aya-Vision-32B24.2725.2751.4528.3637.1841.4525.0033.28InternVL3-38B99.0091.3663.4542.6456.7330.7354.6462.65Qwen2-VL-72B95.5575.3655.2740.6440.5542.3633.5554.75QVQ-72B-Preview77.8252.1853.0936.7341.8247.7334.6449.14Qwen2.5-VL-72B99.3687.8265.9145.2743.3658.8241.0063.08InternVL3-78B99.7395.5569.4552.2749.2758.1857.0968.79ModelsMissing Piece Selection Easy HardPiece Localization Easy HardConnection VerificationAnomaly DetectionOrder RestorationOverallHuman Performance99.55100.0095.4591.3693.1897.2797.7396.36Proprietary ModelsClaude-3.7-Sonnet100.0095.4555.4547.2742.7368.1838.6463.96Gemini-2.5-Flash98.1893.1858.1855.4542.2766.8232.2763.76Gemini-2.5-Flash-Thinking99.5595.9171.8251.8255.0060.9157.2770.33Gemini-2.5-Pro100.0096.3677.7356.8257.2771.3670.9175.78Open-source ModelsQwen2.5-VL-72B99.0986.8267.7342.2740.0057.7333.6461.04InternVL3-78B99.5595.4570.4553.6444.5561.3656.8268.83</p>
<p>Table 3 :
3
Comparing Top-Performing VLMs with Human Performance on Jigsaw-Puzzles-Lite.The human performance is highlighted in Dark Green .Results of reasoning-enhanced are marked in bold.The top three overall performance are highlighted in Dark Blue , Medium Blue , and Light Blue , respectively.</p>
<p>AcknowledgmentsThe authors wish to thank the anonymous reviewers for their helpful comments.This work was supported by the Key R&amp;D Program of Zhejiang (2024C01036).
Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, arXiv:2503.01743Congcong Chen, and 1 others. 2025. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 1 others. 2023arXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang ; Elovici, Gabriel Stanovsky, Roy Schwartz, arXiv:2502.13923Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionNitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, YuvalJun Tang, and 1 others. 2025. 2023arXiv preprintQwen2. 5-vl technical report</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, arXiv:2505.08751Walter Beller-Morales, and 1 others. 2025. Aya vision: Advancing the frontier of multilingual multimodality. arXiv preprint</p>
<p>Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, arXiv:2504.07491Kimi-vl technical report. 2025arXiv preprintand 1 others</p>
<p>Jigsaw puzzling taps multiple cognitive abilities and is a potential protective factor for cognitive aging. Patrick Fissler, Olivia Caroline Kster, Daria Laptinskaya, Laura Sophia Loy, Christine Af Von Arnim, Iris-Tatjana Kolassa, Frontiers in aging neuroscience. 104080852018</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, arXiv:2306.133942024Preprint</p>
<p>Bringing grok to everyone. Toru Ishikawa and Nora S Newcombe. 2021. Why spatial is special in education, learning, and everyday activities. Grok, 2024</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40e2532017</p>
<p>Seedbench: Benchmarking multimodal large language models. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, Lawrence Zitnick, Computer vision-ECCV 2014: 13th European conference. zurich, SwitzerlandSpringer2014. September 6-12, 201413</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, European conference on computer vision. Springer2024Conghui He, Ziwei Liu, and 1 others</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognition2019</p>
<p>Unsupervised learning of visual representations by solving jigsaw puzzles. European conference on computer vision. Springer2025. 2016Mistral small 3.1. Mehdi Noroozi and Paolo Favaro</p>
<p>Capture: Evaluating spatial reasoning in vision language models via occluded object counting. Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal, arXiv:2504.154852025arXiv preprint</p>
<p>Qvq: To see the world with wisdom. Qwen, 2024</p>
<p>Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, International conference on machine learning. PmLR</p>
<p>Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine Ssstrunk, Filippos Kokkinos, arXiv:2503.23064Vgrp-bench: Visual grid reasoning puzzle benchmark for large vision-language models. 2025arXiv preprint</p>
<p>Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. 2021arXiv preprint</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Satjip: Spatial and augmented temporal jigsaw puzzles for video anomaly detection. Liheng Shen, Tetsu Matsukawa, Einoshin Suzuki, Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer2024</p>
<p>Towards vqa models that can read. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue, arXiv:2504.10342Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. 2025arXiv preprint</p>
<p>Ilias Stogiannidis, Steven Mcdonagh, Sotirios A Tsaftaris, arXiv:2503.19707Mind the gap: Benchmarking spatial reasoning in vision-language models. 2025arXiv preprint</p>
<p>Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong Duan, Yanan Sun, Zhening Xing, Wenran Liu, Kaifeng Lyu, Kai Chen, arXiv:2503.19990Lego-puzzles: How good are mllms at multi-step spatial reasoning?. 2025arXiv preprint</p>
<p>Jordi Ashish V Thapliyal, Xi Pont-Tuset, Radu Chen, Soricut, arXiv:2205.12522Crossmodal-3600: A massively multilingual multimodal evaluation dataset. 2022arXiv preprint</p>
<p>Video anomaly detection by solving decoupled spatiotemporal jigsaw puzzles. Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, Di Huang, European Conference on Computer Vision. Springer2022</p>
<p>Wenbin Ge, and 1 others. 2024a. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, arXiv:2409.12191arXiv preprint</p>
<p>Xinglin Wang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Boyuan Pan, Heda Wang, Yao Hu, Kan Li, arXiv:2408.09150Coglm: Tracking cognitive development of large language models. 2024barXiv preprint</p>
<p>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, 2014Transactions of the association for computational linguistics2</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024Yuxuan Sun, and 1 others</p>
<p>Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, Guangtao Zhai, arXiv:2501.13953Redundancy principles for mllms benchmarks. 2025arXiv preprint</p>
<p>Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Weijie Hao Tian, Su, arXiv:2504.10479Jie Shao, and 1 others. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint</p>
<p>A Examples of Other Spatial Reasoning Benchmarks. </p>            </div>
        </div>

    </div>
</body>
</html>
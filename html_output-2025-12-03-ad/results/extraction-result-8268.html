<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8268 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8268</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8268</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-266690857</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.17445v2.pdf" target="_blank">State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> Current Large Language Model-based agents reason within an exploration-evaluation framework, navigating problem-solving processes in a tree-like manner. However, these methods often neglect successful reasoning trajectories once a problem is resolved, leading to inefficient use of these trajectories for future analogous problems. To address this inefficiency, we adopt a state machine to record experience derived from previous reasoning trajectories. Within the state machine, states represent decomposed sub-problems, while state transitions reflect the dependencies among sub-problems. The state machine records both successful and failed trajectories. Utilizing the experience from the state machine, our proposed State Machine of Thoughts (SMoT) selects the most optimal sub-solutions and avoids incorrect ones. Our experiments show that SMoT can significantly improve problem-solving abilities in two exploration-intensive problems: the 24-point game and a taxi navigation reinforcement learning game.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8268.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8268.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State Machine of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constructs a state machine from past reasoning trajectories (conducive and non-conducive) and uses it to retrieve promising sub-solutions and prune unproductive sub-problems, falling back to LLM generation when no prior exists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based GPT-3.5-turbo LLM used in experiments with sampling temperature 0.7; authors note it has relatively weak numerical reasoning so some trajectory parsing is done by Python routines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['state-machine retrieval (experience replay)', 'tree-of-thoughts style exploration/backtracking (when no prior)', 'LLM-based evaluation (solvability scoring)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SMoT stores sub-problems as states and sub-solutions as transitions labeled conducive (+) or non-conducive (−) via top-down and bottom-up traversals of prior ToT trees. At proposal time it first queries the state machine for recorded conducive sub-solutions and uses them directly; otherwise it uses a ToT-style LLM proposer. For evaluation it uses the state machine's solvability flags when present (absolutely solvable/unsolvable) and otherwise uses an LLM evaluator to score possible/ impossible.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (uses prior recorded single best sub-solutions when available — similar/retrieval style — and falls back to diverse ToT exploration when not recorded)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared SMoT (state-machine + fallback ToT) against CoT and ToT baselines on two tasks; ablations include varying the fraction of stored states (100%→1%) and adding noise to state labels (0%→80%) to measure effect of fewer/more corrupted prior experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Taxi Navigation (5x5 grid pickup/dropoff RL toy) and 24-Point Card Game (numerical reasoning combining four numbers to reach 24; hard problems indexed 901-1000 used for evaluation), both chosen because they require exploration of many intermediate subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Taxi (average across 5 scenarios): CoT success 0% (#LLM infers 30.0), ToT 30% (#LLM infers 37.2), SMoT 99% (#LLM infers 14.4). 24-Point: CoT 0% (#LLM infers 3.0), ToT 20% (#LLM infers 88.8), SMoT 56% (#LLM infers 36.2). Ablation (24-point): with 100% states SMoT success 56% (#LLM 36.2); progressively fewer states decreased efficiency and eventually accuracy (e.g., 1% states → 30% success, #LLM 86.3). Robustness to noisy state machine: 0% noise 56% success (#LLM 36.2); 20% noise 43% (39.0); 40% noise 38% (55.7); 60% noise 41% (47.0); 80% noise 30% (51.7).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>SMoT reuses successful sub-trajectories to avoid unnecessary exploration and to discard known unproductive branches, substantially reducing LLM calls and increasing success; when many states are missing or heavily noisy performance degrades but can still outperform ToT in some degraded regimes. SMoT exploits repetition of subproblems across tasks to improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Leveraging a state machine of past reasoning trajectories enables LLM agents to select proven sub-solutions and avoid incorrect ones, improving both accuracy and inference efficiency relative to single-path CoT and exploration-heavy ToT on exploration-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8268.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8268.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that structures reasoning as a tree of intermediate 'thoughts' allowing exploration, backtracking, and evaluation of multiple candidate reasoning trajectories rather than a single chain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based GPT-3.5-turbo LLM used as the proposer/evaluator in ToT implementations in this paper; breadth limit B used (e.g., B=20 in 24-point experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tree-structured exploration/backtracking', 'multi-trajectory evaluation', 'breadth-first search (BFS) or depth-first search (DFS) over thought tree']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ToT proposes multiple sub-solutions for a sub-problem, expands candidate next states into a tree, evaluates solvability of intermediate states, and backtracks/continues based on evaluation; authors implemented ToT with BFS and a breadth limit (B).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (explicitly samples and evaluates multiple different reasoning trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a baseline contrasted with CoT (single chain) and SMoT (state-machine augmented). Experiments show ToT explores many intermediate options leading to higher success than CoT but with higher LLM inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same as SMoT: Taxi Navigation and 24-Point Card Game; ToT used to generate trajectories for constructing SMoT state machines as well.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Taxi average success 30% (#LLM infers 37.2). 24-Point success 20% (#LLM infers 88.8). ToT used to explore trajectories that were then recorded (top-down) or labeled non-conducive (bottom-up) when building the state machine.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ToT's exploration is crucial for constraint-satisfaction tasks (e.g., 24-point, taxi), improving over single-chain CoT, but exploration/backtracking increases computational cost and uncertainty; also non-conducive branches identified by ToT can be misclassified if sampling is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>ToT's diverse exploration improves success on exploration-heavy tasks compared to single-chain CoT, but is less efficient than SMoT which reuses prior successful trajectories to prune the search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8268.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8268.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique exposing intermediate reasoning steps (a single reasoning chain) in prompts to elicit step-by-step reasoning from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo used to produce single-chain CoT outputs in experiments; sampling temperature 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['single-chain chain-of-thought prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT provides few-shot examples of intermediate steps so the LLM produces a single chain of intermediate reasoning steps; in experiments CoT often samples only a single path resulting in inconsistency and poor performance for the tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single reasoning chain per query)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a baseline compared to ToT (diverse) and SMoT (state-machine augmented). In taxi experiments CoT and CoT-SC were found suboptimal in preliminary tests and CoT exhibited 0% success on average.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Taxi Navigation and 24-Point Card Game (same experimental setups as SMoT/ToT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Taxi average success 0% (#LLM infers 30.0). 24-Point success 0% (#LLM infers 3.0).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT's single-path sampling is insufficient for problems requiring diverse exploration of intermediate steps, leading to low success and hitting preset inference limits.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Single-chain CoT often fails on exploration-intensive tasks; sampling multiple chains (ToT, CoT-SC) or incorporating prior experience (SMoT) is necessary to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8268.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8268.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that samples multiple chain-of-thought reasoning trajectories and selects the most consistent final answer, improving over a single greedy CoT decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-consistency (multiple sampled chains + majority/consensus selection)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT-SC generates multiple diverse chain-of-thought samples for a question and aggregates final answers (e.g., by majority vote) to improve robustness; the paper mentions CoT-SC as prior work but did not incorporate it in main experiments as it performed poorly in preliminary taxi tests.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (samples multiple chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned in related work and noted to have been excluded from further analysis in taxi experiments due to suboptimal preliminary performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned generically in related work; not experimentally evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Cited as a method that alleviates inconsistency in single-path CoT by sampling multiple trajectories; authors found it performed poorly in their preliminary taxi trials and thus excluded it from main comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Sampling multiple chains (self-consistency) can improve CoT but was not effective in the authors' preliminary taxi setup, and thus not used in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8268.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8268.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based reasoning framework that models reasoning as a graph where nodes (thoughts) can be aggregated, refined, or used to generate new thoughts, implementing divide-and-conquer strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph of thoughts: Solving elaborate problems with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['graph-structured reasoning (aggregation, refining, generation)', 'divide-and-conquer decomposition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GoT models the reasoning process as a graph allowing consolidation (aggregation), refinement (retrospection), and generation operations to break complex problems into independent subproblems; authors mention it as related work suited to decomposable tasks but did not use it experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (graph operations allow multiple, non-linear reasoning paths and combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned in related work; not used in experiments. The paper contrasts GoT's divide-and-conquer suitability for decomposable tasks with SMoT's reuse of prior trajectories for repeated subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned conceptually (suitable for tasks like sorting, set intersection, keyword counting); no experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GoT is appropriate for problems that can be decomposed into independent subproblems, but differs from SMoT which emphasizes reuse of prior experience across similar subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Graph-based divide-and-conquer is complementary conceptually, but SMoT specifically targets reuse of repeated subproblems through a learned state machine.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8268.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8268.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (thought-action-observe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework combining reasoning (thought) and acting in an environment, where the agent alternates between internal reasoning and external actions/observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['interleaved reasoning and acting (thought-action-observe loop)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ReAct employs a three-step cycle (thought, action, observe) to ground reasoning in external interactions; the paper mentions ReAct and notes that in their taxi experiments ReAct's thinking sometimes fails to consider past routes leading to loops.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (interleaves internal reasoning and environment interaction; diversity depends on proposer/evaluator implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned as a related method; authors evaluated it informally and observed looping behavior in taxi scenario but did not include it as a formal baseline in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned in context of environment-grounded tasks (e.g., navigation); not part of main experimental comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors observed that ReAct's thinking phase sometimes neglects prior route information and can cause looping behavior in taxi navigation, illustrating a shortcoming compared to SMoT's reuse of past trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Interleaving action and reasoning can be valuable, but without explicit memory of prior trajectories it may perform poorly on tasks with repeated subproblem structure (e.g., taxi navigation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models. <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8268",
    "paper_id": "paper-266690857",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "SMoT",
            "name_full": "State Machine of Thoughts",
            "brief_description": "A method that constructs a state machine from past reasoning trajectories (conducive and non-conducive) and uses it to retrieve promising sub-solutions and prune unproductive sub-problems, falling back to LLM generation when no prior exists.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Transformer-based GPT-3.5-turbo LLM used in experiments with sampling temperature 0.7; authors note it has relatively weak numerical reasoning so some trajectory parsing is done by Python routines.",
            "reasoning_methods": [
                "state-machine retrieval (experience replay)",
                "tree-of-thoughts style exploration/backtracking (when no prior)",
                "LLM-based evaluation (solvability scoring)"
            ],
            "reasoning_methods_description": "SMoT stores sub-problems as states and sub-solutions as transitions labeled conducive (+) or non-conducive (−) via top-down and bottom-up traversals of prior ToT trees. At proposal time it first queries the state machine for recorded conducive sub-solutions and uses them directly; otherwise it uses a ToT-style LLM proposer. For evaluation it uses the state machine's solvability flags when present (absolutely solvable/unsolvable) and otherwise uses an LLM evaluator to score possible/ impossible.",
            "reasoning_diversity": "both (uses prior recorded single best sub-solutions when available — similar/retrieval style — and falls back to diverse ToT exploration when not recorded)",
            "reasoning_diversity_experimental_setup": "Compared SMoT (state-machine + fallback ToT) against CoT and ToT baselines on two tasks; ablations include varying the fraction of stored states (100%→1%) and adding noise to state labels (0%→80%) to measure effect of fewer/more corrupted prior experiences.",
            "task_or_benchmark": "Taxi Navigation (5x5 grid pickup/dropoff RL toy) and 24-Point Card Game (numerical reasoning combining four numbers to reach 24; hard problems indexed 901-1000 used for evaluation), both chosen because they require exploration of many intermediate subproblems.",
            "performance_results": "Taxi (average across 5 scenarios): CoT success 0% (#LLM infers 30.0), ToT 30% (#LLM infers 37.2), SMoT 99% (#LLM infers 14.4). 24-Point: CoT 0% (#LLM infers 3.0), ToT 20% (#LLM infers 88.8), SMoT 56% (#LLM infers 36.2). Ablation (24-point): with 100% states SMoT success 56% (#LLM 36.2); progressively fewer states decreased efficiency and eventually accuracy (e.g., 1% states → 30% success, #LLM 86.3). Robustness to noisy state machine: 0% noise 56% success (#LLM 36.2); 20% noise 43% (39.0); 40% noise 38% (55.7); 60% noise 41% (47.0); 80% noise 30% (51.7).",
            "qualitative_findings": "SMoT reuses successful sub-trajectories to avoid unnecessary exploration and to discard known unproductive branches, substantially reducing LLM calls and increasing success; when many states are missing or heavily noisy performance degrades but can still outperform ToT in some degraded regimes. SMoT exploits repetition of subproblems across tasks to improve efficiency.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Leveraging a state machine of past reasoning trajectories enables LLM agents to select proven sub-solutions and avoid incorrect ones, improving both accuracy and inference efficiency relative to single-path CoT and exploration-heavy ToT on exploration-intensive tasks.",
            "uuid": "e8268.0",
            "source_info": {
                "paper_title": "State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts",
            "brief_description": "A prompting framework that structures reasoning as a tree of intermediate 'thoughts' allowing exploration, backtracking, and evaluation of multiple candidate reasoning trajectories rather than a single chain.",
            "citation_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Transformer-based GPT-3.5-turbo LLM used as the proposer/evaluator in ToT implementations in this paper; breadth limit B used (e.g., B=20 in 24-point experiments).",
            "reasoning_methods": [
                "tree-structured exploration/backtracking",
                "multi-trajectory evaluation",
                "breadth-first search (BFS) or depth-first search (DFS) over thought tree"
            ],
            "reasoning_methods_description": "ToT proposes multiple sub-solutions for a sub-problem, expands candidate next states into a tree, evaluates solvability of intermediate states, and backtracks/continues based on evaluation; authors implemented ToT with BFS and a breadth limit (B).",
            "reasoning_diversity": "diverse (explicitly samples and evaluates multiple different reasoning trajectories)",
            "reasoning_diversity_experimental_setup": "Used as a baseline contrasted with CoT (single chain) and SMoT (state-machine augmented). Experiments show ToT explores many intermediate options leading to higher success than CoT but with higher LLM inference cost.",
            "task_or_benchmark": "Same as SMoT: Taxi Navigation and 24-Point Card Game; ToT used to generate trajectories for constructing SMoT state machines as well.",
            "performance_results": "Taxi average success 30% (#LLM infers 37.2). 24-Point success 20% (#LLM infers 88.8). ToT used to explore trajectories that were then recorded (top-down) or labeled non-conducive (bottom-up) when building the state machine.",
            "qualitative_findings": "ToT's exploration is crucial for constraint-satisfaction tasks (e.g., 24-point, taxi), improving over single-chain CoT, but exploration/backtracking increases computational cost and uncertainty; also non-conducive branches identified by ToT can be misclassified if sampling is insufficient.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "ToT's diverse exploration improves success on exploration-heavy tasks compared to single-chain CoT, but is less efficient than SMoT which reuses prior successful trajectories to prune the search.",
            "uuid": "e8268.1",
            "source_info": {
                "paper_title": "State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought",
            "brief_description": "A prompting technique exposing intermediate reasoning steps (a single reasoning chain) in prompts to elicit step-by-step reasoning from LLMs.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "GPT-3.5-turbo used to produce single-chain CoT outputs in experiments; sampling temperature 0.7.",
            "reasoning_methods": [
                "single-chain chain-of-thought prompting"
            ],
            "reasoning_methods_description": "CoT provides few-shot examples of intermediate steps so the LLM produces a single chain of intermediate reasoning steps; in experiments CoT often samples only a single path resulting in inconsistency and poor performance for the tested tasks.",
            "reasoning_diversity": "similar (single reasoning chain per query)",
            "reasoning_diversity_experimental_setup": "Used as a baseline compared to ToT (diverse) and SMoT (state-machine augmented). In taxi experiments CoT and CoT-SC were found suboptimal in preliminary tests and CoT exhibited 0% success on average.",
            "task_or_benchmark": "Taxi Navigation and 24-Point Card Game (same experimental setups as SMoT/ToT).",
            "performance_results": "Taxi average success 0% (#LLM infers 30.0). 24-Point success 0% (#LLM infers 3.0).",
            "qualitative_findings": "CoT's single-path sampling is insufficient for problems requiring diverse exploration of intermediate steps, leading to low success and hitting preset inference limits.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Single-chain CoT often fails on exploration-intensive tasks; sampling multiple chains (ToT, CoT-SC) or incorporating prior experience (SMoT) is necessary to improve performance.",
            "uuid": "e8268.2",
            "source_info": {
                "paper_title": "State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency",
            "brief_description": "An approach that samples multiple chain-of-thought reasoning trajectories and selects the most consistent final answer, improving over a single greedy CoT decoding.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reasoning_methods": [
                "self-consistency (multiple sampled chains + majority/consensus selection)"
            ],
            "reasoning_methods_description": "CoT-SC generates multiple diverse chain-of-thought samples for a question and aggregates final answers (e.g., by majority vote) to improve robustness; the paper mentions CoT-SC as prior work but did not incorporate it in main experiments as it performed poorly in preliminary taxi tests.",
            "reasoning_diversity": "diverse (samples multiple chains)",
            "reasoning_diversity_experimental_setup": "Mentioned in related work and noted to have been excluded from further analysis in taxi experiments due to suboptimal preliminary performance.",
            "task_or_benchmark": "Mentioned generically in related work; not experimentally evaluated in this paper.",
            "performance_results": null,
            "qualitative_findings": "Cited as a method that alleviates inconsistency in single-path CoT by sampling multiple trajectories; authors found it performed poorly in their preliminary taxi trials and thus excluded it from main comparisons.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Sampling multiple chains (self-consistency) can improve CoT but was not effective in the authors' preliminary taxi setup, and thus not used in reported experiments.",
            "uuid": "e8268.3",
            "source_info": {
                "paper_title": "State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GoT",
            "name_full": "Graph of Thoughts",
            "brief_description": "A graph-based reasoning framework that models reasoning as a graph where nodes (thoughts) can be aggregated, refined, or used to generate new thoughts, implementing divide-and-conquer strategies.",
            "citation_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reasoning_methods": [
                "graph-structured reasoning (aggregation, refining, generation)",
                "divide-and-conquer decomposition"
            ],
            "reasoning_methods_description": "GoT models the reasoning process as a graph allowing consolidation (aggregation), refinement (retrospection), and generation operations to break complex problems into independent subproblems; authors mention it as related work suited to decomposable tasks but did not use it experimentally.",
            "reasoning_diversity": "diverse (graph operations allow multiple, non-linear reasoning paths and combinations)",
            "reasoning_diversity_experimental_setup": "Mentioned in related work; not used in experiments. The paper contrasts GoT's divide-and-conquer suitability for decomposable tasks with SMoT's reuse of prior trajectories for repeated subproblems.",
            "task_or_benchmark": "Mentioned conceptually (suitable for tasks like sorting, set intersection, keyword counting); no experiments in this paper.",
            "performance_results": null,
            "qualitative_findings": "GoT is appropriate for problems that can be decomposed into independent subproblems, but differs from SMoT which emphasizes reuse of prior experience across similar subproblems.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Graph-based divide-and-conquer is complementary conceptually, but SMoT specifically targets reuse of repeated subproblems through a learned state machine.",
            "uuid": "e8268.4",
            "source_info": {
                "paper_title": "State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (thought-action-observe)",
            "brief_description": "A framework combining reasoning (thought) and acting in an environment, where the agent alternates between internal reasoning and external actions/observations.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reasoning_methods": [
                "interleaved reasoning and acting (thought-action-observe loop)"
            ],
            "reasoning_methods_description": "ReAct employs a three-step cycle (thought, action, observe) to ground reasoning in external interactions; the paper mentions ReAct and notes that in their taxi experiments ReAct's thinking sometimes fails to consider past routes leading to loops.",
            "reasoning_diversity": "both (interleaves internal reasoning and environment interaction; diversity depends on proposer/evaluator implementations)",
            "reasoning_diversity_experimental_setup": "Mentioned as a related method; authors evaluated it informally and observed looping behavior in taxi scenario but did not include it as a formal baseline in tables.",
            "task_or_benchmark": "Mentioned in context of environment-grounded tasks (e.g., navigation); not part of main experimental comparisons.",
            "performance_results": null,
            "qualitative_findings": "Authors observed that ReAct's thinking phase sometimes neglects prior route information and can cause looping behavior in taxi navigation, illustrating a shortcoming compared to SMoT's reuse of past trajectories.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Interleaving action and reasoning can be valuable, but without explicit memory of prior trajectories it may perform poorly on tasks with repeated subproblem structure (e.g., taxi navigation).",
            "uuid": "e8268.5",
            "source_info": {
                "paper_title": "State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        }
    ],
    "cost": 0.01379075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving
9 Mar 2024</p>
<p>Jia Liu liujia08@kuaishou.com 
Hefei University of Technology</p>
<p>Jie Shuai 
Hefei University of Technology</p>
<p>Xiyao Li lixiyao@kuaishou.com 
Hefei University of Technology</p>
<p>Kuaishou Technology 
Hefei University of Technology</p>
<p>State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving
9 Mar 202478EA3D050269AD7079F018DD76CE55F9arXiv:2312.17445v2[cs.AI]
Current Large Language Model-based agents reason within an exploration-evaluation framework, navigating problem-solving processes in a tree-like manner.However, these methods often neglect successful reasoning trajectories once a problem is resolved, leading to inefficient use of these trajectories for future analogous problems.To address this inefficiency, we adopt a state machine to record experience derived from previous reasoning trajectories.Within the state machine, states represent decomposed sub-problems, while state transitions reflect the dependencies among sub-problems.The state machine records both successful and failed trajectories.Utilizing the experience from the state machine, our proposed State Machine of Thoughts (SMoT) selects the most optimal sub-solutions and avoids incorrect ones.Our experiments show that SMoT can significantly improve problem-solving abilities in two exploration-intensive problems: the 24-point game and a taxi navigation reinforcement learning game.</p>
<p>Introduction</p>
<p>In recent years, there has been notable progress in large language models (LLMs) that are built upon large Transformer [Vaswani et al., 2017] architecture and massive unsupervised training data [Touvron et al., 2023;Brown et al., 2020].Moreover, when appropriately fine-tuned and aligned [Ouyang et al., 2022], these LLMs have showcased remarkable zero-shot or few-shot performance across various downstream tasks involving semantic understanding, language generation, and reasoning [Bubeck et al., 2023].</p>
<p>Whilst LLMs have exhibited impressive performance [Touvron et al., 2023;Brown et al., 2020;Bubeck et al., 2023], they are sometimes directly produce wrong answers when faced with complex problems.A potential resolution comes from chain-of-thoughts (CoT) prompting approach [Xie et al., 2023;Shinn et al., 2023;Wu et al., 2023].These solutions harnesses well-crafted prompts to elicit the LLM's reasoning ability to think intermediate steps, leading to the final accurate answers.Specifically, Wei et al. first propose CoT method which provides LLMs with intermediate reasoning step examples as prompts.Through emulation of the CoT prompting, LLMs were able to decompose a complex problem into intermediate easier sub-problems.Through solving the easier sub-problems one by one, CoT can more accurately deriving the final answers.Furthermore, Yao et al. adopt treeof-thoughts (ToT) prompting method to implement the "backtracing" strategy.ToT explore various sub-solutions for the current sub-problems, followed by the evaluation of each reasoning trajectory to determine whether to continue or backtrack.Besta et al. employ a graph-based model to implement the "divide and conquer" approach in the reasoning process.In the thought graph, a single node, representing a problem, can be decomposed into several similar sub-problems, each of which can be more effectively handled.</p>
<p>The ToT prompting method has demonstrated the significance of investigating diverse sub-solutions to effectively tackle problems with constraints, such as the 24-point game.However, despite ToT's effectiveness in enhancing gamesolving ability, it often overlooks the utility of experience gleaned from past problem-solving trajectories when addressing similar problems.As depicted in Figure 1, the experience drawn from resolving the array [2,4,6,12] in 24 point game -where subsets like [2,3,6] and [3,8] successfully yield the sum of 24, while a subset such as [2,4,6] does not -can be helpful.For instance, when agents facing a new sequence [2,2,6,6], they can deduce that the operation "6-2=4" is unlikely to lead to a solution, given that the left numbers [2,4,6] do not sum to 24 from previous experience.In contrast, the operation "6/2=3" should be selected because it leaves behind the set [2,3,6], which can indeed reach the total of 24.In this paper, we aim to extract and apply insights from previous reasoning trajectories to enhance the problem-solving process.However, this goal is met with two challenges:</p>
<p>• How can we extract experience from past trajectories and record experience for best combination with LLM?</p>
<p>• In what ways can the extracted experience be applied to improve the reasoning ability for problem-solving?</p>
<p>To this end, we propose State Machine of Thought (SMoT) method to address above two challenges with two steps: (1) To extract experience state machine from past reasoning trajectories; (2) To explore and evaluate with state machine for
•• ••• ••• ••• 6, 6 ••• 2+4=6 Input: 2, 2, 6, 6 2, 3, 6 6/2=3 8, 3 2+6=8 3*8=24 24 6-2=4 2, 2, 1 6/6=1 2, 4, 6 ••• ••• ••• ••• ••• ••• ••• ••• Past reasoning trajectories Trajectories for a new problem ❌ ✅ ✅ 2, 3, 6 8, 3 24 ••• 2, 4, 6 2, 3, 4 ••• ••• ••• Experience ❌ ✅ ✅ ✅ ✅ ❌ ❌ ❌ ✅ Figure 1:
The motivation for improving solving 24-Point Card Games: The goal is to find a way to combine four given numbers to reach the total of 24.By documenting which sets of numbers can successfully achieve or fail to reach this total based on past trajectories, we can store the experience that helps agents quickly decide the best next move when encountering the same numbers in future games.</p>
<p>better problem reasoning.Specifically, we employ a state machine to record experience from prior reasoning paths to handle the first challenge.The state machine [Yannakakis, 2000;Girault et al., 1999], a well-recognized computational framework, modeling the transitions among various states within the problem-solving process.In our context, we define subproblems as states, and view reasoning or sub-solutions as the transitions connecting these sub-problems.Specifically, when faced with a sub-problem and its possible transitions to adjacent states, the state machine helps us identify which transition or neighboring state is conducive to resolving the issue and which is not.To extract the experience gained from historical reasoning trajectories, we implement a bidirectional traversal strategy, using both top-to-bottom and bottom-to-top approaches, to distill effective and ineffective trajectories from previous reasoning trees.</p>
<p>The state machine embedded in SMoT model yields two key advantages: it enables direct retrieval of beneficial subsolutions and the dismissal of ineffective sub-problems, thus pruning the exploration reasoning tree.These two advantages help us to handle the second challenge.In practice, SMoT consults the state machine to source promising sub-solutions, bypassing LLM inference when possible.If no such subsolutions are present, SMoT defers to LLMs for generation.Additionally, SMoT uses the state machine to swiftly eliminate known unproductive sub-problems during evaluation.Finally, we compared our proposed SMoT model with stateof-the-art baselines on the 24-point card game and the classic reinforcement learning game of taxi navigation.These two games are conditional constraint problems that require agents to explore as many potential sub-solutions as possible.The experimental results clearly demonstrate SMoT's improvement in problem-solving accuracy and efficiency.</p>
<p>2 Related Work</p>
<p>Task Decomposition and Planning</p>
<p>Wei et al. initially discovered that prompting LLMs to decompose a complex problem into subtasks and gradually complete them via chains-of-thought (CoT) could effectively improve LLMs' reasoning performance.Following this line, numerous thought prompting strategies have been proposed to enhance LLMs' reasoning abilities [Wang et al., 2022;Yao et al., 2023a;Besta et al., 2023].Wang et al. found that CoT only samples a single reasoning path when answer a question, so the reasoning results suffer from inconsistency issues.Therefore, they proposed the self-consistency strategy, wherein multiple reasoning paths are sampled for a question to boost the reasoning performance of LLMs.</p>
<p>Although LLMs have exhibited fine planning ability in general domains, some researchers have noted that existing these models still encountering serious difficulties in designing effective plans for domain-specific tasks [Zhang et al., 2023;Ge et al., 2023;Liu et al., 2023].Therefore, Liu et al. incorporate LLMs to transform the given visionand-language navigation task into a description in planning domain definition language (PPDL) and then leverages indomain planners for finding an optimal plan.Similarly, Dagan et al. utilize LLMs to consider high-level plans and incorporate in-domain models to design low-level action sequences.Moreover, a classical path planner are introduced into the robot agent Sayplan [Rana et al., 2023] to obtain shorter horizon plan sequences.</p>
<p>Reflection and Refinement</p>
<p>While existing LLMs possess strong capabilities for task planning, correctly outlining the reasoning paths from the beginning is still challenging.Consequently, researchers have suggested introducing reflection mechanisms whereby the inference process reflects upon and re-plan following paths [Madaan et al., 2023].For example, Tree-of-Thoughts proposed by Yao et al. structures reasoning paths as a tree.This framework enables LLM agents to self-evaluation and consider diverse reasoning paths during reasoning.Besta et al. modelled the reasoning process in graph formulation, introducing three inference operations: aggregation, refining, and generation.Aggregation consolidates several thoughts into an improved one.Refining retrospects and refines the current thought.Generation contemplates subsequent reasoning steps based on the current thought.</p>
<p>However, only rely on self-evaluation is hard to correctly evaluate current reason path.Therefore, some works utilize external feedback to evaluation [Wang et al., 2023;Rana et al., 2023;Hao et al., 2023].For instance, ReAct [Yao et al., 2023b] implemented a three-step reasoning strategy of thought action observe.Here, agents observe the ef-
Input Top-to-bottom traversal Bottom-to-top traversal ❌ ✅ ❌ ✅
The incorrect output</p>
<p>❌</p>
<p>The correct output</p>
<p>State Machine of Thoughts</p>
<p>In this section, we present a detailed description of our proposed SMoT, which comprises two main components: (1) learning a knowledge state machine from previous reasoning trajectories, and (2) utilizing the prior state machine to reason about new problems.</p>
<p>The Construction of the Knowledge State Machine</p>
<p>Formally, the knowledge state machine is defined as a fourtuple: {S, A, s 0 , µ}, where:</p>
<p>• S is a finite set of symbols denoting sub-problems (i.e., states within the state machine), • A represents the set of sub-solutions (i.e., actions within the state machine), • s 0 ∈ S is the initial problem (i.e., the initial state of the state machine), • µ is the transition function mapping transitions µ :
s k a k+1 − −− → s k+1
, where a sub-problem s k ∈ S transitions to another sub-problem s k+1 ∈ S through the subsolution a k+1 ∈ A.</p>
<p>The state machine records both conducive and nonconducive transitions, i.e., subsequent sub-solutions and subproblems, given a particular sub-problem.When an agent encounters the same sub-problem again, the state machine can suggest effective sub-solutions and indicate ineffective ones, thus aiding the agent in solving problems both efficiently and accurately.</p>
<p>Learning the State Machine from Past Reasoning Trajectories</p>
<p>The Tree-of-Thoughts (ToT) approach adopts a tree structure to explore various sub-solutions while reasoning through complex problems.In the reasoning tree, a complete trajectory from the input node to the output node can be denoted by a transition path [s 0 , (a 1 , s 1 ), . . ., (a K , s K )], where state s k transitions to sub-problem s k+1 through the application of sub-solution a k+1 .As depicted in Figure 2, complete trajectories can be categorized as either successfully solving the problem or failing to do so.Consequently, we propose two methods to utilize these types of trajectories for constructing the knowledge state machine: a top-to-bottom traversal to construct conducive state transitions and a bottom-to-top traversal to construct non-conducive transitions.</p>
<p>Top-to-bottom traversal.The top-to-bottom traversal (depicted as the dashed green line in Figure 2) represents the path from the input node to the correct output node.Since these trajectories lead to the correct answers, all transitions within these paths are conducive to problem-solving.Therefore, we record all transitions along these paths in the state machine and denote them with a plus sign + to signify that the states and transitions are conducive, with a typical transi-
tion denoted by s k + a k+1 + − −− → s k+1
+ .Bottom-to-top traversal.Conversely, the bottom-to-top traversal (illustrated by the red dashed lines in Figure 2) begins from a failed output at the bottom and progresses upwards to the top input node to identify non-conducive transitions.An intermediate node in the tree is deemed nonconducive if all its child nodes are also non-conducive.We implement a "breadth-first" approach starting from the bottom failed output nodes and moving up to the top root nodes.Upon identifying the entire non-conducive trajectories, we document all transitions within these paths in the state machine, marking the states and transitions with a minus sign −
as s k − a k+1 − − −− → s k+1
− .It is important to note that non-conducive transitions identified from ToT's reasoning trees may not be definitively incorrect, as the sub-solutions proposed for a subproblem might not encompass all possible solutions.Consequently, conducive trajectories could be misclassified.To mitigate this issue, one potential solution is to sample as many sub-solutions as possible.</p>
<p>The state query function.To equip LLM-based agents with insights from the state machine, we introduce two state query functions:</p>
<p>• Conducive Sub-solution Query: Function f s (s k ) takes a sub-problem s k as input and produces a sequence of tuples (a k+1 1 , s k+1 1 ), . . ., (a k+1 N , s k+1 N ) , where each tuple contains a next sub-solution a k+1 i and the subsequent sub-problem s k+1 i , for i = 1, . . ., N .This function aims to offer sub-solutions that have been empirically proven to be conducive to solving the problem in past reasoning trajectories.</p>
<p>• Sub-problem Solvability Query: Function f p (s k ) accepts a sub-problem s k and returns 1 if the sub-problem is conducive to solving the overall problem, and 0 otherwise.This binary indicator quickly informs the agent about the potential worth of pursuing the sub-problem within the context of the broader problem-solving effort.</p>
<p>These query functions act as interfaces between the agent's current context and the accumulated knowledge within the state machine, enabling the agent to make informed decisions based on historical data.</p>
<p>Reasoning with the Prior State Machine</p>
<p>Existing prompting methods, such as ToT, adopt a paradigm of proposing sub-solutions and evaluating sub-problems to identify feasible reasoning trajectories.By constructing a state machine from past trajectories, we can enhance agents' capabilities in two ways: (1) conducive sub-solutions and sub-problems can directly offer optimal sub-solutions for the agent; (2) non-conducive sub-solutions can be accurately evaluated as ineffective.</p>
<p>State machine enhanced sub-solution proposer.The sub-solution proposer P(s k ) aims to propose subsolutions and the corresponding transited sub-problems (a k+1 n=1 , s k+1 n=1 ), . . ., (a k+1 n=N , s k+1 n=N ) , given the current sub-problem s k .In this step, we utilize the conducive subsolution query function f s (s k ) to query the state machine for optimal sub-solutions.If sub-solutions exist in the state machine, SMoT directly adopts these solutions instead of generating new ones using LLMs.Conversely, if no sub-solution is present in the state machine, SMoT reverts to the ToT approach, which involves proposing sub-solutions using LLMs P l (s k , p θ ), where p θ denotes the LLM with parameter θ.</p>
<p>State machine enhanced next sub-problem evaluator.</p>
<p>Upon obtaining potential sub-solutions and their corresponding transited new sub-problems, the sub-problem evaluator V(s k ) → v is adopt to evaluate sub-problems, where v denotes the sub-problem's solvability.In SMoT, the evaluator provides four levels of solvability scores:</p>
<p>• Absolutely solvable: This score indicates that the subsolution proposed from the state machine is assuredly solvable and requires no further evaluation.</p>
<p>• Possible: This score, evaluated by LLMs, indicates that the sub-problem, not recorded in the state machine, has possible solvability.</p>
<p>• Impossible: This score, evaluated by LLMs, indicates that the sub-problem, not recorded in the state machine, is unsolvable.</p>
<p>• Absolutely unsolvable: This score indicates that the sub-solution is recorded as unproductive in the state machine.</p>
<p>SMoT's evaluator is compatible with the existing ToT method.Specifically, if the sub-problem's solvability has been recorded in the state machine (checked by the subproblem solvability query function f p (s k )), we rely on the solvability recorded in the state machine.If not, the evaluator reverts to the ToT evaluator V l (s k , p θ ) → v, where p θ is the LLM with parameter θ.</p>
<p>The Search Algorithm of SMoT</p>
<p>SMoT is compatible with the existing ToT method, allowing both breadth-first search (BFS) and depth-first search (DFS)
for s k n ∈ Q do 5: if f s (s k n ) is not empty then 6: Q ′k ∪ f s (s k n ) 7:
else 8:
Q ′k ∪ P l (s k n , p θ , B) 9: /<em> sub-problem evaluation V(s k n ) − → v </em>/ 10: for s k n ∈ Q ′k do 11: if s k n in f p then 12: v k n ← − f p (s k n ) 13:
else 14:
v k n ← − V l (s k n , p θ ) 15:
/<em> choose most conducive sub-problems </em>/ 16:
Q k ← − argmax s k n ∈Q ′k V(s k n ) 17:
return solution algorithms to be applicable for SMoT.Due to space constraints, we only introduce the BFS implementation of SMoT in Algorithm 1 .</p>
<p>Comparison between SMoT and Existing Prompting Approaches</p>
<p>In this section, we elaborate on the conceptual framework of SMoT and compare its features with those of existing prompting methods in Figure 3.The Chain-of-Thought (CoT) approach [Wei et al., 2022] provides Large Language Models (LLMs) with a series of few-shot examples that delineate a step-by-step reasoning process.The LLMs are then expected to emulate a similar reasoning sequence when addressing a query.Self-Consistency (CoT-SC) [Wang et al., 2022] seeks to enhance the basic greedy decoding strategy utilized in CoT prompting by sampling multiple diverse reasoning trajectories through few-shot CoT and electing the most consistent response from the generated answers.</p>
<p>Tree of Thoughts (ToT) [Yao et al., 2023a] employs a method of exploration, evaluation, and backtracking as it searches for the appropriate reasoning pathway.Should the current trajectory prove insufficient in conclusively determining the answer, it will backtrack to a previous step or even earlier in the process to explore alternative viable routes toward an optimal solution.This technique is particularly effective for problems that require constraint satisfaction, such as the 24-game or crossword puzzles.Nevertheless, for issues where the reasoning pathway is pre-established, the extra steps of exploration and backtracking not only contribute to unnecessary computational effort but also amplify the uncertainty within the reasoning process.</p>
<p>Graph of Thoughts (GoT) [Besta et al., 2023] adopts a divide-and-conquer strategy, breaking a multifaceted prob-  lem into a series of simpler sub-problems until these subproblems can be effortlessly resolved.Consequently, the GoT methodology is especially apt for problems that can be divided, including tasks like sorting, set intersection, and keyword counting.The decomposition of the central problem simplifies the resolution process by allowing for the recursive solution of its independent subcomponents.</p>
<p>In contrast to ToT or GoT, which require supplementary reasoning steps for exploration and backtracking, SMoT capitalizes on a state machine as prior knowledge for problemsolving.The state machine constructs a bridge that applies experience from past reasoning trajectories to assist LLMs in reasoning subsequent similar problems.This structured guidance ensures that LLM reasoning remains within predefined exploration boundaries, thus enabling the avoidance of unproductive sub-problems at an early stage and the selection of sub-solutions that have been deemed conducive to resolving the task.</p>
<p>Experiments</p>
<p>In this section, we compare SMoT with other existing prompting methods on two tasks that pose significant challenges to current Large Language Models (LLMs): a toy reinforcement learning game named "Taxi Navigation" and a numerical reasoning game called "24-Point Card Gam".Both games require diverse exploratory strategies during the intermediate reasoning steps, and the same intermediate subproblems often recur across different tasks.Unless specified otherwise, our experiments were conducted using the GPT-3.5-turboLLM with a sampling temperature set to 0.7.</p>
<p>Taxi</p>
<p>The Taxi environment is a classical reinforcement learning problem 1 that involves navigating a taxi to passengers in a 5 × 5 grid world, picking them up, and dropping them off at one of four designated colored locations.As depicted in Figure 4(a), the passenger is initially at one of the colored locations, and their destination is another colored location.The environment provides observations that include the taxi's coordinates, the passenger's location, and their destination.The action space encompasses four movement directions (north, south, east, and west) and actions to pick up or drop off passengers.The episode concludes once the passenger is dropped off at their destination.The success of the mission depends upon delivering the passenger to their destination.</p>
<p>Task Setup</p>
<p>To ensure a fair comparison, we establish five challenging scenarios, illustrated in Figure 4, where the taxi's starting position, the passenger's origin, and the desired destination are deliberately predetermined.For each scenario, we evaluate a method 20 times.We utilize the task success ratio and the number of LLM inferences to assess accuracy and efficiency.</p>
<p>Baselines</p>
<p>For the taxi navigation task, we selected CoT (Chain-of-Thought) and ToT (Tree-of-Thought) as comparative meth- ods, where ToT is implemented using Breadth-first Search (BFS).Standard CoT, as well as CoT-SC, exhibited suboptimal performance in our preliminary trials and were hence excluded from further analysis.At each step, the LLM is prompted with observations from the environment.Another method of note is RaAct [Yao et al., 2023b], which employs a three-step reasoning process involving thought, action, and observation.However, empirical tests revealed that RaAct's thinking phase often fails to effectively consider previous routes, occasionally leading to the taxi driving in loops.</p>
<p>SMoT Implementation Details</p>
<p>The construction of knowledge state machine.Initially, we utilize ToT to explore the effective trajectories, which describe the paths from all coordinates to all colored locations, such as, [(2,2), (west, (1,2)), (north, (1,3)), ... , (west, red location)].After gathering these valid trajectories, we utilize the top-to-bottom traversal method to construct the state machine which only contains the conducive move for each coordinate to every colored locations.The number of states recorded in state machine is 96.</p>
<p>Construction of the knowledge state machine Initially, we employ ToT to explore effective trajectories that describe paths from all coordinates to all colored locations, such as [(2,2), (west, (1,2)), (north, (1,3)), ..., (west, red location)].</p>
<p>After compiling these valid trajectories, we use a top-tobottom traversal method to construct a state machine that includes only the beneficial moves for each coordinate to every colored location.The state machine records a total of 96 states.</p>
<p>Reason with the prior state machine.During each step of exploration, the LLM is prompted with the current observations from the environment, which encompass the taxi's coordinates, the passenger's location, and their destination.To leverage the knowledge state machine, the LLM is also provided with advantageous movements from the taxi's current location to the four colored locations.Overall Performance Table 1 presents a comparison of overall performance.We observe the following: Firstly, the CoT method was rarely able to complete the task successfully, with the number of LLM inferences consistently reaching the preset maximum limit of 30.Secondly, the ToT method exhibited a significant improvement with a 30% success ratio, suggesting that the exploration of different actions is critical for this task.</p>
<p>Thirdly, the SMoT method achieved the best performance in terms of both accuracy and efficiency.This experiment demonstrates that for tasks with a high repetition rate of subproblems, SMoT can achieve accurate and efficient reasoning for the task by preliminarily exploring all sub-problems and accumulating experience.</p>
<p>24-point Card Games</p>
<p>The 24-Point Card Game is a popular mathematical card game that challenges players to manipulate four given numbers.The objective is to combine these numbers using the arithmetic operations of addition, subtraction, multiplication, and division to achieve a total of 24.Each number must be used exactly once.This task has been adopted to test the numerical reasoning abilities of agents in ToT method [Yao et al., 2023a].</p>
<p>Task Setup</p>
<p>We follow the task setup in ToT [Yao et al., 2023a], where the experiments on conducted on the relatively hard games indexed 901-1,000 problems from the 4num.com.For each problem, the output equation is equal to 24 will be regard as success otherwise be fail.</p>
<p>Baselines</p>
<p>Following the baseline selection in the taxi problem and the experiments in ToT, we adopt CoT and ToT as our baselines.</p>
<p>CoT consists of prompts with several task reasoning trajectories.The implementation of ToT involves two main steps: sub-solution proposal and sub-problem evaluation.The subsolution step involves selecting two numbers and a mathematical operation to calculate a new number, with a breadth limit B set to 20.The evaluation step involves judging the solvability of the sub-problem, which is the possibility of the given number reaching 24.The LLM adopted in this paper is GPT-3.5-turbo,which exhibits relatively weak numerical reasoning capabilities.Therefore, we employ Python to infer the equation from a given ToT reasoning trajectory.SMoT Implementation Details Construction of the knowledge state machine.We utilize the baseline tool ToT to explore trajectories that describe the paths from the input four numbers to the final answer.For example, a trajectory might look like [(1,2,3,4), (1<em>2=2, (2, 3, 4)), (2</em>3=6, (6, 4)), (6*4=24, ( 24))] where each mathematical operation is considered a sub-solution and the remaining numbers are the subsequent sub-problems.Owing to the time efficiency required for the ChatGPT API, we collected these trajectories from the indexed problems 1-900, using a Python script with a backtracking algorithm.After collecting these trajectories, we employed both top-down and bottomup traversal algorithms to construct state machines, which include both beneficial and non-beneficial sub-solutions.In our experiments, the number of sub-problems recorded in the state machine reached 15,310.</p>
<p>Reasoning with the prior state machine.At the subsolution proposing stage, SMoT initially adopts sub-solutions from the state machine without subsequent evaluation.If the state machine lacks sub-solutions, SMoT reverts to ToT and uses an LLM to propose sub-solutions.During the subproblem evaluation stage, SMoT discards non-beneficial subsolutions and sub-problems recorded in the state machine.</p>
<p>Overall Performance</p>
<p>As illustrated in Table 2, the performance on this task was extremely poor, with a 0% success rate, attributed to a lack of exploration of the diverse intermediate steps.The ToT methodology demonstrated a marked improvement over CoT, in line with the experimental trends observed within ToT.However, there was a notable increase in the number of LLM reasoning times required by ToT.Our proposed SMoT approach achieved the best success ratio of 56%, and the average number of LLM reasoning iterations was significantly reduced to 36.2, considerably less than that required by ToT.This emphatically underscores the superiority of incorporating prior SM knowledge in enhancing the success rate of problem-solving while simultaneously reducing the number of LLM reasoning iterations needed.</p>
<p>Ablation Study of SMoT with Varying Numbers of States</p>
<p>To further investigate the impact of the number of states in a state machine on SMoT's problem-solving capabilities, a series of experiments were conducted on the game of 24-point using different numbers of states.The setups ranged from 100% to 1% of the original state count (as shown in Table 3).The data indicates an upward trend in the average number of LLM inferences as the number of states decreases.Regarding the success rate, SMoT maintained a relatively steady performance, around 56%, when the state count was reduced from 100% to 40%.However, a noticeable decline in the success</p>
<p>The Robustness Study for Noisy Knowledge State Machine</p>
<p>To evaluate the robustness of SMoT when incorporating noisy experience in the state machine, we transformed a proportional amount of conducive sub-solutions and sub-problems into non-conducive ones.Table 4 presents the performance of SMoT in conditions with noise levels ranging from 20% to 80%.It is observed that the introduction of noise significantly reduces SMoT's success rate, yet it remains superior to that of ToT baseline.Furthermore, SMoT sustains a success rate around 40% amidst 20%, 40%, and 60% noise levels.At a high noise threshold of 80%, the success rate falls to 30%, underscoring the adverse effects of incorrect state machine on the efficacy of SMoT.</p>
<p>Conclusion</p>
<p>In this paper, we introduced the State Machine of Thought (SMoT) paradigm, which leverages experience extracted from past reasoning trajectories to guide LLMs in effective problem-solving.Specifically, we utilize state machine to record the experience, where states represent the decomposed sub-problems and state transitions denote the sub-solutions.Integrate with the experience state machine, SMoT could will well handle the same sub-problems which are recorded in the state machine.The experiment conducted on a toy reinforce learning game and 24-point numerical reasoning task clearly demonstrate the effectiveness and efficiency of proposed SMoT.</p>
<p>Figure 2 :
2
Figure 2: The illustration of two traversal methods for constructing state machine.The green and red box denote the sub-problems evaluated conducive and non-conducive to solve the task, respectively</p>
<p>problems recorded in state machine Ineffective sub-problems recorded in state machine Sub-solutions</p>
<p>Figure 3 :
3
Figure 3: Comparison of State Machine of Thoughts (SMoT) with other prompting strategy</p>
<p>Figure 4 :
4
Figure 4: Five manually designed situations for experiments</p>
<p>Algorithm 1 SMoT-BFS search algorithm Input: the raw problem s 0 , conducive sub-solution query function f s , sub-problem solvability query function f p , LLMbased proposer P l , LLM-based evaluator V l , LLM p θ , step limit K, and breadth limit B 1: Let Q ← − {s 0 }.
3:/<em> sub-solution proposing </em>/4:
2: for k = 1, . . ., K do</p>
<p>Table 1 :
1
The performance comparison of the taxi task in terms of success ratio and the average count of LLM infers
MethodsSuccess ratio# LLM infersCoTToT SMoT CoT ToT SMoTSituation 10% 20% 100% 30.0 25.612.0Situation 20% 50%95% 30.0 39.515.2Situation 30% 35% 100% 30.0 56.215.0Situation 40% 30% 100% 30.0 33.615.0Situation 50% 15% 100% 30.0 30.915.0Average0% 30%99% 30.0 37.214.4</p>
<p>Table 2 :
2
The overall performance of 24-point game
Success Ratio #LLM infersCoT0%3.0ToT20%88.8SMoT56%36.2</p>
<p>Table 3 :
3
The ablation study of SMoT with different state numbers
Success Ratio #LLM infersSMoT-(100% states)56%36.2SMoT-(80% states)55%40.5SMoT-(60% states)56%41.6SMoT-(40% states)57%50.5SMoT-(20% states)50%60.1SMoT-(10% states)52%67.8SMoT-(5% states)42%72.2SMoT-(1% states)30%86.3ToT20%88.8</p>
<p>Table 4 :
4
The robustness study of SMoT with noisy state machine rate begins when the state count falls to 20% and below, dropping from 50% to 30%.Notably, at 1% state retention, SMoT achieved a 30% accuracy rate, significantly outperforming the ToT model's 20% accuracy.These findings suggest that SMoT retains a considerable problem-solving capacity even with a severely limited number of states, outpacing the baseline ToT model in performance.
success ratio #LLM infersSMoT-(80% noise)30%51.7SMoT-(60% noise)41%47.0SMoT-(40% noise)38%55.7SMoT-(20% noise)43%39.0SMoT-(0% noise)56%36.2</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Besta, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2023. 2023. 2020. 2020NeurIPS</p>
<p>Sparks of artificial general intelligence: Early experiments with gpt-4. Bubeck, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 1862023. 2023. 2023. 2023. 2023. 2023. June 1999NeurIPS</p>
<p>Reasoning with language model is planning with world model. Hao, 2023. 2023</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. Liu, CoRL. Neurips, 2023. 2023. 2023. 2023. 2022. 2022. 2023. 2023. 2017Attention is All you Need</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Wang , ICLR2022. 2022</p>
<p>Voyager: An open-ended embodied agent with large language models. Wang , 2023. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Wei, NeurIPS. 2022. 2022</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Wu, 2023. 2023</p>
<p>Selfevaluation guided beam search for reasoning. Xie, Theoretical Computer Science: Exploring New Frontiers of Theoretical Informatics. 2023. 2023. 2000. 2023a. 2023Hierarchical State Machines. Tree of Thoughts: Deliberate problem solving with large language models</p>
<p>Building cooperative embodied agents modularly with large language models. Yao, ICLR. 2023b. 2023. 2023. 2023React: Synergizing reasoning and acting in language models</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5003 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5003</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5003</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-c88cafa3e980765a64febe369ceb7c2aa7261d2a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a" target="_blank">Complexity-Based Prompting for Multi-Step Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning that substantially improves multi- step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks and two BigBenchHard tasks.</p>
                <p><strong>Paper Abstract:</strong> We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5003.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5003.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity-Based Prompting (GPT-3, greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complexity-Based Prompting with GPT-3 (text-davinci-002) using greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Select few-shot chain-of-thought (CoT) prompts that are 'complex' (contain more reasoning steps per example) and use them to prompt GPT-3; generation uses greedy decoding producing a single chain per test instance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (175B parameters) provided by OpenAI, used via prompting APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complexity-based CoT prompting (greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Few-shot chain-of-thought prompting where the chosen in-context examples are the most 'complex' (measured by number of reasoning steps); decoding is greedy so a single reasoning chain is used per test instance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade School Math 8K: math word problems requiring multi-step arithmetic reasoning; metric is solve rate (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 55.4% (greedy decoding, Complex CoT) on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Handcrafted CoT greedy 48.1%; Random CoT greedy 49.7% (both with GPT-3 text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using complex CoT prompts (more steps per example) with GPT-3 and greedy decoding substantially improves accuracy over handcrafted and random CoT prompts (gain ≈ +7.3 percentage points on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No notable improvement when same approach is applied to much smaller models (see Table 5); some datasets/tasks (e.g., certain out-of-distribution settings) show smaller or inconsistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5003.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity-Based Consistency (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complexity-Based Consistency (sample N chains, majority vote among top-K complex chains) with GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>At decoding, sample many CoT outputs and take the majority answer only over the K most complex generated chains (ranked by number of reasoning steps) instead of over all samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (175B parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complexity-based consistency (top-K complex voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Diverse decoding by sampling N chains (they used N=50) and selecting the majority answer among the top-K most complex sampled chains (K < N, typically 30–40), thereby biasing the ensemble toward more complex reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step math word problems; metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy 72.6% (Text-davinci-002, Complex CoT + Vote Complex / top-K voting)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Self-consistency (vote among all samples) with handcrafted CoT: 64.0% (Text-davinci-002); Complex CoT + vote among all: 71.5% (Text-davinci-002) -> selecting top-K complex gave 72.6% (additional gain).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Voting among the most complex sampled reasoning chains yields higher accuracy than voting among all sampled chains (self-consistency); optimal performance achieved for some K < N, showing the benefit of privileging complex reasoning paths in the ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Voting only among the simplest sampled chains consistently underperformed compared to voting among all or top-K complex chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5003.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity-Based Prompting (Codex, greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complexity-Based Prompting with Codex (code-davinci-002) using greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use complex in-context CoT examples to prompt Codex and generate a single greedy reasoning chain per test instance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI Codex variant (175B parameters) specialized/trained with code-heavy data but used here for natural-language CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complexity-based CoT prompting (greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Few-shot CoT prompting where the prompt examples have many reasoning steps; decoding is greedy (single chain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MathQA, MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math word problem benchmarks requiring multi-step reasoning; performance measured as accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K accuracy 66.6% (greedy Complex CoT); MathQA accuracy 47.3% (greedy Complex CoT); MultiArith 95.8% (Complex CoT greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Codex Handcrafted CoT greedy: GSM8K 61.0%, MathQA 29.3%, MultiArith 95.8%; Random CoT greedy: GSM8K 60.4%, MathQA 40.5%, MultiArith 97.3%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Complex prompts improved Codex performance substantially (e.g., GSM8K +5.6 pp and MathQA +18.0 pp in greedy decoding) compared to handcrafted and random prompts; gains vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MultiArith was already high for Codex and improvements were small or zero in some comparisons; retrieval-based selection sometimes outperformed complexity on MathQA (but retrieval requires full-training-set annotation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5003.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity-Based Consistency (Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complexity-Based Consistency with Codex (top-K complex voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling-based ensemble over Codex outputs but voting restricted to the top-K most complex sampled reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>175B-parameter Codex model used with sampled decoding (N=50) and top-K voting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complexity-based consistency (top-K complex voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample multiple CoT outputs (N=50) and take majority vote only among the K most complex (by step count) outputs; empirically K<N (e.g., K≈30–40) yielded optimal results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MathQA, MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math word problem benchmarks; metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K accuracy 82.9% (Complex CoT + Vote Complex) for Codex; MathQA 60.0% (Complex CoT + Vote Complex); MultiArith 99.8% (+0.1 over baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Self-consistency (vote among all samples) for Codex Handcrafted CoT: GSM8K 74.6%; Complex CoT + vote among all: 82.6% -> top-K complex voting gave 82.9% (small further gain).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selecting and voting among the most complex sampled outputs yields state-of-the-art performance on multiple math benchmarks, improving over both standard self-consistency and greedy decoding baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Improvements are model- and dataset-dependent; in some extremely high-performing datasets (e.g., MultiArith for Codex) gains are marginal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5003.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Thought Prompting (Wei et al., 2022b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Provide few-shot examples that include intermediate reasoning steps (rationales) before final answers to elicit multi-step reasoning from large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various large LMs (baseline comparisons in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Originally demonstrated on very large transformer LMs (e.g., PaLM); in this paper CoT is used as the primary prompting paradigm for GPT-3 and Codex experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-thought prompting (single sample or few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Few-shot in-context learning where each example includes step-by-step intermediate reasoning lines before the final answer; generation can be greedy or sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MultiArith, MathQA, StrategyQA, Date Understanding, Penguins</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks for multi-step math and non-math reasoning used throughout the paper as primary tasks for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline handcrafted CoT with GPT-3 greedy: GSM8K 48.1% (text-davinci-002); Codex greedy 61.0% (handcrafted CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Complex CoT (described in this paper) improved these numbers substantially (e.g., GPT-3 Complex CoT greedy 55.4% on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT is necessary to elicit multi-step reasoning in large LMs; however, the choice of which CoT examples to include matters: more complex CoT examples outperform standard handcrafted CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT prompting effectiveness depends on model scale; smaller models do not reliably gain from CoT or from complexity-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5003.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (Wang et al., 2022b) sampling and majority-vote ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sample multiple chain-of-thought outputs and take the majority-voted answer across sampled chains to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various; implemented in paper with GPT-3/Codex</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling-based ensemble approach applied to large LMs (N up to 50 sampled reasoning chains).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-consistency (majority vote among all samples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate many sampled chain-of-thought outputs for each question and choose the final answer by majority vote across all samples, reducing sensitivity to any single sampled chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MultiArith, MathQA (used as baseline ensemble approach)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning benchmarks; evaluated using majority-vote ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: Text-davinci-002 Handcrafted CoT + majority vote: 64.0% on GSM8K (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Complexity-based consistency (vote among top-K complex chains) improved further to 72.6% (Text-davinci-002) on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency substantially boosts performance relative to greedy decoding; complexity-based selection within the self-consistency framework further improves results by focusing votes on more complex chains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-consistency's benefit can be improved upon by selecting a subset of sampled chains (top-K complex) rather than voting over all samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5003.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-based Prompt Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-based example selection (Rubin et al., 2022) used as a comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieve training instances most similar to a test question and use them as in-context examples; requires dense retrieval over large annotated training set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to retrieve prompts for in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval pipeline combined with language models (reported results in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval uses sentence embeddings (e.g., SBERT) to find nearest training examples; prompts differ per test instance and typically require full-training-set CoT annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Retrieval-based CoT selection (per-instance retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar/diverse (varies per test case)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>For each test question, retrieve the most similar annotated training examples and use them as the few-shot CoT prompt (different prompt per test case).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MathQA (validation comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MathQA: operation-formalism math problems (noisier annotations), used to evaluate selection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Retrieval selection validation accuracy on MathQA: 69.5% (requires full training-set annotation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Complexity-based few-shot selection validation on MathQA: 42.5% (few-shot, 8 annotated examples)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval can outperform complexity-based few-shot selection on MathQA, but retrieval requires substantially more annotation (full training-set) and relies on high similarity between train and test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Retrieval's advantage on MathQA arises in part because many dev questions are near-duplicates of training questions; complexity-based selection remains more annotation-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5003.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complexity Benefit Absent in Small Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of benefit from complexity-based prompting in smaller models (e.g., text-curie-001, Flan-T5 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying complexity-based prompt selection to smaller LMs did not yield the performance gains observed in large models, indicating the effect is an emergent property of scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-curie-001 (6.7B), Flan-T5 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller transformer-based LMs (6.7B and 11B parameters respectively) evaluated to probe scale dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Complexity-based CoT prompting (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Select complex CoT prompts (many steps) and prompt smaller models using greedy decoding; tested whether complexity yields improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith, GSM8K (validation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math reasoning benchmarks used to test whether complexity-based prompting generalizes across model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Small models showed no meaningful gains from complex prompts: e.g., Flan-T5 11B on GSM8K handcrafted 19.5% vs complex 21.0% (tiny change); text-curie-001 on MultiArith handcrafted 3.8% vs complex 3.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Large models (GPT-3/Codex) showed substantial gains from complexity-based prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Complexity-based prompting appears to be an emergent phenomenon that requires large model scale (≈100B+ parameters); smaller models do not reliably benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No counterexample within small models; uniformly small or negative deltas indicate absence of benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5003.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5003.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voting among Simple Chains</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority voting restricted to simplest sampled CoT outputs (top-K simplest)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>As an ablation, the authors vote among the K simplest sampled reasoning chains and compare performance to voting among all and voting among top-K complex chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / Codex (evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LMs (175B) sampled to produce multiple CoT outputs (N=50); subsets of outputs selected by simplicity/complexity for voting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Simple-chain restricted voting (top-K simple)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample multiple CoT outputs and restrict majority voting to the K outputs with the fewest reasoning steps (simplest chains).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MathQA (validation analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning benchmarks used for ablation studies comparing different voting subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Consistently underperformed relative to voting among all samples and voting among top-K complex chains (exact numbers vary; plotted in Figure 6 and text states 'always underperform').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Voting among all (self-consistency) and voting among top-K complex chains outperform simple-chain voting; top-K complex often achieves the best results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Biasing the ensemble toward more complex sampled chains improves performance, while privileging simpler sampled chains degrades performance; this supports the hypothesis that complex reasoning chains are more reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No dataset in the paper showed simple-chain voting outperforming the other voting schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complexity-Based Prompting for Multi-Step Reasoning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>On the advance of making language models better reasoners <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5003",
    "paper_id": "paper-c88cafa3e980765a64febe369ceb7c2aa7261d2a",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Complexity-Based Prompting (GPT-3, greedy)",
            "name_full": "Complexity-Based Prompting with GPT-3 (text-davinci-002) using greedy decoding",
            "brief_description": "Select few-shot chain-of-thought (CoT) prompts that are 'complex' (contain more reasoning steps per example) and use them to prompt GPT-3; generation uses greedy decoding producing a single chain per test instance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-002)",
            "model_description": "Autoregressive transformer language model (175B parameters) provided by OpenAI, used via prompting APIs.",
            "reasoning_method_name": "Complexity-based CoT prompting (greedy)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Few-shot chain-of-thought prompting where the chosen in-context examples are the most 'complex' (measured by number of reasoning steps); decoding is greedy so a single reasoning chain is used per test instance.",
            "task_name": "GSM8K (math word problems)",
            "task_description": "Grade School Math 8K: math word problems requiring multi-step arithmetic reasoning; metric is solve rate (accuracy).",
            "performance": "accuracy 55.4% (greedy decoding, Complex CoT) on GSM8K",
            "comparison_with_other_method": true,
            "performance_other_method": "Handcrafted CoT greedy 48.1%; Random CoT greedy 49.7% (both with GPT-3 text-davinci-002)",
            "key_findings": "Using complex CoT prompts (more steps per example) with GPT-3 and greedy decoding substantially improves accuracy over handcrafted and random CoT prompts (gain ≈ +7.3 percentage points on GSM8K).",
            "counter_examples_or_negative_results": "No notable improvement when same approach is applied to much smaller models (see Table 5); some datasets/tasks (e.g., certain out-of-distribution settings) show smaller or inconsistent gains.",
            "uuid": "e5003.0",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Complexity-Based Consistency (GPT-3)",
            "name_full": "Complexity-Based Consistency (sample N chains, majority vote among top-K complex chains) with GPT-3",
            "brief_description": "At decoding, sample many CoT outputs and take the majority answer only over the K most complex generated chains (ranked by number of reasoning steps) instead of over all samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-002)",
            "model_description": "Autoregressive transformer language model (175B parameters).",
            "reasoning_method_name": "Complexity-based consistency (top-K complex voting)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Diverse decoding by sampling N chains (they used N=50) and selecting the majority answer among the top-K most complex sampled chains (K &lt; N, typically 30–40), thereby biasing the ensemble toward more complex reasoning paths.",
            "task_name": "GSM8K (math word problems)",
            "task_description": "Multi-step math word problems; metric: accuracy.",
            "performance": "accuracy 72.6% (Text-davinci-002, Complex CoT + Vote Complex / top-K voting)",
            "comparison_with_other_method": true,
            "performance_other_method": "Self-consistency (vote among all samples) with handcrafted CoT: 64.0% (Text-davinci-002); Complex CoT + vote among all: 71.5% (Text-davinci-002) -&gt; selecting top-K complex gave 72.6% (additional gain).",
            "key_findings": "Voting among the most complex sampled reasoning chains yields higher accuracy than voting among all sampled chains (self-consistency); optimal performance achieved for some K &lt; N, showing the benefit of privileging complex reasoning paths in the ensemble.",
            "counter_examples_or_negative_results": "Voting only among the simplest sampled chains consistently underperformed compared to voting among all or top-K complex chains.",
            "uuid": "e5003.1",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Complexity-Based Prompting (Codex, greedy)",
            "name_full": "Complexity-Based Prompting with Codex (code-davinci-002) using greedy decoding",
            "brief_description": "Use complex in-context CoT examples to prompt Codex and generate a single greedy reasoning chain per test instance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "OpenAI Codex variant (175B parameters) specialized/trained with code-heavy data but used here for natural-language CoT prompting.",
            "reasoning_method_name": "Complexity-based CoT prompting (greedy)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Few-shot CoT prompting where the prompt examples have many reasoning steps; decoding is greedy (single chain).",
            "task_name": "GSM8K, MathQA, MultiArith",
            "task_description": "Math word problem benchmarks requiring multi-step reasoning; performance measured as accuracy.",
            "performance": "GSM8K accuracy 66.6% (greedy Complex CoT); MathQA accuracy 47.3% (greedy Complex CoT); MultiArith 95.8% (Complex CoT greedy).",
            "comparison_with_other_method": true,
            "performance_other_method": "Codex Handcrafted CoT greedy: GSM8K 61.0%, MathQA 29.3%, MultiArith 95.8%; Random CoT greedy: GSM8K 60.4%, MathQA 40.5%, MultiArith 97.3%",
            "key_findings": "Complex prompts improved Codex performance substantially (e.g., GSM8K +5.6 pp and MathQA +18.0 pp in greedy decoding) compared to handcrafted and random prompts; gains vary by dataset.",
            "counter_examples_or_negative_results": "MultiArith was already high for Codex and improvements were small or zero in some comparisons; retrieval-based selection sometimes outperformed complexity on MathQA (but retrieval requires full-training-set annotation).",
            "uuid": "e5003.2",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Complexity-Based Consistency (Codex)",
            "name_full": "Complexity-Based Consistency with Codex (top-K complex voting)",
            "brief_description": "Sampling-based ensemble over Codex outputs but voting restricted to the top-K most complex sampled reasoning chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "175B-parameter Codex model used with sampled decoding (N=50) and top-K voting.",
            "reasoning_method_name": "Complexity-based consistency (top-K complex voting)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample multiple CoT outputs (N=50) and take majority vote only among the K most complex (by step count) outputs; empirically K&lt;N (e.g., K≈30–40) yielded optimal results.",
            "task_name": "GSM8K, MathQA, MultiArith",
            "task_description": "Math word problem benchmarks; metric: accuracy.",
            "performance": "GSM8K accuracy 82.9% (Complex CoT + Vote Complex) for Codex; MathQA 60.0% (Complex CoT + Vote Complex); MultiArith 99.8% (+0.1 over baseline).",
            "comparison_with_other_method": true,
            "performance_other_method": "Self-consistency (vote among all samples) for Codex Handcrafted CoT: GSM8K 74.6%; Complex CoT + vote among all: 82.6% -&gt; top-K complex voting gave 82.9% (small further gain).",
            "key_findings": "Selecting and voting among the most complex sampled outputs yields state-of-the-art performance on multiple math benchmarks, improving over both standard self-consistency and greedy decoding baselines.",
            "counter_examples_or_negative_results": "Improvements are model- and dataset-dependent; in some extremely high-performing datasets (e.g., MultiArith for Codex) gains are marginal.",
            "uuid": "e5003.3",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) Prompting",
            "name_full": "Chain of Thought Prompting (Wei et al., 2022b)",
            "brief_description": "Provide few-shot examples that include intermediate reasoning steps (rationales) before final answers to elicit multi-step reasoning from large language models.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Various large LMs (baseline comparisons in paper)",
            "model_description": "Originally demonstrated on very large transformer LMs (e.g., PaLM); in this paper CoT is used as the primary prompting paradigm for GPT-3 and Codex experiments.",
            "reasoning_method_name": "Chain-of-thought prompting (single sample or few-shot)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Few-shot in-context learning where each example includes step-by-step intermediate reasoning lines before the final answer; generation can be greedy or sampled.",
            "task_name": "GSM8K, MultiArith, MathQA, StrategyQA, Date Understanding, Penguins",
            "task_description": "Benchmarks for multi-step math and non-math reasoning used throughout the paper as primary tasks for comparisons.",
            "performance": "Baseline handcrafted CoT with GPT-3 greedy: GSM8K 48.1% (text-davinci-002); Codex greedy 61.0% (handcrafted CoT).",
            "comparison_with_other_method": true,
            "performance_other_method": "Complex CoT (described in this paper) improved these numbers substantially (e.g., GPT-3 Complex CoT greedy 55.4% on GSM8K).",
            "key_findings": "CoT is necessary to elicit multi-step reasoning in large LMs; however, the choice of which CoT examples to include matters: more complex CoT examples outperform standard handcrafted CoT prompts.",
            "counter_examples_or_negative_results": "CoT prompting effectiveness depends on model scale; smaller models do not reliably gain from CoT or from complexity-based selection.",
            "uuid": "e5003.4",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Self-Consistency (baseline)",
            "name_full": "Self-Consistency (Wang et al., 2022b) sampling and majority-vote ensemble",
            "brief_description": "Sample multiple chain-of-thought outputs and take the majority-voted answer across sampled chains to improve robustness and accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Various; implemented in paper with GPT-3/Codex",
            "model_description": "Sampling-based ensemble approach applied to large LMs (N up to 50 sampled reasoning chains).",
            "reasoning_method_name": "Self-consistency (majority vote among all samples)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate many sampled chain-of-thought outputs for each question and choose the final answer by majority vote across all samples, reducing sensitivity to any single sampled chain.",
            "task_name": "GSM8K, MultiArith, MathQA (used as baseline ensemble approach)",
            "task_description": "Multi-step reasoning benchmarks; evaluated using majority-vote ensembles.",
            "performance": "Example: Text-davinci-002 Handcrafted CoT + majority vote: 64.0% on GSM8K (Table 1).",
            "comparison_with_other_method": true,
            "performance_other_method": "Complexity-based consistency (vote among top-K complex chains) improved further to 72.6% (Text-davinci-002) on GSM8K.",
            "key_findings": "Self-consistency substantially boosts performance relative to greedy decoding; complexity-based selection within the self-consistency framework further improves results by focusing votes on more complex chains.",
            "counter_examples_or_negative_results": "Self-consistency's benefit can be improved upon by selecting a subset of sampled chains (top-K complex) rather than voting over all samples.",
            "uuid": "e5003.5",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Retrieval-based Prompt Selection",
            "name_full": "Retrieval-based example selection (Rubin et al., 2022) used as a comparison",
            "brief_description": "Retrieve training instances most similar to a test question and use them as in-context examples; requires dense retrieval over large annotated training set.",
            "citation_title": "Learning to retrieve prompts for in-context learning",
            "mention_or_use": "use",
            "model_name": "Retrieval pipeline combined with language models (reported results in this paper)",
            "model_description": "Retrieval uses sentence embeddings (e.g., SBERT) to find nearest training examples; prompts differ per test instance and typically require full-training-set CoT annotations.",
            "reasoning_method_name": "Retrieval-based CoT selection (per-instance retrieval)",
            "reasoning_method_type": "similar/diverse (varies per test case)",
            "reasoning_method_description": "For each test question, retrieve the most similar annotated training examples and use them as the few-shot CoT prompt (different prompt per test case).",
            "task_name": "MathQA (validation comparison)",
            "task_description": "MathQA: operation-formalism math problems (noisier annotations), used to evaluate selection methods.",
            "performance": "Retrieval selection validation accuracy on MathQA: 69.5% (requires full training-set annotation).",
            "comparison_with_other_method": true,
            "performance_other_method": "Complexity-based few-shot selection validation on MathQA: 42.5% (few-shot, 8 annotated examples)",
            "key_findings": "Retrieval can outperform complexity-based few-shot selection on MathQA, but retrieval requires substantially more annotation (full training-set) and relies on high similarity between train and test cases.",
            "counter_examples_or_negative_results": "Retrieval's advantage on MathQA arises in part because many dev questions are near-duplicates of training questions; complexity-based selection remains more annotation-efficient.",
            "uuid": "e5003.6",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Complexity Benefit Absent in Small Models",
            "name_full": "Lack of benefit from complexity-based prompting in smaller models (e.g., text-curie-001, Flan-T5 11B)",
            "brief_description": "Applying complexity-based prompt selection to smaller LMs did not yield the performance gains observed in large models, indicating the effect is an emergent property of scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-curie-001 (6.7B), Flan-T5 (11B)",
            "model_description": "Smaller transformer-based LMs (6.7B and 11B parameters respectively) evaluated to probe scale dependence.",
            "reasoning_method_name": "Complexity-based CoT prompting (few-shot)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Select complex CoT prompts (many steps) and prompt smaller models using greedy decoding; tested whether complexity yields improvements.",
            "task_name": "MultiArith, GSM8K (validation)",
            "task_description": "Math reasoning benchmarks used to test whether complexity-based prompting generalizes across model scales.",
            "performance": "Small models showed no meaningful gains from complex prompts: e.g., Flan-T5 11B on GSM8K handcrafted 19.5% vs complex 21.0% (tiny change); text-curie-001 on MultiArith handcrafted 3.8% vs complex 3.3%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Large models (GPT-3/Codex) showed substantial gains from complexity-based prompting.",
            "key_findings": "Complexity-based prompting appears to be an emergent phenomenon that requires large model scale (≈100B+ parameters); smaller models do not reliably benefit.",
            "counter_examples_or_negative_results": "No counterexample within small models; uniformly small or negative deltas indicate absence of benefit.",
            "uuid": "e5003.7",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Voting among Simple Chains",
            "name_full": "Majority voting restricted to simplest sampled CoT outputs (top-K simplest)",
            "brief_description": "As an ablation, the authors vote among the K simplest sampled reasoning chains and compare performance to voting among all and voting among top-K complex chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 / Codex (evaluated in paper)",
            "model_description": "Large LMs (175B) sampled to produce multiple CoT outputs (N=50); subsets of outputs selected by simplicity/complexity for voting.",
            "reasoning_method_name": "Simple-chain restricted voting (top-K simple)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample multiple CoT outputs and restrict majority voting to the K outputs with the fewest reasoning steps (simplest chains).",
            "task_name": "GSM8K, MathQA (validation analyses)",
            "task_description": "Multi-step reasoning benchmarks used for ablation studies comparing different voting subsets.",
            "performance": "Consistently underperformed relative to voting among all samples and voting among top-K complex chains (exact numbers vary; plotted in Figure 6 and text states 'always underperform').",
            "comparison_with_other_method": true,
            "performance_other_method": "Voting among all (self-consistency) and voting among top-K complex chains outperform simple-chain voting; top-K complex often achieves the best results.",
            "key_findings": "Biasing the ensemble toward more complex sampled chains improves performance, while privileging simpler sampled chains degrades performance; this supports the hypothesis that complex reasoning chains are more reliable.",
            "counter_examples_or_negative_results": "No dataset in the paper showed simple-chain voting outperforming the other voting schemes.",
            "uuid": "e5003.8",
            "source_info": {
                "paper_title": "Complexity-Based Prompting for Multi-Step Reasoning",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "On the advance of making language models better reasoners",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1
        }
    ],
    "cost": 0.01656025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Complexity-Based Prompting for Multi-step REASONING</h1>
<p>Yao Fu ${ }^{\mathbf{1}}$, Hao Peng ${ }^{\mathbf{A}}$, Ashish Sabharwal ${ }^{\mathbf{A}}$, Peter Clark ${ }^{\mathbf{A}}$, Tushar Khot ${ }^{\mathbf{A}}$<br>${ }^{A}$ University of Edinburgh ${ }^{\mathbf{A}}$ Allen Institute for AI<br>yao.fu@ed.ac.uk, haop@allenai.org, ashishs@allenai.org, peterc@allenai.org, tushark@allenai.org</p>
<h4>Abstract</h4>
<p>We study the task of prompting large-scale language models to perform multistep reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexitybased prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multistep reasoning tasks over strong baselines. We further extend our complexitybased criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.</p>
<h2>1 INTRODUCTION</h2>
<p>We consider the problem of prompting large language models for multi-step reasoning. Recent breakthroughs (Wei et al., 2022b; Wang et al., 2022b) show that language models, when large enough ( $&gt;100 \mathrm{~B}$ parameters), exhibit the emergent ability (Wei et al., 2022a) of performing complex multi-step reasoning when provided with only a few reasoning examples. In the regime of large models, prompting achieves comparable or even better performance than full training set finetuning while being substantially more sample-efficient (Wei et al., 2022b; Kojima et al., 2022; Lewkowycz et al., 2022). In particular, Wei et al. (2022b) show that chain-of-thoughts (CoT) prompts, sequences of short sentences describing intermediate reasoning steps towards final answers (Fig. 1A), can elicit strong reasoning capabilities from large language models for complex tasks such as math problems.</p>
<p>This work studies example selection in chain-of-thoughts multi-step reasoning. Example selection is a central problem in the prompting literature (Liu et al., 2022; Rubin et al., 2022; Su et al., 2022; Lazaridou et al., 2022). It asks what instances make the best prompts for solving the tasks of interest. For CoT prompting, example selection is further related to annotation efficiency, as CoT requires manually-annotated reasoning chains. For datasets where reasoning annotations are easy to obtain, one may want to know which annotated chains make the best prompt; if the annotations are hard to obtain, one may identify the best cases to annotate, rather than annotating the entire dataset.</p>
<p>We propose complexity-based prompting, a new example selection scheme for chain-of-thoughts multi-step reasoning. Existing sample selection methods are usually based on manual tries (Wei</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A: Chain of thoughts (in blue) are intermediate reasoning steps towards a final answer. The input of CoT prompting is a stack of few (often 8) CoT cases before a test question. Then the language model will continue generating an output CoT for the test question. B: Chains of harder reasoning complexity are chains with more reasoning steps ( 9 steps in this case, v.s. only 2 steps in subfigure A). C: During decoding, we sample $N$ reasoning chains from the language model ( $N=5$ here), and take the majority answer over the $K$ ( $K=3$ here) most complex generated chains.
et al., 2022b), heuristic rules (Wallace et al., 2019), optimization and search (Shin et al., 2020), or retrieval from a large training set (Rubin et al., 2022). Different from these schemes, complexitybased prompting chooses examples with complex reasoning chains, i.e., chains with more reasoning steps, as the prompt. Fig. 1A shows a simple example with 2 reasoning steps, versus the example in subfigure B is a complex case with 9 reasoning steps. As we will show in the experiments (§4.2), the reasoning performance of GPT-3 175B (Brown et al., 2020) clearly improves with the increased input prompt complexity, where complex prompts achieve better performance than simple prompts.
We further extend the complexity-based selection criteria from the input space (the prompts) to the output space (reasoning chains generated by the language model). Our extension is based on the idea of self-consistency (Wang et al., 2022b;a), where they sample multiple reasoning chains (instead of using greedy decoding) from the model that lead to possibly different answers, then choose the majority of the generated answers. Here we propose complexity-based consistency, where instead of taking a majority vote among all generated chains, we vote over the top $K$ complex chains, as shown in Fig. 1C. In $\S 4.2$, we will show that complexity-based consistency leads to further performance gains, on top of the existing gain from complexity-based prompting.
Putting everything together, our methods achieve new state of the art performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins) with substantial performance gains over Wei et al. (2022b). We show that, compared with existing sample selection schemes, complexity-based prompting achieves better performance in most cases (see $\S 4.2$ ). Furthermore, performance gains from complex samples are consistent in different prompt distributions (in-distribution, transfer, and noisily-labeled, see $\S 4.2$ ) and are also consistent with regard to alternative proxies for complexity (e.g., question or formula lengths, see $\S 4.3$ ) when the dataset does not contain annotated reasoning chains. A careful analysis shows that the number of reasoning steps is the most prominent factor, over confounders like prompt lengths or the number of input cases (§4.3). We hope this work will open new research possibilities in in-context learning, large language models, and multi-step reasoning.</p>
<h1>2 Related Work</h1>
<p>Emergent Abilities and Multi-Step Reasoning With the recent trend in scaling language models (Brown et al., 2020; Chowdhery et al., 2022), a central question is what unique abilities emerge as models become large (Kaplan et al., 2020; Wei et al., 2022a). Generally, the ability to follow the format of given prompts (typically few-shot) thus solving the corresponding tasks (also</p>
<p>referred as in-context learning), is something that large language models are particularly skilled at (Shin et al., 2020; Liu et al., 2021). Among the wide language understanding task spectrum, we are particularly interested in multi-step reasoning because of its two uniqueness: (1). multistep reasoning is a task where large models substantially outperform smaller models (Wei et al., 2022b), versus performance gains on tasks like sentiment classification can be very limited with large models (Shin et al., 2020); (2). multi-step reasoning is where few-shot prompting starts to outperform full training set fine-tuning, even when fine-tuning is conducted on the same large model (Lewkowycz et al., 2022). This work takes an important step forward in multi-step reasoning by showing the critical role of prompt complexity.</p>
<p>Chain-of-Thoughts Reasoning A prominent work demonstrating the multi-step reasoning of language models is chain-of-thoughts prompting (Fig. 1A), proposed by Wei et al. (2022b). They show that the reasoning ability can only be elicited by chain of thoughts, but not standard prompting where an answer directly follows a question without intermediate reasoning steps. Further works show that CoT can be improved by self-consistency (Wang et al., 2022b), pretraining the model with latex-formated data (Lewkowycz et al., 2022), context selection (Creswell et al., 2022), or even adding certain magic phrases like "Let's think step by step" (Kojima et al., 2022). The original CoT paper (Wei et al., 2022b) uses 8 manually written examples as the prompt, which are reused by most follow-up works. Our work sits in the context of CoT reasoning, and propose a new complexitybased prompt selection that substantially outperforms the original CoT.</p>
<p>Example Selection for Prompting Designing prompts can be challenging due to the instability, as multiple works have shown the performance is sensitive to prompt, task, dataset, and model changes (Zhao et al., 2021; Lu et al., 2022; Su et al., 2022). Despite works on automatic prompt searching (which is more suitable for smaller models, e.g., Shin et al., 2020; Li \&amp; Liang, 2021), currently, prompt engineering for large models is (still) a community-wide collective trial and error effort (there is even a prompt marketplace named PromptBase). The difficulty is that it is extremely hard to extract generalizable regularity from empirical observations that can form effective selection criteria. One notable exception is similarity-based prompt selection, which retrieves the most similar training instances as the prompt for a given test case (Rubin et al., 2022). Yet for CoT prompting, retrieving different prompts for different test cases requires reasoning chain annotations for the whole training set, which compromises the advantage of being few-shot. Given this background, our core contribution is identifying complexity as an effective and robust selection criterion and in many cases, it outperforms existing prompt selection schemes while being annotation-efficient.</p>
<p>Relation to Classical Semantic Parsing The procedure of chain of thoughts prompting is conceptually similar to classical semantic parsing where one generates a logical form then executes it upon a knowledge base to reach a final answer (Liang, 2016; Cheng et al., 2019). The practice of sampling then voting is also similar to marginalizing out semantic parses (Yin et al., 2018). There are further works linking the relationship between in-context learning and classical Bayesian inference (Wei et al., 2021; Xie et al., 2022). From our perspective, we tend to view chain-ofthoughts as flexible, language model styled "logical forms" which are "executed" by the language model itself. We leave further study on connecting classical parsing and CoT to future work.</p>
<h1>3 COMPLEXITY-BASED PROMPTING</h1>
<p>We study multi-step reasoning tasks, and use math word problems, mathematical problems expressed in natural language, as our testbed. This task, as is measured by solve rate (accuracy), is to predict the answer (typically a number) of a given math word problem via intermediate steps. We follow the chain-of-thoughts prompting framework and compare all prompting schemes using GPT-3 text-davinci-002 and Codex code-davinci-002. An example problem, as well as the chain-of-thoughts workflow, is shown in Fig. 1A. The input is a stack of a few (often 8) CoT cases followed by a test question, then the language model continues generating an output CoT for the test question. Our goal is to improve the reasoning accuracy by identifying and exploiting more effective input and output reasoning chains.</p>
<h1>3.1 Selecting Complex Samples as Prompts</h1>
<p>Our method is to simply choose complex prompts over simple ones. We hypothesize that language models' reasoning performance will increase if we use complex instances as in-context "training example," as they intuitively subsume simpler instances (Richardson \&amp; Sabharwal, 2022). We define complex instances as instances with more reasoning steps (Fig. 1B), as the name "multistep reasoning" indicates. Note that using reasoning steps as the notion of complexity is also the practice of previous works like (Sugawara et al., 2018; Lai et al., 2021). We further define a step as a line, separated by the linebreak " $\backslash \mathrm{n}$ ".</p>
<p>There are two aspects that need more discussion: (1) The notion of complexity. There are other complexity indicators than number of steps, such as questions lengths or the length of the underlying formula for solving a given problem. We will show that the trend that better performance comes with more complex prompts is consistent across various complexity indicators, such as question lengths and formula lengths. Consequently, for datasets that do not have annotated reasoning chains, we can use questions lengths to identify complex instances, then only annotate the identified few-shot instances, thus reducing the annotation cost. (2) Confounders of number of steps. The increase in performance with more complex examples in the prompt could be explained by correlated factors like the increase in the total number of reasoning steps in the prompts or just the increased length of the prompt. To account for this, we evaluate prompts with simpler examples but the same number of reasoning steps (e.g. 24 cases with 3 steps vs. 8 cases with 9 steps, both of 72 steps in total). We also consider prompts of the longest lengths (but not most steps). We show that the number of steps per example is the most prominent source of performance gains over confounders.</p>
<h3>3.2 COMPLEXITY-BASED CONSISTENCY</h3>
<p>Complexity-based prompting can be further enhanced with a new output selection method following the same intuition, which we present in this section. Existing evidence shows that the expressive neural models can take shortcuts during reasoning, relying on spurious correlations that inevitably exist in the training data (Mudrakarta et al., 2018; Sugawara et al., 2018; Lai et al., 2021). This often leads to suboptimal generalization to unseen data. To alleviate this issue, we explicitly promote outputs with more complex reasoning chains at inference time. Specifically, our method follows the self-consistency practice in Wang et al. (2022b), which samples $N$ reasoning chains for a test question. Different reasoning chains may lead to different answers, and Wang et al. (2022b) takes the majority answer as the prediction. In our case, instead of voting among all $N$ chains, we only vote among top $K(K \leq N)$ complex (more steps) reasoning chains, as shown in Fig. 1C. We dub our method Complexity-based Consistency. Note that when $K=N$ we recover the original selfconsistency method. In our experiments, we set $N$ to 50 , and observe that the optimal $K$ is always smaller than $N$ (typically 30-40). This provides clear evidence that voting among more complex reasoning chains generalizes better than voting among all. We also show that if we do the opposite and vote among answers produced by $K$ simplest reasoning chains, the accuracy is always worse than voting among all. This further validates that complex chains, not simple chains, should be considered more during decoding.</p>
<h2>4 EXPERIMENTS</h2>
<p>We first discuss our experimental settings in $\S 4.1$. In $\S 4.2$ and $\S 4.3$, we present the following results: (1) our method substantially outperforms the original CoT (Wei et al., 2022b). It establishes new state-of-the-art results on three math reasoning datasets (GSM8K; Cobbe et al., 2021; MultiArith; Roy \&amp; Roth, 2015; MathQA; Amini et al., 2019), a temporal reasoning task (Date Understanding; Suzgun et al., 2022), and the referential game task (Penguins; Suzgun et al., 2022). On StrategyQA (Geva et al., 2021), a commonsense reasoning dataset, our approach matches the existing state-of-the-art performance. (2) Performance gains from complex prompts are consistent: no matter what large model we use (GPT-3 or Codex), what distribution the prompt come from (in-distribution, noisy distribution, and distribution shift), or whether there exists prompt format perturbation or confounders, complex prompts consistently outperform simpler prompts; (3) Compared with other example selection schemes (random, heuristic and retrieval), complexity-based example selection often achieves the best or competitive results with minimal annotation budget.</p>
<h1>4.1 EXPERIMENTAL SETTINGS</h1>
<p>Datasets We use three math word problems datasets (GSM8K, MultiArith, and MathQA) and three non-math reasoning (StrategyQA, Date Understanding, and Penguins) as our testbed. We choose GSM8K and MultiArith also because they are the datasets used by prior work on CoTs (Wei et al., 2022b; Wang et al., 2022b; Kojima et al., 2022), allowing fair comparison to existing methods. MathQA's annotation are much noisier than others, and we use it to evaluate the robustness of our approach. There are 1.3 K test instances in GSM8K, 600 in MultiArith, and 600 in MathQA. For each dataset, we randomly draw 200 instances from the training data to create a validation split. The cost of prompting GPT-3 is proportional to the size of test set. For the non-math datasets, StrategyQA is a multi-step commonsense reasoning task with 800 test instances. Date Understanding is a temporal reasoning task with 250 test instances. Penguins is a referential game (a referential game asks questions referring to different objects, e.g., is penguin A older than penguin B and C) with 146 test instances. Both Date Understanding and Penguins are subsets of the BigBench Hard datasets (datasets that previously fine-tuning struggles with, see Suzgun et al., 2022). Evaluating on a 200instances validation set costs about 6-8 US dolars for greedy decoding (1 output chain) and \$12-\$24 for sampling 50 output chains. Prompting Codex is currently (November 2022) free and we hope OpenAI could continue making it free to the community.</p>
<p>Language Models We consider two paradigms: fine-tuning and prompting. For fine-tuning, we report the existing SOTA performance: a fine-tuned GPT3 with a verifier (Cobbe et al., 2021) on GSM8K, a relevance and LCA operation classifier (Roy \&amp; Roth, 2015) on MultiArith and a customized sequence to sequence model (Amini et al., 2019) on MathQA. For prompting, we consider the following language models: (1). LaMDA (Thoppilan et al., 2022), a 137B model used as the baseline in Wei et al. (2022b); (2). PaLM (Chowdhery et al., 2022), the primary 540B model used in the CoT papers; (3). Minerva (Lewkowycz et al., 2022), a 540B large model that trains on $\mathrm{I} 5 \mathrm{~T}_{\mathrm{E}} \mathrm{Xdata}$; it achieves SOTA performance in math reasoning on GSM8K; (4). GPT-3 175B (text-davinci-002 from Brown et al., 2020) (5). Codex (code-davinci-002 from Chen et al., 2021, also 175B). We further consider the DiVeRSe (Li et al., 2022) method which equips an additional trained verified to GPT-3/ Codex and is the previous SOTA on GSM8K. Our experiments are mostly conducted on GPT-3 and Codex because they are the accessible to the public thus more reproducable. LaMDA, PaLM and Minerva are not accessible to the public, and their numbers are from their corresponding papers.</p>
<p>Prompts and Hyperparameters The training sets of GSM8K and MathQA contain human annotated reasoning chains, within which we search for complex prompts. MultiArith does not have annotated reasoning chains, so we consider two strategies. (1). in-distribution annotation, which uses question lengths as an alternative proxy for complexity, then manually annotates reasoning chains for complex questions; (2). prompts transfer from GSM8K training data. All prompts for math datasets contain 8 cases (a case $=$ a question + a chain of thoughts + an answer). For nonmath datasets, since they do not have annotated reasoning chain, we again, use question length as the complexity proxy and manually annotates reasoning chains for complex questions. Following Kojima et al. (2022), we add "Let's think step by step" before the reasoning chains for all prompting schemes to improve the performance.</p>
<h3>4.2 Main ReSults</h3>
<p>Overall Test Performance on Math Datasets Table 1 shows the overall performance of models. We consider two decoding strategies: (1) greedy decoding (the first block of Table 1) and (2) majority vote ( $\S 3.2$; the second block of Table 1). Note that PaLM and Minerva are more than three times larger than GPT-3 and Codex, the model we use to evaluate our method, and Minerva is additionally pretrained on latex data. Therefore, they are by no means comparable to the methods based on GPT-3 or Codex. We nevertheless outperform all of them.</p>
<p>We consider three prompting schemes: (1). Handcrafted CoT constructed originally by Wei et al. (2022b) then reused in following-up works (Wang et al., 2022b; Kojima et al., 2022; Wang et al., 2022a). (2). Random CoT: randomly drawing samples from the training set. GSM8K and MathQA training data have reasoning chain annotations, so we directly use them. MultiArith does not have reasoning annotations, so we randomly sample eight training cases then annotate the chains manually. (3). Complex CoT. For GSM8K and MathQA, we choose eight training cases with</p>
<p>Table 1: Complexity-based prompting, when applied on Codex (code-davinci-002), achieves new state-of-the-art performance on GSM8K, MultiArith, and MathQA. $\dagger$ models are not publicly accessible, and the numbers are from their papers. Our performance gain (+blue) is computed over the original handcrafted CoT used in Wei et al. (2022b), which is our primary baseline. Our methods substantially increase the performance over Wei et al. (2022b), with an average +5.3 gain on GPT-3 and +6.2 on Codex.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">#Params</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">MathQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Previous finetuning SOTA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\leq 175$ B</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;">Greedy decoding (Wei et al., 2022b)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LaMDA ${ }^{\dagger}$ (Thoppilan et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PaLM ${ }^{\dagger}$ (Chowdhery et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Minerva ${ }^{\dagger}$ (Lewkowycz et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Text-davinci-002</td>
<td style="text-align: center;">Handcrafted CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">34.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Complex CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$55.4(+7.3)$</td>
<td style="text-align: center;">$94.2(+3.4)$</td>
<td style="text-align: center;">$36.0(+5.9)$</td>
</tr>
<tr>
<td style="text-align: center;">Code-davinci-002</td>
<td style="text-align: center;">Handcrafted CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Complex CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$66.6(+5.6)$</td>
<td style="text-align: center;">$95.8(+0.0)$</td>
<td style="text-align: center;">$47.3(+18.0)$</td>
</tr>
<tr>
<td style="text-align: center;">Voting among multiple outputs (Wang et al., 2022b)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LaMDA ${ }^{\dagger}$ (Thoppilan et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">DiVeRSe (Li et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PaLM ${ }^{\dagger}$ (Chowdhery et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Minerva ${ }^{\dagger}$ (Lewkowycz et al., 2022)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Text-davinci-002</td>
<td style="text-align: center;">Handcrafted CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Complex CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ Vote Complex</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$72.6(+8.6)$</td>
<td style="text-align: center;">$98.7(+0.5)$</td>
<td style="text-align: center;">$50.2(+6.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Code-davinci-002</td>
<td style="text-align: center;">Handcrafted CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">55.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Complex CoT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">58.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ Vote Complex</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$82.9(+8.3)$</td>
<td style="text-align: center;">$99.8(+0.1)$</td>
<td style="text-align: center;">$60.0(+5.0)$</td>
</tr>
</tbody>
</table>
<p>the most numbers of reasoning steps; For MultiArith, we use the question length as the proxy for complexity, and manually annotate reasoning chains for the eight training cases with the longest questions. Complex prompt selection results in substantially more reasoning steps: it averages 9.0 steps on GSM8K, while the handcrafted and random schemes yield 3.4 and 2.8 steps respectively. The trends are similar on the other two datasets. The handcrafted prompts uses the same fixed prompt for all three datasets but the cases within the prompt does not come from any of the datasets (so they are in a sense, out of distribution). Complex prompts and random prompts all come from their corresponding training sets (so these two are in a sense, in-distribution).</p>
<p>As Table 1 shows, our method achieves substantially better performance than the baselines. Besides, our proposal of voting among complex chains outperforms voting among all (last two lines in Table 1. Furthermore, our performance using GPT-3 is close to PaLM and Minerva, two language models that are more than three times larger than GPT-3 and are not publicly accessible. These results directly demonstrate the effectiveness of our methods.</p>
<p>Consistent Performance Improvements on Different Reasoning Tasks Table 2 shows that the advantage of complex prompts holds for different types of reasoning tasks. When prompted with complex examples, GPT-3/ Codex achieves new SOTA performance on Date Understanding and Penguins datasets where complex prompts consistently improves performance over simpler prompts.</p>
<p>Performance Improvements Breakdown Improving prompting performance commonly requires prompt engineering. A common scepticism or criticism towards performance improvements</p>
<p>Table 2: Complex prompts give comparable performance to PaLM on StrategyQA (commonsense reasoning), and achieve new state of the art performance on Date Understanding (temporal reasoning), and Penguins (referential game) datasets. Accuracy gain (+blue) is computed over the original handcrafted CoT used in Wei et al. (2022b;a). All results use greedy decoding.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">#Params</th>
<th style="text-align: right;">StrategyQA</th>
<th style="text-align: right;">Date Understanding</th>
<th style="text-align: right;">Penguins</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">Handcrafted</td>
<td style="text-align: center;">540B</td>
<td style="text-align: right;">77.8</td>
<td style="text-align: right;">79.2</td>
<td style="text-align: right;">65.1</td>
</tr>
<tr>
<td style="text-align: left;">Text-davinci-002</td>
<td style="text-align: left;">Handcrafted</td>
<td style="text-align: center;">175B</td>
<td style="text-align: right;">66.9</td>
<td style="text-align: right;">82.8</td>
<td style="text-align: right;">76.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Simple</td>
<td style="text-align: center;">175B</td>
<td style="text-align: right;">71.1</td>
<td style="text-align: right;">76.4</td>
<td style="text-align: right;">61.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Complex</td>
<td style="text-align: center;">175B</td>
<td style="text-align: right;">$\mathbf{7 7 . 0 (+ 1 0 . 1 )}$</td>
<td style="text-align: right;">$82.4(-0.4)$</td>
<td style="text-align: right;">$79.5(+2.8)$</td>
</tr>
<tr>
<td style="text-align: left;">Code-davinci-002</td>
<td style="text-align: left;">Handcrafted</td>
<td style="text-align: center;">175B</td>
<td style="text-align: right;">73.1</td>
<td style="text-align: right;">86.0</td>
<td style="text-align: right;">78.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Simple</td>
<td style="text-align: center;">175B</td>
<td style="text-align: right;">74.4</td>
<td style="text-align: right;">83.2</td>
<td style="text-align: right;">69.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Complex</td>
<td style="text-align: center;">175B</td>
<td style="text-align: right;">$73.9(+0.8)$</td>
<td style="text-align: right;">$\mathbf{8 6 . 8}(+3.6)$</td>
<td style="text-align: right;">$\mathbf{8 0 . 8}(+2.7)$</td>
</tr>
</tbody>
</table>
<p>Table 3: GSM8K validation set performance improvements broken down on to various design choices. More than half of the accuracy improvements can be attributed to our complexity-based prompting and output selection (indicated by $\dagger$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Greedy Decoding</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: left;">Majority Vote</th>
<th style="text-align: center;">Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT Original</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: left;">CoT Original</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">Add "Let's think step by step"</td>
<td style="text-align: center;">$48.5(+5.0)$</td>
<td style="text-align: left;">Add "Let's think step by step"</td>
<td style="text-align: center;">$61.0(+5.5)$</td>
</tr>
<tr>
<td style="text-align: left;">(Kojima et al., 2022)</td>
<td style="text-align: center;">$54.0(+5.5)$</td>
<td style="text-align: left;">and change "Q: " to "Question:"</td>
<td style="text-align: center;">$67.0(+6.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Use complex prompt ${ }^{\dagger}$</td>
<td style="text-align: center;">$58.0(+4.0)$</td>
<td style="text-align: left;">Using within complex sample ${ }^{\dagger}$</td>
<td style="text-align: center;">$71.0(+4.0)$</td>
</tr>
<tr>
<td style="text-align: left;">Change "Q: " to "Question: "</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Validation set performance. X-axis means reasoning steps and y-axis means accuracy. More reasoning steps in prompts overall achieve higher accuracy when prompts are in-distribution (left), noisily labeled (middle), and out of distribution (right).
on leaderboards like Table 1 is whether the accuracy gains are from the proposed method or other independent engineering efforts. Here we list all the efforts we made for leaderboard climbing on Table 3. While techniques like adding "Let's think step by step" (Kojima et al., 2022) improves the accuracy, the performance gains can be primarily attributed to complexity-based prompting, validating the effectiveness of our methods.
Consistent Performance Improvements in Different Prompt Distributions We investigate the performance of our complexity-based prompting when the prompts are: (1) from clean indistribution training set (GSM8K); (2) from noisy annotation (MathQA); (3) are transferred from another dataset (MultiArith). Here as MultiArith does not have annotated reasoning chains, and their questions are similar to the ones in GSM8K; we use (transfer) prompts from GSM8K for MultiArith. Figure 2 shows that in general, more complex prompts achieve better performance, and this trend is consistent in all the three settings, except for one particular case on MultiArith.
Comparison to other Example Selection Schemes As we view the reasoning complexity as the basis of a new example selection scheme, we compare it with existing selection schemes. We consider: (1) random selection; (2) Centroid, where we select examples whose question embeddings (produced by a pretrained sentence encoder Reimers \&amp; Gurevych, 2019) are the closest</p>
<p>Table 4: Comparison to other prompt example selection schemes (validation accuracy). On GSM8K and MultiArith, complexity-based selection outperforms all the baselines. On MathQA, although retrieval performs better than complexity, it requires substantially more annotation.</p>
<table>
<thead>
<tr>
<th></th>
<th>#Annotations</th>
<th>GSM8K</th>
<th>MultiArith</th>
<th>MathQA</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>Few-shot (8)</td>
<td>52.5</td>
<td>86.5</td>
<td>33.0</td>
</tr>
<tr>
<td>Centroid</td>
<td>Few-shot (8)</td>
<td>52.0</td>
<td>92.0</td>
<td>32.0</td>
</tr>
<tr>
<td>Retrieval</td>
<td>Full training set $(\geq 10000)$</td>
<td>56.0</td>
<td>88.0</td>
<td>$\mathbf{6 9 . 5}$</td>
</tr>
<tr>
<td>Complexity (ours)</td>
<td>Few-shot (8)</td>
<td>$\mathbf{5 8 . 5}$</td>
<td>$\mathbf{9 3 . 0}$</td>
<td>42.5</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: X-axis means reasoning steps of dev set cases and y-axis frequency. The direction of generalization on the two datasets is intriguing and show different patterns: on GSM8K, simple prompts perform better for simple cases ( $\leq 3$ steps) while complex prompts perform better for complex cases; on MathQA, simple prompts do not have advantages for simple case and complex prompts seem to perform better on most of the groups.
to the embeddings of all other questions, i.e., questions at the center part of the dataset. The intuition is that centroid examples may be the most typical or representative cases of a dataset; (3) Retrieval, where we retrieve questions from a training set whose embeddings are closest test question measured in Euclidean distance. Notably, there are important differences between retrieval and other methods: retrieval uses different prompts for different test cases, while other methods use fixed prompts for all. Therefore, the annotation cost of retrieval scales with the size of the test set, and is usually about the full-training-set-sized annotation (more than 10 K cases), while others only require few-shot annotation (in our cases, only 8 examples).</p>
<p>As shown in Table 4, complexity-based selection outperforms all other methods on GSM8K and MultiArith. On MathQA, although retrieval-based selection outperforms complexity-based selection, it has two importance restrictions that we do not have: (1) as mentioned, retrieval requires substantially more CoT annotation, while we only requires few-shot; (2) the performance of retrieval is critically determined by how similar the test cases and the training questions are to each other, and the similarity may not always hold. We further note that on MathQA, many dev. questions are quite similar to their retrieved training questions (some of them only have minor changes like " 8 apples plus 9 bananas" to " 10 apples plus 5 bananas" while the underlying computations are the same). So in general, complexity-based prompting has the advantage of good performance while being annotation efficient.</p>
<p>Direction of Generalization Intuitively, one may attribute the improvements of complexitybased prompting to accuracy gains on complex test cases. Yet interestingly, our analysis suggests the opposite. Fig. 3 compares the validation set accuracy of complex and simple prompts, varying the number of reasoning steps in the gold annotation. We observe a clear trend on both GSM8K and MathQA: complex prompts perform on par with simple prompts on hard cases, while achieving more clear gains on cases with fewer number of reasoning steps. This finding suggests that complexity-based prompting generalizes to simpler test cases. We conjecture that this is because the reasoning capabilities elicited by complex prompts may cover simple questions better. Further investigation into the underlying mechanism is definitely interesting, and is left to future work.</p>
<p>Performance on Small Models Does smaller models also enjoy the performance gain from complex prompts? Unfortunately, this seems to be not the case. As is shown in Table 5, complex prompts cannot induce meaningful performance gain over the original or random prompts. This</p>
<p>Table 5: Complexity-based prompting is an emergent ability of large models. If applied to smaller models, complex prompts cannot induce significant performance gain (recall in Table 1 complex prompts induces average +6.2 , maximum +18.0 accuracy gain on Codex).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">text-curie-001 6.7B</td>
<td style="text-align: center;">Flan-T5 11B</td>
<td style="text-align: center;">Flan-T5 11B</td>
</tr>
<tr>
<td style="text-align: left;">Handcrafted</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">19.5</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">21.0</td>
</tr>
<tr>
<td style="text-align: left;">Complex</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">21.0</td>
</tr>
<tr>
<td style="text-align: left;">$\Delta$</td>
<td style="text-align: center;">-0.5</td>
<td style="text-align: center;">-0.2</td>
<td style="text-align: center;">+1.5</td>
</tr>
</tbody>
</table>
<p>Table 6: Alternative complexity measure: Q Len. = question length, F Len. = formula length. More complex prompts consistently outperform simpler ones.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Q Len.</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">F Len.</th>
<th style="text-align: center;">MathQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Simple</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: left;">Mid</td>
<td style="text-align: center;">226</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: left;">Complex</td>
<td style="text-align: center;">815</td>
<td style="text-align: center;">$\mathbf{5 2 . 5}$</td>
<td style="text-align: center;">165</td>
<td style="text-align: center;">$\mathbf{4 3 . 5}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Relationship between confounders.
indicates that, like the chain-of-thoughts prompting itself, complexity-based prompting is also an emergent ability that exist only when the model scale is large enough.</p>
<h1>4.3 ANALYSIS</h1>
<p>In this section, we develop in-depth analysis supporting our claims. All experiments in this section are performed on validation sets. We first show that the performance improvements with more reasoning complexity is consistent in terms of: (1). different proxies for complexity and (2). different step formatting. Then we show that the number of reasoning step is the most prominent factor for performance improvements over its confounders. Finally, we strengthen our conclusion of complexity-based consistency, and show that the optimal performance is always achieved by majority voting over complex chains, not simple chains.
Alternative Proxies for Complexity Complexity-based prompting is equally applicable when the data does not come with reasoning chain annotations, as we have already shown that selecting cases with longest questions also improves performance (§4.2). In Table 4, we confirm that in addition to number of steps, either using questions length or formula length as the measure of complexity, the optimal performance is achieved with complex prompts. These results mean that the effectiveness of complex prompts are consistent with regard to the notion of complexity.
Sensitivity Analysis on Step Format A common concern with prompting is that the performance can be sensitive to the format of the input (Shin et al., 2020; Liu et al., 2022) and may change with input perturbations. Here we study one important perturbation: the splitter of steps, which is an existing concern of CoT-styled prompting in Rong; Akyurek \&amp; Akyurek (2022). As alternatives to the linebreak "in" we use, we consider two more types of splitters: (1). explicit phrases "step i" (2). two punctuation marks, period "." and semicolon ";" The performance is shown in Table 7. Although these perturbations do have an influence on the performance, complex prompts consistently lead to better performance with regard to different step formatting.
Output Step Distribution As a sanity check, in Fig. 5, we show that complex prompts induce complex reasoning than simple prompts (Codex outputs on GSM8K and MathQA). This means that complex prompts are indeed discouraging the model from taking easier reasoning path, thus potentially avoiding shortcuts.
Confounder Analysis All experiments so far keeps the number of instance to be 8 in all prompts. Yet when choosing complex examples with more reasoning steps, we observe that the following factors are correlated (also illustrated in Fig. 4): (1). when per-case reasoning step</p>
<p>Table 7: Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Linebreak "in"</th>
<th style="text-align: center;">Period "."</th>
<th style="text-align: center;">Explicit "step i"</th>
<th style="text-align: center;">Semicolon ";"</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8K-Complex</td>
<td style="text-align: center;">$\mathbf{5 8 . 5}$</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K-Simple</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: left;">MathQA-Complex</td>
<td style="text-align: center;">$\mathbf{4 2 . 5}$</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">39.5</td>
</tr>
<tr>
<td style="text-align: left;">MathQA-Simple</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">37.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Output step distribution. X-axis means reasoning steps and y-axis means frequency. As a sanity check, complex prompts indeed induce complex outputs than simple prompts.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Majority voting over top K complex / simple generated samples. The optimal performance is achieved on selecting complex samples over simple samples.
increases (for example, in GSM8K we choose cases with 9 reasoning steps), the total number of step in the whole prompt also increase (in GSM8K, we have 8 cases in the prompt, so there are 8 $\times 9=72$ steps in total). This might be compensated by using more number of simple cases (e.g., 24 simple cases, each with 3 steps, can also make 72 steps in total). These factors are shown in the upper part of Table 8. (2). when per-case step increases, the full length of the prompt (= number of characters) also increases, which may be compensated by longer (more characters) but less step examples. These factors are shown in the lower part of Table 8. From the accuracy results we can see that: (1). keeping full number of reasoning steps the same, using more number of simple cases does not outperform less number of complex cases; (2). longest prompts does not outperform complex prompts. (3). yet we do need a moderate per-step length because keeping total number of step 72, moderate per-step length prompts outperforms shorter per-step length prompts. This means that despite the existence of confounders, the number of reasoning steps per example is the most prominent factor for performance gain given moderate per-step length.
Voting among Complex Chains Outperforms Voting among All Now we analyze the properties of complexity-based consistency, which generalizes the reasoning complexity selection criteria from the input space (prompts) to the output space (sampled solutions from the language model). Complexity-based consistency first sample $N$ reasoning chains from the model, then take the majority answer voted from the top $K$ complex chains. Here we set $N=50$, and control $K=10,20,30,40,50$. Note that when $K=50$ we recover the original self-consistency (no complexity-based selection). As a further comparison, we consider the other way around: instead of voting over top $K$ complex samples, we vote over top $K$ simple samples. As is shown in Fig. 6, we see: (1). voting over simple samples always underperform full sample; , indicating this is not a</p>
<p>Table 8: Confounder analysis. Although there exist confounders like number of cases or total prompt length, the number of reasoning step is the most prominent factor for performance gain given moderate per-step length.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">More number of simple cases v.s. less but complex cases</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MathQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Total reasoning step</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;">Number of cases in prompt</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Per-case reasoning step</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">5.625</td>
</tr>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: center;">Most number of reasoning steps v.s. longest prompt</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MathQA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Number of cases in prompt</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Length of prompt</td>
<td style="text-align: center;">12.6 k</td>
<td style="text-align: center;">8.4 k</td>
<td style="text-align: center;">7.6 k</td>
<td style="text-align: center;">4.9 k</td>
</tr>
<tr>
<td style="text-align: center;">Number of total reasoning step</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;">Per-step length</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">137</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: center;">Shorter per-step length v.s. Longer per-step length</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MathQA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Number of total reasoning step</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;">Number of cases in prompt</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Per-step length</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">42.5</td>
</tr>
</tbody>
</table>
<p>correct direction for performance; (2). both datasets achieve the best performance on some $K^{\star}&lt;N$ with complex voting. These results again, validate the choice of complex samples.</p>
<h1>5 CONCLUSION</h1>
<p>This paper proposes a new complexity-based instance selection scheme for prompting language models to perform multi-step reasoning. In addition to substantial performance improvements on math word reasoning tasks, our methods exhibit multiple advantages such as being intuitive, annotation-efficient, and robustly effective in different in-context learning settings. We hope this work will open new research possibilities in prompting, language models, and multi-step reasoning.</p>
<h2>REFERENCES</h2>
<p>Ekin Akyurek and Afra Feyza Akyurek. Notes on teaching gpt-3 adding numbers, 2022. URL https://lingo.csail.mit.edu//blog/arithmetic_gpt3.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357-2367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology. org/N19-1245.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Jianpeng Cheng, Siva Reddy, Vijay Saraswat, and Mirella Lapata. Learning an Executable Neural Semantic Parser. Computational Linguistics, 45(1):59-94, 03 2019. ISSN 0891-2017. doi: 10.1162/coli_a_00342. URL https://doi.org/10.1162/coli_a_00342.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Yuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, and Dongyan Zhao. Why machine reading comprehension models learn shortcuts? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 989-1002, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.85. URL https:// aclanthology.org/2021.findings-acl.85.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, 2021.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.</p>
<p>Percy Liang. Learning executable semantic parsers for natural language understanding. Communications of the ACM, 59(9):68-76, 2016.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100-114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https: //aclanthology.org/2022.deelio-1.10.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8086-8098, 2022.</p>
<p>Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamdhere. Did the model understand the question? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1896-1906, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1176. URL https: //aclanthology.org/P18-1176.</p>
<p>PromptBase. Promptbase. https://promptbase.com.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 3982-3992, 2019.</p>
<p>Kyle Richardson and Ashish Sabharwal. Pushing the limits of rule reasoning in transformers through natural language satisfiability. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 11209-11219, 2022.</p>
<p>Frieda Rong. Extrapolating to unnatural language processing with gpt-3's in-context learning: The good, the bad, and the mysterious. http://ai.stanford.edu/blog/ in-context-learning/.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/ D15-1202. URL https://aclanthology.org/D15-1202.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655-2671, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.naacl-main.191. URL https://aclanthology.org/2022.naacl-main.191.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222-4235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020. emnlp-main. 346 .</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022.</p>
<p>Saku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4208-4219, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1453. URL https:// aclanthology.org/D18-1453.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2153-2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL https:// aclanthology.org/D19-1221.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. Advances in Neural Information Processing Systems, 34:16158-16170, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI.</p>
<p>Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 754-765, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/ P18-1070. URL https://aclanthology.org/P18-1070.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12697-12706. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/zhao21c.html.</p>
<h1>A APPENDIX</h1>
<p>You may include other additional sections here.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\text {a }}$ Work done during internship at Allen Institute for AI&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
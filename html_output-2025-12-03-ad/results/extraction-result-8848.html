<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8848 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8848</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8848</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a1d1983a7b19845141e6505bd32dc395e5a136ba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a1d1983a7b19845141e6505bd32dc395e5a136ba" target="_blank">Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper argues that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.</p>
                <p><strong>Paper Abstract:</strong> In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of"grokking"a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8848.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8848.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grokking Transformer (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoder-only transformer trained on small algorithmic binary-operation datasets (GROKKING)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small decoder-only transformer (2 layers, width 128, 4 attention heads, ~4e5 non-embedding parameters) trained on datasets of equations a ∘ b = c (tokens: <a><op><b><=> <c>) for tasks such as modular arithmetic and S5 group composition; displays 'grokking' where validation generalization appears long after perfect training memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decoder-only transformer (2-layer, width 128, 4 heads)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard decoder-only transformer with causal masking; loss/accuracy computed only on the answer token. Final experiments used ~4e5 non-embedding parameters; trained primarily with AdamW (lr=1e-3, weight decay=1) with minibatch size up to 512 and various ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~4e5 non-embedding parameters (embeddings not included)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Small algorithmic binary-operation table completion (modular arithmetic and abstract group operations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict c in equations of form ⟨x⟩⟨op⟩⟨y⟩⟨=⟩⟨x ∘ y⟩ where operands and results are abstract tokens with no internal numeric notation; operations include x+y (mod p), x-y, x/y (mod p), various polynomial forms mod p, conditional ops, and permutation composition in S5. Tasks require learning symbolic/algorithmic structure (group/ring operations) from interaction patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Train small transformer on explicit equation tokens; vary optimization/regulaization: Adam vs AdamW, full-batch vs minibatch, weight decay (towards origin and towards init), gradient noise, Gaussian weight noise, residual dropout, learning-rate tuning; measure learning-time vs dataset fraction; visualize embeddings (t-SNE); measure sharpness of minima.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Training accuracy typically reaches ~100% within 10^3–10^4 updates. Validation accuracy often remains at chance for a long period and then can rise to ~99–100% very late (up to 10^6 updates in some experiments) — the 'grokking' phenomenon. Best-validation-after-1e5-updates varies by operation; weight decay substantially improves data efficiency. Sharpness (φ) correlates with validation accuracy (Spearman ρ ≈ -0.79548, p < 1.4e-5) on S5 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ablations: AdamW with weight decay yields substantially better data efficiency than Adam without weight decay; weight decay toward origin outperforms weight decay toward initialization; adding gradient/noise (minibatches, Gaussian update noise, Gaussian weight noise) helps generalization; full-batch Adam is worse than minibatch variants with noise; learning rate must be tuned within ~1 order of magnitude for grokking to appear.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Some operations (e.g. x^3 + x y^2 + y (mod p) in experiments) did not generalize within optimization budget at any training fraction up to 95% — models memorized training data without finding pattern. Grokking requires large amounts of optimization especially for smaller training fractions; introducing many outliers (random labels) reduces the range of data fractions where generalization occurs; learning-rate sensitivity is high (narrow window).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Small algorithmic datasets are a useful, reproducible testbed for studying late-emerging generalization. Regularization (especially weight decay) and optimization noise appear to favor flatter minima and strongly improve data efficiency and the chance of late generalization; symmetry in operations reduces data required. Grokking seems correlated with finding flatter regions of loss landscape and can occur far after interpolation of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8848.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8848.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>bAbI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards AI-complete question answering: A set of prerequisite toy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of algorithmically generated toy question-answering tasks designed to probe reasoning primitives (memory, deduction, chaining); cited as an example of algorithmically generated reasoning datasets encouraging study of generalization in data-limited regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards ai-complete question answering: A set of prerequisite toy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>bAbI tasks (toy QA reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Algorithmically generated QA tasks that probe elementary reasoning skills (e.g., chaining, deduction, coreference); used as benchmarks for models on symbolic reasoning/generalization in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes that most work on bAbI focuses on point estimates of performance for particular architectures/techniques rather than detailed dynamics of generalization (e.g., late generalization phenomena).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>bAbI-style algorithmic datasets are relevant to studying generalization but prior literature often does not analyze training-generalization dynamics over long optimization trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8848.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8848.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Saxton et al. (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysing mathematical reasoning abilities of neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of procedurally generated mathematical problems (arithmetic, differentiation, etc.) used to analyze neural models' mathematical reasoning; cited as related work but noted to be more complex and sample-hungry than the small binary-op tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Analysing mathematical reasoning abilities of neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Procedurally generated mathematical reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks include multi-step arithmetic and calculus problems generated procedurally; require algorithmic/mathematical reasoning beyond simple binary operations and typically need many samples to master.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Authors of the current paper note these tasks are more involved and often require extremely large numbers of samples, making them ill-suited to observe late generalization effects in small-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Such tasks examine mathematical reasoning abilities but differ in scale and complexity from the small algorithmic binary-op datasets studied for grokking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8848.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8848.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural GPUs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural gpus learn algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that used algorithmic sequence tasks (copying, reversing, sorting, arithmetic) to probe neural architectures' algorithmic learning abilities; cited as part of the algorithmic-dataset literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural gpus learn algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural GPU</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional/GPU-friendly recurrent-like architecture designed to learn algorithms from examples (as described in the cited work); referenced here as prior algorithmic-task work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Algorithmic sequence tasks (copying, reversing, sorting, multi-digit arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic algorithmic tasks that test the ability to learn and generalize algorithmic procedures (often measured across input lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>These algorithmic benchmarks typically focus on generalization with respect to input length or unlimited-data regimes rather than the small-data, long-optimization dynamics emphasized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8848.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8848.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neelakantan et al. (gradient noise)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adding gradient noise improves learning for very deep networks.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper demonstrating that adding noise to gradients can improve optimization for deep models; cited here because it showed a 'grok-like' learning curve on an algorithmic task, though attributed to optimization difficulty rather than late generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adding gradient noise improves learning for very deep networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Algorithmic task used in original Neelakantan et al. experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>An algorithmic learning task where optimization difficulty produced late improvements in training metrics; mentioned as related but distinct from grokking phenomenon studied here.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Adding Gaussian noise to gradients / update directions during optimization</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Paper argued gradient noise helps optimization; current paper notes gradient noise is beneficial for generalization in their setting as well.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>The 'grok-like' curve in Neelakantan et al. is tied to optimization difficulty rather than an observed late rise in validation/generalization distinct from memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Optimization noise can aid finding flatter minima and improve generalization/optimization behavior; consistent with the current paper's observation that noise (minibatching, Gaussian noise) helps grokking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Analysing mathematical reasoning abilities of neural models. <em>(Rating: 2)</em></li>
                <li>Towards ai-complete question answering: A set of prerequisite toy tasks <em>(Rating: 2)</em></li>
                <li>Neural gpus learn algorithms. <em>(Rating: 2)</em></li>
                <li>Adding gradient noise improves learning for very deep networks. <em>(Rating: 1)</em></li>
                <li>Neural turing machines. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8848",
    "paper_id": "paper-a1d1983a7b19845141e6505bd32dc395e5a136ba",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "Grokking Transformer (this work)",
            "name_full": "Decoder-only transformer trained on small algorithmic binary-operation datasets (GROKKING)",
            "brief_description": "A small decoder-only transformer (2 layers, width 128, 4 attention heads, ~4e5 non-embedding parameters) trained on datasets of equations a ∘ b = c (tokens: &lt;a&gt;&lt;op&gt;&lt;b&gt;&lt;=&gt; &lt;c&gt;) for tasks such as modular arithmetic and S5 group composition; displays 'grokking' where validation generalization appears long after perfect training memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Decoder-only transformer (2-layer, width 128, 4 heads)",
            "model_description": "Standard decoder-only transformer with causal masking; loss/accuracy computed only on the answer token. Final experiments used ~4e5 non-embedding parameters; trained primarily with AdamW (lr=1e-3, weight decay=1) with minibatch size up to 512 and various ablations.",
            "model_size": "~4e5 non-embedding parameters (embeddings not included)",
            "reasoning_task_name": "Small algorithmic binary-operation table completion (modular arithmetic and abstract group operations)",
            "reasoning_task_description": "Predict c in equations of form ⟨x⟩⟨op⟩⟨y⟩⟨=⟩⟨x ∘ y⟩ where operands and results are abstract tokens with no internal numeric notation; operations include x+y (mod p), x-y, x/y (mod p), various polynomial forms mod p, conditional ops, and permutation composition in S5. Tasks require learning symbolic/algorithmic structure (group/ring operations) from interaction patterns.",
            "method_or_approach": "Train small transformer on explicit equation tokens; vary optimization/regulaization: Adam vs AdamW, full-batch vs minibatch, weight decay (towards origin and towards init), gradient noise, Gaussian weight noise, residual dropout, learning-rate tuning; measure learning-time vs dataset fraction; visualize embeddings (t-SNE); measure sharpness of minima.",
            "performance": "Training accuracy typically reaches ~100% within 10^3–10^4 updates. Validation accuracy often remains at chance for a long period and then can rise to ~99–100% very late (up to 10^6 updates in some experiments) — the 'grokking' phenomenon. Best-validation-after-1e5-updates varies by operation; weight decay substantially improves data efficiency. Sharpness (φ) correlates with validation accuracy (Spearman ρ ≈ -0.79548, p &lt; 1.4e-5) on S5 experiments.",
            "baseline_comparison": "Ablations: AdamW with weight decay yields substantially better data efficiency than Adam without weight decay; weight decay toward origin outperforms weight decay toward initialization; adding gradient/noise (minibatches, Gaussian update noise, Gaussian weight noise) helps generalization; full-batch Adam is worse than minibatch variants with noise; learning rate must be tuned within ~1 order of magnitude for grokking to appear.",
            "limitations_or_failures": "Some operations (e.g. x^3 + x y^2 + y (mod p) in experiments) did not generalize within optimization budget at any training fraction up to 95% — models memorized training data without finding pattern. Grokking requires large amounts of optimization especially for smaller training fractions; introducing many outliers (random labels) reduces the range of data fractions where generalization occurs; learning-rate sensitivity is high (narrow window).",
            "insights_or_conclusions": "Small algorithmic datasets are a useful, reproducible testbed for studying late-emerging generalization. Regularization (especially weight decay) and optimization noise appear to favor flatter minima and strongly improve data efficiency and the chance of late generalization; symmetry in operations reduces data required. Grokking seems correlated with finding flatter regions of loss landscape and can occur far after interpolation of training data.",
            "uuid": "e8848.0",
            "source_info": {
                "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "bAbI",
            "name_full": "Towards AI-complete question answering: A set of prerequisite toy tasks",
            "brief_description": "A suite of algorithmically generated toy question-answering tasks designed to probe reasoning primitives (memory, deduction, chaining); cited as an example of algorithmically generated reasoning datasets encouraging study of generalization in data-limited regimes.",
            "citation_title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "bAbI tasks (toy QA reasoning tasks)",
            "reasoning_task_description": "Algorithmically generated QA tasks that probe elementary reasoning skills (e.g., chaining, deduction, coreference); used as benchmarks for models on symbolic reasoning/generalization in low-data regimes.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Paper notes that most work on bAbI focuses on point estimates of performance for particular architectures/techniques rather than detailed dynamics of generalization (e.g., late generalization phenomena).",
            "insights_or_conclusions": "bAbI-style algorithmic datasets are relevant to studying generalization but prior literature often does not analyze training-generalization dynamics over long optimization trajectories.",
            "uuid": "e8848.1",
            "source_info": {
                "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Saxton et al. (math reasoning)",
            "name_full": "Analysing mathematical reasoning abilities of neural models.",
            "brief_description": "Study of procedurally generated mathematical problems (arithmetic, differentiation, etc.) used to analyze neural models' mathematical reasoning; cited as related work but noted to be more complex and sample-hungry than the small binary-op tasks in this paper.",
            "citation_title": "Analysing mathematical reasoning abilities of neural models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "Procedurally generated mathematical reasoning tasks",
            "reasoning_task_description": "Tasks include multi-step arithmetic and calculus problems generated procedurally; require algorithmic/mathematical reasoning beyond simple binary operations and typically need many samples to master.",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "Authors of the current paper note these tasks are more involved and often require extremely large numbers of samples, making them ill-suited to observe late generalization effects in small-data regimes.",
            "insights_or_conclusions": "Such tasks examine mathematical reasoning abilities but differ in scale and complexity from the small algorithmic binary-op datasets studied for grokking.",
            "uuid": "e8848.2",
            "source_info": {
                "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Neural GPUs",
            "name_full": "Neural gpus learn algorithms.",
            "brief_description": "Prior work that used algorithmic sequence tasks (copying, reversing, sorting, arithmetic) to probe neural architectures' algorithmic learning abilities; cited as part of the algorithmic-dataset literature.",
            "citation_title": "Neural gpus learn algorithms.",
            "mention_or_use": "mention",
            "model_name": "Neural GPU",
            "model_description": "Convolutional/GPU-friendly recurrent-like architecture designed to learn algorithms from examples (as described in the cited work); referenced here as prior algorithmic-task work.",
            "model_size": null,
            "reasoning_task_name": "Algorithmic sequence tasks (copying, reversing, sorting, multi-digit arithmetic)",
            "reasoning_task_description": "Synthetic algorithmic tasks that test the ability to learn and generalize algorithmic procedures (often measured across input lengths).",
            "method_or_approach": null,
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": null,
            "insights_or_conclusions": "These algorithmic benchmarks typically focus on generalization with respect to input length or unlimited-data regimes rather than the small-data, long-optimization dynamics emphasized in this paper.",
            "uuid": "e8848.3",
            "source_info": {
                "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Neelakantan et al. (gradient noise)",
            "name_full": "Adding gradient noise improves learning for very deep networks.",
            "brief_description": "Paper demonstrating that adding noise to gradients can improve optimization for deep models; cited here because it showed a 'grok-like' learning curve on an algorithmic task, though attributed to optimization difficulty rather than late generalization.",
            "citation_title": "Adding gradient noise improves learning for very deep networks.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "Algorithmic task used in original Neelakantan et al. experiments",
            "reasoning_task_description": "An algorithmic learning task where optimization difficulty produced late improvements in training metrics; mentioned as related but distinct from grokking phenomenon studied here.",
            "method_or_approach": "Adding Gaussian noise to gradients / update directions during optimization",
            "performance": null,
            "baseline_comparison": "Paper argued gradient noise helps optimization; current paper notes gradient noise is beneficial for generalization in their setting as well.",
            "limitations_or_failures": "The 'grok-like' curve in Neelakantan et al. is tied to optimization difficulty rather than an observed late rise in validation/generalization distinct from memorization.",
            "insights_or_conclusions": "Optimization noise can aid finding flatter minima and improve generalization/optimization behavior; consistent with the current paper's observation that noise (minibatching, Gaussian noise) helps grokking.",
            "uuid": "e8848.4",
            "source_info": {
                "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models.",
            "rating": 2
        },
        {
            "paper_title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "rating": 2
        },
        {
            "paper_title": "Neural gpus learn algorithms.",
            "rating": 2
        },
        {
            "paper_title": "Adding gradient noise improves learning for very deep networks.",
            "rating": 1
        },
        {
            "paper_title": "Neural turing machines.",
            "rating": 1
        }
    ],
    "cost": 0.01204775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GROKKING: GENERALIZATION BEYOND OVERFITTING ON SMALL ALGORITHMIC DATASETS</h1>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin<br>OpenAI<br>Vedant Misra*<br>Google</p>
<h4>Abstract</h4>
<p>In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.</p>
<h2>1 INTRODUCTION</h2>
<p>The generalization of overparameterized neural networks has long been a source of interest to the machine learning community since it defies intuitions derived from classical learning theory. In this paper we show that training networks on small algorithmically generated datasets can reliably exhibit unusual generalization patterns, clearly decoupled from performance on the training set, in a significantly more pronounced way than such effects manifest on datasets derived from natural data (see Figure 1, left, for an example). Such experiments can be quickly reproduced on a single GPU, and this makes them convenient testbeds for theories of generalization.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left. Grokking: A dramatic example of generalization far after overfitting on an algorithmic dataset. We train on the binary operation of division mod 97 with $50 \%$ of the data in the training set. Each of the 97 residues is presented to the network as a separate symbol, similar to the representation in the figure to the right. The red curves show training accuracy and the green ones show validation accuracy. Training accuracy becomes close to perfect at $&lt;10^{3}$ optimization steps, but it takes close to $10^{6}$ steps for validation accuracy to reach that level, and we see very little evidence of any generalization until $10^{5}$ steps. Center. Training time required to reach 99\% validation accuracy increases rapidly as the training data fraction decreases. Right. An example of a small binary operation table. We invite the reader to make their guesses as to which elements are missing.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The datasets we consider are binary operation tables of the form $a \circ b=c$ where $a, b, c$ are discrete symbols with no internal structure, and o is a binary operation. Examples of binary operations include addition, composition of permutations, and bivariate polynomials. Training a neural network on a proper subset of all possible equations then amounts to filling in the blanks of the binary op table, much like solving a Sudoku puzzle. An example is shown on the right in Figure 1. Since we use distinct abstract symbols for all distinct elements $a, b, c$ involved in the equations, the network is not made aware of any internal structure of the elements, and has to learn about their properties only from their interactions with other elements. For example the network doesn't see numbers in decimal notation, or permutations in line notation.</p>
<p>Our contributions are as follows:</p>
<ul>
<li>We show that neural networks are capable of generalizing to the empty slots in a variety of binary op tables.</li>
<li>We show that, long after severely overfitting, validation accuracy sometimes suddenly begins to increase from chance level toward perfect generalization. We call this phenomenon 'grokking'. An example is shown in Figure 1.</li>
<li>We present the data efficiency curves for a variety of binary operations.</li>
<li>We show empirically that the amount of optimization required for generalization quickly increases as the dataset size decreases.</li>
<li>We compare various optimization details to measure their impact on data efficiency. We find that weight decay is particularly effective at improving generalization on the tasks we study.</li>
<li>We visualize the symbol embeddings learned by these networks and find that they sometimes uncover recognizable structure of the mathematical objects represented by the symbols.</li>
</ul>
<h1>2 Method</h1>
<p>All of our experiments used a small transformer trained on datasets of equations of the form $a \circ b=c$, where each of " $a$ ", " $\circ$ ", " $b$ ", " $=$ ", and " $c$ " is a separate token. Details of the operations studied, the architecture, training hyperparameters and tokenization can be found in Appendix A.1.</p>
<h2>3 EXPERIMENTS</h2>
<h3>3.1 GENERALIZATION BEYOND OVERFITTING</h3>
<p>Deep learning practitioners are used to seeing small improvements in validation accuracy after validation loss stops decreasing. A double descent of validation loss has been documented in some circumstances, but is considered unusual among practitioners Nakkiran et al. (2019); Belkin et al. (2018); d'Ascoli et al. (2020). On the small algorithmic datasets that we study, improved generalization after initial overfitting occurs for a range of models, optimizers, and dataset sizes, and in some cases these effects are extremely pronounced. A typical example is shown for modular division in Figure 1. There we see that validation accuracy starts increasing beyond chance level only after 1000 times more optimization steps than are required for training accuracy to get close to optimal. In Figure 4 the training/validation losses are also plotted and we see the double descent of the validation loss.</p>
<p>We found these behaviors to be typical for all the binary operations for dataset sizes that were close to the minimal dataset size for which the network generalized within the allotted optimization budget. For larger dataset sizes, the training and validation curves tend to track each other more closely.</p>
<h3>3.1.1 LEARNING TIME CURVES</h3>
<p>In a typical supervised learning problem, decreasing the amount of training data decreases the converged generalization performance of the model when the optimization procedure is capable of interpolating the training data. In our setting, we observe a different phenomenon: while the converged performance stays constant at $100 \%$ within a range of training dataset sizes, the optimization time required to achieve that performance grows quicky as the dataset size is decreased.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left. Different optimization algorithms lead to different amounts of generalization within an optimization budget of $10^{5}$ steps for the problem of learning the product in the abstract group $S_{5}$. Weight decay improves generalization the most, but some generalization happens even with full batch optimizers and models without weight or activation noise at high percentages of training data. Suboptimal choice hyperparameters severely limit generalization. Not shown: training accuracy reaches 100% after $10^{3}$–$10^{4}$ updates for all optimization methods. Right. Best validation accuracy achieved after $10^{5}$ steps on a variety of algorithmic datasets, averaged over 3 seeds. Generalization happens at higher percentages of data for intuitively more complicated and less symmetrical operations.</p>
<p>Figure 1 (center) shows median number of optimization steps until validation performance first reaches 99% for the product in abstract group $S_{5}$. In the vicinity of 25–30% of data, a decrease of 1% of training data leads to an increase of 40–50% in median time to generalization. While the number of steps until validation accuracy &gt; 99% grows quickly as dataset size decreases, the number of steps until the train accuracy first reaches 99% generally trends down as dataset size decreases and stays in the range of $10^{3}$–$10^{4}$ optimization steps. We've observed a similar pattern of exponential increase in optimization time until reaching generalization as dataset size decreases on all the algorithmic tasks for which we could get the networks to generalize.</p>
<h3>3.2 GROKKING ON A VARIETY OF PROBLEMS</h3>
<p>We've measured the mean accuracy across three runs for training datasets consisting of different fractions of all available equations for a variety of binary operations listed in Appendix A.1.1. The results are presented in Figure 2 (right).</p>
<p>Since the operands are presented to the neural network as unrelated abstract symbols, the operations $x+y\ (\bmod p - 1)$ and $x * y\ (\bmod p)$ with a prime number $p$ and non-zero $x, y$ are indistinguishable from the neural network's perspective (and similarly $x - y\ (\bmod p - 1)$ and $x / y\ (\bmod p)$). This is because every nonzero residue modulo a prime can be represented as a power of a primitive root. This representation shows the equivalence (up to renaming of symbols) of modular addition modulo $p - 1$ and modular multiplication modulo $p$. We see in Figure 2 (right) that $x - y$ and $x / y$ indeed take about the same amount of data for generalization to occur.</p>
<p>Some of the operations listed in Figure 2 (right) are symmetric with respect to the order of the operands ($x + y, x * y, x^2 + y^2$ and $x^2 + xy + y^2$). Such operations tend to require less data for generalization than closely related non-symmetrical counterparts ($x - y, x / y, x^2 + xy + y^2 + x$). We believe this effect might be partially architecture-dependent, since it's easy for a transformer to learn a symmetric function of the operands by ignoring positional embedding.</p>
<p>Some operations (for example $x^3 + xy^2 + y (\bmod 97)$) didn't lead to generalization within the allowed optimization budget at any percentage of data up to 95%. The converged models effectively just memorized the training dataset without finding any real patterns in the data. To such a model, the data is effectively random.</p>
<p>The operation $[x / y (\bmod p)$ if $y$ is odd, otherwise $x - y (\bmod p)$] requires the network to learn a mix of several simple operations - in particular the role of $x$ has to be interpreted as a residue in the additive group when it's paired with an even $y$, and as a residue in the multiplicative group when it's paired with an odd $y$. This shows that generalization can happen even for operations that are not cleanly interpretable via group or ring operations.</p>
<h3>3.3 Ablations and Tricks</h3>
<p>We've tried various forms of regularization to see what can induce networks to generalize better on our datasets. Here we present the data efficiency curves on a particular dataset $S_{5}$ for a variety of interventions: full-batch gradient descent, stochastic gradient descent, large or small learning rates,</p>
<p>residual dropout <em>Srivastava et al. (2014)</em>, weight decay <em>Loshchilov &amp; Hutter (2017)</em> and gradient noise <em>Neelakantan et al. (2015)</em>. The results are shown in Figure 2 (left).</p>
<p>We find that adding weight decay has a very large effect on data efficiency, more than halving the amount of samples needed compared to most other interventions. We found that weight decay towards the initialization of the network is also effective, but not quite as effective as weight decay towards the origin. This makes us believe that the prior, that approximately zero weights are suitable for small algorithmic tasks, explains part, but not all of the superior performance of weight decay. Adding some noise to the optimization process (e.g. gradient noise from using minibatches, Gaussian noise applied to weights before or after computing the gradients) is beneficial for generalization, consistent with the idea that such noise might induce the optimization to find flatter minima that generalize better. We found that learning rate had to be tuned in a relatively narrow window for the generalization to happen (within 1 order of magnitude).</p>
<h3>3.4 Qualitative Visualization of Embeddings</h3>
<p>In order to gain some insight into networks that generalize, we visualized the matrix of the output layer for the case of modular addition and $S_{5}$. In Figure 3 we show t-SNE plots of the row vectors. For some networks we find clear reflections of the structure of the underlying mathematical objects in the plots. For example the circular topology of modular addition is shown with a ‘number line’ formed by adding 8 to each element. The structure is more apparent in networks that were optimized with weight decay.</p>
<h2>4 Discussion</h2>
<p>We have seen that in the datasets we studied, small algorithmic binary operation tables, effects such as double descent or late generalization, and improvements to generalization from interventions like weight decay can be striking. This suggests that these datasets could be a good place to investigate aspects of generalization. For example, we plan to test whether various proposed measures of minima flatness correlate with generalization in our setting.</p>
<p>We have also seen that visualizing the embedding spaces of these neural networks can show natural kinds of structure, for example in problems of modular arithmetic the topology of the embeddings tends to be circles or cylinders. We also see that the network tends to idiosyncratically organize the embeddings by various residues. Whilst the properties of these mathematical objects are familiar to us, we speculate that such visualizations could one day be a useful way to gain intuitions about novel mathematical objects.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left. t-SNE projection of the output layer weights from a network trained on $S_{5}$. We see clusters of permutations, and each cluster is a coset of the subgroup $\langle(0,3)(1,4),(1,2)(3,4)\rangle$ or one of its conjugates. Right. t-SNE projection of the output layer weights from a network trained on modular addition. The lines show the result of adding 8 to each element. The colors show the residue of each element modulo 8.</p>
<p>In addition, we document an interesting phenomenon, where the number of optimization steps needed to reach a given level of performance increases quickly as we reduce the size of the training dataset. Since this represents a way trade compute for performance on smaller amounts of data, it would be useful to investigate in future work whether the effect is also present for other datasets.</p>
<h1>REFERENCES</h1>
<p>Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning practice and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.</p>
<p>Stéphane d'Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting: Where \&amp; why do they appear? arXiv preprint arXiv:2006.03509, 2020.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. Advances in neural information processing systems, 28: $1828-1836,2015$.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation, 9(1):1-42, 1997.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</p>
<p>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016. URL http://arxiv.org/abs/1609.04836.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.</p>
<p>Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019.</p>
<p>Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.</p>
<p>Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</p>
<p>Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines-revised. arXiv preprint arXiv:1505.00521, 2015.</p>
<p>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 ADDITIONAL EXPERIMENTAL DETAILS</h2>
<h2>A.1.1 BinaRY OPERATIONS</h2>
<p>The following are the binary operations that we have tried (for a prime number $p=97$ ):</p>
<p>$$
\begin{aligned}
&amp; x \circ y=x+y(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x-y(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x / y(\bmod p) \text { for } 0 \leq x&lt;p, 0&lt;y&lt;p \
&amp; x \circ y=[x / y(\bmod p) \text { if } y \text { is odd, otherwise } x-y(\bmod p)] \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x^{2}+y^{2}(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x^{2}+x y+y^{2}(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x^{2}+x y+y^{2}+x(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x^{3}+x y(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x^{3}+x y^{2}+y(\bmod p) \text { for } 0 \leq x, y&lt;p \
&amp; x \circ y=x \cdot y \text { for } x, y \in S_{5} \
&amp; x \circ y=x \cdot y \cdot x^{-1} \text { for } x, y \in S_{5} \
&amp; x \circ y=x \cdot y \cdot x \text { for } x, y \in S_{5}
\end{aligned}
$$</p>
<p>For each binary operation we constructed a dataset of equations of the form $\langle x\rangle\langle o p\rangle\langle y\rangle\langle=\rangle\langle x \circ y\rangle$, where $\langle a\rangle$ stands for the token corresponding to element $a$.</p>
<p>For each training run, we chose a fraction of all available equations at random and declared them to be the training set, with the rest of equations being the validation set.</p>
<h2>A.1.2 MODEL AND OPTIMIZATION</h2>
<p>We trained a standard decoder-only transformer Vaswani et al. (2017) with causal attention masking, and calculated loss and accuracy only on the answer part of the equation. For all experiments we used a transformer with 2 layers, width 128, and 4 attention heads, with a total of about $4 \cdot 10^{5}$ non-embedding parameters.
We have tuned optimization hyperparameters by running experiments on modular addition and product in $S_{5}$. For final configuration of hyperparameters we have chosen a balance of performance we saw on $S_{5}$ and simplicity (for example we chose not to anneal the learning rate for the experiments in the paper even though it performed better in some situations). For most experiments we used AdamW optimizer with learning rate $10^{-3}$, weight decay $1, \beta_{1}=0.9, \beta_{2}=0.98$, linear learning rate warmup over the first 10 updates, minibatch size 512 or half of training dataset size (whichever was smaller) and optimization budget of $10^{5}$ gradient updates.
In section 3.3 we have also tried the following variants (listed in the reading order for Figure 2 left):</p>
<ul>
<li>Adam optimizer with full batch (i.e. exact gradient of the loss on the whole training dataset)</li>
<li>
<p>Adam optimizer</p>
</li>
<li>
<p>Adam optimizer with full batch and Gaussian noise added to the update direction for each parameter $(W \leftarrow W+\operatorname{lr} \cdot(\Delta W+\epsilon)$, where $\epsilon$ is sampled from unit Gaussian, $\Delta W$ is the standard Adam weight update, and lr is the learning rate)</p>
</li>
<li>Adam optimizer on model with residual dropout 0.1 added</li>
<li>AdamW optimizer with weight decay 1 (default setting in most other experiments)</li>
<li>AdamW optimizer with weight decay 1 towards the initialization instead of the origin</li>
<li>Adam optimizer with learning rate $3 \cdot 10^{-4}$</li>
<li>Adam optimizer with learning rate $3 \cdot 10^{-3}$</li>
<li>Adam optimizer on model with Gaussian weight noise of standard deviation 0.01 (i.e. each parameter $W$ replaced by $W+0.01 \cdot \epsilon$ in the model, with $\epsilon$ sampled from unit Gaussian).</li>
</ul>
<p>For experiments reported in Section 3.1.1 we increased the optimization budget to $5 \cdot 10^{5}$ optimization steps in order to capture the increase of time to perfect generalization better.</p>
<p>For the experiments reported in Section 3.1 we increased the optimization budget to $10^{6}$, and used Adam optimizer with no weight decay, for emphasizing how late into the optimization process the generalization can begin.</p>
<p>We've repeated each experiment for each dataset size with 3 random seeds, with the exception of experiments in section 3.1.1, where we've aggregated results over 7 random seeds.</p>
<h1>A. 2 Additional Figures</h1>
<p>In Figure 4 we show the loss curves that correspond to the accuracy curves in Figure 1.
In Figure 5 we show an example of a binary operation table that the network can actually solve.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The loss curves for modular division, train and validation. We see the validation loss increases from $10^{2}$ to about $10^{5}$ optimization steps before it begins a second descent.</p>
<h2>A. 3 Related Work</h2>
<p>In this paper we study training and generalization dynamics on small simple algorithmic datasets. In the past, algorithmic datasets have been used to probe the capability of neural networks to perform symbolic and algorithmic reasoning. For example the tasks of copying, reversing, and sorting randomly generated sequences, and performing arithmetic operations of multi-digit numbers, have been used as standard benchmarks for sequence-to-sequence models Graves et al. (2014), Weston et al. (2014) Kaiser \&amp; Sutskever (2015) Reed \&amp; De Freitas (2015), Grefenstette et al. (2015), Zaremba \&amp; Sutskever (2015), Graves (2016), Dehghani et al. (2018). Typically in these works however the emphasis is on the performance in the unlimited data regime, with generalization often studied with respect to input sequence length. Some papers study the sample complexity on algorithmic tasks Reed \&amp; De Freitas (2015), but mostly focus on the impact of architectural choices. In contrast we study the phenomenon of generalization in data-limited regime, with an emphasis on phenomena that we believe to be architecture-agnostic.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: One of the binary operation tables presented to the networks that the network can perfectly fill in. Each symbol is represented as a letter in English, Hebrew, or Greek alphabet for reader's convenience. We invite the reader to guess which operation is represented here.</p>
<p>Algorithmically generated reasoning datasets like bAbI Weston et al. (2015) encourage work on studying generalization in data-limited regime. Most results on such datasets however focus on a point estimate of performance of a particular architecture or training technique, whereas our main interest is in pointing out the change in generalization past the point where a particular architecture can memorize the training data completely.
Neelakantan et al. (2015) has a "grok-like" learning curve on an algorithmic task, but it is related to optimization difficulty, whereas our phenomenon is specifically about generalization.
In Saxton et al. (2019) they study generalization on procedurally generated math problems such as arithmetic and differentiation, but for the most part these tasks are more involved than the simple binary op problems we have studied and as such do not lend themselves to observing the kinds of phenomena we describe in this paper, since they would require an extremely large number of samples to master.</p>
<p>In Jiang et al. (2019) they studied a large number of generalization or complexity measures on convolutional neural networks to see which, if any, are predictive of generalization performance. They find that flatness based measures that aim to quantify the sensitivity of the trained neural network to parameter perturbations are the most predictive. We conjectured that the grokking phenomena</p>
<p>we report in this work may be due to the noise from SGD driving the optimization to flatter/simpler solutions that generalize better and hope to investigate in future work whether any of these measures are predictive of grokking.
Zhang et al. (2016) finds that neural networks of sizes typically used in deep learning can interpolate arbitrary training data, and yet generalize when trained with semantically meaningful labels using appropriate optimization procedures. Our work shows a related phenomenon where neural networks can interpolate a small algorithmic training dataset without generalizing, but start generalizing when trained with SGD for longer.
Nakkiran et al. (2019); Belkin et al. (2018) focus on the phenomenon of double descent in loss as a function of model and optimization procedure capacity. They find that the classical U-shaped validation loss curve is followed in some settings (including neural network training) by a second descent of loss that starts around the minimal capacity that is needed to interpolate any training data. We observe a second descent in validation loss (though not accuracy) as a function of the amount of training in some of our experiments, and it happens past the point of interpolating the training data. We believe that the phenomenon we describe might be distinct from the double descent phenomena described in Nakkiran et al. (2019); Belkin et al. (2018) because we observe the second descent in loss far past the first time the training loss becomes very small (tens of thousands of epochs in some of our experiments), and we don't observe a non-monotonic behavior of accuracy. The setting of small algorithmic datasets that we study also provides a smaller, more tractable playground for studying subtle generalization phenomena than natural datasets studied in Nakkiran et al. (2019).</p>
<h1>A. 4 GENERALIZATION WITH MEMORIZING SEVERAL OUTLIERS</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Effect on data efficiency of introducing $k \in[0,10,100,1000,2000,3000]$ outliers (examples with random labels) into the training data. Small number of outliers doesn't noticeably impact generalization performance, but a large number hinders it significantly.</p>
<p>In this section we show data efficiency curves for a modified version of a binary op by introducing $k$ outliers to the training dataset. More precisely, at the beginning of the experiment we randomly sample $k$ equations from the training set and replace their answers with answers to other $k$ equations randomly sampled from the training data. The rest of the equations in the training data and all the equations in the validation data are kept as before.
In this situation one could imagine one of the following scenarios unfolding. If the model class of neural networks optimized and regularized as before was not large enough to interpolate such "noisy" dataset, one could imagine the procedure converging to a solution that generalizes well, but denoises the training data (i.e. predicts $c=a \circ b$ as an answer even for the outlier equations $a, b \rightarrow c^{\prime}$ with $c^{\prime} \neq c$ ). On the other extreme it could be that the optimization procedure can find networks that interpolate the data, but the resulting models don't generalize, because they are forced to represent a considerably more complicated function than before (a simple function $+k$ exceptions encoded in the training data).
In our experiments we find that the first option doesn't happen - all experiments reach 100\% training accuracy at some point, and this point is not considerably affected by changing the number of outliers $k$. The second phenomenon happens in a range of training data percentages and number of outliers $k$ - increasing $k$ decreases the range of training data percentages for which the optimization procedure converges to models that generalize. However the effect of introducing a small number of outliers (up to 1000) is not very pronounced - see Figure 6. We interpret this as additional evidence that the capacity of the network and optimization procedure is well beyond the capacity needed for</p>
<p>memorizing all the labels on the training data, and that generalization happening at all requires a non-trivial explanation.</p>
<h1>A. 5 GENERALIZATION MEASURES</h1>
<p>We believe it is useful to explore how predictive common generalization measures are of generalization on small algorithmic datasets presented in this paper. In a preliminary investigation we found that sharpness Hochreiter \&amp; Schmidhuber (1997) of the minimum found by a trained network measure seems to be predictive of generalization on one of these datasets. We trained multiple networks with different initialization seeds for a fixed number a steps on the $S 5$ composition objective, until approximately half of them achieved high validation accuracy. We then used the method described in Keskar et al. (2016) to calculate the sharpness approximation value, $\phi$. We found that the validation accuracy and the $\phi$ score across our trained networks had Spearman correlation coefficient of -0.79548 (significant with $p&lt;0.000014$ ). This is suggestive that grokking may only happen after the network's parameters are in flatter regions of the loss landscape. It would be valuable for future work to explore this hypothesis, as well as test other generalization measures.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Networks trained on the $S 5$ composition objective appear to only grok in relatively flat regions of the loss landscape.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Vedant was at OpenAI at the time of this work&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
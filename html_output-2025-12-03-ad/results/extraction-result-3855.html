<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3855 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3855</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3855</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-270738074</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.18403v1.pdf" target="_blank">LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</a></p>
                <p><strong>Paper Abstract:</strong> There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide J UDGE -B ENCH , an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3855.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3855.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Overall variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overall human-LLM agreement variability across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds substantial variance in agreement between LLM judges and human judgments across models, datasets, and judged properties, with no single LLM reliably matching humans across all evaluation dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge compared to aggregated human judgments across 20 datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Multiple (translation, summarization, dialogue, toxicity, reasoning, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Substantial variance; many model scores remain notably below the human upper bound across most datasets (exceptions on a few datasets like QAGS, Recipe-generation, NewsRoom). Exact per-dataset metrics reported in paper (Spearman ρ for graded, Cohen's κ for categorical) but no single high global agreement value.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Agreement strength depends on property; different LLMs better capture different quality dimensions (e.g., some better at acceptability/verbosity, others at coherence/consistency). No uniform sensitivity across dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large inconsistency across datasets and properties makes LLMs unreliable as general-purpose replacements for human judges; performance varies with data source (human vs. machine-generated) and annotator expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Some datasets show high alignment (e.g., instruction-following and some summarization/reasoning items), indicating situational strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Validate and calibrate LLM judges against task-specific human annotations before deployment; do not assume a single LLM will generalize across properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3855.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-specific reliability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tasks where LLM judges are reliable (instruction following, reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study reports that for certain tasks — notably instruction following and generation of mathematical reasoning traces — current LLMs can be used reliably as evaluators, showing good alignment with human judgments on those properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-judge vs. human annotations on instruction-following and reasoning-trace datasets (e.g., LLMBar, ROSCOE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Instruction following; multi-step mathematical reasoning traces</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported higher agreement on these tasks relative to many others; exact per-dataset Spearman/Cohen values are reported in paper tables (no single scalar provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>On these tasks LLMs mirror human judgments more closely, likely because evaluation criteria align with model-internal representations (e.g., following instructions or checking step correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even for these tasks, agreement can vary across specific datasets and prompting strategies; not all models or splits show uniform performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Strength: LLMs often rank among top-performing evaluators for these tasks, sometimes approaching human upper bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>LLM judges may be deployed for these tasks after task-specific validation; still advisable to check against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3855.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert vs Non-expert gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Higher LLM agreement with non-expert than expert human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All models achieved higher correlations with annotations by non-expert human judges than with expert annotators on graded annotations, suggesting LLMs align more with surface-level judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Comparison of LLM correlations with expert vs. non-expert human annotations (graded datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Multiple graded annotation datasets (e.g., SummEval, NewsRoom, recipe grading)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Qualitative statement: correlations higher with non-experts; specific Spearman values for datasets provided in the paper but not summarized as a single figure here.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>LLMs appear to rely on surface-level features that non-experts may also use, while experts apply stricter, domain-specific criteria that LLMs less often match.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This suggests LLMs may miss fine-grained, expert-level evaluation criteria and may overfit to superficial signals.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Strength: better consistency with non-expert crowdsourced judgments, which might be sufficient for some large-scale evaluation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When replacing non-expert human evaluations, LLMs may be more suitable than when replacing expert annotations; still perform validation with target annotator type.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3855.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human vs machine text bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differential alignment on human-generated vs. machine-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds all tested LLMs align better with human judgments when evaluating human-written text than when assessing machine-generated outputs, indicating systematic differences in how LLMs judge human vs. model generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Comparison of LLM-humans agreement on items involving human language vs. machine-generated outputs (categorical and graded)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Broad across datasets with human and machine-generated items</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Qualitative: consistent improvement in alignment on human language items versus machine-generated items; per-dataset metric values in paper figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>LLMs are relatively less reliable at evaluating machine-generated outputs, possibly due to biases toward their own generation patterns or difficulty detecting model-specific errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Using LLMs to evaluate system outputs (i.e., other models) can lead to distorted evaluation results; prior work has also reported positive bias toward machine-generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Exercise caution when using LLMs to evaluate outputs of other models; validate on model-generated data and check for generation-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3855.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toxicity & safety failures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Poor alignment and guardrail-induced failures on toxicity and safety evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs show low or even negative alignment with human judgments on toxicity/safety datasets (DICES, Medical-safety), with low valid response rates and a tendency to refuse, produce explanations, or choose 'Unsure'/'Unsafe' labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM judgments vs. human safety/toxicity annotations on DICES, ToxicChat, Medical-safety</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Toxicity and safety assessment for prompts and conversational responses</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported as low and sometimes negative correlations; valid response rates particularly low for some models/datasets (details in Appendix F and G of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Models often avoid making direct judgments due to guardrails, provide explanations instead, or prefer conservative labels (e.g., 'Unsure'), diverging from human annotators' more decisive labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Guardrails and refusal behavior distort comparisons; models' safety conservatism and label preferences reduce utility as direct replacements for human safety annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>ToxicChat performance was reasonable in contrast to other safety datasets, indicating dataset-dependent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not rely solely on off-the-shelf LLM judges for safety/toxicity tasks; consider special handling for guardrails, or keep human-in-the-loop for critical safety annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3855.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT ineffectiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting does not reliably improve LLM-human agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Elicitation methods such as Chain-of-Thought (CoT) prompting produced inconsistent improvements and do not systematically increase agreement between LLM judges and human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Comparison of standard prompting vs. CoT prompting for LLM judges across multiple datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Various (reasoning, translation, safety, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>CoT sometimes improved agreement for particular model-dataset pairs (reported in Appendix H/Table 7) but overall did not yield consistent gains across the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>CoT can help on specific reasoning/math/symbolic tasks but offers limited or inconsistent benefit for many annotated properties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT increases computation and response verbosity and does not guarantee better alignment; benefits appear task- and model-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>On a few datasets/models CoT improved agreement scores, particularly in reasoning contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not assume CoT will improve evaluator alignment; empirically test CoT on the target task before adopting it broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3855.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Valid response handling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of invalid/guardrailed LLM responses and response-rate imputation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Because models sometimes do not respond as requested (refusals, explanations), the study replaced invalid LLM responses with randomly sampled human annotations to maintain comparable judgment counts, a protocol choice that affects evaluation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Experimental protocol handling invalid/guardrailed LLM outputs across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>All datasets where LLM responses can be invalid (esp. safety/medical)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>The paper reports per-model valid response rates (Figure 5, Appendix F); some models had <100% valid responses and replacement influenced aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Invalid responses (refusals/explanations) break the direct comparability of LLM labels to human judgments and can artificially lower measured agreement or increase variance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Imputing random human labels for invalid LLM outputs introduces noise and can bias correlation estimates; guardrail behaviour therefore complicates using LLMs as drop-in human surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report valid-response rates and treat guardrail-induced refusals carefully; consider alternative protocols (e.g., excluding refused items or special prompting) and always disclose imputation effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3855.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Upper-bound comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison to human upper bound via bootstrapped inter-annotator agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors estimate an upper bound for model-human correlation based on average agreement between single human raters and aggregated human responses, and find that model performance often remains below these upper bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Computation of human upper bounds (bootstrapped single-rater vs aggregated responses) and comparison to LLM correlations</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Graded and categorical annotation datasets with multiple human annotations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Upper bounds computed per property using bootstrapping (Spearman ρ for graded, Cohen's κ for categorical); many model scores are below these estimated upper bounds (exceptions exist).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Quantifies room for improvement: models not only differ from humans but often fail to reach inter-human agreement levels that bound achievable correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Upper-bound estimates are themselves noisy and subject to sampling error; in some cases model scores may exceed these estimated bounds due to estimation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Some LLMs approach or occasionally exceed the estimate due to estimation variance, but generally performance is lower than the human-derived ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use human inter-annotator agreement as a benchmark when evaluating LLM judges to contextualize achievable alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3855.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Engagingness gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistently low correlation on 'engagingness' property</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All LLMs showed consistently low correlation with human judgments on the engagingness attribute in dialogue datasets, indicating difficulty capturing conversational appeal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM judgment vs human annotations on engagingness in dialogue datasets (TopicalChat, PersonaChat)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Dialogue engagingness evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported as consistently low across all models (detailed Spearman values are in Figure 3 / Appendix tables).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>LLMs are less sensitive to what humans rate as 'engaging' (subjective qualities like interestingness), possibly because such judgments rely on human preferences and world knowledge not captured by surface metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Using LLMs to judge subjective conversational qualities can miss human-centric signals important for engagement.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Retain human evaluation for subjective properties like engagingness or use diverse panels/ensembles of models and calibration to better approximate human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3855.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3855.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Practical recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recommendations for using LLMs as judges (validation, calibration, reproducibility caution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors recommend careful validation and calibration of LLM judges against task-specific human annotations before deployment and warn about reproducibility concerns for proprietary models that may be retrained or retired.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>General best-practices distilled from empirical findings across the benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>General across NLP evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Proprietary models risk reproducibility (retraining/retirement); LLMs can share or amplify human biases; guardrails affect safety annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Open-source large models (e.g., Llama-3.1-70B, Mixtral-8x22B) can approach proprietary models (GPT-4o) in some tasks, offering reproducible alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Validate LLM judges with task-specific human data; prefer task-appropriate models; disclose valid-response rates and imputation; combine human and model judgments where necessary; be cautious with closed-source models for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following. <em>(Rating: 2)</em></li>
                <li>Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models. <em>(Rating: 2)</em></li>
                <li>Is ChatGPT a good NLG evaluator? a preliminary study <em>(Rating: 1)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 2)</em></li>
                <li>JudgeBench: A benchmark for evaluating LLM-Automated evaluation of written discourse coherence <em>(Rating: 1)</em></li>
                <li>JudgeLM: Fine-tuned Large Language Models are Scalable Judges <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3855",
    "paper_id": "paper-270738074",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "Overall variability",
            "name_full": "Overall human-LLM agreement variability across tasks",
            "brief_description": "The paper finds substantial variance in agreement between LLM judges and human judgments across models, datasets, and judged properties, with no single LLM reliably matching humans across all evaluation dimensions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge compared to aggregated human judgments across 20 datasets",
            "task_or_domain": "Multiple (translation, summarization, dialogue, toxicity, reasoning, etc.)",
            "llm_model_name": null,
            "agreement_rate": "Substantial variance; many model scores remain notably below the human upper bound across most datasets (exceptions on a few datasets like QAGS, Recipe-generation, NewsRoom). Exact per-dataset metrics reported in paper (Spearman ρ for graded, Cohen's κ for categorical) but no single high global agreement value.",
            "qualitative_differences": "Agreement strength depends on property; different LLMs better capture different quality dimensions (e.g., some better at acceptability/verbosity, others at coherence/consistency). No uniform sensitivity across dimensions.",
            "limitations_or_failure_cases": "Large inconsistency across datasets and properties makes LLMs unreliable as general-purpose replacements for human judges; performance varies with data source (human vs. machine-generated) and annotator expertise.",
            "counterexamples_or_strengths": "Some datasets show high alignment (e.g., instruction-following and some summarization/reasoning items), indicating situational strengths.",
            "recommendations_or_best_practices": "Validate and calibrate LLM judges against task-specific human annotations before deployment; do not assume a single LLM will generalize across properties.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.0",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Task-specific reliability",
            "name_full": "Tasks where LLM judges are reliable (instruction following, reasoning traces)",
            "brief_description": "The study reports that for certain tasks — notably instruction following and generation of mathematical reasoning traces — current LLMs can be used reliably as evaluators, showing good alignment with human judgments on those properties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-judge vs. human annotations on instruction-following and reasoning-trace datasets (e.g., LLMBar, ROSCOE)",
            "task_or_domain": "Instruction following; multi-step mathematical reasoning traces",
            "llm_model_name": null,
            "agreement_rate": "Reported higher agreement on these tasks relative to many others; exact per-dataset Spearman/Cohen values are reported in paper tables (no single scalar provided here).",
            "qualitative_differences": "On these tasks LLMs mirror human judgments more closely, likely because evaluation criteria align with model-internal representations (e.g., following instructions or checking step correctness).",
            "limitations_or_failure_cases": "Even for these tasks, agreement can vary across specific datasets and prompting strategies; not all models or splits show uniform performance.",
            "counterexamples_or_strengths": "Strength: LLMs often rank among top-performing evaluators for these tasks, sometimes approaching human upper bounds.",
            "recommendations_or_best_practices": "LLM judges may be deployed for these tasks after task-specific validation; still advisable to check against human annotations.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.1",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Expert vs Non-expert gap",
            "name_full": "Higher LLM agreement with non-expert than expert human annotators",
            "brief_description": "All models achieved higher correlations with annotations by non-expert human judges than with expert annotators on graded annotations, suggesting LLMs align more with surface-level judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Comparison of LLM correlations with expert vs. non-expert human annotations (graded datasets)",
            "task_or_domain": "Multiple graded annotation datasets (e.g., SummEval, NewsRoom, recipe grading)",
            "llm_model_name": null,
            "agreement_rate": "Qualitative statement: correlations higher with non-experts; specific Spearman values for datasets provided in the paper but not summarized as a single figure here.",
            "qualitative_differences": "LLMs appear to rely on surface-level features that non-experts may also use, while experts apply stricter, domain-specific criteria that LLMs less often match.",
            "limitations_or_failure_cases": "This suggests LLMs may miss fine-grained, expert-level evaluation criteria and may overfit to superficial signals.",
            "counterexamples_or_strengths": "Strength: better consistency with non-expert crowdsourced judgments, which might be sufficient for some large-scale evaluation needs.",
            "recommendations_or_best_practices": "When replacing non-expert human evaluations, LLMs may be more suitable than when replacing expert annotations; still perform validation with target annotator type.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.2",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Human vs machine text bias",
            "name_full": "Differential alignment on human-generated vs. machine-generated text",
            "brief_description": "The paper finds all tested LLMs align better with human judgments when evaluating human-written text than when assessing machine-generated outputs, indicating systematic differences in how LLMs judge human vs. model generations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Comparison of LLM-humans agreement on items involving human language vs. machine-generated outputs (categorical and graded)",
            "task_or_domain": "Broad across datasets with human and machine-generated items",
            "llm_model_name": null,
            "agreement_rate": "Qualitative: consistent improvement in alignment on human language items versus machine-generated items; per-dataset metric values in paper figures/tables.",
            "qualitative_differences": "LLMs are relatively less reliable at evaluating machine-generated outputs, possibly due to biases toward their own generation patterns or difficulty detecting model-specific errors.",
            "limitations_or_failure_cases": "Using LLMs to evaluate system outputs (i.e., other models) can lead to distorted evaluation results; prior work has also reported positive bias toward machine-generated outputs.",
            "counterexamples_or_strengths": null,
            "recommendations_or_best_practices": "Exercise caution when using LLMs to evaluate outputs of other models; validate on model-generated data and check for generation-specific biases.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.3",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Toxicity & safety failures",
            "name_full": "Poor alignment and guardrail-induced failures on toxicity and safety evaluations",
            "brief_description": "LLMs show low or even negative alignment with human judgments on toxicity/safety datasets (DICES, Medical-safety), with low valid response rates and a tendency to refuse, produce explanations, or choose 'Unsure'/'Unsafe' labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM judgments vs. human safety/toxicity annotations on DICES, ToxicChat, Medical-safety",
            "task_or_domain": "Toxicity and safety assessment for prompts and conversational responses",
            "llm_model_name": null,
            "agreement_rate": "Reported as low and sometimes negative correlations; valid response rates particularly low for some models/datasets (details in Appendix F and G of paper).",
            "qualitative_differences": "Models often avoid making direct judgments due to guardrails, provide explanations instead, or prefer conservative labels (e.g., 'Unsure'), diverging from human annotators' more decisive labels.",
            "limitations_or_failure_cases": "Guardrails and refusal behavior distort comparisons; models' safety conservatism and label preferences reduce utility as direct replacements for human safety annotation.",
            "counterexamples_or_strengths": "ToxicChat performance was reasonable in contrast to other safety datasets, indicating dataset-dependent behavior.",
            "recommendations_or_best_practices": "Do not rely solely on off-the-shelf LLM judges for safety/toxicity tasks; consider special handling for guardrails, or keep human-in-the-loop for critical safety annotations.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.4",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoT ineffectiveness",
            "name_full": "Chain-of-Thought prompting does not reliably improve LLM-human agreement",
            "brief_description": "Elicitation methods such as Chain-of-Thought (CoT) prompting produced inconsistent improvements and do not systematically increase agreement between LLM judges and human annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Comparison of standard prompting vs. CoT prompting for LLM judges across multiple datasets",
            "task_or_domain": "Various (reasoning, translation, safety, etc.)",
            "llm_model_name": null,
            "agreement_rate": "CoT sometimes improved agreement for particular model-dataset pairs (reported in Appendix H/Table 7) but overall did not yield consistent gains across the benchmark.",
            "qualitative_differences": "CoT can help on specific reasoning/math/symbolic tasks but offers limited or inconsistent benefit for many annotated properties.",
            "limitations_or_failure_cases": "CoT increases computation and response verbosity and does not guarantee better alignment; benefits appear task- and model-specific.",
            "counterexamples_or_strengths": "On a few datasets/models CoT improved agreement scores, particularly in reasoning contexts.",
            "recommendations_or_best_practices": "Do not assume CoT will improve evaluator alignment; empirically test CoT on the target task before adopting it broadly.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.5",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Valid response handling",
            "name_full": "Impact of invalid/guardrailed LLM responses and response-rate imputation",
            "brief_description": "Because models sometimes do not respond as requested (refusals, explanations), the study replaced invalid LLM responses with randomly sampled human annotations to maintain comparable judgment counts, a protocol choice that affects evaluation outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Experimental protocol handling invalid/guardrailed LLM outputs across datasets",
            "task_or_domain": "All datasets where LLM responses can be invalid (esp. safety/medical)",
            "llm_model_name": null,
            "agreement_rate": "The paper reports per-model valid response rates (Figure 5, Appendix F); some models had &lt;100% valid responses and replacement influenced aggregate metrics.",
            "qualitative_differences": "Invalid responses (refusals/explanations) break the direct comparability of LLM labels to human judgments and can artificially lower measured agreement or increase variance.",
            "limitations_or_failure_cases": "Imputing random human labels for invalid LLM outputs introduces noise and can bias correlation estimates; guardrail behaviour therefore complicates using LLMs as drop-in human surrogates.",
            "counterexamples_or_strengths": null,
            "recommendations_or_best_practices": "Report valid-response rates and treat guardrail-induced refusals carefully; consider alternative protocols (e.g., excluding refused items or special prompting) and always disclose imputation effects.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.6",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Upper-bound comparison",
            "name_full": "Comparison to human upper bound via bootstrapped inter-annotator agreement",
            "brief_description": "The authors estimate an upper bound for model-human correlation based on average agreement between single human raters and aggregated human responses, and find that model performance often remains below these upper bounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Computation of human upper bounds (bootstrapped single-rater vs aggregated responses) and comparison to LLM correlations",
            "task_or_domain": "Graded and categorical annotation datasets with multiple human annotations",
            "llm_model_name": null,
            "agreement_rate": "Upper bounds computed per property using bootstrapping (Spearman ρ for graded, Cohen's κ for categorical); many model scores are below these estimated upper bounds (exceptions exist).",
            "qualitative_differences": "Quantifies room for improvement: models not only differ from humans but often fail to reach inter-human agreement levels that bound achievable correlation.",
            "limitations_or_failure_cases": "Upper-bound estimates are themselves noisy and subject to sampling error; in some cases model scores may exceed these estimated bounds due to estimation noise.",
            "counterexamples_or_strengths": "Some LLMs approach or occasionally exceed the estimate due to estimation variance, but generally performance is lower than the human-derived ceiling.",
            "recommendations_or_best_practices": "Use human inter-annotator agreement as a benchmark when evaluating LLM judges to contextualize achievable alignment.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.7",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Engagingness gap",
            "name_full": "Consistently low correlation on 'engagingness' property",
            "brief_description": "All LLMs showed consistently low correlation with human judgments on the engagingness attribute in dialogue datasets, indicating difficulty capturing conversational appeal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM judgment vs human annotations on engagingness in dialogue datasets (TopicalChat, PersonaChat)",
            "task_or_domain": "Dialogue engagingness evaluation",
            "llm_model_name": null,
            "agreement_rate": "Reported as consistently low across all models (detailed Spearman values are in Figure 3 / Appendix tables).",
            "qualitative_differences": "LLMs are less sensitive to what humans rate as 'engaging' (subjective qualities like interestingness), possibly because such judgments rely on human preferences and world knowledge not captured by surface metrics.",
            "limitations_or_failure_cases": "Using LLMs to judge subjective conversational qualities can miss human-centric signals important for engagement.",
            "counterexamples_or_strengths": null,
            "recommendations_or_best_practices": "Retain human evaluation for subjective properties like engagingness or use diverse panels/ensembles of models and calibration to better approximate human preferences.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.8",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Practical recommendations",
            "name_full": "Recommendations for using LLMs as judges (validation, calibration, reproducibility caution)",
            "brief_description": "The authors recommend careful validation and calibration of LLM judges against task-specific human annotations before deployment and warn about reproducibility concerns for proprietary models that may be retrained or retired.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "General best-practices distilled from empirical findings across the benchmark",
            "task_or_domain": "General across NLP evaluation tasks",
            "llm_model_name": null,
            "agreement_rate": null,
            "qualitative_differences": null,
            "limitations_or_failure_cases": "Proprietary models risk reproducibility (retraining/retirement); LLMs can share or amplify human biases; guardrails affect safety annotations.",
            "counterexamples_or_strengths": "Open-source large models (e.g., Llama-3.1-70B, Mixtral-8x22B) can approach proprietary models (GPT-4o) in some tasks, offering reproducible alternatives.",
            "recommendations_or_best_practices": "Validate LLM judges with task-specific human data; prefer task-appropriate models; disclose valid-response rates and imputation; combine human and model judgments where necessary; be cautious with closed-source models for reproducibility.",
            "citation": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
            "uuid": "e3855.9",
            "source_info": {
                "paper_title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following.",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_at_evaluating_instruction_following"
        },
        {
            "paper_title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models.",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "Is ChatGPT a good NLG evaluator? a preliminary study",
            "rating": 1,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 2,
            "sanitized_title": "benchmarking_cognitive_biases_in_large_language_models_as_evaluators"
        },
        {
            "paper_title": "JudgeBench: A benchmark for evaluating LLM-Automated evaluation of written discourse coherence",
            "rating": 1,
            "sanitized_title": "judgebench_a_benchmark_for_evaluating_llmautomated_evaluation_of_written_discourse_coherence"
        },
        {
            "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "rating": 1,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        }
    ],
    "cost": 0.014803499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks
2 Jun 2025</p>
<p>Anna Bavaresco 
University of Amsterdam</p>
<p>Raffaella Bernardi 
University of Trento</p>
<p>Leonardo Bertolazzi 
University of Trento</p>
<p>Desmond Elliott 
University of Copenhagen</p>
<p>Raquel Fernández 
University of Amsterdam</p>
<p>Albert Gatt 
Utrecht University</p>
<p>Esam Ghaleb 
Max Planck Institute for Psycholinguistics
6 ETH Zürich</p>
<p>Mario Giulianelli 
Michael Hanna 
University of Amsterdam</p>
<p>Alexander Koller 
Saarland University</p>
<p>André F T Martins 
Universidade de Lisboa &amp; Unbabel</p>
<p>Philipp Mondorf 
LMU Munich &amp; MCML</p>
<p>Vera Neplenbroek 
University of Amsterdam</p>
<p>Sandro Pezzelle 
University of Amsterdam</p>
<p>Barbara Plank 
LMU Munich &amp; MCML</p>
<p>David Schlangen 
University of Potsdam
11 Heriot-Watt University 12 Amsterdam UMC</p>
<p>Alessandro Suglia 
Aditya K Surikuchi 
University of Amsterdam</p>
<p>Ece Takmaz 
Utrecht University</p>
<p>Alberto Testoni 
LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks
2 Jun 202549AEFCC66998A3BD206D03A19D5A24EEarXiv:2406.18403v3[cs.CL]
There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models.We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations.Our evaluations show substantial variance across models and datasets.Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated.We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.</p>
<p>Introduction</p>
<p>For many natural language processing (NLP) tasks, the most informative evaluation is to ask humans to judge the model output.Such judgments are traditionally collected in lab experiments or through crowdsourcing, with either expert or non-expert annotators, as illustrated in Fig. 1.Recently, there has been a trend towards replacing human judgments with automatic assessments obtained via large language models (LLMs; Chiang and Lee, 2023;Wang et al., 2023;Liu et al., 2023;Li et al., 2024;Zheng et al., 2024, inter alia).For example, the LLM * Authors listed in alphabetical order.</p>
<p>Instruction: On a scale of 1 (very unlikely) to 5 (very likely), how plausible is it that the last response belongs to the dialogue?</p>
<p>A: Made it all the way through four years of college playing ball but B: I also like The Cosby Show  could be instructed to rate a response generated by a dialogue system for its perceived plausibility, on a scale from 1 to 5.This drastically reduces the evaluation effort and is claimed to yield more reliable results across multiple evaluation rounds (Landwehr et al., 2023;Jiang et al., 2023b;Reiter, 2024;Dubois et al., 2024).</p>
<p>At the same time, the use of LLMs as judges of linguistic output raises new concerns: LLMs may be prone to errors or systematic biases that differ from those of humans, especially on subtle tasks such as evaluating toxicity, or reasoning.This may distort evaluation results and lead to incorrect conclusions.The problem is aggravated by explicit or implicit data leakage (Balloccu et al., 2024), which undermines the ability to make broad, generalisable claims beyond the single specific dataset under analysis.Specifically for closed models such as OpenAI's GPT series, there are serious reproducibility concerns, as LLMs may be retrained or retired at any time, making subsequent comparisons invalid or impossible.</p>
<p>Previous studies offer mixed evidence regarding the reliability of LLM evaluators.Some research concludes that they are effective, correlating well with human judgments (Liu et al., 2023;Zheng et al., 2024;Chen et al., 2023;Verga et al., 2024;Törnberg, 2023;Huang et al., 2024;Naismith et al., 2023;Gilardi et al., 2023;Kocmi and Federmann, 2023b), albeit with some caveats (Wang et al., 2023;Wu and Aji, 2025;Hada et al., 2024;Pavlovic and Poesio, 2024).In some cases, LLM evaluators can also provide pairwise preference judgments (Kim et al., 2024;Liusie et al., 2024;Liu et al., 2024a;Park et al., 2024;Tan et al., 2025), or fine-grained evaluation beyond a single score, such as error spans (Fernandes et al., 2023;Kocmi and Federmann, 2023a).In contrast, some studies highlight substantial biases in LLMs' behaviour as evaluators, both as compared against human judgments (Koo et al., 2024;Zeng et al., 2024;Baris Schlicht et al., 2024) and through intrinsic analyses (Wang et al., 2024;Liu et al., 2024b;Stureborg et al., 2024).These discrepancies likely stem from the limitations of this previous work, which typically relies on a few datasets and models, often restricted to closed-source proprietary models.The observation of such limitations has motivated recent work to develop finetuning methods for LLM judges designed to overcome certain biases (Zhu et al., 2025).</p>
<p>In this paper, we examine how well current LLMs can approximate human evaluators on a large scale.We prompt 11 among the most recent open-weight and proprietary LLMs to generate judgments on 20 datasets with human annotations on a wide range of quality dimensions, prompt styles, and tasks.Our evaluation goes beyond existing work by including a wide variety of datasets that differ in the type of task (e.g., translation, dialogue generation, etc.), the property being judged (e.g., coherence, fluency, etc.), the type of judgments (categorical or graded), and the expertise of human annotators (experts or non-experts).We provide JUDGE-BENCH, a benchmark which includes, upon release, a total of over 70,000 test instances with associated human judgments with an extensible codebase.</p>
<p>Our results indicate that LLMs align well with human judgments on certain tasks, like instruction following.However, their performance is inconsistent across and within annotation tasks.Elicitation methods like Chain-of-Thought prompting (Wei et al., 2022) do not reliably improve agreement, in line with recent findings (Sprague et al., 2025).Some proprietary models-in particular, GPT-4o-align better to humans, but there is a rather small gap with large open-source models, holding promise for the reproducibility of future evaluation efforts.Altogether, at the current stage of LLM development, we recommend validating LLM judges against task-specific human annotations before deploying them for any particular task.</p>
<p>Construction of JUDGE-BENCH</p>
<p>One key feature that differs across the datasets included in JUDGE-BENCH is the source of the data being evaluated, i.e., whether the items to be judged are generated by a model or produced by humans, as illustrated in Figure 1.</p>
<p>For model-generated items, the goal is to evaluate an NLP system.This includes both classic tasks such as machine translation or dialogue response generation, as well as less standard tasks for which automation has recently become an option thanks to LLMs, such as the generation of plans or logical arguments.For human-generated items, the goal is to assess properties of interest such as grammaticality or toxicity.This distinction allows us to understand whether LLMs have a positive bias towards machine-generated outputs-a tendency reported in prior work (Xu et al., 2024).</p>
<p>The datasets we consider cover a wide span of properties of interest, ranging from grammaticality and toxicity to coherence, factual consistency, and verbosity, inter alia.Many properties are relevant across multiple tasks (e.g., fluency and coherence), while others are more task-specific (e.g., the success of a generated plan or the correctness of a multi-step mathematical reasoning trace).</p>
<p>Our study focuses on English datasets or language pairs which include English as one of the languages.We keep track of whether the original annotation guidelines are available and whether the annotations are provided by experts or nonexperts.We retain all available individual annotations.Dataset information is summarised in Table 2, Appendix A. All 20 datasets are formatted following a precise data schema to facilitate the integration of additional datasets.This makes JUDGE-BENCH easily extensible.We provide more details about the data schema in Appendix B.</p>
<p>Dataset (# properties judged)</p>
<p>GPT-4o</p>
<p>Llama-3.1-70B Mixtral-8x22B Gemini-1.(Groeneveld et al., 2024), Starling-7B (Zhu et al., 2024), and Mistral (Jiang et al., 2023a).See Appendix E for inference procedure details.</p>
<p>Prompts.Since most datasets include the original instructions used to gather human judgments, we use these instructions directly as prompts for the model, with additional guidelines to constrain the models' output and minimise verbosity: 'Answer with one of {}.Do not explain your answer.'When the original instruction for collecting human judgments is unavailable, we create a prompt based on relevant information from the original paper, such as the task description and the definitions of the evaluation metrics.We also experiment with alternative prompting strategies, including Chainof-Thought, few-shot prompts, and prompt paraphrases.However, none of these strategies leads to systematic improvements.See Appendix H for full details and results.All prompts are provided in the codebase.</p>
<p>Evaluation.Models do not always respond to the prompts as requested (e.g., they may refuse to
O L M o -7 B S t a r l i n g -7 B L l a m a -3 . 1 -8 B C o m m -R 4 C o m m -R + M i s t r a l -7 B M i x t r a l -8 x 7 B L l a m a -3 . 1 -7 0 B G e m i n i -1 . 5 M i x t</p>
<p>Spearman Correlation</p>
<p>Comm answer if they perceive the prompt as sensitive).
-R+ Mixtral-8x22B Llama-3.1-70B Mixtral-8x7B Gemini-1.5 Llama-3.1-8B GPT-4o
We therefore use the following evaluation protocol:</p>
<p>• To obtain the same number of judgments across models for a given dataset, we replace invalid LLM responses with judgments randomly sampled from the relevant set of categorical or graded annotations.Figure 5 in Appendix F shows the rate of valid responses per model.• Graded annotations, such as in WMT 2020 (Freitag et al., 2021), assess language quality on a continuous scale (e.g., a score from 0 to 100, or Likert-scale ratings), capturing varying degrees of fluency, adequacy, or overall translation quality; whereas categorical annotations, like those in CoLa (Warstadt et al., 2019), involve binary judgments (e.g., grammatically acceptable or not).</p>
<p>For the former, we compute Spearman's correlation (ρ) between model and human judgments; for the latter, we compute Cohen's κ.</p>
<p>• When multiple individual human judgments are available (typically three, see Table 2 in Appendix A), we estimate an upper bound by computing the average Spearman's ρ or Cohen's κ between bootstrapped single-rater responses and the aggregated responses across raters.Appendix C provides details on the upper bounds.</p>
<p>Results</p>
<p>Scores vary substantially across models.For any given model, they vary both across datasets and properties being judged.Table 1 presents detailed results for the 6 models that exhibit the largest rate of valid responses (≥98%).GPT-4o ranks first across several evaluation scenarios, but the Llama-3.1-70B and Mixtral-8x22B open models are relatively close and outperform GPT-4o on some assessment types, such as categorical sentence acceptability (CoLa) and graded summary quality (SummEval).Overall, the high degree of variability is not fully accounted for by the inherent difficulty of the annotation tasks, as reflected in the human upper bound.Moreover, except for a few datasets (e.g., QAGS, Recipe-generation, and NewsRoom), model scores remain notably below the upper bound.Among the property types with the lowest human-model alignment are toxicity and safety (in particular on DICES and Medical-safety), where model scores can be even negative and valid response rates particularly low (see Fig. 6 in Appendix F).This is due in part to the guardrails associated with these tasks (Weidinger et al., 2023).We find that, especially in the medical domain, many models tend to provide explanations instead of producing a judgment (see Appendix G).</p>
<p>Despite the high variability across models and datasets, we observe several notable trends.For graded annotations (Fig. 2), all models achieve higher correlations with annotations by non-expert human judges compared to expert annotators, echoing recent findings by Aguda et al. (2024).One possible explanation is that non-experts might rely on surface-level features, which could align more closely with the patterns LLMs are most attuned to, while experts apply stricter, domain-specific criteria.This remains speculative and calls for further investigation.</p>
<p>Figure 3 shows correlation results across different datasets for the subset of properties that exclusively have graded judgments.When applicable, we average results across datasets including annotations for the same property.We provide more details about these properties in Appendix D. The proprietary models GPT-4o and Gemini-1.5 exhibit the highest scores when evaluating acceptability and verbosity, while the two Mixtral open models show the strongest correlations for coherence and consistency.Correlation with the engagingness property remains consistently low across all models.Overall, no single model demonstrates a clear superiority over others across all properties; instead, different quality dimensions are better assessed by different models.This calls into question the widespread practice of using a single modeltypically a proprietary one like those from the GPT family-to evaluate a diverse range of linguistic properties.
O L M o -7 B S t a r li n g -7 B C o m m -R 4 M is t r a l-7 B C o m m -R + M ix t
Finally, as shown in Figure 4, all models achieve better alignment with human judgments when evaluating human language than when assessing machine-generated text, both for categorical and graded annotations.This result aligns with the findings by Xu et al. (2024), suggesting that LLMs display a bias towards their own generation.More broadly, this trend calls for caution when using LLMs to automatically evaluate the output of NLP systems.</p>
<p>Conclusions</p>
<p>In response to current trends in evaluation, in this paper we conducted a large-scale study of the correlation between human and LLM judgments across 20 datasets, considering factors such as the properties being assessed, the expertise level of the human judges, and whether the data is model-or human-generated.On some tasks, such as instruc-tion following and the generation of mathematical reasoning traces, models can be reliably used as evaluators.Overall, however, models' agreement with human judgments varies widely across datasets, evaluated properties, and data sources; and depends on the level of expertise of human judges.Furthermore, elicitation strategies such as Chain-of-Thought prompting do not consistently improve agreement levels, in line with recent findings (Sprague et al., 2025).We recommend validation and calibration of LLMs against task-specific human judgments prior to their deployment as evaluators.To facilitate this process, we release JUDGE-BENCH, a benchmark that enables systematic evaluation across a diverse range of tasks and is easily extensible to include any new task of interest.</p>
<p>Limitations</p>
<p>One limitation of the experimental design of our work is that correlation with human judges may not be the most appropriate way to validate LLM evaluators.Indeed, there may be domains where human annotators and LLM evaluators appear aligned simply because they are affected by similar biases.Therefore, depending on the task at hand, it may be necessary to validate the reliability of human annotators as well.</p>
<p>Another limitation concerns the use of existing tasks and datasets without reassessing their quality or representativeness of actual downstream tasks.While we did our best to select a wide set of tasks meaningful to the NLP community, we acknowledge that these tasks could not be equally meaningful for end-users, and that employing existing datasets could arguably lead to potential risks and shortcomings, such as data leakage.</p>
<p>In contrast to approaches that use LLMs for pairwise preference evaluation, e.g., PairEval (Park et al., 2024) or JudgeBench (Tan et al., 2025),1 this paper focuses on evaluating the performance of LLMs on generating judgements for categorical and graded responses.We leave the extension of JUDGE-BENCH to include pairwise preference evaluation and other recent evaluation methods, such as Prometheus 2 (Kim et al., 2024), for future work.</p>
<p>Finally, our work mostly focuses on Englishlanguage datasets-with the exception of datasets focussing specifically on machine-translation outputs.It remains to be seen whether LLMs' metaevaluation abilities vary across different languages.</p>
<p>Appendix A Datasets</p>
<p>This section provides brief descriptions of the datasets employed in our study.ToxicChat (Lin et al., 2023).collect binary judgments on the toxicity and 'jailbreaking' nature (prompt hacks deliberately intended to bypass safety policies and induce models to generate unsafe content) of human prompts to LLMs.While the original dataset contains a mix of human-and automatically-annotated instances, here we only consider the human-annotated prompts.</p>
<p>LLMBar (Zeng et al., 2024).LLMBar is a dataset targeted at evaluating the instructionfollowing abilities of LLMs.Each entry of this dataset consists of an instruction paired with two different outputs, one correctly following the instruction and the other deviating from it.LLMBar has an adversarial split where deviating outputs are carefully constructed to 'fool' LLM-based evaluators and a natural split where deviating outputs are more naturalistic.(Aroyo et al., 2023) Toxicity &amp; Safety 1,340 ~70 + ~120 Categorical ✗ Mixed ToxicChat (Lin et al., 2023) Toxicity &amp; Safety 5,654 -Categorical ✗ ✓ Topical Chat (Mehri and Eskenazi, 2020) Dialogue 60 3 Graded + Categorical ✗ ✓ Persona Chat (Mehri and Eskenazi, 2020) Dialogue 60 3 Graded + Categorical ✗ ✓ WMT 2020 En-De (Freitag et al., 2021) Machine Translation 14,122 3 Graded ✗ ✓ WMT 2020 Zh-En (Freitag et al., 2021) Machine Translation 19,974 3 Graded ✗ ✓ WMT 2023 En-De (Kocmi et al., 2023) Machine Translation 6,588 -Graded ✗ ✓ WMT 2023 Zh-En (Kocmi et al., 2023) Machine Translation 13,245 -Graded ✗ ✓ G-Eval / SummEval (Liu et al., 2023 Topical Chat and Persona Chat (Mehri and Eskenazi, 2020).These datasets contain human judgments on the quality of machine-and human-generated responses based on the provided dialogue context.The annotated dialogues were selected from Topical Chat (Gopalakrishnan et al., 2019)-a dataset collecting humanhuman conversations on provided facts-and Persona Chat (Zhang et al., 2018), which contains human-human persona-conditioned conversations.Each response is evaluated on 6 attributes: Understandable, Natural, Maintains Context, Interesting/Engaging, Uses Knowledge, and Overall Quality.</p>
<p>ROSCOE (Golovneva et al., 2023).collect human judgments assessing the quality of GPT-3's reasonings.</p>
<p>The output reasonings are elicited by inputting GPT-3 with questions selected from 4 commonly used reasoning datasets, i.e., CosmosQA (Huang et al., 2019), DROP (Dua et al., 2019), e-SNLI (Camburu et al., 2018) and GSM8K (Cobbe et al., 2021).While ROSCOE provides annotations on each step of the reasoning trace, here we only consider the global judgments over the whole reasoning.</p>
<p>QAGS (Wang et al., 2020).QAGS consists of annotations judging the factual consistency of onesentence model-generated summaries of news articles.The gold-standard summaries and articles are collected from CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018).</p>
<p>Medical-safety (Abercrombie and Rieser, 2022).This dataset consists of 3701 pairs of medical queries (collected from a subreddit on medical advice) and both machine-generated and humangenerated answers.Queries were classified by human annotators according to their severity (from 'Not medical' to 'Serious', with 'Serious' indicating that emergency care would be required) and answers were categorised based on their risk level (from 'Non-medical' to 'Diagnosis/Treatment').</p>
<p>DICES (Aroyo et al., 2023).The DICES datasets consist of a series of machine-generated responses whose safety is judged based on the previous conversation turns (context).While the original dataset provides fine-grained annotations with answers to questions targeting specific aspects of safety, here we only consider the 'overall' categorisation comprehensive of all aspects.In DICES 990 safety is judged by crowdsourced annotators, whereas in DICES 350 both expert and crowdsourced annotations are provided.</p>
<p>Inferential strategies (Mondorf and Plank, 2024).This dataset contains annotations on the logical validity of reasoning steps that modelsin this case, Llama-2-chat-hf3 (Touvron et al., 2023), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a) and Zephyr-7b-beta (Tunstall et al., 2023)-generate when prompted to solve problems of propositional logic.Binary labels are assigned to each response, indicating whether the rationale provided by the model is sound (True) or not (False).Each model is assessed on 12 problems of propositional logic across 5 random seeds, resulting in a total of 60 responses per model.Dailydialog (Wallbridge et al., 2022).Switchboard includes acceptability judgments collected using stimuli from the Switchboard Telephone Corpus (Godfrey et al., 1992).More specifically, the judgments refer to how plausible it is that a specific response belongs to a telephonic dialogue.The same kind of judgments are provided for Dailydialog, which collects written dialogues intended to mimic conversations that could happen in real life.</p>
<p>Switchboard and</p>
<p>Recipe-generation (Stein et al., 2023).This dataset contains human annotations assessing the quality of machine-generated recipes based on 6 attributes: grammar, fluency, verbosity, structure, success, overall.</p>
<p>NewsRoom (Grusky et al., 2018).This dataset includes human judgments the quality of systemgenerated summaries of news articles.More specifically, annotators evaluated summaries across two semantic dimensions (informativeness and relevancy) and two syntactic dimensions (fluency and coherence).</p>
<p>SummEval and G-Eval (Fabbri et al., 2021;Liu et al., 2023).These datasets include summaries generated by multiple recent summarisation models trained on the CNN/DailyMail dataset (Hermann et al., 2015).Summaries are annotated by both expert judges and crowdsourced workers on 4 dimensions: coherence, consistency, fluency, relevance.</p>
<p>WMT 2020 En-De and Zh-En (Freitag et al., 2021).These datasets are a re-annotated version of the English-to-German and Chinese-to-English test sets taken from the WMT 2020 news translation task.The annotation was carried out by raters who are professional translators and native speakers of the target language using a Scalar Quality Metric (SQM) evaluation on a 0-6 rating scale.</p>
<p>WMT 2023 En-De and Zh-En (Kocmi et al., 2023).These datasets are the English-to-German and Chinese-to-English test sets taken from the General Machine Translation Task organised as part of the 2023 Conference on Machine Translation (WMT).In contrast to previous editions, the evaluation of translation quality was conducted by a professional or semi-professional annotator pool rather than utilising annotations from MTurk.Annotators were asked to provide a score between 0 and 100 on a sliding scale.</p>
<p>B The JUDGE-BENCH Data Schema</p>
<p>To facilitate extending our benchmark, we adopt a shared schema used to pre-process all datasets.Our publicly available code base includes an example2 of this format as well as instructions on how to verify that newly added datasets comply with it.</p>
<p>The Json-based JUDGE-BENCH data schema ensures that the following fields are included for each dataset:</p>
<p>• dataset: the name of the dataset;</p>
<p>• dataset_url: the URL where the original dataset can be downloaded, as opensourced by their creators; • annotations: an overview of the properties annotated for each dataset, along with information on how they are measured and prompt-like instructions similar to those originally given to the human annotators (when applicable); • instances: dataset instances including the piece of text to be judged, aggregated human judgments and, when available, individual human annotations.We note that, while we do not systematically explore inter-annotator variations at the instance level, the data schema we adopt allows for conducting this type of analysis in future work.</p>
<p>C Upper Bound Estimation for Model Correlations</p>
<p>Whenever multiple human annotations were publicly available for a property (see Table 3 for inter-annotator agreement scores), we computed upper-bound estimates for the correlations achievable by models.The intuition behind these estimates, borrowed from neuroscience (Nili et al., 2014), is that the maximum correlation a model can achieve with aggregated human responses is bounded by the average correlation between singleparticipant responses and the aggregated responses across participants.We applied a similar logic to the human judgments used in the present study and combined it with a bootstrapping approach.For each annotated property, we bootstrapped singleparticipant responses by sampling 1000 times from the available human responses, excluding data points where a single annotation was available.Next, we computed the alignment between each of the bootstrapped-participant arrays and the array of aggregated responses.Alignment was computed as Spearman's correlation for graded judgments and Cohen's kappa for categorical judgments.Finally, we estimated the upper bound as the average of the 1000 alignment measures.In cases where alignment between bootstrapped and aggregated responses could not be computed-because the variance of the bootstrapped responses was nullvalues were replaced with an average of the 'nonnan' correlations.We emphasise that these upper bounds are estimates and, as such, are subject to errors.Therefore, it may happen that model performance exceeds these upper bounds.</p>
<p>D Properties with Graded Judgments</p>
<p>In Figure 3, we display results for a set of graded properties annotated in one or more of the datasets we consider.The properties are defined as follows:</p>
<p>• Acceptability refers to whether it is plausible or not that a response belongs to a telephonic dialogue and was annotated in Swithboard and Dailydialog;</p>
<p>• Coherence was annotated for summaries and model-generated reasonings as part of the datasets NewsRoom, ROSCOE, and SummEval;</p>
<p>• Consistency refers to the alignment between facts described in a summary and in its source text, and was annotated in SummEval;</p>
<p>• Engaging indicates whether a response generated in the context of a dialogue is dull or interesting and was annotated in TopicalChat and PersonaChat;</p>
<p>• Fluency measures whether a piece of text is grammatically correct and well-formatted, and was annotated in NewsRoom, SummEval and Recipegeneration;</p>
<p>• Informativeness refers to the extent to which a summary captures the key points of the full text, and was annotated for summaries as part of the NewsRoom dataset;</p>
<p>• Relevance refers to whether a summary selects important information as opposed to including redundancies, and was annotated for NewsRoom and SummEval;</p>
<p>• Verbosity indicates whether a generated recipe is concise and avoids unnecessary repetitions, and was annotated in Recipe-generation.</p>
<p>E Inference Details</p>
<p>All open-model checkpoints were obtained using the HuggingFace pipeline and we access all proprietary models using their corresponding API libraries.The proprietary models were accessed from 06-06-2024 to 13-06-2024, for standard prompting and from 09-10-2024 to 13-12-2024, for CoT prompting.We obtain the model responses using greedy decoding, which we operationalise for the proprietary models by setting the temperature parameter to 0. We allow open models to generate a maximum of 25 new tokens and proprietary models to generate a maximum of 5 new tokens.For CoT prompting, we allow for a maximum of 1000 new tokens.We leverage Nvidia A100 (80 GB) GPUs for a total of 321 compute hours.The cost of running experiments using Gemini-1.5-flashwas C30.31, while the cost of experiments using GPT-4o was approximately $565.  5 and 6.</p>
<p>F Valid Response Rates</p>
<p>Starling-7B</p>
<p>OLMo</p>
<p>G More Details on Toxicity and Safety Evaluation</p>
<p>For the Medical-safety dataset, models often refused to answer.Instead they tended to generate explanations, copy what they had in the prompt, or tried to be generally helpful because they saw that it was a medical issue.Since we take a random answer when no answer could be detected, this contributes to lower the results obtained on this task.Scores for the DICES dataset were also low, even though the valid response rate was high, because in this case there is the 'Unsure' option, which (along with 'Unsafe') models preferred over calling anything 'Safe'.For ToxicChat, models performed reasonably well.</p>
<p>H Additional Results</p>
<p>In Table 6 we report human-model alignment scores per dataset for all models tested, thus complementing Table 1 in the paper.</p>
<p>Chain-of-Thought Prompts.For the results with CoT prompting, we use the same original instructions used to gather human judgments as prompts for the model but adapt the additional guidelines to emphasise multi-step reasoning rather than constrain the models' output.Specifically, we append the original instructions with the following additional guideline: 'Always end your answer with either {} regarding the entire context.Let's think step by step.', in which {} is replaced with an enumeration of all possible answer labels formatted as 'Therefore, {label A} is correct, or therefore, {label B} is correct, or therefore [...].'.This also allows for automatically extracting the final answers from model responses during evaluation.In this study, we evaluate nine models and exclude Mixtral-8x22B and Comm-R+ due to computational constraints.For the CoLa-grammar dataset, we obtain GPT-4o responses only for ten percent of its instances (that are randomly sampled) to address the slow processing times and rate limitations.While CoT prompting leads to improved agreement scores and correlations when used with some models for certain datasets (see Table 7), its overall effectiveness compared to the results obtained using standard prompts without CoT (see Table 6) is inconsistent.</p>
<p>Prompt Paraphrases.We experiment with paraphrased prompts for three datasets that models struggle with: DICES-350-expert, WMT 2023 En-De, and WMT 2023 Zh-En.The paraphrase for dices-350-expert elaborates on the concept of safety, compared to its short original prompt, whereas the paraphrases for the WMT datasets are more concise regarding what comprises a good translation compared to the original.We do not observe consistent improvements when using paraphrased prompts compared to the original prompts (Table 4).</p>
<p>Few-shot Prompts.For the three datasets above-DICES-350-expert, WMT 2023 En-De, and WMT 2023 Zh-En-we also experiment with few-shot prompts (Table 4), where we provide the model with 6 examples for DICES-350-expert, 3 of safe conversations and 3 of unsafe conversations, and 4 examples for each WMT 2023 dataset, 2 of high-scoring translations and 2 of low-scoring translations.Using few-shot prompts does not improve correlations for dices-350-expert.On the WMT 2023 datasets, we observe higher correlations for Llama 3.1 8B but very moderate or no improvements on the other two models.Given that these improvements are inconsistent across datasets, we did not scale up the experiments to all 20 datasets and 11 models.</p>
<p>Your task is to evaluate the quality of machine translation output on a scale from 0 to 100 [...].Evaluation Criteria: [...] Source: Great backpack but overkill on the straps Reference: Toller Rucksack, aber bei den Riemen übertrieben Translation: Toller Rucksack, aber übertrieben auf den Riemen</p>
<p>Figure 1 :
1
Figure 1: Evaluation by expert and non-expert human annotators and by LLMs for two tasks involving humangenerated (left) and machine-generated text (right).</p>
<p>Figure 2 :
2
Figure 2: Average model correlation with human experts vs. non-experts in datasets with graded annotations.</p>
<p>Figure 3 :
3
Figure 3: Correlation for properties with graded judgments.Averages and error bars when the property is present in more than one dataset.</p>
<p>Figure 4 :
4
Figure4: Scores (Cohen's κ for categorical annotations and Spearman's correlation for graded annotations) on test items involving human language vs. machine-generated outputs.</p>
<p>Figure 5 :
5
Figure 5: Valid response rate per model.</p>
<p>Figure 6 :
6
Figure 6: Average ratios of valid responses across datasets over the 11 models we tested.</p>
<p>Philipp Mondorf and Barbara Plank.2024.Comparing inferential strategies of humans and large language models in deductive reasoning.In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9370-9402, Bangkok, Thailand.Association for Computational Linguistics.Lianghui Zhu, Xinggang Wang, and Xinlong Wang.2025.JudgeLM: Fine-tuned Large Language Models are Scalable Judges.In Proceedings of the 13th International Conference on Learning Representations (ICLR).arXiv.ArXiv:2310.17631 [cs].
Alex Warstadt and Samuel R. Bowman. 2020. Linguis-Rickard Stureborg, Dimitris Alikaniotis, and Yoshitic analysis of pretrained sentence encoders with ac-Suhara. 2024.Large language models areceptability judgments. Preprint, arXiv:1901.03438.inconsistent and biased evaluators.Preprint,arXiv:2405.01724.Ben Naismith, Phoebe Mulcaire, and Jill Burstein. 2023. Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-man. 2019. Neural Network Acceptability Judg-ments. Transactions of the Association for Com-putational Linguistics, 7:625-641.Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2025. JudgeBench: A benchmark for evaluating LLM-Automated evaluation of written discourse coherence Jason Wei, Xuezhi Wang, Dale Schuurmans, Maartenbased judges. In The Thirteenth International Con-using GPT-4. In Proceedings of the 18th Workshop Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,ference on Learning Representations.on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 394-403, Toronto, Canada. Association for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. and Denny Zhou. 2022. Chain-of-Thought prompt-ing elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex-treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-guage Processing, pages 1797-1807, Brussels, Bel-gium. Association for Computational Linguistics. Laura Weidinger, Maribeth Rauh, Nahema Marchal, Ar-ianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Grif-fin, Ben Bariach, et al. 2023. Sociotechnical safety evaluation of generative ai systems. arXiv preprint arXiv:2310.11986. Hamed Nili, Cai Wingfield, Alexander Walther, Li Su, William Marslen-Wilson, and Nikolaus Kriegeskorte. Minghao Wu and Alham Fikri Aji. 2025. Style over sub-2014. A toolbox for representational similarity anal-stance: Evaluation biases for large language models. ysis. PLoS computational biology, 10(4):e1003553. In Proceedings of the 31st International ConferencePetter Törnberg. 2023. ChatGPT-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. arXiv preprint arXiv:2304.06588. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda-tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar San-on Computational Linguistics, pages 297-312, Abu OpenAI. 2024. Gpt-4o model card. Dhabi, UAE. Association for Computational Linguis-seviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of lm alignment. Preprint,tics.arXiv:2310.16944.Wenda Xu, Guanglei Zhu, Xuandong Zhao, LiangmingPat Verga, Sebastian Hofstatter, Sophia Althammer, Yix-Pan, Lei Li, and William Wang. 2024. Pride and Prej-uan Su, Aleksandra Piktus, Arkady Arkhangorodsky,udice: LLM Amplifies Self-Bias in Self-Refinement. Maja Pavlovic and Massimo Poesio. 2024. The ef-In Proceedings of the 62nd Annual Meeting of the fectiveness of LLMs as annotators: A comparative Association for Computational Linguistics (Volume 1: overview and empirical analysis of direct represen-Long Papers), pages 15474-15492, Bangkok, Thai-tation. In Proceedings of the 3rd Workshop on Per-spectivist Approaches to NLP (NLPerspectives) @ land. Association for Computational Linguistics.Minjie Xu, Naomi White, and Patrick Lewis. 2024. Replacing Judges with Juries: Evaluating LLM Gen-erations with a Panel of Diverse Models. arXiv preprint arXiv:2404.18796. Sarenne Carrol Wallbridge, Catherine Lai, and PeterLREC-COLING 2024, pages 100-110, Torino, Italia. ELRA and ICCL. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024. Evaluating large lan-guage models at evaluating instruction following. In Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste The Twelfth International Conference on Learning Representations. Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-rat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-locking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per-sonalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Ehud Reiter. 2024. Can LLM-based eval replace human Meeting of the Association for Computational Lin-evaluation? Blog post. guistics (Volume 1: Long Papers), pages 2204-2213,Bell. 2022. Investigating perception of spoken dia-logue acceptability through surprisal. In Proc. Inter-speech 2022, pages 4506-4510. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac-tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Compu-tational Linguistics, pages 5008-5020, Online. Asso-ciation for Computational Linguistics. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Melbourne, Australia. Association for Computationaland Jie Zhou. 2023. Is ChatGPT a good NLG evalua-Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Linguistics.tor? a preliminary study. In Proceedings of the 4thXinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Dur-rett. 2025. To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning. In Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. The Thirteenth International Conference on Learning Judging LLM-as-a-judge with MT-Bench and Chat-Representations. bot Arena. Advances in Neural Information Process-New Frontiers in Summarization Workshop, pages 1-11, Singapore. Association for Computational Lin-guistics. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong,ing Systems, 36.Qi Liu, Tianyu Liu, and Zhifang Sui. 2024. Large lan-guage models are not fair evaluators. In Proceedingsof the 62nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 9440-9450, Bangkok, Thailand. Associationfor Computational Linguistics.
ChaeHun Park, Minseok Choi, DohyunLee, and Jaegul  Choo.2024.PairEval: Open-domain dialogue evaluation with pairwise comparison.In First Conference on Language Modeling.Katharina Stein, Lucia Donatelli, and Alexander Koller.2023.From sentence to action: Splitting AMR graphs for recipe instructions.In Proceedings of the Fourth International Workshop on Designing Meaning Representations, pages 52-67, Nancy, France.Association for Computational Linguistics.Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao.2024.Starling-7B: Improving LLM helpfulness &amp; harmlessness with RLAIF.In First Conference on Language Modeling.</p>
<p>Table 2 summarises relevant dataset information.Note that dataset sizes as reported in Table 2 refer to the number of annotated samples (not to the total number of collected annotations) and might therefore differ from the figures reported in the original papers.Table 3 reports Krippendorf's α for those datasets with multiple public human annotations.
CoLa (Warstadt et al., 2019). The Corpus ofLinguistic Acceptability (CoLA) consists of 10657sentences from 23 linguistics publications, expertlyannotated for acceptability (grammaticality) bytheir original authors.CoLa-grammar (Warstadt and Bowman, 2020).The dataset consists of a grammatically annotatedversion of the CoLA development set. Each sen-tence in the CoLA development set is labelled withboolean features indicating the presence or absenceof a particular grammatical construction (usuallysyntactic in nature). Two related sets of featuresare considered: 63 minor features correspond tofine-grained phenomena, and 15 major features cor-respond to broad classes of phenomena.</p>
<p>Table 2 :
2
Balloccu et al. (2024)eatures of the datasets considered in the study.Note that 'Size' refers to the number of annotated samples, not to the total number of human annotations.'#Annot.'refers to the number of available individual annotations, if any, which we use to estimate the human upper bound.Note that datasets with only a single annotation per sample, or which only report the average over multiple annotations are not included in '# Annot.'.Information on possible data leakage was retrieved fromBalloccu et al. (2024).
)Summarisation1,600-Graded✓✓QAGS (Wang et al., 2020)Summarisation9533 Categorical✓✗NewsRoom (Grusky et al., 2018)Summarisation4203 Graded✓✗✓LLMBar (Zeng et al., 2024)Instruction Following 419-Categorical✓✓✗</p>
<p>Table 3 :
3
Inter-rater agreement for datasets with multiple human annotations.Datasets in blue concern humangenerated language, while those in red concern modelgenerated text.
DatasetKrippendorf's αTopical Chat0.08CategoricalQAGS DICES-990 DICES-350-crowdsourced Persona Chat0.49 0.14 0.16 0.33Inferential strategies1.0Dailydialog0.59Switchboard0.57GradedPersona Chat Topical Chat Recipe-generation NewsRoom0.33 0.08 0.41 0.11WMT 2020 En-De0.5WMT 2020 Zh-En0.09</p>
<p>Table 5 reports the rate of valid responses for each model and dataset.Valid response rates are summarised per model and dataset in Figures</p>
<p>Table 4 :
4
Cohen's kappa for DICES-350-expert and Spearman's correlation for two WMT 2023 datasets, comparing the original prompt and CoT prompt to few-shot prompts and prompt paraphrases for a selection of models.For datasets with more than one paraphrased prompt, we report the average and standard deviation across paraphrases.For Spearman's correlations, we report the number of significant correlations (p &lt; 0.05) for each model and dataset in brackets.
1.0Valid Response ratio0.0 0.2 0.4 0.6 0.80.740.870.880.890.890.920.920.920.930.94 0.94 0.940.940.950.950.960.960.960.960.960.960.960.97 Acceptability 0.97 0.97 Task Reasoning Summarisation Dialogue Toxicity \ Safety Translation Instruction Following 0.98 Planningmedical-safetyinferential-strategiesdices-350-crowdsourcedrecipe-crowd-sourcing-datadices-350-expertdices-990summevalllmbar-naturaldailydialog-acceptabilitywmt-23-en-dewmt-23-zh-enwmt-human-zh-enllmbar-adversarialwmt-human-en-deqagsroscoe-droppersona-chatroscoe-cosmoscola-grammartopical-chatroscoe-esnlitoxic-chatswitchboard-acceptabilitynewsroomroscoe-gsm8kcola
Some time after an early version of this paper became available as a pre-print, accompanied by our Judge-Bench code, the independent work by Tan et al. (2025) appeared, which describes a benchmark that the authors named JudgeBench. This name clash is unfortunate, but since in the meantime our paper has seen some uptake, we have decided against trying to resolve it.
https://github.com/dmg-illc/JUDGE-BENCH/blob/ master/data/example.json
AcknowledgementsThis work emerged from discussions at a workshop organised by Raquel Fernández and Sandro Pezzelle at MFO, the Oberwolfach Research Institute for Mathematics in the German Black Forest, on behalf of the ELLIS NLP programme.The event was funded by the state of Baden-Württemberg (Germany) and organised in collaboration with the ELLIS Institute Tübingen and the Max Planck Institute for Intelligent Systems.We furthermore acknowledge our funders.In particular, AB, RF, and AT were supported by the European Research Council (ERC Consolidator Grant DREAM 819455 to RF), as well as BP (ERC Consolidator Grant DIALECT 101043235 to BP).DE was supported by a research grant (VIL53122) from VILLUM FONDEN.EG was supported by the Dutch Research Council (Gravitation grant 024.001.006 to the Language in Interaction consortium).MG was supported by an ETH Zurich Postdoctoral Fellowship.MH was supported in part by an OpenAI Superalignment Fellowship.AM was supported by the European Research Council (DECOLLAGE, ERC-2022-CoG 101088763) and by Fundação para a Ciência e Tecnologia through contract UIDB/50008/2020.We acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy).-0.02 ±0.14 (0) -0.09 ±0.17 (1) 0.03 ±0.13 (0) -0.06 ±0.14 (0) Topical Chat (4) 0.26 ±0.03 (2) 0.28 ±0.1 (2) 0.13 ±0.04 (0) 0.17 ±0.12 (1) 0.21 ±0.18 (1) 0.14 ±0.05 (0) 0.07 ±0.07 (0) 0.15 ±0.13 (0) 0.29 ±0.11 (3) 0.14 ±0.16 (1) 0.08 ±0.21 (1) Recipe-generation (6) 0.78 ±0.05 (6) 0.66 ±0.07 (6) 0.6 ±0.15 (6) 0.67 ±0.09 (5) 0.57 ±0.24 (5) 0.32 ±0.28 (5) 0.06 ±0.26 (3) 0.34 ±0.09 (5) 0.28 ±0.08 (4) 0.04 ±0.17 (1) 0.1 ±0.08 (0) ROSCOE-GSM8K (2) 0.82 ±0.12 (2) 0.83 ±0.11 (2) 0.81 ±0.14 (2) 0.81 ±0.12 (2) 0.79 ±0.13 (2) 0.68 ±0.2 (2) 0.7 ±0.08 (2) 0.76 ±0.15 (2) 0.63 ±0.18 (2) 0.46 ±0.13 (2) 0.1 ±0.07 (1) ROSCOE-eSNLI (2) 0.49 ±0.24 (2) 0.4 ±0.16 (2) 0.38 ±0.17 (2) 0.35 ±0.21 (2) 0.32 ±0.12 (2) 0.09 ±0.08 (0) 0.28 ±0.21 (1) 0.19 ±0.16 (1) 0.32 ±0.12 (2) 0.11 ±0.06 (0) 0.11 ±0.17 (1) ROSCOE-DROP (2) 0.57 ±0.22 (2) 0.59 ±0.16 (2) 0.44 ±0.15 (2) 0.44 ±0.13 (2) 0.32 ±0.12 (2) 0.21 ±0.22 (1) 0.37 ±0.18 (2) 0.23 ±0.1 (2) 0.22 ±0.22 (1) 0.16 ±0.17 (1) 0.15 ±0.21 (1) ROSCOE-CosmosQA (2) 0.57 ±0.18 (2) 0.55 ±0.18 (2) 0.51 ±0.16 (2) 0.57 ±0.17 (2) 0.53 ±0.21 (2) 0.33 ±0.25 (2) 0.48 ±0.17 (2) 0.44 ±0.26 (2) 0.57 ±0.2 (2) 0.13 ±0.04 (1) 0.49 ±0.24 (2) NewsRoom (4) 0.59 ±0.02 (4) 0.59 ±0.03 (4) 0.44 ±0.05 (4) 0.55 ±0.03 (4) 0.5 ±0.07 (4) 0.36 ±0.06 (4) 0.16 ±0.05 (4) 0.45 ±0.04 (4) 0.26 ±0.06 (4) 0.21 ±0.08 (4) -0.01 ±0.04 (0) SummEval (4) 0.35 ±0.06 (4) 0.44 ±0.14 (4) 0.54 ±0.08 (4) 0.38 ±0.02 (4) 0.48 ±0.02 (4) 0.19 ±0.06 (4) 0.13 ±0.06 (4) 0.29 ±0.09 (4) 0.4 ±0.12 (4Table 6: Scores per dataset for all models we evaluate: Cohen's kappa for categorical annotations and Spearman's correlation for graded annotations.For Spearman's correlations, we report the number of significant correlations (p &lt; 0.05) for each model and dataset in brackets.Datasets in blue concern human-generated language while those in red concern model-generated text.0.11 ±0.06 (0) 0.06 ±0.15 (0) 0.17 ±0.21 (1) -0.04 ±0.22 (1) 0.04 ±0.13 (0) -0.01 ±0.22 (1) 0.06 ±0.18 (0) Topical Chat (4) 0.22 ±0.02 (0) 0.14 ±0.13 (1) 0.11 ±0.1 (0) 0.06 ±0.18 (1) 0.1 ±0.14 (0) 0.16 ±0.17 (1) 0.25 ±0.05 (2) 0.17 ±0.09 (1) 0.08 ±0.13 (0) Recipe-generation (6) 0.67 ±0.12 (6) 0.64 ±0.14 (6Table 7: Scores per dataset for all models we evaluate using CoT prompts: Cohen's kappa for categorical annotations and Spearman's correlation for graded annotations.For Spearman's correlation, we report the number of significant correlations for each model and dataset in brackets.Datasets in blue concern human-generated language while those in red concern model-generated text.
Riskgraded safety for handling medical queries in conversational AI. Gavin Abercrombie, Verena Rieser, 10.18653/v1/2022.aacl-short.30Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingOnline only. Association for Computational Linguistics20222Short Papers)</p>
<p>Large language models as financial data annotators: A study on effectiveness and efficiency. D Toyin, Suchetha Aguda, Elena Siddagangappa, Simerjot Kochkina, Dongsheng Kaur, Charese Wang, Smiley, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>A I , Meta , Llama 3.1 model card. 2024</p>
<p>Dices dataset: Diversity in conversational ai evaluation for safety. Lora Aroyo, Alex Taylor, Mark Díaz, Christopher Homan, Alicia Parrish, Gregory Serapio-García, Vinodkumar Prabhakaran, Ding Wang, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Leak, cheat, repeat: Data contamination and evaluation malpractices in closedsource LLMs. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondrej Dusek, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers)</p>
<p>Pitfalls of conversational LLMs on news debiasing. Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek, Proceedings of the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024. the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024Torino, ItaliaELRA and ICCL2024</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. 201831</p>
<p>Exploring the use of large language models for reference-free text quality evaluation: An empirical study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, 10.18653/v1/2023.findings-ijcnlp.32Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings). BaliAssociation for Computational Linguistics2023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Chatbot arena: an open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I Jordan, Joseph E Gonzalez, Ion Stoica, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024ICML'24. JMLR.org</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, ArXiv, abs/2110.141682021</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202436</p>
<p>SummEval: Re-evaluating Summarization Evaluation. Alexander R Fabbri, Wojciech Kryściński, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 92021</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, F T André, Graham Martins, Ankush Neubig, Jonathan H Garg, Markus Clark, Orhan Freitag, Firat, Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine Translation2023</p>
<p>Experts, errors, and context: A large-scale study of human evaluation for machine translation. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, Wolfgang Macherey, 10.1162/tacl_a_00437Transactions of the Association for Computational Linguistics. 20219</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 12030e23050161202023</p>
<p>Switchboard: Telephone speech corpus for research and development. John J Godfrey, Edward C Holliman, Jane Mc-Daniel, Acoustics, speech, and signal processing. IEEE Computer Society19921</p>
<p>ROSCOE: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Peng Chen, Spencer Poff, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations. Martin Corredor,. 2023</p>
<p>Topical-chat: Towards knowledge-grounded open-domain conversations. Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur, 10.21437/Interspeech.2019-3079Proc. Interspeech. Interspeech2019. 2019</p>
<p>Olmo: Accelerating the science of language models. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. Max Grusky, Mor Naaman, Yoav Artzi, 10.18653/v1/N18-1065Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers; New Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian De Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in neural information processing systems. 201528</p>
<p>ChatGPT rates natural language explanation quality like humans: But on which scales?. Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/D19-1243Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023aarXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , 10.18653/v1/2023.acl-long.792Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, 10.18653/v1/2024.emnlp-main.248Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Masaaki Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, Mariya Shmatova, 10.18653/v1/2023.wmt-1.1Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational LinguisticsJun Suzuki. 2023</p>
<p>GEMBA-MQM: Detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, 10.18653/v1/2023.wmt-1.64Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023a</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023b</p>
<p>Benchmarking cognitive biases in large language models as evaluators. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, 10.18653/v1/2024.findings-acl.29Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Memories for virtual AI characters. Fabian Landwehr, Erika Varis Doggett, Romann M Weber, 10.18653/v1/2023.inlg-main.17Proceedings of the 16th International Natural Language Generation Conference. the 16th International Natural Language Generation ConferencePrague, Czechia2023Association for Computational Linguistics</p>
<p>Leveraging large language models for NLG evaluation: Advances and challenges. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma, 10.18653/v1/2024.emnlp-main.896Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang, 10.18653/v1/2023.findings-emnlp.311Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, First Conference on Language Modeling. 2024a</p>
<p>LLMs as narcissistic evaluators: When ego inflates evaluation scores. Yiqi Liu, Nafise Moosavi, Chenghua Lin, 10.18653/v1/2024.findings-acl.753Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b</p>
<p>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark Gales, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers)</p>
<p>USR: An unsupervised and reference free evaluation metric for dialog generation. Shikib Mehri, Maxine Eskenazi, 10.18653/v1/2020.acl-main.64Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>            </div>
        </div>

    </div>
</body>
</html>